id,text,summary
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,c,"Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
9,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
c,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,c,"Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
c,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,c,"Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
27,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
28,"Despite the unprecedented zero-shot capacity and photorealism achieved by the recent progress in text-to-image
synthesis [3, 20, 32?34, 36, 46], the current state-of-the-art
models still struggle with text prompts containing multiple objects and attributes with complex spatial relationships
among them [2, 8, 9, 12, 44]. Some objects, attributes, and
spatial compositions specified in the text prompts are often
swapped or completely missing in the synthesized image.
Our work aims to mitigate this problem by grounding the
text-to-image synthesis using explicit layouts without extra
training and auxiliary models.
The deep level of language understanding exhibited by
these models can be attributed to using pretrained language
models [30] as the text encoder [36]. The computed text
embeddings are processed using the cross-attention layers
in the denoising models [26, 27]. Upon careful analysis of
the failure example generated by Stable Diffusion [34], we
identify a potential cause of the failure above in the attention
layers [41], where the pixels with similar features produce
similar attention queries and consequently attend to a similar set of regions or tokens. The information of these pixels could thus be mixed through the attention layers. Note
that such pixels could come from two different objects with
similar features. For example, given the prompt ?A dog on
the right of a cat?, a pixel associated with the token ?dog?
could have similar features to the pixels in the ?cat? region.
As a result, the model could incorrectly attend to the ?cat?
token through the cross-attention layers or the ?cat? region
through self-attention layers, causing the missing object or
blended attribute issues.
Previous studies propose to mitigate this issue by manipulating the cross-attention maps during the sampling process [8, 9, 12]. However, existing work neglects that the
same feature mixing issue also occurs in the self-attention
layers. One immediate question when addressing this issue in self-attention layers is how to discriminate the pixels that are truly from the same object and those pixels that
have similar features. To this end, we leverage explicit layout representations following the previous works [9,23]. In
this paper, we propose two novel losses based on the input layout during the sampling process to refocus the attention in both self- and cross-attention layers. Our attentionrefocusing losses show that the attention can be effectively
refocused to the desired region instead of a similar but irrelevant region.
We use up-to-date LLMs to generate explicit layout representations in our attention-refocusing losses. We demonstrate that these models have strong spatial reasoning capabilities. We design prompts with in-context learning to
query LLMs about the spatial relationship of the objects
given a challenging text prompt. The LLMs are asked to
either predict the location of the objects or draw visual representations like vector graphics directly.
We show that when combining the bounding boxes generated by GPT4 [28] and our attention-refocusing losses,
our method significantly and consistently improves over
several strong baselines on the DrawBench [36] and HRS
benchmarks [2]. Our main contributions are summarized
below:
? We propose attention-refocusing losses to regularize
both cross- and self-attention layers during the sampling to improve the controllability given the layout;
? We explore using LLMs to generate layouts given text
prompts, allowing the exploitation of the up-to-date
LLMs with trained text-to-image models;
? We perform a comprehensive experiment on existing methods of grounded text-to-image generation and
show that our method compares favorably against the
state-of-the-art models.
","Driven by scalable diffusion models trained on largescale paired text-image datasets, text-to-image synthesis
methods have shown compelling results. However, these
models still fail to precisely follow the text prompt when
multiple objects, attributes, and spatial compositions are
involved in the prompt. In this paper, we identify the potential reasons in both the cross-attention and self-attention
layers of the diffusion model. We propose two novel losses
to refocus the attention maps according to a given layout
during the sampling process. We perform comprehensive
experiments on the DrawBench and HRS benchmarks using
layouts synthesized by Large Language Models, showing
that our proposed losses can be integrated easily and effectively into existing text-to-image methods and consistently
improve their alignment between the generated images and
the text prompts"
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
40,,
