Unnamed: 0,titles,abstract,introduction
316,Deep Person Re-Identification with Improved Embedding and Efficient Training.txt,"Person re-identification task has been greatly boosted by deep convolutional
neural networks (CNNs) in recent years. The core of which is to enlarge the
inter-class distinction as well as reduce the intra-class variance. However, to
achieve this, existing deep models prefer to adopt image pairs or triplets to
form verification loss, which is inefficient and unstable since the number of
training pairs or triplets grows rapidly as the number of training data grows.
Moreover, their performance is limited since they ignore the fact that
different dimension of embedding may play different importance. In this paper,
we propose to employ identification loss with center loss to train a deep model
for person re-identification. The training process is efficient since it does
not require image pairs or triplets for training while the inter-class
distinction and intra-class variance are well handled. To boost the
performance, a new feature reweighting (FRW) layer is designed to explicitly
emphasize the importance of each embedding dimension, thus leading to an
improved embedding. Experiments on several benchmark datasets have shown the
superiority of our method over the state-of-the-art alternatives on both
accuracy and speed.","Person reidentiﬁcation aims to reidentify a query per son across multiple nonoverlapping cameras. The task is challenging since pedestrian images from different camera views suffer from large variations in poses, lightings and backgrounds. Many earlier works solve the reidentiﬁcatio n problem by dividing it into two separated parts: feature ex traction [13, 14, 36, 32, 9, 11, 16, 31] and metric learn ing [18, 34, 35, 3, 10, 13, 14]. A large number of hand crafted features are designed to enhance the robustness of pedestrian images to pose, viewpoint and illumination changes. After the feature is extracted, metric learning is ∗Corresponding author. 1See the code on https://github.com/jhb86253817/tfreidFigure 1: The difference between the stateoftheart CNN [6, 39] and our proposed CNN for person reidentiﬁcation. (a) The current best CNN has two branches, which takes a pair of images as input. (b) Our proposed CNN does not re quire image pairs or triplets for training since it utilizes the combination of identiﬁcation loss and center loss. More over, a new feature reweighting (FRW) layer is designed so that the importance of each embedding dimension can be adaptively adjusted. applied to learn a metric for the features so that the im ages of the same person are close while the ones of different pedestrians are far away from each other in the metric space. In recent years, convolutional neural networks (CNNs) have achieved promising results on person reidentiﬁcation [1, 4, 5, 6, 12, 15, 19, 20, 23, 24, 25, 26, 29, 33, 39, 2, 40] due to their advantages on feature learn ing. Different from previous works, CNNs learn features and metrics jointly from data in an endtoend manner. Then an embedding is learned to measure the similarities between images using Euclidean distance. The loss func tion of a CNN plays an important role on its performance. Veriﬁcation loss [1, 4, 5, 12, 19, 23, 24, 25, 26, 33] is popular among CNNs on person reidentiﬁcation task beneﬁted from its simple motivation: reducing the variance between intraclass embeddings while increasing the distinction between interclass ones. However, veriﬁcati on loss takes image pairs or triplets for training, the number o f which grows rapidly as the number of classes grows. When(a) without FRW layer  (b) with FRW layer Figure 2: The effect of applying FRW layer. (a) Without FRW layer, the 2D embeddings of a matched image and a nonmatched image have the same distance to the query embedding. (b) With FRW layer, the distance between the matched embedding and the query is closer than the non matched one because the more essential dimension is en larged while the less important one is shrinked. there are numerous person identities, veriﬁcation loss is likely to show slow convergence and unstable performance. Identiﬁcation loss is usually used for classiﬁcation task, and it has also been applied to person reidentiﬁcation task [29, 38, 30] due to its simplicity and discriminative ability. Though identiﬁcation loss can separate intercla ss embeddings efﬁciently, it does not explicitly reduce the intraclass variance. Thus, the performance may be limited since the embeddings of the same person can have large distance on test data due to viewpoint, pose and background variations. To absorb the merits of the above two losses, recent works [39, 6] tend to combine them (please see Fig. 1a), and have achieved promising results. However, the inefﬁciency issue from veriﬁcation loss still remains despite their performance improvement. In addition to the inefﬁciency problem, the existing deep embedding models draw little attention to the impor tance of each embedding dimension. They simply accu mulate the squared difference of each dimension ( i.e. Eu clidean distance) to measure the distance between embed dings [4, 5, 6, 24, 25, 29, 39]. In other words, each di mension of an embedding contributes equally to the total distance. Imagine that a matched embedding and a non matched embedding have the same distance to the embed ding of the query image (Fig. 2a), Euclidean distance method is not able to distinguish the matched one from the two. If a model learns to measure the importance of each dimension, then reweights the embeddings so that the im portant dimension is emphasized while the unimportant one is depressed (Fig. 2b), such problem can be alleviated. Un fortunately, few works have considered the importance of different embedding dimensions. To overcome the above two shortcomings, this paperproposes a new CNN model for person reidentiﬁcation. Speciﬁcally, we employ identiﬁcation loss with center loss to train CNN, which does not require image pairs or triplets as input. Center loss [28] aims to pull images to the cor responding class center so that the intraclass variance is reduced. It functions similarly as veriﬁcation loss but the learning process is more efﬁcient. Meanwhile, a new fea ture reweighting (FRW) layer to adaptively learn the impor tance of each dimension has been designed. The FRW layer is placed after the embedding layer, performing element wise multiplication upon its input. By doing so, the model gains the freedom to explicitly adjust the scales of the learned embeddings so that some less important features could be squeezed to avoid overﬁtting. Fig. 1b shows the structure of our proposed CNN. The contributions of this paper can be summarized as follows: •We employ identiﬁcation loss with center loss to train a deep CNN model without constructing image pairs or triplets as input, thus improving the training efﬁciency. •We design a new FRW layer to explicitly emphasize the importance of each embedding dimension, leading to an improved embedding to boost the performance. •Experiments on CUHK03 [12], CUHK01 [11] and VIPeR [7] have validated the superiority of our method over the stateofthearts. 2. Related Work "
149,A Dual Approach to Scalable Verification of Deep Networks.txt,"This paper addresses the problem of formally verifying desirable properties
of neural networks, i.e., obtaining provable guarantees that neural networks
satisfy specifications relating their inputs and outputs (robustness to bounded
norm adversarial perturbations, for example). Most previous work on this topic
was limited in its applicability by the size of the network, network
architecture and the complexity of properties to be verified. In contrast, our
framework applies to a general class of activation functions and specifications
on neural network inputs and outputs. We formulate verification as an
optimization problem (seeking to find the largest violation of the
specification) and solve a Lagrangian relaxation of the optimization problem to
obtain an upper bound on the worst case violation of the specification being
verified. Our approach is anytime i.e. it can be stopped at any time and a
valid bound on the maximum violation can be obtained. We develop specialized
verification algorithms with provable tightness guarantees under special
assumptions and demonstrate the practical significance of our general
verification approach on a variety of verification tasks.","Neural networks and deep learning have revolutionized machine learning achieving state of the art performance on a wide range of complex prediction tasks [Krizhevsky et al., 2012, Goodfellow et al., 2016]. However, in re cent years, researchers have observed that even state of the art networks can be easily fooled into changing their dvij@cs.washington.edupredictions by making small but carefully chosen modiﬁ cations to the input data (known as adversarial perturba tions ) [Szegedy et al., 2013, Kurakin et al., 2016, Carlini and Wagner, 2017a, Goodfellow et al., 2014, Carlini and Wagner, 2017b]. While modiﬁcations to neural network training algorithms have been proposed to mitigate this phenomenonMadry et al. [2018], a comprehensive so lution that is fully robust to adversarial attacks remains elusive [Carlini and Wagner, 2017b, Uesato et al., 2018]. Neural networks are typically tested using the standard machine learning paradigm: If the performance (accu racy) of the network is sufﬁciently high on a holdout (test) set that the network did not have access to while training, the network is deemed acceptable. This is justi ﬁed by statistical arguments based on an i.i.d. assumption on the data generating mechanism, that is each input out put pair is generated independently from the same (un known) data distribution. However, this evaluation pro tocol is not sufﬁcient in domains with critical safety con straints [Marston and Baca, 2015]. In these cases, we may require a stronger test: for example, we may require that the network is robust against adversarial perturba tions within certain bounds. Adversarial evaluation. In the context of adversarial examples, a natural idea is to test neural networks by checking if it is possible to generate an adversarial at tack to change the label predicted by the neural network [Kurakin et al., 2016] and train them to be robust to these examples Madry et al. [2018]. Generating adversarial ex amples is a challenging computational task itself, and the attack generated by a speciﬁc attack algorithm may be far from optimal. This may lead one to falsely conclude that a given model is robust to attacks even though a stronger adversary may have broken the robustness. Recent work [Athalye et al., 2018, Uesato et al., 2018] has shown that evaluating models against weak adversaries can lead to incorrect conclusions regarding the robustness of the model. Thus, there is a need to go beyond evaluation usarXiv:1803.06567v2  [cs.LG]  3 Aug 2018ing speciﬁc adversarial attacks and ﬁnd approaches that provide provable guarantees against attacks by any ad versary . Towards veriﬁable models. Veriﬁcation of neural net works has seen signiﬁcant research interest in recent years. In the formal veriﬁcation community, Satisﬁa bility Modulo Theory (SMT) solvers have been adapted for veriﬁcation of neural networks [Ehlers, 2017, Huang et al., 2017, Katz et al., 2017]. While SMT solvers have been successfully applied to several domains, applying them to large neural networks remains a challenge due to the scale of the resulting SMT problem instances. Fur thermore, these approaches have been largely limited to networks with piecewise linear activation functions since most SMT solvers are unable to deal efﬁciently with nonlinear arithmetic. More recently, researchers have proposed a set of approaches that make use of branch and bound algorithms either directly or via mixedinteger programming solvers [Bunel et al., 2017, Cheng et al., 2017, Tjeng and Tedrake, 2017]. While these approaches achieve strong results on smaller networks, scaling them to large networks remains an open challenge. These ap proaches also rely heavily on the piecewise linear struc ture of networks where the only nonlinearities are max pooling and ReLUs. Towards scalable veriﬁcation of general models. In this paper, we develop a novel approach to neural net work veriﬁcation based on optimization and duality. The approach consists of formulating the veriﬁcation prob lem as an optimization problem that tries to ﬁnd the largest violation of the property being veriﬁed. If the largest violation is smaller than zero, we can conclude that the property being veriﬁed is true. By using ideas from duality in optimization, we can obtain bounds on the optimal value of this problem in a computationally tractable manner. Note that this approach is sound but incomplete , in that there may be cases where the prop erty of interest is true, but the bound computed by our algorithm is not tight enough to prove the property. This strategy has been used in prior work as well [Kolter and Wong, 2018, Raghunathan et al., 2018]. However, our results improve upon prior work in the following ways: 1. Our veriﬁcation approach applies to arbitrary feed forward neural networks with any architecture and any activation function and our framework recovers previous results [Ehlers, 2017] when applied to the special case of piecewise linear activation functions. 2. We can handle veriﬁcation of systems with discrete inputs and combinatorial constraints on the input space, including cardinality constraints.3. The computation involved only requires solving an unconstrained convex optimization problem (of size linear in the number of neurons in the network), which can be done using a subgradient method ef ﬁciently. Further, our approach is anytime , in the sense that the computation can be stopped at any time and a valid bound on the veriﬁcation objective can be obtained. 4. For the special case of single hidden layer networks, we develop specialized veriﬁcation algorithms with provable tightness guarantees. 5. We attain state of the art veriﬁed bounds on ad versarial error rates on image classiﬁers trained on MNIST and CIFAR10 under adversarial perturba tions in the inﬁnity norm. 2 Related Work "
255,FOCAL: A Forgery Localization Framework based on Video Coding Self-Consistency.txt,"Forgery operations on video contents are nowadays within the reach of anyone,
thanks to the availability of powerful and user-friendly editing software.
Integrity verification and authentication of videos represent a major interest
in both journalism (e.g., fake news debunking) and legal environments dealing
with digital evidence (e.g., a court of law). While several strategies and
different forensics traces have been proposed in recent years, latest solutions
aim at increasing the accuracy by combining multiple detectors and features.
This paper presents a video forgery localization framework that verifies the
self-consistency of coding traces between and within video frames, by fusing
the information derived from a set of independent feature descriptors. The
feature extraction step is carried out by means of an explainable convolutional
neural network architecture, specifically designed to look for and classify
coding artifacts. The overall framework was validated in two typical forgery
scenarios: temporal and spatial splicing. Experimental results show an
improvement to the state-of-the-art on temporal splicing localization and also
promising performance in the newly tackled case of spatial splicing, on both
synthetic and real-world videos.","Authenticity assessment for video sequences is nowadays a paramount task in several contexts, such as citizen journalism and fake news debunking, as well as evidence validation in legal procedures and fraud detection. This concern has gained importance during the last years because of the wide availability of powerful and easilyoperable video editing programs (e.g., Adobe Premiere, Apple Final Cut, etc.) and the widespread use of video data in communication and documenting activities. Moreover, the development of deep learning solutions for the automatic creation and editing of image and video contents have posed new challenges to forensic analysts, since a malicious user has the opportunity to create fake contents that overcome most of the existing detectors. As a matter of fact, forensic analysts have been constantly investigating innovative and accurate solutions for forgery detec tion and localization. Among the ﬁrst strategies being proposed, we can ﬁnd detectors that identify the acquisition device [ 1,2], physical inconsistencies [ 3], video recapturing [ 4], frame dele tion and insertion [ 5,6], or codecrelated operations [ 7,8,9]. Most of these detectors verify the selfconsistency [10,11] of video processing footprints, i.e., the uniformity of traces left ?This work has been partially supported by the University of Padova project Phylo4n6 prot. BIRD165882 /16. This material is based on research sponsored by DARPA and Air Force Research Laboratory (AFRL) under agreement num ber FA87501620173. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the o cial policies or endorsements, either expressed or implied, of DARPA and Air Force Research Laboratory (AFRL) or the U.S. Government.on the signal across di erent frames and regions of the video sequence. Whenever an external element is included within an original image or video, the forensic footprints in the altered region change with respect to untouched ones. Revealing such a discrepancy allows detecting the possible presence of a forgery. Extending the preliminary work in [ 12], the current paper proposes a FOrgery loCALizer (FOCAL) that checks the self consistency of multiple and independent forensic traces related to video coding (Figure 1). Di erently from the previous work, where forgeries only consisted in concatenating video sequences from di erent sources ( temporal splicing ), this new approach is also able to precisely localize an altered region within a single frame ( spatial splicing ) as well as along time dimension. Given an input video, each frame is split into smaller patches, and a feature vector is extracted from each one of them. The set of features corresponds to the output values of the ﬁnal softmax layers from multiple convolutional neural networks (CNNs) dedicated to the classiﬁcation of di erent coding parameters, such as coding standard and quality level. These CNNs share anexplainable architecture, which was speciﬁcally designed to look for coding artifacts by aligning the receptive ﬁelds of the network ﬁlters to the quantization block boundaries, where the most signiﬁcant traces are typically visible. An unsupervised fusion technique was designed to merge the outputs of these heterogeneous feature descriptors into a human readable heatmap, which characterizes each framepatch from the analyzed video with a likelihood measure that models the probability of being forged. This approach also makes the frame work scalable and extendible at will, allowing the introduction of additional detectors and feature descriptors to contribute to the overall heatmap. Preprint submitted to Elsevier September 7, 2020arXiv:2008.10454v2  [cs.CV]  4 Sep 2020Feature fusion Patches reordering 64×64 patches Model 1 Model 2 Model M Set of detectors  Feature maps Detection  Original frame  Forged frameFigure 1: FOrgery loCALizer (FOCAL) framework. A forged videoframe is split into 64 64 patches and fed to a set of pretrained detectors (e.g. classiﬁers of the video coding standard and quality). Extracted features are rearranged into featuremaps and a fusion function merges them into a single detection heatmap. Dashed and solid lines are used to denote patchwise and framewise operations, respectively. Experimental validation takes into account di erent forgery setups, such as temporal and spatial splicing, in controlled and uncontrolled environments. Results show that the proposed solution is able to improve the performance of [ 12], thanks to the newly adopted network architecture, and to obtain convincing results in the detection of local forgeries as well, with an area under the curve (AUC) of the receiver operating characteristic (ROC) curve of 0 :94. The rest of the paper is organized as follows. Section 2 brieﬂy overviews the literature on video forgery detection and local ization, distinguishing between temporal and spatial forgeries. Section 3 formally deﬁnes the problem addressed by the paper and the notation used. Section 4 presents the proposed CNN for extracting codingrelated features, with special emphasis on the architectural choices. Section 5 illustrates our forgery localization framework, from the feature extraction step to the ﬁnal feature fusion and heatmap generation, in both temporal and spatial forgery cases. Section 6 reports all the details about the experimental setup, the training phase, the generation of the synthetic dataset and the obtained results. Finally, Section 7 concludes the paper and outlines possible future work. 2. Related work "
496,Shuffled Patch-Wise Supervision for Presentation Attack Detection.txt,"Face anti-spoofing is essential to prevent false facial verification by using
a photo, video, mask, or a different substitute for an authorized person's
face. Most of the state-of-the-art presentation attack detection (PAD) systems
suffer from overfitting, where they achieve near-perfect scores on a single
dataset but fail on a different dataset with more realistic data. This problem
drives researchers to develop models that perform well under real-world
conditions. This is an especially challenging problem for frame-based
presentation attack detection systems that use convolutional neural networks
(CNN). To this end, we propose a new PAD approach, which combines pixel-wise
binary supervision with patch-based CNN. We believe that training a CNN with
face patches allows the model to distinguish spoofs without learning background
or dataset-specific traces. We tested the proposed method both on the standard
benchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.
The proposed approach shows its superiority on challenging experimental setups.
Namely, it achieves higher performance on OULU-NPU protocol 3, 4 and on
inter-dataset real-world experiments.","In recent years, facial recognition systems are widely used as they are robust and reliable for common usage. However, these recognition systems have to be careful about the au thenticity of a given face input. If the given input is recorded from a video of an authorized user, the recognition system should not recognize the person in the video and give access to the system. Presentation attack detection (PAD) systems aim to prevent this problem by evaluating the liveness of the given person’s image. In recent years, PAD methods improved signiﬁcantly with the progress in deep learning methods and publicly available large, representative datasets [Bo17, LJL18, Zh20, Co16]. Most of the signiﬁcant progress has been achieved when researchers found different cues to decide liveness of a face [Li16, MHP11, At17]. These different cues used with com plex deep neural networks to create PAD systems that are very successful in intradataset benchmark results. However, the real challenge in PAD still remains as an interdataset benchmark which shows the real performance of the PAD systems in realworld like sce nario. Most of the systems that use CNNs overﬁt the data easily by memorizing reﬂection 1Department of Computer Engineering, Istanbul Technical University, Turkey, fkantarcia,ekenel g@itu.edu.tr 2Sodec Technologies, Istanbul, Turkey, hasan.dertli@sodecapps.comarXiv:2109.03484v2  [cs.CV]  9 Sep 2021Alperen Kantarcı, Hasan Dertli, Hazım Kemal Ekenel Fig. 1: Overview of the proposed method and illumination effects. To address this problem, in this paper, we propose a new training procedure for face PAD systems. We show that our training method utilizes the pixelwise binary loss in a better way. Moreover, we show that our proposed method improves model performances on realworld experiments. 2 Related Work "
46,Correctness Verification of Neural Networks.txt,"We present a novel framework for specifying and verifying correctness
globally for neural networks on perception tasks. Most previous works on neural
network verification for perception tasks focus on robustness verification.
Unlike robustness verification, which aims to verify that the prediction of a
network is stable in some local regions around labelled points, our framework
provides a way to specify correctness globally in the whole target input space
and verify that the network is correct for all target inputs (or find the
regions where the network is not correct). We provide a specification through
1) a state space consisting of all relevant states of the world and 2) an
observation process that produces neural network inputs from the states of the
world. Tiling the state and input spaces with a finite number of tiles,
obtaining ground truth bounds from the state tiles and network output bounds
from the input tiles, then comparing the ground truth and network output bounds
delivers an upper bound on the network output error for any inputs of interest.
The presented framework also enables detecting illegal inputs -- inputs that
are not contained in (or close to) the target input space as defined by the
state space and observation process (the neural network is not designed to work
on them), so that we can flag when we don't have guarantees. Results from two
case studies highlight the ability of our technique to verify error bounds over
the whole target input space and show how the error bounds vary over the state
and input spaces.","Neural networks are now recognized as powerful function approximators with impressive performance across a wide range of applications. A large body of work focuses on providing formal guarantees for neural networks [ 12,18,40,38,34,8,1]. For perception tasks (e.g. vision, speech recognition), most work has focused on robustness verication [ 12,18,40,38,34], which aims to verify if the network prediction is stable for all inputs in some neighborhood around a selected input point. However, there is yet no systematic way to specify, then verify, that the network is correct (within a specied tolerance) globally over target regions of the input space of interest. A shorter version of this paper is published in NeurIPS 2019 Workshop on Machine Learning with Guarantees.arXiv:1906.01030v3  [cs.LG]  23 Aug 20222 Yichen Yang and Martin Rinard We present the rst framework for specifying and verifying the correctness of neural networks on perception tasks. Neural networks are often used to predict some property of the world given an observation such as an image or audio recording. We provide a specication through 1) a state space consisting of a symbolic representation of all target states of the world and 2) an observation process that produces neural network inputs from these target states. Then the target inputs of interest are all inputs that can be observed from the state space via the observation process. We dene the set of target inputs as the target input space . Because the quantity that the network predicts is some property of the observed state of the world, the state denes the ground truth output (and therefore denes the correct output for each target input to the neural network). We present Tiler , an algorithm for correctness verication of neural networks. Evaluating the correctness of the network on a single state is straightforward | use the observation process to obtain the possible inputs for that state, use the neural network to obtain the possible outputs, then compare the outputs to the ground truth from the state. To do correctness verication, we generalize this idea to work with tiled state and input spaces. We cover the state and input spaces with a nite number of tiles: each state tile comprises a set of states; each input tile is the image of the corresponding state tile under the observation process. The state tiles provide ground truth bounds for the corresponding input tiles. We use recently developed techniques from the robustness verication literature to obtain network output bounds for each input tile [ 44,12,39,3,23,38,16]. A comparison of the ground truth and network output bounds delivers an error upper bound for that region of the state space. The error bounds for all the tiles jointly provide the correctness verication result. The proposed framework also enables detecting illegal inputs { inputs that are not within (or close to) the target input space given by the specication. The neural network is not designed to work on these inputs, so we can  ag them when they occur in runtime. The result is that the client or user of the neural network knows when the correctness guarantee holds or may not hold. By utilizing neural network prediction to guide the search, we are able to speed up the detecting process signicantly. We present two case studies. The rst involves a world with a (idealized) xed road and a camera that can vary its horizontal oset and viewing angle with respect to the centerline of the road (Section 5.1). The state of the world is therefore characterized by the oset and the viewing angle . A neural network takes the camera image as input and predicts the oset and the viewing angle. The state space includes the andof interest. The observation process is the camera imaging process, which maps camera positions to images. This state space and the camera imaging process provide the specication. The target input space is the set of camera images that can be observed from all camera positions of interest. For each image, the camera positions of all the states that can produce the image give the possible ground truths. We tile the state space using a grid on ( ;). Each state tile gives a bound on the ground truth of and . We then apply the observation process to project each state tile into the imageCorrectness Verication of Neural Networks 3 space. We compute a bounding box for each input tile and apply techniques from robustness verication [ 38] to obtain neural network output bounds for each input tile. Comparing the ground truth bounds and the network output bounds gives upper bounds on network prediction error for each tile. We verify that our trained neural network provides good accuracy across the majority of the state space of interest and bound the maximum error the network will ever produce on any target input. The second case study veries a neural network that classies a LiDAR measurement of a sign in an (idealized) scene into one of three shapes (Section 5.2). The state space includes the position of the LiDAR sensor and the shape of the sign. We tile the state space, project each tile into the input space via the LiDAR observation process, and again apply techniques from robustness verication to verify the network, including identifying regions of the state and input space where the network may deliver an incorrect classication. 1.1 Contributions This paper makes the following contributions: Specication: We show how to use state spaces and observation processes to specify the correctness of neural networks for perception  the specication identies the target inputs of interest and the correct output for each target input. To our knowledge, this is the rst systematic approach to provide a global correctness specication for perception neural networks. Verication: We present an algorithm, Tiler , for correctness verication. With state spaces and observation processes providing the specication, this is the rst algorithm (to our knowledge) for verifying that a neural network produces the correct output (up to a specied tolerance) for every target input of interest. The algorithm can also compute tighter correctness bounds for focused regions of the state and input spaces. Detecting inputs outside the target input space: We show how to use the speci cation to detect and  ag inputs that are not within the veried target input space. Case Studies: We apply the framework to specify and verify neural networks that predicts camera position from an input image and neural networks that classies the shape of the sign from input LiDAR measurements. 2 Related Work "
79,Reachable Polyhedral Marching (RPM): A Safety Verification Algorithm for Robotic Systems with Deep Neural Network Components.txt,"We present a method for computing exact reachable sets for deep neural
networks with rectified linear unit (ReLU) activation. Our method is
well-suited for use in rigorous safety analysis of robotic perception and
control systems with deep neural network components. Our algorithm can compute
both forward and backward reachable sets for a ReLU network iterated over
multiple time steps, as would be found in a perception-action loop in a robotic
system. Our algorithm is unique in that it builds the reachable sets by
incrementally enumerating polyhedral cells in the input space, rather than
iterating layer-by-layer through the network as in other methods. If an unsafe
cell is found, our algorithm can return this result without completing the full
reachability computation, thus giving an anytime property that accelerates
safety verification. In addition, our method requires less memory during
execution compared to existing methods where memory can be a limiting factor.
We demonstrate our algorithm on safety verification of the ACAS Xu aircraft
advisory system. We find unsafe actions many times faster than the fastest
existing method and certify no unsafe actions exist in about twice the time of
the existing method. We also compute forward and backward reachable sets for a
learned model of pendulum dynamics over a 50 time step horizon in 87s on a
laptop computer. Algorithm source code:
https://github.com/StanfordMSL/Neural-Network-Reach.","In this paper we present the Reachable Polyhedral March ing (RPM) algorithm for computing forward and backward reachable sets of deep neural networks with rectiﬁed linear unit (ReLU) activation. This is a critical building block to proving safety properties for autonomous systems with learned perception, dynamics, or control components in the loop. Speciﬁcally, given a set of input values, RPM will compute the set of all output values that can be obtained under the ReLU network (the forward reachable set, or image, of the input set). Similarly, given a set of output values, RPM will compute the set of all input values that can lead to those output values under the ReLU network (the backward reachable set, or preimage, of the output values). When the ReLU network is part of a dynamical process, as is common in robots with deep learned perception or control components, RPM can compute such reachable sets for multiple time steps into the future or the past without explicitly iterating over each time step. Figure 1 shows the incremental nature of how RPM enumerates input space polyhedra over which the ReLU network is afﬁne. This work was supported by NSF grant 1830402, DARPA grant D18AP00064, NASA grant 80NSSC20M0163, and the Dwight D. Eisen hower Transportation Fellowship. The NASA University Leadership Initia tive (grant #80NSSC20M0163) provided funds to assist the authors with their research, but this article solely reﬂects the opinions and conclusions of its authors and not any NASA entity. We are grateful for this support. 1Department of Aeronautics and Astronautics, Stanford University, Stan ford, CA 94305, USA, fjosephav, schwager g@stanford.edu Fig. 1: Snapshots of how RPM incrementally enumerates the 2D input space polyhedra for which a random ReLU network is afﬁne, resulting in the explicit piecewiseafﬁne representation. Polyhedron color is random. All existing algorithms that compute exact reachable sets iterate through the network layerbylayer [1], [2], [3], which may become intractable if the network is applied iteratively in a feedback loop. RPM applies a fundamentally different approach to avoid layerbylayer computation. Instead, we compute the reachable set one polyhedral cell at a time, where each cell represents a region of the input space over which the network is afﬁne. Each cell can be computed quickly through a series of linear programs equal to the number of neurons in the network, regardless of width or depth. Our algorithm computes cells until the explored set of cells ﬁlls the desired reachable set. In this way, our method is geometrically similar to fast marching methods in optimal control [4], path planning [5], [6], and graphics [7], [8]. We demonstrate RPM in two examples related to the safety veriﬁcation of dynamical systems with learned components. First, we compute forward reachable and backward reachable sets for a learned dynamical model of a pendulum over 50 time steps in the future and past, respectively. Furthermore, we compare with a state of the art reachability method [3] in a safety veriﬁcation problem involving ACAS Xu, a neural network policy for aircraft collision avoidance. Our algorithm ﬁnds unsafe inputs for this network many times faster than [3] and when no unsafe input exists, our algorithm certiﬁes safety approximately 2 times slower. The paper is organized as follows. We give related work in Section II and give background and state the problem in Section III. Section IV presents our main algorithm, and explains its derivation. In Section V we describe how the RPM algorithm can be used to perform forward and backward reachability over multiple time steps. In Section VI we present the aforementioned examples of RPM. II. R ELATED WORK "
246,VSMask: Defending Against Voice Synthesis Attack via Real-Time Predictive Perturbation.txt,"Deep learning based voice synthesis technology generates artificial
human-like speeches, which has been used in deepfakes or identity theft
attacks. Existing defense mechanisms inject subtle adversarial perturbations
into the raw speech audios to mislead the voice synthesis models. However,
optimizing the adversarial perturbation not only consumes substantial
computation time, but it also requires the availability of entire speech.
Therefore, they are not suitable for protecting live speech streams, such as
voice messages or online meetings. In this paper, we propose VSMask, a
real-time protection mechanism against voice synthesis attacks. Different from
offline protection schemes, VSMask leverages a predictive neural network to
forecast the most effective perturbation for the upcoming streaming speech.
VSMask introduces a universal perturbation tailored for arbitrary speech input
to shield a real-time speech in its entirety. To minimize the audio distortion
within the protected speech, we implement a weight-based perturbation
constraint to reduce the perceptibility of the added perturbation. We
comprehensively evaluate VSMask protection performance under different
scenarios. The experimental results indicate that VSMask can effectively defend
against 3 popular voice synthesis models. None of the synthetic voice could
deceive the speaker verification models or human ears with VSMask protection.
In a physical world experiment, we demonstrate that VSMask successfully
safeguards the real-time speech by injecting the perturbation over the air.","The recent development in Artificial Intelligence (AI) has spurred the research on intelligent voice control. It is now possible to im personate someone’s voice almost impeccably, as shown in various “deepfake"" campaigns for misinformation and fraud [ 1]. Deep learn ing based voice synthesis is the core technology used in deepfakes to generate speeches of arbitrary target speaker’s voice without specific speech samples. There are two types of speech synthesis Hey Siri, open  the door. The door is  opened.(a) Synthetic voice commands are used to bypass the speaker verifica tion process and illegally control the voice assistants. ..123456.. Login  successful! (b) Synthetic voice is used to hack into private accounts. Hello Bob, I am Alice. Can  you lend me some money?Of course! (c) The adversary can steal the victim’s identity using voice synthesis. Figure 1: Voice synthesis attack deceives both ASV system and human ears in realworld scenarios. approaches: texttospeech (TTS) synthesis and voice conversion (VC). TTS generates synthetic speeches using the script texts and a single speech sample from the speaker. However, it usually outputs unnatural and mechanical human speeches. VC, on the other hand, directly transforms the voice of the original speaker to that of an other speaker without altering the linguistic information within the speech. VC is capable of generating vivid speech with real human speech characteristics, such as pauses, moods, and even accents. Advanced voice synthesis has been used in benign applications, such as reading stories to kids with their deceased grandma’s voice [ 2], or letting your favorite movie star to guide your routes [ 3]. However, as shown in Fig. 1, human ears or automatic speaker verifi cation (ASV) systems verify speaker identity through voice patterns, voice synthesis technology causes potential security threats. First, voice assistants often require the authenticated voice to activate Siri or Google Assistant before users operate the locked smartphone. Attackers could easily take control of the smartphone and send malicious commands with synthesized voice. Second, ASV systems have been integrated in many popular Apps for user authentication. For example, WeChat requires the user to speak a 6digit numberarXiv:2305.05736v1  [cs.SD]  9 May 2023before the firsttime login on an unseen device [ 4]. Such user verifi cation is vulnerable to voice synthesis attacks. Third, attackers can leverage synthetic voice to impersonate the victim in a fraud phone call [ 5]. It is difficult for human ears to differentiate highquality synthetic speech from natural human voice [6]. In response to the threat of voice synthesis attacks, researchers have developed synthetic human voice detection models to identify fake human speech [ 7–9]. However, implementing voice synthesis detection on all mobile devices can be costly, and these detection methods may not achieve the claimed high accuracy in realworld scenarios [ 6]. To defend against unauthorized voice synthesis at tacks, AttackVC [ 10] adds adversarial perturbations to victim’s speech samples to disallow adversaries from generating synthetic voices from the victim. The synthesized speech derived from the protected speech will sound like a distinct voice from another pre determined target speaker rather than the victim speaker1. Never theless, the application of AttackVC is somewhat restricted. The PGD method used to optimize the adversarial perturbation requires the availability of the entire speech and a significant number of iterations to generate the adversarial speech samples. Therefore, the optimization process will not be able to keep up with the real time speech. As a result, live human speech and voice signals in realtime applications, such as voice messages, online meetings, are still vulnerable to voice synthesis threats. In this work, we propose the first realtime voice protection method, VSMask , to protect the realtime speech in both digital and physical world. Instead of optimizing the perturbation itself, we train a neural network to anticipate the most efficient pertur bations in the future. As a result, the protective perturbation can be instantly incorporated into the realtime audio stream without causing extra time delay. In addition, to protect the entire speech, we introduce universal perturbation headers to mask the beginning of the speech regardless of the speech content. Based on human auditory perception characteristics, we introduce a supplementary weightbased amplitude constrained method to further alleviate the audio distortion of the protected speech. In summary, the paper makes following contributions: •We propose VSMask , a realtime protection system against voice synthesis attacks. By predicting the perturbation for upcoming streaming speech, VSMask can shield the user’s voice with negligible delays. •We propose an optimization scheme to train universal per turbation to protect the exposed audio at the beginning of speech. This allows VSMask to indiscriminately protect speech data with different lengths and sizes. We further in troduce a weightbased amplitude constrained method to reduce human perceptibility for adversarial perturbations. •We evaluate VSMask performance on three different voice synthesis models. The experimental results show that VS Mask is capable of providing realtime defense against voice synthesis attacks on both digital and physical spaces, ef fectively shielding ASV systems and the human auditory system from malicious voice synthesis. 1“victim speaker"" refers to the target of voice synthesis attack. “target speaker"" refers to a different speaker predetermined by VSMask to interfere with the voice synthesis. Input speech xSpeaker  encoder Es Input speech pContent  encoder EcDecoderInput text tTexttospeech  encoder EttsEtts(t) Ec(p)Es(x) Synthetic speech F(t, x) or F( p, x) Please don’t  clone my voice!  Figure 2: The speaker encoder in voice synthesis models is susceptible to adversarial attacks. 2 BACKGROUND 2.1 Voice Synthesis The purpose of voice synthesis attacks is to generate artificial speech resembling real human voices. Fig. 2 shows a general voice synthesis framework including both TTS and VC. For TTS and VC models, the speech content embeddings are respectively ex tracted by the content encoders 𝐸𝑡𝑡𝑠or𝐸𝑐from text tor speech p. The speaker encoder 𝐸𝑠encodes the victim speaker character istics𝐸𝑠(x), where xis a speech sample from the victim speaker 𝑥. Finally, the decoder will generate a synthetic speech 𝐹(x,p)or 𝐹(x,t)with voice features from an arbitrary victim speaker 𝑥. 2.2 Defense Against Voice Synthesis AttackVC [ 10] introduced embedding attack that aims at the speaker encoder model 𝐸𝑠. If we input an adversarial speech sample x+𝛿 with little difference from x, the encoded vector 𝐸𝑠(x+𝛿)is close to another target speaker 𝑦rather than the real victim speaker 𝑥. The adversarial attack for voice protection can be formulated in Problem (1) as follows: minimize 𝛿L(𝐸𝑠(x+𝛿),𝐸𝑠(y))−𝜆L(𝐸𝑠(x+𝛿),𝐸𝑠(x)) subject to∥𝛿∥∞<𝜖,(1) whereLis mean squared error (MSE) loss function, and yis a speech sample from target speaker 𝑦. The first term in Problem (1) minimizes the loss between the speaker embedding vector of ad versarial input 𝐸𝑠(x+𝛿)and the target speaker 𝐸𝑠(y)to force the model to generate speech voice similar to the target speaker 𝑦, while the second term eliminates the victim speaker voice 𝐸𝑠(x). Eventually, with protected speech input 𝑥+𝛿, the synthetic voice cannot spoof the speaker verification or human ears anymore. In Problem (1), it is necessary to set a predetermined target speaker 𝑦. We run an additional experiment without setting a target speaker, (i.e., by setting 𝜆→∞ ). The result shows: despite that the synthe sized speech becomes noisy, it still sounds like the voice from the victim speaker. 2.3 Realtime Adversarial Attack Since human speech is a streaming audio signal, it is impractical to generate realtime adversarial perturbation using the PGD method. VoiceCamo [ 11] presents a realtime adversarial attack towards speech recognition (SR) models, which could forecast the upcoming adversarial perturbation by optimizing the model 𝑔𝜃. The predictive model optimization in VoiceCamo is illustrated as follows: maximize 𝜃E(𝑥𝑡,𝑦𝑡)[L𝐶𝑇𝐶(¯𝑦𝑡,𝑦𝑡)] s.t. ¯𝑦𝑡=𝑓𝜓(𝑥𝑡+𝑔𝜃(𝑥𝑡−𝑟−𝛿))and∥𝑔𝜃(𝑥𝑡)∥∞<𝜖,(2) 2VSMask (a) Realtime communication applications. VSMask (b) Social media platforms. … VSMask (c) Realworld conversations. Figure 3: VSMask provides protection in both online and of fline scenarios. whereL𝐶𝑇𝐶 is connectionist temporal classification (CTC) loss function,𝑥𝑡is a streaming speech audio, and 𝑦𝑡is the true label of the speech transcript. 𝑓𝜓is the speechtotext model with fixed parameter𝜓,𝑔𝜃is the predictive model to forecast the perturbation, and𝑥𝑡−𝑟−𝛿is the speech signal captured at time 𝑡−𝑟−𝛿. After predicting the first perturbation, the input window slides to the next time slot to predict the second perturbation, and so on so forth. Finally, the speech recognition accuracy will be significantly degraded when the realtime attack activates. 3 THREAT MODEL The stateoftheart voice synthesis models allow the attackers to generate voice with a single speech sample from the victim speaker. In this section, we define the threat model of voice synthesis at tacks according to the adversary’s knowledge about the victim and potential defenses. 3.1 Adversary Knowledge The attacker can leverage voice synthesis models to generate artifi cial speech with the victim speaker’s voice, in order to compromise the speaker verification systems or human ears. The model could be welltrained or locally trained by the adversaries. They can also test the synthesized speech on speaker recognition models to evaluate the voice similarity. If they realize the samples are protected, they can apply denoising or other signal processing methods to recover the clear audio. 3.2 Adversary Capability The adversary is able to save audio signals played in mobile devices, for example, smartphones or laptops, or record speech in physical world with a microphone. The victim speaker’s speech samples are available from multiple sources. Realtime Communication Applications. As shown in Fig. 3(a), it is common for people to send voice memos or make voice calls using realtime communication apps. The attackers may hide in chat groups and record anyone’s voice for voice synthesis. Mean while, many people meet online since the COVID19 pandemic. The attackers can join public online meetings and record the victim’svoice without seeking permission. But existing defense methods fail to provide protection for the realtime speech. Social Network Platforms. People are willing to share their daily life on social media. There are billions of active users uploading their selfmade videos to TikTok, YouTube and Instagram [ 12]. Usually, all visitors have the access to download and reuse them. Even with a short speech, malicious attackers could clone your voice and impersonate your identity. Although existing defense methods are able to protect your voice on your social media, it requires long processing time before uploading [10]. Realworld Conversation. In addition, realworld human speech is threatened by voice synthesis attacks. While people are speaking in a presentation or conversation, an adversary can stealthily record the speech and launch voice synthesis attacks. But existing defenses fail to mask our voice efficiently to allow the protection against malicious voice synthesis. It is also challenging to physically inject perturbation into real speech. 3.3 Defender Knowledge Meanwhile, we consider a whitebox scenario where the defenders have complete knowledge of the adversarial voice synthesis model, including model structure and model parameters. To optimize VSMask , defenders also need to collect a few clear speech samples from the protected victim speaker 𝑥. In addition, defenders have multiple target speaker candidates 𝑦𝑖speech samples from public datasets, and they can select the best target 𝑦for the victim speaker 𝑥to maximize the voice disparity. Raw speech audios will be locally processed by VSMask and uploaded to realtime applications or social media platforms without extra time delay. 4VSMASK SYSTEM DESIGN 4.1 Design Consideration One typical attack approach towards ASR system is to hide the adversarial perturbations in the psychoacoustic features of human speech [ 13], which could be a potential solution for voice synthesis defense. However, these perturbations require the availability of the entire speech to mask them. As a result, it cannot be applied in realtime applications. We also notice that universal perturbations can fool the ASR sys tem regardless of the original speech contents [ 14]. But such short perturbations are too short to protect the long speech from voice synthesis attacks. To extend the protection scope, we attempt to leverage periodical universal perturbation to shield human speech. Unfortunately, the performance of periodical perturbation method is unsatisfying. The defense is effective only when the perturbation signal𝛿is strong. Generally, short universal perturbations have lim ited degrees of freedom, which limits the protection performance.z In contrast, longer perturbation period usually causes overfitting; besides, periodical perturbation can be denoised by attackers once they capture the perturbation at the interval of the speech. Thus, it is crucial to develop a robust and realtime protection mechanism to safeguard against voice synthesis attacks. 4.2 Realtime Predictive Model Preprocessing. Fig. 4 shows the system framework of VSMask . The input of VSMask is a streaming human speech signal, which is vulnerable to voice synthesis attacks. Once VSMask detects human 3TimePreprocessingPredictive  ModelPretrained Universal  Perturbation Header Weight ed ConstraintReal time Perturbation 𝑡 0 𝑡+∆𝑡𝑡+∆𝑡+𝛾𝑡+∆𝑡+2𝛾 Figure 4: VSMask system framework. speech starts, it will capture the upcoming audio signal. During pre processing, the audio signal is transformed into a melspectrogram, with the frequencies converted to the nonlinear mel scale to match human hearing. The resulting melspectrogram has the same di mensions as the input of the speaker encoder 𝐸𝑠in the targeted voice synthesis model. Model Architecture. Next, the converted melspectrogram works as the input of VSMask predictive model 𝑓𝜃. The output of the predictive model is also a melspectrogram with shorter length. Learned from VoiceCamo [ 11],VSMask selects a realtime pre dictive model composed of 7 downsampling blocks and 5 up sampling blocks, which outperforms models with less layers and maintains similar performance as models with more layers. Each downsampling block contains a reflection padding followed by a 2D convolution layer, a 2D batch form, and a PReLU activation function; each upsampling block consists of a 2D ConvTranspose layer and a leaky ReLU activation function. The final upsampling block is followed by a tanh function to constrain ∥𝛿∥∞. The output melspectrogram will be converted to timedomain audio signal and injected into the raw audio. We illustrate the finetuning process and model architecture details in Appendix A. Optimization and Workflow. Suppose the speaker encoder func tion in the target voice synthesis model is 𝐸𝑠, we could formulate predictive model training as the following optimization problem: minimize 𝜃L(𝐸𝑠(x+𝛿),𝐸𝑠(y))−𝜆L(𝐸𝑠(x+𝛿),𝐸𝑠(x)), subject to 𝛿𝛾 𝑡+Δ𝑡=𝑓𝜃(x𝑡)and∥𝛿∥∞<𝜖,(3) whereLis MSE loss function, xis the input streaming speech from the victim speaker 𝑥,yis a speech sample from the selected target speaker𝑦to mislead𝐸𝑠, and𝛿is the perturbation predicted by model𝑓𝜃.𝛿𝛾 𝑡+Δ𝑡is the perturbation added at 𝑡+Δ𝑡with length 𝛾. Since the predictive model 𝑓requires an input signal x𝑡with a short length t, VSMask does not require the entire speech in advance, facilitating realtime protection. As illustrated in Figure 4, VSMask initially generates a perturba tion of length 𝛾(represented by the blue waveform) after a short time delay of Δ𝑡. Subsequently, the input window is slid to the next time slot from 𝛾to𝑡+𝛾to generate the second perturbation (repre sented by the red waveform), and this process repeats. In contrast to AttackVC, VSMask finetunes the predictive model parameters 𝜃rather than manipulating the perturbation 𝛿. As a result, once the predictive model 𝑓𝜃is established, VSMask can identify the best perturbation 𝛿in a single computation step, rather than undergoing a large number of iterations. (a) The spectrogram of the natural speech x. (b) The spectrogram of the protected speech x+𝛿. (c) The spectrogram of synthetic speech 𝐹(x). (d) The spectrogram of synthetic speech 𝐹(x+𝛿). Figure 5: Speech protected by VSMask effectively degrades the voice synthesis model performance. 4.3 Universal Perturbation Header However, VSMask predictive model has some deficiencies, since the voice synthesis models can generate synthetic speech via very short speech samples from the victim speaker. For example, the victim speaker gives a speech example – “please protect my speech from voice synthesis"" . The predictive model could well protect the trailing phrases “my speech from voice synthesis"" . Yet, the leading words of the speech “please protect"" are still exposed to the attackers, asVSMask prediction model cannot generate perturbation for the beginning of the speech. Therefore, the attacker may be able to clip the speech audio and synthesize speech with an audio segment. To address this challenge, we resort to a universal perturbation added at the beginning of the streaming speech signal to protect the entire speech. Recent work [ 14] shows that regardless of the utterance, there exists universal perturbations that can force the ASR system to output specific transcripts, e.g., “yes"" or “no"". This universal perturbation maintains high success rate especially when the input speech is short. Fortunately, the “blind spot"" of the predic tive model is also small. Therefore, it is feasible to train a universal perturbation forcing the speaker encoder to output the selected target speaker 𝑦embedded vector 𝐸𝑠(y)rather than the real vector 𝐸𝑠(x), regardless of the speech contents. In order to train such a universal perturbation header 𝛿ℎ, we clip the training speech samples from the victim speaker 𝑥to audio clips x𝑖with the same length 𝑡+Δ𝑡. Then, we use PGD method to optimize the universal perturbation header 𝛿ℎas follows: minimize 𝛿ℎ∑︁ x𝑖∈DL𝑠(𝛿ℎ,x𝑖,y), subject toL𝑠(𝛿ℎ,x𝑖,y)=L(𝐸𝑠(x𝑖+𝛿ℎ),𝐸𝑠(y)) −𝜆L(𝐸𝑠(x𝑖+𝛿ℎ),𝐸𝑠(x𝑖))and∥𝛿ℎ∥∞<𝜖.(4) whereDis the collection of clipped speech x𝑖. Before the optimization process in Section 4.2, we add the uni versal header into all training samples to enhance the protection performance. After combining the predictive model and universal header, VSMask successfully protects the streaming speech in real time scenarios. Fig. 5(a) shows a natural speech sample xexposed to the attackers. Fig. 5(b) is the audio spectrogram of protected speech x+𝛿. The perturbation causes little difference between these two speech samples. Fig. 5(c) and Fig. 5(d) are respectively the synthetic speech spectrograms from xandx+𝛿with the same speech content. Even though xandx+𝛿are very similar, the synthetic speech 𝐹(x) shows male voice patterns whereas 𝐹(x+𝛿)presents female voice with higher frequencies. 44.4 Weightbased Noise Mitigation Inevitably, the injected protective perturbation will cause audio distortion. Prior work usually constrains the perturbation amplitude to minimize the human perception, or considers the amplitude in the optimization to achieve the best trade off between noise level and performance. However, even as the perturbation is welloptimized, there is still significant background noise in the speech signal. To degrade the perturbation perceptibility of VSMask , we look into human hearing sensitivity. Fig. 6 shows equalloudness con tour [ 15], a measurement of human hearing perception under dif ferent frequency tones. For low frequency ( <1.6kHz) or high frequency ( >4kHz), higher sound pressure is required to cause the same loudness level in human ears. For medium frequency band between 1.6kHz and 4kHz, human ears have the highest sensitivity. Based on this phenomenon, one potential approach is to hide all perturbations in the low frequency band. However, our experiment shows that it fails to provide sufficient protection for our speeches. Therefore, we need to inject a global perturbation into the en tire spectrogram. To mitigate human hearing perception, we are supposed to reduce the perturbation amplitude between 1.6kHz and4kHz. However, it is expected that reducing the perturbation amplitude also degrades the protection performance. To compen sate the scarcity in medium frequency, we increase the amplitudes in other frequency bands which are hard to sense for human ears. Therefore, we rewrite the amplitude constraint as follows: 𝛿=𝛿𝑙𝛿𝑚𝛿ℎ𝑇, s.t.∥𝛿𝑙∥∞<𝜖1,∥𝛿𝑚∥∞<𝜖2,∥𝛿ℎ∥∞<𝜖3,(5) where𝜖2<𝜖3<𝜖1. By adding such weightbased constraint on the perturbation amplitude, we successfully improve the protected audio quality by limiting the human perception of perturbation without sacrificing the protection performance. 4.5 VSMask Application Scenarios As we mentioned in Section 3, the attackers have many different approaches to obtain the speech audio from the victim speaker. Here, we describe the versatile application scenarios of VSMask . Online Protection. The first application is to protect human voice in realtime communication and social media Apps. For these Apps, VSMask can work as a plugin component to protect the user’s voice. Once VSMask detects the start of the speech, it will automatically add the universal perturbation header, and concurrently capture the speech audio. After injecting the perturbation header, VSMask has predicted the following perturbation for the next time slot, and then it iteratively protects the upcoming speech stream. Based on the prediction mechanism, VSMask causes no extra time delay on the realtime communication. Realworld Physical Protection. In realworld scenarios, the at tackers may stealthily record the victim’s voice and implement voice synthesis attack. To counter such adversaries, users can pro tect themselves by installing VSMask on their mobile devices. While talking, VSMask will play perturbation noise signal and effectively inject adversarial speech samples in the eavesdropped audio. In the end, it will protect our realworld speech against voice synthesis attack, as shown in the experiments in Section 6.                                                                                                     Figure 6: Equalloudness contour. 5 EVALUATION 5.1 Target Models and Experiment Design Target Voice Synthesis Model. Moreover, we select the following three representative voice synthesis models to evaluate VSMask performance: •AdaINVC [ 16]: an unsupervised oneshot voice conversion model based on instance normalization; •AutoVC [ 17]: a new style auto encoder with carefully de signed bottleneck, which is the first work achieving zero shot voice conversion with nonparallel speech data; •SV2TTS [ 18]: a TTS model that converts text transcripts to human speech with the target speaker voice. Dataset. For AdaINVC and AutoVC models, we use CSTR VCTK Corpus [ 19] dataset, a widely used speech dataset in the voice con version studies, to evaluate the performance of VSMask . VCTK includes speech samples from 109 English speakers with different accents. Each speaker reads up to 400 sentences randomly selected from newspapers or elicitation. For SV2TTS model, we select Lib riSpeech dataset [ 20] for evaluation. In particular, as some speech samples are too short to activate VSMask predictive model, we only use speech audios longer than 3 seconds for training and testing. 5.2 VSMask Effectiveness on ASV System ASV System for Evaluation. To evaluate VSMask effectiveness of reducing the similarity between the synthetic speech voice and victim speaker’s voice, we utilize SpeechBrain [ 21], a stateofthe art model to calculate the voice similarity of two speech samples. SpeechBrain speaker verification model is composed of emphasized channel attention, propagation and aggregation in time delay neu ral network (ECAPATDNN) to extract speaker embeddings. The similarity scores are calculated by cosine similarity of embedding vectors of two speech samples. The higher the score is, the greater the possibility that two speech samples are from the same speaker. If the score is higher than a threshold 𝑘, the speech can pass the ver ification. In our experiment, we set the default threshold 𝑘=0.25, which achieves the best accuracy for clear human speech samples. VSMask and Baseline Methods Setup. In this section, we intro duce VSMask evaluation setup along with 4 other baseline methods for comparison. We evaluate the methods in the digital domain by injecting perturbations into digital audio signals. VSMask : First, for VSMask method, we set 𝜆=1in Problem (3) and (4) according to the 𝜆value in [ 6], the input time window length 𝑡as 5Table 1: We compare VSMask defense performance with other online or offline baseline defense methods. VSMask outper forms all other realtime protection methods and achieves similar performance as offline PGD method. Type Method Score Accuracy STOI NonsyntheticRaw speech 0.780 98 .8% 1.000 Protected speech 0.429 68 .5% 0.839 Type MethodM2M F2F M2F F2M Score ASR Score ASR Score ASR Score ASR SyntheticRaw speech 0.595 91.9% 0.612 93.2% 0.561 88.3% 0.546 86.0% Random noise 0.516 86.6% 0.538 89.0% 0.505 84.0% 0.473 81.5% Periodical perturbation 0.192 11.0% 0.203 12.5% 0.177 9.8% 0.156 8.6% Offline PGD 0.064 0.0% 0.085 0.0% 0.049 0.0% 0.055 0.0% VSMask 0.077 0.0% 0.104 0.0% 0.056 0.0% 0.073 0.0% M2M: MaletoMale, F2F: FemaletoFemale, M2F: MaletoFemale, F2M: FemaletoMale. 1.25seconds. To ensure realtime prediction, it is important to avoid setting a very short timedelay. On the other hand, a long timedelay can lead to a degradation in the performance of VSMask . Therefore, we have set both the timedelay ( Δ𝑡) and output length ( 𝛾) to be 0.4 seconds. Therefore, the perturbation header length is 𝑡+Δ𝑡=1.65 seconds. Moreover, we set the perturbation amplitude constraint 𝜖= 0.10. The perturbation amplitude after noise mitigation approach is:𝜖3=𝜖,𝜖1=1.15𝜖and𝜖2=0.85𝜖. In addition, we randomly select 30 male and 30 female speakers from the dataset. We use AdaINVC model to generate 100 synthe sized speech samples for each speaker. For female victim speakers, we select a male speaker as the target speaker, and vice versa. For voice conversion models, the gender of the speaker matters, since male and female have different voice patterns, e.g., female speakers usually have higher frequency utterances. We split the results ac cording to different genders of victim speaker and speech source speaker. Random Noise: One possible way to degrade the voice synthesis model performance is to add random noise into the speech signal x. In the experiment, we set the random noise amplitude 𝜖𝑛=0.15. Periodical Perturbation: Since the universal header can protect short speech regardless of the speech content, periodical universal per turbation is a potential solution for realtime defense. In the experi ment, we recurrently inject the same universal perturbation header with amplitude 𝜖𝑝=0.12into the raw speech audio and evaluate its protection performance. Offline PGD Method: In addition, we compare our method with offline PGD method in AttackVC [ 10]. We apply the embedding attack method as shown in Problem (1), while the iteration number is1,500,𝜖𝑜𝑓𝑓=0.10. Even though offline PGD method is not feasible for realtime protection, it can help to find the upper bound of protection performance. VSMask Evaluation on ASV System. We quantitatively evaluate the ASV system performance with nonsynthetic speech samples. For the clear audios without VSMask protection, the ASV systemachieves accuracy above 98%. But for speech audios protected by VS Mask , similar to the result in AttackVC, the ASV accuracy degrades since the perturbation distorts the voice patterns in the speech. In this way, VSMask may affect speaker recognition functions in realworld scenario such as voice assistant activation. Fortunately, mobile devices can cancel out the perturbation from loudspeakers using echo cancellation [ 22]. So VSMask will not interfere with the activation process. In addition, we use ShortTime Objective Intelligibility (STOI) [ 23] to measure the speech quality of protected audio. The result shows that the speech under VSMask protection is fully intelligible for human ears. Next, we compare VSMask with different baseline defense meth ods, where the target voice synthesis model is AdaINVC. We use attack success rate (ASR) to evaluate the performance of voice syn thesis attacks. For reference, when the attacker clones voice with raw speech audios, more than 85% synthetic audios can pass the ASV system with high similarity score. While the voice conver sion is intragender (maletomale or femaletofemale), the ASR is even higher. If the victim speaker is female, voice conversion shows slightly higher ASR than male victim speaker. This result confirms the effectiveness of compromising ASV systems with voice synthesis attacks. Then, we evaluate the baseline realtime defense methods. We first synthesize voice from speech with white noise. When the perturbation amplitude is small ( 𝜖𝑛=0.15), random noise cannot deteriorate the voice synthesis performance. Actually, the embed ding speaker vectors 𝐸𝑠(x)and𝐸𝑠(x+𝛿)show little difference when𝛿is weak random noise. If stronger noise is implemented, it works because of poor speech quality. When the noise amplitude 𝜖>0.4, no synthetic speech samples can compromise ASV model. But the protected audio x+𝛿is illegible for human ears. Conse quently, strong noise is not a feasible protection method in real world. Second, we use the periodical universal perturbation for defense. As shown in Table 1, periodical universal perturbation significantly reduces the similarity of the synthetic voice and the victim’s voice. But there are still more than 10% intragender synthetic audios 6                                                                          VSMask(a) The similarity scores for different voice synthesis models                                                              VSMask(b) The ASR for different voice synthesis models Figure 7: For all target voice synthesis models, VSMask sig nificantly reduce the voice similarity. With VSMask protec tion, no synthetic speech can pass the speaker verification system. that could pass ASV systems. The reason is that when perturbation amplitude is small, universal perturbation cannot compromise the entire speech, especially for long speech samples. We can increase the amplitude or cycle length to improve the periodical method performance. However, if the perturbation is too strong, it can lead to significant audio distortion, and using a long period for pertur bation may not be effective in protecting short speech segments. Finally, we compare the offline PGD method performance with VSMask . Theoretically, offline PGD method achieves the best perfor mance for defending against voice synthesis attacks. No synthetic audio from offline PGD protected speech can pass ASV systems. Its performance can be further enhanced by increasing the iterations. But for 1,500 iterations, its time consumption is approximately 20× audio length. The computation time could be much longer when the model is deployed on devices without high performance GPU. We mark VSMask performance in bold in Table 1. Compared with all other online baseline methods, VSMask has the strongest protective impact. And compared with offline PGD method, VSMask achieves similar offline protection performance without incurring any time delay. For both intragender and intergender voice conversions, no synthetic speech samples can pass ASV system. Therefore, the results demonstrate that VSMask can successfully protect human speech against unauthorized voice synthesis attacks. 5.3 VSMask Evaluation on Different Voice Synthesis Models In this section, we evaluate VSMask performance on different voice synthesis models. From Section 5.2, we learn that for voice conver sion models, intragender voice conversion has better similarity than intergender voice conversion. For AutoVC model we generate 50 intragender synthesized speech samples for each speaker. Fig. 7(a) and Fig. 7(b) show the synthetic speech similarity and ASR before VSMask protection. For VC models, after being trained with data in VCTK corpus, both AdaINVC and AutoVC synthetic voice samples can pass ASV with more than 90%success rate. Au toVC shows smaller variance because of its welldesigned bottle neck mechanism. But AdaINVC has better overall performance on similarity scores. We also test TTS model and compare its per formance with VC models. The SV2TTS model is trained with 16 kHz speech samples. To guarantee its best performance, we select60 victim speakers (30 males and 30 females) from LibriSpeech dataset. Also, we generate 50 synthesized speech samples from each speaker for testing. Then, we test all generated texttospeech samples in ASV system. Compared with VC models, regardless of the input phrase length, SV2TTS achieves the best similarity score and highest ASR. The potential reason is that SV2TTS generates speech content directly from text input, so that it contains no prior existing voice pattern. But for VC models, even though they are well optimized, there is residual voice pattern from the source speech p. This will deteriorate the voice similarity of the synthetic speech and victim speaker. In contrast, we also display the similarity score and ASR after VSMask protection. For VC models, VSMask substantially reduces the similarity score between synthetic speech and the raw speech from the victim speaker. None of the synthetic speech samples could compromise ASV system. Since the synthetic voice patterns in SV2TTS are entirely determined by the reference speech x+𝛿, this model is more susceptible to VSMask protection compared to VC models. Overall, under VSMask protection, it is unlikely for attackers to spoof speaker verification system by either VC or TTS. 5.4 Human Study Recent work [ 6] shows voice synthesis attack not only compromises ASV systems, but also deceives human ears. In this section, we run a human study of VSMask2. We select AdaINVC and AutoVC for synthetic voice generation because voice conversion models have more natural utterance than TTS. In the human study evaluation, we recruit 25 anonymous volun teers with normal hearing ability (14 males and 11 females, ages from 20 to 38 years old), and ask them to listen to 12 groups of speech samples through the same loudspeaker with fixed volume (∼60 dBA). 6 groups of synthetic speech samples are from AdaIN VC model and 6 groups are from AutoVC model. For each model, half of samples are maletomale and the other half is femaleto female. Each group contains 4 audio samples: ➀raw speech audio without protection, ➁the same speech audio but with VSMask protection, ➂synthetic voice sample from the raw speech, ➃the same synthetic voice sample from VSMask protected speech. All the speech sample lengths are from 4 seconds to 8 seconds. First, we ask the volunteers about the noise level in VSMask protected speech. After listening to the raw speech and protected speech, the listeners are suppose to measure the perturbation per ceptibility of the protected sample. Options from low to high are: (A) Cannot hear any noise; (B) Can hear noise, but negligible; (C) Can hear noise, but acceptable; (D) Can hear conspicuous noise; (E) Very strong noise, unacceptable. We collect all answers and present them in Fig. 8(a). The re sults depict that for both AdaINVC and AutoVC, the noise level in the protected voice samples is acceptable for human ears. Around 60% answers are ""negligible"" or ""imperceptible"", which means that VSMask protected speech audio has similar clarity as raw audios. More than 90% answers consider the noise in protected audios is acceptable for human ears. Only 2 out of 300 answers point out that the protected speech has ""unacceptable"" noise. 2The IRB request has been approved by the university board. 7No noise Negligible Acceptable Unacceptable Conspicuous                                           (a) The noise level of protected speech for voice con version models. Different, for sure Different, not sure Can’t distinguish Same, for sure Same, not sure                                          ntag (b) Human evaluation of VSMask protection performance on AdaINVC model.                                                Different, for sure Different, not sure C  ’  dis i  uish Same, for sure Same, not sure(c) Human evaluation of VSMask protection performance on AutoVC model. Figure 8: Subjective human evaluation results about the protected speech noise level and the protection performance of VS Mask . Next, we ask the volunteers to evaluate the similarities between the raw speech and synthetic speech samples. Similarities options from low to high are: (A) Different voice, for sure; (B) Different voice, but not sure; (C) Cannot distinguish; (D) Same voice, but not sure; (E) Same voice, for sure. Consider the voice conversion performance difference caused by different speaker genders, we separately display results of maletomale and femaletofemale speech samples. Fig 8(b) and Fig 8(c) show the voice similarity between the ➀ raw audio & ➂synthetic audio from raw speech, and ➀raw audio &➃synthetic audio from protected speech. For both AdaINVC and AutoVC, most synthetic speeches from raw speech can spoof human ears. More than 75% answers indicate that the synthetic speech is definitely or probably from the same speaker as the raw audio. Only 4% answers indicate that the voice is absolutely from another speaker. Therefore, the results show that synthetic voice has sufficient similarity as the victim speaker when launching intra gender voice conversion. In contrast, for synthetic speech samples from VSMask protected speech (M2MVM and F2FVM), they could not fool human ears any more. More than 75% answers believe that the synthetic audios from VSMask protected speeches have definitely different voice from the victim speaker. In total, over 97% answers claim that they will doubt the speaker identity behind the voice. Nobody consider that the synthetic speech matches the victim’s voice very well. Furthermore, the offline PGD method has been shown to achieve comparable protection performance in preventing deepfake voices from deceiving human ears [10]. In addition, we present the impact of weightbased noise mitigation in Appendix B. 5.5 Crossmodel Evaluation AttackVC [ 10] evaluates defense performance under blackbox scenarios by training substitute models. However, sometimes we have no information about which voice synthesis model will be implemented. In this section, we evaluate VSMask performance under crossmodel conditions. We apply VSMask to generate adversarial speech samples tar geting one voice synthesis model, and apply them on other models. The crossmodel ASR is listed in Table 2. Overall, VSMask is still effective for crossmodel scenario. But the performance is degradedTable 2: We generate the adversarial examples from the source model and apply the target model for voice synthe sis. The result shows that VSMask can still reduce the ASR under crossmodel setup. TargetSourceAdaINVC AutoVC SV2TTS AdaINVC  15.0% 10.5% AutoVC 12.8%  0.0% SV2TTS 7.3% 15.2%  as the result of different encoder models, preprocessing methods, and training data. For example, AdaINVC applies 512dim mel spectrogram as speaker encoder input, but 80dim for AutoVC speaker encoder. Also, SV2TTS is trained with 16 kHz audios, but AdaINVC and AutoVC use 48 kHz audios. When we input 16 kHz speech sample in AutoVC, it could not output qualified audios. Moreover, the adversarial samples targeting AdaINVC can reduce the similarity of AutoVC synthetic speech as well. When we use adversarial samples from AdaINVC or AutoVC on SV2TTS model, a few synthetic samples can still compromise ASV system. We can further apply AdaBelief method [ 24] to improve the transferability of protected samples by gradually reducing the learning rate, which we leave for future work. 5.6 Adaptive Attack Evaluation Denoisers are widely deployed in speech enhancement models for audio quality improvement. Adversarial perturbations can also be mitigated by some audio signal transformation methods. To evaluate VSMask robustness against adaptive attackers, we apply denoiser and WaveGuard [ 25] to remove the perturbation in pro tected audios and launch voice synthesis by AdaINVC model. All synthetic speech samples are from intragender speech synthesis. A simple denoiser fails to remove the welldesigned perturbation in protected speech, and the synthetic speech similarity is even fur ther degraded since the denoiser eliminates some lowpower signals in the speech. Typically, speechtotext models can still accurately transcribe speech with lower resolution. This allows WaveGuard to 8Table 3: VSMask performance when adaptive attackers ap ply denoiser and WaveGuard. Adaptive methodsNone DenoiserWaveGuard DownUp (𝑓=24k)Quan. Dequan.Mel. (Bin=128)LPC (Ord.=10) Score 0.096 0.090 0.078 0.082 0.080 0.073 compromise speech audio quality while mitigating the adversarial perturbation. We evaluated four WaveGuard signal transforma tion methods, including DownsamplingUpsampling with different frequencies, QuantizationDequantization, Melspectrogram con version with different mel bins, and Linear Predictive Coding (LPC) with different orders, on speech samples protected by VSMask . We list the best similarity score in Table 3. In contrast to adversarial examples that target speechtotext models, WaveGuard methods cannot undermine the protection performance of VSMask since VSMask optimizes the perturbation in melspectrogram, which is resilient to regular signal transformation techniques. In this way, the compression or transformation of audio cannot accurately re store the initial clear speech. Furthermore, WaveGuard reduces the audio quality of the synthetic speech. As a result, VSMask maintains a high level of robustness against denoising and signal transformation approaches. For sophisticated attackers, it is possi ble to recover the raw audio by reverse engineering if they have full knowledge about VSMask . Other approaches, for example, ran domized smoothing [ 26], adversarial training [ 27] and diffusion model [28] are potential solutions to evade VSMask . 6 DISCUSSION AND LIMITATIONS Defense in Realworld Scenarios . Fig. 9(a) displays the setup of physical world VSMask protection. The speaker is reading tran scripts from VCTK dataset in a quiet room. Meanwhile, VSMask is deployed on a laptop 50 cm away from the speaker. At the same time, an eavesdropper is stealthily recording the speech audio through an iPhone 13 one meter away from the speaker and VSMask . Then, the eavesdropper will synthesize the speaker’s voice in the recorded speech by AdaINVC model. Meanwhile, we put a sound level me ter close to the microphone to measure the volume of sounds. The background noise in the room is 40dBA, and we maintain the speech loudness around 75dBA. While the victim is talking in quiet room, the attacker can eavesdrop clear speech signal and generate qualified deepfake voice. Notably, in realworld scenarios, different from cyberspace, the perturbation 𝛿is usually distorted during overtheair transmis sion. To overcome the challenge, we implement a band pass filter (500∼4000 Hz) to filter vulnerable frequencies, and room impulse response (RIR) filter to mitigate the signal distortion in airborne transmission [ 29]. When VSMask is applied in a new environment, the RIR filter should be redesigned. Fig. 9(b) shows the protection impact under realworld scenarios. When the average perturbation loudness of VSMask reaches 46 dBA, it can effectively compromise the target voice synthesis model without affecting the speech intel ligibility. In comparison, random white noise with the same volume fails to degrade the similarity score much. When the perturbation volume reaches 50 dBA, VSMask can successfully reduce the ASR Speaker VSMaskMicrophone(a) Physical world experiment setup.                                                                             (b)VSMask protection performance in realworld scenario. Figure 9: We implement VSMask in realworld scenario, where it shows much stronger protection impact than white noise. to 0% while most synthetic speech generated from white noise masked speech can still bypass the ASV model. The experiment is launched in a quiet environment. If the environment is noisy (Volume > 50 dBA), the synthesized speech quality will also be affected even without VSMask . In particular, adaptive attackers may use a microphone array to denoise VSMask perturbation by phase cancellation [ 30]. We can enhance the spatial complexity and increase the difficulty of VSMask perturbation cancellation by utilizing multichannel loudspeakers. Realtime Feasibility . Training a predictive model for one speaker on an NVIDIA A6000 GPU costs less than 5 minutes. In the inference process, one single computation of the predictive model costs ∼13 milliseconds on a common CPU. We also measure the time delay caused by bidirectional transformation between audio signal and melspectrogram, which is ∼200 milliseconds. Therefore, the total time cost for generating voice perturbation is around 0.2 seconds, and the latency of 0.4 seconds is sufficient for VSMask to achieve realtime applicability. Adversarial Training . Adversarial training can effectively im prove the robustness of deep learning models [ 27]. Adaptive attack ers can apply adversarial training to improve the similarity between the synthetic voice and the victim speaker protected by VSMask . However, adversarial training inevitably reduce the synthesis per formance when dealing with clear speech audios. Moreover, recent work [ 31] shows that adversarial training takes up to 10 days with a powerful GPU group even for a mini dataset. Therefore, given VSMask ’s realtime protection feature, adversarial training is not a realistic solution that can be adopted by attackers to defeat VSMask . Limitations .VSMask has several limitations. First, specially de signed adaptive attacks are not considered in this paper, e.g., ad versaries who have full knowledge about VSMask . The adaptive attackers can purify the speech by a welldesigned neural network or diffusion models, and we leave the exploration of these eva sive approaches in future work. Second, we only launch realworld evaluation in a single room in the physical experiments. In differ ent environments, the performance of VSMask might be different. Third, VSMask cannot provide 100% successful protection regard less of the voice synthesis model. A few synthetic speech can still pass ASV systems under pure blackbox setup. Finally, the proposed method does not provide a formal privacy guarantee for human speech. This paper aims to conduct empirical privacy analysis with VSMask and provide a practical solution to enhance speech privacy in realtime physical and digital scenarios. 97 RELATED WORK "
22,Spectrum Translation for Cross-Spectral Ocular Matching.txt,"Cross-spectral verification remains a big issue in biometrics, especially for
the ocular area due to differences in the reflected features in the images
depending on the region and spectrum used.
  In this paper, we investigate the use of Conditional Adversarial Networks for
spectrum translation between near infra-red and visual light images for ocular
biometrics. We analyze the transformation based on the overall visual quality
of the transformed images and the accuracy drop of the identification system
when trained with opposing data.
  We use the PolyU database and propose two different systems for biometric
verification, the first one based on Siamese Networks trained with Softmax and
Cross-Entropy loss, and the second one a Triplet Loss network. We achieved an
EER of 1\% when using a Triplet Loss network trained for NIR and finding the
Euclidean distance between the real NIR images and the fake ones translated
from the visible spectrum. We also outperform previous results using baseline
algorithms.","Biometric recognition has gained attention in the last years due to an in crement of interest in secure systems for border control and fo rensic purposes but also forprivate reasons like access control to oﬃces and elect ronic devices like computers and smartphones. In this context, deep learning sy stems like convolutional neural networks (CNNs) have contributed signiﬁca ntly to their extension due to their state of the art performance in many diﬀere nt signal Preprint submitted to Journal Name February 18, 2020processing, computer vision, and pattern recognitiontasks Sund ararajanand Woodard [1]. Over theyears, many diﬀerent bodyregions have been proposeda spoten tial biometric recognition cues like ﬁngerprints, voice, hand geomet ry, gait, face, iris...Jain et al. [2]. The ocular area has proven to be not only one o f the most discriminative regions of the face, but it also gives quite stable p erfor mance among population and age. Many researchers have investiga ted the diﬀerent parts of this area for biometric purposes Rattani and De rakhshani [3]. E.g. iris’s texture pattern recognition Daugman [4], vein pattern s in the sclera Zhou et al. [5] or retina Hill [6] and the general shape and tex ture of the periocular area Park et al. [7]. Other newer and more complex cue s based onocular movements have arisen as biometric options due to their dis tinctive patterns and their diﬃculties to spoof. Some examples on what this t ype of algorithms look are tracking eye movements or gaze due to certain s timulus Rigas et al. [8], eye muscle movements Komogortsev et al. [9], or eye ﬁ xation and saccades Holland and Komogortsev [10]. Depending on which ocular biometric you are using and the acquisition constraint of the system, a speciﬁc spectral bandwidth can achie ve better performance. In the case of iris recognition, the use of NearInf rared (NIR) cameras allows extracting more detailed textures images of the iris p attern, especially when dealing with dark irises Daugman [4]. However, RGB image s of iris can still be used for recognition depending on the circumstanc es, e.g., when the acquisition distance has to be more signiﬁcant. Other syst ems like periocular or sclera are usually collected under visual light conditions (VIS) inRGBorgrayscaleimages. Forthesclera, standardcamerascanc apturethe vein patterns in the upper part of this region, and more speciﬁcally, the use of the blue and green channels gives better performance than NIR cameras used for Iris recognition Rattani and Derakhshani [11]. The perio cular area has been studied under NIR and VIS light AlonsoFernandez and Bigu n [12]. Undervisiblelight, onecanusefurtheracquisitiondistancesanddet ectbetter skin texture; however, NIR can also achieve good performance an d be easily combined with the iris in multimodal biometric systems AlonsoFernand ez et al. [13]. In this paper, we focus on the periocular area and iris pattern for c ross spectral biometric recognition. The usefulness of such systems r elies upon their ﬂexibility, allowing good performance in long or short distances, change on the type of camera used, security requirements, or when dealin g with legacy images from previous or diﬀerent systems. More concretely , we ana 2lyze the use of Conditional Generative Adversarial Networks (CGA Ns) Mirza and Osindero [14] for ImagetoImage transformations, in this ca se change of spectrum, in terms of visual and recognition quality. We also inves tigate the opportunities that this brings for biometric veriﬁcation algorith ms since it would allow closing the gap between the diﬀerence in the spectrum im ages that makes crossspectral veriﬁcation systems to perfor m worse than samespectrum matching. Therestofthepaperisorganizedasfollows: Section2summarizesr elated research work in the ﬁeld of crossspectral ocular recognition; in Section 3 we explain the diﬀerent parts involved in the experimentation of this res earch; in Section 4 we present the results from all of our experiments; Sec tion 5 analyze the results of the experiments, try to extract any insight from them and propose future work based on them. 2. Related Work "
39,Deep multi-metric learning for text-independent speaker verification.txt,"Text-independent speaker verification is an important artificial intelligence
problem that has a wide spectrum of applications, such as criminal
investigation, payment certification, and interest-based customer services. The
purpose of text-independent speaker verification is to determine whether two
given uncontrolled utterances originate from the same speaker or not.
Extracting speech features for each speaker using deep neural networks is a
promising direction to explore and a straightforward solution is to train the
discriminative feature extraction network by using a metric learning loss
function. However, a single loss function often has certain limitations. Thus,
we use deep multi-metric learning to address the problem and introduce three
different losses for this problem, i.e., triplet loss, n-pair loss and angular
loss. The three loss functions work in a cooperative way to train a feature
extraction network equipped with Residual connections and
squeeze-and-excitation attention. We conduct experiments on the large-scale
\texttt{VoxCeleb2} dataset, which contains over a million utterances from over
$6,000$ speakers, and the proposed deep neural network obtains an equal error
rate of $3.48\%$, which is a very competitive result. Codes for both training
and testing and pretrained models are available at
\url{https://github.com/GreatJiweix/DmmlTiSV}, which is the first publicly
available code repository for large-scale text-independent speaker verification
with performance on par with the state-of-the-art systems.","SV (speaker veriﬁcation) is a key technology for intelligent interaction. It can be widely used in ﬁnancial payment, crim inal investigation, national defense and other ﬁelds. It is one application in speech recognition that aims to verify a claimed identity based on his /her utterance [1]. This task is a 1 : 1 match where one speaker’s voice is matched to a particular template. SV can be categorized into textdependent and textindependent [2, 3]. The textdependent SV system requires the speech to be produced from a ﬁxed or prompted text phrase, while the textindependent SV system operates on unconstrained speech. Therefore, textindependent SV is a more challenging problem, but it is more useful in practical applications. Generally, a deep learningbased SV system contains the training step and testing step [3]. In the training step, we use a large collection of utterances to train an SV neural network. The learned deep neural network model is used as a universal feature extractor for any testing speaker. Then in the testing step, two di erent utterances are separately sent to the learned deep model for feature extraction, and we compute the similar ity based on the two feature vectors to perform SV . In the test phase, the false acceptance /rejection rates depend on the prede ﬁned threshold [4]. The equal error rate (EER) metric projects the error when the two aforementioned rates are equal. The basic training veriﬁcation system is shown in Figure 1. Corresponding author Email address: xgwang@hust.edu.cn (Xinggang Wang) Figure 1: Illustration of SV using a deep metric learning network. Before the era of deep learning, traditional SV models made remarkable achievements. For example, the Gaussian mixture model with a universal background model (GMMUBM) [5] uses a su ciently large speech dataset of several hours from multiple sources. For a Ddimensional feature vector x, the mixture density used for the likelihood function is deﬁned as P(xj)=MX i=1wipi(x): (1) The density is a weighted linear combination of Munimodal Gaussian densities, pi(x), each of which is parameterized by a mean D1 vector,, and a DDcovariance matrix i[6]: Pi(x)=1 (2)D=2jij1=2exp( "
297,Robust Training and Verification of Implicit Neural Networks: A Non-Euclidean Contractive Approach.txt,"This paper proposes a theoretical and computational framework for training
and robustness verification of implicit neural networks based upon
non-Euclidean contraction theory. The basic idea is to cast the robustness
analysis of a neural network as a reachability problem and use (i) the
$\ell_{\infty}$-norm input-output Lipschitz constant and (ii) the tight
inclusion function of the network to over-approximate its reachable sets.
First, for a given implicit neural network, we use $\ell_{\infty}$-matrix
measures to propose sufficient conditions for its well-posedness, design an
iterative algorithm to compute its fixed points, and provide upper bounds for
its $\ell_\infty$-norm input-output Lipschitz constant. Second, we introduce a
related embedded network and show that the embedded network can be used to
provide an $\ell_\infty$-norm box over-approximation of the reachable sets of
the original network. Moreover, we use the embedded network to design an
iterative algorithm for computing the upper bounds of the original system's
tight inclusion function. Third, we use the upper bounds of the Lipschitz
constants and the upper bounds of the tight inclusion functions to design two
algorithms for the training and robustness verification of implicit neural
networks. Finally, we apply our algorithms to train implicit neural networks on
the MNIST dataset and compare the robustness of our models with the models
trained via existing approaches in the literature.","Recent advances in machine learning have led to increasing deployment of neural networks in realworld applications, including natural language processing, computer vision, and selfdriving vehicles. Despite their remarkable computa tional power, neural networks are notoriously vulnerable to adversarial attacks; a small perturbations in the input can lead to large deviations in the output (Szegedy et al., 2014). Understanding this input sensitivity is essential in safety critical applications, since the consequences of adversarial perturbations can be disastrous. Unfortunately, many of the existing approaches for robustness analysis of neural networks either (i) are based on speciﬁc attacks and do not provide any formal guarantees (Athalye et al., 2018), or (ii) provide guarantees which are too conservative (Szegedy et al., 2014), or (iii) are not scalable to largescale prob lems (Combettes & Pesquet, 2020). As a result, there has been a huge interest in developing computationally tractable and nonconservative algorithms for training and veriﬁca tion of robust neural networks. Implicit neural networks are a class of learning models that replace the explicit hidden layers with an implicit equa tion (Bai et al., 2019; El Ghaoui et al., 2021). Compared to traditional neural networks, implicit neural networks are known to have advantages including (i) being more suitable for certain class of learning problems such as constrained optimization problems (Amos & Kolter, 2017) (ii) being more memory efﬁcient while maintaining comparable accu racy (Bai et al., 2019), and (iii) showing improved training due to fewer vanishing and exploding gradients (Kag et al., 2020). Despite their beneﬁts, implicit networks can suffer from wellposedness issues and convergence instabilities. Additionally, their inputoutput behavior may suffer from robustness issues and adversarial perturbations. We note that many of the classical robustness analysis tools for tradi tional neural networks are either not applicable to implicit neural networks or will lead to conservative results. Indeed, robustness of implicit neural networks is not yet well un derstood and open questions remain regarding their robust training and veriﬁcation. Most of the existing approaches for studying robustness of neural networks focus on either the `2norm or`1arXiv:2208.03889v1  [cs.LG]  8 Aug 2022Robust Training and Veriﬁcation of Implicit Neural Networks norm robustness measures. For neural networks with high dimensional inputs and subject to dense perturbations, `2 norm robustness measures are known to provide overly con servative estimates of robustness and are less informative than their`1norm counterparts. In this paper, we pro pose a framework based on contraction theory with respect to nonEuclidean `1norm to study wellposedness, sta bility, and robustness of implicit neural networks. To pro vide robustness guarantees, we overapproximate reachable set of implicit neural networks using (i) `1norm input output Lipschitz constants, and (ii) inputoutput tight in clusion functions. We note that, in general, ﬁnding the Lipschitz constants and tight inclusion functions of implicit neural networks can be computationally challenging. Us ing our nonEuclidean contractive approach, we provide nonconservative and computationally tractable estimates of the`1norm inputoutput Lipschitz constants and the tight inclusion functions of implicit neural networks. We then use these estimates of the Lipschitz constants and the inclusion functions to design two algorithms for training and veriﬁcation of implicit neural networks with respect to `1box input perturbations. Finally, we evaluate the perfor mance and efﬁciency of our algorithms for training robust implicit neural networks on the MNIST dataset. This paper is a review of the accepted papers (Jafarpour et al., 2021; 2022) and the submitted paper (Davydov et al., 2022). 2. Related works "
138,A writer-independent approach for offline signature verification using deep convolutional neural networks features.txt,"The use of features extracted using a deep convolutional neural network (CNN)
combined with a writer-dependent (WD) SVM classifier resulted in significant
improvement in performance of handwritten signature verification (HSV) when
compared to the previous state-of-the-art methods. In this work it is
investigated whether the use of these CNN features provide good results in a
writer-independent (WI) HSV context, based on the dichotomy transformation
combined with the use of an SVM writer-independent classifier. The experiments
performed in the Brazilian and GPDS datasets show that (i) the proposed
approach outperformed other WI-HSV methods from the literature, (ii) in the
global threshold scenario, the proposed approach was able to outperform the
writer-dependent method with CNN features in the Brazilian dataset, (iii) in an
user threshold scenario, the results are similar to those obtained by the
writer-dependent method with CNN features.","Signature Veriﬁcation (SV) systems are used to automatically recognize whether the signature provided by an user actually belongs to the individual who he/she claims to be. Therefore, these systems are useful in many realworld applications, such as credit card transactions or document authentication. Speciﬁcally, the problem of automatic Handwritten Signature Veriﬁcation (HSV) can be deﬁned as follows: given a learning set containing genuine signatures of a set of users, a model is trained to classify the signatures as genuine or forgeries. Genuine signatures are those that really belong to the indicated user; in turn, forgeries are those created by other people [1]. The signatures to be veriﬁed by the HSV systems can be acquired in two ways: ofﬂine (static) and online (dynamic). In ofﬂine SV , the signature is acquired after the writing process is completed. In this case, the signature is treated as an image. On the other hand, in online veriﬁcation, a device is used to collect data as it is produced. So, online models can obtain additional information from the users during writing to perform This article has been accepted for publication in BRACIS 2018 but has not yet been fully edited. Some content may change prior to ﬁnal publication.the veriﬁcation, such as the position or slope of the pen, or the writing pressure [1]. Another important point that deserves to be highlighted when dealing with the HSV problem is the model’s user horizon. If a model is trained for each user, the system is called writerdependent (WD). In this case, initially, a training set is constructed as follows: genuine signatures of the tested user are treated as positive instances and signatures from other users as negative. Next, a binary classiﬁer is trained for each user. Although WD systems achieve good results for the HSV task, requiring a classiﬁer for each user increases the complexity and the cost of the system operations as more users are added [2]. On the other hand, HSV systems used to classify signatures of any available user in the dataset are known as writer independent (WI) systems. In this context, a single model is trained for all users from a dissimilarity space. Thus, the classiﬁcation inputs are vectors of dissimilarity, which represent the difference between the features of a queried signature and a reference signature of the user. When compared to the WD approach, WI systems are less complex, but in general obtain worse results [3]. Recently, Hafemann et al. [3] proposed an approach to deal with the ofﬂine HSV problem that uses concepts from both WI and WD systems. The approach carries out feature learning from the signature images in a WI format, using a deep convolutional neural network (CNN) called SigNet. After being trained, the CNN is used to extract representative features from the signatures, which are used to train a writerdependent Support Vector Machine (SVM) classiﬁer for each writer. Their results showed a signiﬁcant improvement in performance when compared to the previous stateoftheart methods. The main objective of this paper is to investigate whether the deep CNN features learned by the Hafemann et al. model [3] (available online1) can also lead to good results in a writer independent HSV context. To this end, it is proposed the 1http://en.etsmtl.ca/Unitesderecherche/LIVIA/Rechercheet innovation/Projets/SignatureVeriﬁcationarXiv:1807.10755v1  [cs.CV]  26 Jul 2018This article has been accepted for publication in BRACIS 2018 but has not yet been fully edited. Some content may change prior to ﬁnal publication. use of dichotomy transformation [4] combined with an SVM as a writerindependent classiﬁer to perform the signature veriﬁcation. The following points will be analyzed: (i) which partial decisions fusion rule is the best (functions max, mean, median and min are tested). (ii) The inﬂuence of the number of signatures used in the reference set. (iii) A comparison with other studies from the literature. The experiments are carried out using the GPDS and the Brazilian PUCPR datasets. The remaining of this paper is organized as follows: Section II presents the related work on signature veriﬁcation, separating WD and WI approaches. Section III details the proposed method, and Section IV describes the used experimental protocol and the discussion of the results. Lastly, Section V concludes the paper and discusses future works. II. R ELATED WORKS "
196,The Power of Typed Affine Decision Structures: A Case Study.txt,"TADS are a novel, concise white-box representation of neural networks. In
this paper, we apply TADS to the problem of neural network verification, using
them to generate either proofs or concise error characterizations for desirable
neural network properties. In a case study, we consider the robustness of
neural networks to adversarial attacks, i.e., small changes to an input that
drastically change a neural networks perception, and show that TADS can be used
to provide precise diagnostics on how and where robustness errors a occur. We
achieve these results by introducing Precondition Projection, a technique that
yields a TADS describing network behavior precisely on a given subset of its
input space, and combining it with PCA, a traditional, well-understood
dimensionality reduction technique. We show that PCA is easily compatible with
TADS. All analyses can be implemented in a straightforward fashion using the
rich algebraic properties of TADS, demonstrating the utility of the TADS
framework for neural network explainability and verification. While TADS do not
yet scale as efficiently as state-of-the-art neural network verifiers, we show
that, using PCA-based simplifications, they can still scale to mediumsized
problems and yield concise explanations for potential errors that can be used
for other purposes such as debugging a network or generating new training
samples.","In recent years, neural networks have been a driving force behind many of the most exciting success stories in machine learning. From image recognition [ GWK¸18] and speech recognition [ BMR¸20] to playing complex gamesonasuperhumanlevel[ VBC¸19],neuralnetworks have achieved results that were almost unthinkable even a decade ago. However, while the size, performance and scope of neural networks steadily increases, their opaqueness re mainsanequallyimportantandessentiallyunsolvedprob lem [AB18]. Frequently denoted as “blackbox”models, the decisions of neural networks are to this day hard to explain and, likewise, their properties hard to verify. In this paper, we are concerned with Typed Aﬃne De cision Structures (TADS) [ SNMB22 ], a novel decision treelike data structure that represents piecewise aﬃne functions. TADS are speciﬁcally designed to act as in terpretablewhiteboxmodelsthatcanpreciselyrepresent anypiecewiselinearneuralnetworkinanunderstandable fashion. While TADS are structurally wellsuited for global model explanation and veriﬁcation of neural networks, thefullexplanationofevenmediumsizedneuralnetworks is well out of scope. It is wellknown that the semantic complexity of a neural network—with respect to manydiﬀerentmeasuresofcomplexity—growsexponentiallyin itssize. Asaconsequence,anypreciseglobalexplanation ofsuchamodelincursexponentialscalingissues[ BS14a, MPCB14, FRH¸19]. In this paper, we are interested in applying TADS to verifying local properties of neural networks, most notablyrobustnessproperties[ CW17,LLWX18 ,SZS¸13, MST20]. Robustness properties encode that, at certain pointsthatauserdesires,aneuralnetwork’sclassiﬁcation is invariant to small changes of its input. For example, in imagerecognition,ifoneknowsthatanimagerepresentsa certainobject,asingleﬂipofapixelshouldnotdrastically change the networks correctclassiﬁcation of said image. Robustnesspropertiesarethemostcommonlyconsidered properties in neural network veriﬁcation and make up the majority of current benchmarks in the VNNComp veriﬁcation competition [BLJ21]. ToapplyTADS,whichareinprincipleglobalmodelex planations, to local properties, we will introduce precon dition projection, a transformation of TADS that restricts their domain to a certain region of interest. Further, we show how the algebraic properties of TADS can be used to directly model the classiﬁcation behavior of a neural network. Thisisimportantasneuralnetworks,although often used as classiﬁers (assigning one of ﬁnitely many classestoaninput),arefundamentallyregressionmodels 1arXiv:2304.14888v1  [cs.LG]  28 Apr 20232 G. Nolte et al. (assigning real values to their input). By modeling the argmax function directly on a TADS level, this gap can be bridged in an elegant fashion. Finally,wewillpresentacasestudyinwhichweapply TADStorobustnessanalysisandpresentitsadvantages. At present, TADS do not yet scale well to larger prob lems. We will introduce an approach that uses the well understood dimensionality reduction technique PCA to proveanunderapproximationtotherobustnessproperty of interest. This approach mitigates the scaling issues in curredbyTADSs,butlacksreliableguaranteesonrobust ness. Thus, we introduce another approach that directly trains neural networks to operate on inputs that are sim pliﬁed by PCA. This method is of similar computational complexity than the underapproximation approach, but yields neural networks for which TADS can give reliable robustness guarantees, while incurring only a small loss in neural network accuracy. Lastly, we will show on a concrete example how a TADSbasedrobustnessprooflookslikeandwhataddi tionalinformationityieldsbeyondalreadyexistingveri ﬁcation tools. We will show how this information can be used to characterize precisely and completely the entire set of inputs that violate a given property, and how it can be used to ﬁnd “closest” adversarial examples, if they exist. 2 Preliminaries 2.1 Linear Algebra and Notation Thefollowingnotationsoflinearalgebraarebasedonthe book[Axl97]. Therealvectorspace ¹R𝑛¸ºwith𝑛¡0 is an algebraic structure with the operations ¸:R𝑛R𝑛!R𝑛vector addition :RR𝑛!R𝑛scalar multiplication which are deﬁned as ¹𝑥1𝑥𝑛º¸¹𝑦1𝑦𝑛º=¹𝑥1¸𝑦1𝑥𝑛¸𝑦𝑛º 𝜆¹𝑥1𝑥𝑛º=¹𝜆𝑥1𝜆𝑥𝑛º. A real vector¹𝑥1𝑥𝑛ºofR𝑛is abbreviated as®𝑥. To refer to its𝑖th component, we write 𝑥𝑖(in contrast,®𝑥𝑖 denotesthe𝑖thvectorinsomeenumeration). Thedimen sion of a real vector space R𝑛is given as dimR𝑛=𝑛. A matrix 𝑾is a collection of real values arranged in a rectangular array with 𝑛rows and𝑚columns. 𝑾=©­­­­ «𝑤11𝑤12 𝑤 1𝑚 𝑤21𝑤22 𝑤 2𝑚  𝑤𝑛1𝑤𝑛2 𝑤𝑛𝑚ª®®®® ¬Toindicatethenumberofrowsandcolumns,onesays 𝑾 hastype𝑛𝑚commonly notated as 𝑾2R𝑛𝑚. An elementat position 𝑖𝑗of thematrix 𝑾is denoted by𝑾𝑖𝑗:=𝑤𝑖𝑗(where 1𝑖𝑛and1𝑗𝑚). A matrix 𝑾2R𝑛𝑚can be reﬂected along the main diagonal resulting in the transpose 𝑾>of shape𝑚𝑛 deﬁned by the equation "
441,Neural Networks with Manifold Learning for Diabetic Retinopathy Detection.txt,"Widespread outreach programs using remote retinal imaging have proven to
decrease the risk from diabetic retinopathy, the leading cause of blindness in
the US. However, this process still requires manual verification of image
quality and grading of images for level of disease by a trained human grader
and will continue to be limited by the lack of such scarce resources.
Computer-aided diagnosis of retinal images have recently gained increasing
attention in the machine learning community. In this paper, we introduce a set
of neural networks for diabetic retinopathy classification of fundus retinal
images. We evaluate the efficiency of the proposed classifiers in combination
with preprocessing and augmentation steps on a sample dataset. Our experimental
results show that neural networks in combination with preprocessing on the
images can boost the classification accuracy on this dataset. Moreover the
proposed models are scalable and can be used in large scale datasets for
diabetic retinopathy detection. The models introduced in this paper can be used
to facilitate the diagnosis and speed up the detection process.","Diabetic retinopathy (DR) is a severe and common eye dis ease which is caused by changes in the retinal blood vessels. Diabetic retinopathy is the leading cause of blindness in the working age US population (age 2074 years). Ninetyﬁve percent of this vision loss is preventable with timely diagno sis. The national eye institute1estimates that from 2010 to 2050, the number of Americans with diabetic retinopathy is expected to nearly double, from 7.7 million to 14.6 million. By 2030, a half a billion people worldwide are expected to develop diabetes and a third of these likely will have DR. DR can be detected during a dilated eye exam by an ophthalmol ogist or optometrist in which the pupils are pharmacologi cally dilated and the retina is examined with specialized con densing lenses and a light source in real time. Primary barri ers for diabetic patients in accessing timely eye care include lack of awareness of needing eye exams, inequitable distri bution of eye care providers and the addition costs in terms of travel, time off from work, and additional medical fees of 1https://nei.nih.gov/eyedata/diabetic (a)  (b)  (c) (d)  (e) Fig. 1 . Illustration of the stages of DR development: (a) Nor mal , (b) Mild, (c) Moderate , (d) Severe and (e) Proliferative diabetic retinopathy conditions. seeing another care provider [1]. Teleophthalmology, or us ing nonmydriatic camera in nonophthalmic settings, such as primary care ofﬁces, to capture digital images of the central region of the retina in diabetic patients has increased the rate of annual diabetic retinopathy detection [2]. This methodol ogy has been validated against dilated retinal examinations by eye specialists and is accepted by the UK National Health Service and US National Center for Quality Assurance. In corporating an individuals retinal photos for those with dia betic retinopathy has been shown to be associated with better glycemic control for poorly controlled diabetic patients [3]. The procedures for DR detection can often be difﬁcult and dependent on scarce resources such as trained medical professionals. The severity of this widespread problem then in combination with the cost and efﬁciency of the detection procedures motivates computeraided models that can learn the underlying patterns of this disease and speedup the detec tion process. In this paper, we introduce a set of neural net works (NNs) that can facilitate diabetic retinopathy detection by training a model on color fundus photography inputs. Fundus photography or fundography is the process of photographing of the interior surface of the eye, including the retina, optic disc, macula, and posterior pole [4]. FunarXiv:1612.03961v1  [cs.CV]  12 Dec 2016(a)  (b)  (c)  (d)  (e) Fig. 2 . Input images from the sample dataset in ﬁve labeled classes: (a) Normal , (b) Mild, (c) Moderate , (d) Severe and (e) Proliferative DR are shown. dus photography is used by trained medical professionals for monitoring progression or diagnosis of a disease. Figure 1 il lustrates example fundography images from a sample dataset for various stages2of diabetic retinopathy disease. The digital fundus images often contain a set of lesions which can indicate the signs of DR. Automated DR detection algorithms are composed of multiple modules: image prepro cessing, feature extraction and model ﬁtting (training). Im age preprocessing reduces noise and disturbing imaging ar tifacts, enhances image quality using appropriate ﬁlters and deﬁnes the region of interest (ROI) within the images [5]. Il lumination and quality variations often make preprocessing an important step for DR detection. Several attempts in com puter vision literature have been proposed to best represent the ROI [6, 7] in fundus images. Image enhancement, mass screening (including detection of pathologies and retinal fea tures), and monitoring (including feature detection and regis tration of retinal images) are often cited as three major con tributions of preprocessing to DR [7]. Greenplane represen tation of RGB fundus images to increase the contrast for red lesions [6] and shadow correction to address the background vignetting effect [8] are perhaps among the most popular pre processing steps for the task in hand [9, 10]. Landmark and feature detection [11] and segmentation [12, 9] on digital fun dus images have also been explored as preprocessing steps. The second step in developing an automatic DR detec tion algorithm is the choice of a learning model or classi ﬁer. DR detection has been explored through various classi ﬁers such as fuzzy Cmeans clustering [13], neural networks [14] and a set of feature vectors on benchmark datasets [7]. While the detection and preprocessing algorithms appear to be maturing, the automatic detection of DR remains an ac tive area of research. Scalable detection algorithms are of ten required to validate the results on larger, welldeﬁned, but more diverse datasets. The quality of input fundus pho tographs, image clarity problem, lack of available benchmark datasets and noisy dataset labels often present challenges in automating the diabetic retinopathy detection of input data or developing a robust set of feature vectors to represent the in put data. In this paper, we explore neural networks with do main knowledge preprocessing and introduce manifold learn 2The stages of DR correspond to class labels for DR detection and clas siﬁcation. Class labels are assigned based on dividing the stages of disease into 5 different classes as illustrated in Figure 2.ing techniques that can boost the classiﬁcation accuracy on a sample dataset. This paper is organized as follows: In section 2 we present the outline of our proposed model. Preprocessing is presented in Section 2.1. We present the comparative results of DR de tection on a sample dataset in Section 3. Finally we conclude this paper in Section 4 and propose future research directions. 2. METHOD "
118,Boosting the Adversarial Transferability of Surrogate Model with Dark Knowledge.txt,"Deep neural networks (DNNs) for image classification are known to be
vulnerable to adversarial examples. And, the adversarial examples have
transferability, which means an adversarial example for a DNN model can fool
another black-box model with a non-trivial probability. This gave birth of the
transfer-based adversarial attack where the adversarial examples generated by a
pretrained or known model (called surrogate model) are used to conduct
black-box attack. There are some work on how to generate the adversarial
examples from a given surrogate model to achieve better transferability.
However, training a special surrogate model to generate adversarial examples
with better transferability is relatively under-explored. In this paper, we
propose a method of training a surrogate model with abundant dark knowledge to
boost the adversarial transferability of the adversarial examples generated by
the surrogate model. This trained surrogate model is named dark surrogate model
(DSM), and the proposed method to train DSM consists of two key components: a
teacher model extracting dark knowledge and providing soft labels, and the
mixing augmentation skill which enhances the dark knowledge of training data.
Extensive experiments have been conducted to show that the proposed method can
substantially improve the adversarial transferability of surrogate model across
different architectures of surrogate model and optimizers for generating
adversarial examples. We also show that the proposed method can be applied to
other scenarios of transfer-based attack that contain dark knowledge, like face
verification.","Deep neural networks (DNNs) have achieved substantial success on many computer vision tasks [1]. However, they are shown to be vulnerable to adver sarial examples. Adversarial examples [2] are carefully crafted data which could fool the DNNs by adding imperceptible noises on legitimate data. They cause the security problems in many applications, such as face recognition and autonomous driving, etc. The transferability of adversarial examples has attracted much attention. It means that, an adversarial example that fools one DNN model can fool another (blackbox) DNN model with a nontrivial probability. Consequently, an adversary can train a surrogate model locally (training stage), and then generate adversarial examples to fool the surrogate model (generating stage). Finally, the generated adversarial examples can be directly used to attack an unknown blackbox victim model (attacking stage). This process is called transferbased adversarial attack , illustrated in Fig. 1. Sometimes the training stage is omitted by using a pretrained model as the surrogate. The technique of adversarial example optimizer has been proposed for gen erating highly transferable adversarial examples [3{5] (in the generating stage), thus improving the success rates of the transferbased attack. In contrast, we aim to train a surrogate model (in the training stage) so that it could yield adversarial examples with better success rates of the transferbased attack Adversarial  ExampleNormal Surrogate  Model Victim  ModelDNN OptimizerAdversarial  Example Optimizer Raw Dataset Enhance  Dark Knowledge Augmented Dataset DNN OptimizerDark  Surrogate  ModelAdversarial  Example OptimizerAdversarial  ExampleClean Image cat? dog! cat! 1. Training Stage 2. Generating Stage 3. Attacking Stage Pretrained Teacher  ModelExtracted  Dark KnowledgeMisclassify with low probability. Misclassify with almost zero probability. Misclassify with relatively high probability. Fig. 1 An illustration of transferbased adversarial attack and the proposed method. The two images in Raw Dataset are from ImageNet, labeled as \persian cat"" and \papillon"" respectively. Note that they also have features of other cats and dogs, as well as pillow and car. The normal surrogate model is trained by onehot labels, and its adversarial transfer ability is relatively weak. In contrast, the dark surrogate model we proposed is trained with enhanced dark knowledge. Thus, it demonstrates stronger adversarial transferability.Springer Nature 2021 L ATEX template Boosting the Adversarial Transferability of Surrogate Model with Dark Knowledge 3 when the architecture of the surrogate model and the adversarial example opti mizer are xed. In analogy to the commonly used term \the transferability of adversarial example"", we propose the concept \the adversarial transferability of surrogate model"" to describe the ability of a surrogate model on generating better adversarial examples for the transferbased attack, using a xed adver sarial example optimizer. Improving the adversarial transferability of surrogate model is still an underexplored research area. To the best of our knowledge, the only related work is [6], which reveals that a slightly robust model has better adversarial transferability, at the cost of large computational time for training the model. Data and labels are two important components in training DNNs. It is noticed that the surrogate models used in prior works were trained with one hot labels. However, the onehot label does not contain rich information of an image. Besides the features of the true class, an image often contains the fea tures of similar classes and even multiple objects (see Fig. 1). In contrast to onehot labels, knowledge distillation [7] utilizes soft labels as the predicted probability distribution from a trained teacher model. The soft label is also known as \dark knowledge"", which has been widely used to compress neural networks [7, 8] and improve inference accuracy [9{11]. Inspired by this, we pro pose to leverage the dark knowledge to boost the adversarial transferability of surrogate model in this paper. We rst conduct a case study on ImageNet dataset to investigate the eect of dark knowledge on adversarial transfer ability. The experimental results show that dark knowledge can improve the adversarial transferability of surrogate models without querying the target models. Specically, the adversarial transferability of surrogate model can be improved when anymodel for the same task is used to extract dark knowl edge and it is then used for training the surrogate model. Then, we propose to enhance the dark knowledge of training data by applying mixing augmentation skills. A case study is conducted to show that the adversarial transferability is remarkably improved by this enhancement. On the contrary, the adversarial transferability is impaired when the mixing augmentation is simply used for training the surrogate model. The surrogate model trained with dark knowledge is called \dark"" surro gate model (DSM) in this work. The corresponding process of transferbased attack is also shown in Fig. 1. The proposed method modies the training stage, which enhances the dark knowledge by applying mixing augmentation on the training data and using soft labels extracted from a pretrained teacher model. We have conducted extensive experiments on attacking image classi cation models to show that the proposed method remarkably and consistently improves the adversarial transferability of surrogate model. In addition, the proposed method can be applied to other transferbased attack scenarios that contain dark knowledge, such as face verication, image retrieval, and text classication, to improve the success rate of the transferbased attack. As an example, the experiments on applying DSM to attack face verication models are presented.Springer Nature 2021 L ATEX template 4 Boosting the Adversarial Transferability of Surrogate Model with Dark Knowledge The major contributions of this work are as follows. •For improving the success rates of the transferbased adversarial attack, we propose to use the dark knowledge during the training of the surrogate model, so as to obtain a \dark"" surrogate model (DSM). •The method for training the DSM is proposed, which leverages dark knowl edge to rene two key components of training surrogate models: data and labels. Firstly, a pretrained DNN model, regarded as a teacher model, is employed to generate soft labels with dark knowledge. Secondly, the mixing augmentation skills are applied to enhance the dark knowledge of the train ing data explicitly. Using the soft labels with enhanced dark knowledge, the DSM is trained to achieve signicantly improved adversarial transferability. •Extensive experiments on image classication are conducted to validate the proposed method. At rst, the DSM is trained by using a pretrained model of the same architecture as the teacher model. Compared with the transferbased attack using the pretrained model as the surrogate model, the proposed method with DSM improves the attack success rates of the untargeted attack on nine blackbox victim models by up to 19.8% ,22.9% and9.7% for the ResNet18, DenseNet121 and MobileNetv2 based surrogate models, respectively. Then, by using dierent teacher models, the maxi mum increments of attack success rate can be further improved to 22.9% , 36.0% and23.7% , respectively. For targeted attack, the proposed method can bring up an increase of 17.1% in the attack success rate over the state oftheart approach for training the surrogate model for better adversarial transferability [6]. •We have also applied the proposed method to the problem of attacking face verication models. On the stateoftheart ArcFace model [12], the proposed method improves the success rates of dodging attack by 16.3% and impersonate attack by 16.2% . For reproducibility, we will share the codes of the proposed method and experimental data on https://github.com/ydc123/Dark Surrogate Model. 1.1 Related Work "
103,Understanding and mitigating noise in trained deep neural networks.txt,"Deep neural networks unlocked a vast range of new applications by solving
tasks of which many were previously deemed as reserved to higher human
intelligence. One of the developments enabling this success was a boost in
computing power provided by special purpose hardware, such as graphic or tensor
processing units. However, these do not leverage fundamental features of neural
networks like parallelism and analog state variables. Instead, they emulate
neural networks relying on binary computing, which results in unsustainable
energy consumption and comparatively low speed. Fully parallel and analogue
hardware promises to overcome these challenges, yet the impact of analogue
neuron noise and its propagation, i.e. accumulation, threatens rendering such
approaches inept. Here, we determine for the first time the propagation of
noise in deep neural networks comprising noisy nonlinear neurons in trained
fully connected layers. We study additive and multiplicative as well as
correlated and uncorrelated noise, and develop analytical methods that predict
the noise level in any layer of symmetric deep neural networks or deep neural
networks trained with back propagation. We find that noise accumulation is
generally bound, and adding additional network layers does not worsen the
signal to noise ratio beyond a limit. Most importantly, noise accumulation can
be suppressed entirely when neuron activation functions have a slope smaller
than unity. We therefore developed the framework for noise in fully connected
deep neural networks implemented in analog systems, and identify criteria
allowing engineers to design noise-resilient novel neural network hardware.","A fundamental aspect of neural networks (NNs) is the propagation of information along the weighted connec tions between the countless nonlinear elements (neurons), making parallelism an essential prerequisite for an e cient implementation. Current digital computing archi tectures are, however, mostly serial and store connection weights in a memory spatially separated from the NN's nonlinear neurons. This so called von Neumann bottle neck induces substantial energy penalties, and maximal computing performance can only be achieved if a NN's highly distributed topology is faithfully reproduced by the computing hardware. Only then overheads due to ex cessive information transduction are avoided: each neu ron corresponds to a nonlinear component or circuit, each connection to a direct physical link. Beyond computingarchitecture aspects, there are also remarkable dierences arising when considering princi ples of information encoding. On one hand we have pro grams and computers based on symbolic, i.e. Boolean logic, for which the corruption of information has a dis astrous impact. A computer's binary voltage signal is therefore corrupted around once every 10 days, which at 1 GHz modulation bandwidth corresponds to one in 10241. Information theory and thermodynamics link this error probability to an energy overhead dictated by the funda mental laws of physics. NNs, on the other hand, lever age emergence in ensembles of nonlinear neurons. Signals a)Electronic mail: nadya.i.semenova@gmail.comare therefore 'averaged' across numerous analogue states, and the impact of reduced signal nesse can potentially be mitigated. Already today special purpose NN chips, i.e. the newest generation of tensor and graphic process ing units, allow low resolution (26 bit) computing, and algorithms of limited bit resolution are considered2. Re ducing the energy consumption of current NN hardware is therefore crucial already today, and research activity along these lines3has lately exploded. Unfortunately, reducing bitresolution mostly oers a linearly proportional energy saving. Crucially, ana logue signals are viable at such low resolution al lows boosting energy eciency ranging between 3 to 6 orders of magnitude1. Novel analogue compo nents such as lasers4, memristors5and spintorque oscillators6have been shown to serve as excellent ana logue neurons. Simultaneously, parallel networks based on holography7, diraction8,9, meshes of integrated MachZehnder interferometers10, wavelength division multiplexing11and 3D printed optical interconnects12{14 have demonstrated fully parallel networking. Finally, the concept of inmemory computing targets encoding a NN's topology based on tunable analogue circuits in electronic15{17and photonic systems18. Current estimations only consider the impact of addi tive noise on the level of a single neuron. This falls short of describing the reality of noise propagation in deep NNs (DNNs) in various aspects. Firstly, neurons and connec tions can exhibit dierent types of noise. Connections im plemented via phase change materials18exhibit thermal and parametric noise, electronic DNNs based on Boolean connections are sensitive comparator noise19, electronic multilayer perceptrons are sensitive to temperature andarXiv:2103.07413v3  [cs.NE]  16 Dec 20212 voltage  uctuations20. A wide range of analogue noisy hardware is therefore to be considered21{24. Secondly, noise in DNNs will be sensitive to the connection statis tics of each particular topology. For example, fully con nected layers strongly average signals, while only local connections do so signicantly less. Motivated by poten tially substantial benets, the central importance of noise for future NN hardware solutions is moving into focus. Noise in analogue components mostly is additive and/or multiplicative and can be correlated and/or uncorrelated with other elements of the circuit. Similar generalizations are possible for DNN topologies, where statistical meth ods rather than a description of each connection provide powerful tools. The growing interest in analog DNN hardware increas ingly identies noise as a common presence. In19the au thors consider an analog processor based on binary con nections and study the eect of comparator noise, which is additive. In20, the authors consider an analog circuit of a multilayer perceptron and investigate the eect of temperature and voltage variation on accuracy, which corresponds to correlated additive noise. Current stud ies exclusively focus on noise suppression in particular hardware21{25, but fail to capture the general principles of noise in analogue DNNs. The description and analy sis of noise in highdimensional nonlinear systems is an established eld of research in nonlinear dynamics26{29. However, these do only consider the eect of static, i.e. not trained networks. Here, we combine both approaches for the rst time in order to develop the general theory of noise propaga tion in DNN that comprise noisy nonlinear neurons and have been trained for specic tasks. In our previous work we considered the impact of noise on feedforward and re current NNs only comprising linear neurons and networks with uniform, i.e. untrained connections30. Here, we an alytically investigate the general aspects of noise in DNNs comprising nonlinear neurons. Importantly, we trained our DNNs according to standard error backpropagation. We demonstrate excellent accuracy of our analytical framework for DNNs trained in classication and ana logue function approximation, hence for the majority of tasks in the machine learning context. We nd that nonlinear neurons decorrelate noise otherwise correlated across neuron populations. Previously we identied such correlated noise as the most relevant perturbation, and noteworthy nonlinearity can therefore be benecial for a NNs signal to noise ratio (SNR). Our most important nding however is that noise propagation between con secutive network layers can be stopped completely, i.e. the accumulation of noise can be avoided. We derive the general conditions for such noisefreezing and show that these are easily met in DNN concepts as well as hard ware. Finally, even without such noise freezing, a DNN's SNR is generally bound and increasing its depth does not further reduce its SNR below a limit.II. NEURAL NETWORKS AND NOISY NEURONS Noise at the output of a DNN imposes an upper limit of its computing accuracy. For example, a perfect DNN model solving the MNIST problem without any misclas sication would be limited in its misclassication to the DNN processors output signal quality. For a low signal to noise ratio (SNR) of 10, such a system would create a misclassication 10% of the time. A detailed under standing of noise propagation is therefore of fundamental importance for next generation DNN hardware in order to combine energyeciency with computing misclassi cation . A. Deep neural networks DNNs may be found in multiple congurations, which can be categorized according to their connection topol ogy. A simple DNN comprising input and output layers, each comprising a single linear neuron, plus two hidden layers hosting the nonlinear neurons is schematically il lustrated in Fig. 1. Each neuron is identied by its layer indexn2[1;N] and intralayer position i2[1;In] withN andInas the number of layers and neurons in a partic ular layer, respectively. At each integer instant t2[1;T] the network receives input signal ut. In general, a neuron's internal state ~ xt n;icombines in put from other neurons according to connectivity weight matrix W ~xt n;i=In"
89,Scalable Verification of GNN-based Job Schedulers.txt,"Recently, Graph Neural Networks (GNNs) have been applied for scheduling jobs
over clusters, achieving better performance than hand-crafted heuristics.
Despite their impressive performance, concerns remain over whether these
GNN-based job schedulers meet users' expectations about other important
properties, such as strategy-proofness, sharing incentive, and stability. In
this work, we consider formal verification of GNN-based job schedulers. We
address several domain-specific challenges such as networks that are deeper and
specifications that are richer than those encountered when verifying image and
NLP classifiers. We develop vegas, the first general framework for verifying
both single-step and multi-step properties of these schedulers based on
carefully designed algorithms that combine abstractions, refinements, solvers,
and proof transfer. Our experimental results show that vegas achieves
significant speed-up when verifying important properties of a state-of-the-art
GNN-based scheduler compared to previous methods.","Designing efficient job scheduling for multiuser distributedcomputing clusters is a challenging and important task [Barroso et al .2013]. One of the main evaluation metrics of a schedule is performance, for example optimizing job completion time on a job profile. However, the user expectation typically requires that the scheduler satisfy a number of important properties beyond performance, such as strategyproofness, sharing incentive, and stability [Ghodsi et al .2011; Zaharia et al.2010]. If a scheduler lacks any of these properties, the result could be catastrophic, potentially costing millions of dollars at scale. For example, if the scheduler is not strategyproof (meaning that users can benefit from misrepresenting their job attributes), the users would be incentivized to manipulate their jobs to get them scheduled earlier than they are supposed to. The result could be long waiting times for all users or inefficient overall operation of the cluster [Zaharia et al. 2010]. Authors’ addresses: Haoze Wu, Department of Computer Science, Stanford University, USA; Clark Barrett, Department of Computer Science, Stanford University, USA; Mahmood Sharif, School of Computer Science, Tel Aviv University, Israel; Nina Narodytska, VMware Research, USA; Gagandeep Singh, School of Computer Science, University of Illinois at UrbanaChampaign, USA. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). ©2022 Copyright held by the owner/author(s). 24751421/2022/10ART162 https://doi.org/10.1145/3563325 Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.arXiv:2203.03153v4  [cs.AI]  15 Sep 2022162:2 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh Recently, a class of job schedulers [Mao et al .2019; Park et al .2021; Sun et al .2021] based on Graph Neural Networks (GNNs) were shown to achieve significant performance improvement over schedulers using handcrafted heuristics. However, whether these GNNbased job schedulers possess essential properties is not known and, more importantly, there are no tools available to check whether these properties hold. Formally guaranteeing these properties is known to be difficult and until now has only been achieved for simple handcrafted policies [Shenker and Stoica 2013]. Introducing techniques for proving or disproving such properties for GNNbased schedulers would allow system designers to better evaluate the policies implemented by these schedulers, making adjustments so that the scheduler satisfies users’ expectations without sacrificing the performance too much. However, GNNs’ decisionmaking processes are complex and opaque, making it challenging to formally validate these properties. In this work, we focus on the formal verification of GNNbased job schedulers, which, to the best of our knowledge, has not been considered in prior work. In particular, given a specification over a GNNbased job scheduler, our goal is to either formally prove the specification holds or disprove it with a counterexample. While there is a growing body of work on formally analyzing and verifying properties of deep neural networks applied in the vision, robotics, and natural language processing domains, work on formal analysis of ML models in the systems domain has been limited. This may be explained in part by the unique challenges posed by the systems domain, some of which we outline below. Computation graph with 100+ layers. GNNbased systems, including schedulers, perform a messagepassing algorithm as part of the inference stage. While message passing can be un rolled into a sequence of affine and nonlinear activation layers, the resulting network is quite deep, typically containing over 100layers. Existing stateoftheart verifiers are designed to handle shallower networks (typically <20layers) and begin to lose substantial precision [Boopathy et al . 2019; Dutta et al .2018; Ehlers 2017; Gehr et al .2018; Huang et al .2017; Lyu et al .2020; Müller et al.2021b; Raghunathan et al .2018; Salman et al .2019; Singh et al .2019a, 2018a, 2019b,c; Tjan draatmadja et al .2020; Tjeng et al .2019; Tran et al .2020; Wang et al .2018a,b, 2021b; Weng et al . 2018; Wong and Kolter 2018; Wu et al .2020a; Xiang et al .2018; Zelazny et al .2022; Zhang et al . 2018] or scalability [Anderson et al .2019; Bak et al .2020; Botoeva et al .2020; Bunel et al .2020, 2018; Ehlers 2017; Fischetti and Jo 2017; Fromherz et al .2020; Henriksen and Lomuscio 2021; Katz et al.2017, 2019; Khedr et al .2020; Lu and Kumar 2019; Tjeng et al .2019; Tran et al .2020; Vincent and Schwager 2020; Wu et al .2022; Xu et al .2020] with increasing network depth. To deal with this challenge, we propose a new, general framework for iterative forwardbackward abstraction refinement that balances the analysis precision and speed for deeper networks. Rich specifications. Many desirable properties require reasoning about sets of nodes rather than a single node. For example, one might specify “no task from job A is scheduled before at least one task from job B is finished” for strategyproofness. Properties like this contain a large disjunction: we need to check the requirement for each task in job A. We propose an abstraction technique that can reason about multiple disjuncts simultaneously to speed up verification of such properties. As with robustness properties, many desirable properties for schedulers can be defined both globally and locally. We focus on the latter which is popular in the neural network verification literature and stronger than empirical evaluation on finite sets of inputs as done in the past for more complex scheduling policies [Kandasamy et al .2020]. Our framework can theoretically also handle global properties. We discuss the practical difficulties of it in Sec. 7 and leave it as future work. Sequential decision making. Schedulers perform sequential decisions to schedule tasks. Therefore, to thoroughly analyze the learned schedulers, it is not sufficient to only reason about singlestep Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:3 inputoutput properties considered by stateoftheart verifiers, as a malicious user can craft a job that affects the scheduler’s behavior downstream. Therefore, we consider multistep verification to reason about bounded traces produced by a sequence of scheduling actions from the scheduler. This adds an extra layer of complexity on top of the already challenging singlestep verification, as we need to reason about all the different states along different traces, which requires unrolling the system. To address this, we introduce a prooftransfer encoding of the system which only registers incremental changes in the network encoding along the traces. This significantly speeds up complete verification in the multistep setting. This work. We present the first approach for formally analyzing statebased and tracebased properties of GNNbased job schedulers. We build general algorithms for singlestep and multistep verification. Our main contributions are: •We present a new, generic iterative refinement framework for forwardbackward analysis of neural networks. Our framework can be instantiated with popular numerical domains such as Zonotope or DeepPoly to iteratively refine the analysis results. •We provide a novel, tunable node abstraction for a set of node embeddings produced by the GNN to speed up the verification of properties with multiple disjuncts. •We present an algorithm for multistep verification based on trace enumeration. To improve speed, we leverage proof transfer to reuse encodings for the parts of the GNN structures that do not change across time steps. •We provide an endtoend implementation of our approach in a framework called vegas (verification of GNNbased schedulers) and evaluate its effectiveness for checking desirable statebased and tracebased reachability properties for the stateoftheart GNNbased sched uler Decima [Mao et al .2019]. Our results show that analysis with vegas is significantly more precise and scalable than baselines based on existing stateoftheart verifiers. Using vegas we prove that Decima satisfies the strategyproofness property in many cases but not always. Thus adjustments in the training procedure are potentially needed to make Decima fully strategyproof. Our system and benchmarks are available at this repository. 2 OVERVIEW In this section, we first describe our verification workflow and then explain our key technical contributions using small intuitive examples. Formal details are in later sections. 2.1 Verification Workflow Our verification workflow shown in Fig. 1 has three components: (a) the system to verify; (b) a formal language for specifying properties; and (c) the verification engine vegas . GNNbased scheduling system. GNN arises as a natural solution for learningbased job schedulers on clusters because many clusters (e.g., Spark) encode jobs as directed acyclic graphs (DAGs), with each node representing a computational stage consisting of one or more tasks that can be run in parallel. Each node is associated with a feature vector (described in App. C) containing all the state information for that node, including the average task duration and the number of remaining tasks. There is an edge from stage v𝑖to stage v𝑗if the latter takes the outputs of the former as inputs. That is, v𝑗cannot be scheduled before v𝑖is completed. We call a node with no children a frontier node. The input to the GNNbased scheduler is a set of jobs to schedule. The output of the GNN is a score p𝑖for each frontier node v𝑖, representing the estimated reward if v𝑖is scheduled next. The node with the highest score is selected to be scheduled. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:4 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh Fig. 1. Overview of our verification workflow. It has three main components: (a) the system to verify; (b) a formal language for specifying properties; and (c) the verification engine vegas . As is typical in a GNN, the GNNbased job scheduler contains a message passing component which computes a latent representation (i.e., an embedding) 𝑒𝑖for each node 𝑣𝑖. We define message passing precisely in Sec. 3.1. The score p𝑖for a frontier node v𝑖is computed from a prediction network which takes 𝑒𝑖as input. The scheduling action (i.e., the node with the highest score) is reported to the environment, which schedules the reported node and produces a new state (with, for example, nodes removed). Specifications. We consider a wide range of specifications of the form 𝜙𝑖𝑛→𝜙𝑜𝑢𝑡. We assume 𝜙𝑖𝑛is a conjunction of linear constraints over the GNN input features, and consider post conditions 𝜙𝑜𝑢𝑡in both singlestep and multistep settings. Single step post conditions are logical constraints over linear inequalities on the outputs of the network. Multistep post conditions are defined in terms of unreachability of “bad” traces (i.e., sequences of scheduling decisions). Verifier. Our verification engine vegas has two main components, a singlestep engine and a multistep engine. Motivated by the unique challenges in this verification setting, we propose a forwardbackward abstraction refinement framework (Sec. 4) which goes beyond the forward propagationonly abstract interpretation, as well as a nodeabstraction scheme (Sec. 5) for handling disjunctions in the verification query. The multistep engine runs a trace enumeration procedure that repeatedly invokes the singlestep engine. We propose an efficient encoding of the unrolled system referred to as the prooftransfer encoding (Sec. 6) which significantly reduces the runtime. 2.2 Forwardbackward abstraction refinement As mentioned earlier, one of the distinctive features in GNN verification is the need to reason about very deep computational graphs resulting from the unrolling of the messagepassing procedure. Forward abstract interpretation techniques are less effective here as the imprecision can grow exponentially with increasing depth of the computation graph. We propose to refine the forward abstraction by backward refinement guided by the output constraints. This yields a general forward backward abstraction refinement loop (Sec. 4). While our ideas are driven to tackle challenges in GNN verification, we formalize and implement the proposed technique in a general manner so that it can be applied for neural networks with different architectures and activations. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:5 Running example. We illustrate the forwardbackward abstraction refinement on a pretrained fullyconnected feedforward neural network with Leaky ReLU activation functions ( 𝜎(𝑥)= max(𝛼𝑥,𝑥)) shown in Fig. 2. Here 𝛼is a hyperparameter of Leaky ReLU. For numerical simplicity, we assume 𝛼= 0.1 in this example. We use Leaky ReLU as an example since it is used in the stateoftheart GNNbased job scheduler Decima which we set out to verify. Note that while the running example uses a feedforward neural network for simplicity, in practice the architecture of a GNN is much more complex (e.g., contains residual connections). We discuss how to handle forwardbackward analysis in the GNN setting in Sec. 4.4. The network here consists of four layers: an input layer, two hidden layers, and an output layer with two neurons each. The outputs of a (noninput) layer are computed by applying an affine transformation to the last layer’s outputs followed by the activation function. The activation function is often not applied at the output layer (also in this example). The values on the edges represent the learned weights of the affine transformations. The values above or below the neurons represent the learned biases (translation values) of the affine transformation. For example, the top neuron in the first hidden layer, 𝑥4, can be computed as 𝜎(𝑥2), where𝑥2, the preactivation value of the neuron, is equal to 𝑥0+𝑥1. Specification. Let us assume a hypothetical job profile with two disconnected nodes. Suppose their feature vectors (1dimensional in this case) range from [0,1]. Our goal is to prove that the score for the second node ( 𝑥11) is always greater than the score for the first node ( 𝑥10), for any possible values of the two features in the range [0,1]. Forward abstract interpretation. A typical abstract interpretation on neural networks [Singh et al.2019a, 2018a, 2019b; Zhang et al .2018] involves propagating the input set (represented in predefined abstraction domain such as Zonotope or DeepPoly) forward layer by layer (via pre defined abstract transformers) to compute an overapproximation of the reachable output set. The specification holds if the overapproximated output set is disjoint from the bad states ( ¬𝜙𝑜𝑢𝑡). In this work, we use the DeepPoly domain [Singh et al .2019b] for forward abstract interpretation, though the refinement technique applies to any subpolyhedral abstract domain [Singh et al .2018b]. We show the intervals derived by the DeepPoly analysis in the bottom (blue) box in Fig. 2. The steps to deriving these bounds are omitted here (can be found in App. A) . As shown in the figure, the output bounds derived by DeepPoly are not precise enough to prove the property. Backward abstraction refinement. Normally, at this point, the abstractionbased analysis is incon clusive and we have to resort to searchbased methods (e.g. MILP or SMT solvers) which perform case analysis on the Leaky ReLUs and have an exponential runtime in the worst case. Instead, we observe that the forward abstract interpretation typically ignores the post condition when comput ing the overapproximation at each layer leaving room for refinement guided by the verification conditions and proving the property without invoking searchbased methods. In the case where a complete search is unavoidable, a refined overapproximation still helps to prune the search space. We illustrate a forwardbackward abstraction refinement on our example. We associate two abstract elements, a forward one 𝑎𝑖and a backward one 𝑎′ 𝑖from the same abstract domain (e.g., DeepPoly) with each neuron 𝑥𝑖. The analysis alternates between forward and backward passes, which respectively update the forward abstract elements and the backward abstract elements. We construct new forward transformers that consider both the forward and the backward abstract elements (Sec. 4), yielding more precise forward analysis. The backward elements are initialized to ⊤in the beginning, and therefore the first forward pass results in the same results as before. This is followed by a backward pass. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:6 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh Fig. 2. A toy example for forwardbackward abstraction refinement. The backward analysis starts with the “bad” output set ¬𝜙𝑜𝑢𝑡:=𝑥10≥𝑥11, which we use to refine 𝑎′ 10and𝑎′ 11, which are currently set to ⊤. We first compute the bounds of 𝑥10and𝑥11, conditioned on the existing bounds and ¬𝜙𝑜𝑢𝑡. This yields a tighter lower bound and upper bound for 𝑥10and𝑥11 respectively. These new bounds are then used to refine the underlying backward abstract element a’11:a’11=𝑇cond(a11,𝑥11∈[− 2,1]), where a11is the forward abstract element updated from the forward pass and 𝑇condis the conditional transformer from the domain. a’10is updated similarly. We now move to the last hidden layer. Again, we first compute sound intervals for neurons 𝑥8 and𝑥9with Linear Programming (LP). For instance, to compute an upper bound for 𝑥9, we can cast the following optimization problem: 𝑢9=max 𝑥8:11𝑥9,s.t.𝑥10=−𝑥8−𝑥9,𝑥11=𝑥8+2𝑥9+2 𝑥10∈[− 2,1],𝑥11∈[− 2,1],𝑥8∈[− 0.045,2] We obtain a tightened bound 𝑥9≤−0.275. Importantly, now the underlying backward abstract element𝑎′ 9is set to𝑇cond(𝑎9,[−0.3,−0.275])where the inputoutput relationship for the Leaky ReLU is linear (and can be captured exactly by domains like DeepPoly). The exactness significantly improves the analysis precision in the next forward pass. Note that in the most general form (and in our implementation), two LPs per neuron are called to tighten the bounds. While this incurs overhead, the process is highly parallelizable as neurons from the same layer can be processed independently. More importantly, we observe that this tractable overhead (we prove complexity in Sec. 4) usually pays off in practice on challenging benchmarks which would otherwise require extensive search by a complete procedure. After refining 𝑎′ 8and𝑎′ 9, we process 𝑥6and𝑥7, where𝑥8=𝜎(𝑥6)and𝑥9=𝜎(𝑥7). Due to the nonlinearity of the activation function, a precise encoding of Leaky ReLU results in a Mixed Integer Linear Program, which is in general challenging to solve. Therefore, we encode a sound linear relaxation [Ehlers 2017] of the activation function. Next, using the same procedure, we derive that 𝑥4∈[− 7.5,−1.5]. Note that this is disjoint from the interval derived during forward analysis. Intuitively, this means that for 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡to hold,𝑥4 must be less than or equal to 1.5, while for 𝜙𝑖𝑛to hold,𝑥4must be between 0.1 to 1. This implies that¬𝜙𝑜𝑢𝑡cannot hold for any input satisfying 𝜙𝑖𝑛, and the property is proved without the use of searchbased methods. In the case where the backward analysis does not prove the property by itself, we could perform forward analysis again by taking the refined backward abstract elements into consideration. This could result in a tighter overapproximation of the output set compared to the first forward analysis. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:7 Performing backward analysis again from this tighter overapproximation could in turn result in further refinement. Thus, we could alternate between forward and backward analysis to keep refining the abstractions until either the property is proved or some convergence condition is met. We define this forwardbackward analysis formally in Section 4. 2.3 Node abstraction with iterative refinement If the postcondition 𝜙𝑜𝑢𝑡contains multiple conjuncts, the bad output set ¬𝜙𝑜𝑢𝑡becomes disjunctive. This is a common occurrence in practice as the postcondition often specifies that a set of output neurons all satisfy a certain property 𝑃. For instance, the simple postcondition “node v1is always scheduled” can be formally specified as 𝜙𝑜𝑢𝑡:=Óp1>p𝑖, where p𝑖extends over the score of all frontier nodes other than p1. In this case, the bad output set ¬𝜙𝑜𝑢𝑡becomesÔp1≤p𝑖. A naïve approach would analyze each disjunct individually, which becomes expensive, especially when the number of disjuncts is large. However, we observe that the special structure of a GNN used in node prediction tasks allows us to reason about multiple nodes simultaneously. We illustrate this idea on the weaker postcondition “node v1has higher score than frontier nodes in job 2” (in Fig. 1 (a)), that is,Ó 𝑖∈{4,5,6}p1>p𝑖. As shown in Fig. 3, a vanilla approach would individually check for the unsatisfiability of the three formulas p1≤p4,p1≤p5, and p1≤p6. However, we observe that in GNNs for node prediction tasks [Wu et al .2020b], the prediction for a node v𝑖is often computed by applying the same prediction network to the embedding e𝑖. It is therefore natural to consider an abstraction 𝑒′at the embedding level which contains all values that𝑒4,𝑒5, and𝑒6can take. As illustrated in Fig. 3, suppose 𝑒4∈[− 2,2],𝑒5∈[− 1,3], and𝑒6∈[3,5]. An abstract embedding 𝑒′can then take values in [−2,5]. Ifp1>p′holds, or equivalently, p1≤p′ is unsatisfiable, then the original postcondition must hold. We state and prove this formally in Sec. 5. On the other hand, if we obtain a spurious counterexample, the analysis is inconclusive Fig. 3. A toy example for node abstraction.and we need to refine the abstraction. One way to refine the abstraction is by reducing the num ber of node embeddings considered in the ab stract embedding. In particular, we heuristically remove the embedding that reduces the volume of the abstraction the most, in this case 𝑒6, and try to reason about p1>p6andp1>p′individ ually. Now𝑒′is more constrained ( e′∈[− 2,3]), and the property is more likely to hold on e′. We can perform this refinement iteratively until either the property is proved or a real counter example is found. In the worst case, no node abstraction can be performed and we would have to examine each node individually. In prac tice, it often pays off to speculatively reason about multiple neurons simultaneously, espe cially when the number of disjuncts is large. 2.4 Beyond singlestep verification Building on top of our singlestep engine combining forwardbackward refinement and node abstrac tion, we present a procedure for verifying multistep properties in the form of trace (un)reachability (𝜙𝑖𝑛→unreach(𝑇)). As we shall see, this allows us to define meaningful specifications over the jobscheduling system. We illustrate the procedure on the example in Fig. 4. The postcondition we verify states that “ v5cannot be scheduled before v3”. Our procedure proves this by computing the set Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:8 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh of feasible traces from the initial state by repeatedly invoking the singlestep engine. For example, at step 0, we ask the single step engine to check 𝜙𝑖𝑛→(v1≥v4)and𝜙𝑖𝑛→(v1≤v4). If both can be violated (i.e., v1andv4can be scheduled), we proceed by computing the reachable traces from v1 andv4, respectively, and so forth. During this process, whenever we construct a reachable (partial) Fig. 4. A toy example for multistep verification.trace𝑡, we check whether it matches the prefix of any traces in 𝑇. If it does not (e.g., v1→v4→v3), we do not need to con tinue growing that trace as the property must hold for any trace with this prefix. If the partial trace matches exactly with a trace in𝑇(e.g., v4→v1→v2→v5), then the postcondition is violated. Finally, if it is inconclusive, then we need to continue growing the trace. Fig. 5. Example of a naïve encoding of two steps.When computing the possible next ac tions from a partial trace, a complete en coding of the system requires unrolling, i.e., an encoding of the system over multi ple steps. For example, as shown in Fig. 5, suppose in the first step node v4is sched uled; then, in the second step, the graph is updated with one removed node. A naïve encoding would reencode the entire neu ral network for step 2 and invoke the singlestep engine on this widened neu ral network to check whether p1@2≥p5@2andp5@2≥p1@2are respectively feasible under the additional constraint that p4@1≥p1@1. A naïve encoding that introduces a fresh encoding of the network for each time step quickly becomes intractable. On the other hand, an incomplete encoding that ignores the previous time steps and only encodes the current step can find spurious counterexamples. Fig. 6. Example of the prooftransfer encoding of the same two steps as in Fig. 5.In order to obtain an efficient encod ing that still tracks all constraints across time steps, we propose building a meta network that captures the incremental changes in the network structure across time steps and reusing the parts that do not change. We refer to this graph as a proof transfer network . Fig. 6 shows the construc tion of the prooftransfer network for the same two steps as in Fig. 5. In particular, the removal of v4only affects nodes in the bottom job (i.e., v5) and the message pass ing steps for the other three nodes remain unchanged. This means that at each time step, we can reuse the GNN encoding for all but one job DAG (a disconnected component of the input graph), which results in significantly slower growth in the size of the encoding as the time step increases. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:9 3 PRELIMINARIES 3.1 Graphs and Graph Neural Networks Definition 3.1 (Graph). We define a graph 𝐺with𝑁nodes as a tuple(𝐴,𝑋)where𝐴is an𝑁×𝑁 adjacency matrix and 𝑋is the set of node attributes. There is an edge from node v𝑖tov𝑗if𝐴𝑖𝑗=1. The neighborhood of a node v𝑖is defined as 𝑁(v𝑖)={v𝑗|𝐴𝑖𝑗=1}. The node attributes X∈R𝑁×𝑑 make up a node feature matrix with 𝑥𝑖∈R𝑑representing the feature vector of a node 𝑣𝑖. The input to a GNN is a graph 𝐺=(𝐴,𝑋). In the case of job scheduling, an input graph can be disjoint, which is useful for modeling collections of jobs. Graph Convolutional Networks. Graph Neural Networks (GNNs) [Duvenaud et al .2015; Kipf and Welling 2017; Niepert et al .2016] are a class of deep neural networks for supervised learning on graphs. They have been successfully employed for node, edge, and graph classification in a variety of realworld applications including recommender systems [Ying et al .2018], protein prediction [Fout et al .2017], and malware detection [Wang et al .2019]. We focus on an important and widely used subset of GNNs called spatial graph convolutional networks (GCN) [Wu et al .2020b]. Given an input graph, a GCN generates an embedding e𝑖∈R𝑑for each node v𝑖by aggregating its own features x𝑖and all nodes reachable from v𝑖through a sequence of message passing steps. In each message passing step, a node vwhose neighbors have aggregated messages from all of their children computes its own embedding as: e𝑖=𝑔h∑︁ 𝑣𝑗∈𝑁(𝑣𝑖)𝑓(e𝑗)i +x𝑖 where𝑓(·)and𝑔(·)are feedforward neural networks.1In practice, message passing is performed for a fixed number of rounds: in the first round, messagepassing steps are performed on a pre selected initial set of nodes, and in a subsequent round, messagepassing steps are performed on neighbors of the nodes that participated in the previous round. A distinct characteristic of GCNs compared to previous GNN architectures is that there are no cyclic mutual dependencies in message passing, i.e., a GCN can be unrolled. However, unlike the neural networks targeted by existing verifiers, the networks obtained from unrolling are much deeper (100+ layers in practice). Furthermore, the unrolled networks are not simple feedforward networks but also contain complex residual connections. For simplicity, we assume that the unrolling results in a feedforward network for the rest of this paper unless specified otherwise. Our approach for handling residual connections is described in Sec. 4.4. Node regression/classification with GCN. The embeddings e𝑖after message passing can subse quently be used for different graph analytics tasks. We focus on node prediction tasks where the goal is to predict on nodes indexed by a set Θ. Note that Θmight not include all the nodes in the input graph, as for a scheduler some of the nodes may not be eligible for scheduling at a given time step due to sequential dependencies. The prediction p𝑖for a node v𝑖is computed by feeding the node embedding e𝑖into a feedforward network h. We allow the flexibility to augment the input to hwith an additional nonlinear embedding z, computed from the node features and embeddings via a summary network s:z=s({(x𝑖,e𝑖),v𝑖∈𝐺}). In short, p𝑖:=h(e𝑖,s({(x𝑖,e𝑖),v𝑖∈𝐺})). 3.2 Verification of GNNs We consider verifying a specification 𝜙over a GNN 𝐹, where𝜙has the form 𝜙𝑖𝑛→𝜙𝑜𝑢𝑡. The precondition 𝜙𝑖𝑛defines a set of inputs to 𝐹, and the specification states that for each input 1This definition of message passing covers common forms of GCNs as seen in [Defferrard et al .2016; Khalil et al .2017; Kipf and Welling 2017]. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:10 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh point satisfying 𝜙𝑖𝑛, the postcondition 𝜙𝑜𝑢𝑡must hold. In this work, we limit the form of 𝜙𝑖𝑛to describing a set of inputs where the graph structure is constant and the node features are defined by a conjunction of linear constraints. Formally, given an input graph 𝐺=(𝐴,𝑋)and a GNN𝐹(m,h,s) with messagepassing component mparameterized by 𝑔and𝑓, prediction network h, and summary network s, the concrete behavior of 𝐹can be expressed with the following set of constraints ( x𝑖’s, e𝑖’s,z, and p𝑖’s are interpreted as realvalued variables): 𝑀:=  𝜙𝑖𝑛(x1,..., x𝑁) e1,..., e𝑁=m({x𝑖,𝑖∈[1,𝑁]},𝐴) z=s({x𝑖,e𝑖|v𝑖∈𝐺})Ó 𝑖∈Θp𝑖=h(e𝑖,z)(1):=𝑀𝑖𝑛 We use𝑀𝑖𝑛to denote all outputs before the prediction network his applied. This will be used when we define the node abstraction scheme in Sec. 5. The verification problem is to check whether𝑀→𝜙𝑜𝑢𝑡is valid, or equivalently, whether 𝑀∧¬𝜙𝑜𝑢𝑡is unsatisfiable. Under the latter interpretation, the verification problem is to show that under the constraints 𝑀, the “bad”states described by¬𝜙𝑜𝑢𝑡cannot be reached. Singlestep post conditions. We support singlestep post conditions of the form 𝜙𝑠𝑖𝑚𝑝 :=Ô 𝑗𝜓𝑗(𝑃), where𝜓𝑗(𝑃)is an atomic linear constraint over the output variables, i.e., (Í ℓ∈[1,𝑁]𝑎ℓ·pℓ)⊲⊳𝑏, where 𝑎ℓand𝑏are constants and ⊲⊳∈{=,<,≤}. We refer to this as a simple post condition because the “bad” states can be described as a conjunction of linear constraints, and checking 𝜙𝑠𝑖𝑚𝑝 amounts to showing that 𝑀∧Ó 𝑗¬𝜓𝑗(𝑃). Moreover, we support richer post conditions that state that multiple simple post conditions must hold simultaneously: 𝜙𝑐𝑜𝑚𝑝𝑙𝑒𝑥 =Ó 𝑖𝜙𝑖 𝑠𝑖𝑚𝑝. We describe our novel abstraction to efficiently handle such post conditions in Sec. 5. 3.3 GNNbased job scheduling As a proof of concept, we focus on verifying the stateoftheart GNNbased job scheduler, Dec ima [Mao et al .2019]. Formally, the input to the scheduler is a graph 𝐺with𝐾disconnected components 𝐺1,...,𝐺𝐾, representing the current state of the cluster. Each disconnected component 𝐺𝑘is a job DAG. The output of Decima is a single score p𝑖for each frontier node v𝑖. We use front(𝐺)to denote the frontier nodes. The node to schedule next is arg maxv𝑖∈front(𝐺)p𝑖. Our goal is to reason about Decima not only in the singlestep setting, but also in the multistep setting. which we describe next. Multistep setting. We consider a setting with an initial state 𝐺=(𝐴,𝑋), a GNNbased scheduler 𝐹, and a cluster environment T(𝐺,v)↦→𝐺′, which takes the current state 𝐺and a node v∈front(𝐺), and outputs a new state 𝐺′representing the new state after vis scheduled. We restrict Tto perform two types of graph updates: 1) removal/addition of nodes; and 2) affine transformations of features of existing nodes: 𝑥′ 𝑖↦→𝐴𝒙𝒊+𝒃. This precisely captures the actual cluster environment for a wide range of initial states. For simplicity, we only consider static job profiles with no new incoming jobs (i.e., only node removal and no node addition). Given 𝐺,𝐹, andT, we refer to a finite sequence of scheduling decisions 𝐹(𝐺),𝐹(T(𝐺,𝐹(𝐺))),...as atrace . Multistep post conditions. We consider multistep post conditions defined in terms of trace reachability. The post condition 𝜙𝑜𝑢𝑡is of the form unreach(𝑇)where𝑇is a finite set of finite traces of possibly different lengths. The specification states that none of the traces in 𝑇are feasible if the initial state satisfies 𝜙𝑖𝑛. As we will see in Sec. 7, this form of specifications allows us to specify meaningful multistep properties for the jobscheduler. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:11 4 FORWARDBACKWARD ANALYSIS Algorithm 1 ForwardBackward Analysis. 1:Input: neural network 𝑓and specification 𝜙𝑖𝑛→𝜙𝑜𝑢𝑡 2:Output: HOLD/VIOLATED/UNKNOWN 3:function forwardBackwardAnalysis (𝜙) 4: a(1),..., a(L)↦→⊤,a’(1),..., a’(L)↦→⊤ 5: while¬stopCondition()do 6: a(1),..., a(L)↦→forward(𝑓,{𝑎ℓ},{𝑎′ℓ},𝜙𝑖𝑛) 7: if𝑇cond(𝑎𝐿,¬𝜙𝑜𝑢𝑡)=⊥then 8: return HOLD 9:𝑏𝑜𝑢𝑛𝑑𝑠↦→[] 10: forℓ=𝐾,𝐾−1,..., 2do 11:𝑏𝑜𝑢𝑛𝑑𝑠[ℓ]↦→ computeBounds(𝑓,ℓ,{𝑎ℓ},{𝑎′ℓ},𝜙𝑜𝑢𝑡) 12: a’ℓ↦→𝑇cond(aℓ,(𝑥(ℓ)∈𝑏𝑜𝑢𝑛𝑑𝑠[ℓ]))⊓ a’ℓ 13: ifa’ℓ=⊥then 14: return HOLD 15: return checkSat(𝜙𝑖𝑛∧( ℓ∈[1,𝐿]Ó(𝜑(ℓ) nonlinear∧𝜑(ℓ) linear))∧¬𝜙𝑜𝑢𝑡)In this section, we describe our forwardbackward abstraction re finement framework in more for mal terms. We consider an 𝐿 layer feedforward neural network 𝑓:R𝑛0→R𝑛𝐿, where𝑛0and𝑛𝐿 are the number of input and out put neurons respectively. For an input𝑥, we use𝑓ℓ(𝑥)and𝑓ℓ:𝐿(𝑥) respectively to denote the network output at an intermediate layer ℓ and all layers between 𝑙and𝐿. We consider the affine and the non linear activation layers as separate. As illustrated in Sec. 2, our key in sight is to iteratively refine the ab straction obtained from a forward abstract interpretation with an LPbased backward pass and viceversa. Alg. 1 shows the pseudocode for our framework. Next we describe each step in greater detail and prove soundness properties. 4.1 Forward abstract interpretation The forward analysis in our framework is generic as it can be instantiated with any subpolyhedral domain including the popular domains for neural network verification such as Boxes [Wang et al . 2018b], Zonotope [Gehr et al .2018; Singh et al .2018a], DeepPoly [Singh et al .2019b], or Polyhedra [Singh et al .2017]. We use A𝑛to denote an abstract element overapproximating the concrete values of𝑛numerical variables. We require that the abstract domain is equipped with the following components: •A concretization function 𝛾𝑛:A𝑛→P( R𝑛)that computes the set of concrete points from R𝑛represented by an abstract element 𝑎∈A𝑛. •A bottom element⊥∈A𝑛such that𝛾𝑛(⊥)=∅. •A sound abstraction function 𝛼𝑛:P(R𝑛)→A𝑛that computes an abstract element over approximating a region 𝜙𝑖𝑛∈P(R𝑛)provided as input to the neural network. We have 𝜙𝑖𝑛⊆𝛾𝑛(𝛼𝑛(𝜙𝑖𝑛))for all𝜙𝑖𝑛∈ P(R𝑛). Note that we do not require 𝛼𝑛to compute the smallest abstraction for 𝜙𝑖𝑛, however, the input regions considered in our experiments can be exactly abstracted with common domains such as DeepPoly and Zonotope. •A bounding box function 𝜄𝑛:A𝑛→R𝑛×R𝑛, where𝛾𝑛(𝑎)⊆Î 𝑖[𝑐𝑖,𝑑𝑖]for(𝒄,𝒅)=𝜄𝑛(𝑎)for all𝑎∈A𝑛. •A sound conditional transformer 𝑇cond(𝑎,𝐶)that for each 𝑎∈A𝑛and set of linear constraints 𝐶defined over 𝑛real variables satisfies 𝛾𝑛(𝑇cond(𝑎,𝐶))⊆𝛾𝑛(𝑎), i.e., the conditional output does not contain more points than the input 𝑎. •A sound abstract transformer 𝑇# of:A𝑚→A𝑛for each layerwise operation 𝑜:R𝑚→R𝑛(e.g., affine, nonlinear activations, etc.) in the neural network. •A sound meet transformer ⊓for each𝑎,𝑎′∈A𝑛satisfying𝛾𝑛(𝑎⊓𝑎′)⊆𝛾𝑛(𝑎)and𝛾𝑛(𝑎⊓𝑎′)⊆ 𝛾𝑛(𝑎′). Our framework associates an abstract element 𝑎ℓfrom the forward pass and an element 𝑎′ℓfrom the backward pass with each layer ℓ. Both elements are constructed such that they individually Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:12 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh overapproximate the set of concrete values at layer ℓwith respect to 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡at each iteration of the while loop in Alg. 1. Initially, all elements are ⊤. Constructing forward transformers. The forward pass shown at Line 6 in Alg. 1 first constructs an abstraction of the input region 𝑎1=𝛼𝑛0(𝜙𝑖𝑛). We propagate 𝑎1through the different layers of the network via a novel construction that creates new higher order abstract transformers 𝑇# ofbfrom existing𝑇# offor each operation 𝑜. For a layer ℓ, the construction of 𝑇# ofbtakes𝑇# ofand the abstract elements𝑎ℓ−1,𝑎′ℓ−1at layerℓ−1as inputs. Its output is the new forward abstract element at layer ℓ: 𝑎ℓ=𝑇# ofb(𝑇# of,𝑎ℓ−1,𝑎′ℓ−1)=𝑇# of(𝑎ℓ−1)⊓𝑇# of(𝑎′ℓ−1) (2) We next prove the soundness of our construction. Theorem 4.1. 𝑇# ofbis a sound abstract transformer, that is, given an input that includes all concrete values at layer 𝑙−1with respect to 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡, the transformer’s output includes all concrete values possible at layer 𝑙with respect to 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡. Proof. Let𝑆ℓ−1and𝑆ℓrespectively be the set of concrete values at layer ℓ−1andℓwith respect to 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡. For sound abstractions 𝑎ℓ−1,𝑎′ℓ−1, we have𝑆ℓ−1⊆𝛾𝑛(𝑎ℓ−1)and𝑆ℓ−1⊆𝛾𝑛(𝑎′ℓ−1). Since 𝑇# ofis sound,𝑆ℓ⊆𝛾𝑛(𝑇# of(𝑎ℓ−1))and𝑆ℓ⊆𝛾𝑛(𝑇# of(𝑎′ℓ−1)). Thus𝑆ℓ⊆𝛾𝑛(𝑇# of(𝑎ℓ−1))∩𝛾𝑛(𝑇# of(𝑎′ℓ−1))⊆ 𝛾𝑛(𝑇# of(𝑎ℓ−1)⊓𝑇# of(𝑎′ℓ−1))=𝛾𝑛(𝑇# ofb(𝑇# of,𝑎ℓ−1,𝑎′ℓ−1)) □ Corollary 4.2. The output of 𝑇# ofbis included in the output of 𝑇# offor inputs soundly abstracting the concrete values with respect to 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡. That is, the output of 𝑇# ofbis at least as precise as 𝑇# of. (2)invokes the original transformer 𝑇# oftwice along with the ⊓transformer. For most popular domains,⊓is asymptotically cheaper than forward , therefore the asymptotic cost of 𝑇# ofbis same as𝑇# of. Since𝑇# ofbobtained for each layer is sound, the forward propagation produces 𝛾𝑛𝐿(𝑎𝐿)⊇ 𝑓(𝜙𝑖𝑛)⊇𝑓(𝜙𝑖𝑛)∧¬𝜙𝑜𝑢𝑡at the output layer. In this section, we assume that the set of bad outputs ¬𝜙𝑜𝑢𝑡can be described as a conjunction of linear constraints. In the next section, we will consider a more general class of ¬𝜙𝑜𝑢𝑡containing disjunctions also. For our restriction here, we can compute 𝑇cond(𝑎𝐿,¬𝜙𝑜𝑢𝑡), and if it is equal to ⊥, we have proved that the specification must hold since 𝛾𝑛𝐿(𝑇cond(𝑎𝐿,¬𝜙𝑜𝑢𝑡))⊇𝑓(𝜙𝑖𝑛)∧¬𝜙𝑜𝑢𝑡(Line 7 in Alg. 1). 4.2 Backward abstract interpretation The forward propagation ignores the post condition 𝜙𝑜𝑢𝑡during the construction of the abstract element𝑎𝐿. A refinement of the abstraction can be obtained by taking 𝜙𝑜𝑢𝑡into consideration. The backward pass shown at Line 10 in Alg. 1 is designed to accomplish this refinement. The backward pass first updates the backward element at the output layer 𝑎′ 𝐿=𝑇cond(𝑎𝐿,¬𝜙𝑜𝑢𝑡)⊓ 𝑎′ 𝐿. It can be seen that this update is sound. The backward pass at a nonoutput layer 𝑘performs two steps. First, we use Linear Programming (LP) to compute refined lower and upper bounds 𝒄ℓ,𝒅ℓ, taking into consideration both 𝜙𝑜𝑢𝑡and a linear overapproximation of the network behavior from layerℓto the output layer with respect to 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡. We then refine the backward abstract element using the domain conditional transformer 𝑇condand the linear constraints 𝑐ℓ 𝑖≤𝑥ℓ 𝑖≤𝑑ℓ 𝑖for each neuron 𝑥ℓ 𝑖in layerℓ>1, i.e., we compute 𝑎′ℓ=𝑇cond(𝑎ℓ,(Û 𝑖𝑐𝑖≤𝑥ℓ 𝑖≤𝑑𝑖))⊓𝑎′ℓ. (3) We now describe our linear encoding of the network for computing the refined interval bounds 𝒄ℓ,𝒅ℓof the neurons in layer ℓ. We start with an empty set of constraints and iteratively collect linear Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:13 constraints𝜑𝑘(𝒙𝒌−1,𝒙𝒌)overapproximating the network behavior with respect to 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡for each layer𝑘>ℓ. If𝑘is an affine layer, then we add constraints of the form 𝑥𝑘=𝐴𝑘−1𝒙𝑘−1+𝑏𝑘−1, where𝐴𝑘−1,𝑏𝑘−1∈Rare the learned weights and biases respectively of the affine layer. If 𝑘is an activation layer, then we add the constraints from a linear approximation [Singh et al .2019b] of the layerwise activation 𝒙𝑘=𝜎(𝒙𝑘−1). The linear approximation can be obtained directly using the corresponding domain transformer 𝑇# ofor a more precise relaxation based on the constraints from the forward abstract element. If 𝑘is the output layer, then we add the constraint ¬𝜙𝑜𝑢𝑡to our encoding. For each 𝑘, we also add the constraints 𝐶𝑘obtained from the bounding box 𝜄𝑛of the abstract element 𝑎𝑘at layer𝑘. Let𝜑ℓdenote the conjunction of constraints collected above. To obtain the refined lower and upper bounds for layer ℓwith𝑚neurons, we need to solve the following two LPs for each neuron 𝑥ℓ 𝑖: 𝑐ℓ 𝑖=min 𝒙ℓ,...,𝒙𝑳 s.t.𝜑ℓ𝑥ℓ 𝑖,𝑑ℓ 𝑖=max 𝒙ℓ,...,𝒙𝑳 s.t.𝜑ℓ𝑥ℓ 𝑖 (4) While we refine all neurons in the backward step to gain maximum precision, it is possible to tune the cost and the precision of the backward pass by selectively refining only a subset of the neurons. Note that we do not refine the backward element at the input layer, and it is always ⊤ which preserves soundness. Theorem 4.3. For each layer ℓ, the refined bounds 𝒄ℓ,𝒅ℓcomputed by the LP overapproximate the network output with respect to 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡. Proof. We show that the constraints in the LP overapproximate the network behavior from layer ℓto the output which guarantees the soundness of the refined bounds. The proof is by induction. For the base case, the LP constraints at the output layer satisfy ¬𝜙𝑜𝑢𝑡∧𝐶𝐿⊇𝑓(𝜙𝑖𝑛)∧¬𝜙𝑜𝑢𝑡. For the inductive step, suppose 𝑘∈[ℓ+𝑢+1,𝐿]Ó(𝜑𝑘(𝒙𝒌−1,𝒙𝒌)∧𝐶𝑘)⊇𝑓ℓ+𝑢+1:𝐿(𝜙𝑖𝑛)∧¬𝜙𝑜𝑢𝑡holds with𝑢≥0. For the layerℓ+𝑢, the LP will add the conjunction satisfying 𝜑𝑘(𝒙ℓ+𝒖−1,𝒙(ℓ+𝒖))∧𝐶ℓ+𝑢⊇𝑓ℓ+𝑢:ℓ+𝑢+1(𝜙𝑖𝑛)∧ ¬𝜙𝑜𝑢𝑡(since both the box constraints from 𝐶ℓ+𝑢and the constraints from 𝜑𝑘(𝒙(ℓ+𝒖−1),𝒙(ℓ+𝒖)) overapproximate 𝑓ℓ+𝑢:ℓ+𝑢+1(𝜙𝑖𝑛)). Therefore, 𝑘∈[ℓ+𝑢,𝐿]Ó(𝜑𝑘(𝒙𝒌−1,𝒙𝒌)∧𝐶𝑘)⊇𝑓ℓ+𝑢:𝐿(𝜙𝑖𝑛)∧¬𝜙𝑜𝑢𝑡and the induction holds. □ Since the bounds computed by the LP are sound, the update (3)is sound at each intermediate layerℓ. Therefore the backward pass computes a sound approximation at each iteration of the while loop of Alg. 1. Due to the soundness of the backward pass, if we find that a backward element at a layerℓ>1became⊥after applying (3), then we can soundly return HOLD (Line 13 of Alg. 1). Theorem 4.4. For a layerℓ, let𝑎′ℓand𝑎′ℓ newbe the backward abstract elements before and after the refinement respectively. Then, 𝛾𝑛(𝑎′ℓ new)⊆𝛾𝑛(𝑎′ℓ)holds, i.e., the backward pass does not make the backward abstraction less precise. Proof. Follows from the definition of 𝑇condand⊓. □ Corollary 4.5. Let𝑞be the total number of neurons in the network 𝑓, then the complexity of computing (4)for all neurons in layers 1<𝑘≤𝐾is𝑂(𝑞·𝐿𝑃(𝑝,𝑞))where𝐿𝑃(𝑝,𝑞)is the cost of solving an LP with 𝑝constraints defined over 𝑞variables. 4.3 Iterative forwardbackward refinement We perform an iterative refinement procedure in Alg. 1 by repeatedly performing forward and backward analysis till a stopping criteria is met. Each forwardbackward pass results in elements Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:14 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh at least as precise as in previous iterations due to Theorem 4.4 and (2). For arbitrary choices of abstract elements and transformers allowed by our framework, the forwardbackward analysis is not guaranteed to converge to a fixed point: another iteration of the while loop does not refine the analysis results. In practice, for piecewiselinear activations like Leaky ReLU used in Decima, we terminate the analysis when a refinement round does not fix the phase of any activations. This is usually achieved within 6 iterations. If, after the forwardbackward analysis has finished, no abstract element has been refined to ⊥, we resort to a nonlinear encoding of the network outputs ℓ∈[1,𝐿]Ó𝜑ℓ nonlinearcombined with linear constraints ℓ∈[1,𝐿]Ó𝜑ℓ linearfrom our final abstract elements. For example, we use a MILP encoding for piecewise linear activations such as Leaky ReLU used in Decima which results in complete verification: the property is satisfied if and only if the set of constraints 𝜙𝑖𝑛∧( ℓ∈[1,𝐿]Ó(𝜑ℓ nonlinear∧𝜑ℓ linear))∧¬𝜙𝑜𝑢𝑡is unsatisfiable. The nonlinear solver benefits in speed from a reduction in the search area/branches due to the precise constraints added from our refined abstraction. Theorem 4.6. Alg. 1 is sound, i.e., 𝜙𝑖𝑛∧¬𝜙𝑜𝑢𝑡is unsatisfiable when the algorithm returns HOLD . Proof. The individual steps in Alg. 1 are sound. □ 4.4 Handling GNN architectures. GNNs often contain residual connections due to message passing. In those cases, we still need to make sure that when performing abstraction refinement for a certain layer 𝑘, we have already refined all subsequent layers that take 𝑘’s outputs as inputs. To obtain this refinement order, we first construct a DAG from the neural network architecture where each node represents a layer and an edge exists from layer 𝑘to layerℓif the output of the former feeds into the latter. Drawing inspirations from dataflow analysis [Aho et al .2007], the backward abstraction refinement is conducted in postorder. 5 NODE ABSTRACTION FOR GNN VERIFICATION. Alg. 1 is our core algorithm for verifying a single step property 𝜙𝑖𝑛→𝜙𝑜𝑢𝑡where¬𝜙𝑜𝑢𝑡is of the formÓΣ𝑎𝑖·p𝑖⊲⊳𝑐𝑖(i.e., a conjunction of linear constraints over the output layer) for a feedforward neural network. However, only allowing this form of bad outputs is restrictive as in practice the bad outputs specified in many properties (e.g., robustness, strategyproofness) are disjunctive sets. In particular, the output property often specifies that a subset of output neurons Θall satisfy certain simple postcondition 𝜙, i.e., 𝜙𝑜𝑢𝑡:=Û p𝑖∈Θ𝜙(p𝑖) (5) Existing works often handle this type of post condition by considering each disjunct individually: we can use Alg. 1 (or any procedure that can handle simple postconditions) to check whether 𝜙𝑖𝑛→𝜙(p𝑖)holds for each output variable p𝑖. The original property holds if the solver returns HOLD every time, and is violated if the solver returns VIOLATED for one of the disjuncts. However, this strategy could be inefficient when the number of disjuncts is large. Instead, in this section, we describe a general procedure to efficiently handle post conditions of this form tailored for node prediction/classification tasks in GNNs. We believe our contributions for verification presented here can be adapted to handle GNN application domains other than jobscheduling such as recommender systems and malware detection. Our key insight is that each score 𝑠𝑖is computed by applying the same transformation to the node embedding e𝑖,p𝒊:=h(e𝑖,s({x𝑖,e𝑖|v𝑖∈𝐺})). Therefore, to simultaneously reason about the GNN’s output for a group of nodes {v𝑖|𝑖∈Θ}, it is natural to consider an abstraction that Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:15 treats{e𝑖,𝑖∈Θ}as an equivalence class. Given a set of constraints 𝑀as defined in (1)that exactly captures the concrete behaviors of the GNN with respect to 𝜙𝑖𝑛, we can construct an abstraction e𝑀by mapping each constraint “ p𝑖=h(e𝑖,z)” to “ p′=h(e′,z)∧𝐼(e′)”, where e′andp′are free realvalued variables and 𝐼is an invariant which we will describe next: e𝑀:=( 𝑀𝑖𝑛(defined in (1)) p′=h(e′,z)∧𝐼(e′) Definition 5.1 (Soundness). We say e𝑀is asound abstraction of 𝑀with respect to a simple output property𝜙, if e𝑀→𝜙(p′)=⇒𝑀→Û 𝑖∈Θ𝜙(p𝑖). By definition, given a sound abstraction, we can prove the original property by proving a simple specification on e𝑀. Lemma 5.2. if𝑀𝑖𝑛→𝐼(e𝑖)for each𝑖∈Θ, thene𝑀instantiated with 𝐼is sound. Proof. Suppose e𝑀→𝜙(p′). It follows that∀e,𝑀𝑖𝑛∧𝐼(e)→𝜙(h(e,z)). For each𝑖∈Θ, we can instantiate ewith e𝑖and get𝑀𝑖𝑛∧𝐼(e𝑖)→𝜙(h(e𝑖,z)). Since𝑀𝑖𝑛→𝐼(e𝑖), we have𝑀𝑖𝑛→ 𝜙(h(e𝑖,z)). This is equivalent to 𝑀𝑖𝑛∧(p𝑖=h(e𝑖,z))→𝜙(p𝑖). Notice that 𝑀𝑖𝑛∧(p𝑖=h(e𝑖,z))is a subset of constraints in 𝑀. Therefore, 𝑀→𝜙(p𝑖). □ Algorithm 2 Node abstraction with iterative refinement. 1:Input: a message passing component 𝑚, a summary component sand a prediction component h, and a spec ification𝜙:𝜙𝑖𝑛→Ó 𝑖∈Θ𝜙(p𝑖) 2:Output: HOLD/VIOLATED/UNKNOWN 3:function checkWithNodeAbtraction (𝑚,s,h,𝜙) 4:𝐴,𝐶↦→Θ,∅ 5: while𝐶≠Θdo 6:𝑎𝑒1,...,𝑎𝑒𝑁↦→forward(𝑀,𝜙𝑖𝑛) 7:𝐼↦→getConvexRelaxation (𝑎𝑒1,...,𝑎𝑒𝑁) 8: e𝑀↦→createAbstraction (𝜙𝑖𝑛,𝑚,s,h,𝐼) 9:𝑟↦→forwardBackwardAnalysis (e𝑀,𝜙(p′)) 10: if𝑟=HOLD then 11:𝐶↦→𝐶∪𝐴 12:𝐴↦→∅ 13: else 14:𝑎𝑒𝑘↦→pickNodeFromAbstraction (𝐴) 15:𝐴↦→𝐴\{𝑘} 16:𝑡↦→forwardBackwardAnalysis (𝐹,𝜙𝑖𝑛→𝜙(p𝑘)) 17: if𝑡=HOLD then 18: 𝐶↦→𝐶∪{𝑘} 19: else 20: return t 21: return HOLDNode abstractions. While the choice of𝐼is flexible as long as it yields a sound abstraction, there is a tradeoff controlled by 𝐼between the difficulty of proving e𝑀→𝜙(p′)and the like lihood that this property holds. For example, the weakest invariant is ⊤, which leaves e′unconstrained, mak ing it less likely that 𝜙(𝑝′)holds. On the other hand, the strongest invari ant isÔ 𝑖∈Θe′=e𝑖, which just moves the disjunction in the output property to be over e′and does not reduce the computational complexity. A general recipe for constructing 𝐼relies on the forward abstract inter pretation. After performing the for ward analysis on GNN 𝐹with pre condition𝜙𝑖𝑛using the abstract trans formers𝑇# 𝑔, for each e𝑖, we can obtain a (typically) convex region 𝜓𝑖from the abstract element 𝑎𝑒𝑖such that𝜓𝑖 overapproximates the values 𝑒𝑖can take under the precondition 𝜙𝑖𝑛, or in other words, 𝑀𝑖𝑛→𝜓𝑖(𝑒𝑖). Next, we can compute a convex relaxation of the union of the convex regions and use that as our invariant 𝐼, i.e.,𝐼=conv(Ð 𝑖∈Θ𝜓𝑖). In practice, we take 𝜓𝑖to be the tightest intervals of e𝑖, and let𝐼be the join of those intervals. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:16 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh Theorem 5.3. If𝑀𝑖𝑛→𝜓𝑖(e𝑖)for each𝑖∈Θ, then the abstraction e𝑀instantiated by 𝐼= conv(Ð 𝑖∈Θ𝜓𝑖)is sound. Proof. For any e𝑖where𝑖∈Θ, by the definition of convex approximation 𝜓𝑖(e𝑖) →𝐼(e𝑖). Therefore𝑀𝑖𝑛→𝐼(e𝑖)and by Lemma 5.2 the statement holds. □ Note that e𝑀can be viewed as the concrete semantics of a GNN e𝐹with an augmented input e′ and one output p′. Under this view, checking e𝑀→𝜙(p′)is equivalent to checking the specification 𝜙𝑖𝑛(x1,..., x𝑁)∧𝐼(e′)→𝜙(p′)one𝐹. Iterative refinement of node abstraction. Ife𝑀→𝜙(p′)is proved, then the original property also holds. Otherwise we cannot conclude that the original property is violated. In that case, we can obtain a refinement of the overapproximation by considering fewer nodes in the abstraction. This yields an iterative refinement procedure as described in Alg. 2. We maintain two sets of indices: 𝐴is the set of node indices treated as equivalent in the abstraction and a node index 𝑖∈𝐶if 𝜙𝑖𝑛→𝜙(p𝑖)has been proved. Given a specification of the form described in Eq. (5), we start by creating an abstract network 𝐹′that treats all nodes in the disjuncts as equivalent. If the simple property𝜙𝑖𝑛→𝜙p′can be proved on F’, then we add all node indices in the equivalence class 𝐶. If we fail to prove the property, we refine the abstraction by considering fewer nodes in the abstraction. In particular, we heuristically pick one node (Line 13) to remove from 𝐴. In practice, we pick the node whose removal results in the largest decrease in the volume of the convex relaxation 𝐼. In the next iteration, we obtain a different abstract network 𝐹′that abstracts over fewer nodes. Theorem 5.4. Ife𝑀is a sound abstraction of 𝑀andforwardBackwardAnalysis is sound and complete, then Alg. 2 is sound and complete. Proof. If the specification can be violated, then 𝑟at Line 9 must always equal VIOLATED and each iteration picks and checks a different conjunct (Lines 1416). Since forwardBackwardAnalysis is sound and complete, it must return VIOLATED for one conjunct. Now suppose the specification holds. Since the relaxation is sound and forwardBackwardAnalysis is sound and complete, 𝐶 must only contain indices corresponding to conjuncts that hold. Moreover, each iteration must increase the size of 𝐶(Lines 11 and 18). Since Θis finite, the algorithm will return HOLD in finite number of steps. □ 6 REASONING OVER TRACES So far we have introduced a verification engine in Alg. 2 for singlestep properties with precondition 𝜙𝑖𝑛that can be expressed as a conjunction of linear constraints on the input node features and post condition 𝜙𝑜𝑢𝑡of the form described in (5). Building upon this engine, we now develop an analysis to reason about the system behavior across multiple time steps. In this setting, we want to prove that bad traces from a set 𝑇are not feasible starting from any state satisfying 𝜙𝑖𝑛that can be expressed as a conjunction of linear constraints on the input node features. Next, we develop a baseline algorithm that iterates over traces and performs early punning of safe traces. Then, we discuss the precision and performance tradeoff in this procedure and describe an efficient encoding of the system across multiple steps that still preserves completeness. Multistep verification with trace enumeration. We first present a multistep verification procedure in Alg. 3 and then describe our optimizations for improving its speed. At a highlevel, the algorithm searches for an initial state in 𝜙𝑖𝑛that would result in a trace 𝑡∈𝑇by trying to compute all possible traces starting from 𝜙𝑖𝑛. The progress of the search is tracked by a stack (initialized with an emptry trace nil) containing the set of (partial) traces that need to be explored. Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:17 During the search, we pick a partial trace 𝑡(Line 6) from the stack and check whether it matches any traces in 𝑇(Line 7). There is a match between two traces if they have the same length and the same action sequence. There are three outcomes of our check. If there is a match (MATCH ), then the search terminates as we have found a (potential) violation of the property, and we return either VIOLATED orUNKNOWN depending on whether the encoding and the base solver (getPossibleActions ) is complete. Otherwise, if no trace in 𝑇has𝑡as a prefix, then the check returns NOMATCH . In this case, we can conclude any trace with prefix 𝑡is not in𝑇and move on to analyze another trace. If this check is inconclusive (i.e., no trace in 𝑇is equal to𝑡, but some traces have𝑡as prefix), then we must expand this trace to determine whether there is a potential property violation. That is, we need to compute the possible next actions of the GNN agent conditioned on the initial state 𝜙𝑖𝑛, the transition system T, and the current trace 𝑡(Line 1018). Algorithm 3 Multistep verification via trace enumera tion. 1:Input: a message passing component 𝑚, a summary component s, a prediction component h, an initial state𝐺0, a transition system T, and a specification 𝜙:𝜙𝑖𝑛→unreach(𝑇) 2:Output: HOLD/VIOLATED/UNKNOWN 3:function checkWithTraceEnumeration (𝑚,𝑓,𝑔,𝐺 0,𝜙) 4:𝑠𝑡𝑎𝑐𝑘↦→{nil} 5: while¬𝑠𝑡𝑎𝑐𝑘. empty()do 6:𝑡↦→𝑠𝑡𝑎𝑐𝑘. pop() 7:𝑟↦→match(𝑡,𝑇) 8: if𝑟=MATCH then return VIOLATED/UNKNOWN 9: else if𝑟=NOMATCH then continue 10: else𝑀,𝐺,𝑘↦→𝜙𝑖𝑛,𝐺0,0 11: forv𝑖in𝑡do 12: 𝑀↦→𝑀∧encodeNetwork(𝑚,h,s,𝐺) 13: 𝑀↦→𝑀∧encodeAction(𝑀,v𝑖) 14: 𝑀↦→𝑀∧encodeFeatureUpdates (T,𝐺,v𝑖) 15: 𝐺,𝑘↦→T(𝐺,v𝑖),𝑘+1 16:𝑀↦→𝑀∧encodeNetwork(𝑚,h,s,𝐺) 17: Θ↦→candidate(𝐺) 18:𝑄↦→getPossibleActions (𝑀,Θ) 19:𝑠𝑡𝑎𝑐𝑘↦→𝑠𝑡𝑎𝑐𝑘∪{𝑡:: v𝑖|v𝑖∈𝑄} 20: return HOLD 21:function getPossibleActions (𝑀,Θ,𝐺) 22:𝑄↦→{} ,𝜙(v)↦→(Ô 𝑗∈Θv𝑗≥v) 23: for𝐽∈𝐺do 24:𝜙𝑜𝑢𝑡↦→Ó v𝑖∈𝐽∩Θ𝜙(v↦→v𝑖) 25:𝑄↦→𝑄∪checkWithNodeAbstraction′(𝑀,𝜙𝑜𝑢𝑡) 26: return QNote that an exact encoding of the transition system up to 𝑡(Lines 1115) in volves the precise encoding of 1) the net work at each time step (Line 12); 2) the action taken at each time step (Line 13); and 3) the updates of the feature vec tors (Line 14).An incomplete encoding can be achieved by ignoring the first two components. This amounts to ignoring the previous trace and only encoding the network at the current step. We ex plore the runtimeprecision tradeoff of the complete encoding in our experimen tal section. The algorithm returns HOLD if no traces from 𝑇are matched during the enumeration. Notice that 𝑇is only used in the MATCH function for checking whether the current trace we are explor ing is a “bad” trace. Therefore, in prac tice, instead of providing 𝑇as a concrete set of traces, it is sufficient and relatively easier to provide an implementation of theMATCH function corresponding to the property. ThegetPossibleActions method at Line 18 can build on top of any single step verification engine including, in particular, the procedure introduced in Sec. 5. One instantiation tailored to GNNbased job schedulers is described in Alg. 3. Our goal is to check whether an action vindexed by a set Θcan be scheduled. This can be formulated as checking whether the postcondition that vis not the maximum (Line 22) holds. We use the techniques introduced in Sec. 5 to reason about candidate actions belonging to the same job DAG simultaneously. This is based on the observation that candidate actions corresponding to the same job often have similar verification results. Note that here we use a slightly modified version of Alg. 2 where instead of Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:18 Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh returning HOLD/VIOLATED/UNKNOWN we return all disjuncts in the post condition that does not hold. This amounts to modifying Line 20 in Alg. 3 to continue the search instead of returning. Theorem 6.1. Alg. 3 terminates if 𝑇has finite length traces. Proof. Let𝐾be the maximum number of possible actions at a given time step and 𝑅be the longest trace length in 𝑇. In the worst case, there are a finite number of traces ( 𝐾𝑅) to check. This is because the algorithm: (a) does not check the same trace twice (this can be proved by induction on the length of the partial trace 𝑡); and (b) does not expand any 𝑡whose prefix does not match a trace in𝑇(Line 9 of Algorithm 3). □ Theorem 6.2 (Soundness). If the encodings (Lines 12,13,14, 16) and getPossibleActions are sound, then Alg. 3 is sound. Proof. The assumptions guarantee that 𝑄is a superset of the actual feasible actions. By induc tion on the length of the trace added to 𝑠𝑡𝑎𝑐𝑘 , we can prove that the traces added to 𝑠𝑡𝑎𝑐𝑘 are a superset of the actual reachable set of traces. Therefore, if no trace added to the stack matches any trace in𝑇, no actual reachable trace can match any trace in 𝑇. □ Prooftransfer encoding. In general, if there are changes in node features or graph structures, the message passing would result in different node embeddings. A naïve complete encoding would re encode messagepassing (and subsequent GNN components) for every step. This quickly becomes too expensive as the number of time steps increases. However, taking a closer look at the message passing scheme, we observe that the effect of the graph structure/feature updates on the message passing is local to the disconnected component where the updates occur. This means that we only need to reencode disconnected components of the graphs that are updated. In the case of Decima, we observe that between every scheduling event (invokation of the GNN agent), only a small subset of the job DAGs are updated. This results in significant savings in the length of the encoding and runtime as demonstrated by our experimental results. 7 SPECIFICATIONS FOR JOB SCHEDULER In this section we define the properties we verify for GNNbased schedulers like Decima. We emphasize that our framework allows the user to specify a rich set of verification properties. Here, we focus on two formulations of the strategyproofness properties to demonstrate the capabilities of our method. We choose to study strategyproofness as it is not only important in practice but also representative of the general form of specifications that our framework can handle. We additionally investigate the datalocality property of the scheduler in App. D.1, which was identified as one of the critical factors in job scheduler performance [Zaharia et al. 2010]. Strategyproofness is a desirable property of schedulers that intuitively means: “a user cannot benefit by misrepresenting their need.” For example, we expect that the user cannot get their jobs scheduled earlier by requiring more resources for them. If this basic property does not hold, malicious users can mislead the system into stalling all but their jobs. Interestingly, this property holds for simple schedulers such as FIFO (firstinfirstout) and CMMF (Constrained MaxMin Fairness) [Shenker and Stoica 2013]. However, due to the noninterpretable nature of the GNNbased scheduler, strategyproofness cannot be guaranteed by construction. Definition 7.1 (Singlestep strategyproofness). Given an initial job profile 𝐺=(𝐴,𝑋)containing K jobs𝐺1,...,𝐺𝐾, suppose the scheduler picks a node from job 𝐺𝑘. For each node feature vector x𝒊, letx𝑖𝑑andx𝑖𝑡denote the entries of estimated total duration and the number of tasks, respectively. Let𝐺𝑎∈𝐺be a job other than 𝐺𝑘(e.g., the job of an adversarial user). Let 𝐶and𝐶𝑎denote the Proc. ACM Program. Lang., Vol. 6, No. OOPSLA2, Article 162. Publication date: October 2022.162:19 frontier nodes in 𝐺and𝐺𝑎, respectively. The job scheduler is strategyproof with respect to 𝐺and 𝐺𝑎, where𝑎≠𝑘, if∀𝐺′=(𝐴,𝑋′), Û v𝑖∈𝐶𝑎(x′ 𝑖𝑑∈[x𝑖𝑑,𝛼𝑑x𝑖𝑑] ∧x′ 𝑖𝑡∈[x𝑖𝑡,𝛼𝑡x𝑖𝑡] ∧x′ 𝑖𝑑 x′ 𝑖𝑡≥x𝑖𝑑 x𝑖𝑡) →Û v𝑖∈𝐶𝑎 ¬"
360,Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models.txt,"Interpretation and diagnosis of machine learning models have gained renewed
interest in recent years with breakthroughs in new approaches. We present
Manifold, a framework that utilizes visual analysis techniques to support
interpretation, debugging, and comparison of machine learning models in a more
transparent and interactive manner. Conventional techniques usually focus on
visualizing the internal logic of a specific model type (i.e., deep neural
networks), lacking the ability to extend to a more complex scenario where
different model types are integrated. To this end, Manifold is designed as a
generic framework that does not rely on or access the internal logic of the
model and solely observes the input (i.e., instances or features) and the
output (i.e., the predicted result and probability distribution). We describe
the workflow of Manifold as an iterative process consisting of three major
phases that are commonly involved in the model development and diagnosis
process: inspection (hypothesis), explanation (reasoning), and refinement
(verification). The visual components supporting these tasks include a
scatterplot-based visual summary that overviews the models' outcome and a
customizable tabular view that reveals feature discrimination. We demonstrate
current applications of the framework on the classification and regression
tasks and discuss other potential machine learning use scenarios where Manifold
can be applied.","Recent technical breakthroughs in the machine learning ﬁeld have led to highly improved accuracies and utilization in many scenarios, including sophisticated pattern recognition tasks [2,19]. However, these technical advances pose two major challenges. First, the complexity of the models being designed and adopted has signiﬁcantly increased to the point that is difﬁcult for model developers to explain why and • Jiawei Zhang and David S. Ebert are with Purdue University, Email: fzhan1486jebertd g@purdue.edu. This work was done while the ﬁrst author was at Uber Technologies, Inc. • Yang Wang and Lezhi Li are with Uber Technologies, Inc, Email: fgnavvyjlezhi.li g@uber.com. • Piero Molino is with Uber AI Labs, Email: piero@uber.com.how the model works. Second, model developers often lack solid reasoning or evidence to guide their development and debugging due to the hidden mechanisms of the models, making this iterative process more timeconsuming and errorprone. Both of these challenges require more effective approaches that enable interpretation and explanation of machine learning processes [15, 16, 34]. Visual and interactive interfaces have proved to be effective in terms of enabling users to integrate domain knowledge in the process of interpreting and diagnosing these complex models [4, 10, 27, 39]. Typ ical solutions include visualizing the internal structure or intermedi ate states of the model to enhance the understanding and interpreta tion [12,25,41], evaluating and analyzing the performance of models or algorithms [5, 22, 33], and interactively improving the models at differ ent development stages such as feature engineering or hyperparameter tuning through integration of domain knowledge [6, 29, 43]. Never 1arXiv:1808.00196v1  [cs.LG]  1 Aug 2018theless, the focus of these approaches has been mostly restricted to a speciﬁc model type or task type (i.e., classiﬁcation tasks), lacking the ability to extend to more complex industrylevel use scenarios where the size and the complexity of both the model and the task increase. In this paper, we present an interactive framework called Manifold to address these problems of integrating, evaluating and debugging multiple machine learning models. The design process of the frame work has been guided by three major phases that are typically involved in diagnosing and comparing machine learning models: inspection (hypothesis), explanation (reasoning), and reﬁnement (veriﬁcation). The Manifold interface supports these phases through two main visual components. First, we design a novel scatterplotbased visual tech nique that provides a comparative visual summary of the diversity and complementarity of the model pairs, and allows the users to effectively inspect symptom data instances and make hypotheses accordingly. The technique consists of multiple encoding schemes that are ﬂexible and adaptable to various task types such as classiﬁcation or regression. Sec ond, we design a tabular view for the users to visually discriminate features extracted from symptom instances and identify which features are more inﬂuential in the models’ outcome, thus providing explana tions for the hypotheses generated earlier on. These explanations can then be incorporated into a new iteration of the model development in order to validate and reﬁne the model. Comparing to stateoftheart solutions in this area, we focus on generality as the primary property of the framework. Manifold is modelagnostic, in the sense that it does not need access to the internal logic of the model and only relies on the input instances and the output results, allowing the framework to support a broad range of model types, as long as they target the same machine learning task and have a consistent format of input and output. Furthermore, Manifold is built upon scalable WebGLbased rendering frameworks [1, 40] and consists of several visual summarization and interaction designs, making it possible to handle largescale input instances while reducing potential computational and cognitive overload. In this paper, we describe the usage of the framework on two typical supervised learning tasks, multi class text classiﬁcation and regression, and discuss ideas for extending Manifold to a broader range of machine learning tasks. 2 R ELATED WORK "
233,Multi-Task Siamese Neural Network for Improving Replay Attack Detection.txt,"Automatic speaker verification systems are vulnerable to audio replay attacks
which bypass security by replaying recordings of authorized speakers. Replay
attack detection (RA) detection systems built upon Residual Neural Networks
(ResNet)s have yielded astonishing results on the public benchmark ASVspoof
2019 Physical Access challenge. With most teams using fine-tuned feature
extraction pipelines and model architectures, the generalizability of such
systems remains questionable though. In this work, we analyse the effect of
discriminative feature learning in a multi-task learning (MTL) setting can have
on the generalizability and discriminability of RA detection systems. We use a
popular ResNet architecture optimized by the cross-entropy criterion as our
baseline and compare it to the same architecture optimized by MTL using Siamese
Neural Networks (SNN). It can be shown that SNN outperform the baseline by
relative 26.8 % Equal Error Rate (EER). We further enhance the model's
architecture and demonstrate that SNN with additional reconstruction loss yield
another significant improvement of relative 13.8 % EER.","Automatic speaker veriﬁcation (ASV) systems are nowadays increasingly used for various applications. However, ASV systems are vulnerable to audio spooﬁng attacks , which at tempt to gain unauthorized access by manipulating the audio input. One of the most popular and effective audio spooﬁng attacks are replay attacks (RA)s. In an RA the attacker fools the ASV system by replaying a recording of an authorized speaker. Considering how effective and cheap RAs are, it is necessary to augment an ASV system with an RA detection system in practice. The public benchmark ASVspoof initiative started with the ASVspoof 2015 challenge which dealt with texttospeech and voice conversion spooﬁng attacks [1]. ASVspoof 2017 [2] was the ﬁrst challenge concerned with RA detection and thus created a benchmark data set consisting of voice command recordings. ASVspoof 2019 [3], then introduced amuch larger corpus of longer and textindependent recordings for RA detection. The performance of RA detection systems has been thought highly dependent on their input feature process ing [4]. Correspondingly, earlier work has largely dealt with handcrafted feature processing and it has been found that high frequency and phase information can be helpful for RA detection ( e.g.in [5, 6]). Popular input features that emerged include linear frequency cepstral coefﬁcients (LFCC) [7] and group delay (GD) grams [8]. In recent years, input features derived from shorter handcrafted feature processing pipelines, such as the log power magnitude spectra ( LOGSPEC ) [9], attracted more interest. In contrast to LFCC, LOGSPEC pre serves much more of the information present in the original raw signal and thus relies on deep neural netwokrs (DNN)s as powerful feature extractors [9, 10, 11, 12, 13]. Overall, there is currently no conclusive consensus about the best input feature for RA detection. As the quality of recording and replaying devices is get ting better, detecting the difference between genuine and spoofed audios is becoming more difﬁcult. Thus, it becomes necessary to improve the discriminability and generalizabil ity of RA detection systems. Besides common regularization techniques, like data augmentation and Dropout ( cf.with [10, 13]), multiple teams have used discriminative loss func tions and multitask learning (MTL) [14] for better feature discrimination and generalization ( cf.with [9, 12, 15]). Siamese Neural Networks (SNN) [16] have shown to sig niﬁcantly improve the discriminability and generalizability of models [17]. In this paper, we propose to use SNN in an MTL setting for RA detection. More generally, we investigate to what extent adding discriminative loss functions in a MTL setting can improve the performance of RA detection systems on the ASVspoof 2019 challenge Physical Access (PA) data. The analysis is conducted on multiple input features. It is made sure that none of the systems rely on additional data and labels and that all of our settings follow the realworld application implementation. Our main contributions include: 1) Proposal of SNN in MTL setting for improved discrim inability and generalizability of RA detection systems; 2) Ex tensive analysis of discriminative loss functions on multiple input features; 3) Enhancement of a popular architecture forarXiv:2002.07629v1  [eess.AS]  16 Feb 2020RA detection with secondorder statistics pooling; 4) Com bination of reconstruction loss (ReL) with SNN in an MTL setting. 2. RELATED WORK "
490,Generate and Verify: Semantically Meaningful Formal Analysis of Neural Network Perception Systems.txt,"Testing remains the primary method to evaluate the accuracy of neural network
perception systems. Prior work on the formal verification of neural network
perception models has been limited to notions of local adversarial robustness
for classification with respect to individual image inputs. In this work, we
propose a notion of global correctness for neural network perception models
performing regression with respect to a generative neural network with a
semantically meaningful latent space. That is, against an infinite set of
images produced by a generative model over an interval of its latent space, we
employ neural network verification to prove that the model will always produce
estimates within some error bound of the ground truth. Where the perception
model fails, we obtain semantically meaningful counter-examples which carry
information on concrete states of the system of interest that can be used
programmatically without human inspection of corresponding generated images.
Our approach, Generate and Verify, provides a new technique to gather insight
into the failure cases of neural network perception systems and provide
meaningful guarantees of correct behavior in safety critical applications.","One would often like to know whether or not a neural network perception system produces correct estimates and, moreover, when it will fail to do so. The na ve approach to this question is to gather more data against which to evaluate system performance. This gives a necessarily incomplete picture: we can only ever evaluate a nite set of individual data points (e.g., images), and, in most cases, it will be impossible to meaningfully capture the totality of relevant data the system might encounter in its operational environment by such a set. Indeed, 1arXiv:2012.09313v1  [cs.LG]  16 Dec 2020Figure 1: Example of conguration space for simple distance to line estimation. Heredis the distance in meters to the line and is the yaw angle. part of the appeal of neural networks and machine learning is their ability to, in many cases, successfully generalize to unseen data. A recent alternative approach utilizes neural network verication tools built on techniques such as satisability modulo theories (SMT), mixed integer linear programming (MILP), or other techniques from logic and computational geome try to mathematically analyze the neural network and produce either proof arti facts, capturing mathematical guarantees on performance, or counterexamples, indicating cases where mathematical guarantees are impossible. Crucially, be cause of the analytical approach of neural network verication, the guarantees obtained are universally quantied and therefore apply to an entire region of the input space of the network encompassing innitely many input values. The skeptical reader should compare this with cases such as linear programming, where (in infeasible instances) similar guarantees over innite sets are possible. Neural network verication promises to powerfully complement testing, but it has primarily been leveraged in the perception domain in connection with local adversarial robustness properties of the form: Given a model and xed image x, does there exist a perturbation bounded in norm below some xed constant >0such that the perturbed image (x+)is classied dierently from xby the model? Beyond local adversarial robustness and related properties, such as invari ance under specied transformations, the use of neural network verication to analyze perception systems is a challenge, due to the fact that most of the desirable properties of such systems cannot be formally specied. Having a for mal, mathematically welldened, specication is a prerequisite for the use of these techniques. E.g., how would one specify the property that a classier will correctly classify pedestrians in a formal, and nonstatistical/empirical, mathe matical manner? It is apparent that the formal specication of such properties in a way that can be analyzed by neural network verication is impossible. Nonetheless, our goal in this paper is to show that, by the use of generative 2models (which introduce an empirical/statistical component to the analysis), it is possible to use neural network verication techniques to specify and formally reason over more general model properties than local adversarial robustness and transformation invariance. If we are interested in global correctness with respect to semantically mean ingful specications, then the images over which we reason will need to be drawn from a semantically meaningful latent space. Our approach to global correctness rst denes a semantically meaningful conguration space capturing the prop erties of the scenario depicted in images of the kinds we wish to reason over. As a motivating example, our work has focused on the analysis of a perception system for estimating the distance to a lane marker from images produced by a front facing vehicle mounted camera. The conguration space consists of the vehicle's distance to the line d, and yaw angle with respect to the line, as depicted in Figure 1. To then verify properties of a neural network perception system with respect to our conguration space we require a function (the decoder ) mapping from conguration space to image space, which can then be composed inline with our regression perception system (the regressor ). The formal analysis using neural network verication then reasons over a semantically meaningful region of the conguration space, and the output to be checked for correctness is that of our regressor. Each interval in our conguration space, assuming it is not degenerate, represents an innite set of images against which the perception system will be veried for correctness. Our approach, Generate and Verify , learns a neural network decoder from conguration space to image space against which the regressor can be veried. Our notion of global correctness is thus modulo the learned decoder. In the event the neural network verication cannot prove correctness it will generally return a counterexample in conguration space, which can then be provided to the decoder to yield the exact image on which our perception system fails. In practice, we divide the set of intervals over our conguration space into smaller boxes so that we may parallelize computation, gather diverse counterexamples, and identify smaller regions over which our regressor may be provably correct. Our approach yields proof artifacts (certicates), and provides a method to nd counterexamples, that traditional sample and test techniques cannot provide. 2 Related Works "
26,PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition.txt,"Face recognition networks encode information about sensitive attributes while
being trained for identity classification. Such encoding has two major issues:
(a) it makes the face representations susceptible to privacy leakage (b) it
appears to contribute to bias in face recognition. However, existing bias
mitigation approaches generally require end-to-end training and are unable to
achieve high verification accuracy. Therefore, we present a descriptor-based
adversarial de-biasing approach called `Protected Attribute Suppression System
(PASS)'. PASS can be trained on top of descriptors obtained from any previously
trained high-performing network to classify identities and simultaneously
reduce encoding of sensitive attributes. This eliminates the need for
end-to-end training. As a component of PASS, we present a novel discriminator
training strategy that discourages a network from encoding protected attribute
information. We show the efficacy of PASS to reduce gender and skintone
information in descriptors from SOTA face recognition networks like Arcface. As
a result, PASS descriptors outperform existing baselines in reducing gender and
skintone bias on the IJB-C dataset, while maintaining a high verification
accuracy.","Over the past few years, the accuracy of face recognition networks has significantly improved [ 44,45,38,16,10,18]. These improvements have led to the deployment of face recognition systems in a large number of applications. How ever, recent studies [ 17,26,48] have also shown that face recognition networks encode information about protected attributes such as race, gender, and age, while being trained for identity classification. Encoding of sensitive attributes raises concerns regarding privacy and bias. Privacy concerns: Many largescale face verification and identification systems employ a database that stores face descriptors of identities, as opposed to face images. Face descriptors refer to the features extracted from the penulti mate layer of a previously trained face recognition network. *These authors have contributed equally to this work. Identity 1  MaleIdentity 2  MaleIdentity 3  FemaleIdentity 4  MaleIdentity 1Identity 2Identity 3Identity 4 PrivacyLeakageScenarioStep 2Malicious Gender classifier CXis trained on descriptors of Dataset DXPretrainedFace Recognition Network (P)Private database Dof face descriptors Step 1Face descriptors are extractedfor dataset DX, using PDataset DX(with gender labels)Identity 1Identity 2Identity 3Identity 4 Step 3Trained classifier CXpredicts Gender for descriptors in private database DIdentities for which descriptors are extracted (using pretrained network P)and stored in a private database DFigure 1. Suppose a malicious agent Xhas gained access to a private database D(blue) which consists of a pretrained network Pand face descriptors of four identities. The agent can use Pto extract descriptors (red) for a genderlabeled dataset DX(Step 1). Using these descriptors, the agent can train a gender classifier CX (Step 2). Using the trained CX, the agent can predict the gender of the descriptors in D(Step 3) and thus cause privacy breach. Storing descriptors, rather than images, allows for very fast gallery lookup and verification against known subjects. This also acts as an additional layer of security by not storing potentially sensitive information present in the original face images. However, since some sensitive information is still encoded in these descriptors (e.g. race, gender, age), a mali cious agent with access to these descriptors can potentially extract this information and use it for nefarious purposes. An example scenario is presented in Figure 1. Bias concerns: Encoding of protected attributes such as gender or race in face descriptors results in bias w.r.t. these attributes when used for face recognition. A recent study from NIST [ 23] found evidence that characteristics such as gender and ethnicity impact verification and matching per formance of face descriptors. Similarly, it has been shown that most facebased gender classifiers perform significantly better on male faces with light skintone than female faces with dark skintone [12]. One method of addressing privacy and bias issues is by producing face descriptors that are independent of the protected attribute(s). For instance, Debface [ 21] proposes an endtoend method for producing face descriptors that are disentangled from protected attributes using an adversarial approach. Another common strategy for mitigating bias is to train face recognition systems using training datasets that are balanced in terms of sensitive attributes. However, building large datasets that are balanced in terms of the attributes we want to protect is difficult, expensive, and timeconsuming. Moreover, once such a ‘fair’ dataset is constructed, we still need to perform the costly operation of training a large recog nition network from scratch. Endtoend training of a largescale network requires ac cess to a large dataset and computing power, and is time consuming. Application of adversarial losses while training (as done in [ 21]), also slows down the training process. Sev eral works [ 53,11,28] show that reducing the information of sensitive attributes while training a network results in a drop in overall performance. Even if a new network is trained to generate attributeagnostic face descriptors, we need to replace the existing network (say, Pin Fig 1), and recompute the descriptors for all the identities by feeding in the respective face images. In this work, we propose a solution that addresses the fol lowing four points: (i) reduces the opportunity for leakage of protected attributes in face descriptors. (ii) mitigates bias with respect to multiple attributes (gender and skintone). (iii) operates on existing descriptors and does not require expen sive endtoend training. (iv) does not require a balanced training dataset. The proposed method trains a lightweight model that transforms face descriptors obtained from an existing face recognition model, and maps them to an attribute agnostic representation. We achieve this using a novel adversarial training procedure called Protected Attribute Suppression System (PASS). Unlike other works that adversarially sup press protected attributes [ 21,53] using endtoend training, we operate on descriptor space. Once trained, PASS may be easily applied to other existing face descriptors. In summary, we make the following contributions in this paper: 1.We present PASS, an adversarial method that aims to reduce the information of sensitive attributes in face de scriptors from any face recognition network, while main taining high face verification performance. We show the efficacy of PASS to reduce gender and skintone informa tion in face descriptors, and thus considerably reduce the associated biases. Moreover, PASS can be used on top of face descriptors obtained from anyface recognition network. We show these results on two SOTA pretrained networks: Arcface [16] and Crystalface [38]. 2.Our descriptorbased model cannot include CNNbased discriminators, which poses new challenges. We present a novel discriminator training strategy in PASS, to enforceMethod Target task Sensitive attribute [55, 31] Analogy completion Gender [52] Object classification Gender [53] Action classification Identity, private attributes [13] Action recognition Scene [6] Gender/Age prediction Age/Gender [20] Preserve pose/illumination/expresssion Identity [28] Smile, highcheekbones Gender, makeup [7] Face detection Skintone [37] Face attractiveness Gender [51] Face recognition Race [21] Face recognition Age,gender,race PASS (Ours) Face recognition Gender, skintone Table 1. Methods that adversarially remove sensitive attributes in "
129,Online Verification of Deep Neural Networks under Domain Shift or Network Updates.txt,"Although neural networks are widely used, it remains challenging to formally
verify the safety and robustness of neural networks in real-world applications.
Existing methods are designed to verify the network before deployment, which
are limited to relatively simple specifications and fixed networks. These
methods are not ready to be applied to real-world problems with complex and/or
dynamically changing specifications and networks. To effectively handle such
problems, verification needs to be performed online when these changes take
place. However, it is still challenging to run existing verification algorithms
online. Our key insight is that we can leverage the temporal dependencies of
these changes to accelerate the verification process. This paper establishes a
novel framework for scalable online verification to solve real-world
verification problems with dynamically changing specifications and/or networks.
We propose three types of acceleration algorithms: Branch Management to reduce
repetitive computation, Perturbation Tolerance to tolerate changes, and
Incremental Computation to reuse previous results. Experiment results show that
our algorithms achieve up to $100\times$ acceleration, and thus show a
promising way to extend neural network verification to real-world applications.","Neural networks are widely applied to safety/security critical applications, such as autonomous driving (Chen et al. 2015), ﬂight control (Ng et al. 2006), facial recogni tion (Meng et al. 2017), and stock trading (Sezer, Ozbayo glu, and Dogdu 2017). These applications require the net work to behave as expected. We can write these expectations in mathematical speciﬁcations, and use formal veriﬁcation to check whether the network satisﬁes the speciﬁcations. A speciﬁcation is usually encoded as an inputoutput property: given an arbitrary input from an input set, whether the output of the network falls in a speciﬁed output set. For example, for a classiﬁcation network, the input set can be: images of an apple from different angles, and the speciﬁed output set is all outputs that classify the image as “apple”. There are many existing works that can formally verify these input output properties for neural networks (Liu et al. 2020). However, applying existing neural network veriﬁcation methods to realworld applications still faces many chal lenges. Most existing methods are designed for ofﬂine ver iﬁcation before the network is deployed. But in realworldapplications, there are two cases that ofﬂine veriﬁcation can not handle in advance. The ﬁrst case is when the speciﬁca tion is datadependent and the data domain is timevarying in real time . If we perform ofﬂine veriﬁcation in this case, the ofﬂine speciﬁcation must include all possible data do mains, which can form an extremely large speciﬁcation that may essentially cover the whole input space and is compu tationally intractable to verify. E.g., to prove the robustness of an object detector for video streaming. It is not enough to just show that the detector can tolerate certain input per turbations on images from a ﬁnite training set. Because the streaming may contain unseen images. Ofﬂine veriﬁcation requires us to verify the detector on all potential images, which is impossible. The second case is when the neural network evolves after deployment . In fact, many applications require the neural network to adapt online (Si, Wei, and Liu 2019). For example, a metalearned robot adapts to unseen tasks (Finn, Abbeel, and Levine 2017) and a behavior pre diction network adapts to subjects. If the potential online evolution is not considered, the ofﬂine veriﬁcation results cannot guarantee the safety or robustness of the neural net work during online execution. On the other hand, it is com putationally intractable to make the ofﬂine veriﬁcation cover all possible online evolution. New methods are desired to ad dress these challenges. All these challenges motivate online veriﬁcation. Instead of verifying a hard and complex problem once for all, we canverify a sequence of timevarying problems on the ﬂy to provide guarantees that the neural network is safe and robust to use now and in the near future . The speciﬁcation for these online problems can be centered around the current data distribution and current network instead of covering all possible data distributions. E.g., we only need to verify the object detector is robust for the current image of a video streaming instead of all potential images. Nevertheless, the tradeoff introduced by turning ofﬂine veriﬁcation into online veriﬁcation is that the online prob lems need to be veriﬁed in realtime when either the speci ﬁcation or the network changes. But existing veriﬁcation al gorithms are too slow for realtime execution. On the other hand, online problems exhibit temporal dependencies (e.g., a video stream contains a sequence of correlated images, and the parameters in a network change incrementally dur ing online adaptation). We can exploit these temporal depenarXiv:2106.12732v2  [cs.LG]  3 Feb 2023dencies to accelerate the veriﬁcation process. In this work, we consider two common types of temporal dependency: temporal dependency in the input data under domain shift and temporal dependency in the network parameters under network updates. We ﬁrst analyze the computation bottle neck of existing veriﬁcation algorithms and propose three principles to accelerate the veriﬁcation for online problems: Branch Management to reduce repetitive computation, Per turbation Tolerance to tolerate changes, and Incremental Computation to reuse previous results. Then we derive con crete algorithms based on these principles and achieve up to 100acceleration. Our analysis and solutions are based on reachabilitybased veriﬁcation methods, but the core algo rithms can also be applied to other veriﬁcation methods. In summary, our contributions are threefold: 1) introduc ing online veriﬁcation to greatly reduce veriﬁcation difﬁ culty for problems with timevarying speciﬁcations or time varying networks; 2) analyzing computation bottlenecks of existing veriﬁcation algorithms and proposing three accel eration principles: Branch Management, Perturbation Toler ance, and Incremental Computation; 3) developing concrete algorithms for online veriﬁcation based on these principles. The algorithms achieve up to 100speed up. 2 Problem formulation for Online Veriﬁcation This section provides a formal description of online veriﬁ cation problems progressively. We consider feedforwarding ReLU neural networks with inputoutput speciﬁcations. 2.1 Neural network and speciﬁcations Consider an nlayer feedforward neural network that rep resents a function fwith input x2DxRk0andy2 DyRkn,i.e.y=f(x), wherek0is the input dimension andknis the output dimension. Each layer in fcorresponds to a function fi:Rki"
123,Convolutional Spiking Neural Networks for Detecting Anticipatory Brain Potentials Using Electroencephalogram.txt,"Spiking neural networks (SNNs) are receiving increased attention as a means
to develop ""biologically plausible"" machine learning models. These networks
mimic synaptic connections in the human brain and produce spike trains, which
can be approximated by binary values, precluding high computational cost with
floating-point arithmetic circuits. Recently, the addition of convolutional
layers to combine the feature extraction power of convolutional networks with
the computational efficiency of SNNs has been introduced. In this paper, the
feasibility of using a convolutional spiking neural network (CSNN) as a
classifier to detect anticipatory slow cortical potentials related to braking
intention in human participants using an electroencephalogram (EEG) was
studied. The EEG data was collected during an experiment wherein participants
operated a remote controlled vehicle on a testbed designed to simulate an urban
environment. Participants were alerted to an incoming braking event via an
audio countdown to elicit anticipatory potentials that were then measured using
an EEG. The CSNN's performance was compared to a standard convolutional neural
network (CNN) and three graph neural networks (GNNs) via 10-fold
cross-validation. The results showed that the CSNN outperformed the other
neural networks.","Signiﬁcant advancements in computing hardware, such as graphics processing units and ﬁeldprogrammable gate arrays, along with the availability of large datasets, have enabled researchers to develop highly effective neural networks in the last decade. However, training and utilizing these networks often involves a large amount of energy consumption, thus restricting the deployment of neural networks for datalimited and energylimited settings, which are typically found in applications in dynamic/mobile environments. On the contrary, biological neural networks need only very few or even only one data point to perform at a competitive level compared to ‘traditional’ neural networks (refer to page 54 in [1]).Therefore, machine learning architectures that more closely resemble the human brain in their function are now receiving increased attention. One such example is the increasingly popular spiking neural network (SNN) [2]–[4], which in cludes layers composed of spiking neurons , which mimic the synaptic connections in the brain by emitting aperiodic spikes as the output. This sparse and discrete behavior of SNNs has been shown to reduce energy consumption by orders of magnitude when implemented on emerging neuromorphic hardware. However, shallow SNNs are insufﬁcient to detect patterns that occur at any random time/location in tasks such as object detection/segmentation, as in the case of multi layer perceptrons. This has lately led to the development of hybrid convolutional and spiking neural networks, referred to as convolutional spiking neural networks (CSNNs) [5]– [7], which combine the power of extracting spatiotemporal features in convolutional layers with the energy efﬁciency in spiking layers. CSNNs have started gaining attention in diverse applications such as computer vision [8], [9], speech recognition [10], handgesture recognition [11] and detection of alzheimer’s disease [12], in the past few years. However, their full potential is yet to be explored. The main contribution of this paper is investigating the use of CSNNs in advanced driverassist systems (ADAS) in modern vehicles via utilizing electroencephalograms (EEGs), a method of measuring and recording electrical potentials from across various points in the human brain. ADAS is a group of assistive technologies that are being rapidly introduced to improve road safety and reduce trafﬁc accidents [13]. These technologies accompany and assist humans with driving and/or parking decisions and help steer them towards a safe operation of the vehicle. EEGbased ADAS leverages a subﬁeld of brain computerinterface (BCI) [14] in developing effective ADAS technologies. Furthermore, the potential of CSNNs has not been explored yet in EEGbased ADAS, although some initial studies have been made to demonstrate the effectiveness of shallow SNNs [15]–[21]. EEGADAS technologies can mostly be divided into two groups: (i) those that determine the current driver state [22]– [25], and (ii) those that predict driver’s intention to make a speciﬁc action in the future (e.g. braking) [26]–[28]. The present study focuses on the latter and seeks to predict driver’s braking decisions via detecting anticipatory brain potentials inarXiv:2208.06900v1  [cs.NE]  14 Aug 20222 EEG signals. Anticipatory brain potentials have been observed as early as 130 ms [29] and 320 200 ms [30] before the actual action. II. R ELATED WORK "
122,CosFace: Large Margin Cosine Loss for Deep Face Recognition.txt,"Face recognition has made extraordinary progress owing to the advancement of
deep convolutional neural networks (CNNs). The central task of face
recognition, including face verification and identification, involves face
feature discrimination. However, the traditional softmax loss of deep CNNs
usually lacks the power of discrimination. To address this problem, recently
several loss functions such as center loss, large margin softmax loss, and
angular softmax loss have been proposed. All these improved losses share the
same idea: maximizing inter-class variance and minimizing intra-class variance.
In this paper, we propose a novel loss function, namely large margin cosine
loss (LMCL), to realize this idea from a different perspective. More
specifically, we reformulate the softmax loss as a cosine loss by $L_2$
normalizing both features and weight vectors to remove radial variations, based
on which a cosine margin term is introduced to further maximize the decision
margin in the angular space. As a result, minimum intra-class variance and
maximum inter-class variance are achieved by virtue of normalization and cosine
decision margin maximization. We refer to our model trained with LMCL as
CosFace. Extensive experimental evaluations are conducted on the most popular
public-domain face recognition datasets such as MegaFace Challenge, Youtube
Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art
performance on these benchmarks, which confirms the effectiveness of our
proposed approach.","Recently progress on the development of deep convo lutional neural networks (CNNs) [15, 18, 12, 9, 44] has signiﬁcantly advanced the stateoftheart performance on Corresponding authors Training  Faces Testing  FacesConvNetLoss Layers … Cosine SimilarityVerification IdentificationLabels  Learned by Softmax Learned by LMCL Cropped FacesFigure 1. An overview of the proposed CosFace framework. In the training phase, the discriminative face features are learned with a large margin between different classes. In the testing phase, the testing data is fed into CosFace to extract face features which are later used to compute the cosine similarity score to perform face veriﬁcation and identiﬁcation. a wide variety of computer vision tasks, which makes deep CNN a dominant machine learning approach for computer vision. Face recognition, as one of the most common com puter vision tasks, has been extensively studied for decades [37, 45, 22, 19, 20, 40, 2]. Early studies build shallow mod els with lowlevel face features, while modern face recogni tion techniques are greatly advanced driven by deep CNNs. Face recognition usually includes two subtasks: face ver iﬁcation and face identiﬁcation. Both of these two tasks involve three stages: face detection, feature extraction, and classiﬁcation. A deep CNN is able to extract clean high level features, making itself possible to achieve superior performance with a relatively simple classiﬁcation architec ture: usually, a multilayer perceptron networks followed byarXiv:1801.09414v2  [cs.CV]  3 Apr 2018a softmax loss [35, 32]. However, recent studies [42, 24, 23] found that the traditional softmax loss is insufﬁcient to ac quire the discriminating power for classiﬁcation. To encourage better discriminating performance, many research studies have been carried out [42, 5, 7, 10, 39, 23]. All these studies share the same idea for maximum discrimi nation capability: maximizing interclass variance and min imizing intraclass variance. For example, [42, 5, 7, 10, 39] propose to adopt multiloss learning in order to increase the feature discriminating power. While these methods improve classiﬁcation performance over the traditional softmax loss, they usually come with some extra limitations. For [42], it only explicitly minimizes the intraclass variance while ignoring the interclass variances, which may result in sub optimal solutions. [5, 7, 10, 39] require thoroughly schem ing the mining of pair or triplet samples, which is an ex tremely timeconsuming procedure. Very recently, [23] pro posed to address this problem from a different perspective. More speciﬁcally, [23] (Asoftmax) projects the original Euclidean space of features to an angular space, and intro duces an angular margin for larger interclass variance. Compared to the Euclidean margin suggested by [42, 5, 10], the angular margin is preferred because the cosine of the angle has intrinsic consistency with softmax. The for mulation of cosine matches the similarity measurement that is frequently applied to face recognition. From this perspec tive, it is more reasonable to directly introduce cosine mar gin between different classes to improve the cosinerelated discriminative information. In this paper, we reformulate the softmax loss as a cosine loss byL2normalizing both features and weight vectors to remove radial variations, based on which a cosine margin termmis introduced to further maximize the decision mar gin in the angular space. Speciﬁcally, we propose a novel algorithm, dubbed Large Margin Cosine Loss (LMCL), which takes the normalized features as input to learn highly discriminative features by maximizing the interclass cosine margin. Formally, we deﬁne a hyperparameter msuch that the decision boundary is given by cos(1)"
300,Learning Privacy Preserving Encodings through Adversarial Training.txt,"We present a framework to learn privacy-preserving encodings of images that
inhibit inference of chosen private attributes, while allowing recovery of
other desirable information. Rather than simply inhibiting a given fixed
pre-trained estimator, our goal is that an estimator be unable to learn to
accurately predict the private attributes even with knowledge of the encoding
function. We use a natural adversarial optimization-based formulation for
this---training the encoding function against a classifier for the private
attribute, with both modeled as deep neural networks. The key contribution of
our work is a stable and convergent optimization approach that is successful at
learning an encoder with our desired properties---maintaining utility while
inhibiting inference of private attributes, not just within the adversarial
optimization, but also by classifiers that are trained after the encoder is
fixed. We adopt a rigorous experimental protocol for verification wherein
classifiers are trained exhaustively till saturation on the fixed encoders. We
evaluate our approach on tasks of real-world complexity---learning
high-dimensional encodings that inhibit detection of different scene
categories---and find that it yields encoders that are resilient at maintaining
privacy.","Images and videos are rich in information about the en vironments they represent. This information can then be used to infer various environment attributes such as loca tion, shapes and labels of objects, identities of individuals, classes of activities and actions, etc. But often, it is desir able to share data—with other individuals, untrusted ap plications, over a network, etc.—without revealing values of certain attributes that a user may wish kept private. For such cases, we seek an encoding of this data that is privacy preserving , in that the encoded data prevents or inhibits the estimation of speciﬁc sensitive attributes, but still retains other information about the environment—information that may be useful for inference of other, desirable, attributes. When the relationship between data and attributes can beexplicitly modeled, it’s possible to derive an explicit form for this encoding [7]. This includes the case where the goal is to encode a ﬁxed dataset with known values of the pri vate label (where privacy can be achieved, for example, by partitioning the dataset into subsets with different values of the private label, and explicitly transforming each set to the same value [23]). This work deals instead with the setting where the relationship between data and private attributes is not explicit, and is learned through training an estimator. Our goal is to ﬁnd an encoding that prevents or inhibits such a trained estimator or classiﬁer from succeeding. Note that we do not want an encoding that simply confounds a ﬁxed classiﬁer or estimator. Rather, we want that even af ter the encoding is ﬁxed , a classiﬁer that has knowledge of the encoding, and which can therefore be trained on en coded training data, is unable to make accurate predictions when generalizing beyond the training set. This can be es pecially challenging in the vision setting when, for example, an image has potentially multiple, redundant cues towards a private environment attribute. While a speciﬁc censorship strategy could cause failures in a given estimator by inter fering with the cues it depends on, given a chance to retrain, the estimator learns to use different cues still present. To address this issue, we consider a formulation to learn an encoding function, through adversarial training against a classiﬁer that is simultaneously training to succeed at re covering the private attribute from encoded data. The en coder, in turn, trains to prevent this inference, while also maintaining some notion of utility—a generic objective of maintaining variance in its outputs or promoting the success of a second classiﬁer training for a different attribute. This is a natural formulation given the success of ad versarial optimization [11], and has in fact previously been considered in the privacy setting [8, 21, 14]. However, we ﬁnd standard adversarial optimization to be insufﬁcient for achieving privacy against complex inference tasks—when producing highdimensional encodings that inhibit recov ery of an image attribute whose value may be indicated by multiple redundant cues in the input, against highcapacity classiﬁers modeled as deep neural networks that are able to discover and exploit such cues. In these cases, we ﬁndarXiv:1802.05214v3  [cs.LG]  4 Dec 2018Figure 1. Training Classiﬁers on Learned Privacypreserving Encoders. We show the evolution of validation set accuracy when learning classiﬁers on outputs of encoders trained to preserve privacy and maintain utility. We consider two settings: encoders trained to inhibit detection of the “Army Base” scene category while promoting “Airport Terminal”, and viceversa. For each case, we examine classiﬁers trained for both private and desirable tasks on encoded images. We compare to training classiﬁers on the original images themselves, as well as on blurred images as a naive nontask speciﬁc baseline. For private tasks, we include comparisons to encoders also trained in the same adversarial framework, but with standard GAN updates rather than our approach. Our encoders preserve information for desirable tasks, while degrading the ability to solve private tasks—both in terms of training speed and ﬁnal achieved accuracy. Moreover, our approach yields encoders that are far more effective at preserving privacy than with standard GAN updates—even though the latter are able to inhibit private classiﬁers within adversarial training, those classiﬁers recover to a much greater degree once the encoders are ﬁxed. that there is often a signiﬁcant gap between the performance of the private attribute classiﬁer within and after adversar ial optimization. An encoder training simultaneously with the classiﬁer is able to keep the latter at bay, but the clas siﬁer recovers once it is able to train against an encoder that has been ﬁxed. We also ﬁnd that training against high capacity classiﬁers leads to instability in the adversarial op timization, with the encoder often converging to a trivial lo cally optimal solution of producing a constant output—thus achieving perfect privacy but eliminating all utility. A key contribution of our work is thus our modiﬁed opti mization approach, that implicitly regularizes the optimiza tion by using a form of normalization in the encoder for sta bility, and uses modiﬁed gradientbased updates to promote learning encoding functions that permanently limit recovery of private attributes. We also adopt a rigorous experimental protocol for evaluating privacypreserving encoders, where classiﬁers are trained exhaustively till saturation on learned encoders after they have been ﬁxed (see Fig. 1). We adopt this protocol to evaluate our approach on a task with real world complexity—namely, inhibiting detection of scene categories in images from the Places365 dataset [28]. 2. Related Work "
105,Neural Network Branching for Neural Network Verification.txt,"Formal verification of neural networks is essential for their deployment in
safety-critical areas. Many available formal verification methods have been
shown to be instances of a unified Branch and Bound (BaB) formulation. We
propose a novel framework for designing an effective branching strategy for
BaB. Specifically, we learn a graph neural network (GNN) to imitate the strong
branching heuristic behaviour. Our framework differs from previous methods for
learning to branch in two main aspects. Firstly, our framework directly treats
the neural network we want to verify as a graph input for the GNN. Secondly, we
develop an intuitive forward and backward embedding update schedule.
Empirically, our framework achieves roughly $50\%$ reduction in both the number
of branches and the time required for verification on various convolutional
networks when compared to the best available hand-designed branching strategy.
In addition, we show that our GNN model enjoys both horizontal and vertical
transferability. Horizontally, the model trained on easy properties performs
well on properties of increased difficulty levels. Vertically, the model
trained on small neural networks achieves similar performance on large neural
networks.","Despite their outstanding performances on various tasks, neural networks are found to be vulner able to adversarial examples (Goodfellow et al., 2015; Szegedy et al., 2013). The brittleness of neural networks can have costly consequences in areas such as autonomous driving, ﬁnance and healthcare. When one requires robustness to adversarial examples, traditional model evaluation ap proaches, which test the trained model on a holdout set, do not sufﬁce. Instead, formal veriﬁcation of properties such as adversarial robustness becomes necessary. For instance, to ensure selfdriving cars make consistent correct decisions even when the input image is slightly perturbed, the required property to verify is that the underlying neural network outputs the same correct prediction for all points within a norm ball whose radius is determined by the maximum perturbation allowed. Several methods have been proposed for verifying properties on neural networks (NN). Bunel et al. (2018) showed that many of the available methods can be viewed as instances of a uniﬁed Branch and Bound (BaB) framework. A BaB algorithm consists of two key components: branching strate gies and bounding methods. Branching strategies decide how the search space is recursively split into smaller space. Bounding methods compute bounds of each split space to tighten the bounds of the ﬁnal objective function over the whole search space. In this work, we focus on improving the branching strategies. By directly working with a general framework, our identiﬁed algorith mic improvements can be combined with any bounding method, leading to potential performance improvement for BaB based veriﬁcation algorithms. Branching strategy has a signiﬁcant impact on the overall problemsolving process, as it directly decides the total number of steps, consequently the total time, required to solve the problem at hand. The quality of a branching strategy is even more important when NN veriﬁcation problems are considered, which generally have a very large search space. Each input dimension or each activation unit can be a potential branching option and neural networks of interest often have high dimensional input and thousands of hidden activation units. With such a large search space, an effective branching strategy could mean a large reduction of the total number of branches required, consequently time required to solve a problem. Developing an effective strategy is thus of signiﬁcant importance to the success of BaB based NN veriﬁcation. 1arXiv:1912.01329v1  [cs.LG]  3 Dec 2019So far, to the best of our knowledge, branching rules adopted by BaB based veriﬁcation methods are either random selection (Katz et al., 2017; Ehlers, 2017) or handdesigned heuristics (Wang et al., 2018b; Bunel et al., 2018; Royo et al., 2019; Bunel et al., 2019). Random selection is gen erally inefﬁcient as the distribution of the best branching decision is rarely uniform. In practice, this strategy often results in exhaustive search to make a veriﬁcation decision. On the other hand, hand designed heuristics often involve a tradeoff between effectiveness and computational cost. For instance, strong branching is generally one of the best performing heuristics for BaB methods in terms of the number of branches, but it is computationally prohibitive as each branching decision requires an expensive exhaustive search over all possible options. The heuristics that are currently used in practice are either inspired by the corresponding dual problem when veriﬁcation is formu lated as an optimization problem (Bunel et al., 2018; Royo et al., 2019) or incorporating the gradient information of the neural network (Wang et al., 2018b). These heuristics normally have better com putational efﬁciency. However, given the complex nature of the search space, it is unlikely that any handdesigned heuristics is able to fully exploit the structure of the problem and the data dis tribution encountered in practice. As mentioned earlier, for large size NN veriﬁcation problems, a slight reduction of branching quality strategy could lead to exponential increase in the total number of branches required to solve the problem. A computationally cheap but high quality branching strategy is thus much needed. In order to exploit the inherent structure of the problem and the data, we propose a novel machine learning framework for designing a branching strategy. Our framework is both computational efﬁ cient and effective, giving branching decisions that are of a similar quality to that of strong branch ing. Speciﬁcally, we make following contributions: We use a graph neural network (GNN) to exploit the structure of the neural network we want to verify. The embedding vectors of the GNN are updated by a novel schedule, which is both computationally cheap and memory efﬁcient. In detail, we mimic the forward and backward passes of the neural network to update the embedding vectors. In addition, the proposed GNN allows a customised schedule to update embedding vectors via shared parameters. That means, once training is done, the trained GNN model is applicable to various veriﬁcation properties on different neural network structures. We train GNNs via supervised learning. We provide ways to generate training data cheaply but inclusive enough to represent branching problems at different stages of a BaB process for various veriﬁcation properties. With the ability to exploit the neural network structure and a comprehen sive training data set, our GNN is easy to train and converges fast. Our learned GNN also enjoys transferability both horizontally and vertically. Horizontally, al though trained with easy properties, the learned GNN gives similar performance on medium and difﬁcult level properties. More importantly, vertically, given that all other parts of BaB algorithms remain the same, the GNN trained on small networks performs well on large networks. Since the network size determines the total cost for generating training data and is positively correlated with the difﬁculty of learning, this vertical transferability allows our framework to be readily applicable to large scale problems. We further enhance our framework via online learning. For a learned branching strategy, it is expected that the strategy can fail to output satisfactory branching decisions from time to time. To deal with this issue, we provide an online scheme for ﬁnetuning the GNN along the BaB process in order to best accommodate the veriﬁcation property at hand. Finally, we supply a dataset on convolutional NN veriﬁcation problems, covering problems at different difﬁculty levels over neural networks of different sizes. We hope that by providing a large problem dataset it could allow easy comparisons among existing methods and additionally encourage the development of better methods. Since most veriﬁcation methods available work on ReLUbased deep neural networks, we focus on neural networks with ReLU activation units in this paper. However, we point out that our framework is applicable to any neural network architecture. 2 R ELATED WORKS "
205,Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons.txt,"It is well-known that standard neural networks, even with a high
classification accuracy, are vulnerable to small $\ell_\infty$-norm bounded
adversarial perturbations. Although many attempts have been made, most previous
works either can only provide empirical verification of the defense to a
particular attack method, or can only develop a certified guarantee of the
model robustness in limited scenarios. In this paper, we seek for a new
approach to develop a theoretically principled neural network that inherently
resists $\ell_\infty$ perturbations. In particular, we design a novel neuron
that uses $\ell_\infty$-distance as its basic operation (which we call
$\ell_\infty$-dist neuron), and show that any neural network constructed with
$\ell_\infty$-dist neurons (called $\ell_{\infty}$-dist net) is naturally a
1-Lipschitz function with respect to $\ell_\infty$-norm. This directly provides
a rigorous guarantee of the certified robustness based on the margin of
prediction outputs. We then prove that such networks have enough expressive
power to approximate any 1-Lipschitz function with robust generalization
guarantee. We further provide a holistic training strategy that can greatly
alleviate optimization difficulties. Experimental results show that using
$\ell_{\infty}$-dist nets as basic building blocks, we consistently achieve
state-of-the-art performance on commonly used datasets: 93.09% certified
accuracy on MNIST ($\epsilon=0.3$), 35.42% on CIFAR-10 ($\epsilon=8/255$) and
16.31% on TinyImageNet ($\epsilon=1/255$).","Modern neural networks are usually sensitive to small, ad versarially chosen perturbations to the inputs (Szegedy et al., 2013; Biggio et al., 2013). Given an image xthat is cor rectly classiﬁed by a neural network, a malicious attacker may ﬁnd a small adversarial perturbation such that the perturbed image x+, though visually indistinguishable from the original image, is assigned to a wrong class with high conﬁdence by the network. Such vulnerability creates security concerns in many realworld applications. Developing a model that can resist small `1perturbations has been extensively studied in the literature. Adversarial training methods (Szegedy et al., 2013; Goodfellow et al., 2015; Madry et al., 2017; Zhang et al., 2019; Ding et al., 2020) ﬁrst generate ontheﬂy adversarial examples of the inputs, then update model parameters using these perturbed samples together with the original labels. While such ap proaches can achieve decent empirical robustness, the eval uation is restricted to a particular (class of) attack method, and there are no formal guarantees whether the resulting model is robust against other attacks (Athalye et al., 2018; Tramer et al., 2020; Tjeng et al., 2019). Another line of algorithms train provably robust models for standard networks by maximizing the certiﬁed radius provided by robust certiﬁcation methods, typically using linear relaxation (Wong & Kolter, 2018; Weng et al., 2018; Mirman et al., 2018; Zhang et al., 2018; Wang et al., 2018; Singh et al., 2018), semideﬁnite relaxation (Raghunathan et al., 2018; Dvijotham et al., 2020), interval bound relax ation (Mirman et al., 2018; Gowal et al., 2018) or their combinations (Zhang et al., 2020b). However, most of these methods are sophisticated to implement and computationally expensive. Besides these approaches, Cohen et al. (2019a); Salman et al. (2019); Zhai et al. (2020) study the certiﬁed guarantee on `2perturbations for Gaussian smoothed classi ﬁers. However, recent works suggest that such methods are hard to extend to the `1perturbation scenario if the input dimension is large. In this work, we propose a new approach by introducing a novel type of neural network that naturally resists local adversarial attacks, and can be easily certiﬁed under `1perarXiv:2102.05363v4  [cs.LG]  14 Jun 2021Towards Certifying `1Robustness using Neural Networks with `1dist Neurons turbation. In particular, we propose a novel neuron called `1dist neuron. Unlike the standard neuron design that uses a linear transformation followed by a nonlinear activa tion, the`1dist neuron is purely based on computing the `1distance between the inputs and the parameters. It is straightforward to see that such a neuron is 1Lipschitz with respect to`1norm, and the neural networks constructed with`1dist neurons (called `1dist nets) enjoy the same property. Based on such a property, we can efﬁciently obtain the certiﬁed robustness for any `1dist net using the margin of the prediction outputs. Theoretically, we investigate the expressive power of `1 dist nets and their robust generalization ability. We ﬁrst prove a Lipschitzuniversal approximation theorem which shows that`1dist nets can approximate any 1Lipschitz function (with respect to `1norm) arbitrarily well. We then give upper bounds of robust test error, which would be small if the`1dist net learns a large margin classiﬁer on the training data. These results demonstrate the excellent expressivity and generalization ability of the `1dist net function class. While`1dist nets have nice theoretical guarantees, train ing such a network is still challenging. For example, the gradient of the parameters for `1norm distance is sparse, which makes the optimization difﬁcult. In addition, we ﬁnd that commonly used tricks and techniques in conventional network training cannot be taken for granted for this funda mentally different architecture. We address these challenges by proposing a holistic strategy for `1dist net training. Speciﬁcally, we show how to initialize the model param eters, apply proper normalization, design suitable weight decay mechanism, and overcome the sparse gradient prob lem via smoothed approximated gradients. Using the above methods, training an `1dist net is just as easy as training a standard network without any adversarial training, even though the resulting model is already provably robust. Furthermore, the `1dist net has wide adaptability by serv ing as a robust feature extractor and combining itself with conventional networks for practical applications. After building a simple 2layer perceptron on top of an `1dist net, we show that the model allows fast training and certiﬁcation, and consistently achieves stateoftheart certiﬁed robust ness on a wide range of classiﬁcation tasks. Concretely, we reach 93.09% certiﬁed accuracy on MNIST under pertur bation= 0:3,79.23% on FashionMNIST under = 0:1, 35.42% on CIFAR10 under = 8=255, and 16.31% on TinyImageNet under = 1=255. As a comparison, these re sults outperform the previous bestknown results (Xu et al., 2020a), in which they achieve 33.38% certiﬁed accuracy on CIFAR10 dataset, and achieve 15.86% certiﬁed accuracy on TinyImageNet using a WideResNet model which is 33 times larger than the`1dist net.Our contributions are summarized as follows: •We propose a novel neural network using `1dist neu rons, called`1dist net. We show that any `1dist net is 1Lipschitz with respect to `1norm, which directly guarantees the certiﬁed robustness (Section 3). •In the theoretical part, we prove that `1dist nets can approximate any 1Lipschitz function with respect to `1norm. We also prove that `1dist nets have a good robust generalization ability (Section 4). •In the algorithmic part, we provide a holistic training strategy for `1dist nets, including parameter initial ization, normalization, weight decay and smoothed approximated gradients (Section 5). •We show how to combine `1dist nets with standard networks and obtain robust models more effectively (Section 6). Experimental results show that we can consistently achieve stateoftheart certiﬁed accuracy on MNIST, FashionMNIST, CIFAR10 and TinyIma geNet dataset (Section 7). •Finally, we provide all the implementation details and codes at https://github.com/zbh2047/L infdistnet. 2. Related Work "
278,Specification-Guided Safety Verification for Feedforward Neural Networks.txt,"This paper presents a specification-guided safety verification method for
feedforward neural networks with general activation functions. As such
feedforward networks are memoryless, they can be abstractly represented as
mathematical functions, and the reachability analysis of the neural network
amounts to interval analysis problems. In the framework of interval analysis, a
computationally efficient formula which can quickly compute the output interval
sets of a neural network is developed. Then, a specification-guided
reachability algorithm is developed. Specifically, the bisection process in the
verification algorithm is completely guided by a given safety specification.
Due to the employment of the safety specification, unnecessary computations are
avoided and thus the computational cost can be reduced significantly.
Experiments show that the proposed method enjoys much more efficiency in safety
verification with significantly less computational cost.","Artiﬁcial neural networks have been widely used in machine learning systems. Though neural networks have been show ing effectiveness and powerful ability in resolving complex problems, they are conﬁned to systems which comply only to the lowest safety integrity levels since, in most of time, a neural network is viewed as a black box without effective methods to assure safety speciﬁcations for its outputs. Neural networks are trained over a ﬁnite number of input and out put data, and are expected to be able to generalize to produce desirable outputs for given inputs even including previously unseen inputs. However, in many practical applications, the number of inputs is essentially inﬁnite, this means it is im possible to check all the possible inputs only by performing experiments and moreover, it has been observed that neural networks can react in unexpected and incorrect ways to even slight perturbations of their inputs [1], which could result in unsafe systems. Hence, methods that are able to provide for mal guarantees are in a great demand for verifying speciﬁca Authors are with the Department of Electrical Engineering and Com puter Science, Vanderbilt University, Nashville, Tennessee 37212, USA. Email: xiangwming@gmail.com (Weiming Xiang); HoangDung Tran (trhoangdung@gmail.com); taylor.johnson@gmail.com (Taylor T. Johnson).tions or properties of neural networks. Verifying neural net works is a hard problem, even simple properties about them have been proven NPcomplete problems [2]. The difﬁculties mainly come from the presence of activation functions and the complex structures, making neural networks largescale, nonlinear, nonconvex and thus incomprehensible to humans. The importance of methods of formal guarantees for neural networks has been wellrecognized in literature. There ex ist a number of results for veriﬁcation of feedforward neural networks, especially for Rectiﬁer Linear Unit (ReLU) neural networks, and a few results are devoted to neural networks with broad classes of activation functions. Motivated to gen eral class of neural networks such as those considered in [3], our key contribution in this paper is to develop a speciﬁcation guided method for safety veriﬁcation of feedforward neural network. First, we formulate the safety veriﬁcation problem in the framework of interval arithmetic, and provide a com putationally efﬁcient formula to compute output interval sets. The developed formula is able to calculate the output inter vals in a fast manner. Then, analogous to other stateoftheart veriﬁcation methods, such as counterexampleguided abstrac tion reﬁnement (CEGAR) [4] and property directed reachabil ity (PDR) [5], and inspired by the MooreSkelboe algorithm [6], a speciﬁcationguided algorithm is developed. Brieﬂy speaking, the safety speciﬁcation is utilized to examine the existence of intersections between output intervals and un safe regions and then determine the bisection actions in the veriﬁcation algorithm. By making use of the information of safety speciﬁcation, the computation cost can be reduced sig niﬁcantly. We provide experimental evidences to show the ad vantages of speciﬁcationguided approach, which shows that our approach only needs about 3%–7% computational cost of the method proposed in [3] to solve the same safety veriﬁca tion problem. 2 Related Work "
335,Security Analysis for Distributed IoT-Based Industrial Automation.txt,"With ever-expanding computation and communication capabilities of modern
embedded platforms, Internet of Things (IoT) technologies enable development of
Reconfigurable Manufacturing Systems---a new generation of highly modularized
industrial equipment suitable for highly-customized manufacturing. Sequential
control in these systems is largely based on discrete events, while their
formal execution semantics is specified as Control Interpreted Petri Nets
(CIPN). Despite industry-wide use of programming languages based on the CIPN
formalism, formal verification of such control applications in the presence of
adversarial activity is not supported. Consequently, in this paper we focus on
security-aware modeling and verification challenges for CIPN-based sequential
control applications. Specifically, we show how CIPN models of networked
industrial IoT controllers can be transformed into Time Petri Net (TPN)-based
models, and composed with plant and security-aware channel models in order to
enable system-level verification of safety properties in the presence of
network-based attacks. Additionally, we introduce realistic channel-specific
attack models that capture adversarial behavior using nondeterminism. Moreover,
we show how verification results can be utilized to introduce security patches
and motivate design of attack detectors that improve overall system resiliency,
and allow satisfaction of critical safety properties. Finally, we evaluate our
framework on an industrial case study.","Advanced capabilities of smart Internet of Things (IoT) devices have lead to their widespread adoption in industrial automation system, rapidly advancing reconﬁgurable manufac turing [1]; the rise of the fourth industrial revolution, known as Industry 4.0 [2], introduces the new era of highlycustomized (rather than highlyserialized) manufacturing [3]. In this vi sion, manufacturing resources are highly modularized, pro viding the necessary ﬂexibility to adapt to dynamical market demands. Efﬁcient structural and functional changes are sup ported by Reconﬁgurable Manufacturing Systems (RMS) that can be conﬁgured adhoc with little or zero downtime [4]. The foundation of RMS are modules controlled by smart In dustrial IoT (IIoT)enabled controllers. IIoT endpoints (some times referred to as industrial assets ) are heterogeneous by deﬁnition—they represent multivendor components whose deployment environment dynamically changes depending on the process needs and current conﬁguration of RMS. Fur thermore, a plethora of different communication technologies (wired and wireless) and protocols are employed [5]. Seamless reconﬁguration, integration and reliable functioning of RMS requires that components are highly autonomous. Speciﬁcally, they must be capable of seamlessly communicating with each other using compatible protocols (integrability), exchanging both lowlevel controlrelated and highlevel processbound information (interoperability) and to interact with each other in different ways to enable operation in a plethora of conﬁg urations (composability) [6]. Reconﬁgurability is naturally supported by distributed con trol architectures; conventionally centralized controllers are responsible of all aspects of control—from lowlevel event signaling, to highlevel coordination. Their complexity hinders reconﬁgurability both from the hardware perspective (i.e., requiring component rewiring), and the software aspect (i.e., having to ensure the control software is aware of and functions correctly under the new hardware conﬁguration). Thus, the new generation of smart manufacturing resources must exploit not only functionallyrequired components (such as sensors and actuators) but also intrinsic computation and communica tion capabilities of IIoTenabled controllers in order to enable a higher level of automation and autonomy. Control distribution enables decoupling of ﬁnegrained details about how control over the speciﬁc physical resource is performed, from the re sources coordination problem which only needs to worry about what the manufacturing resources are capable of performing.arXiv:2006.00044v1  [eess.SY]  29 May 20202 Local CIPN controller models Channel/ Attacker  modelLocal TPN  controller models 11 CIPN1CIPN→TPN  transformation1ctrl 1ctrlTPN1ctrl 1plant 1plant 1TPNplant Plant models TPNch Executable code (e.g., C) Code generation 1 Inclusion of security  mechanisms1 23 Resiliency analysis (safety/security verification) Fig. 1. Our methodology for resilient IIoTbased distributed automation—in Phase 1, the composition of existing distributed control models, which are used to generate executable code for IIoT controllers, with channel and plant models is used to formally verify properties of interest in Phase 2. Finally, in Phase 3, the results of the security analysis are used to enhance system resiliency, by adding suitable security mechanisms during code generation. However, the networked nature of the new generation of distributed automation systems makes them susceptible to networkbased attacks [7], similar to security vulnerabilities reported in other cyberphysical systems domains (e.g., [8]). For example, an adversary may inject false events [9], de lay or deny network access to legitimate controllers [10], or manipulate control commands [11] sent over unsecure communication channels. On the one hand, providing security guarantees is critical in distributed sequential control systems where progress is directly impacted by communication un availability. Yet, despite devastating effects such attacks could have on operation of distributed industrial automation systems, existing approaches to securing such systems are somewhat ad hoc; commonly, the beneﬁts of included security mechanisms for control performance (i.e., QualityofControl—QoC) are unclear and hard to evaluate if no formal system analysis can be performed. Consequently, to enable building of secure and correctbydesign RMS, in this work we introduce efﬁcient techniques for systematic security analysis of distributed con trol applications deployed on IIoTenabled local controllers (LCs). We also show how results of the security analysis can be used to improve automation performance and safety guarantees in the presence of attacks, by adding suitable security mechanisms that address the detected vulnerabilities. This results in the overall framework, shown in Fig. 1, for formal safety analysis and patching of distributed sequential automation systems under adversarial inﬂuences. Coordination between components in a large fraction of IoT systems is based on discrete events. While a plethora of formal modeling frameworks is employed under the umbrella of IoT (e.g., [12], [13], [14]), industrial automation systems are com monly based on GRAFCET (IEC 60848)/SFC (IEC 611313) control designs, and consequently on the underlying formal semantics of Control Interpreted Petri Nets (CIPN). Therefore, we focus on formal security analysis of IIoTenabled con trollers that are described using CIPNs; such controllers may be developed directly, or automatically derived using methods for distribution of existing centralized sequential automation designs (e.g., [15]), which allow for deployment of legacy control applications over IIoTenabled smart controllers.While inherent determinism of CIPNs is not a limitation when the formalism is used to specify controllers’ behaviors, it prevents the use of CIPNs to model malicious actions [7]. On the other hand, the sister formalism of Time Petri Nets (TPN) supports nondeterminism, which makes it a great candidate for securityaware modeling. Thus, for the ﬁrst phase of our secu rity analysis, we introduce methods for automatic transforma tion of domainspeciﬁc CIPNbased controller speciﬁcations (i.e., designs) into TPNcompliant representations. These TPN models enable closedloop system modeling and analysis, by composing them with corresponding nondeterministic plant and securityaware communication channel models; we show how such securityaware models can be developed with the desired level of abstraction that allows us to capture impacts of attacks on automation performance. While our framework generally supports any communication channel implementa tion, we focus on the IEEE 802.15.4based implementation featured in our evaluation setup. In Phase 2, we employ open veriﬁcation tools (e.g., [16]) to perform systemwide veriﬁcation of safety and QoCrelevant properties in the presence of attacks, based on the afore mentioned securityaware closedloop system model; it is important to highlight that we make no assumptions about the attacker’s choice among all possible malicious actions nor the times when they (i.e., attack actions) may occur. By enabling security analysis within the same family of formalisms (i.e., using a formalism that is closely related to the formalism used to design controllers), we provide convenient domainspeciﬁc interpretation of analysis results. This allows us to exploit veriﬁcation results in Phase 3 to orchestrate security patches in code generation which is performed based on original CIPNbased models. Finally, we show the applicability of our methodology on a realworld industrial case study—a security analysis of an IIoTenabled manipulator system. Speciﬁcally, the contributions of this work are as follows: Securityaware framework for veriﬁcation of systemlevel properties for distributed discreteevent controllers (based on CIPNs) in the presence of networkbased attacks; TPNbased nondeterministic modeling of networkbased attacks on distributed controller communication, with em phasis on capturing impacts on automation performance; Extension of the control software development cycle from securityaware analysis to ﬁrmware patching, in order to ensure correct operation in the presence of attacks; Fullstack proofofconcept case study based on industry grade components demonstrating applicability of the de veloped secure automation framework. This paper is organized as follows. Sec. II gives an overview of relevant related work, while Sec. III provides the problem deﬁnition with emphasis on distributed IIoTbased automa tion. Sec. IV introduces TPNbased securityaware modelings and Sec. V derives the securityaware communication model. Speciﬁcation and veriﬁcation of relevant formal properties is presented in Sec. VI, as well as the loop closure from veriﬁcation to code generation to include security patches and improve system resiliency. Industrial case studies are discussed in Sec. VII, before concluding remarks (Sec. VIII).3 II. R ELATED WORK "
458,Neural Models of the Psychosemantics of `Most'.txt,"How are the meanings of linguistic expressions related to their use in
concrete cognitive tasks? Visual identification tasks show human speakers can
exhibit considerable variation in their understanding, representation and
verification of certain quantifiers. This paper initiates an investigation into
neural models of these psycho-semantic tasks. We trained two types of network
-- a convolutional neural network (CNN) model and a recurrent model of visual
attention (RAM) -- on the ""most"" verification task from \citet{Pietroski2009},
manipulating the visual scene and novel notions of task duration. Our results
qualitatively mirror certain features of human performance (such as sensitivity
to the ratio of set sizes, indicating a reliance on approximate number) while
differing in interesting ways (such as exhibiting a subtly different pattern
for the effect of image type). We conclude by discussing the prospects for
using neural models as cognitive models of this and other psychosemantic tasks.","Semantics – the scientiﬁc study of meaning – has traditionally studied the truthconditions of sen tences and how the meanings of subsentential ex pressions combine to generate them. How ex actly truthconditions are represented and then de ployed in concrete acts of production and compre hension has often not been seen as belonging to the purview of semantics properly. A recent line of work, however, has argued that the mental representation of the meanings of expressions bias behavior in cognitive tasks in ways that allow us to adjudicate between truthconditionally equivalent but representation ally distinct semantic theories. In particular, Piet roski et al. (2009) considered the veriﬁcation of the sentence “Most of the dots are yellow”. The meaning of ‘most’ can be expressed in distinct, buttruthconditionally equivalent ways. For instance (where, in the running example, Ais the set of dots, and Bthe set of yellow things): Jmost K(A)(B) = 1 iffjA\Bj>jAnBj Jmost K(A)(B) = 1 iff there is f:AnB! A\Bthat is onetoone, but not onto The former says that the number of dots which are yellow is larger than the number of nonyellow dots, while the latter says that the former can be paired off with the latter, with some yellow dots remaining. Whilst these representations are truth conditionally equivalent, each is associated with a distinct veriﬁcation strategy to evaluate those truth conditions. When deciding whether most of the dots are yellow: the former representation is asso ciated with an algorithm for computing and com paring two cardinalities, while the latter represen tation is associated with an algorithm for checking whether a certain correspondence between yellow and nonyellow dots exists. Whilst a speaker may be capable of implementing many possible strate gies, Pietroski et al. (2009)’s claim is that, all other things being equal, speakers are biased towards us ing the default strategy associated with their rep resentation. Pietroski et al. (2009) sought to determine whether speakers prefer one of the above repre sentations by testing which veriﬁcation strategy they typically use. By manipulating the arrange ment of the dots in images against which ‘most’ was veriﬁed, they created conditions which should ease the implementation of one of the strategies (e.g. dots arranged in pairs should favour corre spondence). They found no difference in veriﬁ cation accuracy between three of the four image types used. Participants were signiﬁcantly more accurate on the remaining image type, which con sisted of two paired columns of colour sorted dots.arXiv:1904.02734v1  [cs.CL]  4 Apr 2019Their analysis suggested that the participants used the columns’ lengths as a proxy for set cardinality, rather than using a correspondence strategy. The results of the remaining three image types were explaind very well by a psychophyiscal model of approximate number. Given that this system can not be used to implement a correspondence strat egy, they concluded that the meaning of ‘most’ is best represented in the former way.1 In this paper, we begin to develop robust mech anistic cognitive models of their sentence veriﬁca tion task to help elucidate the factors underlying the psychosemantics of ‘most’. In particular, we are interested in the following question: do var ious neural models show the potential to be de veloped into good cognitive models of the mean ing of ‘most’? A good cognitive model does at least two things: (i) ﬁts human data well and (ii) has movable parameters that enable new predic tions to be made. To address this question, we subjected two different classes of models – convo lutional networks and recurrent models of visual attention – to the experimental design from Piet roski et al. (2009), together with an additional and novel manipulation for ‘task duration’ (inspired by Register et al. (2018)). This allows us to assess the models along both dimensions (i) and (ii). Our key contributions are: Subjecting neural models to prominent tasks from the psychosemantics literature. Operationalizing ‘task duration’ in two dis tinct ways: depth of a convolutional network, and the number of glimpses in a model of vi sual attention. The key ﬁndings from our experiments are: Both models exhibit patterns of behavior qualitatively similar to humans, including sensitivity to dot ratio. The psychophysical model of approximate number ﬁts model data well, with parameters not too far from human participants. Model performance is effected by the image type in a subtly different way than human performance. 1See Lidz et al. (2011) for further research in this direc tion, distinguishing between more candidate representations.The effect of task duration is more robust for the convolutional networks than for visual at tention. After discussing related work in the next sec tion, we outline the hypotheses of our experiment, before a full explanation of our methods and re sults. We conclude by discussing the results and outlining future work. 2 Related Work "
397,CS-Rep: Making Speaker Verification Networks Embracing Re-parameterization.txt,"Automatic speaker verification (ASV) systems, which determine whether two
speeches are from the same speaker, mainly focus on verification accuracy while
ignoring inference speed. However, in real applications, both inference speed
and verification accuracy are essential. This study proposes cross-sequential
re-parameterization (CS-Rep), a novel topology re-parameterization strategy for
multi-type networks, to increase the inference speed and verification accuracy
of models. CS-Rep solves the problem that existing re-parameterization methods
are unsuitable for typical ASV backbones. When a model applies CS-Rep, the
training-period network utilizes a multi-branch topology to capture speaker
information, whereas the inference-period model converts to a time-delay neural
network (TDNN)-like plain backbone with stacked TDNN layers to achieve the fast
inference speed. Based on CS-Rep, an improved TDNN with friendly test and
deployment called Rep-TDNN is proposed. Compared with the state-of-the-art
model ECAPA-TDNN, which is highly recognized in the industry, Rep-TDNN
increases the actual inference speed by about 50% and reduces the EER by 10%.
The code will be released.","Automatic speaker veriﬁcation (ASV) systems determine whether two speeches are from the same speaker or not. The frontend mod els of ASV systems to extract speaker embeddings from speech utter ances to represent the corresponding speaker, such as ivector [1], d vector [2], and xvector [3]. The backend functions of ASV systems calculate similarity scores between two embeddings, e.g., probabilis tic linear discriminant analysis(PLDA)[4] and cosine similarity. Currently, ASV systems are widely used in many realworld ap plications, such as criminal investigation, payment [5], and realtime meeting recordings [6]. In those realistic scenarios, the inference speed is becoming increasingly important. Recently, the use of multibranch topology (e.g., ResNet [7] has a convolutional branch and a shortcut branch) [8, 9] to improve the theoretical efﬁciency and accuracy of models has been reported. These designs capture multiscale information through multitype Junhai Xu is the corresponding author. Thanks to NSFC of China (No.61876131, No. U1936102), Key R&D Program of Tian jin (No.19ZXZNGX00030). Regional Innovation Cooperation Project of Sichuan (Grant No.22QYCX0082)convolution kernels with a small number of channels. Additionally, many studies [10, 11] have used multibranch designs to strengthen the timedelay neural network [3](TDNN). ECAPATDNN[10] with multibranch topology has exhibited excellent performance in recent ASV competitions [12, 13, 14, 15, 16]. Unfortunately, the actual in ference speed of the multibranch designs is typically much slower than its theoretical speed. The main reason is that the multibranch designs increase the memory access cost (MAC) and are unfriendly for parallel computing [17]. Therefore, this topology does not solve the contradiction of speed and performance in the real applications. To improve the actual inference speed for the multibranch mod els, Ding et al. [18] proposed a reparameterization approach to con vert the multibranch network to a plain topology in the inference period. However, their strategy is based on the structure of “conv bnactivation,” but the mainstream TDNNbased ASV models rely on the “convactivationbn” structure to realize good veriﬁcation ac curacy (we conﬁrm that this is reasonable in Section 4.2). Many successful implementations of ASV have adopted the structure, such as the xvector [3] (implemented by Kaldi [19]), ETDNN [20], and ECAPATDNN [10], etc. In those ASV models, since their convo lutional layer is not adjacent to the batch normalization (BN) [21] layer, they cannot be reweighted through Ding’s method. In this study, we propose the crosssequential reparameterization (CSRep, Fig. 1) to increase the inference speed and veriﬁcation accuracy of ASV systems, especially for the models based on the “convactivationbn”1architecture. Speciﬁcally, we build a multi branch network with the “convactivationbn” structure during the training period. In the inference period, CSRep adjusts the order of modules of the sequential layer to the “bnconvactivation” loss lessly, causing the BN to be adjacent to the convolutional layer. However, the “bnﬁrst”2case will generate inﬂuences on each channel of the convolutional layer. We propose the “bnﬁrst” re parameterization method to model these inﬂuences by the discrete convolution operator. Then, CSRep adopts this approach to con vert the multibranch network to a TDNNlike plain topology while maintaining the multiscale information capturing ability. There fore, the model adopted CSRep achieves an efﬁcient architecture with friendly parallel computing. The contributions of this study are listed as follows. (1) We pro 1The sequential layer [22] is a layer including multiple modules by order. For example, the “convactivationbn” is a sequential layer with the order of 1D (TDNN) or 2D convolutional layer, activation function, and BN layer. Fig. 1 (a) shows the diagram of the sequential layer with three branches. 2The “bnﬁrst” means the case that the BN layer precedes the convolu tional layer in the sequential layer. e.g., the “bnconvactivation.”arXiv:2110.13465v2  [cs.SD]  4 Apr 2022(a)  (b)  (c)  (d) Fig. 1 :The topology changes for our model when adopting the proposed CSRep: (a) original topology of model, (b) step, (c) steps 2 and 3, and (d) step 4. pose CSRep to increase the inference speed, decrease the parame ters, and reduce the ﬂoatingpoint operations (FLOPs) of networks with the “convactivationbn” structure while maintaining the accu racy. (2) We also combine the “bnﬁrst” and “convﬁrst” [18] re parameterization as an outofthebox method, which can be conve niently used for all types of sequential layers (“convﬁrst” and “bn ﬁrst”) to improve the inference speed. (3) Based on CSRep, we construct RepTDNN. It achieves the stateoftheart performance (1.09% EER on V oxCeleb1test, without any data augmentation), and its inference speed is 50% faster than the stateoftheart model ECAPATDNN3. The remainder of the paper is organized as follows. Related works are brieﬂy introduced in Section 2, the CSRep method and the novel RepTDNN model are detailed in Section 3, experimental results are presented and analyzed in Section 4, and the conclusions are given in Section 5. 2. RELATED WORK "
337,Confidence Composition for Monitors of Verification Assumptions.txt,"Closed-loop verification of cyber-physical systems with neural network
controllers offers strong safety guarantees under certain assumptions. It is,
however, difficult to determine whether these guarantees apply at run time
because verification assumptions may be violated. To predict safety violations
in a verified system, we propose a three-step confidence composition (CoCo)
framework for monitoring verification assumptions. First, we represent the
sufficient condition for verified safety with a propositional logical formula
over assumptions. Second, we build calibrated confidence monitors that evaluate
the probability that each assumption holds. Third, we obtain the confidence in
the verification guarantees by composing the assumption monitors using a
composition function suitable for the logical formula. Our CoCo framework
provides theoretical bounds on the calibration and conservatism of
compositional monitors. Two case studies show that compositional monitors are
calibrated better than their constituents and successfully predict safety
violations.","Autonomouscyberphysicalsystems,suchasselfdrivingc arsand service robots, are increasingly deployed in complex and sa fety criticalenvironmentsinoursociety[10,40,48].Recently ,thebreak throughcapabilitiesinhandlingsuchenvironmentscamefr omthe useoflearning components,which may behaveunpredictably .To consistentlyrelyonsuchcapabilities,oneneedstoensure thatthe system wouldnottoendanger thelivesand propertyaroundit ,or atleastthatanearlyenoughwarningisgiventoavertthedis aster. When assuring a complex cyberphysical system, one can ob tain strong safety guarantees from closedloopreachabili ty veriﬁ cation, recently extended to explicitlycheck neural netwo rk (NN) controllers [18, 45]. To provide its guarantees, the veriﬁc ation re lies on assumptions aboutsystem’s dynamics, perception, a nd en vironment.Shouldthesystemﬁnditselfincircumstancesno tmatch ing these assumptions,the veriﬁcation guarantees are void — and remarkably diﬃcult to reobtain at run time due to limited sc ala bility.On another front, many runtime monitoring techniques were developed to detect anomalies, such as model inconsistenci es and outofdistributionsamples[3,5,28].Thesetoolscanpro videvalu able situational insights, but their outputs often lack a di rect con nection to the veriﬁcation guarantees or systemlevel safe ty. For example, it is not clear to which extent an outofdistribut ion im age of a stop sign invalidates a collisionsafety guarantee for an autonomouscar. Thus,itisbothchallengingandimportanttoquantifyandmo n itorthetrustindesigntimeveriﬁcationguaranteesatrun time.In particular,itisvitaltoknowwhentheguaranteesnolonger apply, soastoswitchtoabackupcontroller,executearecoveryman euver, or ask for human assistance. The monitoring of veriﬁcation g uar antees has the potential to predict otherwise unforeseen fa ilures insituations forwhich thesystem wasnot trainedor designe d. Tomonitorveriﬁcationguarantees,weproposequantifying the conﬁdence in theassumptions of veriﬁcation. By conﬁdence we meananestimateoftheprobabilitythattheassumptionhold s.Al thoughanassumptionmaynotbedirectlyobservable,itsmon itor would over time accumulate conﬁdence, which, if properly ca li brated,wouldbeclosetothetruechanceofsatisfyingtheas sump tiongiventheobservations.Ifalltheassumptionsaresati sﬁed,our veriﬁcationretroactivelyguaranteessafety.Suchassump tionscan be monitored with oﬀtheshelf techniques [5, 7, 32, 39, 47] , and their conﬁdences would be combined into a single conﬁdence i n the guarantees. In a safetycritical system, this conﬁdenc e should notbeoverestimated. Thispaperintroducesthe C/o.scC/o.scframeworkforco mposingco nﬁ dencesfrom monitors of veriﬁcation assumptions, consisting of three steps: (i) verifythe system under explicit assumptions, such thatapropositionalformulaovertheseassumptionsentail sthesys tem’ssafety,(ii)buildawellcalibrated conﬁdencemonitor foreach assumption, (iii) use a composition function informed by the for mula from the ﬁrst step to combine the monitor outputs into a composed conﬁdence. This conﬁdence quantiﬁes the chance th at theveriﬁcationguarantees applyat thatmoment. We develop the theoretical conditions under which the com posed conﬁdence is calibrated and conservative, up to a boun ded error, with respect to the true probability of safety. These con ditions are that (a) the system model under veriﬁcation can e x plain most safe behaviors, (b) a violation of assumptions wo uld likely lead to a failure, and (c) the composition function is cali brated/conservativewithrespecttotheassumptions.Weal soproveICCPS ’22,May 04–06,2022, Milan, Italy Ruchkin etal. calibrationerrorboundsfortwocompositionfunctions—pr oduct andweightedaverage—andaconservatismboundfortheprodu ct. Weevaluate C/o.scC/o.scontwosystemswithNNcontrollers:amoun tain car and an underwater vehicle. Experiments show that ou r compositional monitors are useful for safety prediction, o utper form the individual monitors, and can be tuned for conservat ism. Our datadriven composition functions improve the perform ance furtherif provided thedatarelatingthemonitorsand theas sump tions. To summarize,this papermakes fourcontributions: •TheC/o.scC/o.scframework for composing conﬁdence monitors ofveriﬁcationassumptionswithﬁvecompositionfunctions . •Suﬃcient conditions for bounded calibration of composite conﬁdencetothesafetychance,withtheexpectationsfrom models,assumptions,monitors,and compositionfunctions . •Upperboundsontheexpectedcalibrationerroroftwocom positionfunctions,and theconservatism error ofone. •Two case studies that demonstrate the utility of the frame work and thetradeoﬀs betweencompositionfunctions. The rest of the paper proceeds as follows. The necessary back  ground on veriﬁcation and monitoring is given in the next sec  tion. Section 3 surveys the existing research. Section 4 pre sents the key contributions: the framework, the endtoend calib ration conditions,andboundsontheerrorsofcompositionfunctio ns.Sec tion5describestwocasestudiesandtheexperimentalresul ts.The paperwraps upwitha brief discussion inSection6. 2 BACKGROUND Herewedescribethepreliminaries ofveriﬁcationandmonit oring. 2.1 Veriﬁcation D/e.sc/f.sc/i.sc/n.sc/i.sc/t.sc/i.sc/o.sc/n.sc 1 (S/y.sc/s.sc/t.sc/e.sc/m.sc). Asystem/u1D460=(/u1D44B,/u1D44B0,/u1D44C,/u1D448,ℎ,/u1D439/u1D451,/u1D439/u1D45A) consists ofthe following elements: •State space /u1D44B: continuous, unbounded, ﬁnitedimensional, containing states /u1D465(which includethediscretetime) •Initial states /u1D44B0⊂/u1D44B •Observationspace /u1D44C,containing observations /u1D466.alt •Actionspace /u1D448 •Controllerℎ:/u1D44C→/u1D448,implemented witha neural network •Dynamics models /u1D439/u1D451: a setof functions /u1D453/u1D451:/u1D44B,/u1D448→/u1D44B •Measurement models /u1D439/u1D45A: aset of functions /u1D453/u1D45A:/u1D44B→/u1D44C Asystemdeterminesasetofstatetraces /u1D47F(/u1D460)andasetofobser vation traces /u1D480(/u1D460)resulting from executing every combination of functions from /u1D439/u1D451×/u1D439/u1D45Aon every initial state in /u1D44B0indeﬁnitely. A particularrealizationofasystemisapairofstateandobse rvation vectors/u1D499,/u1D49A.altthatoccurforspeciﬁc /u1D4650∈/u1D44B0,/u1D453/u1D451∈/u1D439/u1D451,and/u1D453/u1D45A∈/u1D439/u1D45A. Asafety property /u1D711is a Boolean predicate over traces: /u1D711(/u1D499) ∈ {T,F}.Aproperty /u1D711issatisﬁedonsystem /u1D460,denoted/u1D460|=/u1D711,iﬀevery tracefrom thatsystem satisﬁes /u1D711:∀/u1D499∈/u1D47F(/u1D460),/u1D711(/u1D499)=T.Thus,this paperconsidersarbitrarydeterministictemporalsafetyp roperties. Averiﬁcation assumption /u1D434is a restrictiononthesystem’s “un knowns”—thestatesandmodelsofthesystem—so, /u1D434⊆/u1D44B×/u1D439/u1D451× /u1D439/u1D45A.Theassumptionholdsonatrace (/u1D499,/u1D49A.alt)ifforanycombination of/u1D4650,/u1D453/u1D451,and/u1D453/u1D45Athatrealizes this traceitis truethat (/u1D4650,/u1D453/u1D451,/u1D453/u1D45A) ∈ /u1D434.Whenassumptions /u1D4341.../u1D434/u1D45Barecombinedwithapropositionallogical formula /u1D713,/u1D434/u1D713=/u1D713(/u1D4341.../u1D434/u1D45B)is also an assumption. The meaning of propositional operators (∧,∨,¬,=⇒ )is deﬁned by the corresponding set operations (intersection for ∧, union for ∨, etc). A system /u1D460=(/u1D44B,/u1D44B0,/u1D44C,/u1D448,ℎ,/u1D439/u1D451,/u1D439/u1D45A)under assumption /u1D434= (/u1D44B′ 0,/u1D439′ /u1D451,/u1D439′/u1D45A), denoted as /u1D460/u1D434, is an intersection of the initial states andrespectivemodels: /u1D460/u1D434=(/u1D44B,/u1D44B0∩/u1D44B′ 0,/u1D44C,/u1D448,ℎ,/u1D439/u1D451∩/u1D439′ /u1D451,/u1D439/u1D45A∩/u1D439′/u1D45A). Foragivensystem /u1D460,averiﬁcationresultofproperty /u1D711,denoted as/u1D449/u1D460,/u1D711,isa functionthatmaps anyassumption /u1D434to{T,F}.Itrep resentstheoutcomeofaveriﬁcationeﬀortunderthatassump tion, regardless of the exact method. Value Tis assigned only if /u1D711was guaranteedbytheveriﬁcationalgorithm,whereas Fisassigned in all the other cases (counterexample exists, uncertainty to o high, timelimitreached,etc).Sinceveriﬁcationisoverapprox imateand exhaustive, it never produces a false safety outcome: /u1D449/u1D460,/u1D711(/u1D434)= T=⇒/u1D460/u1D434|=/u1D711. Such anassumption /u1D434is calledsuﬃcient for/u1D711. 2.2 Conﬁdence Monitoring Intuitively,wewanttocomputethe conﬁdence in(i.e., anestimate of the probability of) the system satisfying a safety proper ty/u1D711in the future given a preﬁx of observations /u1D49A.alt. Conﬁdences are com puted by CPS monitors in uncertain conditions, when the exac t state, dynamics, and measurement model are not known. There  fore, we represent the selection of the actual system as a ran dom samplingofthesystem’sunknowns—theinitialstate /u1D4650,dynamics /u1D453/u1D451,andmeasurement function /u1D453/u1D45A—fromsomeunknowndistribu tionDover/u1D44B,/u1D439/u1D451,and/u1D439/u1D45A. Onceweﬁxthedistribution D,itinducesthedistribution D/u1D499,/u1D49A.alt on the system’s realization (/u1D499,/u1D49A.alt). Therefore, our monitoring goal istoestimatetheprobabilityofsafetygivenobservations uptothe current moment, namely Pr(/u1D499,/u1D49A.alt)∼D(/u1D499,/u1D49A.alt)/parenleftbig/u1D711(/u1D499)=T|/u1D49A.alt1../u1D45B/parenrightbig, where /u1D49A.alt1../u1D45Bmeans theﬁrst /u1D45Belements of /u1D49A.alt.Wepursuethisgoal bymon itoring conﬁdence in assumptions. Since an assumption /u1D434can be seen as a predicate over random (/u1D4650,/u1D453/u1D451,/u1D453/u1D45A) ∼ D, its satisfaction is alsorandom: /u1D434∼ D/u1D434,whereD/u1D434is inducedby D. A conﬁdence monitor /u1D440:/u1D480→ [0,1]for assumption /u1D434takes /u1D49A.alt1../u1D45Bandoutputsitsestimateof Pr/u1D49A.alt∼D/u1D49A.alt(/u1D49A.alt1../u1D45B∈/u1D480(/u1D460/u1D434)),thatis,its degreeofbeliefthattheobservationscamefromasystemwhe re/u1D434 holds.Themonitor’soutput, /u1D440(/u1D49A.alt),isstochasticbecauseitdepends on/u1D49A.alt.Sincemonitorsestimateprobabilities,wemeasurethequa lity ofmonitorsusingthreetypesofcalibrationerrorwithresp ectto/u1D434: •Expectedcalibrationerror (/u1D438/u1D436/u1D438): /u1D438/u1D436/u1D438(/u1D440,/u1D434):=E /u1D49A.alt∼D/u1D49A.alt[|Pr/u1D434∼D/u1D434/parenleftbig/u1D434|/u1D440(/u1D49A.alt)/parenrightbig−/u1D440(/u1D49A.alt)|] •Maximumcalibrationerror (/u1D440/u1D436/u1D438): /u1D440/u1D436/u1D438(/u1D440,/u1D434):=max /u1D45D∈[0,1][|Pr/u1D434∼D/u1D434,/u1D49A.alt∼D/u1D49A.alt/parenleftbig/u1D434|/u1D440(/u1D49A.alt)=/u1D45D/parenrightbig−/u1D45D|] •Conservative calibrationerror( /u1D436/u1D436/u1D438): /u1D436/u1D436/u1D438(/u1D440,/u1D434):=max /u1D45D∈[0,1][/u1D45D−Pr/u1D434∼D/u1D434,/u1D49A.alt∼D/u1D49A.alt/parenleftbig/u1D434|/u1D440(/u1D49A.alt)=/u1D45D/parenrightbig] /u1D438/u1D436/u1D438and/u1D440/u1D436/u1D438are widelyused measures of calibration[13, 25]. The concept of /u1D436/u1D436/u1D438is novel — we introduce it to asymmetrically quantifysafetyincriticalsystems:falsealarmsaresafe, butmissed alarmsarenot. /u1D440/u1D436/u1D438isthestrictestmeasureofmonitorqualitybe cause/u1D440/u1D436/u1D438≥/u1D438/u1D436/u1D438and/u1D440/u1D436/u1D438≥/u1D436/u1D436/u1D438.When/u1D438/u1D436/u1D438=0,the monitorConfidence Composition for Monitors of Verification Assumptions ICCPS ’22,May 04–06, 2022,Milan, Italy iscalibratedinexpectation .When/u1D440/u1D436/u1D438=0,themonitoris perfectly calibrated .When/u1D436/u1D436/u1D438≤0,themonitor is conservative . For brevity, we will omit the distributions when they are cle ar fromthecontextandrefertomonitoroutput /u1D440(/u1D49A.alt)asjust/u1D440.Weas sumethatthis outputischaracterizedbyacontinuousproba bility densityPr(/u1D440)withﬁnite expectation E[/u1D440]and variance Var [/u1D440]. 3 RELATED WORK "
243,Detection of Face Recognition Adversarial Attacks.txt,"Deep Learning methods have become state-of-the-art for solving tasks such as
Face Recognition (FR). Unfortunately, despite their success, it has been
pointed out that these learning models are exposed to adversarial inputs -
images to which an imperceptible amount of noise for humans is added to
maliciously fool a neural network - thus limiting their adoption in real-world
applications. While it is true that an enormous effort has been spent in order
to train robust models against this type of threat, adversarial detection
techniques have recently started to draw attention within the scientific
community. A detection approach has the advantage that it does not require to
re-train any model, thus it can be added on top of any system. In this context,
we present our work on adversarial samples detection in forensics mainly
focused on detecting attacks against FR systems in which the learning model is
typically used only as a features extractor. Thus, in these cases, train a more
robust classifier might not be enough to defence a FR system. In this frame,
the contribution of our work is four-fold: i) we tested our recently proposed
adversarial detection approach against classifier attacks, i.e. adversarial
samples crafted to fool a FR neural network acting as a classifier; ii) using a
k-Nearest Neighbor (kNN) algorithm as a guidance, we generated deep features
attacks against a FR system based on a DL model acting as features extractor,
followed by a kNN which gives back the query identity based on features
similarity; iii) we used the deep features attacks to fool a FR system on the
1:1 Face Verification task and we showed their superior effectiveness with
respect to classifier attacks in fooling such type of system; iv) we used the
detectors trained on classifier attacks to detect deep features attacks, thus
showing that such approach is generalizable to different types of offensives.","Deep Learning (DL) quickly occupied a central role in recent AIrelated technological breakthroughs covering multiple elds and applications: vision (e.g., image classication [1], object detection [2]), natural language processing [3] and the combination of them (e.g., multimodal [4], sentiment analysis [5]). Despite achieving stateoftheart performance in many scenarios, deep learning models still suer from deciencies that strongly limit their adoption in sensit ive applications. Among others, the vulnerability of DL models in adversarial settings still poses challenges: it is relatively easy for an attacker to manipulate the output of a model by tampering its input often in an imperceptible way. The existence of these perturbed inputs | known as adversarial examples [6, 7] | constitutes one of the major roadblocks in securityrelated applications such as DLbased biometrics systems for surveillance and access control that, despite performing brilliantly in natural settings [8], can be easily evaded by knowledge able adversaries. Face Recognition enabled by Deep Neural Networks (DNN) is a case in point. Several successful applications of deep models to FR have been proposed in the literature [9{11]. Indeed, this kind of technology enables AI surveillance programs in multiple countries [12] and has already found its way into consumers products [8]. However, researchers already showed how this kind of systems can be jeopardized by adversarial attacks both in the digital [13, 14] and physical domain [15, 16]. In order to counteract adversarial vulnerability, a considerable research eort provided a multitude of defensive approaches for adversarial attacks that can be 2roughly categorized in two methodologies, that is rectication and adversarial input detection . In rectication methods, the goal is to recover the intended output of the model by increasing the robustness of the system, e.g. by trying to remove adversarial perturbation from the input [17, 18] or by increasing the robustness of the model itself [19, 20]. On the other hand, adversarial detection aims at detecting an occurred attack by analyzing the behavior of the model (without changing it) and signaling anomalous events [21{24]. Notwithstand ing, many of the proposed adversarial detection methods fall prey to strong adversaries too [25], recent techniques exploiting the training data manifold to ground the predictions of a model [26, 27] exhibit good tradeos between de tection performance and resilience to attacks [28] (as well as tackling a more general problem, that is obtaining good condence measurements for predictions of deep models [29]). While most of the adversarial detection schemes are tested on small or low resolution benchmarks (such as MNIST and CIFAR datasets), this work aims at evaluating one of the aforementioned trainingmanifoldbased adversarial de tection methodologies, specically [30], in a realistic securityrelated application that is facial recognition. Facial recognition systems usually do not usually implement recognition based on deeplearning classiers but rather follow a similaritybased approach: deep models are used to extract features from visual facial data, and decisions rely on similarity measurements among those features. Indeed, standard bench marks for facial recognition, such as IJBB [31] and IJBC [32], dene two evaluation protocols, that is 1:1 Face Verication and 1:N Face Identication. The former requires to investigate if a person's identity is known or not by comparing its features vector against a database of known identities, while the latter requires to match two images to assess if they belong to the same person or not. Sticking to those protocols, we provide an analysis of adversarial attacks and further detection in facial recognition systems that implement face identication and verication relying on stateoftheart deep learning models. In particular, 3our contributions are the following: i)we tested our recently proposed detec tion technique [30] against classier attacks, i.e. adversarial samples crafted to fool a stateoftheart FR neural network acting as a classier; ii)we generated deep features attacks, using a kNN algorithm as a guidance, to attack a FR system that fullls the Face Identication task by means of a DL model, acting as a backbone features extractor, followed by a kNN which gives back the query identity based on features similarity; iii)we used deep features attacks to fool a FR system on the Face Verication task, and we showed their superior eect iveness with respect to classier attacks in fooling such type of system; iv)we used the detectors trained on classier attacks to detect deep features attacks, thus showing that such approach is generalizable to dierent types of attacks. The rest of the paper is organized as follows. In Section 2, we brie y reviewed some related works. In Section 3, we described the algorithms used to craft adversarial examples, while in Section 4, we described the adversarial detection technique used in our study. In Section 5, we presented the experimental cam paigns that we conducted, and nally, in Section 6, we reported the conclusions of our work. 2. Related Work "
324,Utilization of Deep Reinforcement Learning for saccadic-based object visual search.txt,"The paper focuses on the problem of learning saccades enabling visual object
search. The developed system combines reinforcement learning with a neural
network for learning to predict the possible outcomes of its actions. We
validated the solution in three types of environment consisting of
(pseudo)-randomly generated matrices of digits. The experimental verification
is followed by the discussion regarding elements required by systems mimicking
the fovea movement and possible further research directions.","Humans do not look at a scene in a passive, ﬁxed, steady manner. Instead, their eyes move around, activelly locating and analysing interesting parts of the scene, constantly building up its mental, threedimensional model. Those rapid jerklike movements of the eyeball are known as saccades and subserve vision by redirecting the fovea along with the associated visual axis to a new region of (a)  (b)  (c)  (d) Fig. 1.The desired behaviour of a system realising saccadicbased: (a) image classiﬁcation/semantic description of the scene, (b) visual object search, (c) an exemplary maze of digits (20x20) with indicated: goal (red cross), current agent pose (white circle) and saccadic path (green line) along with (d) the current agent observation (7x7)arXiv:1610.06492v1  [cs.CV]  20 Oct 2016interest. Human eyes ﬁxate mainly on certain elements of the scene that carry or mightcarryessentialorusefullinformation,whereassaccadicmovementsdepend not only on the objects present in the scene, but also on the task the observer has to achieve [1]. There are several possible applications of artiﬁcial systems mimicking the fovea movement, with three being particularly interesting, i.e. image classiﬁca tion, semantic description of the scene and visual object search. For example, as a result of saccadicbased analysis visualized in ﬁg. 1a the system might classify the whole image and return a single label (""desert"") or return the whole seman tic description of the scene (“white truck on a road in a desert”), whereas as a result of the visual object search (ﬁg. 1b) the system should indicate that the object ""white truck"" was found in the position (x,y). In this paper we have focused on the the latter problem, i.e. learning of vi sual traces mimicking the saccadic motion enabling object search. Sec. 2 brieﬂy presents the general idea and recent applications of Deep Reinforcement Learn ing, whereas sec. 3 describes principles of the operation of the developed system. In sec. 4 we introduce an environment called a maze of digits that we have used for veriﬁcation of our system along with the obtained results, followed by a brief discussion of the solution and future works in sec. 5. 2 Related works "
341,HufuNet: Embedding the Left Piece as Watermark and Keeping the Right Piece for Ownership Verification in Deep Neural Networks.txt,"Due to the wide use of highly-valuable and large-scale deep neural networks
(DNNs), it becomes crucial to protect the intellectual property of DNNs so that
the ownership of disputed or stolen DNNs can be verified. Most existing
solutions embed backdoors in DNN model training such that DNN ownership can be
verified by triggering distinguishable model behaviors with a set of secret
inputs. However, such solutions are vulnerable to model fine-tuning and
pruning. They also suffer from fraudulent ownership claim as attackers can
discover adversarial samples and use them as secret inputs to trigger
distinguishable behaviors from stolen models. To address these problems, we
propose a novel DNN watermarking solution, named HufuNet, for protecting the
ownership of DNN models. We evaluate HufuNet rigorously on four benchmark
datasets with five popular DNN models, including convolutional neural network
(CNN) and recurrent neural network (RNN). The experiments demonstrate HufuNet
is highly robust against model fine-tuning/pruning, kernels cutoff/supplement,
functionality-equivalent attack, and fraudulent ownership claims, thus highly
promising to protect large-scale DNN models in the real-world.","The rapid development of artificial intelligence and machine learn ing technologies in recent years has driven the broad adoption of deep neural networks (DNNs) in numerous applications such as computer vision [ 17,18,35], natural language processing [ 10,46, 48], and speech recognition [ 2,31]. DNNs are valuable intellec tual property (IP) of their owners due to the tremendous amount of expertise, effort, and computing power involved in the design, training, validating, and commercializing them. Such IP should be protected according to the IP protection law. For example, the European Patent Office has recently amended its “Guidelines for Examination” on how patents related to artificial intelligence and machine learning technologies should be assessed [23]. Attackers with full access to the structures and parameters of DNN models may steal and possibly modify them and fraudulently claim ownership. Such IP stealing may happen in various realworld settings such as: (i) Owners outsource the training of their DNN 1Hufu, also known as tiger tally, was used by an emperor to command the army in ancient China. It was divided into two pieces, with the emperor keeping the right piece and the local commander keeping the left. When a new general started to command the army on behalf of the emperor, he/she was required to present the right piece that matched the left one from the local commander [45].models to third parties, (ii) Owners upload their DNN models and services to cloud service providers, and (iii) Owners license their DNN models and services to third parties. Attackers may achieve their goals via malware infection [ 21,43], insider threats [ 8,34], industrial espionage, and many other ways, leading to serious IP infringement and significant economic loss to owners. Watermarking, a process of hiding digital information in car rier data, has been used for media rights management [ 5,16] and software ownership protection [ 9,40]. It has also been developed for the IP protection of DNNs in recent years. Most DNN water marking techniques embed backdoors [ 1,14,25,29,32,51] into DNN models such that watermarked DNNs behave in a designed, distinguishable manner given specific inputs held by model own ers as secret triggers. However, recent works [ 7,26] show that finetuning [ 30,50] and pruning [ 15] can eliminate the embedded backdoors (i.e., watermarks) without significant performance loss or dramatic change of the protected models. Most existing DNN watermarking techniques are also vulnerable to fraudulent owner ship claims—attackers may discover adversarial samples and use them as “secret triggers”. It is known that adversarial samples are inherent to DNN models and can cause the models to demonstrate distinguishable behaviors [ 3,38]. Attackers can claim ownership over the stolen models by querying them with adversarial samples. Challenges in watermarking DNNs . First, watermarks could be eliminated/undermined by attackers via finetuning, pruning, or kernels cutoff/supplement, by which attackers change model parameters or the number of kernels but maintain the performance of the watermarked models. The change of model parameters may effectively remove the embedded watermarks as shown in previous studies on DNN watermarking techniques [ 1,14,32,39,51]. Second, it is challenging to design a robust watermark without significantly sacrificing the performance of the original model. Strong robustness tends to embed a large amount of watermark information into DNN models, downgrading the watermarked models’ performance to an unacceptable level. In specific applications such as autonomous driving, even 1% of performance loss is intolerable. It is by no means trivial to find a way of embedding enough watermark information for strong robustness while keeping high fidelity of DNN models. Third, it is also challenging to prevent attackers from forging new watermarks and claim ownership fraudulently over stolen DNN models. In backdoorbased watermarking, owners of DNN models keep secret triggers as ownership proofs. However, attackers canarXiv:2103.13628v1  [cs.CR]  25 Mar 2021discover adversarial samples as their triggers and confuse DNN ownership judgment effectively. Such attacks are difficult to address under the assumption that attackers understand the underlying watermarking mechanisms and explore all possible ways in forging valid watermarks to claim ownership over stolen models. HufuNet Approach . To address these challenges, we propose 𝐻𝑢𝑓𝑢𝑁𝑒𝑡 , a novel whitebox watermarking scheme for DNN mod els. HufuNet approach can be divided into three phases: watermark generation, watermark embedding, and ownership verification. In the watermark generation phase, a neural network with a small number of parameters, called HufuNet, is trained to yield a high accuracy when it is tested. The set of training samples and testing samples are public references that must be used later in the ownership verification phase. After HufuNet is trained and tested, it is split into two pieces, with the left piece as a watermark to be embedded into a DNN model for ownership protection, and the right kept by the model owner as a secret for ownership verification. In the watermark embedding phase, each convolution kernel of the left piece of HufuNet is embedded into the target DNN model’s architecture that consists of a large number of parameters. The location to embed each convolution kernel is computed and se lected to increase watermark forgery attack difficulty. During the watermarked DNN model training, the parameters incorporated from HufuNet are frozen while the rest of the parameters in the model are updated. This watermark embedding process ensures that the watermarked DNN embeds the left piece of HufuNet, yet it is fully trained on its own dataset to preserve the performance on its main task. In the ownership verification phase, the model’s owner extracts the embedded watermark from a suspect DNN model at computed locations and recovers the left piece of HufuNet accordingly. Next, the owner brings out the right piece of HufuNet, merges it with the recovered left piece to form a whole HufuNet. The whole HufuNet is tested on the public set of testing samples (previously used in watermark generation). If and only if the accuracy change of the whole HufuNet (compared to the original HufuNet) is no more than a predetermined threshold, the ownership can be claimed over the suspect model. HufuNet is evaluated on five popular DNN models (including both CNN and RNN models) and four benchmark datasets. Ex perimental results show that HufuNet achieves stateoftheart performance in the aspects as below. In particular, after the wa termark is embedded, the accuracy of these DNN models does not decline, with a 0.23% increase on average, achieving excellent fi delity compared to their nonwatermarked versions under the same training settings. The finetuning attack decreases the accuracy of the five watermarked DNNs by 1.20% on average, but the accuracy of ownership verification decreases by only 0.36%. When the water marked DNN models’ parameters are pruned from 10% to 90%, the accuracy of the watermarked DNN models may drop significantly (i.e., 61.10% on average) to an unacceptable level. However, the accuracy of ownership verification for these pruned models drops insignificantly (i.e., 29.43% on average). Regarding kernels cutoff attack, when 6.75%, 2.30%, and 4.77% kernels are eliminated from VGG11, Resnet18, GoogLeNet respectively, we can still retrieve the remaining kernels correctly to rebuild HufuNet with watermarkdetection accuracy at 100%. Moreover, our watermark is shown to be capable of dealing with functionalityequivalent (via structure adjustment, parameter adjustment or channel expansion) attacks without any performance downgrade. We also evaluate HufuNet against synthetic attacks, integrating finetuning, pruning, and functionalityequivalent attack together, and the accuracy of the retrieved HufuNet is still always above the threshold to verify the ownership as long as the adversaries would preserve the original functionality of the stolen model. The experimental results also demonstrate that statistical approaches (examining the distribution of parameter values and their gradients) cannot help learn the em bedded watermark’s existence. Finally, our case studies show that it is infeasible for the attacker to retrieve our HufuNet or forge a valid HufuNet for fraudulent ownership claim. Contributions . Our main contributions are outlined below: •New technique. We propose a new DNN watermarking approach, which embeds a piece of the watermark into a DNN model for ownership protection and withholds the other piece for ownership verification. To the best of our knowledge, this is the first approach that splits a watermark into two pieces and combines them for ownership verification. •Implementation and evaluation. We implement our watermark HufuNet and rigorously evaluate it using both CNN and RNN mod els. Experimental results show that our new DNN watermarking approach can simultaneously achieve robustness, adaptiveness, stealthiness, security, integrity, and fidelity. We release our Hu fuNet framework to the community2, together with the training and testing dataset for HufuNet, hoping to contribute to the IP protection of neural network models. 2 Background 2.1 Digital Watermarking Digital watermarking is a contentbased, information hiding tech nique for embedding/detecting digital information into/from carrier data (e.g., multimedia, documents, software, databases, etc.). It is desired that the embedded watermark should not impact the regu lar use of carrier data, and be difficult to be detected or removed. Recently, digital watermarking has been applied to the protection of DNN models, and can be classified into whitebox approaches and blackbox approaches. The whitebox approaches assume that the owner of a DNN model can access the “internals” of a suspect model to verify the ownership of the model. For instance, Uchida et al. use a parameter regularizer to embed a watermark, which is a matrix of parameters, into the target model’s parameter space dur ing its training process. When verifying ownership over a suspect model, the owner retrieves the matrix from the model, computes the retrieved matrix’s product and a pregenerated key vector, and compares the result with a local secret kept by the owner [39]. In contrast, blackbox watermarking approaches mostly rely on probing a suspect model for ownership verification, thus relaxing the requirement of the internal knowledge of the suspect model. Blackbox watermarking approaches can further be categorized into two classes, blind [ 25,29] approaches and nonblind approaches [ 1, 2https://github.com/HufuNet/HufuNet 214,32,51], depending on whether or not an embedded watermark can be perceived by the human visual system [25]. Most blackbox watermarking schemes are implemented based on embedding backdoors [ 6,13] into the target DNN models. Em bedding a backdoor into a neural network model is to deliberately train the model using a training dataset that is augmented with cer tain specific inputs (i.e., triggers) and their desired labels (typically wrong labels), which should be selected uniquely by the model’s owner and thus demonstrate the model’s distinguishable behav ior. Once trained, the model should output the desired labels for the specific inputs. The mapping between the specific inputs and their desired labels is considered as a backdoor, and used as a wa termark. The specific inputs are kept as a secret by the model’s owner. The model’s owner can claim his/her ownership over a suspect model if the backdoor/watermark is detected in the sense that the model outputs the desired labels given the specific inputs from the owner’s secret. However, recent research indicates that most backdoorbased watermarks are vulnerable to be detected and destroyed [4, 26, 41]. 2.2 Watermark Destructing Approaches Entire model retraining. The most effective method to destruct an embedded watermark in a model is to retrain it completely. However, since such entire model retraining typically requires similar amount of expertise, effort, training data and computing power as training their own model from scratch, adversaries would rather train their own model than stealing to avoid any potential legal issues. Finetuning. Finetuning is typically used to optimize an already trained DNN model to augment it with additional tasks or reinforce its performance. Finetuning involves an update to the parameter values in the model, thus possible destroying the embedded water mark. Generally, the scale of finetuning is flexible, depending on the training dataset and computing power available. Therefore, ad versaries can finetune the stolen model according to their capacity to destruct the watermark. Pruning. Model pruning approach is commonly used for model compression to produce smaller, more memoryefficient and power efficient models with negligible loss inaccuracy. Typically, it prunes lessconnected, unimportant neurons in the neural network by zeroizing them without changing the network structure. Model pruning can be used by those adversaries lack of training power to destruct the embedded watermark in their stolen model, since it does not involve any retraining. Functionalityequivalent attack. Functionality equivalent at tack can be launched via structure adjustment and parameter adjust ment. The attackers do not need any training resources or training dataset to retrain the model. Instead, they can simply reorder the se quence of output channels on one convolution layer, and adjust the corresponding sequence of input channels on the next convolution layer to obtain a functionalityequivalent model (called structure adjustment in this paper), due to the isomorphism of many CNN structures. Furthermore, if the stolen DNN model is with ReLU as the activation function, the attackers can also scale the weights of all the neurons on one layer by a positive constant 𝑐>0, and then scale the next layer by 1/𝑐to get a functionalityequivalent model(called parameter adjustment in this paper) to destruct embedded watermarks. Functionalityequivalent attack can also be launched via channel expansion. In particular, adversaries randomly choose two adjacent layers of their stolen model with matrices 𝐴and𝐵. Then they increase the output dimension of 𝐴(by increasing its output channels) to get 𝐴′, and𝐵to get𝐵′, with the restriction of (𝐴′𝐵′)𝑥=(𝐴𝐵)𝑥, where𝑥is the feature map propagated in neu ral networks. Finally, the adversaries can further apply structure adjustment or finetuning to undermine the embedded watermarks. Kernels cutoff and supplement. The adversary can cut off (com pletely remove some kernels, rather than zeroizing them as pruning) or supplement some kernels in the stolen model to obtain a new model with similar performance but different number of kernels and structure. To ensure similar performance as the stolen model, the adversary should cut off or supplement kernels carefully: if some output channels (kernels) of one layer are cut off or supple mented, then the corresponding input channel of the next layer should also be cut off or supplemented. Besides, since kernels cutoff will reduce the performance of the model, the adversary may not want to cut off too many kernels from the stolen model. Kernels cutoff or supplement can also be leveraged by the adversary to destruct embedded watermarks. 3 Problem Statement 3.1 A Motivating Example With the advancement of AI techniques, neural networks are in volved in various complicated tasks such as autonomous driving, speech recognition, medical diagnosis, etc. Such tasks typically rely on DNN models that are trained on a large amount of data using intensive GPU resources. For instance, the training of BERT [ 11], a language understanding model, took 3.3 days (79.2 hours) on 4 DGX 2H servers with 64 Tesla V100 GPUs, and received $3,751$12,571 bills from a cloud computing platform [ 36]. Moreover, it is estimated that training GPT3, an autoregressive language model, would cost around $12 million [ 44]. Hence, protecting the IP of model own ers is on demand. However, the existing backdoorbased water marking approaches provide no sufficient protection for model owners [ 1,14,25,29,32,51]. Consider a motivating example as shown in Figure 1, where Alice develops a valuable DNN model 𝐷𝑁𝑁𝐴𝑙𝑖𝑐𝑒 . Realizing the necessity of IP protection, Alice embeds several backdoor samples (i.e., triggers and their desired labels) into the model as a digital watermark 𝑤𝑎𝑡𝑒𝑟𝑚𝑎𝑟𝑘 𝐴𝑙𝑖𝑐𝑒 . Then, Alice deploys𝐷𝑁𝑁𝐴𝑙𝑖𝑐𝑒 on a cloud platform in operation. Now assume that an attacker Eve (e.g., the cloud platform provider, an insider, or a hacker) who has access to the structure and pa rameters of 𝐷𝑁𝑁𝐴𝑙𝑖𝑐𝑒 steals the model and recommercializes it for profit. To evade Alice’s detection, Eve finetunes and/or prunes 𝐷𝑁𝑁𝐴𝑙𝑖𝑐𝑒 , aiming to remove Alice’s watermark. To prepare for pos sible ownership arguments, Eve also injects a number of backdoor samples of her own choice into the stolen model as her own digital watermark𝑤𝑎𝑡𝑒𝑟𝑚𝑎𝑟𝑘 𝐸𝑣𝑒3. The question is, how Alice can testify that the model 𝐷𝑁𝑁𝐸𝑣𝑒is stolen from her model 𝐷𝑁𝑁𝐴𝑙𝑖𝑐𝑒 . Alice may present her set of triggers as inputs to 𝐷𝑁𝑁𝐸𝑣𝑒and observe the model’s output. Even if we assume that Eve’s finetuning and/or 3Alternatively, Eve can also generate a number of adversarial examples on the stolen model as her own digital watermark 𝑤𝑎𝑡𝑒𝑟𝑚𝑎𝑟𝑘 𝐸𝑣𝑒. 3Embed WatermarkAlice Alice DNN Expertise Big  DataPowerful Computing☁ ️Cloud Platform Deploy DNNAlice Eve Cloud Platform ProviderSteal Fine tune, Prune Embed WatermarkEveDNNAlice Verify Figure 1: A Motivating Example pruning is not effective in removing Alice’s watermark, Eve can still confuse ownership verification by presenting her set of triggers or adversarial examples, thus claiming her ownership of 𝐷𝑁𝑁𝐸𝑣𝑒. In the worst case, the pruning and/or finetuning of 𝐷𝑁𝑁𝐴𝑙𝑖𝑐𝑒 performed by Eve removes Alice’s watermark from 𝐷𝑁𝑁𝐸𝑣𝑒. Con sequently, Eve is victorious over Alice in the dispute of ownership. One may argue that Alice may increase the number of embedded triggers, and use it to outperform Eve. However, such approach is not feasible. On the one hand, with the knowledge that more trig gers win, Eve can simply compete with Alice by embedding more backdoors as her own watermark. On the other hand, as the number of embedded triggers increases, the accuracy of the watermarked model decreases as shown in Table 12 in Appendix, which limits the number of backdoors one can embed into a model. We use the backdoor injection approach described in [ 1] to embed different number of backdoors into VGG11 (trained on CIFAR10), and find the accuracy drops by 1.4% with 400 backdoor samples embedded. In addition, we obtained code from authors of [ 1], applied the same setting, trained Resnet18 on CIFAR10, repeated their experiments and got similar results as in their paper. Then we embedded a dif ferent number of backdoors into the above model, and find the accuracy drops 1.68% with 400 backdoor samples embedded. There fore, Alice may not want to embed too many backdoor samples as it may compromise the accuracy of the watermarked model. 3.2 Threat Model We consider whitebox adversaries who have full access to model structure and parameters. We assume that the adversaries, such as prowlers of neural networks, are proficient in machine learn ing and watermarking. To infringe on the IP of a watermarked DNN model, the adversaries may apply various machine learning techniques such as model pruning, model finetuning, kernels cut off/supplement and crafting adversarial samples. Note that kernels cutoff or supplement cannot work for LSTM, since such operations will change the structure of the model, but LSTM is one piece offunctional unit and its internal structure cannot be changed. More over, the adversaries are also capable of launching functionality equivalent attack through structure adjustment or parameter ad justment to fail embedded watermarks. Note that such attack is only effective against CNN models for making the structure adjustment and against ReLU activation function for making the parameter adjustment. It does not work on LSTM, since it uses sigmoid as the activation function (making the parameter adjustment impossible) and the weights in LSTM must be in order (making the structure adjustment impossible). We assume that the adversaries lack neces sary resources, including largescale training datasets and sufficient computing resources, to retrain the entire or a large portion of stolen model. Finally, we also assume adversaries do not make changes beyond the functionalityequivalent and kernels cutoff attacks to the structure, which may reduce the accuracy of the stolen model to an unacceptable level. 4 Approach Figure 2 shows the workflow of HufuNet in three phases: watermark generation, watermark embedding, and ownership verification. Dur ing the watermark generation phase, a model owner trains a unique neural network, HufuNet, on a public training set 𝐷𝑠and divides its parameter space into two pieces: 𝐸𝑚𝑏𝑒𝑑𝑑𝑒𝑑𝑃𝑖𝑒𝑐𝑒𝑜𝑓 𝐻𝑢𝑓𝑢𝑁𝑒𝑡 (𝐸𝑃𝐻) as watermark, and 𝑆𝑒𝑐𝑟𝑒𝑡𝑃𝑖𝑒𝑐𝑒𝑜𝑓 𝐻𝑢𝑓𝑢𝑁𝑒𝑡 (𝑆𝑃𝐻)as local secret. During the watermark embedding phase, the model owner injects the watermark EPH into the parameter space of his/her DNN model (i.e., DNNtobeprotected) for ownership protection and trains it on a large training set 𝐷𝑡with EPH frozen to fulfill its training task (i.e., DNNwatermarked). During the ownership verification phase, the owner extracts a watermark 𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 from a suspect DNN model, and merges it with his/her local SPH to generate a neural network model 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 . The owner can then test 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 on a public testing dataset 𝐷𝑠_𝑇𝐸𝑆𝑇 with the same distribution as 𝐷𝑠, and verify his/her ownership if the suspect model is embedded with the watermark EPH. Following Kerckhoffs’ principle, we assume the watermark embedding algorithm and the ownership verification algorithm are public. While the adversaries may forge HufuNet from the public training dataset 𝐷𝑠, they need to use the public testing dataset 𝐷𝑠_𝑇𝐸𝑆𝑇 and follow the public ownership verification algorithm to claim their ownership over disputed models. 4.1 Watermark Generation The structure of HufuNet consists of several convolutional layers followed by a fully connected layer. The size of each convolution kernel in HufuNet is chosen to be 3∗3in this paper; therefore, the number of parameters for each kernel is nine. After HufuNet is trained on a public data set 𝐷𝑠, all of its convolution layers are used as watermark EPH, while the fully connected layer is kept locally as secret SPH. The size of EPH is typically much smaller than the DNN model in which EPH is embedded. HufuNet can be considered as a mapping from its input space to its output space: R𝑀𝑠→R𝑁𝑠after it is trained using the dataset 𝐷𝑠. The mapping achieves high accuracy on a testing dataset 𝐷𝑠_𝑇𝐸𝑆𝑇 , which has the same distribution as 𝐷𝑠. Both𝐷𝑠and𝐷𝑠_𝑇𝐸𝑆𝑇 are released to the public for training and verifying HufuNet. Note that it is beneficial to standardize 𝐷𝑠and𝐷𝑠_𝑇𝐸𝑆𝑇 so that the adversaries cannot use 4Figure 2: Workflow of HufuNet arbitrary datasets to forge and verify their own HufuNet (refer to evaluation results in Section 5.7). 4.2 Watermark Embedding We focus on CNN models in explaining HufuNet watermark em bedding process, where the DNNtobeprotected includes convolu tional layers. Regarding RNN models, we simply view LSTM as one convolutional layer with 𝑁kernels, each with 3∗3parameters, to embed HufuNet watermarks. Embedding approach. Watermark EPH is embedded into the con volutional layers of DNNtobeprotected. In this process, each con volution kernel of EPH is embedded individually into DNNtobe protected, which is much larger than EPH. A secret key determines the embedding location of each kernel based on its parameter val ues and index, to ensure the stealthiness of EPH in the parameter space of DNNtobeprotected. The secret key should be strong enough to defend against potential attacks to detect the embedded watermark. e.g., brute force searching (refer to evaluation results in Section 5.6). Algorithm 1 shows the details of our watermark embedding ap proach. Line 1 initializes the variables 𝑏𝑖𝑡𝑚𝑎𝑝 , which keeps track of whether each position in DNNtobeprotected has been embedded, and DNNwatermarked 𝑓𝑤𝑚. For each convolution kernel in our watermark EPH (Line 2), we first perform an XOR operation on its index𝑖and all its𝑘∗𝑘parameter (𝑘=3in our EPH), hash the result using a secret 𝑘𝑒𝑦, and finally do a modulo over the number of kernels in DNNtobeprotected to compute the embedding po sition (Line 3). If the position has been embedded, we simply do a linear probing to find the next available position (Line 46). Once the position is found, we embed the convolution kernel into the corresponding position in 𝑓𝑡(Line 7), update 𝑏𝑖𝑡𝑚𝑎𝑝 to indicate the position has been used (Line 8), and continue the next kernel (Line 9). Finally, we output DNNwatermarked 𝑓𝑤𝑚in Line 11. Note that the kernel parameters in EPH are used in determining where EPH is embedded in 𝑓𝑡. The owner of 𝑓𝑡needs to keep the whole HufuNet, including EPH and SPH, for ownership verification. In the process of ownership verification, the preserved EPH is used to locate where it is embedded from a suspect model, while the values in the suspect model are not used to locate the watermark sinceAlgorithm 1 Embedding Algorithm Require:𝑓𝑡: DNNtobeprotected; 𝑁: number of convolution ker nels in𝑓𝑡;𝐾𝑒𝑟𝑛𝑒𝑙 : all kernels in EPH to be embedded; 𝑛: number of kernels in 𝐾𝑒𝑟𝑛𝑒𝑙 ;𝑏𝑖𝑡𝑚𝑎𝑝 : a bitmap to keep track of the us age of each position in 𝑓𝑡;𝑘𝑒𝑦: a secret key Ensure:𝑓𝑤𝑚: DNNwatermarked 1:𝑏𝑖𝑡𝑚𝑎𝑝 =𝑁𝑈𝐿𝐿 ,𝑓𝑤𝑚=𝑓𝑡 2:for𝑖in(1,𝑛)do 3:𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛 =𝐻𝑀𝐴𝐶(𝑘𝑒𝑦,𝑋𝑂𝑅𝑃𝑀𝑉(𝑘𝑒𝑟𝑛𝑒𝑙𝑖)⊕𝑖)%𝑁 4:while𝑏𝑖𝑡𝑚𝑎𝑝𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛 do 5:𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛 =++𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛 %N 6:end while 7:𝑓𝑤𝑚=𝐸𝑀𝐵𝐸𝐷(𝑘𝑒𝑟𝑒𝑟𝑙𝑖,𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛,𝑓 𝑤𝑚) 8:𝑏𝑖𝑡𝑚𝑎𝑝𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛 =1 9:++𝑖 10:end for 11:return𝑓𝑤𝑚 𝑋𝑂𝑅𝑃𝑀𝑉 is the function to 𝑋𝑂𝑅 all the parameter values of a convolution kernel.𝐸𝑀𝐵𝐸𝐷 is the function to embed a convolution kernel of EPH into a specific position of 𝑓𝑡.𝐻𝑀𝐴𝐶 is a keyed cryptographic hash function. they may have been changed in the watermark embedding process and possibly by the adversaries. Training DNNwatermarked. The DNNtobeprotected 𝑓𝑡is a mapping from its input space R𝑀 𝑡to its output space R𝑁 𝑡. Before training, the owner of 𝑓𝑡randomly initializes the parameters of 𝑓𝑡, and embeds the parameters of EPH based on the approach discussed above. The owner then trains 𝑓𝑡on its training dataset 𝐷𝑡. During the training process of 𝑓𝑡, the owner freezes the parameter space of EPH and only updates the rest of the parameter space. In this way, the parameter space of EPH remains unchanged while other parameters of 𝑓𝑡are updated during training. Note that the loss function of 𝑓𝑡remains unchanged during the training process. 4.3 Ownership Verification Given a model 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 , the owner first extracts the parameter space 𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 from𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 . A straightforward extraction method 5is to repeat Algorithm 1 in Section 4.2. Based on the original Hu fuNet kept by the owner, he/she can compute the locations in 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 where EPH is supposedly embedded, and retrieve the corre sponding parameter space as 𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 . Then the owner merges 𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 with the preserved SPH to obtain 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 . If𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 is stolen from 𝑓𝑤𝑚, the rebuilt 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 is ex pected to maintain a high accuracy on the standard test set 𝐷𝑠_𝑇𝐸𝑆𝑇 , which has the same distribution as the standard training dataset 𝐷𝑠. Specifically, the owner evaluates the accuracy 𝐴𝑐𝑐𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 using𝐷𝑠_𝑇𝐸𝑆𝑇 . If the adversary does not retrain 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 ,𝐴𝑐𝑐𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 will have the same value as the accuracy of the original HufuNet (i.e., 𝐴𝑐𝑐𝑜𝑟𝑖). Otherwise, 𝐴𝑐𝑐𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 would drop. To solve this problem, we compare 𝐴𝑐𝑐𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 with𝐴𝑐𝑐𝑜𝑟𝑖 using𝐷𝑖𝑓𝑓𝐴𝑐𝑐=|𝐴𝑐𝑐𝑜𝑟𝑖−𝐴𝑐𝑐𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑|/𝐴𝑐𝑐𝑜𝑟𝑖, if𝐷𝑖𝑓𝑓𝐴𝑐𝑐<𝜏𝑎𝑐𝑐, we think that 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 infringes the IP of the DNNwatermarked 𝑓𝑤𝑚.𝜏𝑎𝑐𝑐is the threshold controlling the difference between 𝐴𝑐𝑐𝑜𝑟𝑖 and𝐴𝑐𝑐𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 . In our evaluation, we find that HufuNet is highly robust against model pruning, finetuning and kernels cutoff. Tak ing CNN as an example, even if the accuracy of 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 drops by 50.74%,𝐴𝑐𝑐𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 can still be higher than 80% and 𝐷𝑖𝑓𝑓𝐴𝑐𝑐is 13.34% (details are in Section 5.3). Therefore, we define 𝜏𝑎𝑐𝑐as 15% in our evaluation, which is good enough to preserve good robustness and integrity of HufuNet. 4.4 Restore before Retrieve As a parameter level watermark, HufuNet may suffer from the functionalityequivalent attacks as discussed in Section 2.2. There fore, even if 𝐷𝑖𝑓𝑓𝐴𝑐𝑐is larger than 𝜏𝑎𝑐𝑐, a model can still be suspi cious (𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 ) if it has exactly the same or quite similar structure as the protected model, e.g., the same number of layers and the same number of parameters (with different order) on each layer, or the same number of layers and the same number of kernels on most layers. In such scenarios, we examine 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 further using the following restore approach and determine whether or not such model has been intentionally adjusted to evade our parameter level watermark. Note that if 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 is an innocent model, the restore approach will not falsely claim ownership over it. Regarding structure adjustment, we recover the order of in put/output channels of 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 according to the locally kept Hu fuNet𝑓𝑤𝑚. We start from the first layer where only the order of the output channels can be changed. We compare the parameters of each output channel of 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 with those of 𝑓𝑤𝑚using cosine similarity, so as to find the optimal match. In this way, we can re cover the order of each output channel of the first layer in 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 according to that of our locally stored 𝑓𝑤𝑚. Then the input channels of the second layer can be restored straightforwardly based on the restored order of the output channels of the first layer. Similarly, the output/input channels of all other layers can be restored accord ingly based on the locally kept 𝑓𝑤𝑚, thus enabling correct retrieval of𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 even if𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 experienced structure adjustment. The above cosine similarity approach also works well when many parameters in output channels are adjusted, such as finetuning (detailed discussion in Appendix A). Regarding parameter adjustment, we first ensure that the or der of the input/output channels of 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 is recovered using theapproach described above. Then we perform Singular Value De composition (SVD) on each convolution kernel of 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 and𝑓𝑤𝑚 to obtain intermediate singular value matrices. For each pair of sin gular value matrix in 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 and singular value matrix in 𝑓𝑤𝑚, we divide each element of the former by each element of the latter; thus we get a series of resultant matrices. The values on the diagonal of each resultant matrix are averaged to compute the scale factor 𝑐, used to obtain the restored convolution kernel (i.e., dividing each element in the corresponding convolution kernel 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 by𝑐). We utilize this method to restore all the convolution kernels. Note that the scale factor is not computed by (i) dividing each element of the convolution kernel in 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 by each element of the convolution kernel in our local 𝑓𝑤𝑚, and (ii) computing the average value of the resultant matrix. This is because the singular values of a convolution kernel represent the scaling characteristics of the kernel; it is more reliable to compute the scaling factor from the singular values rather than from kernel parameters directly. This is clear in the case where an attacker first finetunes a stolen model and then perform the functionalityequivalent attack through pa rameter adjustment. The finetuning typically makes those smaller values in the convolution kernel change more significantly than those larger ones (e.g., increasing from 1 to 2 results in the factor of 2, but increasing from 10 to 11 yields the factor of 1.1). Meanwhile, a highly suspicious model 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 may show mi nor structural differences, e.g., the number of kernels on one or several layers of 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 is smaller or larger than those of our wa termarked model. We further examine whether or not some kernels of such model have been intentionally cut off or supplemented to evade our watermark. First, we still restore the structure of 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 using cosine similarity approach presented above, since kernels cutoff/supplement attacks always change the order of kernels as well, unless all the cutoff/supplemented kernels are the last ones of the layer. With all the kernels of 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 restored and our locally stored𝑓𝑤𝑚as a reference, we fill in missing (cutoff) kernels with all parameters as 0 or cut off extra (supplemented) kernels directly. Note that using the values of the corresponding kernels in 𝑓𝑤𝑚to fill in may cause false positive on innocent models. In this way, we can restore 𝑓𝑠𝑢𝑠𝑝𝑒𝑐𝑡 with the same structure as 𝑓𝑤𝑚and the same number of kernels 𝑁, so Algorithm 1 can be used to extract EPH. 5 Evaluation Based on the watermark destruction approaches discussed in Sec tion 2.2, we evaluate our solution in the following aspects. (i) robust ness: watermarks should still be detected by owners from stolen DNN models even if the models experienced finetuning, pruning and kernels cutoff/supplement (refer to Section 5.3); (ii) adaptive ness: watermarks should still be detected by owners even if the model has been converted to a functionalityequivalent one via structure adjustment, parameter adjustment or channel expansion (refer to Section 5.4); (iii) against synthetic attacks: watermark can still be detected by owners even if the attackers applied fine tuning, pruning and functionalityequivalent attacks together on their stolen models (refer to Section 5.5); (iv) stealthiness: it is diffi cult for attackers to learn the existence of watermark from a stolen DNN model (refer to Section 5.6); (v) security: it is difficult for at tackers to forge a valid watermark for a stolen DNN model (also called ambiguity attack [ 12], refer to Section 5.7); (vi) integrity: it is 6Table 1: Baseline Performance of Ownership Verification VGG11 GoogLeNet Resnet18 Resnet34 LSTM Original177.77% 90.85% 86.14% 70.20% 87.08% Suspect277.77% 90.85% 86.14% 70.20% 87.08% 𝜏3 𝑎𝑐𝑐 15% 15% 15% 15% 15% Combine492.32% 92.32% 92.32% 92.32% 92.32% 1Original indicates the accuracy of the original DNNwatermarked. 2Suspect indicates the accuracy of the suspect model we would like examine. Since there is not any kind of retraining conducted, it demonstrates the same accuracy as that of our DNNwatermarked. 3The threshold 𝜏𝑎𝑐𝑐is set as 15%, so 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 accuracy above 80% indicates ownership verification according to Section 4.3. 4Combine indicates the accuracy of the 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 . Since there is not any kind of retraining conducted, it demonstrates the same accuracy as that of our original HufuNet. highly unlikely for DNN owners to claim ownership over innocent DNN models (refer to Section 5.8); and (vii) fidelity: watermark embedding should impact little on the performance of the original DNN models (refer to Section 5.9). 5.1 Experimental Setup Models . Without loss of generality, we conduct experiments on five models: VGG11 [ 35], GoogLeNet [ 37], Resnet18 and Resnet34 [ 17], and LSTM [ 20], which are all used as DNNstobeprotected. Hufu Net is designed with five convolutional layers (used as EPH) and one fully connected layer (used as SPH). Datasets . We choose four datasets: FashionMNIST [ 47], IMDB [ 27], CIFAR10, and CIFAR100 [ 22] for the above five models accord ingly. We use FashionMNIST to train our HufuNet, CIFAR10 to train VGG11, GoogLeNet as well as Resnet18, CIFAR100 to train Resnet34, and IMDB to train LSTM as shown in Table 13 in Appen dix. The classification task and the main task of the DNN models are used interchangeably thereafter. Platform . All our experiments are conducted on a server running 64bit Ubuntu 18.04 system with Intel Xeon E52620 v4 @ 2.10GHz CPU, 128GB memory, 3TB hard drive and 3 Nvidia GPU TiTan X GPUs each with 12GB memory. 5.2 Baseline Performance We evaluate the ownership verification of HufuNet on the model without any changes between embedding and verifying, i.e., as suming adversaries just steal the model without performing any other operations on the stolen model, as the baseline performance. The number of parameters of the five DNNstobeprotected and HufuNet are shown in Table 14 in Appendix. It can be seen that our watermark EPH only consumes a small portion of the parame ter space of VGG11, GoogLeNet, Restnet18 and Resnet34, ranging from 1.48% to 5.37%), since the number of parameters of these four models are relatively large. Since LSTM is intentionally selected to evaluate the performance of HufuNet on smaller models, EPH consumes up to 13.38% of its parameter space, considering LSTM only contains 9,251 KB parameters in just three layers. The experimental results are shown in Table 1. Since there is no retraining conducted on the original DNNwatermarked, the suspect model (tobeexamined by HufuNet for ownership verifica tion) demonstrates the same accuracy as that of our original model. Similarly,𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 is exactly the same as original EPH, so theaccuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 equals to that of the original Hufu Net. Therefore, the rebuilt 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 can always verify the ownership on the suspect model accurately considering not any kind of retraining on the stolen model is conducted4. According to Section 4.3, with 𝐴𝑐𝑐𝑜𝑟𝑖as 92.32% and 𝜏𝑎𝑐𝑐as 15%, the accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 above 80% means successful ownership verifi cation. In the following experiments, we will just show HufuNet (combined) accuracy to indicate ownership verification. 5.3 Robustness Due to the stealthiness of EPH (as shown in Section 5.6), a targeted destruction (knowing where EPH is and destructing it correspond ingly) is almost impossible. The adversary certainly can initialize and retrain all the convolutional layers of the stolen model. In this scenarios however, the adversary would rather train his/her own model than stealing 𝑓𝑤𝑚, since those two procedures demand simi lar amount of computing resources and training data. In contrast, adversaries would prefer “blindly” revising the model, e.g., fine tuning, pruning, kernels cutoff/supplement, etc., hoping to destruct embedded watermarks. Robustness against finetuning. We trained five models in to tal (with our watermark EPH embedded): VGG11, GoogLeNet, Resnet18 on CIFAR10, Resnet34 on CIFAR100, and LSTM on IMDB. We assume adversaries only have access to the testing dataset (pro vided by the model owner to test the performance of the model, including 10,000 samples), and try to leverage it to finetune the stolen model. Referring to the settings in [ 25], adversaries divide their available dataset into two parts, 80% as the training dataset and 20% as testing dataset. Same as [ 1], we also set initial learn ing rate𝜆=0.001 and the delay factor as 0.0005 to finetune DNNswatermarked using SGD (Stochastic Gradient Descent). The maximum number of epochs for finetuning is set as 100 accord ing to [ 25], and the incremental step of the epoch is set as 10. As𝜆decreases to a very small number, the parameters of DNNs watermarked will rarely change, which indicates the model tends to be stable during finetuning. As shown in Table 2, regarding the DNNswatermarked, their accuracy drops at different scale when finetuned up to 40 epochs. As the number of epochs keeps in creasing, the accuracy either gets improved or becomes stable. In contrast, the accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 tends to be very stable, always above 90%, which suffices ownership verification (above 80%). Therefore, the finetuning process does not affect our EPH. Robustness against pruning. To evaluate the robustness of Hu fuNet against pruning, we first embed our watermark EPH into VGG11, GoogLeNet, Resnet18, Resnet34 and LSTM, and then prune 10% to 90% parameters of each model based on their absolute val ues respectively. We adopt the commonly used pruning technique in [15], which eliminates the parameters with smaller absolute val ues, thus believed to have little impact on the performance of the models. We choose such model pruning approach, which is consis tent with our assumption of the adversaries’ capability, i.e., lack of sufficient training dataset and/or computing resources. Table 3 shows the experimental results on the accuracy of the correspond ing DNNswatermarked and HufuNet after pruning. Based on the re sults, we find that even if the DNNswatermarked have been pruned 4We also evaluate the integrity of HufuNet in Section 5.8, that is, without EPH, we will not claim ownership against innocent models. 7Table 2: Robustness against Finetuning EpochsVGG11 GoogLeNet Resnet18 Resnet34 LSTM DNN watermarkedHufuNetDNN watermarkedHufuNetDNN watermarkedHufuNetDNN watermarkedHufuNetDNN watermarkedHufuNet 0 77.77% 92.32% 90.85% 92.32% 86.14% 92.32% 70.20% 92.32% 87.08% 92.32% 10 75.35% 92.37% 89.55% 91.91% 85.75% 92.34% 66.97% 92.26% 85.90% 92.28% 20 74.70% 92.20% 89.75% 91.99% 85.70% 92.35% 66.78% 92.20% 86.10% 92.36% 30 72.30% 92.21% 89.75% 92.03% 85.60% 92.36% 68.19% 92.15% 85.60% 92.18% 40 73.65% 92.16% 89.70% 92.07% 85.65% 92.36% 67.44% 92.01% 88.20% 92.19% 50 73.35% 92.19% 89.70% 92.06% 85.50% 92.32% 67.48% 91.83% 87.80% 92.22% 60 75.80% 92.18% 89.80% 92.04% 85.50% 92.39% 67.64% 91.88% 88.00% 92.17% 70 75.80% 92.16% 89.70% 92.02% 85.55% 92.37% 67.51% 91.85% 87.30% 92.11% 80 75.85% 92.16% 89.65% 92.01% 85.70% 92.39% 67.81% 91.53% 87.50% 92.14% 90 75.70% 92.14% 89.45% 92.03% 85.70% 92.39% 66.91% 91.32% 87.50% 92.16% 100 75.60% 92.14% 89.45% 92.03% 85.70% 92.39% 67.67% 91.08% 87.60% 92.16% The HufuNet in the table indicates the accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 . Table 3: Robustness against Pruning pct.VGG11 GoogLeNet Resnet18 Resnet34 LSTM DNN watermarkedHufuNetDNN watermarkedHufuNetDNN watermarkedHufuNetDNN watermarkedHufuNetDNN watermarkedHufuNet 0 77.77% 92.32% 90.85% 92.32% 86.14% 92.32% 70.20% 92.32% 87.08% 92.32% 10% 77.68% 92.33% 90.77% 92.29% 86.14% 92.29% 70.29% 92.27% 87.06% 92.33% 20% 77.80% 92.29% 90.55% 92.32% 85.96% 92.34% 70.20% 92.46% 87.18% 92.37% 30% 77.82% 92.27% 90.19% 92.32% 85.75% 92.35% 69.70% 92.20% 87.26% 92.36% 40% 77.64% 92.37% 85.90% 92.36% 82.24% 92.37% 67.25% 91.98% 86.94% 92.19% 50% 77.23% 92.29% 56.46% 92.07% 74.81% 92.09% 56.46% 91.21% 86.85% 91.86% 60% 75.68% 92.25% 19.63% 91.73% 46.84% 91.74% 27.86% 89.19% 86.89% 91.36% 70% 69.84% 91.86% 11.73% 90.29% 11.44% 90.35% 4.86% 85.60% 86.28% 89.95% 80% 50.74% 90.99% 10.00% 87.63% 10.00% 87.80% 1.10% 65.61% 84.16% 85.74% 90% 21.74% 87.54% 10.00% 66.12% 10.00% 67.83% 1.00% 34.82% 60.75% 58.14% pct. indicates the percentage of pruning. The HufuNet in the table indicates the accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 . Table 4: Robustness against Kernels Cutoff DNN VarianceAccuracy of DNNwatermarkedHufuNet Accuracy VGG11Original 77.77% 92.32% Cut off 72.70%  Restore 72.70% 92.14% Resnet18Original 86.14% 92.32% Cut off 80.28%  Restore 80.28% 92.38% GoogLeNetOriginal 90.85% 92.32% Cut off 85.10%  Restore 85.10% 92.02% to be considered as “fail” on the main task (e.g., the accuracy of VGG11, GoogLeNet, Resnet18, Resnet34 has been reduced to 50.74%, 10.00%, 10.00% and 1.10%, respectively) after 80% pruning, the accu racy of𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 is still above 80%, satisfying the preset threshold𝜏𝑎𝑐𝑐. For 90% pruning, the accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 drops below 80%, but this high degree of pruning almost completely ruin the main task of the model. It is interesting that the accuracy of LSTM does not drop as significantly as other models, probably due to its larger parameter redundancy when dealing with simple tasks. Overall, the high accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 upon heavy pruning indicates it is highly robust against pruning. Robustness against kernels cutoff/supplement. We evaluate the robustness of HufuNet against kernels cutoff attack on VGG11, Resnet18, and GoogLeNet. Resnet34 and LSTM are not evaluated, since the structure of the former is similar as Resnet18 and thelatter is innately immune from such attack (as discussed in Section 3.2). Since the removal of the convolution kernels will reduce the performance of 𝑓𝑐𝑢𝑡𝑜𝑓𝑓 , we control the kernels cutoff rate as 6.75%, 2.30% and 4.77% respectively for the above three models, to ensure their accuracy does not drop significantly (around 5%), i.e., 5.07%, 5.86% and 5.75% respectively. Then we use the above proposed cosine similarity approach to restore the structure of 𝑓𝑐𝑢𝑡𝑜𝑓𝑓 and supplement 0 for the missing kernels (cut off). As shown in Table 4, the accuracy of HufuNet is 92.14%, 92.38%, and 92.02% for the three models respectively, which are sufficient to verify the ownership (above 80%). The evaluation result of kernels supplement is quite similar to that of kernels cutoff, since both the structure and the number of kernels are changed in a similar way. Analysis of Robustness. Due to the stealthiness (evaluated in Section 5.6) and small scale of EPH in DNNwatermarked (EPH consumes 1.48%∼3.44% of the parameter space of CNN models, and 13.38% of the parameter space of the LSTM model, based on our experiment in Section 5.2), it is difficult for model pruning and fine tuning to destroy the embedded watermark. The robustness of EPH against pruning and finetuning is also attributed to the parameter redundancy in DNN models. Usually a portion of parameters (e.g., the top 50% parameters with large absolute values) contributes most to the task of a DNN model, while the other parameters can be considered “redundant” in the parameter space, making minor contribution to the model’s task. Model retraining mainly affects those notsoimportant values in the parameter space of DNN watermarked; otherwise, the performance of the retrained model 8would be significantly worse than the watermarked model, which defeats the purpose of ownership verification. As long as a majority of significant parameters in EPH remains stable in model retraining, ownership verification can be performed robustly. 5.4 Adaptiveness Since HufuNet is essentially a parameterlevel watermark, we must ensure the embedded watermark can still be correctly retrieved to verify ownership. We further evaluate it against functionality equivalent attack in terms of structure adjustment and parameter adjustment on three neural network models: VGG11, Resnet18, and GoogLeNet. We choose not to evaluate Resnet34 and LSTM, since the structure of the former is similar to Resnet18 and the latter is a RNN with sigmoid as activation function (which is not subject to functionalityequivalent attack). Adaptiveness against Structure Adjustment. To simulate the maximum capability of attackers, we shuffle the order of all con volution kernels of each layer, and at the same time we adjust the order of the inner layers of the convolution kernels to ensure functionalityequivalent. If there is a BatchNorm layer connected after a convolution layer, we also change its mean and variance according to the order of the corresponding convolution kernels. We use the approach discussed in Section 4.4 to restore the order of kernels. The experimental results are shown in Table 5. After reordering the convolution kernels of the DNNswatermarked, the accuracy of VGG11, Resnet18 and GoogLeNet remains the same (functionally equivalent), but the accuracy of the combined HufuNet (without restore) decreases significantly from 92.32% to18.15%, 13.71% and8.22% respectively. After restoring however, the accu racy of the combined HufuNet resumes to 92.32% , the same as the original HufuNet, thus allowing correct ownership verification. Adaptiveness against Parameter Adjustment. We also evaluate HufuNet against functionalityequivalent attack through parameter adjustment. We change the parameter values of each layer by multi plying these parameters by 𝑐=2𝑛(n is a randomlychoose integer for each layer ranging from 18 to 18), while still maintaining the functionalityequivalent of the overall model. We use the approach as discussed in Section 4.4 to restore the parameter values. The experiment result are shown in Table 6. After adjusting parame ters of the DNNswatermarked (PMCG, i.e., parameter change), the accuracy of VGG11, Resnet18 and GoogLeNet remains the same (functionally equivalent), but the accuracy of the combined Hufu Net (without restore) decreases significantly to 14.68%,15.79% and25.52% respectively. After restore however, the accuracy of the combined HufuNet resumes to 92.32% , the same as the original HufuNet, thus allowing ownership verification. Adaptiveness against Channel Expansion. We also evaluate HufuNet against channel expansion on VGG11. We randomly choose two adjacent layers (the fourth and fifth layers), and double the output dimension of the fifth layer and input dimension of the sixth layer by supplementing output/input channels (details in Appendix A). Finally, we launch the structure adjustment attack to make it more difficult to trace the EPH. We use the approach discussed in Section 4.4 to restore EPH. After channel expansion and structure adjustment, the accuracy of VGG11 remains the same (functionally equivalent), but the accuracy of the combined HufuNet (without restore) drops to 9.91%. After restore however, the accuracy of theTable 5: Adaptiveness against Structure Adjustment DNN VarianceAccuracy of DNNwatermarkedHufuNet Accuracy VGG11Original 77.77% 92.32% Reorder 77.77% 18.15% Restore 77.77% 92.32% Resnet18Original 86.14% 92.32% Reorder 86.14% 13.71% Restore 86.14% 92.32% GoogLeNetOriginal 90.85% 92.32% Reorder 90.85% 8.22% Restore 90.85% 92.32% Table 6: Adaptiveness against Parameter Adjustment DNN VarianceAccuracy of DNNwatermarkedHufuNet Accuracy VGG11Original 77.77% 92.32% PMCG 77.77% 22.29% Restore 77.77% 92.32% Resnet18Original 86.14% 92.32% PMCG 86.14% 14.68% Restore 86.14% 92.32% GoogLeNetOriginal 90.85% 92.32% PMCG 90.85% 25.52% Restore 90.85% 92.32% PMCG refers to the DNNwatermarked whose parameters have been adjusted to launch functionalityequivalent attack. combined HufuNet resumes to 92.32%, the same as the original HufuNet, thus allowing ownership verification. 5.5 Against Synthetic Attacks After stealing a DNN model, the adversaries may try their best to undermine any watermarks embedded inside, e.g., utilizing fine tuning, pruning, functionalityequivalent attack (structure/parame ter adjustment and channel expansion). Below, we synthesized an advanced attack by combining the above attack methods to simu late the adversaries’ best effort, and evaluate HufuNet against such a synthetic attack. First, we randomly choose two adjacent layers (4th, and 5th in our experiment) of VGG11 (with our watermark EPH embedded) to launch the channel expansion attack. Then, we finetune VGG11 as we did in Section 5.3 using the test dataset in 100 epochs. Afterward, we randomly select six layers to launch the functionalityequivalent attack. The structure adjustment is ap plied to two layers. The parameter adjustment is applied to another two layers, and both structure adjustment and parameter adjust ment are applied to the other two layers. In the end, we prune it with the pruning rate of 10%. For watermark detection, we retrieve 𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 from the attacked model at its original embedding locations and merge 𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 with our local SPH to obtain 𝐻𝑢𝑓𝑢𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 . As shown in Table 7, at the pruning rate of 10%, the accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 without restoring (“Direct” in the table) drops to 10.23%. After utilizing the restore approach in Section 4.4, the accuracy of 𝐻𝑢𝑓𝑢𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 increases to 90.74% with 100% parame ters correctly restored, thus allowing ownership verification. We also evaluate our restore approach against heavy pruning over the stolen model, up to 50%. With the pruning rate of 30%, Hufu Net accuracy after restore is 83.95%, still above the threshold for 9Table 7: Against Synthetic Attacks Pruning Rate VGG11HufuNet AccuracyRestored RateDirect Restored 10% 73.85% 10.23% 90.74% 100.00% 20% 73.85% 11.13% 88.08% 99.85% 30% 73.80% 8.87% 83.95% 96.73% 40% 43.30% 10.56% 51.25% 88.82% 50% 9.20% 10.14% 10.17% 75.15% ownership verification (80%), with 96.73% of all embedded kernels correctly restored. The reasons that the restore rate is still above 96% even at the pruning rate of 30% are twofold. On the one hand, largervalue parameters contribute significantly to both the cosine similarity and SVD, while pruning starts to zeorize smallervalue parameters with largervalue ones unchanged. On the other hand, the cosine similarity and SVD compare the stolen model and the watermarked model at the granularity of channels, each of which includes a large number of parameters, i.e., 64∗3∗3. Even at prun ing rate of 30%, there still exist lots of parameters (almost half on average) unchanged in most of the channels, resulting in higher cosine similarity and SVD values based on our experiments. At the pruning rate of 40% however, the accuracy of HufuNet after restore is 51.25% (below the threshold of 80%), but the accuracy of the stolen VGG11 drops to 43.30%, making it almost useless in performing its original task. Overall, based on the evaluation results, synthetic attacks launched by more capable adversaries can still be caught by our HufuNet watermark with the pruning rate up to 30%. As the pruning rate keeps increasing, i.e., above 40%, HufuNet fails ownership verification, but the adversaries end up with an almost useless stolen model (accuracy below 45%). 5.6 Stealthiness According to the results in Section 5.2, our watermark EPH only occupies a small portion of the DNNswatermarked, thus difficult to be noticed. Although there exists a correlation (i.e., HMAC) be tween the embedding location in DNNwatermarked and the kernel parameters plus their indices in HufuNet, the strength of the key used to compute the HMAC and the secrecy of the indices in Hufu Net make learning such correlation computationally hard. Hence, it is difficult for adversaries to learn the existence of the embedded watermark. Instead, they may resort to a statistical approach, e.g., examining the distribution of parameter values or their gradients of correct labels for any embedded watermark’s footprint. The adversaries with expertise in deep learning may have suf ficient knowledge and understanding of their stolen model, e.g., roughly how the parameter values distribute. Therefore, we mea sure whether our watermark EPH will change the distribution of the parameter values in DNNwatermarked. We find that the dis tribution of the parameter distribution of DNNtobeprotected (without EPH) and DNNwatermarked (with EPH) is too close to be distinguished, as shown in Figure 4(a) in Appendix. In addition, GradCAM [ 33] finds that the gradients of the score of true labels flowing back are globalaveragepooled to obtain the neuron im portance weights. The experienced adversaries are very likely to analyze the gradient distribution of neurons to discover any em bedded watermark. Therefore, we choose 8,000 input images in the CIFAR10 testing dataset, and for each correct label, compute the gradient of EPH and other parameters in DNNwatermarked. Wefind their distribution closely resembles each other, thus hard for adversaries to learn any embedded watermark evidence. The gradi ent distribution of EPH and other parameters in DNNwatermarked are compared in Figure 4 (b) in Appendix. 5.7 Security An intuitive idea is that the adversary forges a valid HufuNetstyle watermark from the public dataset 𝐷𝑠and embeds it into DNN watermarked. In particular, the adversary first trains a HufuNet from𝐷𝑠; he/she then chooses a secret key and embeds EPH from the forged HufuNet into DNNwatermarked following the watermark embedding algorithm. In case of ownership dispute, the adver sary may perform ownership verification on his/her stolen model, from which the forged watermark can be detected following the ownership verification algorithm. In such a case, the adversary’s secret key would be different from the owner’s secret key with an overwhelming probability. Therefore, it is highly unlikely that the adversary’s forged watermark overwrites the owner’s watermark in DNNwatermarked. Consequently, the adversary can only present a model with both his/her watermark and the owner’s watermark detected, while the owner of the model can present the original DNNwatermarked with only the owner’s watermark detected. It is thus clear who is the true owner to resolve the dispute. Therefore, the adversary has to leverage existing parameters in the stolen model rather than revising any. Meanwhile, two require ments have to be satisfied to forge a valid and effective HufuNet style watermark: (R1) the accuracy of the forged HufuNet passes the threshold; (R2) the positions where the embedded kernels (EPH) are retrieved should satisfy the correlation based on Line 3 in Algo rithm 1. Below we discuss and evaluate two different watermark forging attacks depending on which requirement to satisfy first: the first accuracy then correlation and first correlation then accuracy. First accuracy then correlation The adversary first trains his/her own HufuNet by designing a simple network structure and using the public training dataset. Once thoroughly tested, the adversary also divides it into two pieces and keeps the right piece as his/her SPH. Instead of embedding the left piece, the adversary searches the stolen model’s parameter space for each of the convolution kernel of his/her EPH. An exact match of all the 3∗3values of a kernel is hard to be found in general, so the adversary searches for closely matched values, e.g., within the range of 25% less and 25% more of the searched value, which may still function similarly in the neural network. With those values found and retrieved (if lucky enough), the adversary can rebuild a 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 by combining them with the local SPH, and test its accuracy on the public training dataset. Note that the high accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 is not enough. The adversary also needs to build the correlation between the embedding location and the parameter values of the kernel plus its original index in their HufuNet based on an appropriate key, according to Line 3 in Algorithm 1. We simulate the attack in two steps. In the first step, we train a HufuNet and check whether we can find a closelymatched kernel in DNNwatermarked (VGG11, Resnet18 and GoogLeNet) for each kernel in EPH, by setting the range as between below n% and above 10n% of the searched values5. However, even when n% is set to be large, e.g., 200%, the small values (e.g., 10e7) in kernels are still difficult to be matched. Therefore, we ignore those small values when we perform kernel matches, since they generally contribute very little to the classification task of DNN models. The threshold to filter out the small values is chosen by setting the pruning rate to be 10%, because the accuracy of HufuNet decreases significantly at a larger pruning rate. As shown in Table 8, when we set the range n% to be 25%, lots of kernels in EPH cannot find matches in DNN watermarked, i.e., total 64.90%, 47.86%, 52.92% found in VGG11, GoogLeNet, and Resnet18, respectively. Until we increase the range to 75%, 90%, and 110% for VGG11, GoogLeNet, and Resnet18 re spectively, all the kernels of EPH can be matched. However, the accuracy of such 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 is quite low, i.e., 53.74%, 60.96%, and 34.00% respectively, failing the adversary’s goal of forging a valid watermark. In the second step, we show the computational difficulty for even a determined adversary to build the correlation for the closelymatched kernels by traversing all possible keys for HMAC function. It takes about 2.18∗10−4seconds to compute one HMAC operation (i.e., SHA256 in our experiment) using our server with Intel Xeon E52620 v4@2.10GHz CPU. According to [ 28], the recommended size of key for SHA256 is 64 bytes, which results in 2512different keys for SHA256. Therefore, it takes 2.18∗10−4∗2512 seconds for the adversary to traverse all the keys to try the correla tion for one kernel in the forged HufuNet, which is computationally hard. Note that such brute force cannot ensure an appropriate key can always be found to build the correlation for all the kernels (i.e., 27,952) in the forged HufuNet. First correlation then accuracy. Since both the training dataset 𝐷𝑠and the testing dataset 𝐷𝑠_𝑇𝐸𝑆𝑇 are standardized for training and verifying HufuNet (see Section 4.1), the adversaries cannot forge a valid HufuNet with arbitrary datasets. However, it does not rule out the possibility of forging a valid HufuNet from the public dataset 𝐷𝑠 and𝐷𝑠_𝑇𝐸𝑆𝑇 with high accuracy. It is possible that the parameter space of DNNwatermarked is large and includes sufficient values required by an adversary to match a forged HufuNet. However, to prevent the adversaries from associating the matched values between DNNwatermarked and a forged HufuNet, our embedding algorithm correlates each EPH kernel’s embedding location in DNN watermarked with a secret key, the parameter values of the EPH kernel, and its index through a secure HMAC function. Since Algorithm 1 is public, the adversary may randomly se lect a key, test each kernel in the stolen model based on Line 3 in Algorithm 1, and try to find all the kernels that satisfy the correla tion. The adversary can explore quite several different keys, e.g., 1,000, and check which key finds the most kernels that satisfy the correlation. If enough such kernels could be found based on a partic ular key, the adversary can claim those kernels are his/her EPH. To forge a valid HufuNetstyle watermark, the adversary simply places those kernels into the convolution layers, randomly initializes a fully connected layer, and trains the fully connected layer using the public dataset 𝐷𝑠. Such watermark might have high accuracy on the public testing dataset as well. Although theoretically plausible, our experiment below shows that it is computationally tricky for 5Based on our experience, a larger range makes it easy to find matched kernels, but cannot guarantee the high accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 , thus failing the ownership claim.Table 8: Security of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡 ModelsFound Percentage at Different Thresholds HufuNet Accuracy125% 50% 75% 80% 90% VGG11 64.90% 97.80% 100% 100% 100% 53.74% Resnet18 52.93% 93.24% 99.99% 100% 100% 34.00% GoogLeNet 47.86% 87.76% 99.84% 99.97% 100% 60.96% 1HufuNet Accuracy is measured when 100% kernels are found at the lowest range for each model. Table 9: Integrity of HufuNet VGG11 GoogLeNet Resnet18 Resnet34 LSTM Innocent 1113.06% 14.39% 10.02% 12.21% 10.21% Innocent 2210.59% 10.00% 9.27% 10.00% 9.99% 1Innocent 1 refers to the innocent model with the same architecture and the same task trained with the same dataset as ours. 2Innocent 2 refers to the innocent model with the same architecture and the same task but trained with different dataset than ours. the adversary to discover an appropriate secret key to find him/her enough kernels satisfying the correlation. We train VGG11 with our HufuNet watermark embedded, as DNNwatermarked. After stealing the model, the adversaries can try a random key on all the 1,024,192 kernels in VGG11, to find those kernels that satisfy the correlation as Line 3 in Algorithm 1. We simulate this attack by testing 100 random keys, and find at most 3 kernels that can satisfy the desired correlation. With such a limited number of kernels, it is impossible to build convolutional layers of any DNN model. Brute force searching of all keys is computationally hard based on our evaluation. In the worst case, the adversary needs 2.18∗10−4∗2512seconds to traverse all the keys for just one kernel as presented above. Note that brute force searching of all possible keys may let the adversary obtain our key used to embed our HufuNet, but it is computationally hard to traverse all the keys (a key of 64 byte long means 2256different keys in total.). 5.8 Integrity We evaluate the integrity of HufuNet by testing its ownership ver ification on innocent models not copied or retrained from ours. In this experiment, we intend to use the innocent models that al ways share the same network structure and the same task as ours, thus highly resembling our watermarked models. In particular, we consider two different kinds of such innocent models: (Innocent 1) the same training dataset as DNNwatermarked and (Innocent 2) a different training dataset than DNNwatermarked. We train each innocent model for 100 epochs, and each epoch begins with different initialization parameters to ensure high accuracy. We use the models obtained above as the innocent models, without em bedding our watermark inside them. Upon ownership verification, we build𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 by combining the local SPH and the 𝐸𝑃𝐻𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 from the innocent models based on the computed embedding locations. The experimental results are shown in Table 9. The accuracy of 𝐻𝑢𝑓𝑢𝑁𝑒𝑡𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑 is 11.98% for Innocent 1 and 9.97% for Innocent 2 on average. Therefore, none of the innocent models can be falsely claimed as ours. For other models with dif ferent structures, main tasks, or training datasets, it is unlikely for HufuNet to claim ownership falsely. 11Table 10: Fidelity of HufuNet VGG11 GoogLeNet Resnet18 Resnet34 LSTM w/o EPH177.16% 90.71% 85.67% 69.98% 87.35% w/ EPH277.77% 90.85% 86.14% 70.20% 87.08% Performance change3 +0.61 % +0.14 % +0.47 % +0.22 % 0.27% 1w/o EPH represents the DNNtobeprotected, without our EPH. 2w/ EPH indicates DNNwatermarked, with our EPH. 3The performance change of the watermarked model compared with the original model. The positive value indicates performance improvement. 5.9 Fidelity Ideally, our watermark EPH should have a negligible impact on the performance of the DNNtobeprotected. To demonstrate such fidelity, we perform experiments on the models trained on datasets as in Table 14 and evaluate the accuracy of DNNstobeprotected and DNNswatermarked. Table 10 shows that the accuracy of DNNs watermarked is always quite close to that of DNNstobeprotected. We believe it is mainly because our watermark EPH only occupies a small portion of the entire parameters of DNNswatermarked and participates in the training of DNNswatermarked, thus incurring little impact on their accuracy. 5.10 Comparison with Stateoftheart We compare our HufuNet with the stateoftheart watermark ap proach [ 1] mainly in the aspect of model finetuning. Firstly, we train Resnet18 with the same hyperparameters and the same dataset using the same preprocessing (i.e., normalization, random crop ping) as in [ 1]. Then, regarding finetuning the model, according to [7], setting a larger initial learning rate and then properly decreas ing it will effectively destruct the embedded watermark without compromising the performance of the model. So we use a larger ini tial learning rate of 0.05 and decrease it to 1e7 gradually after 100 epochs by cosine annealing. To finetune the DNNswatermarked using the backdoor approach [ 1] and our HufuNet, respectively. The two watermarks embedded into the Resnet18 model are fine tuned using 8,000 images and verified using another 2,000 images from the CIFAR10 testing dataset. Table 11 shows the experimental results. We find that the finetuning does not impact the accuracy of the watermarked models for both approaches. However, the ac curacy of their backdoor based watermark [ 1] drops significantly from the beginning of finetuning and continues decreasing as the number of epochs increases. In contrast, the accuracy of HufuNet tends to be stable, always above 92%, resulting in 100% ownership verification (as in Section 5.2, with 𝜏𝑎𝑐𝑐=15% , above 80% HufuNet accuracy indicates ownership verification). We also evaluate the two watermark approaches against pruning. With the pruning rate up to 60%, we find both approaches demonstrate good robustness against pruning, with 100% ownership verification. 6 Related Work "
500,Towards Visual Saliency Explanations of Face Verification.txt,"In the past years, deep convolutional neural networks have been pushing the
frontier of face recognition (FR) techniques in both verification and
identification scenarios. Despite the high accuracy, they are often criticized
for lacking explainability. There has been an increasing demand for
understanding the decision-making process of deep face recognition systems.
Recent studies have investigated the usage of visual saliency maps as an
explanation, but they often lack a discussion and analysis in the context of
face recognition. This paper concentrates on explainable face verification
tasks and conceives a new explanation framework. First, a definition of the
saliency-based explanation method is provided, which focuses on the decisions
made by the deep FR model. Then, a new model-agnostic explanation method named
CorrRISE is proposed to produce saliency maps, which reveal both the similar
and dissimilar regions of any given pair of face images. Besides, two
evaluation metrics are designed to measure the performance of general visual
saliency explanation methods in face verification. Consequently, substantial
visual and quantitative results have shown that the proposed CorrRISE method
demonstrates promising results in comparison with other state-of-the-art
explainable face verification approaches.","Recent years have witnessed great advances in face recognition (FR) technologies due to the rapid develop ment of deep learning techniques. Current deep face recog nition systems achieve nearperfect performance on well known public benchmarks and have been widely deployed in several applications, such as access control and surveil lance. However, the deployment of such biometric sys tems poses a potential threat to privacy and data protection rights, resulting in serious public concern. Besides, deep learningbased systems are often criticized for their “black box” nature, making it difficult to interpret their predictions. To address these issues, it is essential to comprehend the decisionmaking process of deep face recognition technologies, thereby improving their performance and making them more widely accepted in society. This paper focuses on a crucial problem in face recogni tion, i.e. explainable face verification, specifically develop ing insightful explainability tools to interpret the decision making process of a deep face verification system. Vari ous saliency mapbased algorithms have been proposed as forms of explainable artificial intelligence (XAI) to high light either the internal CNN layers [2,23,35] or the impor tant pixels of the input image that are relevant to the model’s decision [3, 5, 18, 27, 29, 36, 38]. While many of them have achieved impressive results, they are mainly designed for classification and detection tasks. Face verification differs from other vision tasks not only due to the notable difference in the output format, but also due to the decisionmaking process, which often involves two images. Explaining an FR model does not mean sim ply highlighting the critical areas using importance maps but, beyond this, it should also interpret why the given face images are matching or nonmatching to the verifi cation system [31]. In this context, this work first sum marizes and improves the mainstream definition of visual saliency mapbased explanations for face verification sys tems. Specifically, an explanation method should reveal the similar region when the FR system believes the input images are matching and the dissimilar region if they are nonmatching. Then a Correlationbased Randomized In put Sampling for Explanation (CorrRISE) algorithm is pro posed, which is modelagnostic and capable of highlighting both the similar and dissimilar regions between any two in put face images. In addition, this paper proposes a new ob jective evaluation methodology to compare different state oftheart XFR methods in a quantitative manner. The con tributions of this paper can be summarized as follows: • A modelagnostic explanation method called Cor rRISE is proposed to highlight the similarity and dis similarity regions between any two face images. • A new evaluation methodology is conceived to quan titatively measure the performance of general saliency mapbased explanation methods for face verification.arXiv:2305.08546v3  [cs.CV]  17 Jul 2023• Extensive experiments on multiple face verification scenarios have been presented, demonstrating the ef fectiveness of the proposed method. 2. Related Work "
10,Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes.txt,"Wide neural networks with random weights and biases are Gaussian processes,
as originally observed by Neal (1995) and more recently by Lee et al. (2018)
and Matthews et al. (2018) for deep fully-connected networks, as well as by
Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional
networks. We show that this Neural Network-Gaussian Process correspondence
surprisingly extends to all modern feedforward or recurrent neural networks
composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph)
convolution, pooling, skip connection, attention, batch normalization, and/or
layer normalization. More generally, we introduce a language for expressing
neural network computations, and our result encompasses all such expressible
neural networks. This work serves as a tutorial on the *tensor programs*
technique formulated in Yang (2019) and elucidates the Gaussian Process results
obtained there. We provide open-source implementations of the Gaussian Process
kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at
github.com/thegregyang/GP4A.","Motivated to understand the Bayesian prior in neural networks (NNs), Neal [41] theoretically showed that inﬁnitely wide, shallow neural networks with random weights and biases are Gaussian processes (GPs). He empirically explored this phenomenon over deep networks as well, but this was not proven rigorously until recently [ 37,40,43,18], with concrete progress made over the intervening years [56,34,22,13]. This neural networkGaussian process correspondence (NNGP correspondence) has not only allowed one to transform the implicit prior of NNs into explicit priors that can be understood analytically [ 46,49,63,59,65], but has also created new stateoftheart kernels by converting from deep neural networks [ 37,43]. Yet, so far the focus has dwelled entirely on multilayer perceptrons (MLPs) or simple convolutional neural networks (CNNs). As new architectures are created with blistering speed, a question starts to emerge and reverberate: Do all inﬁnitely wide, randomly initialized neural networks correspond to Gaussian processes? Even if the answer is yes, at the current rate where each new architecture warrants its own NNGP correspondence paper, theory will never catch up to practice. On a more basic level, what does this question even mean for recurrent neural networks? Our Contributions In this paper, we formulate the notion of a Gaussian process with variable dimensional output (see Deﬁnition 2.1), and show that feedforward and recurrent neural networks ofstandard architectures converge to Gaussian processes in this sense as their widths or number Please see https://arxiv.org/abs/1910.12478 for the full version of this paper. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1910.12478v3  [cs.NE]  8 May 2021of channels go to inﬁnity, when their weights and biases are randomized. Bystandard architecture we mean any architecture that is some composition of multilayer perceptrons (MLPs), recurrent neural networks (RNNs) (e.g., LongShort Term Memory (LSTM) [ 26] or Gated Recurrent Unit (GRU) [ 10]), skip connections [24, 27] , convolutions [ 16,17,47,35,36] or graph convolutions [8,25,15,38,14,31], pooling [ 35,36], batch normalization (batchnorm) [ 28], layer normalization [1] and/or attention [ 2,55].Even more broadly, we design a new language, NETSOR , for expressing neural network computations, and show the GP convergence for all such expressible networks. By demonstrating that NETSOR can implement any network of standard architectures, we obtain the aforementioned results as a corollary. The results for RNNs, batchnorm, layernorm, attention, and their combination with other layers are new. We opensource reference implementations2for the GP kernels of simple RNN, GRU, transformer, and feedforward batchnorm network; see Fig. 3 for an illustration. Relation of This Paper with [60] This paper serves several purposes. 1) Introduce the reader to the tensor programs technique formulated in [ 60], using the Neural NetworkGaussian Process Correspondence as motivation. 2) Promote a redesigned set of notations for tensor programs that hopefully makes the understanding and the application of this technique easier. 3) Prove a more general version of the Gaussian Process results ﬁrst presented in [ 60]. 4) Provide example calculations and reference implementations2of the GP kernels for several architectures like the vanilla RNN, GRU, batchnorm network, and transformers. We assume the reader has not read [ 60] and seek to explain all results in elementary terms. However, we will provide commentary in footnotes throughout the paper on differences from [60]. Regarding 1), this paper will be the ﬁrst in a series to explain the tensor programs technique, each covering a more powerful type of tensor programs, and each motivated by speciﬁc theorems that can be proved or calculations made possible by these new tensor programs. In particular, here we will only talk about tensor programs without matrix transposes. Regarding 3), the results presented here will supersede all results in [ 60] concerning Gaussian Processes, with one caveat that here we will not cover architectures using both a weight Wand its transpose W>in its forward pass (but this result will come for free in a later paper in this series). 2 Gaussian Process with VariableDimensional Output We ﬁrst clarify the notion of a Gaussian process with variable dimension output. Deﬁnition 2.1 (Gaussian Process) .We say a random function f:X!Rm(with ﬁxed dimen sional output) is a Gaussian process if for any ﬁnite subset fx1;:::;xkgX, the random vector (f(x1);:::;f (xk))2Rmkis distributed as a kmdimensional Gaussian. If fhas variable dimen sional output (e.g. fis an RNN), such as when f(x)2Rl(x)for some length function l:X!N3, then we say fis a Gaussian process if for any ﬁnite subset fx1;:::;xkgX, the random vector (f(x1);:::;f (xk))is distributed as a (P il(xi))dimensional Gaussian. To illustrate a GP with variabledimensional output, consider a simple RNN that runs on two input sequences given by the GloVe embeddings [44]4of the words of the two sentences sentence 1 (7 words): “The brown fox jumps over the dog.” sentence 2 (9 words): “The quick brown fox jumps over the lazy dog.”(?) A pseudocode is given in Program 2 in Section 4 (ignore the type annotations like G(n);H(n);A(n) for now). The RNN emits a single scalar after reading each token (in Program 2, this isv>sia=pn, wheresiais the RNN state after reading the ith token of the ath sentence, and vis the readout layer); this number takes into account all of the word embeddings read so far. Thus, it will output a total of 7 scalars after reading sentence 1, and a total of 9 scalars after reading sentence 2. To say that this RNN is a GP would imply that all 7 + 9 = 16 scalars are jointly Gaussiandistributed (corresponding to a 1616kernel), over the randomness of the weights and biases imbued during initialization. This 2github.com/thegregyang/GP4A 3i.e.f:Q x2XRl(x)is a dependent function 4The embedding associates each word to a real vector of 100 dimensions such that semantically similar words are mapped to closer vectors 2is indeed the empirical phenomenon with a width1000 RNN, and Fig. 2(E) visualizes the the joint distribution of the last scalars output by the RNN at the end of each sentence. It clearly exhibits a Gaussian nature, and perfectly ﬁts the theoretically predicted Gaussian distribution (dashed ovals), which we shall describe in Corollary 5.5. 3 Recap: GP Behavior of a Multilayer Perceptron (MLP) Before explaining our main results, we ﬁrst review the argument from prior works [ 37,40,43] for the GP convergence of a wide MLP with randomly initialized weights and biases, and we also demonstrate why such an argument is inadequate for RNNs. Consider an MLP with widths fnlgl, weight matricesfWl2Rnlnl"
190,Make Sure You're Unsure: A Framework for Verifying Probabilistic Specifications.txt,"Most real world applications require dealing with stochasticity like sensor
noise or predictive uncertainty, where formal specifications of desired
behavior are inherently probabilistic. Despite the promise of formal
verification in ensuring the reliability of neural networks, progress in the
direction of probabilistic specifications has been limited. In this direction,
we first introduce a general formulation of probabilistic specifications for
neural networks, which captures both probabilistic networks (e.g., Bayesian
neural networks, MC-Dropout networks) and uncertain inputs (distributions over
inputs arising from sensor noise or other perturbations). We then propose a
general technique to verify such specifications by generalizing the notion of
Lagrangian duality, replacing standard Lagrangian multipliers with ""functional
multipliers"" that can be arbitrary functions of the activations at a given
layer. We show that an optimal choice of functional multipliers leads to exact
verification (i.e., sound and complete verification), and for specific forms of
multipliers, we develop tractable practical verification algorithms.
  We empirically validate our algorithms by applying them to Bayesian Neural
Networks (BNNs) and MC Dropout Networks, and certifying properties such as
adversarial robustness and robust detection of out-of-distribution (OOD) data.
On these tasks we are able to provide significantly stronger guarantees when
compared to prior work -- for instance, for a VGG-64 MC-Dropout CNN trained on
CIFAR-10, we improve the certified AUC (a verified lower bound on the true AUC)
for robust OOD detection (on CIFAR-100) from $0\% \rightarrow 29\%$. Similarly,
for a BNN trained on MNIST, we improve on the robust accuracy from $60.2\%
\rightarrow 74.6\%$. Further, on a novel specification -- distributionally
robust OOD detection -- we improve the certified AUC from $5\% \rightarrow
23\%$.","While neural networks (NNs) have shown signiﬁcant promise i n a widerange of applications (for e.g., [He et al., 2016, Yu and Deng, 2014]), a keybottleneck towards their widespread adoption in safetycritical applications is the lack of formal guarant ees regarding safety and performance. In this direction, there has been considerable progress towar ds developing scalable methods that can provide formal guarantees regarding the conformance of NNs with desired properties [Katz et al., 2017, Dvijotham et al., 2018b, Raghunathan et al., 2018]. Ho wever, much of this progress has been ∗Equal contribution. Authors listed in alphabetical order. Correspondance to lberrada@deepmind.com , sdathath@deepmind.com ,dvij@cs.washington.edu . †DeepMind, London, United Kingdom. 35th Conference on Neural Information Processing Systems ( NeurIPS 2021), Sydney, Australia.in the setting where the speciﬁcations and neural networks d o not exhibit any probabilistic behaviour, or is mostly specialized for speciﬁc probabilistic speciﬁc ations [Weng et al., 2019, Wicker et al., 2020]. In contrast, we introduce a general framework for ver ifying speciﬁcations of neural networks that are probabilistic. The framework enables us t o handle stochastic neural networks such as Bayesian Neural Networks or MonteCarlo (MC) dropou t networks, as well as probabilistic properties, such as distributionally robust outofdistr ibution (OOD) detection. Furthermore, the speciﬁcation can be deﬁned on the output distribution from t he network, which allows us to handle operations such as the expectation on functions of the neura l network output. Probabilistic speciﬁcations are relevant and natural to ma ny practical problems. For instance, for robotics applications, there is uncertainty arising fr om noisy measurements from sensors, and uncertainty regarding the actions of uncontrolled agents ( e.g. uncertainty regarding the behaviour of pedestrians for a selfdriving vehicle). Often these uncer tainties are modelled using a probabilistic approach, where a distribution is speciﬁed (or possibly lea rnt) over the feasible set of events [Thrun et al., 2005]. In such cases, we want to provide guaran tees regarding the network’s conformance to desired properties in the distributional se tting (e.g. given a model of the pedestrian’s uncertain behaviour, guarantee that the probability of col lision for the autonomous vehicle is small). A more general problem includes scenarios where there is unc ertainty regarding the parameters of the distribution used to model uncertainty. Here, in this ge neral setting, we seek to verify the property that the network behaviour conforms with the desired speciﬁ cation under uncertainty corresponding to an entire set of distributions. The key to handling the aforementioned complexity in the spe ciﬁcations being veriﬁed through our framework is the generalization of the Lagrangian duali ty. Speciﬁcally, instead of using the standard Lagrange duality where the multipliers are linear , we allow for probabilistic constraints (constraints between distributions) and use functional mu ltipliers to replace the linear Lagrange multipliers. This allows us to exploit the structure of thes e probabilistic constraints, enabling us to provide stronger guarantees and facilitates the veriﬁ cation of veriﬁcationagnostic networks (networks that are not designed to be veriﬁable). In our pape r, we focus on veriﬁcationagnostic networks as this is desirable for many reasons, as noted in Da thathri et al. [2020]. To summarize, our main contributions are: • We derive a general framework that extends Lagrangian dual ity to handle a wide range of probabilistic speciﬁcations. Our main theoretical resu lt (Theorem 1) shows that our approach (i) is always sound and computes an upper bound on th e maximum violation of the speciﬁcation being veriﬁed, and (ii) is expressive enou gh to theoretically capture tight veriﬁcation (i.e. obtaining both sound and complete veriﬁc ation). • We develop novel algorithms for handling speciﬁc multipli ers and objectives within our framework (Propositions 1, 2). This allows us to apply ou r framework to novel speciﬁcations (such as distributionally robust OOD detect ion, where input perturbations are drawn from entire sets of distributions) by better captu ring the probabilistic structure of the problem. • We empirically validate our method by verifying neural net works, which are veriﬁcation agnostic, on a variety of probabilistic speciﬁcations. We d emonstrate that even with relatively simple choices for the functional multiplier, o ur method strongly outperforms prior methods, which sometimes provide vacuous guarantees only. This further points towards the potential for signiﬁcant improvements t o be had by developing tractable optimization techniques for more complex and exp ressive multipliers within our framework. 2 Probabilistic Speciﬁcations 2.1 Notation Let us consider a possibly stochastic neural network φ:X→P(Y), whereXis the set of possible input values to the model, Yis the set of possible output values, and P(Y)is the set of distributions overY. We assume thatYis a subset of Rl(unless speciﬁed otherwise), where lis the number of labels, and the output of the model are logits corresponding to unnormalized logconﬁdence scores assigned to the labels {1,...,l}. 2The model is assumed to be a sequence of Klayers, each of them possibly stochastic. For k∈ {1,...,K},πk(xk|xk−1)denotes the probability that the output of layer ktakes value xkwhen its input value is xk−1. We write xk∼πk(xk−1)to denote that xkis drawn from the distribution over outputs of layer kgiven input xk−1to layerk. We further assume that each πk(x)has the form σ(˜wx+˜b), whereσis a nonlinear activation function (e.g., ReLU , sigmoid, MaxOut ), and˜wand ˜bare random variables. The stochasticity for layer πkis assumed to be statistically independent of the stochasticity at other layers. For a BNN, ˜wand˜bfollow a diagonal Gaussian distribution (i.e., a Gaussian distribution with a diagonal covariance matrix) , and for a MCDropout network they follow a Bernoullilike distribution. Given a distribution p0over the inputsX, we use φ(p0)to denote (with a slight abuse) the distribution of the random variable φ(X0), whereX0∼p0. 2.2 Problem Formulation. We now introduce the general problem formulation for which w e develop the veriﬁcation framework. Deﬁnition 1 (Probabilistic veriﬁcation problem) .Given a (possibly stochastic) neural network φ:X →P(Y), a set of distributions over the input P0and a functional ψ:P(Y)/mapsto→R, the probabilistic veriﬁcation problem is to check that the foll owing is true: ∀p0∈P0,ψ(φ(p0))≤0. (1) 2.3 Examples of Speciﬁcations Below we provide examples of probabilistic speciﬁcations w hich are captured by the above problem formulation, and that we further empirically validate our f ramework on. In Appendix A, we provide further examples of relevant speciﬁcations (e.g., ensurin g reliable uncertainty calibration) that can be handled by our problem setup. Distributionally Robust OOD Detection. We consider the problem of verifying that a stochastic neural network assigns low conﬁdence scores to all labels fo r OOD inputs, even in the presence of bounded noise perturbations to the inputs. Given a noise dis tribution perturbing an OOD image xood, we require that the expected softmax is smaller than a speciﬁ ed conﬁdence threshold pmaxfor each labeli. Since the precise noise distribution is most often unknown , we wish to consider an entire classPnoise of noise distributions. Denoting by δxthe Dirac distribution around x, the problem is then to guarantee that for every p0inP0={δxood+ω:ω∈Pnoise}and for each possible label i,ψ(φ(p0)):=Ey∼φ(p0)[softmax (y)i]−pmax≤0. Robust OOD detection under bounded ℓ∞ perturbations as considered in Bitterwolf et al. [2020] is a special case of this problem where Pnoise is restricted to a set of δdistributions over points with bounded ℓ∞norm. Robust Classiﬁcation. We also extend the commonly studied robust classiﬁcation pr oblem [Madry et al., 2017] under normbounded perturbations, to t he setting of probabilistic neural networks (e.g. BNNs). Deﬁne P0to be the set of δinput distributions centered at points within anǫball of a nominal point xnom, with label i∈{1,...,l}:P0={δx:/ba∇dblx−xnom/ba∇dbl≤ǫ}. For everyp0∈P0, we wish to guarantee that the stochastic NN correctly class iﬁes the input, i.e. for eachj,ψ(φ(p0)):=Ey∼φ(p0)[softmax (y)i−softmax (y)j]≤0. Note that it is important to take the expectation of the softmax (and not logits) since this is how inference from BNNs is performed. 3 The Functional Lagrangian Framework We consider the following optimization version: OPT= max p0∈P0ψ(φ(p0)), (2) HavingOPT≤0here is equivalent to satisfying speciﬁcation (1) . However , solving problem (2) directly to global optimality is intractable in general, be cause it can possibly be a challenging nonlinear and stochastic optimization problem. However, t o only verify that the speciﬁcation is satisﬁed, it may sufﬁce to compute an upper bound on OPT. Here, we describe how the functional Lagrangian framework allows to derive such bounds by decomp osing the overall problem into smaller, easier subproblems. 33.1 General Framework LetXkdenote the feasible space of activations at layer k, and let pkdenote the distribution of activations at layer kwhen the inputs follow distribution p0(so thatpK=φ(p0)). Assumptions. In order to derive our veriﬁcation framework, we make the fol lowing assumptions: (A1):∃l0≤u0∈Rnsuch that for each input distribution p0∈P0, Support (p0)⊆X0= [l0,u0]. (A2): Each layer is such that if x∈Xk= [lk,uk], then Support (πk(x))⊆Xk+1= [lk+1,uk+1]. Assumption (A1) is natural since the inputs to neural networ ks are bounded. Assumption (A2) can be restrictive in some cases: it requires that the layer outp ut is bounded with probability 1, which is not true, for example, if we have a BNN with a Gaussian poste rior. However, we can relax this assumption to requiring that the output is bounded with high probability, as in Wicker et al. [2020]. Functional Lagrangian Dual. In order to derive the dual, we begin by noting that problem (2 ) can be equivalently written in the following constrained fo rm: max p0∈P0,p1,...,pKψ(pK)s.t.∀k∈{0,...,K−1},∀y∈Xk+1, pk+1(y) =/integraldisplay Xkπk(y|x)pk(x)dx. For thekth constraint, let us assign a Lagrangian multiplier λk+1(y)to each possible y∈Xk+1. Note that λ(y)is chosen independently for each y, henceλis afunctional multiplier . We then integrate over y, which yields the following Lagrangian penalty to be added t o the dual objective: −/integraldisplay Xk+1λk+1(y)pk+1(y)dy+/integraldisplay Xk,Xk+1λk+1(y)πk(y|x)pk(x)dxdy. (3) We now make two observations, which are described here at a hi gh level only and are available in more details in appendix B. First, if we sum these penalties o verkand group terms by pk, it can be observed that the objective function decomposes additiv ely over the pkdistributions. Second, for k∈{1,...,K−1}, eachpkcan be optimized independently (since the objective is sepa rable), and since the objective is linear in pk, the optimal pkis a Dirac distribution, which means that the search over the probability distribution pkcan be simpliﬁed to a search over feasible values xk∈Xk. This yields the following dual: max pK∈PK/parenleftbigg ψ(pK)−/integraldisplay XKλK(x)pK(x)dx/parenrightbigg +K−1/summationdisplay k=1max x∈Xk/parenleftbigg/integraldisplay Xk+1λk+1(y)πk(y|x)dy−λk(x)/parenrightbigg + max p0∈P0/integraldisplay X0/parenleftbigg/integraldisplay X1λ1(y)π0(y|x)dy/parenrightbigg p0(x)dx, (4) where we deﬁnePK/definesφ(P0). In the rest of this work, we refer to this dual as g(λ), and we use the following notation to simplify equation (4): g(λ) = max p0∈P0g0(p0,λ1)+K−1/summationdisplay k=1max xk∈Xkgk(xk,λk,λk+1)+ max pK∈PKgK(pK,λK). (5) The dual g(λ)can be seen as a generalization of Lagrangian relaxation [Be rtsekas, 2015] with the two key modiﬁcations: (i) layer outputs are integrated over possible values, and (ii) Lagrangian penalties are expressed as arbitrary functions λk(x)instead of being restricted to linear functions. Main Result. Here, we relate the functional Lagrangian dual to the speciﬁ cation objective (2). Theorem 1. For any collection of functions λ= (λ1,...,λ K)∈RX1×...×RXK, we have that g(λ)≥OPT. In particular, if a choice of λcan be found such that g(λ)≤0, then speciﬁcation (1) is true. Further, when ψ(pK) =Ey∼pK[c(y)], the dual becomes tight: g(λ⋆) =OPTifλ⋆is set to: λ⋆ K(x) =c(x);∀k∈{K−1,...,1}, λ⋆ k(x) =E y∼πk(x)/bracketleftbig λ⋆ k+1(y)/bracketrightbig . Proof. We give a brief sketch of the proof  the details are in Appendi x B. The problem in constrained form is an inﬁnite dimensional optimization wi th decision variables p0,p1,...,p K 4and linear constraints relating pkandpk+1. The Lagrangian dual of this optimization problem has objective g(λ). By weak duality, we have g(λ)≥OPT. The second part of the theorem is easily observed by plugging in λ⋆ing(λ)and observing that the resulting optimization problem is equivalent to (2). Example. LetP0be the set of probability distributions with mean 0, variance 1, and support [−1,1], and letN[a,b](µ,σ2)denote the normal distribution with mean µand variance σ2with truncated support [a,b]. Now consider the following problem, for which we want to com pute an upper bound: OPT= max p0∈P0EX1[exp(−X1)]s.t.X1|X0∼N[0,1](X2 0,1)andX0∼p0. (6) This problem has two difﬁculties that prevent us from applyi ng traditional optimization approaches like Lagrangian duality [Bertsekas, 2015], which has been u sed in neural network veriﬁcation Dvijotham et al. [2018b]. The ﬁrst difﬁculty is that the cons traint linking X1toX0is stochastic, and standard approaches can not readily handle that. Second , the optimization variable p0can take any value in an entire set of probability distributions, whi le usual methods can only search over sets of real values. Thus standard methods fail to provide the too ls to solve such a problem. Since the probability distributions have bounded support, a possibl e way around this problem is to ignore the stochasticity of the problem, and to optimize over the worst case realization of the random variable X1in order to obtain a valid upper bound on OPTas:OPT≤maxx1∈[0,1]exp(−x1) = 1.However this is an overpessimistic modeling of the problem and the r esulting upper bound is loose. In contrast, Theorem 1 shows that for any function λ:R→R,OPTcan be upper bounded by: OPT≤max x1∈[0,1],p0∈P0exp(−x1)−λ(x1)+EX0∼p0[EX1|X0∼N[0,1](X2 0,1)[λ(X1)]]. This inequality holds true in particular for any function λof the form x/mapsto→θxwhereθ∈R, and thus: OPT≤inf θ∈Rmax x1∈[0,1],p0∈P0exp(−x1)−θx1+EX0∼p0[EX1|X0∼N[0,1](X2 0,1)[θX1]], = inf θ∈Rmax x1∈[0,1],p0∈P0exp(−x1)−θx1+θEX0∼p0[X2 0], = inf θ∈Rmax x1∈[0,1]exp(−x1)−θx1+θ≈0.37. Here, our framework lets us tractably compute a bound on OPTthat is signiﬁcantly tighter compared to the naive supportbased bound. 3.2 Optimization Algorithm Parameterization. The choice of functional multipliers affects the difﬁculty of evaluating g(λ). In fact, since neural network veriﬁcation is NPhard [Katz e t al., 2017], we know that computing g(λ⋆)is intractable in the general case. Therefore in practice, w e instantiate the functional Lagrangian framework for speciﬁc parameterized classes of Lagrangian functions, which we denote asλ(θ) ={λk(x) =λk(x;θk)}K k=1. Choosing the right class of functions λ(θ)is a tradeoff: for very simple classes (such as linear functions), g(λ(θ))is easy to compute but may be a loose upper bound on (2), while more expressive choices lead to tighter r elaxation of (2) at the cost of more difﬁcult evaluation (or bounding) of g(λ(θ)). Optimization. With some abuse of notation, for convenience, we write g0(x0,λ0,λ1):= g0(p0,λ1)andgK(xK,λK,λK+1):=gK(pK,λK), withλ0=λK+1= 0. Then the problem of obtaining the best bound can be written as: minθ/summationtextK k=0maxxkgk(xk,λk,λk+1), where the inner maximizations are understood to be performed over the appropriate domains ( P0forx0,Xk forxk,l= 1,...,K−1andPKforxK). The overall procedure is described in Algorithm 1: θis minimized by a gradientbased method in the outer loop; in the inner loop, the decomposed maximization problems over the xkget solved, potentially in parallel. During optimization, the inner problems can be solved approximately as long as they pr ovide sufﬁcient information about the descent direction for θ. 5Algorithm 1 Veriﬁcation with Functional Lagrangians Input: initial dual parameters θ(0), learningrate η, number of iterations T. fort= 0,...,T−1do{optimization loop } fork= 0toKdo{potentially in parallel } d(k) θ=∇θ/bracketleftbigg max xkgk(xk,λk,λk+1)/bracketrightbigg {potentially approximate maximization } end for θ(t+1)=θ(t)−η/summationtextK k=0d(k) θ{or any gradient based optimization } end for Return: Exact value or guaranteed upper bound on g(λ(θ(T))){final evaluation } Guaranteeing the Final Results. For the ﬁnal veriﬁcation certiﬁcate to be valid, we do requir e the ﬁnal evaluation to provide the exact value of g(λ(θ(T)))or an upper bound. In the following section, we provide an overview of novel bounds that we use in our experiments to certify the ﬁnal results. 3.3 Bounds for Speciﬁc Instantiations The nature of the maximization problems encountered by the o ptimization algorithm depends on the veriﬁcation problem as well as the type of chosen Lagrang ian multipliers. In some easy cases, like linear multipliers on a ReLU layer, this results in trac table optimization or even closedform solutions. In other cases however, obtaining a nontrivial upper bound is more challenging. In this section, we detail two such situations for which novel r esults were required to get tractable bounds: distributionally robust veriﬁcation and expected softmaxbased problems. To the best of our knowledge, these bounds do not appear in the literature a nd thus constitute a novel contribution. Distributionally Robust Veriﬁcation with Linexp Multipli ers. We consider the setting where we verify a deterministic network with stochastic inputs an d constraints on the input distribution p0∈P0. In particular, we consider P0={µ+ω:ω∼Pnoise}, wherePnoise denotes a class of zeromean noise distributions that all satisfy the propert y of having subGaussian tails (this is true for many common noise distributions including Bernoulli, G aussian, truncated Gaussian): SubGaussian tail: ∀i,∀t∈R,E[exp(tωi)]≤exp/parenleftbig t2σ2/2/parenrightbig . We also assume that each component of the noise ωiis i.i.d. The functional Lagrangian dual g(λ) only depends on the input distribution p0viag0, which evaluates to g0(p0,λ1) =Ex∼p0[λ1(x)]. If we choose λ1to be a linear or quadratic function, then g(λ)only depends on the ﬁrst and second moments of p0. This implies that the veriﬁcation results will be unnecess arily conservative as they don’t use the full information about the distribution p0. To consider the full distribution it sufﬁces to add an exponential term which evaluates to the moment genera ting function of the input distribution. Therefore we choose λ1(x) =αTx+exp/parenleftbig γTx+κ/parenrightbig andλ2(x) =βTx. The following result then gives a tractable upper bound on the resulting maximization problems: Proposition 1. In the setting described above, and with sas the elementwise activation function: max p0∈P0g0(p0,λ1)≤αT(wµ+b)+exp/parenleftBig/vextenddouble/vextenddoublewTγ/vextenddouble/vextenddouble2σ2/2+γTb+κ/parenrightBig , max x∈X1g1(x,λ1,λ2)≤max x∈X2,z=s(x)βT(w2z+b2)−αTx−exp/parenleftbig γTx+κ/parenrightbig . The maximization in the second equation can be bounded by sol ving a convex optimization problem (Appendix C.3). Expected Softmax Problems. Several of the speciﬁcations discussed in Section 2.3 (e.g. , distributionally robust OOD detection) require us to bound the expected value of a linear function of the softmax. For speciﬁcations whose function can be expr essed as an expected value: ψ(pK) = Ex∼pK[c(x)], by linearity of the objective w.r.t. the output distributi onpK, the search over the distribution pkcan be simpliﬁed to a search over feasible output values xK: max pK∈PKψ(pK)−/integraldisplay XKλK(x)pK(x)dx= max x∈XKc(x)−λK(x). (7) 6Given this observation, the following lets us certify resul ts for linear functions of the softmax (x): Proposition 2. For afﬁne λK, andc(x)with the following form c(x) =µTsoftmax (x), maxx∈XKc(x)−λK(x)can be computed in time O(3d), whereXK⊆Rd. We provide a proof of this proposition and a concrete algorit hm for computing the solution in Appendix C.2. This setting is particularly important to mea sure veriﬁed conﬁdence and thus to perform robust OOD detection. We further note that while the runtime is exponential in d,d corresponds to the number of labels in classiﬁcation tasks w hich is a constant value and does not grow with the size of the network or the inputs to the netwo rk. Further, the computation is embarassingly parallel and can be done in O(1)time if3dcomputations can be run in parallel. For classiﬁcation problems with 10classes (like CIFAR10 and MNIST), exploiting this paralle lism, we can solve these problems on the order of milliseconds on a clu ster of CPUs. 4 Related Work "
197,Probabilistic Verification of Neural Networks Against Group Fairness.txt,"Fairness is crucial for neural networks which are used in applications with
important societal implication. Recently, there have been multiple attempts on
improving fairness of neural networks, with a focus on fairness testing (e.g.,
generating individual discriminatory instances) and fairness training (e.g.,
enhancing fairness through augmented training). In this work, we propose an
approach to formally verify neural networks against fairness, with a focus on
independence-based fairness such as group fairness. Our method is built upon an
approach for learning Markov Chains from a user-provided neural network (i.e.,
a feed-forward neural network or a recurrent neural network) which is
guaranteed to facilitate sound analysis. The learned Markov Chain not only
allows us to verify (with Probably Approximate Correctness guarantee) whether
the neural network is fair or not, but also facilities sensitivity analysis
which helps to understand why fairness is violated. We demonstrate that with
our analysis results, the neural weights can be optimized to improve fairness.
Our approach has been evaluated with multiple models trained on benchmark
datasets and the experiment results show that our approach is effective and
efficient.","In recent years, neural network based machine learning has foun d its way into various aspects of people’s daily life, such as fraud detection [25], f acial recogni tion [47], selfdriving [13], and medical diagnosis [56]. Although neural n etworks have demonstrated astonishing performance in many applications, there are still concerns on their dependability. One desirable property of neural networks for applications with societal impact is fairness [2]. Since there are often societal bi ases in the training data, the resultant neural networks might be d iscriminative aswell.Thishasbeendemonstratedin[53].Fairnessissuesinneural networksare often more ‘hidden’ than those of traditional decisionmaking soft ware programs since it is still an open problem on how to interpret neural networks. Recently, researchers have established multiple formalization of fa irness re garding diﬀerent subpopulations [24,9,21,28]. These subpopulation s are often determined by diﬀerent values of protected features (e.g., race, religion and eth nic group), which are applicationdependent. To name a few, group fairness re quires that minoritymembers should be classiﬁedat an approximately same rate2 Bing Sun, Jun Sun, Ting Dai, and Lijun Zhang as the majority members [24,9], whereas individual discrimination (a.k .a. causal fairness) states that a machine learning model must output appro ximately the same predictions for instances which are the same except for cert ain protected features [21,28]. We refer readers to [51] for detailed deﬁnitions o f fairness. In this work, we focus on an important class of fairness called independ encebased fairness, which includes the abovementioned group fairness. Recently, there have been multiple attempts on analyzing and improv ing fairness of neural networks, with a focus on fairness testing (e.g ., generating in dividual discriminatory instances) and fairness training (e.g., enhan cing fairness through augmented training). Multiple attempts [27,54,4,58] have be en made on testing machine learning models against individual discrimination, which aims to systematically generate instances that demonstrate individual discrimination. While these approacheshaveimpressive performance in terms of ge neratingsuch instances, they are incapable of verifying fairness. Another line of approaches is on fairness training [16,3,12,36,14,28], this includes approaches which in corpo ratefairnessasanobjectiveinthemodeltrainingphase[16,3,12],a ndapproaches which adopt heuristics for learning fair classiﬁers [36]. While the exper iment re sults show that these approaches improve fairness to certain ext ent, they do not guarantee that the resultant neural networks are fair. In this work, we investigate the problem of verifying neural networ ks against independencebased fairness. Our aim is to design an approach whic h allows us to (1) show evidence that a neural network satisﬁes fairness if it is the case; (2) otherwise, provide insights on why fairness is not satisﬁed and how f airness can be potentially achieved; (3) provide a way of improving the fairness o f the neu ral network. At a highlevel, our approach is designed as follows. Giv en a neural network (i.e., either a feedforward or recurrent neural networ k), we systemat ically sample behaviors of the neural network (e.g., input/output pa irs), based on which we learn a Markov Chain model that approximates the neura l net work. Our algorithm guarantees that probabilistic analysis based on the learned MarkovChain model (such as probabilistic reachability analysis) is pro bably ap proximately correct (hereafter PACcorrect) with respect to a ny computational tree logic (CTL [11]) formulae. With the guarantee, we are thus able t o verify fairness property of the neural network. There are two outcom es. One is that the neural network is proved to be fair, in which case the Markov Ch ain is presented as an evidence. Otherwise, sensitivity analysis based on the Markov Chain is carried out automatically. Such analysis helps us to understa nd why fairness is violated and provide hints on how the neural network cou ld be im proved to achieve fairness. Lastly, our approach optimizes the pa rameters of the ‘responsible’ neurons in the neural network and improve its fairnes s. We have implemented our approach as a part of the SOCRATES frame  work [45]. We apply our approach to multiple neural network models (in clud ing feedforward and recurrent neural networks) trained on be nchmark datasets which are the subject of previous studies on fairness testing. The experiment results show that our approach successfully veriﬁes or falsiﬁes all the models. It also conﬁrms that fairness is a real concern and one of the netw orks (on theProbabilistic Veriﬁcation of Neural Networks Against Grou p Fairness 3 German Credit dataset) fails the fairness property badly. Throug h sensitivity analysis, our approach locates neurons which have the most contr ibution to the violation offairness. Further experiments show that by optimizing t he neural pa rameters (i.e., weights) based on the sensitivity analysis result, we c an improve the model’s fairness signiﬁcantly whilst keeping a high model accuracy . The remaining of the paper is organized as follows. In Section 2, we re view relevantbackgroundanddeﬁneourproblem.InSection3,wepres enteachstepof our approach in detail. In Section 4, we evaluate our approach thro ugh multiple experiments. We review related work in Section 5 and conclude in Sect ion 6. 2 Preliminary In this section, we review relevant background and deﬁne our prob lem. Fairness For classiﬁcation problems, a neural network Nlearns to predict a targetvariable Obasedonasetofinput features X.WewriteYastheprediction of the classiﬁer. We further write F⊆Xas a set of features encoding some protected characteristics such as gender, age and race. Fairne ss constrains how Nmakes predictions. In the literature, there are multiple formal deﬁ nitions of fairness [24,9,21,28]. In this work, we focus on independencebas ed fairness, which is deﬁned as follows. Deﬁnition 1 (Independencebased Fairness (strict)). A neural network Nsatisﬁes independencebased fairness (strict) if the prot ected feature Fis sta tistically independent to the prediction Y. We write Las the prediction set and we have∀l∈L,∀fi,fj∈F such that i/nega⊔ionslash=j, P(Y=l|F=fi) =P(Y=l|F=fj) (1) The deﬁnition states that, N’s prediction is independent of the protected feature F. This deﬁnition is rather strict and thus unlikely to hold in practice. Th e following relaxes the above deﬁnition by introducing a positive toleran ceξ. Deﬁnition 2 (Independencebased Fairness). LetNbe a neural network andξbe a positive realvalue constant. Nsatisﬁes independencebased fairness, with respect to ξ, if and only if,∀l∈L∀fi,fj∈F such that i/nega⊔ionslash=j, |P(Y=l|F=fi)−P(Y=l|F=fj)| ≤ξ (2) Intuitively, the above deﬁnition states that Nis fair as long as the probability diﬀerence is within the threshold ξ. In the following, we focus on Deﬁnition 2 as it is both more general and more practical compared to Deﬁnition 1. Example 1. Let us take the network trained on the Census Income dataset [1 8] as an example. The dataset consists of 32k training instances, eac h of which contains 13 features. The task is to predict whether an individual’s in come exceeds $50K per year. An example instance xwith a prediction ywill be4 Bing Sun, Jun Sun, Ting Dai, and Lijun Zhang x:/angbracke⊔lef⊔3 5 3 0 2 8 3 0 12 0 40 0/angbracke⊔righ⊔,y:/angbracke⊔lef⊔0/angbracke⊔righ⊔. Note that all features are categor ical (i.e., processed using binning). Among all features, gender, ag e and race are considered protected features. The model Ntrained based on the dataset is in the form of a sixlayer fullyconnected feedforward neural n etwork. The following is a fairness property deﬁned based on the protected fea ture gender. |P(Y= 1|F=male)−P(Y= 1|F=female)| ≤0.1 (3) Intuitively,thediﬀerenceintheprobabilityofpredicting1,formales andfemales, should be no more than 10%. Our Problem We are now ready to deﬁne our problem. Deﬁnition 3 (The veriﬁcation problem). LetNbe a neural network. Let φbe an independencebased fairness property (with respect t o protected feature Fand a threshold ξ). The fairness veriﬁcation problem is to verify whether N satisﬁesφor not. One way of solving the problem is through statistical model checking (such as hypothesis testing [40]). Such an approach is however not ideal. W hile it is possible to conclude whether Nis fair or not (with certain level of statistical conﬁdence), the result often provides no insight. In the latter ca se, we would often be interested in performing further analysis to answer ques tions such as whether certain feature or neuron at a hidden layeris particularly r elevantto the fairness issue and how to improve the fairness. The abovemention ed approach oﬀers little clue to such questions. 3 Our Approach In this section, we present details of our approach. Our approach is shown in Algorithm 1. The ﬁrst step is to learn a Markov Chain Dwhich guarantees that probabilistic analysis such as probabilistic reachability analysis based o nDis PACcorrect with respect to N. The second step is to apply probabilistic model checking [39] to verify Dagainst the fairness property φ. In the third step, if the propertyφis not veriﬁed, sensitivity analysis is performed on Dwhich provides us information on howto improve Nin terms offairness. That is, we improvethe fairness of the model by optimizing the neuron weights based on the sensitivity analysis results. Notethat ourapproachreliesonbuilding an approximationofthe neu ralnet work in the form of Markov Chains. There are three reasons why co nstructing such an abstraction is beneﬁcial. First, it allows us to reason about u nbounded behaviors (in the case of a cyclic Markov Chains, which can be constr ucted from recurrent neural networks as we show below) which are known to b e beyond the capability of statistical model checking [40]. Second, the Markov Ch ain model allows us to perform analysis such as sensitivity analysis (e.g., to ident ify neu rons responsible for violating fairness) as well as predict the eﬀect of changingProbabilistic Veriﬁcation of Neural Networks Against Grou p Fairness 5 certain probability distribution (e.g., whether fairness will be improve d), which are challenging for statistical methods. Lastly, in the case that th e fairness is veriﬁed, the Markov Chain serves as a humaninterpretable argum ent on why fairness is satisﬁed. In the following, we introduce each step in detail. We ﬁx a neural netw orkN and a fairness property φof the form|P(Y=l|F=fi)−P(Y=l|F=fj)|≤ ξ. We use the neural network trained on the Census Income datase t (refer to Example 1) as a running example. Algorithm1: verifyrepair(N,φ,µǫ,µδ ) 1Fix the set of states S; 2Learn DTMC Dby learn(N,S,µǫ 2,1−√1−µδ); 3Estimate P(Y=l|F=fi)∀fi∈F; 4Verifyφagainstξ;ifφis veriﬁed then 5return“Veriﬁed” and D; 6else 7Conduct sensitivity analysis on D; 8Perform automatic repair of N; 9returnN′;Algorithm2: learn(N,S,ǫ,δ) 1W:= 0; 2AW:= 0; 3do 4generate new sample traceω 5W:=W+ω; 6updateAW(p,q) for all p∈Sandq∈S; 7updateH(n); 8while∃p∈S,np< H(n) Output: AW 3.1 Step 1: Learning a Markov Chain In this step, we construct a DiscreteTime Markov Chain (DTMC) wh ich ap proximates N(i.e., line 2 of Algorithm 1). DTMCs are widely used to model the behavior of stochastic systems [10], and they are often consider ed reasonably humaninterpretable. Example DTMCs are shown in Figure 1. The deﬁ nition of DTMC is presented in Appendix A.2. Algorithm 2 shows the details of this step. The overall idea is to construct a DTMC, based on which we can perfo rm var ious analysis such as verifying fairness. To make sure the analysis re sult on the DTMC applies to the original N, it is important that the DTMC is constructed in such a way that it preserves properties such as probabilistic reac hability anal ysis (which is necessary for verifying fairness as we show later). Alg orithm 2 is thus base on the recent work published in [10], which develops a sam pling method for learning DTMC. To learn a DTMC which satisﬁes our require ments, we must answer three questions. (1) What are the states Sin the DTMC? The choice of Shas certain con sequences in our approach. First, it constrains the kind of proper ties that we are allowed to analyze based on the DTMC. As we aim to verify fairness , the states must minimally include states representing diﬀerent protect ed features, and states representing prediction outcomes. The reason is that , with these states, we can turn the problem of verifying fairness into probabilis tic reachabil ity analysis based on the DTMC, as we show in Section 3.2. What addition ally are the states to be included depends on the level of details that we would like to have for subsequent analysis. For instance, we include states rep resenting other6 Bing Sun, Jun Sun, Ting Dai, and Lijun Zhang features at the input layer, and states representing the status of hidden neurons. Having these additional states allows us to analyze the correlation b etween the states and the prediction outcome. For instance, having states r epresenting a particular feature (or the status of a neuron of a hidden layer) allo ws us to check how sensitive the prediction outcome is with respect to the feature (or the status of a neuron). Second, the choice of states may have an impact on t he cost of learning the DTMC. In general, the more states there are, the mor e expensive it is to learn the DTMC. In Section 4, we show empirically the impact of ha ving diﬀerent sizes of S. We remark that to represent continuous input features and hidden neural states using discrete states, we discretize their va lues (e.g., using binning or clustering methods such as Kmeans [42] based on a userprovided number of clusters). (2) How do we identify the transition probability matrix? The answer is to repeatedly sample inputs (by sampling based on a prior probability dist ribu tion) and then monitor the trace of the inputs, i.e., the sequence of transitions triggered by the inputs. After sampling a suﬃciently large number of inputs, the transition probability matrix then can be estimated based on the frequency of transitions between states in the traces. In general, the ques tion of estimat ing the transition probability matrix of a DTMC is a wellstudied topic and many approaches have been proposed, including frequency estima tion, Laplace smoothing [10] and GoodTuring estimation [26]. In this work, we ado pt the fol lowing simple and eﬀective estimation method. Let Wbe a set of traces which can be regarded as a bag of transitions. We write npwherep∈Sto denote the number transitions in Woriginated from state p. We writenpqwherep∈Sand q∈Sto be the number of transitions observed from state ptoqinW. Letm be the total number of states in S. The transition probability matrix AW(esti mated based on W) is:AW(p,q) =/braceleftbiggnpq npifnq/nega⊔ionslash= 0 1 motherwise. Intuitively, the probability of transition from state ptoqis estimated as the number of transitions from ptoqdivided by the total number of transitions taken from state pobserved inW. Note that if a state phas not been visited, AW(p,q) is estimated by1 m; otherwise,AW(p,q) is estimated bynpq np. (3) How do we know that the estimated transition probability matrix is ac curate enough for the purpose of verifying fairness? Formally, let AWbe the transition probability matrix estimated as above; and let Abe the actual tran sition probability matrix. We would like the following to be satisﬁed. P(Div(A,AW)>ǫ)≤δ (4) whereǫ >0 andδ >0 are constants representing accuracy andconﬁdence ; Div(A,AW) represents the divergence between AandAW; andPis the proba bility.Intuitively,thelearnedDTMCmustbeestimatedsuchthatthe probability of the divergence between AWandAgreater than ǫis no larger than the con ﬁdence level δ. In this work, we deﬁne the divergence based on the individualProbabilistic Veriﬁcation of Neural Networks Against Grou p Fairness 7 transition probability, i.e., P(∃p∈S,/summationdisplay q∈S/vextendsingle/vextendsingleA(p,q)−AW(p,q)/vextendsingle/vextendsingle>ǫ)≤δ (5) Intuitively, we would like to sample traces until the observed transit ion proba bilitiesAW(p,q) =npq npare close to the real transition probability A(p,q) to a certain level for all p,q∈S. Theorem 1 in the recently published work [10] shows that if we sample enough samples such that for each p∈S,npsatisﬁes np≥2 ǫ2log(2 δ′)/bracketleftBig1 4−/parenleftBig maxq/vextendsingle/vextendsingle/vextendsingle1 2−npq np/vextendsingle/vextendsingle/vextendsingle−2 3ǫ/parenrightBig2/bracketrightBig (6) whereδ′=δ m, we can guarantee the learned DTMC is sound with respect to N intermsofprobabilisticreachabilityanalysis.Formally,let H(n) =2 ǫ2log(2 δ′)[1 4− (maxq|1 2−npq np|−2 3ǫ)2], Theorem 1. Let(S,I,A W)be a DTMC where AWis the transition probability matrix learned using frequency estimation based on ntraces W. For 0< ǫ < 1and0<δ<1, if for allp∈S, np≥H(n), we have for any CTL property ψ, P(/vextendsingle/vextendsingleγ(A,ψ)−γ(AW,ψ)/vextendsingle/vextendsingle>ǫ)≤δ (7) whereγ(AW,ψ)is the probability of AWsatisfyingψ. Appendix A.3 provides the proof. Intuitively, the theorem provides a bound on the number of traces that we must sample in order to guarantee that the learnedDTMC isPACcorrectwith respectto anyCTL property,wh ichprovides a way of verifying fairness as we show in Section 3.2. We now go through Algorithm 2 in detail. The loop from line 3 to 8 keeps sampling inputs and obtains traces. Note that we use the uniform sa mpling by default and would sample according to the actual distribution if it is pr ovided. Next, we update AWas explained aboveat line 6. Then we checkifmore samples are needed by monitoring if a suﬃcient number of traces has been sa mpled according to Theorem 1. If it is the case, we output the DTMC as the result. Otherwise, we repeat the steps to generate new samples and upda te the model. Example 2. In our running example, for simplicity assume that we select gender (as the protected feature) and the prediction outcome to be inclu ded inSand the number of clusters is set to 2 for both layers. Naturally, the tw o clusters identiﬁed for the protected feature are male and female (written a s‘M’and‘F’) and the two clusters determined for the outcome are ‘ ≤50K′and ‘>50K′. A sample trace is w=/angbracke⊔lef⊔Start,‘M′,‘>50K′/angbracke⊔righ⊔, whereStartis a dummy state where all traces start. Assume that we set accuracy ǫ= 0.005 and conﬁdence levelδ= 0.05. Applying Algorithm 2, 2.85K traces are generated to learn the transition matrix AW. The learned DTMC Dis shown in Figure 1a.8 Bing Sun, Jun Sun, Ting Dai, and Lijun Zhang StartM F>50K ≤50K0.4982 0.50180.8796 0.84830.1204 0.15171 1 (a) Learned from a FFNNStartW B Oh1 h2T NT0.36 0.49 0.151 0.99 0.0110.670.030.3 0.960.01 0.02 0.0002 0.00861 1 (b) Learned from an RNN Fig.1: Sample learned DTMCs 3.2 Step 2: Probabilistic Model Checking In this step, we verify Nagainst the fairness property φbased on the learned D. Note that Dis PACcorrect only with respect to CTL properties. Thus it is infeasible to directly verify φ(which is not a CTL property). Our remedy is to compute P(Y=l|F=fi) andP(Y=l|F=fj) separately and then verifyφbased on the results. Because we demand there is always a state in S representing F=fiand a state representing Y=l, the problem of computing P(Y=l|F=fi) can be reduced to a probabilistic reachability checking prob lemψ, i.e., the probability of reaching the state representing Y=lfrom the state representing F=fi. This can be solved using probabilistic model checking techniques. Probabilistic model checking [39] of DTMC is a formal ve riﬁcation method for analyzing DTMC against formallyspeciﬁed quantitative p roperties (e.g., PCTL). Probabilistic model checking algorithms are often base d on a com bination of graphbased algorithms and numerical techniques. For straightfor ward properties such as computing the probability that a U(Until),F(Finally) orG(Globally) path formula is satisﬁed, the problem can be reduced to solving a system of linear equations [39]. We refer to the readers to [39] fo r a complete and systematic formulate of the algorithm for probabilistic model ch ecking. Example 3. Figure 1b shows a DTMC learned from a recurrent neural network trained onJigsawCommentsdataset (referto details on the datas et andnetwork in Section 4.1). The protected features is race. For illustration purpose, let us consider three diﬀerent values for race, i.e., White ( W), Black ( B) and Others (O). For the hidden layer cells, we consider LSTM cell 1 only and cluster it s values into two groups, represented as two states h1andh2. The output has two categories, i.e., Toxic ( T) and NonToxic ( NT). The transition probabilities are shown in the ﬁgure. Note that the DTMC is cyclic due to the recurren t hidden LSTM cells in the network. We obtained P(Y=‘T’|F=‘W’) by probabilistic model checkingasdiscussedabove.The resultantprobabilityis0 .0263.Similarly, P(Y=‘T’|F=‘B’)andP(Y=‘T’|F=‘O’)are0.0362and0.0112respectively. Next we verify the fairness property φbased on the result of probabilistic model checking. First, the following is immediate based on Theorem 1.Probabilistic Veriﬁcation of Neural Networks Against Grou p Fairness 9 Proposition 1. LetD= (S,I,A W)be a DTMC learned using Algorithm 2. LetP(Y=l|F=fi)be the probability computed based on probabilistic model checkingDandPt(Y=l|F=fi)is the actual probability in N. We have P/parenleftbig/vextendsingle/vextendsingleP(Y=l|F=fi)−Pt(Y=l|F=fi)/vextendsingle/vextendsingle>ǫ/parenrightbig ≤δ Theorem 2. LetXbe an estimation of a probability Xtsuch thatP(|X−Xt|> ǫ)≤δ. LetZbe an estimation of a probability Ztsuch thatP(|Z−Zt|>ǫ)≤δ. We haveP(|(X−Z)−(Xt−Zt)|>2ǫ)≤2δ−δ2. Appendix A.4 provides the proof. Hence, given an expected accura cyµǫand a conﬁdence level µδon fairness property φ, we can derive ǫandδto be used in Algorithm 2 as: ǫ=µǫ 2andδ= 1−√1−µδ. We compute the probability of P(Y=l|F=fi) andP(Y=l|F=fj) based on the learned D(i.e., line 3 of Algorithm 1). Next, we compare |P(Y=l|F=fi)−P(Y=l|F=fj)|withξ. If the diﬀerence is no larger than ξ, fairness is veriﬁed. The following establishes the correctness of Algorithm 1. Theorem 3. Algorithm 1 is PACcorrect with accuracy µǫand conﬁdence µδ, if Algorithm 2 is used to learn the DTMC D. Appendix A.5 provides the proof. The overalltime complexity of model learningand probabilistic model c heck ing is linear in the number of traces sampled, i.e., O(n) wherenis the total number of traces sampled. Here nis determined by H(n) as well as the prob ability distribution of the states. Contribution of H(n) can be determined as O(logm µǫ2logµδ) based on Equation 6, where mis the total number of states. In the ﬁrst case, for a model with only input features and output predict ions as states, the probability of reaching each input states are statistically equal if we apply uniform sampling to generate IID input vectors. In this scenario th e overall time complexity is O(mlogm µǫ2logµδ). In the second case, for a model with states repre senting the status of hidden layer neurons, we need to consider th e probability for each hidden neuron states when the sampled inputs are fed into the network N. In the best case, the probabilities are equal, we denote m′as the maximum number of states in one layer among all layers included, the complexit y is then O(m′logm µǫ2logµδ). In the worst case, certain neuron is never activated (or certa in pre deﬁned state is never reached) no matter what the input is. Since t he probability distribution among the hidden states are highly networkdependen t, we are not able to estimate the average performance. Example 4. In our running example, with the learned AWofDas shown in Figure 1a, the probabilities as P(Y= 1|F=‘F’) = 0.8483 andP(Y= 1|F= ‘M’) = 0.8796. Hence,|P(Y= 1|F=‘F’)−P(Y= 1|F=‘M’)|= 0.0313. Next, we compare the probability diﬀerence against the userprovided fa irness criteria ξ. Ifξ= 0.1,Nsatisﬁes fairness property. If ξ= 0.02,Nfails fairness. Note that such a strict criteria is not practical and is used for illustration purp ose only.10 Bing Sun, Jun Sun, Ting Dai, and Lijun Zhang 3.3 Step 3: Sensitivity Analysis In the case that the veriﬁcation result shows φis satisﬁed, our approach outputs Dand terminates successfully. We remark that in such a case Dcan be regarded as the evidence for the veriﬁcation result as well as a humaninterp retable ex planation on why fairness is satisﬁed. In the case that φis not satisﬁed, a nat ural question is: how do we improve the neural network for fairnes s? Existing approaches have proposed methods for improving fairness such a s by training without the protected features [55] (i.e., a form of preprocessin g) or training with fairness as an objective [16] (i.e., a form of inprocessing). In t he following, we show that a postprocessing method can be supported based o n the learned DTMC. That is, we can identify the neurons which are responsible for violat ing the fairness based on the learned DTMC and “mutate” the neura l network slightly, e.g., adjusting its weights, to achieve fairness. We start with a sensitivity analysis to understand the impact of each prob abilistic distribution (e.g., of the nonprotected features or hidden neurons) on the prediction outcome. Let Fbe the set of discrete states representing diﬀerent protected feature values. Let Irepresent a nonprotected feature or an internal neuron. We denote Iiasa particularstate in the DTMC which representscertain group of values of the feature or neuron. Let lrepresent the prediction result that we are interested in. The sensitivity of I(with respect to the outcome l) is deﬁned as follows. sensitivity (I) =/summationdisplay ireach(S0,Ii)∗reach(Ii,l)∗max {f,g}⊆F/parenleftbig reach(f,Ii)−reach(g,Ii)/parenrightbig wherereach(s,s′) for any state sands′represents the probability of reaching s′froms. Intuitively, the sensitivity of Iis the summation of the ‘sensitivity’ of every state Ii, which is calculated as max f,g/parenleftbig reach(f,Ii)−reach(g,Ii)/parenrightbig , i.e., the maximum probability diﬀerence of reaching Iifrom all possible protected feature states. The result is then multiplied with the probability of re aching Iifrom start state S0and the probability of reaching lfromIi. Our approach analyzes all nonprotected features and hidden neurons and iden tify the most sensitive features or neurons for improving fairness in step 4. Example 5. In our running example, based on the learned DTMC Dshown in Figure 1a, we perform sensitivity analysis as discussed above. We ob serve that feature 9 (i.e., representing ‘capital gain’) is the most sensitive, i.e., it has the most contribution to the model unfairness. More importantly, it ca n be observed that the sensitivities of the neurons vary signiﬁcantly, which is a goo d news as it suggests that for this model, optimizing the weights of a few neuro ns may be suﬃcient for achieving fairness. Figure 3 in Appendix A.6 shows the se nsitively analysis scatter plot. 3.4 Step 4: Improving Fairness In this step, we demonstrate one way of improving neural network fairness based on our analysis result, i.e., by adjusting weight parameters of the ne urons identi ﬁed in step 3. The idea is to search for a small adjustment through o ptimizationProbabilistic Veriﬁcation of Neural Networks Against Grou p Fairness 11 techniques suchthat the fairnesspropertyis satisﬁed.In partic ular,weadopt the Particle Swarm Optimization (PSO) algorithm [37], which simulates intellige nt collective behavior of animals such as ﬂocks of birds and schools of ﬁs h. In PSO multiple particles are placed in the search space and the optimization t arget is to ﬁnd the best location, where the ﬁtness function is used to dete rmine the best location. We omit the details of PSO here due to space limitation and pre sent it in Appendix A.7. In our approach, the weights of the most sensitive neurons are th e subject for optimization and thus are represented by the location of the pa rticles in the PSO. The initial location of each particle is set to the original weights a nd the initial velocity is set to zero. The ﬁtness function is deﬁned as follows . fitness=Probdiff+α(1−accuracy ) (8) whereProbdiffrepresents the maximum probability diﬀerence of getting a de sired outcome among all diﬀerent values of the sensitive feature; accuracy is the accuracy of repaired network on the training dataset and consta nt parameter α∈(0,1) determines the importance of the accuracy (relative to the fair ness). Intuitively, the objective is to satisfy fairness and not to sacriﬁce accuracy too much. We set the bounds of weight adjustment to (0 ,2), i.e., 0 to 2 times of the original weight. The maximum number of iteration is set to 100. To further reduce the searching time, we stop the search as soon as the fairn ess property is satisﬁed or we fail to ﬁnd a better location in the last 10 consecutive iterations. Example 6. In our running example, we optimize the weight of ten most sensi tive neurons using PSO for better fairness. The search stops at t he 13rditeration as no better location is found in the last 10 consecutive iterations. T he resul tant probability diﬀerence among the protected features droppe d from 0.0313 to 0.007, whereas the model accuracy dropped from 0 .8818 to 0.8606. 4 Implementation and Evaluation Our approach has been implemented on top of SOCRATES [45], which is a frameworkfor experimentingneutral networkanalysistechnique s.We conducted our experiments on a machine with 1 DualCore Intel Core i5 2.9GHz CP U and 8GB system memory. 4.1 Experiment Setup In the following, we evaluate our method in order to answer multiple re search questions(RQs)basedonmultiple neuralnetworkstrainedon4dat asetsadopted from existing research [54,4,58,28], i.e., in addition to the Census Income [18] dataset asintroduced in Example1, we havethe followingthree data sets.First is theGerman Credit [19] dataset consisting of 1k instances containing 20 features and is used to assessan individual’s credit. Age and gender arethe tw oprotected features. The labels are whether an individual’s credit is good or not. Second12 Bing Sun, Jun Sun, Ting Dai, and Lijun Zhang Table 1: Fairness Veriﬁcation Results Dataset Feature #States #Traces Max Prob. Diﬀ. ResultTime Census Race 812500 0.0588 PASS4.13s Census Age 1223500 0.0498 PASS6.31s Census Gender 52850 0.0313 PASS0.98s Credit Age 1122750 0.1683 Fail6.72s CreditGender 52850 0.0274 PASS1.01s Bank Age 1227200 0.0156 PASS6.33s JigsawReligion 1035250 0.0756 PASS29.6m Jigsaw Race 730550 0.0007 PASS27.3m is theBank Marketing [17] dataset consisting of 45k instances. There are 17 features, among which age is the protected feature. The labels ar e whether the client will subscribe a term deposit. Third is Jigsaw Comment [1] datase t. It consists of 313k text comments with average length of 80 words cla ssiﬁed into toxic and nontoxic. The protected features analysed are race a nd religion. Followingexistingapproaches[54,4,58,28],wetrainthree6layerfeed forward neural networks (FFNN) on the ﬁrst three dataset (with accura cy 0.88, 1 and 0.92 respectively) and train one recurrent neural network, i.e., 8 cell Long Short Term Memory (LSTM), for the last dataset (with accuracy 0.92) an d analyze their fairness against the corresponding protected attributes. For the LSTM model, we adopt the stateoftheart embedding tool GloVe [44]. We use the 50 dimension word vectors pretrained on Wikipedia 2014 and Gigaword 5 dataset. Recall that we need to sample inputs to learn a DTMC. In the case of ﬁ rst three datasets, inputs are sampled by generating randomly values within the range of each feature (in IID manner assuming a uniform distributio n). In the case of the Jigsaw dataset, we cannot randomly generate and rep lace words as the resultant sentence is likely invalid. Inspired by the work in [41,7,3 3], our approach is to replace a randomly selected word with a randomly s elected synonym (generated by Gensim [46]). 4.2 Research Questions and Answers RQ1: Is our approach able to verify fairness? We systematically apply our method to the abovementioned neural networks with respect to each protected feature. Our experiments are conﬁgured with accuracy µǫ= 0.01, conﬁdence levelµδ= 0.1 (i.e.,ǫ= 0.005,δ= 0.05) and fairness criteria ξ= 10% (which is a commonly adopted threshold [6]). Furthermore, in this experimen t, the states in the DTMC Sare set to only include those representing the protected feature and diﬀerent predictions. Table 1 summarizes the results. We succe ssfully ver ify or falsify all models. Out of eight cases, the model trained on the German Credit dataset fails fairness with respect to the feature age(i.e., the maximum probability diﬀerence among diﬀerent age groups is 0.1683 which is gre ater than ξ= 10%). Furthermore, the model trained on the Jigsaw dataset sh ows someProbabilistic Veriﬁcation of Neural Networks Against Grou p Fairness 13 fairness concern with respect to the feature religion(although the probablity diﬀerent is still within the threshold). This result shows that fairnes s violation could be a real concern. RQ2: How eﬃcient is our approach? We answer the question using two mea surements. The ﬁrst measurement is the execution time. The resu lts are shown in the last column in Table 1. For the six cases of FFNN, the average tim e taken to verify a network is around 4.25s, with a maximum of 6.72s for the mo del trained on German Credit on feature ageand a minimum of 0.98 seconds for the model trained on the Census Income dataset on feature gender. For the two cases of LSTM networks, the average time taken is 30 minutes. Com pared with FFNN, verifying an LSTM requires much more time. This is due to three rea sons. Firstly, as mentioned in Section 4.1, sampling texts requires se arching for synonyms. This is nontrivial due to the large size of the dictionary. Secondly, during sampling, we randomly select instances from the training set a nd apply perturbation to them in order to generate new samples. However, most of the instances in the Jigsaw training set does not contain the sensitive wo rd. This leads to an increased number of traces needed to learn a DTMC. Thir dly, the LSTM model takes much more time to make a prediction than that by F FNN in general. It is also observed that for all the cases, the execution t ime is propor tional to the number of traces used in DTMC model learning (as discu ssed in our time complexity analysis). The other measurement is the number of traces Dataset Max Probability DiﬀerenceAccuracy German 0.1683/ma√sto→0.11251.0/ma√sto→0.9450 Census 0.0588/ma√sto→0.02250.8818/ma√sto→0.8645 Jigsaw 0.0756/ma√sto→0.05900.9166/ma√sto→0.9100 Table 2: Fairness Improvement100 200 5001005001,0001,300 #StatesExecution time (s)Analysis nlogn Fig.2: Execution Times vs Number of States that we are required to sample using Algorithm 2. For each model and protected feature, the number of traces generated in Algorithm 2 depends o n number of categorical values deﬁned for this protected feature and the nu mber of predic tions. That is, more categories and predictions result in more state s in the learn a DTMC model, which subsequently lead to more traces required. Fur thermore, the number of tracesrequired alsodepends on the probabilistic dist ribution from each state in the model. As described in Algorithm 2, the minimum numbe r of tracestransitingfromeachstatemustbe greaterthan H(n).Thisisevidencedby results shown in Table 1, where the number of traces vary signiﬁcan tly between models or protected features, ranging from 2K to 35K. Although t he number of14 Bing Sun, Jun Sun, Ting Dai, and Lijun Zhang traces is expected to increase for more complicated models, we belie ve that this is not a limiting factor since the sampling of the traces can be easily par alleled. We further conduct an experiment to monitor the execution time re quired for the same neural network model with a diﬀerent numbers of states in the learned DTMC.Wekeepotherparameters(i.e., µǫ,µδandφ)thesame.Notethathidden neurons are not selected as states to reduce the impact of the st ate distribution. We show one representative result (based on the mode trained on t he Census Income dataset with attribute raceas the protected feature) in Figure 2. As we can see the total execution time is bounded by nlognwhich tally with our time complexity analysis in Section 3. RQ3: Is our approach able to improve fairness and is the sensi tivity analysis useful?The question asks whether the sensitivity analysis results based on the learned DTMC can be used to improve fairness. To answer this quest ion, we systematically perform sensitivity analysis (on both the input featu res and the hidden neurons) and optimize the weights of the neurons which are s ensitive to fairness. We focus on three cases, i.e., the FFNN model trained on t he German Credit model w.r.t ageand on the Census Income model w.r.t raceand the LSTM model trained on the Jigsaw comments w.r.t religion, as the maximum probability diﬀerence for these three cases (as shown in Table 1) is c oncerning (i.e.,>5%). For the former two, we optimize the weights of the top10 sen sitive neurons (including the ﬁrst layer neurons representing other fea tures). For the LSTM model,weoptimizetop3sensitivecells(due tothe smallnumber ofcells). Table 2 shows the fairness improvement as well as the drop in accura cy. It can be observed that in all three cases we are able to improve the fairne ss whilst maintaining the accuracy at a highlevel. Note that the parameter αis set to 0.1 in these experiments and it can be used to achieve better fairness o r accuracy depending the user requirement. RQ4: How does our approach compare with existing alternativ es?The most rel evant tools that we identify are FairSquare [6] and VeriFair [9]. FairSq uare and VeriFair use numerical integration to verify fairness properties of machine learn ing models including neural networks. FairSquare relies on constrain t solving techniques and thus it is diﬃcult to scale to large neural networks. V eriFair is based on adaptive concentration inequalities. We evaluate our appr oach against these two tools on all eight models. For FairSquare and VeriFair, we f ollow the setting of independent feature distribution and check for demogr aphic parity [9]. For both tools, we set c= 0.15 as suggested and keep other parameters as default. As both FairSquare and VeriFair are designed to compare t wo groups of subpopulations, for those protected features that have mo re than two cate gories, we perform binning to form two groups. For the six FFNN mod els, we set timeout value to be 900 sfollowing the setting in VeriFair. As shown in Table 3, FairSquare is not able to scale for large neural network and for all F FNN models it fails to verify or falsify the model in time. Both VeriFair and our appr oach successfully veriﬁed all six FFNN models. But our approachcomplete s the veriﬁ cation within 1s for all models while VeriFair takes 62 times more execut ion time than our approach on average. For the RNN models trained on Jigsa w dataset,Probabilistic Veriﬁcation of Neural Networks Against Grou p Fairness 15 Table 3: Comparison with FairSquare and VeriFair Dataset Prot.Feat.FairSquare VeriFair Ours Result Time Result Time Result Time Census Race  T.O. Pass 2.33s Pass 0.93s Census Age  T.O. Pass 37.14s Pass 0.81s Census Gender  T.O. Pass 2.19s Pass 0.89s Credit Age  T.O. Pass 39.29s Pass 0.90s Credit Gender  T.O. Pass 8.23s Pass 0.82s Bank Age  T.O. Pass 245.34s Pass 0.97s Jigsaw Religion     Pass 29.6m Jigsaw Race     Pass 27.3m neitherFairSquarenorVeriFairisabletoanalyzethem.FairSquares upportsonly loopfree models and, hence, it cannot handle RNN models. Although VeriFair is able to handle RNN networks in general, it does not support text cla ssiﬁers. Hence, compared with existing solutions, our approach is more eﬃcie nt than FairSquare and VeriFair and can support RNNbased text classiﬁer s. 5 Related Work "
184,Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning.txt,"Cross-domain visual data matching is one of the fundamental problems in many
real-world vision tasks, e.g., matching persons across ID photos and
surveillance videos. Conventional approaches to this problem usually involves
two steps: i) projecting samples from different domains into a common space,
and ii) computing (dis-)similarity in this space based on a certain distance.
In this paper, we present a novel pairwise similarity measure that advances
existing models by i) expanding traditional linear projections into affine
transformations and ii) fusing affine Mahalanobis distance and Cosine
similarity by a data-driven combination. Moreover, we unify our similarity
measure with feature representation learning via deep convolutional neural
networks. Specifically, we incorporate the similarity measure matrix into the
deep architecture, enabling an end-to-end way of model optimization. We
extensively evaluate our generalized similarity model in several challenging
cross-domain matching tasks: person re-identification under different views and
face verification over different modalities (i.e., faces from still images and
videos, older and younger faces, and sketch and photo portraits). The
experimental results demonstrate superior performance of our model over other
state-of-the-art methods.","VISUAL similarity matching is arguably considered as one of the most fundamental problems in computer vision and pattern recognition, and this problem becomes more challenging when dealing with crossdomain data. For example, in stillvideo face retrieval, a newly rising task in visual surveillance, faces from still images captured under a constrained environment are utilized as the queries to ﬁnd the matches of the same identity in unconstrained videos. Ageinvariant and sketchphoto face veriﬁcation tasks are also examples of crossdomain image matching. Some ex amples in these applications are shown in Figure 1. Conventional approaches (e.g., canonical correlation analysis [1] and partial least square regression [2]) for cross domain matching usually follow a procedure of two steps: 1) Samples from different modalities are ﬁrst projected into a common space by learning a transformation. One may simplify the computation by assuming that these cross domain samples share the same projection. 2) A certain distance is then utilized for measuring the similarity/disimilarity in the projection space. Usually Euclidean distance or inner product are used. Suppose that xandyare two samples of different modalities, and UandVare two projection matrices ap plied on xandy, respectively. Ux andVy are usually formulated as linear similarity transformations mainly for L. Lin and G. Wang are with School of Data and Computer Science, Sun Yatsen University, Guangzhou, P . R. China. Email: linliang@ieee.org; wanggrun@mail2.sysu.edu.cn. W. Zuo is with School of Computer Science and Technology, Harbin Institute of Technology, Harbin, P . R. China. Email: cswmzuo@gmail.com. X. Feng is with School of Math. and Statistics, Xidian University, Xi’an, P . R. China. Email: xcfeng@mail.xidian.edu.cn. L. Zhang is with Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong. Email: cslzhang@comp.polyu.edu.hk. (a)  (b) (c)  (d) Fig. 1: Typical examples of matching crossdomain visual data. (a) Faces from still images and vidoes. (b) Front and sideview persons. (c) Older and younger faces. (d) Photo and sketch faces. the convenience of optimization. A similarity transforma tion has a good property of preserving the shape of an object that goes through this transformation, but it is limited in capturing complex deformations that usually exist in various real problems, e.g., translation, shearing, and their compositions. On the other hand, Mahalanobis distance, Cosine similarity, and their combination have been widelyarXiv:1605.04039v1  [cs.CV]  13 May 2016IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2 studied in the research of similarity metric learning, but it remains less investigated on how to unify feature learning and similarity learning, in particular, how to combine Ma halanobis distance with Cosine similarity and integrate the distance metric with deep neural networks for endtoend learning. To address the above issues, in this work we present a more general similarity measure and unify it with deep convolutional representation learning. One of the key inno vations is that we generalize the existing similarity models from two aspects. First, we extend the similarity transforma tionsUxandVyto the afﬁne transformations by adding a translation vector into them, i.e., replacing UxandVywith LAx+aandLBy+b, respectively. Afﬁne transformation is a generalization of similarity transformation without the requirement of preserving the original point in a linear space, and it is able to capture more complex deforma tions. Second, unlike the traditional approaches choosing either Mahalanobis distance or Cosine similarity, we com bine these two measures under the afﬁne transformation. This combination is realized in a datadriven fashion, as discussed in the Appendix, resulting in a novel generalized similarity measure, deﬁned as: S(x;y) = [xTyT1]2 4A C d CTB e dTeTf3 52 4x y 13 5; (1) where submatrices AandBare positive semideﬁnite, representing the selfcorrelations of the samples in their own domains, and Cis a correlation matrix crossing the two domains. Figure 2 intuitively explains the idea1. In this example, it is observed that Euclidean distance under the linear trans formation, as (a) illustrates, can be regarded as a special case of our model with A=UTU,B=VTV,C="
483,Verifying Reachability in Networks with Mutable Datapaths.txt,"Recent work has made great progress in verifying the forwarding correctness
of networks . However, these approaches cannot be used to verify networks
containing middleboxes, such as caches and firewalls, whose forwarding behavior
depends on previously observed traffic. We explore how to verify reachability
properties for networks that include such ""mutable datapath"" elements. We want
our verification results to hold not just for the given network, but also in
the presence of failures. The main challenge lies in scaling the approach to
handle large and complicated networks, We address by developing and leveraging
the concept of slices, which allow network-wide verification to only require
analyzing small portions of the network. We show that with slices the time
required to verify an invariant on many production networks is independent of
the size of the network itself.","Perhaps lulled into a sense of complacency because of the Internet’s besteffort delivery model, which makes no explicit promises about network behavior, network operators have long relied on bestguess conﬁgurations and a “we’ll ﬁx it when it breaks” operational attitude. However, as networking matures as a ﬁeld, and institutions increasingly rely on net works to provide reachability, isolation, and other behavioral constraints, there is growing interest in developing rigorous veriﬁcation tools that can ensure that these constraints will be enforced by the network conﬁguration. The ﬁrst generation of such tools – Anteater [26], Veriﬂow [21], and HSA [19,20] – provide highly efﬁcient (in fact, near realtime) checking of reachability (and, conversely isolation) properties and detect anomalies such as loops and black holes. This technical ad vance represents an invaluable step forward for networking. These veriﬁcation tools assume that the forwarding behav ior is set by the control plane, and not altered by the trafﬁc, so veriﬁcation needs to be invoked only when the control plane alters routing entries. This approach is entirely sufﬁcient for networks of routers, which is obviously an important use case. However, modern networks contain more than routers. Most networks contain switches whose learning behavior renders their forwarding behavior dependent on the trafﬁc they have seen. More generally, most networks also contain middleboxes, and middleboxes often have forwarding behav ior that depends on the observed trafﬁc. For instance, ﬁrewalls often rely on outbound “holepunching” to allow hosts to establish ﬂows to the outside world, and content caches forward differently based on whether they have previously cached the desired content. We refer to network elements whose forward ing behavior can be altered by datapath activity as having a “mutable datapath” (in contrast with static datapaths whose behavior is ﬁxed until the control plane intervenes), and addi tional examples of such elements include WAN optimizers, deeppacketinspection boxes, and load balancers. In short, the behavior of a static datapath is only a function of its conﬁguration, while the behavior of a mutable datapath also depends on the entire packet history that it has seen. While classical networking often treats middleboxes as an unfortunate and rare occurrence in networks, in reality mid dleboxes are the most viable way to incrementally deploy new network functionality. Operators have turned to middleboxes to such a great extent that a recent study [37] of ﬁftyseven enterprise networks revealed that these networks are roughly equally divided between routers, switches and middleboxes. Thus, roughly twothirds of the forwarding boxes in enter prise networks can have mutable datapaths that would not conform to the models used in the recently developed network veriﬁcation tools. In addition, the rise of Network Function Virtualization (NFV) [12], in which physical middleboxes are replaced by their virtual counterparts, makes it easier to deploy additional middleboxes without changes in the physi cal infrastructure. Thus, we must reconcile ourselves to the fact that many networks will have substantial numbers of ele ments with mutable datapaths (and hereafter, when referring to such elements we will call them middleboxes). Moreover, not only are middleboxes prevalent, but they are often respon sible for network problems. A recent two year study [34] of a provider found that middleboxes played a role in 43% of their failure incidents, and between 4% and 15% of these failures were the result of middlebox misconﬁguration. The goal of this paper is to extend the notion of veriﬁca tion to networks containing mutable datapaths, so that such middlebox misconﬁguration problems can be prevented.1Fur ther, these techniques should ensure correctness even in the presence of failures, a requirement not addressed by any of the existing network veriﬁcation tools. Our basic approach, which we call Veriﬁcation for Middlebox Networks (VMN) is 1While we are extending the class of networks – to those including mutable datapaths – we are notsigniﬁcantly expanding the class of invariants to be checked; just as in the earlier works, we are focusing on reachability and isolation. 1arXiv:1607.00991v1  [cs.NI]  4 Jul 2016simple: given a topology containing both mutable datapaths (e.g., middleboxes) and static ones ( e.g., routers), we derive a logical formula that model the network as a whole. We then add logical formulas derived from the speciﬁed invariants so that an invariant holds if and only if there is no satisfying assignment for this set of logical formulas as a whole. As de scribed so far, this is a straightforward application of standard program veriﬁcation techniques to networks. However, naïvely applied, this approach would not scale: middlebox code is complex, and checking even simple in variants in modestsized networks would be intractable. Thus, our focus is on how to scale this approach to large networks. VMN uses four techniques to scale veriﬁcation: 1. Limited invariants : Rather than deal with an arbitrary set of invariants, we focus on two speciﬁc categories that are the dominant concerns for network operators. First, we look at invariants describing the set of middleboxes (more generally as a DAG of middleboxes) packets should ﬂow through ( e.g., all http trafﬁc should pass through a ﬁrewall then a cache); we call these pipeline invariants. Second, we also consider invariants that address reachability/isolation between hosts (at the packet or content level), such as packets from host A should not reach host B (and hereafter we will call these reachability invariants). Our contributions relate to verifying reachability invariants, and we rely on established techniques to verify pipeline invariants. 2. Simple highlevel middlebox models : One approach to verifying networks with middleboxes would be to use their full implementation to determine their behavior. This is infeasible for two reasons: (i) most commonly deployed middleboxes are proprietary, and we do not have access to their code, and (ii) model checking even one such box for even the simplest invariants would not scale.2Therefore, we model middleboxes using a simple abstract forwarding model and a set of abstract packet classes used by this model. We do not model the packet classiﬁcation algorithm in middle boxes, and instead rely on an oracle to classify packets.3The forwarding models can typically be derived from a general description of the middlebox’s behavior and can be easily analyzed using standard techniques. 3. Modularized network models : Networks contain ele ments with static datapaths and elements with mutable datap aths. Rather than consider them all within the one veriﬁcation framework, which would overburden a system already hav ing trouble scaling, we treat the two separately: it is the job of the static datapath elements to satisfy the pipeline invari ants (that is, to carry packets through the appropriate set of middleboxes), which we can analyze using existing veriﬁ 2This assertion does not contradict the results in [11], which we discuss later in the paper. 3A third reason we do not apply model checking to the full im plementation, which we discuss in the next section, is that there is a semantic mismatch between raw middlebox code and operator speciﬁed invariants, which are described in terms of basic abstrac tions. In fact, this mismatch is what led us to the combination of an Oracle and an abstract model.cation tools; and it is the job of the processing pipeline to enforce reachability invariants, and that is where we focus our attention. Thus, our resulting system is a hybrid of current staticdatapath veriﬁcation tools and our newlyproposed tool for mutable datapaths. 4. Symmetries and Common Cases If middleboxes are ﬂowparallel ororiginagnostic (both to be deﬁned later, and most middleboxes fall into one or both categories), we can perform networkwide veriﬁcation by examining only a small portion of the network. This allows us to scale veriﬁcation of a single invariant to large networks. Furthermore, opera tional networks exhibit a great deal of symmetry in how they are structured and in the policies they enforce. We exploit this symmetry to reduce the total time taken for verifying all invariants in the network. The combination of these two obser vations allows us to verify the correctness of large networks in a few seconds. Prior work, particularly Buzz [13], have described network tools (akin to ATPG [45]) that can be applied to networks with middleboxes; they generate test packets that (when sent) efﬁciently explore whether or not invariants are violated. In contrast, VMN provides mechanisms for verifying (akin to HSA [19]) reachability invariants in networks with middle boxes. Our contributions also include slicing, which allows veriﬁcation to scale to arbitrarily large networks (through the use of slices ), and checking whether invariant hold during failures. We provide a more detailed comparison between VMN and other systems in §6. In the next section, we discuss all four of these steps more formally, and then in §3 we provide an overview of VMN. We discuss our strategy for performing veriﬁcation on smaller, ﬁxed size subnetworks for scalability in §4. In §5 we evaluate VMN by verifying invariants for a variety of realworld sce narios. Finally we conclude in §6 and §7 with a discussion of related work and a brief summary. 2 Our Approach In this section we provide an overview of our approach. 2.1 Invariants The purpose of veriﬁcation is to test whether some proper ties ( invariants ) hold for a given network, where a network is deﬁned by both its topology (location of routers/switches and middleboxes) and its conﬁguration (routing tables and middlebox settings, including how both routers and middle boxes respond to failures). The veriﬁcation techniques em ployed necessarily depend on the nature of the invariants to be checked. Thus, to understand the veriﬁcation problem we are tackling, we must ﬁrst specify the class of invariants we consider. Verifying arbitrary invariants for networks with mutable datapaths is undecidable4[43], which is why we focus more narrowly on pipeline and reachability invariants for various 4Middleboxes render the network Turing complete, so verifying certain invariants is equivalent to solving the halting problem. 2classes of packets that are deﬁned in terms of hosts, users, source/destination addresses, ingress/egress ports, ﬂows, con tent, application, whether or not a packet is “malicious” (as decided, for example, by a DPI box), whether or not a packet belonging to this ﬂow has been seen before, and other con cepts. These invariants can refer to the current network con ﬁguration, or be predicated on one or more failures in the network ( i.e.,one can insist that reachability or pipeline in variant not only hold for the current network, but for all single failures in that network). 2.2 HighLevel Middlebox Models Some invariants are deﬁned in terms of packet classes based only on intrinsic information — such as physical ports and header ﬁelds — that can be precisely deﬁned by network operators. However, operators frequently rely on invariants deﬁned in terms of higherlevel abstractions — such as what user a ﬂow belongs to, what application sent a ﬂow, whether the packet or ﬂow is malicious, etc. — that depend on other context (for instance authentication services implemented outside of a particular middlebox), and can often not be pre cisely deﬁned (whether a packet is malicious depends on the current set of CVEs [28] and various heuristics). Using highlevel abstractions enables operators to express their in tent (such as to drop all malicious trafﬁc, or drop all trafﬁc from a given user) without having to specify, within the in variant itself, the precise mechanisms used to deﬁne those abstractions. For example, an operator may wish to drop all Skype trafﬁc, but does not know (or care) about the precise mechanisms an applicationlevel ﬁrewall uses to identify such trafﬁc. Therefore, to reﬂect how things are currently done when conﬁguring middleboxes or expressing policies ( e.g., Congress [31]), we allow invariants to be deﬁned in terms of these higherlevel abstractions. How do we model middleboxes when their code is nec essarily in terms of intrinsic information but the invariants can be in terms of higherlevel abstractions? We choose to describe middleboxes in two stages: we start with a simple ab stract forwarding model coupled with a set of abstract packet classes (is a packet malicious, is it a part of a Skype ﬂow, etc.) and any requirements for classifying packets into this type ( e.g.,, to determine whether a packet belong to a Skype connection the middlebox implementation needs to see all packets in a ﬂow). VMN then augments this model by adding aclassiﬁcation oracle which is responsible for classifying packets into these packet classes. The abstract forwarding model describes how the middlebox operates on a packet ( i.e., whether and where it is forwarded, and whether the header is rewritten) given the intrinsic information ( i.e.,packet header and port information), the entire packet history (since mid dlebox datapaths can be mutable, their behavior can depend on packet history), and the abstractions it is assigned by the oracle (is the packet part of a Skype ﬂow?). The type of mid dlebox ( e.g., ﬁrewall or load balancer) describes the basic behaviors supported by the middlebox (for instance, whichabstractions it supports, whether it rewrites packet headers, etc.), but the conﬁguration of the middlebox dictates to which class of packets these behaviors are applied. The task of veriﬁcation that we address in this paper is whether the invariants are obeyed assuming that middlebox implementation can correctly classify packets into abstract types. That is, we verify network and middlebox conﬁgu ration, not the implementation of the abstractions. Clearly, if a middlebox cannot correctly classify trafﬁc ( e.g., Skype trafﬁc) an invariant might not hold. However, this error is not caused by network conﬁguration but rather by bugs in the middlebox implementation. While verifying middlebox implementation is an important task, it is beyond the scope of this paper. Here we focus solely on whether the overall net work conﬁguration upholds invariants assuming middleboxes are correctly implemented. 2.3 Modularized Network Models The tools developed for static datapaths [19, 20] not only ver ify invariants, but also summarize the behavior of such net works as transfer functions. A transfer function for a network of static datapaths maps an incoming packet on a physical port at the edge of the network to one or more outgoing pack ets (perhaps with rewritten header ﬁelds) departing out one or more physical ports at the edge of the network. Thus, we can consider our network as a set of elements with mutable data paths tied together by transfer functions which represent the behavior of the static datapath portion of the network.5When a failure occurs, routing in the static datapaths will change, producing a new transfer function. Rather than model the details of the routing algorithm, we assume we are given a function mapping failure conditions to these new transfer functions ( e.g., a list of backup paths taken in response to failures). The glue provided by these transfer functions is precisely what is responsible for enforcing pipeline invariants. In its simplest incarnation, a pipeline invariant takes the form: all incoming packets with a certain class of headers must pass through the sequence of middleboxes mb1;mb2;mb3; :::be fore being delivered to the intended destination . More compli cated pipeline invariants involve a DAG of middleboxes and specify the appropriate branching at each step ( e.g., allhttp packets leaving the ﬁrewall go to the load balancer, while all other trafﬁc goes directly to the destination). Note that these invariants could refer to physical instances of middleboxes (e.g., packets must traverse this particular middlebox) or a class of middleboxes ( e.g., packets must traverse a ﬁrewall). In what follows, we will assume that all packets belonging to the same ﬂow are processed by the same physical pipeline (this can be easily enforced in existing networks). Once we have decomposed the network into a set of mid dleboxes connected by transfer functions, checking these 5If the static datapaths lead to a loop for a particular packet, we raise an exception (so the network operator is aware of it) and treat the packet as dropped. 3pipeline invariants is straightforward using existing network veriﬁcation tools. The reachability invariants are more difﬁ cult, as we discuss next. 2.4 Scaling Veriﬁcation For moderately sized networks, we can use the techniques dis cussed above to generate logical formulas modeling network behavior, and other formulas corresponding to the invariants that need to be veriﬁed. We can then use an SMT solver ( e.g., Z3 [10]) to check if the invariants hold for the provided net work or not. However as the scale of the network increases, the SMT solver has to account for an exponentially larger state space, slowing down or preventing veriﬁcation. Inspired by compositional veriﬁcation [17, 27] (which al lows the results from verifying components of a program to be combined to reason about the program as a whole) we have identiﬁed a general class of subnetworks, which we refer to as slices , where any invariant that only references middleboxes or endhosts contained in a slice holds for the entire network if and only if it holds for the slice. Therefore, we can scale network veriﬁcation if given a network and an invariant, we can ﬁnd a slice whose size is independent of the size of the network and which contains all middleboxes or endhosts referenced by the invariant. While slices help us rapidly verify that an individual invari ant holds in a network, a network might implement several invariants. Prior work [33] has observed that many oper ational networks have symmetric topologies and policies. Since the proof generated while verifying that a particular invariant holds also applies to all other symmetric invariants, this greatly reduces the time taken to verify the behavior of a network allowing us to scale veriﬁcation to extremely large operational networks. 3 System Design We model network behavior in discrete timesteps. During each timestep a previously sent packet can be delivered to a node (middlebox or host), a host can generate a new packet that enters the network, a middlebox can process a previously received packet, a failure can occur, or a previously failed node can recover. We do not attempt to model the likely order of these various events, but instead consider all such orders in search of invariant violations. To do so, we invoke a scheduling oracle that assigns a single event to each timestep, subject to the constraint that ordering (both for receiving and processing) is maintained for packets sent on the same link. 3.1 Overview VMN accepts as input a set of middlebox models (§3.4), and a set of transfer functions (§3.5) with a mapping between failure conditions and transfer functions. VMN builds on Z3 [29] a state of the art SMT solver. SMT solvers accept as input a set of boolean formulae expressed in terms of a set of variables, and then either ﬁnd an assignment for the variables such that the conjunction of these formulae is true (a satisfying assignment) or a proof that no such assignmentexists. In this section we present our technique for convert ing a network and set of invariants to a set of axioms, and producing a logical formulae including these axioms. We can then use Z3 to check satisﬁability for these formulae, where a satisfying assignment indicates that an invariant is violated by the network. In VMN middleboxes and networks are modeled using quantiﬁed formula, which are axioms describing how re ceived packets are treated, while the classiﬁcation and schedul ing oracles are modeled using variables. In addition to the network model and oracles, we also provide Z3 with the nega tion of the invariant, which we specify in terms of a set of packets. Finding a satisﬁable assignment to these formulae is equivalent to Z3 ﬁnding a set of oracle behaviors that result in the invariant being violated, and proving the formulae un satisﬁable is equivalent to showing that no oracular behavior can result in the invariants being violated. The search problem solved by SMT solvers is undecidable in general, and they rely on heuristics and timeouts to en sure their search procedure terminates (in which case they are unable to determine if a problem is satisﬁable or not). Naïvely generating middlebox and network formulae might yield formulae in an undecidable logic, preventing success ful veriﬁcation. Therefore, one of the core contribution of VMN lies in producing logical formulae for a wide range of networks (and a variety of middleboxes) and invariants in a weak logic that is expressible in Z3 and for which veriﬁcation succeeds in practice. In the rest of this section we explore these models and formula in greater depth. 3.2 Notation We begin by presenting the notation used in this section. We express our models and invariants using a simpliﬁed form of linear temporal logic (LTL) [24] of events, with past operators. We restrict ourselves to safety properties, and hence only need to model events occurring in the past or events that hold globally for all of time. We use LTL for ease of presentation; VMN automatically converts LTL formulas into ﬁrstorder logic (as required by Z3) by explicitly quantifying over time. Our formulas are expressed in terms of three events: snd(s;d;p), the event where a node (end host, switch or mid dlebox) ssends packet ptonode d; and rcv(d;s;p), the event where a node dreceives a packet pfrom node s, and f ail(n), the event where a node nhas failed. Each event happens at a timestep and logical formulas can refer either to events that occurred in the past (represented using ♦) or properties that hold at all times (represented using 2). For example, 8d;s;p:2(rcv(d;s;p) =)♦snd(s;d;p)) says that at all times, any packet preceived by node dfrom node smust have been sent by sin the past. Similarly, 8p:2:rcv(d;s;p) indicates that dwill never receive any packet from s. Header ﬁelds and abstract packet classes are represented using functions, e.g., src(p)anddst(p)represent the source 4Listing 1: Model for a learning ﬁrewall 1@FailClosed 2class LearningFirewall (acl: Set[(Address, ,!Address)]) { 3 val established : Set[Flow] 4 def model (p: Packet) = { 5 when established.contains(flow(p)) => 6 forward (Seq(p)) 7 when acl.contains((p.src, p.dest)) => 8 established += flow(p) 9 forward (Seq(p)) 10 _ => 11 forward (Seq.empty) 12 } 13} and destination address for packet p, and skype ?(p)returns true if and only if pbelongs to a Skype session. 3.3 Reachability Invariants Reachability invariants can be be generally speciﬁes as: 8n;p:2:(rcv(d;n;p)^predicate (p)); which says that node dshould never receive a packet pthat matches predicate (p). The predicate can be expressed in terms of packetheader ﬁelds, abstract packet classes and past events, this allows us to express a wide variety of network properties as reachability invariants, e.g.,: Simple isolation: node dshould never receive a packet with source address s. We express this invariant using the srcfunction, which extracts the source IP address from the packet header: 8n;p:2:(rcv(d;n;p)^src(p) =s): Flow isolation: node dcan only receive packets from s if they belong to a previously established ﬂow. We express this invariant using the f low function, which computes a ﬂow identiﬁer based on the packet header: 8n0;p0;n1;p1:2:(rcv(d;n0;p0)^src(p0) =s^ :(♦snd(d;n1;p1)^f low (p1) =f low (p0))): Data isolation: node dcannot access any data origi nating at server s, this requires that dshould not access data either by directly contacting sor indirectly through network elements such as content cache. We express this invariant using an origin function, that computes the origin of a packet’s data based on the packet header ( e.g., using the xhttpforwardedfor ﬁeld in HTTP): 8n;p:2:(rcv(d;n;p)^origin (p) =s): In addition, VMN can verify several other invariants, includ ing whether packets traverse a certain link or middlebox. 3.4 Modeling Middleboxes Middleboxes in VMN are modeled using a highlevel loop free, event driven language. Restricting the language so it is loop free allows us to ensure that middlebox models are expressible in ﬁrstorder logic (and can serve as input to Z3). We use the eventdriven structure to translate this code to logical formulae (axioms) encoding middlebox behavior. Middlebox models are speciﬁed as a class containing the abstract packet classes the middlebox depends on, its forwarding model , and its failure behavior .Abstract packet classes are speciﬁed as a set of function prototypes. Option ally, models can also specify input constraint that must be met for the implementation to correctly identify that a packet belongs to a particular class, and output constraints restricting the set of classes a packet can belong to. For example, an application ﬁrewall might specify an abstract packet class for each application ( e.g.,skype? ,jabber? ), specify that cor rect identiﬁcation requires all packets in a ﬂow to go through the same middlebox instance, and specify that a packet can belong to at most one application class (a packet cannot be both a Skype packet and a Jabber packet). Middlebox forwarding models are speciﬁed as functions which dictate how packets are modiﬁed and whether they are forwarded or dropped. Complex packet modiﬁcation, e.g., encryption or compression, are modeled as replacing the appropriate packet header ﬁeld (or payload) with a random value, this provides sufﬁcient ﬁdelity for checking reachability invariants. Finally, middlebox failure behavior is speciﬁed either explicitly in the forwarding model, using the fail predicate provided by VMN, or implicitly by specifying whether a middlebox failsclosed ( i.e.,packets are dropped during middlebox fail ures) or failsopen ( i.e.,all received packets are forwarded unmodiﬁed during failure). We provide examples of such speciﬁcation below. Listing 1 shows the speciﬁcation for a stateful ﬁrewall. The model accepts a set of ACLs ( acl on Line 2) as conﬁguration, and maintains ﬂow state (in the established variable de ﬁned on Line 3). On receiving a packet from an established ﬂow, the ﬁrewall forwards the packet (Line 5 and 6), other wise it checks to see if the packet is permitted by the ﬁrewall conﬁguration (Line 6–10). The ﬁrewall forwards the packet if permitted and drops it otherwise. The @FailClosed an notation on line 1 indicates that the ﬁrewall fails closed, and packets are dropped during failure. Similarly, Listing 2 shows the model for a NAT. In this example we explicitly model failure behavior (Line 6), and the NAT drops packets when failed. We also modify the packet’s source and destination port (Line 10–11, 14–15, and 20–21) as a part of the forwarding behavior. We assign ports to new ﬂows at random by calling the remapped_port (line 2) method. VMN translates these highlevel speciﬁcations into a set of parametrized axioms (the parameters allow more than one instance of the same middlebox to be used in a network). For instance, Listing 1 results in the following axioms: established (f low (p)) =)(♦((:f ail(f))^(♦rcv(f;p)))) ^acl(src(p);dst(p)) send (f;p) =)(♦rcv(f;p)) ^(acl(src(p);dst(p)) _established (f low (p))) The boldfaced terms in this axiom are parameters: for each stateful ﬁrewall that appears in a network, VMN adds a new 5Listing 2: Model for a NAT 1class NAT (nat_address: Address){ 2 abstract remapped_port (p: Packet): int 3 val active : Map[Flow, int] 4 val reverse : Map[port, (Address, int)] 5 def model (p: Packet) = { 6 when fail(this) => 7 forward (Seq.empty) 8 dst(p) == nat_address => 9 (dst, port) = reverse[dst_port(p)]; 10 dst(p) = dst; 11 dst_port(p) = port; 12 forward (Seq(p)) 13 active.contains(flow(p)) => 14 src(p) = nat_address; 15 src_port(p) = active(flow(p)); 16 forward (Seq(p)) 17 _ => 18 address = src(p); 19 port = src_port(p) 20 src(p) = nat_address; 21 src_port(p) = remapped_port(p); 22 active(flow(p)) = src_port(p); 23 reverse(src_port(p)) = (address, port); 24 forward (Seq(p)) 25 } 26} axiom by replacing the terms f,aclandestablished with a new instance speciﬁc term. The ﬁrst axiom says that the established set contains a ﬂow if a packet permitted by the ﬁrewall policy ( acl) has been received by fsince it last failed. The second one states that packets sent by fmust have been previously received by it, and are either pr emitted by the acl’s for that ﬁrewall, or belong to a previously established connection. The axioms generated for the NAT are similar, and are elided due to space constraints. We require users of VMN to provide middlebox models, however our models are at a high level and depend only on the type of the middlebox, not its placement or implementa tion details. Previous studies have found that only a limited number of middlebox types are widely deployed [8] in pro duction networks6, and we believe that in a majority of cases users of our tool can reuse existing models. 3.5 Modeling Networks VMN uses transfer functions which were previously devel oped by HSA [20] and VeriFlow [21] to specify a network’s forwarding behavior during a particular failure scenario. The transfer function for a network is a function from a located packet (a packet augmented with the network port where it is located) to a set of located packets indicating where the packets are next sent. For example, the transfer function for a network with 3hosts A(with IP address a),B(with IP address 6The existence of a limited number of middlebox types does not limit the number of deployed middleboxes. Networks commonly include several middleboxes belonging to the same type, this might be for resilience, improving network performance and to reduce the load on each middlebox.b) and C(with IP address c) is given by: f(p;port )8 >< >:(p;A)ifdst(p) =a (p;B)ifdst(p) =b (p;C)ifdst(p) =c VMN assumes switch forwarding tables are static, how ever they might change depending on the failure scenario. Therefore, rather than accepting a single static network topol ogy and conﬁguration as input, VMN accepts a topology and forwarding table corresponding to each failure scenario. Given the topology and switch forwarding tables used by the network in a particular failure scenario, VMN uses VeriFlow to compute a transfer function. In this computed transfer func tion, all ports correspond to either middleboxes or endhosts, i.e.,the transfer function models the network as a set of end hosts and middleboxes connected to a single switch. VMN translates this transfer function to axioms by introducing a single pseudonode ( W) representing the network, and deriv ing a set of axioms for this pseudonode from the transfer function and failure scenario. For example, the previous trans fer function is translated to the following axioms ( f ail(X) here represents the speciﬁed failure model). 8n;p:2f ail(X)^:::snd(A;n;p) =)n=W 8n;p:2f ail(X)^:::snd(W;n;p)^dst(p) =a =)n=A^♦9n0:rcv(n0;W;p) VeriFlow (and HSA) can only produce transfer functions when the topology and forwarding table for a network are loopfree. VMN therefore throws an exception when a static forwarding loop is encountered. Not allowing loops in the forwarding logic is also important for allowing us to express network axioms in ﬁrstorder logic and helps ensure VMN’s veriﬁcation process is decidable. In addition to the axioms for middlebox behavior and net work behavior, VMN also adds axioms restricting the oracles’ behavior, e.g., we add axioms to ensure that any packet de livery event scheduled by the scheduling oracle has a corre sponding packet send event, and we ensure that new packets generated by hosts are well formed. 3.6 Limitations To ensure veriﬁcation is practical and tractable, our models of networks and middleboxes are necessarily abstract. This imposes some limitations for the results returned by VMN. Firstly, we do not verify the classiﬁcation logic in a middle box implementation, our veriﬁcation results are conditioned on packets being correctly classiﬁed by the middlebox. There fore, our results might be wrong when classiﬁcation logic is incorrectly implemented. This is a separable problem: VMN does not obviate the need to verify and test individual mid dleboxes, it just provides a mechanism to verify the behavior of combined middleboxes. Providing tools to test or verify individual middleboxes is outside scope of our work. Secondly, we do not have complete semantic information about abstract packet classes, and this can result in VMN reporting false positives ( i.e.,invariant violations) where none 6exist. For example, consider a network with two application speciﬁc ﬁrewalls, one that can identify Skype trafﬁc, and another that can identify streaming audio services. A priori, VMN has no information indicating that these packet classes are mutually exclusive, and will consider packets which meet both criterion when looking for invariant violations. This can be solved by augmenting VMN’s models with logical constraints encoding these assumptions, however we do not currently include such constraints. Finally, our models do not contain semantics for complex packet modiﬁcations ( e.g., encryption, compression, etc.), and instead just change the affected packet to a random value. Similar to the previous cases this can also result in false positives in the same way as above. Most of these limitations are fundamental, network veri ﬁcation without the use of abstractions is intractable, and is impractical with large models. Our choices allow us to pro vide useful veriﬁcation, as shown §5, and in most practical cases we observed no false positives. 4 Scaling Veriﬁcation Z3 (and other SMT solvers) rely on heuristics and timers to ensure satisﬁability checking terminates, and cannot always prove (or disprove) satisﬁability of large sets of formulae. The size of formulae produced by the techniques in §3 are propor tional to the size of the network being veriﬁed, and therefore cannot be applied to large networks. Scaling therefore re quires that the size of the formulae generated be independent of network size. We rely on network slices as described here. We begin by providing an informal overview of network slices, a more formal description is available in our technical report.7Given a network N, a subnetwork Wis a network formed from a subset of N’s nodes (middleboxes, hosts and switches) and links. All packets sent by hosts in subnetwork Wand received by hosts in Ware said to belong to W. We sayWis closed under forwarding if and only if all packets belonging to Ware only forwarded to nodes in W. Deﬁne a network N’s state to be the cartesian product of the state in all middleboxes in N. We say some state sis reachable in Nif and only if there exists a schedule (given by the output of the scheduling oracle) at the end of which the network has state s. A subnetwork Wof network Nis then closed under state if and only if there exists an equivalence relation between states in Wand states in Nsuch that for all states reachable in N, the equivalent state is reachable in W, and vice versa. A slice is a subnetwork that is both closed under forward ing and state. Any invariant referencing only nodes and pack ets belonging to a slice holds in the original network if and only if it holds in the slice. Consider an invariant Ithat is violated in some network N. Proving that an invariant is vi olated is equivalent to ﬁnding a schedule S(i.e.,a sequence of events) which lead to the invariant being violated. Now 7Anonymized for double blind submission, we can provide proofs and other details on request.consider W, a slice of N, such that Ionly references nodes and packets belonging to W. Intuitively, the closure properties imply that there exists a schedule S0forWthat is equivalent toS. Furthermore, this equivalence implies that S0also leads toIbeing violated in W. Finally, note that Wis a subnetwork ofN, and hence any schedule for Wis also a schedule for N. 4.1 Finding Slices Networks with arbitrary middleboxes need not have slices smaller than the network as a whole. However, we ﬁnd there is a class of networks that do have slices that do not grow with the size of the overall network. These special networks, which we now focus on, obey the following conditions: (a) any middleboxes used in these networks be ﬂowparallel , or originagnostic , (b) network policies be such that we can divide hosts in the network into a set of policy equivalence classes, two hosts are in the same equivalence class if all packets sent and received by them traverse the same set of middlebox types, and are treated according to the same policy, (c) the number of policy classes be independent of the size of the network, (d) the network’s forwarding graph is ﬁnite, and (e) the size of forwarding graphs is independent of network size. We deﬁne our restrictions in greater detail below. A middlebox is ﬂowparallel if middlebox state is parti tioned by ﬂows, and a ﬂow’s state is accessed only when processing that ﬂow; e.g., stateful ﬁrewalls maintain state about whether a particular ﬂow is allowed, however this state does not affect other ﬂows, nor is it updated by any other ﬂow. A middlebox is originagnostic if it is not ﬂowparallel ( i.e., state is shared across ﬂows) and its behavior is agnostic to which ﬂows (and hence hosts) instantiated the state. For ex ample, the behavior of contentcaches often does not depend on the connection that led to content being cached. Any subnetwork that contains only ﬂowparallel middle boxes and is closed under forwarding is also closed under state; the set of packets belonging to the subnetwork can be naturally mapped to a set of ﬂows belonging to the sub network, and the state for those ﬂows can only be affected by nodes in the subnetwork. Therefore, ﬁnding a slice in a network containing only ﬂowparallel middleboxes is equiv alent to ﬁnding a subnetwork that is closed under forward ing. Therefore, verifying an invariant in a network with only ﬂowparallel middleboxes only requires that we consider a subnetwork that is closed under forwarding and includes all nodes speciﬁed by the invariant. Since we assumed the size of the forwarding graph is ﬁnite and independent of network size, this means that the slices used to verify invariants in such networks are also ﬁnite. A subnetwork that is closed under forwarding and con tains only originagnostic orﬂowparallel middleboxes is closed under state if and only if it includes a node from each policy class. This is because all nodes in the same policy class are equivalent for originagnostic middleboxes, i.e., the middlebox cannot distinguish between hosts in the same equivalence class, ensuring that the slice is closed under state. 7InternetCoreCoreAccessAccessAggAggToRToRLBLBFWIDPSLBLBFWIDPS Racks of ServersRacks of ServersFigure 1: Topology for a datacenter network with middle boxes from [34]. The topology contains ﬁrewalls ( FW), load balancers ( LB) and intrusion detection and prevention systems ( IDPS ).  0 1 2 3 4 5 Rules Redundancy Traversal(31.64) (32.96)Time (S)Violated HoldsFigure 2: Time taken to verify each network invariant for scenarios in §5.1. We show time for checking both when invariants are violated (Violated) and veriﬁed (Holds).  0 50 100 150 200 250 300 350 25 50 100 250 500 1000Time (S) # of Policy Equivalence ClassesRules Redundancy TraversalFigure 3: Time taken to verify all network invariants as a function of policy complexity for §5.1. The plot presents minimum, maximum, 5th, 50thand 95 thpercentile time for each. Therefore, when networks meet our requirements, all reacha bility invariants can be veriﬁed in network slices whose size is independent of network size. Finally, we note that our restrictions are commonly met by deployed networks: previous studies have shown that ﬁre walls, proxies and IDSes are the most commonly deployed middlebox types; of these ﬁrewalls are ﬂowparallel , most proxies are originagnostic and many IDSes are off path and do not affect reachability invariants. Furthermore, IDSes can be safely treated as originagnostic middleboxes in VMN, without loss in veriﬁcation ﬁdelity. For ease of management, network policy in large deployed networks is commonly ex pressed in terms of policy classes, and the forwarding graph is restricted for performance. Therefore, we do not believe these restrictions pose a severe challenge for VMN. Finally, VMN can still be used to verify moderate sized networks which violate these restrictions. 4.2 Network Symmetry Slices allow us to scale veriﬁcation of an individual invariant, however a single network might enforce several invariants, and the number of invariants might grow with network size. However, networks are often symmetric with respect to pol icy classes, i.e.,packets whose source and destination belong to the same policy class traverse the same sequence of mid dlebox types. When possible VMN takes advantage of this symmetry to reduce the number of invariants to be veriﬁed. We say two invariants are symmetric when one can be trans formed to another by replacing nodes with other nodes in the same policy class. If an invariant Iholds in a symmetric network, then so do all invariants symmetric to I. When net works are symmetric, VMN uses this observation to divide invariants into symmetric groups and proves just one invari ant in each symmetry group, allowing us to eliminate many invariant checks. 5 Evaluation To evaluate VMN we ﬁrst examine how it would deal with several realworld scenarios and then investigate how it scales to large networks. We ran our evaluation on servers running 10core, 2.6GHz Intel Xeon processors with 256 GB of RAM. We report times taken when veriﬁcation is performed using a single core. Veriﬁcation can be trivially parallelized over mul tiple invariants. We used Z3 version 4.4.2 for our evaluation.SMT solvers rely on randomized search algorithms, and their performance can vary widely across runs. The results reported here are generated from 100 runs of each experiment. 5.1 RealWorld Evaluation A previous measurement study [34] looked at more than 10 datacenters over a 2 year period, and found that conﬁgura tion bugs (in both middleboxes and networks) are a frequent cause of failure. Furthermore, the study analyzed the use of redundant middleboxes for fault tolerance, and found that redundancy failed due to misconﬁguration roughly 33% of the time. Here we show how VMN can detect and prevent the three most common classes of conﬁguration errors, including errors affecting fault tolerance. For our evaluation we use a datacenter topology (Figure 1) containing 1000 end hosts and three types of middleboxes: stateful ﬁrewalls, load balancers and intrusion detection and prevention systems (IDPSs). We use redundant instances of these middleboxes for fault toler ance. For each scenario we report time taken to verify a single invariant (Figure 2), and time taken to verify all invariants (Figure 3); and show how these times grow as a function of policy complexity (as measured by the number of policy equivalence classes). Each box and whisker plot shows mini mum, 5thpercentile, median, 95thpercentile and maximum time for veriﬁcation. Incorrect Firewall Rules: According to [34], 70% of all reported middlebox misconﬁguration are attributed to incor rect rules installed in ﬁrewalls. To evaluate this scenario we begin by assigning each host to one of a few policy groups.8 We then add ﬁrewall rules to prevent hosts in one group from communicating with hosts in any other group. We introduce misconﬁguration by deleting a random set of these ﬁrewall rules. We use VMN to identify for which hosts the desired invariant holds ( i.e.,that hosts can only communicate with other hosts in the same group). Note that all middleboxes in this evaluation are ﬂowparallel, and hence the size of a slice on which invariants are veriﬁed is independent of both policy complexity and network size. In our evaluation, we found that VMN correctly identiﬁed all violations, and did 8Note, policy groups are distinct from policy equivalence class; a policy group signiﬁes how a network administrator might group hosts while conﬁguring the network, however policy equivalence classes are assigned based on the actual network conﬁguration. 8 0 50 100 150 200 250 300 350 400 450 500  10  20  30  40  50  60  70  80  90  100Time (seconds) # of Policy Equivalence ClassesTime to Prove Invariant Violation Time to Prove Invariant HoldsFigure 4: Time taken to verify each data isolation invariant. The shaded region represents the 5th–95thpercentile time.  0 2000 4000 6000 8000 10000 12000 14000 10 25 50 75 100Time (S) # of Policy Equivalence ClassesFigure 5: Time taken to verify all data isolation invariants in the network described in §5.2. not report any false positives. The time to verify a single in variant is shown in Figure 2 under Rules. When verifying the entire network, we only need to verify as many invariants as policy equivalence classes; hosts affected by misconﬁgured ﬁrewall rules fall in their own policy equivalence class, since removal of rules breaks symmetry. Figure 3 (Rules) shows how whole network veriﬁcation time scales as a function of policy complexity. Misconﬁgured Redundant Firewalls Redundant ﬁre walls are often misconﬁgured so that they do not provide fault tolerance. To show that VMN can detect such errors we took the networks used in the preceding simulations (in their properly conﬁgured state) and introduced misconﬁguration by removing rules from some of the backup ﬁrewall. In this case invariant violation would only occur when middleboxes fail. We found VMN correctly identiﬁed all such violations, and we show the time taken for each invariant in Figure 2 under “Redundant”, and time taken for the whole network in Figure 3. Misconﬁgured Redundant Routing Another way that re dundancy can be rendered ineffective by misconﬁguration is if routing (after failures) allows packets to bypass the mid dleboxes speciﬁed in the pipeline invariants. To test this we considered, for the network described above, an invariant requiring that all packet in the network traverse an IDPS be fore being delivered to the destination host. We changed a randomly selected set of routing rules so that some packets would be routed around the redundant IDPS when the primary had failed. VMN correctly identiﬁed all such violations, and we show times for individual and overall network veriﬁcation under “Traversal” in Figures 2 and 3. We can thus see that veriﬁcation, as provided by VMN, can be used to prevent many of the conﬁguration bugs re ported to affect today’s production datacenters. Moreover, the veriﬁcation time scales linearly with the number of policy equivalence classes (with a slope of about three invariants per second). We now turn to more complicated invariants involving data isolation.5.2 Data Isolation Modern data centers also run storage services such as S3 [2], AWS Glacier [1], and Azure Blob Store [3]. These storage ser vices must comply with legal and customer requirements [32] limiting access to this data. Operators often add caches to these services to improve performance and reduce the load on the storage servers themselves, but if these caches are misplaced or misconﬁgured then the access policies could be violated. VMN can verify these data isolation invariants. To evaluate this functionality, we used the topology (and correct conﬁguration) from §5.1 and added a few content caches by connecting them to top of rack switches. We also assume that each policy group contains separate servers with private data (only accessible within the policy group), and servers with public data (accessible by everyone). We then consider a scenario where a network administrator inserts caches to reduce load on these data servers. The content cache is conﬁgured with ACL entries9that can implement this invariant. Similar to the case above, we introduce con ﬁguration errors by deleting a random set of ACLs from the content cache and ﬁrewalls. We use VMN to verify data isolation invariants in this network ( i.e.,ensure that private data is only accessible from within the same policy group, and public data is accessible from everywhere). VMN correctly detects invariant viola tions, and does not report any false positives. Content caches are origin agnostic, and hence the size of a slice used to ver ify these invariants depends on policy complexity. Figure 4 shows how time taken for verifying each invariant varies with the number of policy equivalence classes. In a network with 100different policy equivalence classes, veriﬁcation takes less than 4minutes on average. Also observe that the vari ance for verifying a single invariant grows with the size of slices used. This shows one of the reasons why the ability to use slices and minimize the size of the network on which an invariant is veriﬁed is important. Figure 5 shows time taken to verify the entire network as we increase the number of policy equivalence classes. 9This is a common feature supported by most open source and commercial caches. 9InternetFWGWSubnet 1Subnet 2Subnet 3Figure 6: Topology for enterprise network used in §5.3.1, containing a ﬁrewall ( FW) and a gateway ( GW).  0.01 0.1 1 10 100 Slice 17 47 77Time (S) Network Size (Hosts + Middleboxes)Public Quarantined PrivateFigure 7: Distribution of veriﬁcation time for each invariant in an enterprise network (§5.3.1) with network size. The left of the vertical line shows time taken to verify a slice, which is independent of network size, the right shows time taken when slices are not used.  0.01 0.1 1 10 100 1000 10000 100000 Slice 5 10 15 20Time (S) # of TenantsPrivPriv PubPriv PrivPubFigure 8: Average veriﬁcation time for each invariant in a multitenant datacenter (§5.3.2) as a function of number of tenants. Each tenant has 10 hosts. The left of the vertical line shows time taken to verify a slice, which is independent of the number of tenants. 5.3 Other Network Scenarios We next apply VMN to several other scenarios that illustrate the value of slicing (and symmetry) in reducing veriﬁcation time. 5.3.1 Enterprise Network with Firewall First, we consider a typical enterprise or university network protected by a stateful ﬁrewall, shown in Figure 6. The net work interconnects three types of hosts: 1.Hosts in public subnets should be allowed to both initi ate and accept connections with the outside world. 2.Hosts in private subnets should be ﬂowisolated ( i.e., allowed to initiate connections to the outside world, but never accept incoming connections). 3.Hosts in quarantined subnets should be nodeisolated (i.e.,not allowed to communicate with the outside world). We vary the number of subnets keeping the proportion of subnet types ﬁxed; a third of the subnets are public, a third are private and a third are quarantined. We conﬁgure the ﬁrewall so as to enforce the target in variants correctly: with two rules denying access (in either direction) for each quarantined subnet, plus one rule denying inbound connections for each private subnet. The results we present below are for the case where all the target invariants hold. Since this network only contains a ﬁrewall, using slices we can verify invariants on a slice whose size is independent of network size and policy complexity. We can also leverage the symmetry in both network and policy to reduce the num ber of invariants that need to be veriﬁed for the network. In contrast, when slices and symmetry are not used, the model for verifying each invariant grows as the size of the network, and we have to verify many more invariants. In Figure 7 we show time taken to verify the invariant using slices (Slice) and how veriﬁcation time varies with network size when slices are not used. 5.3.2 MultiTenant Datacenter Next, we consider how VMN can be used by a cloud provider (e.g., Amazon) to verify isolation in a multitenant datacenter. We assume that the datacenter implements the Amazon EC2 Security Groups model [4]. For our test we considered a datacenter with 600physical servers (which each run a virtualswitch) and 210physical switches (which implement equal cost multipath routing). Tenants launch virtual machines (VMs), which are run on physical servers and connect to the network through virtual switches. Each virtual switch acts as a stateful ﬁrewall, and defaults to denying all trafﬁc ( i.e., packets not speciﬁcally allowed by an ACL are dropped). To scale policy enforcement, VMs are organized in security groups with associated accept/deny rules. For our evaluation, we considered a case where each tenant organizes their VMs into two security groups: 1.VMs that belong to the public security group are al lowed to accept connections from any VMs. 2.VMs that belong to the private security group are ﬂow isolated ( i.e.,they can initiate connections to other tenants’ VMs, but can only accept connections from this tenant’s public and private VMs). We also assume that ﬁrewall conﬁguration is speciﬁed in terms of security groups ( i.e.,on receiving a packet the ﬁre wall computes the security group to which the sender and receiver belong and applies ACLs appropriately). For this evaluation, we conﬁgured the network to correctly enforce tenant policies. We added two ACL rules for each tenant’s public security group allowing incoming and outgoing pack ets to anyone, while we added three rules for private security groups; two allowing incoming and outgoing trafﬁc from the tenant’s VM, and one allowing outgoing trafﬁc to other tenants. For our evaluation we consider a case where each tenant has 10VMs, 5public and 5private, which are spread across physical servers. These rules result in ﬂowparallel middleboxes, so we can use ﬁxed size slices to verify each invariant. The number of invariants that need to be veriﬁed grow as a function of the number of tenants. In Figure 8 we show time taken to verify one instance of the invariant when slices are used (Slice) and how veriﬁcation time varies with network size when slices are not used. The invariants checked are: (a) private hosts in one group cannot reach private hosts in another group (PrivPriv), (b) public hosts in one group cannot reach private hosts in another group (PrivPub), and (c) private hosts in one group canreach public hosts in another. 10PeerIDSFWSBSubnet 1Subnet 2Subnet 3(a)  0.1 1 10 100 1000 Slice 3 75 150 180 225Time (S) # of Subnets (b)  0.1 1 10 100 1000 Slice 1 5 10 20 30Time (S) # of Peering Points (c) Figure 9: (a) shows the pipeline at each peering point for an ISP; (b) distribution of time to verify each invariant given this pipeline when the ISP peers with other networks at 5 locations; (c) average time to verify each invariant when the ISP has 75 subnets. In both cases, to the left of the black line we show time to verify on a slice (which is independent of network size) and vary sizes to the right. 5.3.3 ISP with Intrusion Detection Finally, we consider an Internet Service Provider (ISP) that implements an intrusion detection system (IDS). We model our network on the SWITCHlan backbone [41], and assume that there is an IDS box and a stateful ﬁrewall at each peer ing point (Figure 9(a)). The ISP contains public, private and quarantined subnets (with policies as deﬁned in §5.3.1) and the stateful ﬁrewalls enforce the corresponding invariants. Additionally, each IDS performs lightweight monitoring ( e.g., based on packet or byte counters) and checks whether a par ticular destination preﬁx ( e.g., a customer of the ISP) might be under attack; if so, all trafﬁc to this preﬁx is rerouted to a scrubbing box that performs more heavyweight analysis, discards any part of the trafﬁc that it identiﬁes as “attack traf ﬁc,” and forwards the rest to the intended destination. This combination of multiple lightweight IDS boxes and one (or a few) centralized scrubbing boxes is standard practice in ISPs that offer attack protection to their customers.10 To enforce the target invariants (for public, private, and quarantined subnets) correctly, all inbound trafﬁc must go through at least one stateful ﬁrewall. We consider a miscon ﬁguration where trafﬁc rerouted by a given IDS box to the scrubbing box bypasses all stateful ﬁrewalls. As a result, any part of this rerouted trafﬁc that is notdiscarded by the scrub bing box can reach private or quarantined subnets, violating the (simple or ﬂow) isolation of the corresponding hosts. When verifying invariants in a slice we again take advan tage of the fact that ﬁrewalls and IDSes are ﬂowparallel.11 For each subnet, we can verify invariants in a slice contain ing a peering point, a host from the subnet, the appropriate ﬁrewall, IDS and a scrubber. Furthermore, since all subnets belong to one of three policy equivalence classes, and the network is symmetric, we only need run veriﬁcation on three slices. We begin by evaluating a case where the ISP, similar to the SWITCHlan backbone has 5peering points with other net works. We measure veriﬁcation time as we vary the number 10This setup is preferred to installing a separate scrubbing box at each peering point because of the high cost of these boxes, which can amount to several million dollars for a warranteed period of 3 years. 11While IDSes in general might not be ﬂowparallel, the speciﬁc IDS used here is ﬂowparallel with respect to a slice.of subnets (Figure 9(b)), and report time taken, on average, to verify each invariant. When slices are used, the median time for verifying an invariant is 0:21seconds, by contrast when veriﬁcation is performed on the entire network, a network with 250subnets takes approximately 6minutes to verify. Furthermore, when verifying all invariants, only 3slices need to be veriﬁed when we account for symmetry, otherwise the number of invariants veriﬁed grows with network size. In Figure 9(c) we hold the number of subnets constant (at75) and show veriﬁcation time as we vary the number of peering points. In this case the complexity of verifying the entire network grows more quickly (because the IDS model is more complex leading to a larger increase in problem size). In this case, verifying correctness for a network with 50peering points, when veriﬁcation is performed on the whole entire network, takes approximately 10minutes. Hence, being able to verify slices and use symmetry is crucial when verifying such networks. 6 Related Work "
54,Zero-Knowledge Authentication.txt,"In the thesis we focus on designing an authentication system to authenticate
users over a network with a username and a password. The system uses the
zero-knowledge proof (ZKP) system as a password verification mechanism. The ZKP
protocol used is based on the quadratic residuosity problem. The authentication
system is defined as a method in the extensible authentication protocol (EAP).
Using a ZKP system yields interesting security properties that make the system
favourable to be used over insecure networks.",1.1 Structure of the thesis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 Methodologies and Tools 2 
309,An Ensemble Noise-Robust K-fold Cross-Validation Selection Method for Noisy Labels.txt,"We consider the problem of training robust and accurate deep neural networks
(DNNs) when subject to various proportions of noisy labels. Large-scale
datasets tend to contain mislabeled samples that can be memorized by DNNs,
impeding the performance. With appropriate handling, this degradation can be
alleviated. There are two problems to consider: how to distinguish clean
samples and how to deal with noisy samples. In this paper, we present Ensemble
Noise-robust K-fold Cross-Validation Selection (E-NKCVS) to effectively select
clean samples from noisy data, solving the first problem. For the second
problem, we create a new pseudo label for any sample determined to have an
uncertain or likely corrupt label. E-NKCVS obtains multiple predicted labels
for each sample and the entropy of these labels is used to tune the weight
given to the pseudo label and the given label. Theoretical analysis and
extensive verification of the algorithms in the noisy label setting are
provided. We evaluate our approach on various image and text classification
tasks where the labels have been manually corrupted with different noise
ratios. Additionally, two large real-world noisy datasets are also used,
Clothing-1M and WebVision. E-NKCVS is empirically shown to be highly tolerant
to considerable proportions of label noise and has a consistent improvement
over state-of-the-art methods. Especially on more difficult datasets with
higher noise ratios, we can achieve a significant improvement over the
second-best model. Moreover, our proposed approach can easily be integrated
into existing DNN methods to improve their robustness against label noise.","Together with the resurgence and remarkable success of DNNs, largescale datasets have become increasingly com mon. For supervised learning tasks, modern DNNs gener ally require the datasets to be annotated with accurate la Equal contribution.bels to achieve high performance. However, to correctly label large amounts of data is very costly and errorprone, even highquality handlabeled benchmark dataset such as ImageNet [Deng et al. , 2009 ]contains mislabeled sam ples[Northcutt et al. , 2019 ]. There exist alternative, lowcost methods, including largescale annotation through crowd sourcing [Sheng et al. , 2008 ]and online web queries [Divvala et al. , 2014 ], but these inevitably yield a higher proportion of incorrect class labels. DNNs are prone to overﬁtting to corrupted data sam ples, which increases the generalization error of the net work [Zhang et al. , 2017a ]. To address this issue, numerous algorithms have been proposed to train DNNs in a way ro bust to label noise [Wang et al. , 2019; Xu et al. , 2019 ]. The capability of DNNs to ﬁt noisy data has been further stud ied by Chen et al. [2019 ]. They showed that, for symmetric noise, the test accuracy is a quadratic function of the noise ratio, and claim that generalization occurs in the sense of dis tribution. In this paper, we relax their assumptions and give a theoretical analysis of the impact that an imperfect classiﬁer has. Our ﬁndings demonstrate that, while the noise level has a signiﬁcant impact, the performance of the classiﬁer is key. Based on our analysis, we propose ENKCVS, a novel en semble method based on Kfold crossvalidation to increase the generalization performance. We empirically evaluate our solution and demonstrate that it outperforms the stateofthe art, proving the effectiveness of our method. In summary, our contributions are as follows. • We propose a novel method (ENKCVS) based on a combination of Kfold crossvalidation and ensemble learning. Samples are selected from the noisy data by keeping those where the predicted label matches the given (noisy) label. Any nonselected samples can then either be discarded or reweighted to have a lower im pact. Mixup [Zhang et al. , 2017b ]is applied during training to augment the data. • We further propose a label reweighting scheme for sam ples that are likely erroneous. For these uncertain sam ples, we consider both the given label and a generated pseudo label with the weight set using the entropy of the predicted labels given by ENKCVS. • We empirically show that the proposed solution out performs stateoftheart noiserobust methods on imarXiv:2107.02347v1  [cs.LG]  6 Jul 2021age recognition and text classiﬁcation tasks on multiple datasets. Moreover, our solution can easily be incorpo rated into existing network architectures to enhance their robustness to noisy labels. 2 Related Work "
113,Verifying Deep Learning-based Decisions for Facial Expression Recognition.txt,"Neural networks with high performance can still be biased towards
non-relevant features. However, reliability and robustness is especially
important for high-risk fields such as clinical pain treatment. We therefore
propose a verification pipeline, which consists of three steps. First, we
classify facial expressions with a neural network. Next, we apply layer-wise
relevance propagation to create pixel-based explanations. Finally, we quantify
these visual explanations based on a bounding-box method with respect to facial
regions. Although our results show that the neural network achieves
state-of-the-art results, the evaluation of the visual explanations reveals
that relevant facial regions may not be considered.","For healthcare professionals it might not be easy to estimate pain, when patients are not able to communicate their pain intensity. Hence, automatically detecting pain from facial expressions can support the medical decision making process. However, as exemplied by H agele et al. [1], highperforming classiers can be biased towards the training data. It is therefore crucial to make the reasons for a classication outcome transparent instead of just relying on the performance values. To indicate the in uence of each pixel on the classication outcome, H agele et al. [1] utilize layerwise relevance propagation (LRP) [2], which is an explanation method that creates heatmaps for individual images. Performing a manual inspection of all heatmaps would be a tedious task. Kohlbrenner et al. [3] propose a quantication method for evaluating visual explanations generated by LRP. Their method is based on bounding boxes containing the objects relevant to the true class. Based on these contributions, we propose a verication pipeline in order to evaluate neural networks, which detect painrelated facial expressions. Facial expressions are divided into isolated facial movements, each being an Action Unit (AU). Certain combinations of AUs may be indicators for pain [4]. We apply our trained Residual Network (ResNet) to classify these painrelevant Action Units. Afterwards, we use LRP to generate heatmaps in order to visualize the classier's This project is funded by the Federal Ministry of Education and Research, grant no. 01IS18056A and 01IS18056B (TraMeExCo). We thank Sebastian Lapuschkin and Jaspar Pahl for their support.arXiv:2003.00828v1  [cs.CV]  14 Feb 2020decision. Finally, we adapt the boundingbox approach to Action Unit specic boundaries in order to quantify the quality of these heatmaps. This way we aim to assess the domain specic performance of the classier. To the best of our knowledge, LRP has not yet been applied for AU de tection. Furthermore, our verication pipeline integrates a new domainspecic evaluation method to assess the quality of classications with the help of LRP. This way we contribute to transparency for AU classication. The paper is organized as follows: In section 2 we describe the used meth ods, including the network architecture and LRP. In section 3 we describe and discuss our results based on the application of the previously dened verication pipeline. The paper concludes with a summary as well as future prospects in section 4. 2 Methods "
216,Work In Progress: Safety and Robustness Verification of Autoencoder-Based Regression Models using the NNV Tool.txt,"This work in progress paper introduces robustness verification for
autoencoder-based regression neural network (NN) models, following
state-of-the-art approaches for robustness verification of image classification
NNs. Despite the ongoing progress in developing verification methods for safety
and robustness in various deep neural networks (DNNs), robustness checking of
autoencoder models has not yet been considered. We explore this open space of
research and check ways to bridge the gap between existing DNN verification
methods by extending existing robustness analysis methods for such autoencoder
networks. While classification models using autoencoders work more or less
similar to image classification NNs, the functionality of regression models is
distinctly different. We introduce two definitions of robustness evaluation
metrics for autoencoder-based regression models, specifically the percentage
robustness and un-robustness grade. We also modified the existing Imagestar
approach, adjusting the variables to take care of the specific input types for
regression networks. The approach is implemented as an extension of NNV, then
applied and evaluated on a dataset, with a case study experiment shown using
the same dataset. As per the authors' understanding, this work in progress
paper is the first to show possible reachability analysis of autoencoder-based
NNs.","Stateoftheart and welltrained neural networks (NN) can easily be attacked by small perturbations in inputs, leading to signiﬁcant aberrations in their outputs [14, 23, 33]. These input perturbations are not only limited to imagebased networks but also apply to other input types as well, e.g., timeseries data or input signals. Such lack of robustness poses serious risks to information integrity, privacy and security, and can be catastrophic in safetycritical applications [11, 29]. While veriﬁcation of NNs with image inputs is a vastly growing research area; speciﬁcally, with recent ongoing works on safety and robustness checking of feedforward (FFNN), convolutional (CNN), and semantic segmentation networks (SSN); less has been done in the domain of autoencoder veriﬁcation. Classiﬁcation models using autoencoders work almost similar to usual classiﬁers, but there is a need for new research to develop veriﬁcation techniques for regression models. The regressionbased autoencoders regenerate the input in its output and thus can be checked using veriﬁcation techniques whether the recreated output comes within a certain accepted range of the unperturbed input, in case there is a certain fault/attack on its input side. In a prior work, the authors of [36] introduced a novel framework for NN veriﬁcation named Neu ral Network Veriﬁcation (NNV) [38] tool, capable of evaluating the robustness of several DNN ar chitectures, e.g., FFNN, CNN, SSN, etc. Later, a new setbased approach, Imagestar [34, 36] is also incorporated into this tool. In this work in progress work, we explore similar methods in the context of autoencoder veriﬁcation via experimenting on a sampled dataset and checking if the output lies within a predetermined safe threshold around the corresponding uninterrupted input values, given a speciﬁc type of fault in the input.80 Safety and Robustness Veriﬁcation of AutoencoderBased Regression Models using the NNV Tool 2 Related Work "
199,Gaussian speaker embedding learning for text-independent speaker verification.txt,"The x-vector maps segments of arbitrary duration to vectors of fixed
dimension using deep neural network. Combined with the probabilistic linear
discriminant analysis (PLDA) backend, the x-vector/PLDA has become the dominant
framework in text-independent speaker verification. Nevertheless, how to
extract the x-vector appropriate for the PLDA backend is a key problem. In this
paper, we propose a Gaussian noise constrained network (GNCN) to extract
xvector, which adopts a multi-task learning strategy with the primary task
classifying the speakers and the auxiliary task just fitting the Gaussian
noises. Experiments are carried out using the SITW database. The results
demonstrate the effectiveness of our proposed method","  Speaker verification (SV) is the task of verifying a person’s  claimed identity from some speech signal. SV systems  typically  consists of two main stages: (1) a frontend that  converts a variable length utterance to a low  and fixed   dimensional vector, and (2) a backend for calculating the  similarity between speaker representation s. For the past  decade , the combination of i vector [1] and probabilistic  linear discriminant analysis (PLDA) [2] has become the state  oftheart approach in the SV field.         Since the great success of deep learning over a wide  range of machine learning tasks , more attention has been  drawn  to the use of deep neural network (DNN) to generate  speaker vectors  having more discriminative power . In most  deep speaker embedding systems , a frame level feature  extractor is design ed firstly, which can be modeled by  convolution neural network (CNN)  [3, 4], timedelay neural  network (TDNN)  [5, 6], recurrent neural network (RNN)  [7]  or their variants  [8, 9, 10]. Next , a pooling layer is exploit ed  to reduce the temporal dimension of frames level features to  get a fixed dimensional vector  and the speaker representation   is generated from the following stacked fully connected  layers . Typically, statistic s pooling [ 5] combined with  different attention mechanisms [11, 12, 13] is used to replace the average pooling for capturing long term speaker  characteristic  more effectively.          As we all know, a speech contains a lot of information   (such as phoneme , emotion or noise)  and the speaker identity   is weak information. How to extract more robust and  discriminative speaker embedding is always a research focus.  Recently, many deep speaker embedding systems , which   have a primary task of classifying the target speakers and an  auxiliary task , have been proposed.  Some researchers find the  highorder statistics [14] and phonetic labels  [15, 16] of the  input acoustic features  are helpful for training the model .  Furthermore , SNR values, the labels of environment types  [17], channel types  [18] or language types [ 19, 20] of  utterances are also utilize d in some systems to minimize the  domain mismatch  between the training data and test data. In  a word, a ll these methods are implemented using multi task  learning frameworks.   However, a potential proble m of most deep learning  methods is that there is a mismatch of training loss and  LDA/PLDA training objectives, as noticed in [ 21]. In order  to make the embedding output suitable for the backend, a  Gaussian constrained training method is proposed [ 22]. The  strategy is to minimize intra class variations , so the  performance is highly dependent on the center vectors of  different speakers.  In addition , the additive  regularization is  not added on all the speaker embedding layer s, which  weak ens the ability of the final embeddings to fit the  Gaussian  distribution .   In this work, we propose a Gaussian noise constrained  network (GNCN)  which  adopts multi task learning  framework to extract  speaker representations . As the  auxiliary task , normal  distributed noise vectors will be fitted   in the embedding layers. In this way, the secondary objective  can make the distribution of all the x vectors to fit Gaussian   distribution . To the best of our knowledge , no study  has yet  been done on deep speaker embedding from such a  perspective. We evaluated our experiments on the SITW  evaluation dataset. The experimental results show the  proposed methods can improve the performance of the DNN  embedding system.   The rema inder of this paper is organized as follows.  Section 2 describes the related works , including the x vector  baseline system and Gaussian constrained training algorithm .  Section 3 introduces the proposed method. The experimental set up, results and analysis are presented in Section 4. Finally,  the conclusion is given in Section 5.     2. RELATED WORK S  "
358,Neural Abstractions.txt,"We present a novel method for the safety verification of nonlinear dynamical
models that uses neural networks to represent abstractions of their dynamics.
Neural networks have extensively been used before as approximators; in this
work, we make a step further and use them for the first time as abstractions.
For a given dynamical model, our method synthesises a neural network that
overapproximates its dynamics by ensuring an arbitrarily tight, formally
certified bound on the approximation error. For this purpose, we employ a
counterexample-guided inductive synthesis procedure. We show that this produces
a neural ODE with non-deterministic disturbances that constitutes a formal
abstraction of the concrete model under analysis. This guarantees a fundamental
property: if the abstract model is safe, i.e., free from any initialised
trajectory that reaches an undesirable state, then the concrete model is also
safe. By using neural ODEs with ReLU activation functions as abstractions, we
cast the safety verification problem for nonlinear dynamical models into that
of hybrid automata with affine dynamics, which we verify using SpaceEx. We
demonstrate that our approach performs comparably to the mature tool Flow* on
existing benchmark nonlinear models. We additionally demonstrate and that it is
effective on models that do not exhibit local Lipschitz continuity, which are
out of reach to the existing technologies.","Dynamical models describe processes that are ubiquitous in science and engineering. They are widely used to model the behaviour of cyberphysical system designs, whose correctness is crucial when they are deployed in safetycritical domains [10, 13, 49]. To guarantee that a dynamical model satisﬁes a safety speciﬁcation, simulations are useful but insufﬁcient because they are inherently nonexhaustive and they suffer from numerical errors, which may leave unsafe behaviours unidentiﬁed. Formal veriﬁcation of continuous dynamical models tackles the question of determining with formal certainty whether every possible behavior of the model satisﬁes a safety speciﬁcation [45, 51, 112]. In this paper, we present a method to combine machine learning and symbolic reasoning for a sound and effective safety veriﬁcation of nonlinear dynamical models. The formal veriﬁcation problem for continuoustime and hybrid dynamical models is unsolvable in general and, even for models with linear dynamics, complete procedures are available under stringent conditions [11, 12, 72, 86, 87]. For most practical models that contain nonlinear terms [81, 105], The authors are listed alphabetically 36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2301.11683v1  [cs.LO]  27 Jan 2023methods for formal veriﬁcation with soundness guarantees involve laborious safety and reachability "
111,Safety Verification of Deep Neural Networks.txt,"Deep neural networks have achieved impressive experimental results in image
classification, but can surprisingly be unstable with respect to adversarial
perturbations, that is, minimal changes to the input image that cause the
network to misclassify it. With potential applications including perception
modules and end-to-end controllers for self-driving cars, this raises concerns
about their safety. We develop a novel automated verification framework for
feed-forward multi-layer neural networks based on Satisfiability Modulo Theory
(SMT). We focus on safety of image classification decisions with respect to
image manipulations, such as scratches or changes to camera angle or lighting
conditions that would result in the same class being assigned by a human, and
define safety for an individual decision in terms of invariance of the
classification within a small neighbourhood of the original image. We enable
exhaustive search of the region by employing discretisation, and propagate the
analysis layer by layer. Our method works directly with the network code and,
in contrast to existing methods, can guarantee that adversarial examples, if
they exist, are found for the given region and family of manipulations. If
found, adversarial examples can be shown to human testers and/or used to
fine-tune the network. We implement the techniques using Z3 and evaluate them
on state-of-the-art networks, including regularised and deep learning networks.
We also compare against existing techniques to search for adversarial examples
and estimate network robustness.","Deep neural networks have achieved impressive experimental results in image classiﬁ cation, matching the cognitive ability of humans [23] in complex tasks with thousands of classes. Many applications are envisaged, including their use as perception modules and endtoend controllers for selfdriving cars [15]. Let Rnbe a vector space of images (points) that we wish to classify and assume that f:Rn!C, where Cis a (ﬁnite) set of class labels, models the human perception capability, then a neural network classiﬁer is a function ˆf(x) which approximates f(x) from Mtraining examples f(xi;ci)gi=1;::;M. For example, a perception module of a selfdriving car may input an image from a camera and must correctly classify the type of object in its view, irrespective of aspects such as the angle of its vision and image imperfections. Therefore, though they clearly in clude imperfections, all four pairs of images in Figure 1 should arguably be classiﬁed as automobiles, since they appear so to a human eye. ?This work is supported by the EPSRC Programme Grant on Mobile Autonomy (EP/M019918 /1). Part of this work was done while MK was visiting the Simons Institute for the Theory of Computing.arXiv:1610.06940v3  [cs.AI]  5 May 2017Classiﬁers employed in vision tasks are typically multilayer networks, which prop agate the input image through a series of linear and nonlinear operators. They are highdimensional, often with millions of dimensions, nonlinear and potentially dis continuous: even a small network, such as that trained to classify handwritten images of digits 09, has over 60,000 realvalued parameters and 21,632 neurons (dimensions) in its ﬁrst layer. At the same time, the networks are trained on a ﬁnite data set and expected to generalise to previously unseen images. To increase the probability of cor rectly classifying such an image, regularisation techniques such as dropout are typically used, which improves the smoothness of the classiﬁers, in the sense that images that are close (within distance) to a training point are assigned the same class label. automobile to bird  automobile to frog  automobile to airplane  automobile to horse Fig. 1. Automobile images (classiﬁed correctly) and their perturbed images (classiﬁed wrongly) Unfortunately, it has been observed in [13,36] that deep neural networks, includ ing highly trained and smooth networks optimised for vision tasks, are unstable with respect to so called adversarial perturbations . Such adversarial perturbations are (min imal) changes to the input image, often imperceptible to the human eye, that cause the network to misclassify the image. Examples include not only artiﬁcially generated ran dom perturbations, but also (more worryingly) modiﬁcations of camera images [22] that correspond to resizing, cropping or change in lighting conditions. They can be devised without access to the training set [29] and are transferable [19], in the sense that an ex ample misclassiﬁed by one network is also misclassiﬁed by a network with a di erent architecture, even if it is trained on di erent data. Figure 1 gives adversarial pertur bations of automobile images that are misclassiﬁed as a bird, frog, airplane or horse by a highly trained stateoftheart network. This obviously raises potential safety con cerns for applications such as autonomous driving and calls for automated veriﬁcation techniques that can verify the correctness of their decisions. Safety of AI systems is receiving increasing attention, to mention [33,10], in view of their potential to cause harm in safetycritical situations such as autonomous driving. Typically, decision making in such systems is either solely based on machine learning, through endtoend controllers, or involves some combination of logicbased reasoning and machine learning components, where an image classiﬁer produces a classiﬁcation, say speed limit or a stop sign, that serves as input to a controller. A recent trend towards “explainable AI” has led to approaches that learn not only how to assign the classiﬁca tion labels, but also additional explanations of the model, which can take the form of a justiﬁcation explanation (why this decision has been reached, for example identify ing the features that supported the decision) [17,31]. In all these cases, the safety of a decision can be reduced to ensuring the correct behaviour of a machine learning component. However, safety assurance and veriﬁcation methodologies for machine learning are little studied. The main di culty with image classiﬁcation tasks, which play a critical role in per ception modules of autonomous driving controllers, is that they do not have a formal speciﬁcation in the usual sense: ideally, the performance of a classiﬁer should match the perception ability and class labels assigned by a human. Traditionally, the correct ness of a neural network classiﬁer is expressed in terms of risk [37], deﬁned as the probability of misclassiﬁcation of a given image, weighted with respect to the input distribution of images. Similar (statistical) robustness properties of deep neural net work classiﬁers, which compute the average minimum distance to a misclassiﬁcation and are independent of the data point, have been studied and can be estimated using tools such as DeepFool [25] and cleverhans [27]. However, we are interested in the safety of an individual decision , and to this end focus on the key property of the clas siﬁer being invariant to perturbations at a given point . This notion is also known as pointwise robustness [18,12] or local adversarial robustness [21]. Contributions. In this paper we propose a general framework for automated veriﬁ cation of safety of classiﬁcation decisions made by feedforward deep neural networks. Although we work concretely with image classiﬁers, the techniques can be generalised to other settings. For a given image x(a point in a vector space), we assume that there is a (possibly inﬁnite) region around that point that incontrovertibly supports the de cision, in the sense that all points in this region must have the same class. This region is speciﬁed by the user and can be given as a small diameter, or the set of all points whose salient features are of the same type. We next assume that there is a family of operations , which we call manipulations, that specify modiﬁcations to the image under which the classiﬁcation decision should remain invariant in the region . Such manipulations can represent, for example, camera imprecisions, change of camera angle, or replacement of a feature. We deﬁne a network decision to be safefor input xand regionwith respect to the set of manipulations if applying the manipulations on xwill not result in a class change for. We employ discretisation to enable a ﬁnite exhaustive search of the high dimensional region for adversarial misclassiﬁcations. The discretisation approach is justiﬁed in the case of image classiﬁers since they are typically represented as vectors of discrete pixels (vectors of 8 bit RGB colours). To achieve scalability, we propagate the analysis layer by layer , mapping the region and manipulations to the deeper layers. We show that this propagation is sound, and is complete under the additional assumption of minimality of manipulations, which holds in discretised settings. In contrast to existing approaches [36,28], our framework can guarantee that a misclassiﬁcation is found if it exists. Since we reduce veriﬁcation to a search for adversarial examples, we can achieve safety veriﬁcation (if no misclassiﬁcations are found for all layers) or falsiﬁcation (in which case the adversarial examples can be used to ﬁnetune the network or shown to a human tester). We implement the techniques using Z3 [8] in a tool called DLV (Deep Learning Ver iﬁcation) [2] and evaluate them on stateoftheart networks, including regularised and deep learning networks. This includes image classiﬁcation networks trained for clas sifying handwritten images of digits 09 (MNIST), 10 classes of small colour images (CIFAR10), 43 classes of the German Tra c Sign Recognition Benchmark (GTSRB)[35] and 1000 classes of colour images used for the wellknown imageNet largescale visual recognition challenge (ILSVRC) [4]. We also perform a comparison of the DLV falsiﬁcation functionality on the MNIST dataset against the methods of [36] and [28], focusing on the search strategies and statistical robustness estimation. The perturbed images in Figure 1 are found automatically using our tool for the network trained on the CIFAR10 dataset. This invited paper is an extended and improved version of [20], where an extended version including appendices can also be found. 2 Background on Neural Networks We consider feedforward multilayer neural networks [14], henceforth abbreviated as neural networks. Perceptrons (neurons) in a neural network are arranged in disjoint layers, with each perceptron in one layer connected to the next layer, but no connection between perceptrons in the same layer. Each layer Lkof a network is associated with annkdimensional vector space DLkRnk, in which each dimension corresponds to a perceptron. We write Pkfor the set of perceptrons in layer Lkandnk=jPkjis the number of perceptrons (dimensions) in layer Lk. Formally, a (feedforward and deep) neural network Nis a tuple ( L;T;), where L=fLkjk2f0;:::;nggis a set of layers such that layer L0is the input layer and Ln is the output layer, TLLis a set of sequential connections between layers such that, except for the input and output layers, each layer has an incoming connection and an outgoing connection, and =fkjk2f1;:::;nggis a set of activation functions k:DLk"
181,Aurora: a probabilistic algorithm for distributed ledgers enabling trustless synchronization and transaction inclusion verification.txt,"A new node joining a blockchain network first synchronizes with the network
to verify ledger state by downloading the entire ledger history.
  We present Aurora, a probabilistic algorithm that \textit{identifies honest
nodes} for transient or persistent communication in the presence of malicious
nodes in a blockchain network, or ceases operation if it is unable to do so.
The algorithm allows a node joining the network to make an informed decision
about its next synchronization step or to verify that a transaction is
contained in a valid ledger block without downloading the entire ledger or even
the header chain.
  The algorithm constructs a Directed Acyclic Graph on the network topology to
select a subset of nodes including a predefined number of honest nodes with a
given probability.
  It is evaluated on a Bitcoin-like network topology using an open-source
blockchain simulator. We investigate algorithm performance and analyze its
communication complexity. Our results show that the algorithm facilitates
trustless interactions of resource-constrained nodes with a blockchain network
containing malicious nodes to enable a leaner initial blockchain download or an
efficient and trustless transaction inclusion verification. Moreover, the
algorithm can be implemented without any changes to the existing consensus
protocol.","DISTRIBUTED LEDGER Technology (DLT) is a revolu tionary technology for digitizing assets [1] and has become widely known through one of its specializations, the blockchain. Blockchain is a distributed ledger based on a peertopeer (P2P) network to manage transactions for multiple entities in a veriﬁable and traceable manner. Data is written within blocks of blockchain transactions, and does not require a centralized entity, but rather the entire P2P network, to maintain the blocks. The main features of blockchain are immutability, transparency and fault toler ance [2]. Users running blockchain nodes can choose between two types of nodes: full and light. Full nodes download and verify the entire ledger which contains all transactions since the ledger creation. Consequently, they operate in a trustless manner, but also have more stringent hardware requirements. In contrast, light nodes only need to down load part of the ledger (e.g., the header chain) and therefore consume less processing power, network bandwidth and memory compared to full nodes. However, light nodes depend on full nodes — full nodes provide light nodes with the metadata required for their operation. Regardless of the node type and due to the immutability requirement, the size of the ledger is immense as the num ber of transactions continuously increases. This leads to a signiﬁcant load during node synchronization. For example, in January 2020, the Ethereum blockchain had about 250 GB in nonarchived mode [3]. Bitcoin’s header chain had 50 MB in April 2020, while Ethereum’s header chain had 5 GB [4] and is growing about 1 GB per year [5]. Since the synchronization process is memory and time consuming, a new node may either be unable to download the ledger or unwilling to wait too long for the process to complete. For example, nodes in the Internet of Things (IoT) domainare often resourceconstrained devices with limited memory, processing power, and energy, which further exacerbates the problem. As a result, users often opt for thirdparty explorers instead of running their own nodes. In addition to high resource consumption, the process of node synchronization (i.e., bootstrapping) becomes a point of centralization, as it depends in part on a set of well known nodes that are assumed to be both available and honest. Today’s client implementations rely on a list of well known addresses [6] that may not always be available or may even exhibit Byzantine behavior. One could work around the issue of unavailable seeds by manually adding bootstrap peers, but manually added nodes may also exhibit malicious behavior, making a new node vulnerable to various types of attacks, from denial of service to Eclipse attacks [7], [8]. The consequences of a malicious node being a ﬁrst contact node can vary for a victim. They range from a waste of time and resources, in the best case, to a state where, in the worst case, the victim is unaware of the existence of a longer chain because no honest node is available to advertise it. Note that if a new node joins the network through a set of partitioned nodes (which may or may not be malicious), the new node could synchronize with a ledger that is in a state of extended fork, and such a state can lead to a double spending attack. For simplicity, hereinafter we do not distin guish between partitioned and malicious Byzantine nodes, as our algorithm is applicable in both scenarios. The Aurora algorithm, originally proposed in [9], is a consensusagnostic probabilistic algorithm designed to en able a new node to avoid the adverse inﬂuence of Byzantine or malicious nodes in ledger networks by detecting a prede ﬁned number of honest nodes with a high and controllable probability. The identiﬁed nodes are used thereafter for persistent or transient communication of the new node witharXiv:2108.08272v1  [cs.CR]  9 Aug 20212 the network. If the predeﬁned number of honest nodes cannot be identiﬁed, the algorithm stops. Assuming that an honest node reveals to a new node what is accepted as canonical truth by the majority of the voting power, as opposed to a malicious node that tries to subvert the new node, the algorithm saves the node’s resources by efﬁciently discovering a subset of network nodes containing at least 1 or1 +jMjhonest nodes, where Mis the set of malicious network nodes. Such subsets of network nodes can be used for trustless ledger synchronization, or transaction inclusion veriﬁcation without the need to download the entire ledger or even the header chain. While the algorithm can be used by both full and light nodes, its low memory footprint and stochastic behavior make it particularly suitable for resourceconstrained de vices, where it offers the greatest beneﬁt. Although poten tially applicable in other domains, the algorithm offers par ticular beneﬁts in the domain of public and permissionless DLT solutions, as it reinforces the decentralized and trustless ethos on which the technology is based. This work builds on our previous work which intro duced the initial version of the Aurora algorithm [9]. The main contributions of the paper and the most notable dis tinctions compared to our previous work are the following: 1) Redeﬁnition of the algorithm to provide an accurate probabilistic output and communication complexity of the algorithm. The algorithm identiﬁes a subset of network nodes containing a predeﬁned number of honest nodes with high probability as a function of the assumed number of malicious network nodes. 2) A comprehensive evaluation of the redeﬁned al gorithm is performed by simulation on a realis tic network topology using opensource simulation tools in a Bitcoinlike network environment. The simulation results conﬁrm our analytical ﬁndings. 3) Two concrete application examples of the algorithm: a) In a scenario where a new node joins a distributed ledger network, the algorithm assists the node during the synchronization process with a blockchain network with ma licious nodes. b) The algorithm facilitates trustless and efﬁ cient veriﬁcation of transaction inclusion in a block with a predeﬁned correctness proba bility and signiﬁcantly reduces the resource consumption of a device executing the veri ﬁcation procedure. Although our work is relevant to solutions for peer topeer (P2P) networks which detect malicious actors or counter their direct inﬂuence, our work differs from such so lutions in that it does not detect malicious nodes, but rather identiﬁes node sets containing honest nodes. Moreover, our work has a welldeﬁned application domain, namely DLT networks. Comparison with related work is discussed in more detail in Section 2. The paper is organized as follows: Section 2 compares our solution with other existing solutions. Section 3 intro duces necessary terms and deﬁnitions, and enumerates the assumptions under which our algorithm works. Section 4discusses how relevant variables affect the size of sets that contain a predeﬁned number of honest nodes. Section 5 presents the Aurora algorithm and introduces two relevant applications of the algorithm, while Section 6 investigates its communication complexity. Section 7 explains the method ology used for algorithm veriﬁcation, the experiments per formed, and their results. Section 8 discusses possible chal lenges and future work, while Section 9 concludes the paper with a glossary appended at the end of the paper. 2 R ELATED WORK "
192,Leveraging Multiple Online Sources for Accurate Income Verification.txt,"Income verification is the problem of validating a person's stated income
given basic identity information such as name, location, job title and
employer. It is widely used in the context of mortgage lending, rental
applications and other financial risk models. However, the current processes
surrounding verification involve significant human effort and document
gathering which can be both time-consuming and expensive. In this paper, we
propose a novel model for verifying an individual's income given very limited
identity information typically available in loan applications. Our model is a
combination of a deep neural network and hand-engineered features. The hand
engineered features are based upon matching the input information against
income records extracted automatically from various publicly available online
sources (e.g. payscale.com, H-1B filings, government employee salaries). We
conduct experiments on two data sets, one simulated from H-1B records and the
other from a real-world data set of peer-to-peer (P2P) loan applications
obtained from one of the world's largest P2P lending platform. Our results show
a significant reduction in error of 3-6% relative to several strong baselines.
We also perform ablation studies to demonstrate that a combined model is indeed
necessary to achieve state-of-the-art performance on this task.","web to solve the prediction problem. Obviously, using the public web comes with the following challenges:  •Search, Extract, and Match: We need to build queries from input to get a candidate set of web documents and database records that contain income information. We then have to extract data from structured and unstructured sources and filter the sources to keep only those which have the closest match to the input identity.  •Partial and ambiguous information in the input and the Web: An example of this is acronyms and alternate names of companies. For example, “United States Postal Service” can be represented as “USPS”, “U.S.P.S”, “US Postal Service” etc. We need to be able to identify that these are all the same entity.  •Erroneous data on the Web: Not all sources on the web are accurate. Some web sources misrepresent salary information or are out dated.  We have built a robust system that addresses all the aforementioned issues. First, our algorithm leverages search engines to surface salary content on tail entities; the surfaced content includes domains such as payscale.com, salary.com, H1B data, etc. We have also crawled a large number of public government databases and indexed the available information in a structured format so that they can be searched directly. Each of these domains pose a different extraction challenge. For example, in some cases we need to extract content from unstructured text, whereas in other cases we can directly use wrappers based on XPath expressions tailored to specific websites.  Second, to address the issue of partial information (either in the input or on the Web), we often “expand” the scope of our identity: for example, we infer the industry from the company, the experience/level from job title and date of birth if present, and then generalize our search to the given position and industry.  Finally, to address the issue of possibly incorrect information on the Web, we build a model that aggregates salary ranges across several domains, and then computes one unified range by factoring in (a) frequencies of occurrence, (b) trustworthiness of sources, and (c) strength of identity match between the input and a source. For example, if multiple sources indicate that the median salary of a software engineer in San Francisco is around $100,000, but one source indicates the median salary is $50,000, the algorithm would center the range around $100,000.  Table 1. Example Input   We have made the following assumptions in our work:  •Employment information is available and correct.  AttributeExampleNameBarack H ObamaAddressWashington DCDate of BirthAugust 4, 1961EmployerUnited States GovernmentPositionPresidentStated Income$400,000!3 •Identity information is available and correct.  •Verified income provided by the client does not include rent, interest payment, dividends, debts etc.  We introduce the model and delve into each component in detail in Section 2. We discuss the experiments used to validate our model in Section 3. We cover the previous literature in income modeling in Section 4 and conclude the paper in Section 5.  2.Method  In this section, we first describe the input provided for income verification (Section 2.1) and outline our overall approach to tackling it (Section 2.2). Then in Section 2.4 we delve into the system design and describe the individual system components at a highlevel. Sections 2.3 and 2.5 lay out the crux of this paper by describing the systems utilizing online sources and input data respectively in detail.  2.1.Input Description  Table 1 gives an example input to our system. Each row in the input has personally identifiable information (e.g. name, address, dob), employment information (e.g. employer, job title) and the individual’s stated income. Our goal in this paper is to verify that the stated income is accurate. There are two things to note about the input information here.  •Firstly, some of the information may be missing or incomplete. For example, the second row in the table has an incomplete address. However, we are always provided with the employment and income information although in some cases this information may be noisy or incomplete. In this paper, we focus on the problem of verifying the income assuming that the employment information is correct. In practice, we have observed that a very small percentage (i.e. roughly 1%) of people provide incorrect employment details but nearly 25% state a significantly higher income than their actual verifiable income.  •Secondly, we do not use sensitive information such as social security number, email, phone and the experience level for verifying income. This makes our approach broadly applicable since it can be applied in scenarios where the users are averse to giving out such private information. Of course, this information could help improve our models and in the future we would like to explore the possibility of using private information and even previous employment information to improve our models.    !4 "
374,Towards Rigorous Understanding of Neural Networks via Semantics-preserving Transformations.txt,"In this paper we present an algebraic approach to the precise and global
verification and explanation of Rectifier Neural Networks, a subclass of
Piece-wise Linear Neural Networks (PLNNs), i.e., networks that semantically
represent piece-wise affine functions. Key to our approach is the symbolic
execution of these networks that allows the construction of semantically
equivalent Typed Affine Decision Structures (TADS). Due to their deterministic
and sequential nature, TADS can, similarly to decision trees, be considered as
white-box models and therefore as precise solutions to the model and outcome
explanation problem. TADS are linear algebras which allows one to elegantly
compare Rectifier Networks for equivalence or similarity, both with precise
diagnostic information in case of failure, and to characterize their
classification potential by precisely characterizing the set of inputs that are
specifically classified or the set of inputs where two network-based
classifiers differ. All phenomena are illustrated along a detailed discussion
of a minimal, illustrative example: the continuous XOR function.","Neural networks are perhaps today’s most important ma chine learning models, with exciting results, e.g., in im age recognition [ SZ14], speech recognition [ CSW¸18, BMR¸20] and even in highly complex games [ VBC¸19, BBC¸19,SSS¸17]. As the name suggests, neural net works are learned from data using eﬃcient, but ap proximate training algorithms [ Rud16,KB14]. At their core, neural networks are (dataﬂoworiented) computa tiongraphs[ GBC16]. Theyconsistofmanycomputation units,called neurons,thatarearrangedinlayerssuchthat computationsineachlayercanbeperformedinparallel, with successive layers only depending on the preceding layer. Modern neural networks, in practice, possess up to multiple billions of parameters [ BMR¸20] and leverage parallelhardwaresuchasGPUstoperformcomputations of this scale [ OJ04]. This highly quantitative approach is responsiblefor excitingsuccess stories,but alsofortheir mainweakness: Neuralnetworkbehaviorisoftenchaotic and hard to comprehend for a human. Perhaps most infa mously, a neural network’s prediction can change drasti cally under imperceptible changes to its input, socalled adversarial examples [MMS¸17, GSS14, SZS¸13]. The explainability of neural networks, which are com putationallyconsideredasblackboxesduetotheirhighlyparallel and nonlinear nature, is therefore one of the current core challenges in AI research [ DSB17]. The factthatneuralnetworksareincreasinglyusedinsafety criticalsystemssuchasselfdrivingcars[ BGC¸21]turns trustworthinessofmachinelearningintoamust[ DSB17]. However, stateoftheart explanation technology is more about reassuring intuition, e.g., to support cooperative work of humans with AI systems, such as in the ﬁeld of medical diagnostics [ TG20], than about precise ex planationorguarantees[ LPK21]. Moreover,currentap proaches to Neural Network veriﬁcation are still in their infancy in that they are not yet suﬃciently tailored to the nature of Neural Networks to achieve the required scalability or to provide diagnostic information beyond individual witness traces in cases where the veriﬁcation attempts fail (cf., [ BLJ21,KBD¸17,WZX¸21] and Sec tion 8 for a more detailed discussion). In this paper, we present an algebraic approach to the veriﬁcationandexplanationofRectiﬁerNeuralNetworks (PLNN), a very popular subclass of neural networks that semanticallyrepresentpiecewise aﬃnefunctions(PAF) [MPCB14 ]. Key to our approach are Typed Aﬃne Deci sion Structures (TADS) that concisely represent PAF in a whitebox fashion that is as accessible to human under standing as decision trees. TADS can nicely be derived fromPLNNsviasymbolicexecution[ Cla76,Kin76],or, 1arXiv:2301.08013v2  [cs.NE]  28 Apr 20232 M. Schlüter et al. alternatively, compositionally along the PLNN’s layering structure,andtheiralgebraicstructureallowsforelegant solutions to veriﬁcation and explanation tasks: –TADS can be used for PLNNs similarly as Alge braic Decision Diagrams (ADDs) have been used for Random Forests in [ GS21] to elegantly provide model and outcome explanations as well as class characterizations. –UsingthealgebraicoperationsofTADSonecannot only decide the equivalence problem, i.e., whether two PLNNs are semantically equivalent, but also whether they are 𝜖similar, i.e., never diﬀer more than𝜖. Inbothcases,diagnosticinformationinterms of a corresponding ‘diﬀerence’ TADS is provided thatpreciselyspeciﬁeswhereoneoftheseproperties is violated. –TADS comprise noncontinuous piecewise linear operationswhichcannotberepresentedbyPLNNs. This is necessary to not only deal with regression tasks,whereoneaimsatapproximatingcontinuous functions, but also with classiﬁcation tasks with discrete outputdomains.1Inthe latter case,TADS basedclasscharacterizationallowsonetoprecisely characterize the set of inputs that are classiﬁed as members of a given class, or the set of inputs where two (PLNNbased) classiﬁers diﬀer. –Finally, TADS can also proﬁtably be used for the veriﬁcationofpreconditionsandpostconditions,the illustrationofwhichisbeyondthescopeofthispaper, but will be discussed in [ NSMB23 ] in the setting of digit recognition. ThepaperillustratestheessentialfeaturesofTADSusing a minimal, illustrative example: the continuous XOR function. The simplicity of XOR is ideally suited to provide an intuitive entry into the presented theory. A morecomprehensiveexampleispresentedin[ NSMB23 ], where digit recognition based on the MNIST data base is considered. In this highly dimensional setting, speciﬁc scalability measures are required to apply our TADS technology. AfterspecifyingthedetailsofourrunningexampleinSec tion2,Section3sketchesAlgebraicDecisionStructures that later on will be instantiated with Aﬃne Functions recalled in Section 4 to introduce the central notion of this paper, Typed Aﬃne Decision Structures (TADS). Semantically, TADS represent piecewise aﬃne func tions, which marks them as a ﬁtting representation for 1As PLNNs always represent continuous functions, an additional outcome interpretation mechanism is needed to bridge the gap from continuous networks to discrete classiﬁcation tasks. /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000014/uni00000014/uni00000013/uni00000011/uni00000015/uni00000015/uni00000013/uni00000011/uni00000016/uni00000016/uni00000013/uni00000011/uni00000017/uni00000017/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000011/uni00000019/uni00000019/uni00000013/uni00000011/uni0000001a/uni0000001a/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001c/uni0000001c /uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b Figure 1: AbaselinesolutiontotheXORregressionprob lem given by 𝑓¹𝑥𝑦º=j𝑥"
45,Risk of Stochastic Systems for Temporal Logic Specifications.txt,"The wide availability of data coupled with the computational advances in
artificial intelligence and machine learning promise to enable many future
technologies such as autonomous driving. While there has been a variety of
successful demonstrations of these technologies, critical system failures have
repeatedly been reported. Even if rare, such system failures pose a serious
barrier to adoption without a rigorous risk assessment. This paper presents a
framework for the systematic and rigorous risk verification of systems. We
consider a wide range of system specifications formulated in signal temporal
logic (STL) and model the system as a stochastic process, permitting
discrete-time and continuous-time stochastic processes. We then define the STL
robustness risk as the risk of lacking robustness against failure. This
definition is motivated as system failures are often caused by missing
robustness to modeling errors, system disturbances, and distribution shifts in
the underlying data generating process. Within the definition, we permit
general classes of risk measures and focus on tail risk measures such as the
value-at-risk and the conditional value-at-risk. While the STL robustness risk
is in general hard to compute, we propose the approximate STL robustness risk
as a more tractable notion that upper bounds the STL robustness risk. We show
how the approximate STL robustness risk can accurately be estimated from system
trajectory data. For discrete-time stochastic processes, we show under which
conditions the approximate STL robustness risk can even be computed exactly. We
illustrate our verification algorithm in the autonomous driving simulator CARLA
and show how a least risky controller can be selected among four neural network
lane keeping controllers for five meaningful system specifications.","Over the next decade, large amounts of data will be generated and stored as devices that perceive and control the world become more affordable and available. Impressive demonstrations of data driven and machine learning enabled technologies exist already today, e.g., robotic manipulation [44], solving games [ 55,74], and autonomous driving [ 19]. However, occasionally occurring system failures impede the use of these technologies particularly when system safety is a concern. For instance, neural networks, frequently used for perception and control in autonomous systems, are known to be fragile and nonrobust [ 25,77]. Especially the problem of long tails in training data distributions poses challenges, e.g., natural variations in weather and lightning conditions [61]. Moving forward, we expect that system failures appear less frequently due to advancing tech nologies – nonetheless, algorithms for the systematic and rigorous risk verification of such systems Authors’ address: Lars Lindemann, larsl@seas.upenn.edu; Lejun Jiang, lejunj@seas.upenn.edu; Nikolai Matni, nmatni@seas. upenn.edu; George J. Pappas, pappasg@seas.upenn.edu, University of Pennsylvania, 200 South 33rd Street, Philadelphia, Pennsylvania, USA, 19104.arXiv:2205.14523v2  [eess.SY]  8 Oct 20222 Lindemann, et al. is needed. For instance, the National Transportation Safety Board emphasized in a statement in connection with an Uber accident from 2018 “the need for safety risk management requirements for testing automated vehicles on public roads” [ 12]. In this paper, we show how to reason about the risk of systems that are modeled as stochastic processes. We consider a wide range of system specifications formulated in signal temporal logic (STL) [ 9,51] and present a systematic way to quantify and compute the risk of a system lacking robustness against failure. 1.1 Related Work "
217,RecurJac: An Efficient Recursive Algorithm for Bounding Jacobian Matrix of Neural Networks and Its Applications.txt,"The Jacobian matrix (or the gradient for single-output networks) is directly
related to many important properties of neural networks, such as the function
landscape, stationary points, (local) Lipschitz constants and robustness to
adversarial attacks. In this paper, we propose a recursive algorithm, RecurJac,
to compute both upper and lower bounds for each element in the Jacobian matrix
of a neural network with respect to network's input, and the network can
contain a wide range of activation functions. As a byproduct, we can
efficiently obtain a (local) Lipschitz constant, which plays a crucial role in
neural network robustness verification, as well as the training stability of
GANs. Experiments show that (local) Lipschitz constants produced by our method
is of better quality than previous approaches, thus providing better robustness
verification results. Our algorithm has polynomial time complexity, and its
computation time is reasonable even for relatively large networks.
Additionally, we use our bounds of Jacobian matrix to characterize the
landscape of the neural network, for example, to determine whether there exist
stationary points in a local neighborhood. Source code available at
\url{http://github.com/huanzhang12/RecurJac-Jacobian-bounds}.","Deep neural networks have been successfully applied to many applications, but one of the major criticisms is their being black boxes|no satisfactory explanation of their behavior can be easily oered. Given a neural network fpqwith input x, one fundamental question to ask is: how does a perturbation in the input space aect the output prediction? To formally answer this question and bound the behavior of neural networks, a critical step is to compute the uniform bounds of the Jacobian matrixBfpxq Bxfor allxwithin a certain region. Many recent works on understanding or verifying the behavior of neural networks rely on this quantity. For example, once a (local) Jacobian bound is computed, one can immediately know the radius of a guaranteed \safe region"" in the input space, where no adversarial perturbation can change the output label (Hein and Andriushchenko, 2017; Weng et al., 2018b). This is also referred to as the robustness verication problem . In generative adversarial networks (GANs) (Goodfellow et al., 2014), the training process suers from the gradient vanishing problem and can be very unstable. Adding the Lipschitz constant of the discriminator network as a constraint (Arjovsky, Chintala, and Bottou, 2017; Miyato et al., 2018) or as a regularizer (Gulrajani et al., 2017) signicantly improves the training stability of GANs. For neural networks, the Jacobian matrixBfpxq Bxis also closely related to its Jacobian matrix with respect to the weightsBfpx;Wq BW, whose bound directly characterizes the generalization gap in supervised learning and GANs; see, e.g., Vapnik and Vapnik (1998); Sriperumbudur et al. (2009); Bartlett, Foster, and Telgarsky (2017); Arora and Zhang (2018); Zhang et al. (2018b). How to eciently provide a tight bound for Jacobian (or gradient) is still an open problem for deep neural networks. Samplingbased approaches (Wood and Zhang, 1996; Weng et al., 2018b) cannot provide a certied bound and the computed quantity is usually an underestimation; bounding the norm of Jacobian matrix over the entire domain (i.e. global Lipschitz constant) by the product of operator norms of the weight matrices (Szegedy et al., 2013; Cisse et al., 2017; Elsayed et al., 2018) produces a very loose global upper bound, especially when we are only Work done during internship at Microsoft Research 1arXiv:1810.11783v2  [stat.ML]  27 Feb 201910−3 10−2 10−1 100 Radius of ℓ∞ ball 102 103 104 105 106 107 108 Lipschitz Constant Upper BoundMNIST, 10 layer, LeakyReLU FastLip RecurJacB RecurJacF0 RecurJacF1 Global (naive) Global (ours)Figure 1: RecurJac can obtain local and global Lipschitz constants which are magnitudes better than existing algo rithms. See Experiment section for more details. interested in a small local region of a neural network. Additionally, some recent works focus on computing Lipschitz constant in ReLU networks: Raghunathan, Steinhardt, and Liang (2018) solves a semidenite programming (SDP) problem to give a Lipschitz constant, but its computational cost is high and it only applies to 2layer networks; FastLip by Weng et al. (2018b) can be applied to multilayer ReLU networks but the bound quickly loses its power when the network goes deeper. In this paper, we propose a novel recursive algorithm, dubbed RecurJac , for eciently computing a certied Jacobian bound. Unlike the layerbylayer algorithm (FastLip) for ReLU network in (Weng et al., 2018b), we develop a recursive renement procedure that signicantly outperforms FastLip on ReLU networks, and our algorithm is general enough to be applied to networks with most common activation functions, not limited to ReLU. Our key observation is that the Jacobian bounds of previous layers can be used to reduce the uncertainties of neuron activations in the current layer, and some uncertain neurons can be xed without aecting the nal bound. We can then absorb these xed neurons into the previous layers' weight matrix, which results in bounding Jacobian matrix for another shallower network. This technique can be applied recursively to get a tighter nal bound. Compared with the non recursive algorithm (FastLip), RecurJac increases the computation cost by at most Htimes (His depth of the network), which is reasonable even for relatively large networks. We apply RecurJac to various applications. First, we can investigate the local optimization landscape after obtaining the upper and lower bounds of Jacobian matrix, by guaranteeing that no stationary points exist inside a certain region. Experimental results show that the radius of this region steadily decreases when networks become deeper. Second, RecurJac can nd a local Lipschitz constant, which up to two magnitudes smaller than the stateof theart algorithm without a recursive structure (Figure 1). Finally, we can use RecurJac to evaluate the robustness of neural networks, by giving a certied lower bound within which no adversarial examples can be found. 2 Related Work "
467,Verifying Inverse Model Neural Networks.txt,"Inverse problems exist in a wide variety of physical domains from aerospace
engineering to medical imaging. The goal is to infer the underlying state from
a set of observations. When the forward model that produced the observations is
nonlinear and stochastic, solving the inverse problem is very challenging.
Neural networks are an appealing solution for solving inverse problems as they
can be trained from noisy data and once trained are computationally efficient
to run. However, inverse model neural networks do not have guarantees of
correctness built-in, which makes them unreliable for use in safety and
accuracy-critical contexts. In this work we introduce a method for verifying
the correctness of inverse model neural networks. Our approach is to
overapproximate a nonlinear, stochastic forward model with piecewise linear
constraints and encode both the overapproximate forward model and the neural
network inverse model as a mixed-integer program. We demonstrate this
verification procedure on a real-world airplane fuel gauge case study. The
ability to verify and consequently trust inverse model neural networks allows
their use in a wide variety of contexts, from aerospace to medicine.","Neural networks have been used to solve a variety of nonlinea r inverse problems such as state estimation [20], inverse computational mechanics [2 4], inversemodel controller design [15], medical imaging [12], and seismic reﬂectivity estimation [16]. Neural networks are used because they can learn complex functions f rom data and compute outputs quickly, whereas an analytical inverse modeling ap proach may not work for nonlinear systems, and a samplingbased approach may be com putationally expensive. However, neural network inverse models lack accuracy guara ntees, limiting their use in safety critical applications such as transportation. To enable the adoption of these models, we present a method for verifying the accuracy of an i nverse model neural net work. Our approach uses recent advancements in neural netwo rk veriﬁcation to provide formal guarantees of the correctness of the model over its en tire domain. Inverse problems arise when trying to estimate an unobserve d state from a set of observations, where only the forward model is known (i.e., t he model that maps states to observations). This is in contrast to a ﬁltering setting w here a dynamics model as well as an observation model is often available. Analytical solu tions to inverse problems may not exist for nonlinear systems, and may become intractable when the forward model2 C. Sidrane et al. is highdimensional or stochastic. Samplingbased approx imate inverse models may be slow to query and may not converge to a useable point estimate at all. As a result, data driven approaches may be applied. The forward model is used t o produce a dataset of stateobservation pairs that are then used to train a neural network to output a state for a given observation. Once a neural network inverse model is trained, we want guara ntees that the inverse mapping between observations and states has low error, but t his may be challenging due to the complexity of the forward model and neural network . Prior work [28] has approached this problem using sampling to get stochastic bo unds on the error of an inverse model network. Formal approaches [27] have also bee n applied to 1layer net works where the forward model is replaced with a lookup table so that the Lipschitz constants of the simple network and forward model may be easi ly calculated and the accuracy evaluated at grid points in the domain. In addition to these simplifying assump tions, this approach scales exponentially with the number o f dimensions in the domain, making it unsuitable for largescale problems. We verify ReLUbased inverse model neural networks using mi xedinteger linear programming, a technique that has seen signiﬁcant recent us age for neural network ver iﬁcation [1, 5, 7, 8, 19, 25]. This approach allows formal ver iﬁcation over the entire input domain without a dependence on gridding. Instead of re placing the complex for ward model that maps states to observations with a lookup tab le, we encode it into the mixedinteger linear program, preserving the integrity of the model. This is enabled by overapproximating the nonlinear functions in the forwar d model using a technique adapted from veriﬁcation of neural network control systems [23]. The nonlinear func tions are overapproximated using piecewise linear constra ints, which can be encoded into a mixedinteger linear program, alongside the inverse model neural network. We then maximize the error between the portion of the state that the inverse model re constructs and the original state values. This produces a ve riﬁed upper bound on the estimation error of the inverse model. Contributions Our contribution is to provide a new formal approach to verif ying in verse model neural networks by bringing ideas across discip line boundaries – from reachability to inverse models. Compared to prior work, our work: 1. Can handle multilayer ReLU neural networks. 2. Does not require gridding the state space and incurring th e curse of dimensionality. 3. Can compare an inverse model neural network to the true non linear forward model, rather than a lookup table. 4. Can verify stochastic forward models with additive noise . 5. Employs intelligent parallelization in order to reduce v eriﬁcation time. 6. Demonstrates the approach on a realworld case study of ai rcraft fuel measurement. 2 Related Work "
188,Towards Scalable Verification of Deep Reinforcement Learning.txt,"Deep neural networks (DNNs) have gained significant popularity in recent
years, becoming the state of the art in a variety of domains. In particular,
deep reinforcement learning (DRL) has recently been employed to train DNNs that
realize control policies for various types of real-world systems. In this work,
we present the whiRL 2.0 tool, which implements a new approach for verifying
complex properties of interest for DRL systems. To demonstrate the benefits of
whiRL 2.0, we apply it to case studies from the communication networks domain
that have recently been used to motivate formal verification of DRL systems,
and which exhibit characteristics that are conducive for scalable verification.
We propose techniques for performing k-induction and semi-automated invariant
inference on such systems, and leverage these techniques for proving safety and
liveness properties that were previously impossible to verify due to the
scalability barriers of prior approaches. Furthermore, we show how our proposed
techniques provide insights into the inner workings and the generalizability of
DRL systems. whiRL 2.0 is publicly available online.","In recent years, deep neural networks (DNNs) [23] have become highly popular due to their ability to produce state of theart results in multiple ﬁelds, e.g., image recognition [34], text classiﬁcation [37], game playing [45], and many oth ers [7]. DNNs used in such contexts have been shown to suc cessfully learn, by training on data, a model that generalizes to previously unseen inputs. In particular, deep reinforcement learning (DRL ) [40] has been recently used to train DNNs to learn control policies for complex computer and networke d systems, surpassing the stateoftheart in a variety of ap plica tion domains, including database management [60], compile r optimization [41], congestion control [27], [39] on the Int ernet, routing [53], computeresource scheduling [9], [42], adap tive video streaming [38], [43], and many more. Despite the overwhelming success of DNNs, many safety issues pertaining to them have been identiﬁed [22], [51], demonstrating that although DNN models potentially yield excellent performance, they also suffer from many weakness es. For instance, it has been shown that DNNs can be manipulated into performing severe errors through only slight distorti ons to their inputs [17]. This phenomenon, called adversarial perturbations , plagues effectively all modern DNNs. Adversarial perturbations, alongside other safety and sec u rity vulnerabilities, have brought about a surge of interes t in formally verifying the correctness of DNNs. A plethora of [*] This is the extended version of a paper with the same title from the FMCAD 2021 conference. See https://fmcad.org/approaches for DNN veriﬁcation have been proposed in recent years (e.g., [19], [25], [30], [55]). Unfortunately, in gen eral, all proposed tools face signiﬁcant scalability barriers, w hich render them unable to verify stateoftheart, industrial DNNs with millions of parameters. Furthermore, even when applie d to small DNNs, these tools are often restricted to verifying simplistic properties. The scalability challenge is furth er ag gravated in the DRL context, which involves sequential DNN informed decision making, and so reasoning about repeated invocations of the DNN, where the outcome of one invocation can inﬂuence the input to the DNN in subsequent invocations. Consequently, the applicability of recently introduced DN N veriﬁcation tools to complex properties and systems of prac  tical interest remains extremely limited. To begin bridging this gap, we previously introduced a tool called whiRL 1.0 [16], which enables verifying certain safety and liveness properties, or identifying violations , for practical DRL systems. We demonstrated whiRL 1.0 ’s use fulness by verifying properties of interest for three syste ms from the communication networking domain. We identiﬁed such systems to be prime candidates for veriﬁcation for two main reasons: ﬁrst, stateoftheart DNNs in this domain te nd to be of moderate sizes, which are within reach of existing veriﬁcation technology; and second, meaningful and comple x speciﬁcations can be formulated and veriﬁed because the inputs for these systems are carefully handcrafted and reﬂe ct important semantic meaning (as opposed to raw pixel data in computer vision applications, for example). whiRL 1.0 , which combines DNN veriﬁcation techniques with bounded model checking, uses a blackbox DNN veriﬁcation engine as a backend, and can thus beneﬁt from any future improvements to DNN veriﬁcation technology. As exempliﬁed by our promising initial results in [16], whiRL 1.0 constituted a ﬁrst step towards enhancing the reliability of DRL systems. Still, whiRL 1.0 had severe limitations: most notably, al though it successfully generated violations of desired pro per ties, it was incapable of proving that properties of practic al signiﬁcance held without making very strong assumptions, e.g., that runs of the considered system terminate within a v ery small number of steps. However, the executions of realworl d systems are often inﬁnite, or ﬁnite but consisting of many steps. In such scenarios, whiRL 1.0 and other DRL veriﬁcation tools are unable to prove that most relevant properties hold . In this work, we present whiRL 2.0 [1] — a veriﬁcation engine for DRL systems. whiRL 2.0 signiﬁcantly extends the capabilities of the original whiRL 1.0 tool to accommodate2 verifying complex properties. In particular, while whiRL 1.0 was limited to verifying basic safety properties, whiRL 2.0 utilizes kinduction techniques for proving both safety and liveness properties of DRL systems. In addition, whiRL 2.0 uses invariant inference techniques to quickly prove properties that could otherwise be quite difﬁcult to verify. whiRL 2.0 also incorporates abstraction methods for providing some visibility into the DRL system’s operation. We demonstrate the effec tiveness of these techniques by revisiting the three case st udies involving stateoftheart DRL systems to which whiRL 1.0 has been applied in [16]: the Aurora [27] Internet congestion controller, the Pensieve [43] adaptive video streamer, and the DeepRM [42] compute resource scheduler. We are able to prove various properties of these systems that, to the best o f our knowledge, were beyond the reach of prior stateofthe art tools, including the original whiRL 1.0 tool. The rest of this paper is organized as follows. Section II covers basic background on DNNs, DRL systems, and DNN veriﬁcation. Next, in Section III we present our whiRL 2.0 ver iﬁcation tool, and describe its novelties and main componen ts. We present whiRL 2.0 ’s semiautomated invariant inference in Section IV, and discuss the tool’s implementation in Sectio n V. Our case studies are described in Section VI, followed by related work in Section VII. We conclude in Section VIII. II. B ACKGROUND A. Deep Neural Networks and Deep Reinforcement Learning A deep neural network (DNN) [23] is a directed graph, where the nodes (also called neurons) are organized in layer s. In feedforward DNNs, data ﬂows from the ﬁrst ( input ) layer, onto a sequence of intermediate ( hidden ) layers, and ﬁnally into a ﬁnal ( output ) layer. The network is evaluated by as signing values to the input layer’s neurons, and then iterat ively computing the assignment of each of the hidden layers, until reaching the output layer and returning its evaluation to th e user. More speciﬁcally, the value of each neuron in the hidden and output layers is computed using the values of neurons in the preceding layer. Each such layer has a type, which determines the exact way in which its neuron values are computed. One common layer type is the weighted sum layer, in which each neuron is computed as an afﬁne combination of the values of neurons in the preceding layer, based on edge weights and bias values determined as part of the DNN’s training process. Another popular layer type is the rectiﬁed linear unit (ReLU ) layer, where each node yis connected to a single nodexfrom the preceding layer, and its value is computed byy=ReLU(x) = max(0 ,x). In this paper we will focus on weighted sum and ReLU layers, although there exist many additional layer types, such as maxpooling and hyperbolic tangent , to which our technique may be extended. Fig. 1 depicts a toy DNN comprising an input layer with two neurons, followed by a weighted sum layer and a ReLU layer. For input V1= [1,3]T, the second layer’s computed values areV2= [18,−3]T. In the third layer, the ReLU functions are applied, resulting in V3= [18,0]T. Finally, the network’s single output is V4= [54] .v1 1 v2 1v1 2 v2 2v1 3 v2 3v1 42 −4 5 1ReLU ReLU3 −1+1 −2Weighted sumReLU Input Output Fig. 1: A toy DNN. The values above the edges are weights, and t he values below the vertices are biases. Formally, a DNN Nthat receives kinputs and returns n outputs is a mapping Rk→Rn. The DNN consists of a sequence of mlayersL1,...,L m, whereL1is the input layer andLmis the output layer. We use sito denote layer Li’s size, andv1 i,...,vsi ito denote Li’s individual neurons. We refer to the column vector [v1 i,...,vsi i]TasVi. During evaluation, the input values V1are fed to the network’s input layer, and V2,...,V nare computed iteratively. Each weighted sum layer Lihas a weight matrix Wiof dimensions si×si−1and a bias vector Biof sizesi. These WiandBiare set at training time, and determine how Vi is computed: Vi=Wi·Vi−1+Bi. For a ReLU layer Li, the values of Viare computed by applying the ReLU to each individual neuron in its preceding layer: vj i=ReLU(vj i−1). Indeep reinforcement learning (DRL ) [40], a DNN, called theagent , learns a policyπ, which maps each possible observed environment state sto an actiona. During training, at each discrete timestep t∈0,1,2..., arewardrtis displayed to the agent, based on the action atit chose to perform after observing the environment’s state at that time st. This reward is used for tuning the agent DNN’s weights. The DNNs produced using DRL fall within the same general architectur e described above; the difference lies in the training proces s, which is aimed at generating a DNN that computes a mapping πthat maximizes the expected cumulative discounted return Rt=E/bracketleftbig/summationtext tγt·rt/bracketrightbig . The discount factor ,γ∈/bracketleftbig 0,1/parenrightbig , controls the effect that past decisions have on the total expected rew ard. B. Veriﬁcation of Deep Neural Networks A DNN veriﬁcation query typically includes a DNN N, a precondition PonN’s input, and a postcondition Qon N’s output [28]. The veriﬁcation algorithm’s goal is to ﬁnd a concrete input x0such that P(x0)∧Q(N(x0))(theSAT case), or prove that no such x0exists (the UNSAT case). Typically, we use the precondition Pto express some states of the environment that the network might encounter, and use the postcondition Qto encode the negation of the behavior we would like Nto exhibit in these states. Thus, when the veriﬁcation algorithm returns UNSAT , this implies that the desired property always holds. Conversely, a SAT result indicates that the desired property does not always hold, an d this is demonstrated by the discovered counterexample x0. For example, observe the toy DNN in Fig. 1, and suppose we wish to verify that the DNN’s output is strictly larger tha n 5, for any input, i.e., for any x=/an}bracketle{tv1 1,v2 1/an}bracketri}ht, it holds that N(x) = v1 4>5. This is encoded as a veriﬁcation query by choosing a precondition which does not restrict the input, i.e., P=3 (true), and by setting Q= (v1 4≤5), which is the negation of our desired property. For this veriﬁcation query, a sound veriﬁer will return SAT, and a feasible counterexample such asx=/an}bracketle{t0,−1/an}bracketri}ht, which produces v1 4= 0≤5. Hence, the property does not hold for this DNN. Verifying DRL Systems. Beyond the general challenges of verifying DNNs (most notably, scalability), verifying DRL systems involves additional challenges. These challenges stem from the fact that DRL agents typically run within reactive systems, and are invoked multiple times, with the inputs to each invocation usually affected by the outputs of previous invocations. This means that (i) the speciﬁcations for DRL systems need to account for multiple invocations; and (ii) t he scalability issue is aggravated, because the veriﬁer needs to consider multiple consecutive invocations of the network, which is akin to considering a signiﬁcantly larger DNN. While attempts have been made to develop tools tailored for DRL system veriﬁcation (e.g., [16], [32], [44]), two import ant challenges have yet to be addressed. First, existing veriﬁc a tion approaches for DRL systems have focused on refuting properties, and not on proving that they hold; and second, existing approaches were not geared towards verifying reac tive systems. As part of the whiRL project, we make an initial attempt at addressing these two challenges. III. whiRL 2.0 Our contribution in this paper is the whiRL 2.0 veriﬁcation tool, which signiﬁcantly extends our existing DRL veriﬁcat ion engine, whiRL 1.0 . The whiRL 2.0 tool allows to verify complex queries on DRL systems, which were previously beyond our reach. Speciﬁcally, it supports the veriﬁcation of safety and liveness properties of DRL systems using a kinduction based approach. Additionally, it incorporates in variant inference techniques, which facilitate the veriﬁcation of complex safety properties. whiRL 2.0 uses an underlying veriﬁcation engine as a blackbox, and is hence compatible with many existing DNN veriﬁers. Formalizing DRL Agents. DRL agents typically operate within reactive systems: they process a (possibly inﬁnite) sequence of states, each representing a current snapshot of the environment observed by the agent. Each state is obtaine d from its predecessor by triggering the action outputted by t he DRL agent, and allowing the environment to react. In line with the formulation proposed in [16], we formalize the DRL veriﬁcation problem by encoding the DRL system, as well as its environment, into a transition system T=/an}bracketle{tS,I,T/an}bracketri}ht. Each state s∈Sin this transition system is a snapshot of the current observable environment; these states correspond t o the inputs of the DNN agent. We use I⊆Sto denote the set of initial states. The transition relation, T⊆S×S, is deﬁned such that/an}bracketle{txi,xj/an}bracketri}ht∈Tiff the system can transition from state xito statexj; i.e., when the DNN is presented with state xi, it selects some action, to which the environment can respond in a way that leads the system to state xj. Although the DNN is deterministic, the environment is not necessarily so, an d so Tneed not be deterministic. An execution of the system is deﬁned as a sequence of states x1,...,x n, such that x1∈I,and for all 1≤i≤n−1it holds that T(xi,xi+1). The process of encoding a DRL system as a transition system is supported bywhiRL 1.0 , via constructs for representing features common to DRL systems (e.g., inputs in the form of a “sliding window” over the recent history of observations) [16]. Example. As a running example, we focus on the Aurora DRL system [27], which implements a congestion control policy. In today’s Internet, different services (e.g., video streami ng like Netﬂix and Amazon, V oIP services such as Skype) contend over the same network bandwidth, with aggregate demand for bandwidth often exceeding the available supply. If Interne t trafﬁc sources do not pace the rates at which their data is injected into the network, the network will become congeste d, resulting in data being lost or delayed, and, consequently, in bad user experience and even global Internet outages. Con gestion control is the task of determining, for each individ ual Internet trafﬁc source, how quickly its trafﬁc should be inj ected into the network at any given point in time. Congestion contr ol is thus a both fundamental and timely networking challenge. Recently, researchers have proposed employing DRL for this purpose, and presented the Aurora congestion con troller [27]. An Auroracontrolled trafﬁc source uses a DNN to select the next rate at which to send trafﬁc, based on observations regarding the implications of its past choice s of sending rates. Speciﬁcally, Aurora’s inputs are tvectors v−t,...,v −1, containing performancerelated statistics per taining to the sender’s most recent tratechange decisions. These incorporate information about what fraction of sent d ata packets were lost following each rate selection, how long it took the sent packets to reach the trafﬁc’s destination, etc . The DNN’s output determines whether the current rate should be increased, kept steady, or decreased. Changing the sending rate can potentially affect the environment, e.g., an increase t o the rate might lead to packet loss if the new rate exceeds network capacity. These changes to the environment, in turn, affect the future inputs to the DNN. See [27] for additional details. In the formulation of Aurora as a veriﬁcation challenge in [16], each state, which corresponds to a possible input to Aurora’s DNN, is represented by a ttuple of statistics vectors. The state also contains the DNN’s (deterministic) output fo r the input it represents. This is required for deﬁning good an d bad states, as will be discussed later. Congestion controll ers are expected to converge to “good” rate decisions from any starting point. Hence, we let the set of initial states be the set of all states. Recall that the input to the DNN represents a sliding window over tlong histories of statistics vectors. Thus, for each two consecutive states, s1T→s2, it holds that s2is obtained from s1by augmenting the vectors in s1with a statistics vector associated with the DNN’s rate change at states1, and discarding the vector in s1corresponding to the least recent of the tprior rate changes. DRL System Speciﬁcations. Once the DRL system is formu lated as a transition system, we can specify safety and liven ess properties [11] that it should uphold. Safety properties indicate that the system never displays unwanted behavior, and these are often formulated through a predicate PB(s)that returns true iffs∈Sis a bad state, i.e., a state in which the property is violated. The safety veriﬁcation problem then boils down to4 determining whether there is a reachable bad state in T[4]. Liveness properties indicate that the system eventually displays desirable behavior, and these are often formulated through a predicate PG(s)that returns true iff s∈Sis a good state, i.e., a state in which the property is fulﬁlled. Verifying a livene ss property is performed by checking that there are no inﬁnite sequences of consecutive states in which only ﬁnitely many o f the states are good [4]. For instance, a natural safety prope rty with respect to Aurora is that when Aurora observes excellen t network conditions (no packet loss, closetominimum pack et delays), as reﬂected by the statistics vectors fed to the DNN , the DRL agent does not advise to decrease the sending rate in thenext timestep . An example of a liveness property in this setting is that if excellent network conditions persist, Au rora should always eventually increase the sending rate. KInduction. Proving that safety or liveness properties hold (or ﬁnding counterexamples) involves traversing large tr an sition system graphs. For modern DRL systems, this is often infeasible, in particular because the rich environments in which these systems operate can react in many ways after each actio n taken by the agent, resulting in high (or even inﬁnite) out degrees for many states. In whiRL 1.0 , this issue was addressed through the application of bounded model checking (BMC), an approach that explores only a small fraction of the transiti on system graph, namely, states within a kstep distance from an initial state. BMC can ﬁnd safety and liveness violations (i f they are reachable within ksteps) as depicted in Fig. 2, but cannot prove the absence of such violations. 0 1 2 3 46 0 1 2 3 k= 1step7 6 0 1 2 k= 2steps8 7 6 0 1 k= 3stepsBad StateBad State Fig. 2: BMC searches for violations of a safety property. Eac h vector represents a state, and encodes the statistics that Aurora o bserved in the past t= 5 timesteps. The unwanted state is surrounded by a red rectangle, and is reachable only after k= 3 steps from the initial state. Note that consecutive states have shared inp uts shifted, and each timestep sample is depicted in a different color. InwhiRL 2.0 , we address this important gap by adding the means for proving that safety and liveness properties hold. To this end, we employ the method of kinduction [11]. Intuitively, the idea in kinduction is to look for state sequences of length k, which can start from arbitrary states inT(not necessarily from initial states), and for which the property is violated. If a violating execution exists, it mu st contain an indicative klong sequence of steps — a sufﬁx of the execution that ends in the bad state for safety propertie s, or a sequence of nongood states for liveness properties. Thus , if a veriﬁer ﬁnds that a kinduction query is UNSAT , we know that the corresponding property holds. If, however, it returns SAT with a counterexample that does not start at an initial stat e, we cannot conclude whether the property holds, and must increa sekin search of a conclusive answer. Fig. 3 depicts a snapshot of the kinduction process used for proving a safety propert y. 0 1 2 3 46 0 1 2 3...4 2 7 1 56 4 2 7 1 3 4 2 7 17 3 4 2 7Bad State(k+1)steps k steps(k+2)steps Fig. 3: Using kinduction to prove a safety property, i.e., t hat the system never reaches the bad state (surrounded by a red recta ngle). Although there are klong and (k+1)long execution sequences that end in the bad state, there is no such sequence of length (k+2); and due to this and to BMC on the base cases, the property holds. More formally, following the terminology in [4], verifying ωregular liveness properties is reducible to checking pers is tence properties of the form ”eventually forever B”, where Brepresents a “bad” state ( ∃s s.t. B =¬PG(s)). Using k induction in the spirit of [6], [54], we can rule out the exist ence ofklong sequences of bad states for a given k(even ones not starting at an initial state). This is performed by formulat ing the following query: ∃x1,x2,...,x k./parenleftBigk−1/logicalanddisplay i=1T(xi,xi+1)/parenrightBig ∧/parenleftBigk/logicalanddisplay i=1¬PG(xi)/parenrightBig for increasingly large values of k. As soon as one such query returnsUNSAT , we are guaranteed that the liveness property holds. A similar encoding can be used for proving safety properties. We note that realizing kinduction in our casestudies en tailed contending with challenges such as the need to encode veriﬁcation queries that capture the systemenvironment i n teraction from any(possibly noninitial) state. An additional challenge was scalability; duplicating the network to enco de ksteps can induce an exponential blowup in running time. whiRL 2.0 curtails the search space by using bound tightening mechanisms, and by enforcing certain dependencies between the inputs to the kduplicate networks encoded as part of a k induction query. Speciﬁcally, these kinputs typically represent thekrecent observations of the agent’s environment, and can be restricted by requiring them to constitute a “sliding window”: each pair of consecutive inputs must agree on the k−1previous observations that appear in both inputs. BMC and kinduction are related techniques; the former is geared towards refuting a property, and the latter is gear ed towards proving it. In whiRL 2.0 , we take a portfolio approach, as depicted in Fig. 4: we alternate between BMC and k induction queries, until we: (i) refute the property (BMC returnsSAT); or (ii) prove the property ( kinduction returns UNSAT ); or (iii) hit a timeout threshold. When steps 1 and 25 both fail, we increment kby1and repeat the process. Thus, although we do not know in advance whether the property in question holds, we hope that one of the two techniques will either ﬁnd a counterexample or prove the property. verification schema K++KInduction BMC SAT UNSATUNSAT SAT Fig. 4: whiRL 2.0 ’s veriﬁcation schema. Abstraction. In computer networking systems, such as the Aurora congestion controller, the system’s state is often a set of observations about the environment. Through close inspect ion of our considered casestudies, we observe that occasional ly some of the input ﬁelds are irrelevant to the property being checked, in the sense that the property can be proved even when disregarding them. We thus integrate into whiRL 2.0 abstraction capabilities [10] — the ability to strip off irrelevant input ﬁelds, as indicated by the user, when dispatching a veriﬁcation query. The original transition system Tis thus changed into an abstract transition system, T′, which over approximates the original one. Speciﬁcally, the states of T′ are symbolic, each corresponding to multiple states of T; and s′ 1T′ →s′ 2if and only if some states s1ands2, to which s′ 1 ands′ 2correspond, satisfy s1T→s2. If the veriﬁcation engine concludes that the property holds for T′(i.e., the negation of the property is UNSAT ), it follows that it also holds for the originalT. However, a counterexample for T′may be spurious, as it may not be valid for T, in which case the original query may need to be solved to obtain a deﬁnite resul t. For example, in Aurora, the DNN input represents performancerelated statistics pertaining to the tmost recent rate adjustments made by the sender. In Aurora’s implemen tation used for our evaluation, we chose t= 10 (as in [27]). In this context, abstraction might expose, for instance, th at a certain property holds regardless of what values are assign ed to the ﬁelds not relating to the 5most recent rate changes, indicating that the policy is, in essence, dependent only on the5most recently observed statistics vectors. We leverage the fact that inputs to recentlyproposed com puter networked systems consist of fairly few ﬁelds with natural semantic meaning, thus leading to a limited number of actual combinations of input ﬁelds that are abstracted. In Section VI we demonstrate how whiRL 2.0 ’s abstraction capabilities can shed light on the inner workings of the veri ﬁed system, rendering the “blackbox” policy learned by the DRL system somewhat more translucent.IV. I NVARIANT INFERENCE Verifying DRL systems is difﬁcult, as one must often reason about transitions across many states to establish that a pro perty holds. BMC and kinduction can mitigate this issue to some extent, but sometimes this is not enough. To further boost th e scalability of whiRL 2.0 , we enhanced it with semiautomated invariant inference capabilities. In the context of safety veriﬁcation of a transition system graph, an invariant can be regarded as a partition of the state space Sinto two disjoint sets, S1andS2, such that no transition leads from one set to the other: s1∈S1∧s2∈S2⇒ /an}bracketle{ts1,s2/an}bracketri}ht/∈T. Invariants are useful if we know that I⊆S1(all initial states are in S1) andPB(s)⇒s∈S2(all bad states are inS2). In this case, the existence of the invariant immediately guarantees that no bad states are reachable. Unfortunately , discovering such useful invariants is known to be undecidab le in general, and very difﬁcult to accomplish in practice [46] . As part of whiRL 2.0 , we propose a heuristic for semi automated invariant inference, which leverages common tra its of communication networking systems. More precisely, we observe that many relevant properties in these systems can be regarded as Boolean monotonic functions ; they tend to be satisﬁable when the DNN’s input vectors are allowed to ﬂuc tuate extensively, but quickly become unsatisﬁable when th ese input vectors are restricted. Often, ﬁnding the tipping poi nt, i.e., the minimal input restrictions that cause the propert y to shift fromSAT toUNSAT , constitutes an invariant that is useful for proving other properties, and which can also render the policy learned by the DNN more translucent to humans. We demonstrate these notions on the Aurora congestion controller. Recall that Aurora’s output indicates whether the sending rate should be increased, maintained, or decreased . whiRL 2.0 can search for an invariant that translates to the range of inputs for which the DNN outputs that the sending rate should be decreased. Such an invariant can assist in the veriﬁcation of complex properties, and provide human engineers with comprehensible insights into the DRL system . Technically, whiRL 2.0 allows the user to specify the output property and mark the relevant input ﬁelds. For example, in Aurora’s case, “the sending rate should be decreased” as the output property, and a subset of the input statistics as the relevant ﬁelds. Then begins a binary search on the range of the inputs in order to ﬁnd the minimal restrictions that rend er the veriﬁcation query UNSAT . At each step of the binary search, we invoke a blackbox veriﬁcation procedure to solv e the resulting query. This allows us to locate the tipping poi nt up to a prescribed precision. whiRL 2.0 has builtin templates for input and output restrictions, which can be regarded as different strategies for conducting the aforementioned bi nary search. Each template takes into account either the DRL system’s input variables or output variables, and controls them by adjusting their bounds; tightening them to “push” the que ry towards the UNSAT region. Currently, these templates include (i) for a ﬁxed output, tightening or loosening the bounds of the speciﬁed input variables, executing binary search unti l the point in which the query switches from SAT toUNSAT is discovered; and (ii) performing a similar operation, but th is6 time on the bounds of the speciﬁed output variables, while ﬁxing the inputs according to userspeciﬁed constants. Fig. 5 illustrates an invariant search procedure. In this procedure, we have a candidate invariant (the middle blue li ne) IB ✗ Fig. 5: Invariant search procedure. The initial states are the green square labeled I, and the bad states are the red square labeled B.that splits the search space into two parts. Ideally, the reachable states should all be on one side of the par tition, and the bad states on the other side. Our bi nary search automatically adjusts the invariant can didate. In case an initial invariant candidate is too strong (there are reachable states on both sides), it is weakened, and the line is moved towards B. If, however, the initial invariant candidate is too weak (there are bad state s on both sides), it is strengthened, and the line is moved toward s I. Both kinds of adjustments are performed by tightening or loosening the bounds on the input or output variables. V. I MPLEMENTATION We implemented whiRL 2.0 as a Python framework that pro vides general functionality for verifying DRL systems. whiRL 2.0uses Marabou [31], a stateoftheart SMTbased [5], [12], [14] DNN veriﬁer, as a backend (although other veriﬁers coul d also be used). whiRL 2.0 includes the following key modules, which did not exist in whiRL 1.0 : 1)KInduction Query Veriﬁer. A module that allows the user to generate kinduction queries. The module can encode either a safety property or a liveness property, speciﬁed by their PB(s)andPG(s)predicates, respectively. 2)Invariant Finder. A module through which a user can instruct whiRL 2.0 to search for an invariant. The user needs to provide the postcondition Q, and mark the variables to focus on. whiRL 2.0 then performs the previously described semiautomated search procedure, and returns within the speciﬁed parameters a range for which the invariant holds, if such a range is found. 3)Input Abstraction. A module that allows the user to specify, for a given veriﬁcation query, which input ﬁelds should be abstracted. When abstraction is applied, whiRL 2.0will either return UNSAT (if the abstract query returns UNSAT ), or default to the original query if the abstract query returns a spurious counterexample. Additionally, whiRL 2.0 retains some of whiRL 1.0 ’s function ality, most notably its DNN loading interfaces and bounded model checking capabilities. The code for whiRL 2.0 , along side documentation and the experiments described in the pap er, are all available online under a permissive license [1]. VI. C ASE STUDIES We evaluate whiRL 2.0 on three case studies of DRL sys tems: the Aurora [27] congestion controller, the Pensieve [43] adaptive video streamer, and the DeepRM [42] compute re source scheduler. All three case studies, which were used to illustrate the power of whiRL 1.0 in [16], are from theTABLE I: whiRL 2.0 features used in each case study. Aurora Pensieve DeepRM KInduction ✓ ✓ ✗ Bounded Model Checking ✓ ✓ ✓ Invariant ✓ ✗ ✓ Abstraction ✗ ✓ ✓ domain of communication networks1. We have identiﬁed such DRL systems as highly suitable candidates for evaluating DR L system veriﬁcation techniques as they achieve stateofth eart results despite being of moderate sizes, rendering veriﬁca tion tractable. Table I summarizes the whiRL 2.0 capabilities ap plied in each case study. All experiments were conducted on a n HP EliteDesk machine with six Intel i5−8500 cores running at3.00GHz, and with a 32GB memory. A. The Aurora Congestion Controller Aurora [27] is a stateoftheart DRL system that acts as a congestion controller for data transmission [27]. Aurora receives an input vector of size 3t, which consists of obser vations from the previous ttimesteps. Speciﬁcally, the input consists of 3distinct values representing performancerelated statistics for each of the previous trate changes outputted by the DNN: (i) latency gradient : the derivative of latency (packet delays) across time, as measured by the sender, following a change to the rate; (ii) latency ratio : the ratio of the average latency experienced by the sender, following a change to the rate, to the minimum past latency experienced. This value is never smaller than 1; and (iii) sending ratio : the ratio of the rate at which packets are injected into the network by the sender (i.e., the sending rate), to the rate at which the sent packets arrive at the receiver. We note that the latter rate c an be strictly lower than the former rate if the network is congest ed, which can lead to sent packets being forced to wait in in network buffers, or being dropped along the way. The sending ratio is never smaller than 1. Intuitively, simultaneous low latency gradient, latency ratio, and sending ratio are indi cative of excellent network conditions. Aurora has a single output value, which indicates whether the sending rate should be increased (positive output), decreased (negative output) , or maintained (output is zero). When network conditions are good (low latency, no packet loss), this in indicative of the current rate not overshooting the network bandwidth. Hence , we expect the sending rate to increase so as to take over available bandwidth. In contrast, when network conditions are poor (high latency, high packet loss), this is indicative of network congestion, and so we expect Aurora to decrease the rate. See [16], [27] for additional details. In line with previous work [16], [27], we set t= 10 , i.e., the input size to Aurora’s DNN is of size 3t= 30 . Aurora’s DNN has a single hidden ReLU layer with 48neurons, and a single neuron in its output layer. Proving Liveness. In our previous work [16], two liveness properties of Aurora were formulated, but could not be veriﬁ ed using whiRL 1.0 . Using whiRL 2.0 , we successfully proved that both properties from [16] always hold. Details follow. 1For a thorough formulation of the properties, see Appendix A .7 •Property 1: excellent network conditions eventually imply rate increase. When Aurora observes a history of excellent network conditions (low latency, no packet loss) , the DRL system should eventually increase the sending rate, i.e., eventually output positive values. Using whiRL 2.0 ’s kinduction capabilities, we successfully proved that this property, as formulated in [16], indeed holds for any inﬁnit e run. The property was successfully proved, within a few seconds, for k= 2. •Property 2: poor network conditions eventually imply rate decrease. Symmetrically to property 1, when Aurora observes a history of poor network conditions, the DRL system should eventually decrease the sending rate by outputting negative values. By performing kinduction with k= 5, we proved that this property, as formulated in [16], indeed holds for all inﬁnite executions. This query took approximately 4.5hours to solve. SemiAutomatic Invariance Inference. Next, we used whiRL 2.0’s invariant inference capabilities to ﬁnd invariants for proving safety properties of Aurora. •Invariant A: bounding the nextstep decrease in sending rate for excellent network conditions. When Aurora ob serves a history of excellent network conditions (low laten cy, no packet loss), the DRL agent’s output should be non negative, i.e., should not imply a decrease to the sending rate. This safety property was shown to be violated in previous work [16]. Here, we utilize whiRL 2.0 ’s invariance inference techniques to prove a bound on this (undesirable) nextstep decrease in sending rate, to provide visibility i nto the performance of the DRL system. whiRL 2.0 ’s method for producing the desired invariant appears in Alg. 1. The algorithm takes two user inputs: the latency slack ǫ, and the precision η. Theǫinput captures the notion of “excellent network conditions” encoded as inputs to the DNN: the observed latency gradient is restricted to the range [−ǫ,ǫ]; and the observed latency ratio is restricted to the range [ 1,1 +ǫ]. Additionally, the sending ratio is set to1(indicating that sent trafﬁc arrives at the receiver without being delayed or dropped within the network). The algorithm now performs a binary search over the DNN’s output space (leaving the prescribed input ranges for the DNN ﬁxed). Speciﬁcally, the ηinput speciﬁes the desired precision: the output of the algorithm will be an upper boundbon the DNN’s output, such that the output bis impossible, but b+ηis possible, given the aforementioned input restrictions. Recall that the upper bound brelates to the negation of the desired property, and so an upper bound of b implies that Aurora’s DNN will never decrease the sending rate bybor more when network conditions are excellent. This procedure terminates within a few seconds, returning a n upper bound on the input for which the DNN veriﬁer returns UNSAT . The algorithm’s correctness immediately follows from the underlying veriﬁer’s soundness. •Invariant B: inferring when Aurora fails to decrease the nextstep sending rate even though network conditions are poor. We now wish to characterize poor network conditions in which Aurora does not decrease its sending rate, as expected of it. The procedure is described in Alg. 2.Algorithm 1 Finding Invariant A Input:ǫ,η//latency slack, precision Output: UBUNSAT //worstcase output decrease bound 1:UBUNSAT←−∞ //−M, for some large constant M 2:UBSAT←0 3:QUERY←DNN VERIFY (ǫ, output≤0 ) 4:while (|UBSAT−UBUNSAT|≥η)do 5:OUTUPPER←1 2(UBUNSAT +UBSAT) 6:QUERY←DNN VERIFY (ǫ, output≤OUTUPPER ) 7: ifQUERY isSAT thenUBSAT←OUTUPPER 8: ifQUERY isUNSAT thenUBUNSAT←OUTUPPER 9:returnUBUNSAT Now, the sending ratio is not ﬁxed to 1, but is rather within the range [ 1,P], for a userspeciﬁed Pvalue. P represents a userprovided upper bound on ratio of the rate at which packets leave the sender (i.e., the sending rate) to the rate which these packets arrive at the receiver. For a slack ǫ, the procedure again restricts the latency gradient to the range [ −ǫ,ǫ] and the latency ratio to the range [1,1+ǫ]. Intuitively, setting low values for ǫwhile allowing sending ratios to be high corresponds to sending trafﬁc across communication networks in which innetwork buffers are very shallow. In such networks, packets cannot accumulate within the network, resulting in low latencies for packet delivery. However, since innetwork buffers are shallow, packets are dropped once network bandwidth is even slightly exceeded, resulting in high sending ratios when the sending rate signiﬁcantly overshoots the network’ s capacity (and many packets are lost). The algorithm ﬁxes the output’s lower bound to be non negative, and executes a binary search on the input sending ratio. Speciﬁcally, the algorithm returns, for any userch osen value P, a lower bound ( LBUNSAT ) such that Aurora always decreases the sending rate when its observations regarding past sending ratios all lie within the range [LBUNSAT,P]. whiRL 2.0 ﬁnds the invariant within a few seconds. Algorithm 2 Finding Invariant B Input: P≥2//upper bound on the sending ratio Output: LBUNSAT //worstcase sending ratio bound 1:LBSAT,SRLOWER←1 2:LBUNSAT ,SRUPPER←P 3:QUERY←DNN VERIFY (ǫ, output≥0,SRLOWER , SRUPPER ) 4:while (LBSAT+ 1< LBUNSAT )do 5:SRLOWER←1 2(LBSAT +LBUNSAT ) 6:QUERY←DNN VERIFY (ǫ, output≥0,SRLOWER , SRUPPER ) 7: ifQUERY isSAT thenLBSAT←SRLOWER 8: ifQUERY isUNSAT thenLBUNSAT←SRLOWER 9:returnLBUNSAT Observing the bounds produced by Alg. 2 yielded surpris ing insights regarding the decisionmaking policy learned by Aurora. Speciﬁcally, to gain insight into what our discover ed invariants reveal regarding the policies, we created multi ple8 instances of Aurora agents, and trained them all on the same training data until achieving an averaged reward value simi lar to that of the original Aurora controller [27]. We then obser ved that for some of the Aurora instances, the discovered invari  ants depended only on the proportion between the sending ratio’s lower bound ( SRLOWER ) and upper bound ( SRUPPER ), as opposed to their absolute values. Speciﬁcally, for violating counterexamples (inputs to Aurora’s DNN) produced for these instances, the ratio between the highest and lowest pa st sending ratios was at least 2, with lower ratios giving rise to desirable behavior by Aurora. For other trained instance s of Aurora, violating counterexamples only depended on the absolute values of the bounds; e.g., Aurora always decrease s the rate for inputs to the DNN where all sending ratios lie in the range [1,M]for some value M, but not when these lie in the range [1,M+δ] for some small δ. Our ﬁndings show that policies that yield the same expected reward on the training set might generalize very differently to inputs that lie outside this training set, and that our discovered invariants can shed li ght on the generalization strategies of different policies lea rned. B. The Pensieve Video Streamer Pensieve is a DRL system [43] for adaptive bitrate (ABR) selection. To provide high quality of experience for video clients, Pensieve continuously collects statistics about the client’s experience when downloading video chunks (e.g., w as the video rebuffered? how long did it take to download the chunk?) to dynamically adapt the resolution at which the next video chunk is downloaded from the video server. Each video chunk represents a ﬁxedduration video segment (e.g. , 4secondlong chunks in our experiments) encoded in one of several possible resolutions (SD, HD, etc.), with higher resolutions corresponding to larger chunks, in terms of num ber of bits. When clientsensed network conditions are good, we expect the ABR algorithm to decide that the next video chunk will be downloaded in high resolution (HD); and when they are poor, we expect a low resolution (SD) to be selected, to avoid having the client not ﬁnish the download in time, which leads to video rebuffering. The input to Pensieve’s DNN consists of(2t+M+ 3) ﬁelds, where t >0represents the number of recent video chunk downloads considered, and M >0 represents the number of available video resolutions. The i nput comprises: (i) the bitrate (1 ﬁeld) in which the last video chunk was downloaded; (ii) the current video buffer size (1 ﬁeld) of the client, reﬂecting the number of seconds of unwatched vid eo stored at the client; (iii) network throughput measurements for video chunks downloaded in the past ttimesteps ( tﬁelds); (iv) download times for the video chunks downloaded in the past ttimesteps ( tﬁelds); (v) resolution options (M ﬁelds) to download the next chunk; and (vi) the number of remaining chunks to be downloaded (1 ﬁeld). See [43] for a thorough exposition of Pensieve, and [16] for a formalism of the Pensieve veriﬁcation challenge. To maintain consistency with Pensieve’s original hyper parameters, in our experiments t= 8 andM= 6 . Due to the nature of an ABR algorithm, all executions are ﬁnite (downloads ﬁnish in ﬁnite time), and so all relevant propert iesare safety properties. In previous work [16], whiRL 1.0 was applied to check two safety properties of Pensieve: •Property 1. When the chunk download history represents excellent conditions (short download times, large client buffer size), the DRL system should increase the resolution at which chunks are requested before the download ﬁnishes. •Property 2. When the download history represents poor network conditions (long download times, small client buffer size), the DRL system should decrease the resolution at which chunks are requested before the download ﬁnishes. While Property 1 was shown not to hold [16], no counter examples could previously be found for Property 2, and so it could neither be proved nor disproved using existing tools. Using whiRL 2.0 , we were able to prove that Property 2 indeed holds under certain, realistic, assumptions.2To achieve this, we applied kinduction, with k= 1. The result returned by the veriﬁer indicated that the bad states are unreachable , and, hence, that the undesirable behavior cannot occur. The se veriﬁcation queries took approximately 20 minutes to solve . C. The DeepRM Resource Manager DeepRM [42] is a DRLbased resource manager, responsible for allocating various cluster compute resources (e.g., CP U, memory) to queued jobs, in order to optimize the cluster’s throughput. DeepRM receives the following as input: (i) the current resource usage in the system; (ii) a queue with up to Qpending jobs waiting to be scheduled; and (iii) a backlog , indicating the number of jobs waiting to be scheduled that are not yet in the queue. For a ﬁxed Qsized job queue, the DeepRM controller may output one of ( Q+1) possible actions: await action (i.e., no resources will be allocated at this time step), or a schedule qaction for 1≤q≤Q, indicating that job qshould be scheduled next. DeepRM’s output is interpreted as a probability distribution, assigning a certain probabi lity to each of the (Q+1) possible actions. We refer the reader to [42] for a thorough exposition of DeepRM, and to [16] for a formalism of the DeepRM veriﬁcation challenge. In our case study, as in [16], we used a DeepRM system trained with R= 2 resources: CPU andmemory units , and a job queue of size Q= 5. Overall system resources consist of10CPUs and 10memory units. We considered two kinds of jobs: small jobs, which require 1CPU and 1memory unit for a single timestep, and large jobs, which require 10CPUs and10memory units, for t= 20 timesteps. Previous work [16] considered the following safety proper ties for DeepRM: •Property 1. When all resources are fully available, and the queue is ﬁlled with small jobs, DeepRM should never assign the highest probability to the wait action. •Property 2. When no resources are available, and the queue is ﬁlled with small jobs, DeepRM should assign the highest probability to the wait action. •Property 3. When no resources are available, and the queue is ﬁlled with large jobs, DeepRM should assign the highest probability to the wait action. 2We assumed that chunks represent 4secondlong video segments. Con sidered chunk download times are between 4to15seconds per chunk, which implies that downloading each chunk takes longer than consu ming it.9 Using whiRL 1.0 , it was shown [16] that Property 1holds, and that there exist counterexamples for Properties 2and3. However, by using whiRL 2.0 we were able to prove (within a few seconds) a stronger property that, in fact, generalize s properties 1,2and3. By applying whiRL 2.0 ’s abstraction capabilities to both the inputs indicating resource utiliz ation and the output indicating the recommended action, we proved that for anyresource utilization level, when the queue is ﬁlled with identical jobs, the DRL system’s output assigns a highe r probability to schedule 2than to wait. This immediately proves Property 1, and implies that Properties 2and3cannot hold. This ﬁnding sheds new light on previous results, and en hances our understanding of DeepRM: (i) the three original properties do not depend on the current resource utilizatio n. Rather, due to the DRL system learning a suboptimal policy, it is biased towards scheduling a speciﬁc job (job # 2), and may fail to select wait when appropriate; and (ii) the counter examples found for Properties 2and3are not outliers, but rather the general case. Indeed, we were able to use whiRL 2.0 to prove that the inverses of both these properties always ho ld. These results demonstrate that, beyond proving or disprovi ng speciﬁc properties, whiRL 2.0 can shed light on the policy learned by the DRL system, and expose problematic issues. VII. R ELATED WORK "
313,Thoracic Disease Identification and Localization using Distance Learning and Region Verification.txt,"The identification and localization of diseases in medical images using deep
learning models have recently attracted significant interest. Existing methods
only consider training the networks with each image independently and most
leverage an activation map for disease localization. In this paper, we propose
an alternative approach that learns discriminative features among triplets of
images and cyclically trains on region features to verify whether attentive
regions contain information indicative of a disease. Concretely, we adapt a
distance learning framework for multi-label disease classification to
differentiate subtle disease features. Additionally, we feed back the features
of the predicted class-specific regions to a separate classifier during
training to better verify the localized diseases. Our model can achieve
state-of-the-art classification performance on the challenging ChestX-ray14
dataset, and our ablation studies indicate that both distance learning and
region verification contribute to overall classification performance. Moreover,
the distance learning and region verification modules can capture essential
information for better localization than baseline models without these modules.","Radiography has been widely adopted for detecting a number of thoracic diseases. However, detecting diseases in Xray images requires the expert knowledge of radiologists, who are overburdened and often must quickly review each image. Further, the location of an iden tiﬁed disease is generally not annotated and so may be unclear to another doctor reviewing the Xray. Several datasets have been released for disease classiﬁcation from chest Xray images, including [3, 12, 14, 29], and the use of deep learning models in combination with the datasets has resulted in much progress [4, 5, 16, 23, 29, 30]. Nevertheless, the identi ﬁcation and localization of thoracic diseases are still challenging due to subtle interdisease differences and large intradisease variations across different subjects and regions. The only chest Xray dataset with disease bounding boxes, ChestXray14 [29], has boxes only for the test partition. Under this constraint, it is difﬁcult to apply supervised learning c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2006.04203v2  [cs.CV]  11 Aug 20202 ZHANG, CHEN, CHEN: THORACIC DISEASE IDENTIFICATION AND LOCALIZATION Pneumothorax: 0.80Effusion: 0.67 Model learned only from imagelevel labelsDiseaseIdentificationDiseaseLocalizationInput Xray Image+PneumothoraxEffusion Figure 1: Overview of thoracic disease identiﬁcation and localization trained on chest Xrays and imagelevel disease labels. Given a test image, the model predicts how likely the diseases occur in the image and localizes their corresponding bounding boxes. Multiple diseases could coexist in an image. to disease localization, and two popular weakly supervised approaches have been proposed for disease identiﬁcation and localization: CAMbased (class activation map) [34] and MIL based (multiinstance learning). Both embed an Xray image using a pretrained image em bedding network such as ResNet [8] or DenseNet [11]. Under CAMbased, the computed features are used to train a multilabel classiﬁer on disease labels; and localization is per formed based on the class activations in the embedding network [22, 29]. In MILbased, a grid is formed from the embedded features and each element of the grid is classiﬁed as to which, if any, diseases occur, indicating which diseases occur in the image; and localization is performed by combining the grid classiﬁcations [16, 17]. In this paper, we focus on multilabel, weakly supervised thoracic disease identiﬁcation (we will use the terms identiﬁcation andclassiﬁcation interchangeably) and localization in chest Xrays using a weakly supervised learning approach (see Figure 1) . Different from ex isting methods that learn a neural network model on each image independently, we leverage distance learning [1, 10, 13, 26, 32, 33] to learn sufﬁcient feature representations to tackle multilabel disease classiﬁcation. In particular, we exploit triplets of images as the inputs to drive the similarity metric to be small for the pairs of images with similar diseases, and large for the pairs with different ones. Furthermore, we propose a region veriﬁcation module that feeds back the classspeciﬁc attentive regions to verify those regions important to disease classiﬁcation. The size of the regions vary, in contrast to the full images used with previous CAMbased approaches and the smaller contexts from the grids used in the MILbased ap proaches. Finally, with the welllearned disease features, our model can be used for disease localization by incorporating weaklysupervised object detection methods such as CAM. Evaluation on the ChestXray14 dataset shows that our model achieves stateoftheart classiﬁcation performance and consistently outperforms CAM and MIL baselines for multi ple diseases. Ablation studies indicate that both distance learning and region veriﬁcation help classiﬁcation performance, with the strongest contribution from region veriﬁcation. More over, our experiments also indicate that both modules improve disease localization over base line models without these modules, strengthening the support to the improvement on clas siﬁcation since the diseases are better localized by veriﬁed regional features. In conclusion, our contributions include: (1) design an endtoend framework for training multilabel dis ease classiﬁcation and weakly supervised localization simultaneously; (2) extend distance learning to multidisease medical images; (3) propose region veriﬁcation to align disease classiﬁcation of a whole image and the local context surrounding the disease.ZHANG, CHEN, CHEN: THORACIC DISEASE IDENTIFICATION AND LOCALIZATION 3 2 Related Work "
322,End-to-end losses based on speaker basis vectors and all-speaker hard negative mining for speaker verification.txt,"In recent years, speaker verification has primarily performed using deep
neural networks that are trained to output embeddings from input features such
as spectrograms or Mel-filterbank energies. Studies that design various loss
functions, including metric learning have been widely explored. In this study,
we propose two end-to-end loss functions for speaker verification using the
concept of speaker bases, which are trainable parameters. One loss function is
designed to further increase the inter-speaker variation, and the other is
designed to conduct the identical concept with hard negative mining. Each
speaker basis is designed to represent the corresponding speaker in the process
of training deep neural networks. In contrast to the conventional loss
functions that can consider only a limited number of speakers included in a
mini-batch, the proposed loss functions can consider all the speakers in the
training set regardless of the mini-batch composition. In particular, the
proposed loss functions enable hard negative mining and calculations of
between-speaker variations with consideration of all speakers. Through
experiments on VoxCeleb1 and VoxCeleb2 datasets, we confirmed that the proposed
loss functions could supplement conventional softmax and center loss functions.","In recent years, several studies have reported superior results us ing deep neural networks (DNNs) for extracting speaker embed dings compared to conventional stateoftheart ivectorbased [1] speaker veriﬁcation systems [2–8]. Therefore, several re cent studies have mainly focused on designing loss functions to train DNNs to make them suitable for speaker veriﬁcation. Wan et al. proposed a generalized endtoend (GE2E) loss func tion based on centroids, which are the average embeddings for each speaker, to train DNNs with higher generalization perfor mance [5]. Li et al. applied a loss function based on angular softmax, which was proposed for face recognition [9], to create an angular margin between speakers in an embedding space [4]. The conventional studies on loss functions mentioned above do not address the following two problems. The ﬁrst problem is that conventional loss functions only consider a lim ited number of speakers according to minibatch composition. In the process of repeatedly training DNNs with minibatches of a small size, the parameters of a network could be biased to yCorresponding author This work was supported by the Technology Innovation Program (10076583, Development of freerunning speech recognition technolo gies for embedded robot system) funded By the Ministry of Trade, In dustry & Energy(MOTIE, Korea)only the speakers included in one minibatch. The second prob lem is that excessive overhead occurs in performing hard neg ative mining, which is important in metriclearningbased loss functions [10]. Hard negative mining is known to have a signif icant impact on the performance of metric learning. However, it is usually performed at regular intervals because of practical issues. Ideally, hard negative mining should be conducted for each minibatch. This is because hard negative samples will change as weight parameters are updated every minibatch. Al though GE2E has partially solved these problems, there is a lim itation that only few speakers can be considered by hard nega tive mining in GE2E. In this paper, we propose loss functions based on speaker bases to handle these problems. The speaker bases refer to the column vectors of the weight matrix of the output layer. This deﬁnition stems from the fact that since the column vectors rep resent speakers in embedding space (output of the last hidden layer), each column vector can be considered as a basis for that speaker. This concept can be considered rather a general ap proach because it can be applied to any DNNbased speaker embedding extraction system that comprises a fullyconnected code layer. We expect that it would be possible to train all speakers simultaneously and perform hard negative mining in every minibatch using the loss function based on the speaker bases. 2. Related works "
356,Multi-stream Convolutional Neural Network with Frequency Selection for Robust Speaker Verification.txt,"Speaker verification aims to verify whether an input speech corresponds to
the claimed speaker, and conventionally, this kind of system is deployed based
on single-stream scenario, wherein the feature extractor operates in full
frequency range. In this paper, we hypothesize that machine can learn enough
knowledge to do classification task when listening to partial frequency range
instead of full frequency range, which is so called frequency selection
technique, and further propose a novel framework of multi-stream Convolutional
Neural Network (CNN) with this technique for speaker verification tasks. The
proposed framework accommodates diverse temporal embeddings generated from
multiple streams to enhance the robustness of acoustic modeling. For the
diversity of temporal embeddings, we consider feature augmentation with
frequency selection, which is to manually segment the full-band of frequency
into several sub-bands, and the feature extractor of each stream can select
which sub-bands to use as target frequency domain. Different from conventional
single-stream solution wherein each utterance would only be processed for one
time, in this framework, there are multiple streams processing it in parallel.
The input utterance for each stream is pre-processed by a frequency selector
within specified frequency range, and post-processed by mean normalization. The
normalized temporal embeddings of each stream will flow into a pooling layer to
generate fused embeddings. We conduct extensive experiments on VoxCeleb
dataset, and the experimental results demonstrate that multi-stream CNN
significantly outperforms single-stream baseline with 20.53 % of relative
improvement in minimum Decision Cost Function (minDCF).","DEEP learning has achieved outstanding success in vari ous speechoriented tasks, such as auto speech recog nition [1], [2], speaker recognition [3]– [6] and speaker diarization [7], [8], etc. The deep learning paradigm is ad dressed to extract highly abstracted representations by means of welldesigned neural networks based on the feedin data. Most commonly, there are three scenarios to train neural network in deep learning, which are supervised learning [9], semisupervised learning [10] and unsupervised learning (or, more precisely, selfsupervised learning) [11], respectively. In addition, supervised learning with abundant labeled data is the most widely used scenario [12], which is also the scenario used in this paper.Speaker recognition is the ﬁled of recognizing speaker identities based on their voices. In general, it can be clariﬁed into either 1) speaker veriﬁcation or 2) speaker identiﬁcation. Speaker veriﬁcation aims to answer the question “is some body speaking?” with single utterance or “are they from the same speaker?” with pairwise utterances. Speaker identiﬁ cation is used to answer the question “who is speaking?” among a set of enrolled speakers. Speaker veriﬁcation is one case of biometric authentication, where user provides their biometric characteristics in form of voiceprint as passwords. The greatest challenge of speaker veriﬁcation task is the effective usage of datasets obtained from the real world under noisy and unconstrained conditions [13]. In this paper, we aim to address this challenge and propose a new framework VOLUME 4, 2016 1arXiv:2012.11159v2  [cs.SD]  12 Jan 2021Author et al. : Preparation of Papers for IEEE TRANSACTIONS and JOURNALS to extract robust speaker embeddings. A. FREQUENCY SELECTION Normally, the features used for training and testing are extracted in full frequency band, and they are usually low dimensional. As features play an important role in speaker veriﬁcation system, if we use just partial frequency range instead of full frequency range, will the system perform equally well? Follow by this assumption, we ﬁrst segment fullband into several subbands by using frequency selec tor, e.g., low frequency subband and high frequency sub band, and use these features to train several singlestream system respectively, eager to witness the impact of frequency domain on system performance. The feature extractor can select which subbands to use as target frequency domain to generate framelevel features, and we call this idea as frequency selection. B. RESOLUTION AND PROBLEM STATEMENT We hypothesize that machine could dramatically beneﬁt from our proposed frequency selection technique. On the other hand, Convolutional Neural Network (CNN) is one kind of widely used neural networks in image recognition. More recently, CNN is introduced to speaker recognition and achieves competitive results [5], [6], compared with the most famous TimeDelay Neural Network (TDNN) and its vari ations [14]. Despite demonstrating encouraging outcomes, CNN for speaker veriﬁcation is remaining an open topic, which requires more efforts to achieve breakthrough. By investigating various ways of doing so, we bridge frequency selection and CNN, and propose a new framework of multi stream CNN. However, this approach may prompt some questions: Why to use multistream if singlestream can offer us a high enough accuracy? Can machine learn enough knowledge to handle classiﬁcation task by only listening to partial fre quency range? Demonstrated by our experimental results, these are part of questions we are going to delve into and ﬁnd out answer in this paper. C. CONTRIBUTIONS Most of the speaker veriﬁcation systems are deployed based on singlestream within full frequency range. To the best of our knowledge, this paper is the ﬁrst to investigate the impact of frequency domain and to improve system performance by means of frequency selection. Our contributions are as follows: (1) We explore the performance of neural network by feeding in “partial” features extracted from speech within subbands of frequency instead of conventionally used full band. And we ﬁnd that machine can perform equivalently well in some subbands, which are also beneﬁcial to improve the performance of multistream system. (2) We propose the idea of frequency selection, and a novel framework of multistream CNN based on it for speaker veriﬁcation.(3) We make our work open source, and it is available to download at https://github.com/ShaneRun/multistreamCNN . D. STRUCTURE OF THIS PAPER This paper is organized as below. We review on the related work in Section II. Section III describes our proposed method in detail. Section IV presents experiments for pairwise veri ﬁcation, which consists of dataset, training and results. We also make comprehensive comparison in this section in order to demonstrate the efﬁcacy and understand the inﬂuence of frequency selection. Section V contains discussion and future work. Section VI is the conclusion of our work. II. RELATED WORK "
139,Baseline Systems for the First Spoofing-Aware Speaker Verification Challenge: Score and Embedding Fusion.txt,"Deep learning has brought impressive progress in the study of both automatic
speaker verification (ASV) and spoofing countermeasures (CM). Although
solutions are mutually dependent, they have typically evolved as standalone
sub-systems whereby CM solutions are usually designed for a fixed ASV system.
The work reported in this paper aims to gauge the improvements in reliability
that can be gained from their closer integration. Results derived using the
popular ASVspoof2019 dataset indicate that the equal error rate (EER) of a
state-of-the-art ASV system degrades from 1.63% to 23.83% when the evaluation
protocol is extended with spoofed trials.%subjected to spoofing attacks.
However, even the straightforward integration of ASV and CM systems in the form
of score-sum and deep neural network-based fusion strategies reduce the EER to
1.71% and 6.37%, respectively. The new Spoofing-Aware Speaker Verification
(SASV) challenge has been formed to encourage greater attention to the
integration of ASV and CM systems as well as to provide a means to benchmark
different solutions.","Recent years have seen rapid progress in automatic speaker veriﬁcation (ASV) [1–3]. Even for unconstrained in the wild scenarios, the latest systems deliver low equal error rates (EERs) that are close to those for well constrained conditions [2, 4, 5]. However, there is evi dence that these improvements might not offer protection against spooﬁng attacks – the presentation of utterances specially crafted to deceive the ASV system. These authors contributed equally to this work.Solutions to protect ASV systems from such attacks take the form of countermeasures (CMs), typically sepa rate subsystems designed to detect manipulated or syn thetic utterances [6]. The threat of spooﬁng attacks has intensiﬁed in recent times due to the rapid advances in other speech technologies which can be used to gener ate spoofed utterances. They include: speechtospeech voice conversion (VC); texttospeech (TTS) speech syn thesis; replay attacks. Since ASV systems are increas ingly deployed in securitycritical operations as a part of a biometric authentication system, vulnerabilities to spoof ing attacks are unacceptable. In response to the threat, the ASVspoof initiative has held biennial challenges to promote the development of research in spooﬁng detection [6]. Two different use case scenarios have been deﬁned, namely physical access (PA) and logical access (LA). The work in this paper relates to the latter, typically telephony applications and robustness to TTS and VC spooﬁng attacks. When assessed using the ASVspoof 2019 LA evaluation set, today’s leading CM systems deliver EERs of less than 2% [7–17]. While the EER metric was adopted in almost all early work, the ASVspoof community has now transi tioned to the minimum tandem detection cost function (min tDCF) [18] as the primary metric. The tDCF re ﬂects the impact of spooﬁng and countermeasures upon a typicallyﬁxed ASV system. Even with this strategy, CMs are often designed in standalone fashion, indepen dently from ASV . Until now, and with only few notable exceptions [19–26], very little work has investigated the beneﬁt of jointly optimised, or integrated CM+ASV so lutions. The SpooﬁngAware Automatic Speaker Veriﬁca tion (SASV) challenge1, a special session at INTER SPEECH 2022, aims to promote greater research in this direction and extends the traditional ASV scenario to 1https://sasvchallenge.github.ioarXiv:2204.09976v1  [cs.SD]  21 Apr 2022CM ASV CM ASV(a) cascade combination (b) parallel fusionReject AcceptReject Accept Reject AcceptFusionFigure 1: Backend fusion of CM and ASV subsystems. (a): cascaded combination, a form of decision level fu sion. (b): parallel fusion which can operate at either de cision, score, or embedding levels. When parallel fusion operates at the decision level, it is the same as the cas caded combination. consider spooﬁng attacks. The ﬁrst SASV challenge [27] utilises existing ASVspoof databases with metrics ex tended to support the evaluation of integrated CM+ASV solutions. Ultimately, SASV aims to strengthen the foun dations between research in spooﬁng detection and ASV . New contributions reported in this paper include: (i) baseline SASV solutions to integrated CM+ASV leveraging stateoftheart subsystems; (ii) metrics de signed speciﬁcally for the SASV task; (iii) experimental results and detailed, perattack analyses. 2. Related work "
70,Towards A Unified Conformer Structure: from ASR to ASV Task.txt,"Transformer has achieved extraordinary performance in Natural Language
Processing and Computer Vision tasks thanks to its powerful self-attention
mechanism, and its variant Conformer has become a state-of-the-art architecture
in the field of Automatic Speech Recognition (ASR). However, the main-stream
architecture for Automatic Speaker Verification (ASV) is convolutional Neural
Networks, and there is still much room for research on the Conformer based ASV.
In this paper, firstly, we modify the Conformer architecture from ASR to ASV
with very minor changes. Length-Scaled Attention (LSA) method and
Sharpness-Aware Minimizationis (SAM) are adopted to improve model
generalization. Experiments conducted on VoxCeleb and CN-Celeb show that our
Conformer based ASV achieves competitive performance compared with the popular
ECAPA-TDNN. Secondly, inspired by the transfer learning strategy, ASV Conformer
is natural to be initialized from the pretrained ASR model. Via parameter
transferring, self-attention mechanism could better focus on the relationship
between sequence features, brings about 11% relative improvement in EER on test
set of VoxCeleb and CN-Celeb, which reveals the potential of Conformer to unify
ASV and ASR task. Finally, we provide a runtime in ASV-Subtools to evaluate its
inference speed in production scenario. Our code is released at
https://github.com/Snowdar/asv-subtools/tree/master/doc/papers/conformer.md.","Automatic Speaker Veriﬁcation (ASV) is a task to verify the identity of the speaker by voice, which has been welldeveloped and widely applied in many realworld scenarios. Currently, xvector proposed by Snyder et al. [1] is the most popular framework for ASV systems. It includes two parts, where an embedding extractor maps utterances with variable duration to ﬁxeddimensional speaker representations, and then the similarity of the speaker representation can be calcu lated by backend scoring method. Many prior works focused on DNNbased structure have improved the performance of ASV sys tems (e.g., ResNet, Res2Net, ECAPATDNN) [2, 3, 4, 5]. Most of above networks are Convolutional Neural Networks (CNNs), which have the inherent ability of emphasizing the local information. Recently, selfattention mechanisms that directly capture the global information have been explored, and it has helped Trans former [6] achieve remarkable success in Natural Language Pro † Coﬁrst author * Corresponding authorcessing (NLP) and Computer Vision (CV) areas [7, 8]. However, unlike CNNs, Transformer lacks some of the inductive biases, such as translation equivariance and locality, which degrades perfor mance when trained on insufﬁcient data. It is difﬁcult to achieve competitive results by directly applying Transformer to ASV tasks [9, 10]. Conformer [11] is a hybrid architecture which combines selfattention with convolutions, i.e., selfattention learns the global interaction while convolutions capture the local information. It has become a stateofart model in Automatic Speech Recogni tion (ASR). MFAConformer [12] utilizes the Multiscale Feature Aggregation method in ECAPATDNN, successfully introduces Conformer into ASV for the ﬁrst time. However, the uniformity between ASV an ASR deserves further attention. The same Con former establishes connections between ASV and other tasks, which will not only facilitate better research on the link between ASV and ASR, but may also be a foundation of future multitask learning or multimodal machine learning. Hence, in this paper, we mainly concentrate on ASV Conformer which matches ASR encoder. In addition, several studies have injected phonetic information into the DNN structure of the ASV extractor through multitask learning [13, 14, 15], indicates that there exists some positive in terdependence between the speaker identities and ASR tasks when sharing some of the lowlevel computation. Meanwhile, in the ﬁeld of Language Identiﬁcation (LID), providing informative speech rep resentation by a pretrained ASR model in LID system, proved to be effective for the downstream LID task [16]. It is worth mentioned that [17] adopts transfer learning scheme, that is, pretrains a U2++ encoderdecoder [18] model and then further ﬁnetunes the encoder for the LID task, won the ﬁrst place in the OLR 2021 [19]. It can be well explained by the fact that the ASR encoder already has a strong capability to discriminate languages, since the supervised training labels for ASR are languagerelated. Although the association with ASR information in ASV task is not as apparent as in LID, e.g., dif ferent speakers can say the same words. Their deeper dependencies could be digged by appropriate methods. Inspired by these works, we propose a parameter transferring strategy, which can make use of a typical ASR model to improve the performance of ASV system. At last, for the purpose of bridging the gap between production and research, we provide a C++ based runtime tool to evaluate our models’ inference speed in production environment. With Torch Just In Time (JIT) and LibTorch, models trained by Pytorch can be con verted to TorchScrip, and then employed in C++ applications. Our main contributions in this paper are as follows. • We modify Conformers of different conﬁgures from ASR to ASV system. To improve model generalization ability, LengthScaled Attention (LSA) method [20] enables selfattention to better gen eralize to various length inputs, and SharpnessAware MinimizaarXiv:2211.07201v2  [eess.AS]  16 Jan 2023tion (SAM) [21] prevents the loss from falling into the local min ima during training. Our system yields competitive results in pop ular V oxCeleb [22, 23] and CNCeleb [24, 25]. • Through a parameter transferring strategy, we show that ASV Conformer could beneﬁt from ASR information. Parts of the ASR encoder is selected to initialize ASV Conformer, then we retrain the model rather than ﬁnetune it. This method allows model to learn the deep relationship with ASR. • We provide a runtime to conform the production value of our mod els, make it easier and more convenient to deploy ASV models to real applications. 2. METHODS "
251,NeuroDiff: Scalable Differential Verification of Neural Networks using Fine-Grained Approximation.txt,"As neural networks make their way into safety-critical systems, where
misbehavior can lead to catastrophes, there is a growing interest in certifying
the equivalence of two structurally similar neural networks. For example,
compression techniques are often used in practice for deploying trained neural
networks on computationally- and energy-constrained devices, which raises the
question of how faithfully the compressed network mimics the original network.
Unfortunately, existing methods either focus on verifying a single network or
rely on loose approximations to prove the equivalence of two networks. Due to
overly conservative approximation, differential verification lacks scalability
in terms of both accuracy and computational cost. To overcome these problems,
we propose NeuroDiff, a symbolic and fine-grained approximation technique that
drastically increases the accuracy of differential verification while achieving
many orders-of-magnitude speedup. NeuroDiff has two key contributions. The
first one is new convex approximations that more accurately bound the
difference neurons of two networks under all possible inputs. The second one is
judicious use of symbolic variables to represent neurons whose difference
bounds have accumulated significant error. We also find that these two
techniques are complementary, i.e., when combined, the benefit is greater than
the sum of their individual benefits. We have evaluated NeuroDiff on a variety
of differential verification tasks. Our results show that NeuroDiff is up to
1000X faster and 5X more accurate than the state-of-the-art tool.","There is a growing need for rigorous analysis techniques that can compare the behaviors of two or more neural networks trained for the same task. For example, such techniques have applications in better understanding the representations learned by different net works [46], and finding inputs where networks disagree [52]. The need is further motivated by the increasing use of neural network Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASE ’20, September 21–25, 2020, Virtual Event, Australia ©2020 Association for Computing Machinery. ACM ISBN 9781450367684/20/09. . . $15.00 https://doi.org/10.1145/3324884.3416560compression [ 14] – a technique that alters the network’s parame ters to reduce its energy and computational cost – where we expect the compressed network to be functionally equivalent to the origi nal network. In safetycritical systems where a single instance of misbehavior can lead to catastrophe, having formal guarantees on the equivalence of the original and compressed networks is highly desirable. Unfortunately, most work aimed at verifying or testing neural networks does not provide formal guarantees on their equivalence. For example, testing techniques geared toward refutation can pro vide inputs where a single network misbehaves [ 22,31,42,44,51] or multiple networks disagree [ 23,34,52], but they do not guarantee the absence of misbehaviors or disagreements. While techniques geared toward verification can prove safety or robustness prop erties of a single network [ 7–9,15,18,25,38,41,47], they lack crucial information needed to prove the equivalence of multiple networks. One exception is the ReluDiff tool of Paulsen et al. [ 33], which computes a sound approximation of the difference of two neural networks, a problem known as differential verification . While ReluDiff performs better than other techniques, the overly con servative approximation it computes often causes both accuracy and efficiency to suffer. To overcome these problems, we propose NeuroDiff , a new sym bolic andfinegrained approximation technique that significantly increases the accuracy of differential verification while achieving many ordersofmagnitude speedup. NeuroDiff has two key con tributions. The first contribution is the development of convex ap proximations , a finegrained approximation technique for bound ing the output difference of neurons for all possible inputs, which drastically improves over the coarsegrained concretizations used byReluDiff . The second contribution is judiciously introducing symbolic variables to represent neurons in hidden layers whose dif ference bounds have accumulated significant approximation error. These two techniques are also complementary, i.e., when combined, the benefit is significantly greater than the sum of their individual benefits. The overall flow of NeuroDiff is shown in Figure 1, where it takes as input two neural networks fandf′, a set of inputs to the neural networks Xdefined by box intervals, and a small constant ϵ that quantifies the tolerance for disagreement. We assume that f and f′have the same network topology and only differ in the numerical values of their weights. In practice, f′could be the compressed version of f, or they could be networks constructed using the same network topology but slightly different training 1arXiv:2009.09943v1  [cs.LG]  21 Sep 2020ASE ’20, September 21–25, 2020, Virtual Event, Australia Brandon Paulsen, Jingbo Wang, Jiawei Wang, and Chao Wang Inputs NeuroDiff Forward Analysis Convex Approximation Intermediate Variables CheckϵYesVerifiedPartition X Nof f′ X⊆RnϵX1 X2 Proven? Figure 1: The overall flow of NeuroDiff . data. We also note that this assumption can support compression techniques such as weight pruning [ 14] (by setting edges’ weights to 0) and even neuron removal [ 10] (by setting all of a neuron’s incoming edge weights to 0). NeuroDiff then aims to prove ∀x∈ X.|f′(x)−f(x)|<ϵ. It can return (1) verified if a proof can be found, or (2) undetermined if a specified timeout is reached. Internally, NeuroDiff first performs a forward analysis using symbolic interval arithmetic to bound both the absolute value ranges of all neurons, as in single network verification, and the dif ference between the neurons of the two networks. NeuroDiff then checks if the difference between the output neurons satisfies ϵ, and if so returns verified . Otherwise, NeuroDiff uses a gradientbased refinement to partition Xinto two disjoint sub regions X1andX2, and attempts the analysis again on the individual regions. Since X1 andX2form independent subproblems, we can do these analyses in parallel, hence gaining significant speedup. The new convex approximations used in NeuroDiff are signifi cantly more accurate than not only the coarsegrained concretiza tions inReluDiff [33] but also the standard convex approximations in singlenetwork verification tools [ 39,40,47,54]. While these (standard) convex approximations aim to bound the absolute value range ofy=ReLU(x), where xis the input of the rectified linear unit (ReLU) activation function, our new convex approximations aim to bound the difference z=ReLU(x+∆)−ReLU(x), where xandx+∆are ReLU inputs of two corresponding neurons. This is significantly more challenging because it involves the search of bounding planes in a threedimensional space (defined by x,∆and z) as opposed to a twodimensional space as in the prior work. The symbolic variables we judiciously add to represent values of neurons in hidden layers should not be confused with the symbolic inputs used by existing tools either. While the use of symbolic inputs is well understood, e.g., both in singlenetwork verification [ 39,40, 47,54] and differential verification [ 33], this is the first time that symbolic variables are used to substitute values of hidden neurons during differential verification. While the impact of symbolic inputs often diminishes after the first few layers of neurons, the impact of these new symbolic variables, when judiciously added, can be maintained in any hidden layer. We have implemented the proposed NeuroDiff in a tool and evaluated it on a large set of differential verification tasks. Our benchmarks consists of 49 networks, from applications such as aircraft collision avoidance, image classification, and human activity recognition. We have experimentally compared with ReluDiff [33], the stateoftheart tool which has also been shown to be superior1.9 1.1 1.9 1.02.1 0.9 1.11.01.0 1.0n0,1 n0,2 n1,2n1,1 n2,1 n2,2n3,1x1∈[− 2,2] x2∈[− 2,2]2.01.02.0 1.01.01.02.0 1.01.0 1.0 Figure 2: Motivating example. toReluVal [48] and DeepPoly [40] for differential verification. Our results show that NeuroDiff is up to 1,000X faster and 5X more accurate. In addition, NeuroDiff is able to prove many of the same properties as ReluDiff while considering much larger input regions. To summarize, this paper makes the following contributions: •We propose new convex approximations to more accurately bound the difference between corresponding neurons of two structurally similar neural networks. •We propose a method for judiciously introducing symbolic variables to neurons in hidden layers to mitigate the propa gation of approximation error. •We implement and evaluate the proposed technique on a large number of differential verification tasks and demon strate its significant speed and accuracy gains. The remainder of this paper is organized as follows. First, we provide a brief overview of our method in Section 2. Then, we provide the technical background in Section 3. Next, we present the detailed algorithms in Section 4 and the experimental results in Section 5. We review the related work in Section 6. Finally, we give our conclusions in Section 7. 2 OVERVIEW In this section, we highlight our main contributions and illustrate the shortcomings of previous work on a motivating example. 2.1 Differential Verification We use the neural network in Figure 2 as a running example. The network has two input nodes n0,1,n0,2, two hidden layers with two neurons each ( n1,1,n1,2andn2,1,n2,2), and one output node n3,1. Each neuron in the hidden layer performs a summation of their inputs, followed by a rectified linear unit (ReLU) activation function, defined as y=max(0,x), where xis the input to the ReLU activation function, and yis the output. Let this entire network be f, and the value of the output node be n3,1=f(x1,x2), where x1andx2are the values of input nodes n0,1 andn0,2, respectively. The network can be evaluated on a specific input by performing a series matrix multiplications (i.e., affine transformations) followed by elementwise ReLU transformations. For example, the output of the neurons of the first hidden layer is n1,1 n1,2 =ReLU 1.9−1.9 1.0 1 .1 ·x1 x2! =ReLU(1.9x1−1.9x2) ReLU(1.1x1+1.0x2) 2ASE ’20, September 21–25, 2020, Virtual Event, Australia Differential verification aims to compare fto another network f′that is structurally similar. For our example, f′is obtained by rounding the edge weights of fto the nearest whole numbers, a network compression technique known as weight quantization . Thus, f′,n′ k,jandn′ 3,1=f′(x1,x2)are counterparts of f,nk,jand n3,1=f(x1,x2)for0≤k≤2and 1≤j≤2. Our goal is to prove that|f′(x1,x2)−f(x1,x2)|is less than some reasonably small ϵfor all inputs defined by the intervals x1∈[− 2,2]andx2∈[− 2,2]. For ease of understanding, we show the edge weights of fin black, and f′in light blue in Figure 2. 2.2 Limitations of Existing Methods Naively, one could adapt any stateoftheart, singlenetwork verifi cation tool for our task, including DeepPoly [40] and Neurify [47]. Neurify , in particular, takes a neural network and an input region of the network, and uses interval arithmetic [ 27,48] to produce sound symbolic lower and upper bounds for each output node. Typ ically, Neurify would then use the computed bounds to certify the absence of adversarial examples [43] for the network. However, for our task, the bounds must be computed for both net works fandf′. Then, we subtract them, and concretize to compute lower and upper bounds on f′(x1,x2)−f(x1,x2). In our example, the individual bounds would be (approximately, due to rounding) [LB(f),U B(f)]=[−0.94x1−0.62x2−6.51,0.71x1−2.35x2+7.98]and [LB(f′),U B(f′)]=[−0.94x1−0.44x2−6.75,0.75x1−2.25x2+8.00] for nodes n3,1andn′ 3,1, respectively. After the subtraction, we would obtain the bounds [LB(f′)−U B(f),U B(f′)−LB(f)]= [−1.65x1+1.9x2−14.73,1.68x1−1.63x2+14.5]. After concretiza tion, we would obtain the bounds [−21.83,21.12]. Unfortunately, the bounds are far from being accurate. TheReluDiff method of Paulsen et al. [ 33] showed that, by directly computing a difference interval layerbylayer, the accuracy can be greatly improved. For the running example, ReluDiff would first compute bounds on the difference between the neurons n1,1 andn′ 1,1, which is[0,1.1], and then similarly compute bounds on the difference between outputs of n1,2andn′ 1,2. Then, the results would be used to compute difference bounds of the subsequent layer. The reason it is more accurate is because it begins computing part of the difference bound before errors have accumulated, whereas the naive approach first accumulates significant errors at each neuron, andthen computes the difference bound. In our running example, ReluDiff [33] would compute the tighter bounds [−3.1101 ,2.5600]. While ReluDiff improves over the naive approach, in many cases, it uses concrete values for the upper and lower bounds. In prac tice, this approach can suffer from severe errorexplosion. Specifi cally, whenever a neuron of either network is in an unstable state – i.e., when a ReLU’s input interval contains the value 0 – it has to concretize the symbolic expressions. 2.3 Our Method The key contribution in NeuroDiff , our new method, is a symbolic andfinegrained approximation technique that both reduces the approximation error introduced when a neuron is in an unstable state, and mitigates the explosion of such approximation error after it is introduced.2.3.1 Convex Approximation for the Difference Interval. Our first contribution is developing convex approximations to directly bound the difference between two neurons after these ReLU activations. Specifically, for a neuron ninfand corresponding neuron n′inf′, we want to bound the value of ReLU(n′)−ReLU(n). We illustrate the various choices using Figures 3, 4, and 5. The naive way to bound this difference is to first compute ap proximations of y=ReLU(n)andy′=ReLU(n′)separately, and then subtract them. Since each of these functions has a single vari able, convex approximation is simple and is already used by single network verification tools [ 40,47,49]. Figure 6 shows the function y=ReLU(n)and its bounding planes (shown as dashedlines) in a twodimensional space (details in Section 3). However, as we have already mentioned, approximation errors would be accumu lated in the bounds of ReLU(n)andReLU(n′)and then amplified by the interval subtraction. This is precisely why the naive approach performs poorly. TheReluDiff method of Paulsen et al. [33] improves upon the new approximation by computing an interval bound on n′−n, de noted ∆, then rewriting z=ReLU(n′)−ReLU(n)asz=ReLU(n+ ∆)−ReLU(n), and finally bounding this new function instead. Fig ure 3 shows the shape of z=ReLU(n+∆)−ReLU(n)in a three dimensional space. Note that it has four piecewise linear subre gions, defined by values of the input variables nand∆. While the bounds computed by ReluDiff [33], shown as the (horizontal) yellow planes in Figure 4, are sound, in practice they tend to be loose because the upper and lower bounds are both concrete values. Such eager concretization eliminates symbolic information that ∆ contained before applying the ReLU activation. In contrast, our method computes a convex approximation of z, shown by the (tilted) yellow planes in Figure 5. Since these tilted bounding planes are in a threedimensional space, they are sig nificantly more challenging to compute than the standard two dimensional convex approximations (shown in Figure 6) used by single network verification tools. Our approximations have the advantage of introducing significantly less error than the horizon tal planes used in ReluDiff [33], while maintaining some of the symbolic information for ∆before applying the ReLU activation. We will show through experimental evaluation (Section 5) that our convex approximation can drastically improve the accuracy of the difference bounds, and are particularly effective when the input region being considered is large. Furthermore, the tilted planes shown in Figure 5 are for the general case. For certain special cases, we obtain even tighter bounding planes (details in Section 4). In the running example, using our new convex approximations would improve the final bounds to [−1.97,1.42]. 2.3.2 Symbolic Variables for Hidden Neurons. Our second contri bution is introducing symbolic variables to represent the output values of some unstable neurons, with the goal of limiting the prop agation of approximation errors after they are introduced. In the running example, since both n1,1andn′ 1,1are in unstable states, i.e., the input intervals of the ReLUs contain the value 0, we may introduce a new symbol x3=ReLU(n′ 1,1)−ReLU(n1,1). In all sub sequent layers, whenever the value of ReLU(n′ 1,1)−ReLU(n1,1)is needed, we use the bounds [x3,x3]instead of the actual bounds. 3ASE ’20, September 21–25, 2020, Virtual Event, Australia Brandon Paulsen, Jingbo Wang, Jiawei Wang, and Chao Wang Figure 3: The shape of z=ReLU(n+∆)−ReLU(n).  Figure 4: Bounding planes computed by ReluDiff [33]. Figure 5: Bounding planes computed by our new method.LB(n) U B(n)U B(ReLU(n)) LB(ReLU(n)) Figure 6: Bounding planes computed by Neurify [47]. The reason why using x3can lead to more accurate results is because, even though our convex approximations reduce the error introduced, there is inevitably some error that accumulates. Intro ducing x3allows this error to partially cancel in the subsequent layers. In our running example, introducing the new symbolic vari ablex3would be able to improve the final bounds to [−1.65,1.18]. While creating x3improved the result in this case, carelessly introducing new variables for all the unstable neurons can actually reduce the overall benefit (see Section 4). In addition, the computa tional cost of introducing new variables is not negligible. Therefore, in practice, we must introduce these symbolic variables judiciously, to maximize the benefit. Part of our contribution in NeuroDiff is in developing heuristics to automatically determine when to create new symbolic variables (details in Section 4). 3 BACKGROUND In this section, we review the technical background and then intro duce notations that we use throughout the paper. 3.1 Neural Networks We focus on feedforward neural networks, which we define as a function fthat takes an ndimensional vector of real values x∈X, where X⊆Rn, and maps it to an mdimensional vector y∈Y, where Y⊆Rm. We denote this function as f:X→Y. Typically,each dimension of yrepresents a score, such as a probability, that the input xbelongs to class i, where 1≤i≤m. A network with llayers has lweight matrices, each of which is denoted Wk,for1≤k≤l. For each weight matrix, we have Wk∈Rlk−1×lkwhere lk−1is the number of neurons in layer (k−1) and likewise for lk, and l0=n. Each element in Wkrepresents the weight of an edge from a neuron in layer (k−1)to one in layer k. Let nk,jdenote the jthneuron of layer k, and nk−1,idenote theithneuron of layer(k−1). We use Wk[i,j]to denote the edge weight from nk−1,itonk,j. In our motivating example, we have W1[1,1]=1.9andW1[1,2]=1.1. Mathematically, the entire neural network can be represented byf(x)=fl(Wl·fl−1(Wl−1·...f1(W1·x)...)), where fkis the activation function of the kthlayer and 1≤k≤l. We focus on neural networks with ReLU activations because they are the most widely implemented in practice, but our method can be extended to other activation functions, such as siдmoid andtanh , and other layer types, such as convolutional and maxpooling. We leave this as future work. 3.2 Symbolic Intervals To compute approximations of the output nodes that are sound for all input values, we leverage interval arithmetic [ 27], which can be viewed as an instance of the abstract interpretation framework [ 5]. It is wellsuited to the verification task because interval arithmetic 4ASE ’20, September 21–25, 2020, Virtual Event, Australia is soundly defined for basic operations of the network such as addition, subtraction, and scaling. LetI=[LB(I),UB(I)]be an interval with lower bound LB(I)and upper bound UB(I). Then, for intervals I1,I2, we have addition and subtraction defined as I1+I2=[LB(I1)+LB(I2),UB(I1)+UB(I2)] andI1−I2=[LB(I1)−UB(I2),UB(I1)−LB(I2)], respectively. For a constant c, scaling is defined as c×I1=[c×LB(I1),c×UB(I1) when c>0, and c×I1=[c×UB(I1),c×LB(I1)]otherwise. While interval arithmetic is a sound overapproximation, it is not always accurate. To illustrate, let f(x)=3x−x, and say we are interested in bounding f(x)when x∈ [− 1,1]. One way to bound fis by evaluating f(I)where I=[−1,1]. Doing so yields 3×[− 1,1]−[− 1,1]=[−4,4]. Unfortunately, the most accurate bounds are[−2,2]. There are (at least) two ways we can improve the accuracy. First, we can soundly refine the result by dividing the input intervals into disjoint partitions, performing the analysis independently on each partition, and then unioning the resulting output intervals together. Previous work has shown the result will be at least as precise [ 48], and often better. For example, if we partition x∈[− 1,1] into x∈[− 1,0]andx∈[0,1], and perform the analysis for each partition, the resulting bounds improve to [−3,3]. Second, the dependence between the two intervals are not lever aged when we subtract them, i.e., that they were both xterms and hence could partially cancel out. To capture the dependence, we can use symbolic lower and upper bounds [ 48], which are expres sions in terms of the input variable, i.e., I=[x,x]. Evaluating f(I) then yields the interval If=[2x,2x], for x∈[− 1,1]. When using symbolic bounds, eventually, we must concretize the lower and upper bound equations. We denote concretization of LB(If)=2x andUB(If)=2xasLB(If)=−2andUB(If)=2, respectively. Compared to the naive solution, [−4,4], this is a significant im provement. When approximating the output of a given function f:X→Y over an input interval X⊆X, one may prove soundness by showing that the evaluation of the lower and upper bounds on any input x∈Xare always greater than and less than, respectively, to the true value of f(x). Formally, for an interval I, letLB(I)(x)be the evaluation of the lower bound equation on input x, and similarly for UB(I)(x). Then, the approximation is considered sound if ∀x∈X, we have LB(I)(x)≤f(x)≤UB(I)(x). 3.3 Convex Approximations While symbolic intervals are exact for linear operations (i.e. they do not introduce error), this is not the case for nonlinear operations, such as the ReLU activation. This is because, for efficiency reasons, the symbolic lower and upper bounds must be kept linear. Thus, de veloping linear approximations for nonlinear activation functions has become a signifciant area of research for single neural network verification [ 40,47,49,54]. We review the basics below, but caution that they are different from our new convex approximations in NeuroDiff . We denote the input to the ReLU of a neuron nk,jasSin(nk,j)and the output as S(nk,j). The approach used by existing singlenetwork verification tools is to apply an affine transformation to the upper bound of Sin(nk,j)such that UB(Sin(nk,j))(x)≥ 0, where x∈X,andXis the input region for the entire network. For the lower bound, there exist several possible transformations, including the one used by Neurify [47], shown in Figure 6, where n=Sin(nk,j) and the dashed lines are the upper and lower bounds. We illustrate the upper bound transformation for n1,1of our moti vating example. After computing the upper bound of the ReLU input UB(Sin(n1,1))=1.9x1−1.9x2, where x1∈[− 2,2]andx2∈[− 2,2], it computes the concrete lower and upper bounds. We denote these asUB(Sin(n1,1))=−7.6andUB(Sin(n1,1))=7.6. We refer to them aslandu, respectively, for short hand. Then, it computes the line that passes through (u,u)and(0,l). Lettingy=UB(Sin(n1,1)) be the upper bound equation of the ReLU input, it computes the upper bound of the ReLU output as UB(S(n1,1))=u u−l(y−l)= 0.95x1−0.95x2+3.81. When considering a single ReLU of a single network, convex approximation is simple because there are only three states that the neuron can be in, namely active, inactive, and unstable. Fur thermore, in only one of these states, convex approximation is needed. In contrast, differential verification has to consider a pair of neurons, which has up to nine states to consider between the two ReLUs. Furthermore, different states may result in different lin ear approximations, and some states can even have multiple linear approximations depending on the difference bound of ∆=n′−n. As we will show in Section 4, there are significantly more consider ations in our problem domain. 4 OUR APPROACH We first present our baseline procedure for differential verification of feedforward neural networks (Section 4.1), and then present our algorithms for computing convex approximations (Section 4.3) and introducing symbolic variables (Section 4.4). 4.1 Differential Verification – Baseline We build off the work of Paulsen et al. [ 33], so in this section we review the relevant pieces. We assume that the input to NeuroD iffconsists of two networks fand f′, each with llayers of the same size. Let n′ k,jinf′be the neuron paired with nk,jinf. This implicitly creates a pairing of the edge weights between the two networks. We first introduce additional notation. •We denote the difference between a pair of neurons as ∆k,j= n′ k,j−nk,j. For example, ∆1,1=0.1under the input x1= 2,x2=1in our motivating example shown in Figure 2. •We denote the difference in a pair of edge weights as W∆ k[i,j]= W′ k[i,j]−Wk[i,j]. For example, W∆ 1[1,1]=2.0−1.9=0.1. •We extend the symbolic interval notation to these terms. That is, Sin(∆k,j)denotes the interval that bounds n′ k,j−nk,j before applying ReLU, and S(∆k,j)denotes the interval after applying ReLU. Given that we have computed S(nk−1,i),S(n′ k−1,i),S(∆k−1,i)for every neuron in the layer k−1, now, we compute a single S(∆k,j) in the subsequent layer kin two steps (and then repeat for each 1≤j≤lk). First, we compute Sin(∆k,j)by propagating the output intervals from the previous layer through the edges connecting to the target 5ASE ’20, September 21–25, 2020, Virtual Event, Australia Brandon Paulsen, Jingbo Wang, Jiawei Wang, and Chao Wang (x−l)u−l′ u−l+l′ (x−u)u′−l u−l+u′l u Figure 7: Illustration of Lemmas 4.1 and 4.2. neuron. This is defined as Sin(∆k,j)=Õ i S(∆k−1,i)×W′ k[i,j]+S(nk−1,i)×W∆ k−1[i,j] We illustrate this computation on node ∆1,1in our example. First, we initialize S(∆0,1)=[0,0],S(∆0,2)=[0,0]. Then we compute Sin(∆1,1)=[0,0]×2.0+[x1,x1]×0.1+[0,0]×− 2.0+[x2,x2]×− 0.1= [0.1x1−0.1x2,0.1x1−0.1x2]. For the second step, we apply ReLU to Sin(∆k,j)to obtain S(∆k,j). This is where we apply the new convex approximations (Section 4.3) to obtain tighter bounds. Toward this end, we will focus on the following two equations: z1=ReLU(nk,j+∆k,j)−ReLU(nk,j) (1) z2=ReLU(n′ k,j)−ReLU(n′ k,j−∆k,j) (2) While Paulsen et al. [ 33] also compute bounds of these two equa tions, they use concretizations instead of linear approximations , thus throwing away all the symbolic information. For the running exam ple, their method would result in the bounds of S(∆1,1)=[−.4, .4]. In contrast, our method will be able to maintain some or all of the symbolic information, thus improving the accuracy. 4.2 Two Useful Lemmas Before presenting our new linear approximations, we introduce two useful lemmas, which will simplify our presentation as well as our soundness proofs. Lemma 4.1. Letxbe a variable such that l≤x≤ufor constants l≤0and 0≤u. For a constant l′such that l≤l′≤0, we have x≤(x−l)∗u−l′ u−l+l′≥l′. Lemma 4.2. Letxbe a variable such that l≤x≤ufor constants l≤0and 0≤u. For a constant u′such that 0≤u′≤u, we have u′≥(x−u)∗u′−l u−l+u′≤x. We illustrate these lemmas in Figure 7. The solid blue line shows the equation y=xfor the input interval l≤x≤u. The upper dashed line illustrates the transformation of Lemma 4.1, and the lower dashed line illustrates Lemma 4.2. Specifically, Lemma 4.1 shows a transformation applied to xwhose result is always greater than both l′andx. Similarly, Lemma 4.2 shows a transformation applied to xwhose result is always less than both u′andx. These lemmas will be useful in bounding Equations 1 and 2.4.3 New Convex Approximations for S(∆k,j) Now, we are ready to present our new approximations, which are linear symbolic expressions derived from Equations 1 and 2. We first assume that nk,jandn′ k,jcould both be unstable, i.e., they could take values both greater than and less than 0. This yields bounds for the general case in that they are sound in all states of nk,j andn′ k,j(Sections 4.3.1 and 4.3.2). Then, we consider special cases ofnk,jandn′ k,j, in which even tighter upper and lower bounds are derived (Section 4.3.3). To simplify notation, we let n,n′,and∆stand in for nk,j,n′ k,j, and∆k,jin the remainder of this section. 4.3.1 Upper Bound for the General Case. Letl=UB(Sin(∆))and u=UB(Sin(∆)). The upper bound approximation is: UB(S(∆))=  UB(Sin(∆)) UB(Sin(∆))≥ 0 0 UB(Sin(∆))≤ 0 (UB(Sin(∆))−l)∗u u−lotherwise That is, when the input’s (delta) upper bound is greater than 0 for all x∈X, we can use the input’s upper bound unchanged. When the upper bound is always less than 0, the new output’s upper bound is then 0. Otherwise, we apply a linear transformation to the upper bound, which results in the upper plane illustrated in Figure 5. We prove all three cases sound. Proof. We consider each case above separately. In the following, we use Equation 1 to derive the bounds, but we note a symmetric proof using Equation 2 exists and produces the same bounds. Case 1: UB(Sin(∆))≥ 0.We first show that, according to Equa tion 1, when 0≤∆we have z1≤∆. This then implies that, if UB(Sin(∆))≥ 0, then z1≤UB(Sin(∆))(x)for all x∈X, and hence it is a valid upper bound for the output interval. Assume 0≤∆. We consider two cases of n. First, consider 0≤n. Observe 0≤n∧0≤∆=⇒ 0≤n+∆. Thus, the ReLU’s of Equation 1 simplify to z1=n+∆−n=∆=⇒z1≤∆. When n<0, Equation 1 simplifies to z1=ReLU(n+∆). Since n<0, we have n+∆≤∆∧0≤∆=⇒ReLU(n+∆) ≤∆. Thus, z1=ReLU(n+∆)≤∆, so the approximation is sound. Case 2: UB(Sin(∆))≤ 0.This case was previously proven [ 33], but we restate it here. UB(Sin(∆)) ≤ 0⇐⇒ n′≤n=⇒ ReLU(n′)≤ReLU(n) ⇐⇒ ReLU(n′)−ReLU(n)≤0. Case 3. By case 1, any UB(S(∆))that satisfies UB(S(∆))(x)≥0 andUB(S(∆))(x)≥UB(Sin(∆))(x)for all x∈Xis sound. Both in equalities hold by Lemma 4.1, with x=UB(Sin(∆)),l=UB(Sin(∆)), u=UB(Sin(∆))andl′=0. □ We illustrate the upper bound computation on node n1,1of our motivating example. Recall that UB(Sin(n1,1))=0.1x1−0.1x2. Since UB(Sin(n1,1))=−0.4andUB(Sin(n1,1))=0.4, we are in the third case of our linear approximation above. Thus, we have UB(Sin(n1,1))=(0.1x1−0.1x2−(− 0.4))∗0.4 0.4−(−0.4)=0.5x1−0.5x2+ 0.2. This is the upper bounding plane illustrated in Figure 5. The volume under this plane is 50% less than the upper bounding plane ofReluDiff shown in Figure 4. 6ASE ’20, September 21–25, 2020, Virtual Event, Australia 4.3.2 Lower Bound for the General Case. Letl=LB(Sin(∆))and u=LB(Sin(∆)), the lower bound approximation is: LB(S(∆))=  LB(Sin(∆)) LB(Sin(∆))≤ 0 0 LB(Sin(∆))≥ 0 (LB(Sin(∆))−u)∗−l u−lotherwise That is, when the input lower bound is always less than 0, we can leave it unchanged. When it is always greater than 0, the new lower bound is then 0. Otherwise, we apply a transformation to the lower bound, which results in the lower plane illustrated in Figure 5. We prove all three cases sound. Proof. We consider each case above separately. In the following, we use Equation 1 to derive the bounds, but we note a symmetric proof using Equation 2 exists and produces the same bounds. Case 1: LB(Sin(∆))≤ 0.We first show that according to Equa tion 1, when ∆≤0we have ∆≤z1. This then implies that, if LB(Sin(∆))≤ 0, we have LB(Sin(∆))(x)≤z1for all x∈X, and hence it is a valid lower bound for the output interval. Assume ∆≤0. We consider two cases of n+∆. First, let 0≤ n+∆. Observe 0≤n+∆∧∆≤0=⇒ 0≤n, so we can simplify Equation 1 to z1=n+∆−n=∆=⇒∆≤z1. Second, letn+∆<0⇐⇒ ∆<−n. Then, Equation 1 simplifies to z1=−ReLU(n)=−max(0,n)=min(0,−n). Now observe ∆< −n∧∆<0=⇒∆<min(0,−n)=z1. Case 2: LB(Sin(∆))≥ 0.This case was previously proven sound [33], but we restate it here. LB(Sin(∆))≥ 0⇐⇒ n′≥n=⇒ ReLU(n′)≥ReLU(n) ⇐⇒ ReLU(n′)−ReLU(n)≥0. Case 3. By case 1, any LB(S(∆))that satisfies LB(S(∆))(x)≤ 0 andLB(S(∆))(x)≤LB(Sin(∆))(x)for all x∈Xwill be valid. Both inequalities hold by Lemma 4.2, with x=LB(Sin(∆)),u′=0,l= LB(Sin(∆)),andu=LB(Sin(∆)). □ We illustrate the lower bound computation on node n1,1of our motivating example. Recall that LB(Sin(n1,1))=0.1x1−0.1x2. Since LB(Sin(n1,1))=−0.4andLB(Sin(n1,1))=0.4, we are in the third case of our linear approximation. Thus, we have LB(S(n1,1))= (0.1x1−0.1x2−(− 0.4))∗−(−0.4) 0.4−(−0.4)=0.05x1−0.05x2−0.2. This is the lower bounding plane illustrated in Figure 5. The volume above this plane is 50% less than the lower bounding plane of ReluDiff shown in Figure 4. 4.3.3 Tighter Bounds for Special Cases. While the bounds pre sented so far apply in all states of nandn′, under certain con ditions, we are able to tighten these bounds even further. Toward this end, we restate the following two lemmas proved by Paulsen et al. [ 33], which will come in handy. They are related to properties of Equations 1 and 2, respectively. Lemma 4.3. ReLU(n+∆)−n≡max(−n,∆) Lemma 4.4. n′−ReLU(n′−∆)≡min(n′,∆) These lemmas provide bounds when nandn′are proved to be linear based on the absolute bounds that we compute. Figure 8: Tighter upper bounding plane. Figure 9: Tighter lower bounding plane. Tighter Upper Bound When n′Is Linear. In this case, we have UB(S(∆))=UB(Sin(∆)), which is an improvement for the second or third case of our general upper bound. Proof. By our case assumption, Equation 2 simplifies to the one in Lemma 4.4. Thus, z2=min(n′,∆)=⇒z2≤∆. □ Tighter Upper Bound When nIs Linear, UB(Sin(∆))≤− LB(Sin(n)) ≤UB(Sin(∆)).We illustrate the z1plane under these constraints in Figure 8. Let l=UB(Sin(∆)), and let u=UB(Sin(∆)), and l′= −LB(Sin(n)), we use Lemma 4.1 to derive UB(S(∆))=(UB(Sin(∆))− l)∗u−l′ u−l+l′. This results in the upper plane of Figure 8. This improves over the third case in our general upper bound because it allows the lower bound of UB(S(∆))to be less than 0. Proof. By our case assumption, Equation 1 simplifies to the one in Lemma 4.3. By Lemma 4.1, we have for all x∈X,UB(S(∆))(x)≥ −LB(Sin(n))andUB(S(∆))(x)≥UB(Sin(∆))(x). These two inequal ities imply UB(S(∆))≥max(−n,∆). □ Tighter Lower Bound When nIs Linear. Here, we can use the approximation LB(S(∆))=LB(Sin(∆)). This improves over the second and third cases of our general lower bound. Proof. By our case assumption, Equation 1 simplifies to the one in Lemma 4.3. Thus, z1=max(−n,∆)=⇒z1≥∆. □ Tighter Lower Bound when n′is Linear, LB(Sin(∆))≤LB(Sin(n′)) ≤LB(Sin(∆)).We illustrate the z2plane under these constraints in Figure 9. Here, letting l=LB(Sin(∆)),u=LB(Sin(∆)), and u′= LB(Sin(n′)), we can use Lemma 4.2 to derive the approximation LB(S(∆))=(LB(Sin(∆))−u)∗u−l′ u−l+u′. This results in the lower plane of Figure 9. This improves over the third case, since it allows the upper bound to be greater than 0. 7ASE ’20, September 21–25, 2020, Virtual Event, Australia Brandon Paulsen, Jingbo Wang, Jiawei Wang, and Chao Wang Proof. By our case assumption, Equation 2 simplifies to the one shown in Lemma 4.4. By Lemma 4.2, we have for all x∈ X,LB(S(∆))(x)≤ LB(Sin(∆))(x)andLB(S(∆))(x)≤ LB(Sin(n′)). These two inequalities imply LB(S(∆))(x)≤min(n′,∆). □ 4.4 Intermediate Symbolic Variables for S(∆) While convex approximations reduce the error introduced by ReLU, even small errors tend to be amplified significantly after a few layers. To combat the error explosion, we introduce new symbolic terms to represent the output values of unstable neurons, which allow their accumulated errors to cancel out. We illustrate the impact of symbolic variables on n1,1of our motivating example. Recall we have S(∆1,1)=[0.05x1−0.05x2−0.2, 0.05x1−0.05x2+0.2]. After applying the convex approximation, we introduce a new variable x3such that x3=[0.05x1−0.05x2−0.2, 0.05x1−0.05x2+0.2]. Then we set S(∆1,1)=[x3,x3], and propagate this interval as before. After propagating through n2,1andn2,2and combining them at n3,1, the x3terms partially cancel out, resulting in the tighter final output interval [−1.65,1.18]. In principle, symbolic variables may be introduced at any unsta ble neurons that introduce approximation errors, however there are efficiency vs. accuracy tradeoffs when introducing these sym bolic variables. One consideration is how to deal with intermediate variables referencing other intermediate variables. For example, if we decide to introduce a variable x4forn2,1, then x4will have anx3term in its equation. Then, when we are evaluating a sym bolic bound that contains an x4term, which will be the case for n3,1, we will have to recursively substitute the bounds of the pre vious intermediate variables, such as x3. This becomes expensive, especially when it is used together with our bisectionbased refine ment [ 33,48]. Thus, in practice, we first remove any backreferences to intermediate variables by substituting in their lower bounds and upper bounds into the new intermediate variable’s lower and upper bounds, respectively. Given that we do not allow backreferences, there are two ad ditional considerations. First, we must consider that introducing a new intermediate variable wipes out all the other intermediate variables. For example, introducing a new variable at n2,1wipes out references to x3, thus preventing any x3terms from canceling atn3,1. Second, the runtime cost of introducing symbolic variables is not negligible. The bulk of computation time in NeuroDiff is spent multiplying the network’s weight matrices by the neuron’s symbolic bound equations, which is implemented using matrix mul tiplication. Since adding variables increases the matrix size, this increases the matrix multiplication cost. Based on these considerations, we have developed heuristics for adding new variables judiciously. First, since the errors introduced by unstable neurons in the earliest layers are the most prone to explode, and hence benefit the most when we create variables for them, we rank them higher when choosing where to add symbolic variables. Second, we bound the total number of symbolic variables that may be added, since our experience shows that introducing symbolic variables for the earliest Nunstable neurons gives drastic improvements in both run time and accuracy. In practice, Nis set to a number proportional to the weighted sum of unstable neuronsin all layers. Formally, N=ΣL k=1γk×Nk, where Nkis the number of unstable neurons in layer kandγk=1 kis the discount factor. 5 EXPERIMENTS We have implemented NeuroDiff and compared it with ReluD iff[33], the stateoftheart tool for differential verification of neural networks. NeuroDiff builds upon the codebase of ReluD iff[32], which was also used by singlenetwork verification tools such as ReluVal [48] and Neurify [47]. All use OpenBLAS [ 55] to optimize the symbolic interval arithmetic (namely in applying the weight matrices to the symbolic intervals). We note that NeuroDiff uses the algorithm from Neurify to compute S(nk,j)andS(n′ k,j), whereas ReluDiff uses the algorithm of ReluVal . Since Neurify is known to compute tighter bounds than ReluVal [47], we compare to both ReluDiff , and an upgraded version of ReluDiff which uses the bounds from Neurify to ensure that any performance gain is due to our optimizations and not due to using Neurify ’s bounds. We use the name ReluDiff+ to refer to ReluDiff upgraded with Neurify ’s bounds. 5.1 Benchmarks Our benchmarks consist of the 49 feedforward neural networks used by Paulsen et al. [ 33], taken from three applications: aircraft collision avoidance, image classification, and human activity recog nition. We briefly describe them here. As in Paulsen et al. [ 33], the second network f′is generated by truncating the edge weights of ffrom 32 bit to 16 bit floats. ACAS Xu [ 16].ACAS (aircraft collision avoidance system) Xu is a set of fortyfive neural networks, each with five inputs, six hidden layers of 50 units each, and five outputs, designed to advise a pilot (the ownship) how to steer an aircraft in the presence of an intruder aircraft. The inputs describe the position and speed of the intruder relative to the ownship, and the outputs represent scores for different actions that the ownship should take. The scores range from[−0.5,0.5]. We use the input regions defined by the properties of previous work [17, 48]. MNIST [ 21].MNIST is a standard image classification task, where the goal is to correctly classify 28×28pixel greyscale images of handwritten digits. Neural networks trained for this task take 784 inputs (one for each pixel) each in the range [0,255], and compute ten outputs – one score for each of the ten possible digits. We use three networks of size 3x100 (three hidden layers of 100 neurons each), 2x512, and 4x1024 taken from Weng et al. [ 49] and Wang et al.[47]. All achieve at least 95% accuracy on holdout test data. Human Activity Recognition (HAR) [ 1].The goal for this task is to classify the current activity of human (e.g. walking, sitting, laying down) based on statistics from a smartphone’s gyroscopic sensors. Networks trained on this task take 561 statistics computed from the sensors and output six scores for six different activities. We use a 1x500 network. 5.2 Experimental Setup Our experiments aim to answer the following research questions: (1) Is NeuroDiff significantly faster than stateoftheart? 8ASE ’20, September 21–25, 2020, Virtual Event, Australia Figure 10: Comparing the execution times of NeuroDiff andReluDiff+ on all verification tasks. (2) Is NeuroDiff ’s forward pass significantly more accurate? (3) Can NeuroDiff handle significantly larger input regions? (4)How much does each technique contribute to the overall improvement? To answer these questions, we run both NeuroDiff andReluD iff/ReluDiff+ on all benchmarks and compare their results. Both NeuroDiff andReluDiff /ReluDiff+ can be parallelized to use multithreading, so we configure a maximum of 12 threads for all experiments. Our experiments are run on a computer with an AMD Ryzen Threadripper 2950X 16core processor, with a 30minute timeout per differential verification task. While we could try and adapt a singlenetwork verification tool to our task as done previously [ 33], we note that ReluDiff has been shown to significantly outperform (by several orders of magnitude) this naive approach. 5.3 Results In the remainder of this section, we present our experimental results in two steps. First, we present the overall verification results on all benchmarks. Then, we focus on the detailed verification results on the more difficult verification tasks. 5.3.1 Summary of Results on All Benchmarks. Our experimental results show that, on all benchmarks, the improved ReluDiff+ slightly but consistently outperforms the original ReluDiff due to its use of the more accurate component from Neurify instead of ReluVal for bounding the absolute values of individual neurons. Thus, to save space, we will only show the results that compare NeuroDiff (our method) and ReluDiff+ . We summarize the comparison between NeuroDiff andReluD iff+ using a scatter plot in Figure 10, where each point represents a differential verification task: the xaxis is the execution time of NeuroDiff in seconds, and the yaxis the execution time of ReluD iff+ in seconds. Thus, points on the diagonal line are ties, while points above the diagonal line are wins for NeuroDiff . The results show that NeuroDiff outperformed ReluDiff+ for most verification tasks. Since the execution time is in logrithmic scale the speedups of NeuroDiff are more than 1000X for many of these verification tasks. While there are cases where NeuroDiff is slower than ReluDiff+ , due to the overhead of adding symbolic variables, the differences are on the order of seconds. Since theyTable 1: Results for ACAS networks with ϵ=0.05. PropertyNeuroDiff (new) ReluDiff+Speedupproved undet. time (s) proved undet. time (s) φ1 45 0 522.6 44 1 4800.6 9.2 φ3 42 0 2.3 42 0 4.1 1.8 φ4 42 0 1.7 42 0 2.8 1.7 φ5 1 0 0.2 1 0 0.2 1.4 φ6 2 0 0.6 2 0 0.4 0.7 φ7 1 0 1404.4 0 1 1800.0 1.3 φ8 1 0 132.2 1 0 361.8 2.7 φ9 1 0 0.6 1 0 2.3 3.7 φ10 1 0 0.9 1 0 0.7 0.8 φ11 1 0 0.2 1 0 0.3 1.6 φ12 1 0 2.8 1 0 360.9 129.4 φ13 1 0 5.8 1 0 5.1 0.9 φ14 2 0 0.5 2 0 95.9 196.2 φ15 2 0 0.6 2 0 65.0 113.2 Table 2: Results for ACAS networks with ϵ=0.01. PropertyNeuroDiff (new) ReluDiff+Speedupproved undet. time (s) proved undet. time (s) φ1 41 4 11400.1 15 30 55778.6 4.9 φ3 42 0 14.3 35 7 13642.2 957.2 φ4 42 0 3.8 37 5 9115.0 2390.1 φ5 1 0 0.3 0 1 1800.0 5520.5 φ16 2 0 1.0 2 0 0.8 0.8 φ7 0 1 1800.0 0 1 1800.0 1.0 φ8 1 0 1115.9 0 1 1800.0 1.6 φ9 1 0 2.4 0 1 1800.0 738.2 φ10 1 0 1.6 1 0 1.1 0.7 φ11 1 0 0.3 0 1 1800.0 5673.8 φ12 1 0 132.2 0 1 1800.0 13.6 φ13 1 0 15.9 1 0 14.8 0.9 φ14 2 0 1589.3 0 2 3600.0 2.3 φ15 2 0 579.4 0 2 3600.0 6.2 are all on the small MNIST networks and the HAR network that are very easy for both tools, we omit an indepth analysis of them. In the remainder of this section, we present an indepth analysis of the more difficult verification tasks. 5.3.2 Results on ACAS Networks. For ACAS networks, we consider two different sets of properties, namely the original properties from Paulsen et al. [ 33] whereϵ=0.05, and the same properties but with ϵ=0.01. We emphasize that, while verifying ϵ=0.05is useful, this means that the output value can vary by up to 10%. Considering ϵ=0.01means that the output value can vary by up to 2%, which is much more useful. Our results are shown in Tables 1 and 2, where the first column shows the property, which defines the input space considered. The next three columns show the results for NeuroDiff , specifically the number of verified networks (out of the 45 networks), the number of unverified networks, and the total run time across all networks. The next three show the same results, but for ReluDiff+ . The final column shows the average speed up of NeuroDiff . The results show that NeuroDiff makes significant gains in both speed and accuracy. Specifically, the speedups are up to two and three orders of magnitude for ϵ=0.05and 0.01, respectively. In addition, at the more accurate ϵ=0.01level, NeuroDiff is able to complete 53 more verification tasks, out of the total 142 verification tasks. 9ASE ’20, September 21–25, 2020, Virtual Event, Australia Brandon Paulsen, Jingbo Wang, Jiawei Wang, and Chao Wang Figure 11: Percentage of verification tasks completed on the MNIST 4x1024 network for various perturbations. Figure 12: Accuracy comparison for a single forward pass on the MNIST 4x1024 network with perturbation of 8. 5.3.3 Results on MNIST Networks. For MNIST, we focus on the 4x1024 network, which is the largest network considered by Paulsen et al. [ 33]. In contrast, since the smaller networks, namely 3x100 and 2x512 networks, were handled easily by both tools, we omit their results. In the MNISTrelated verification tasks, the goal is to verifyϵ=1for the given input region. We consider the two types of input regions from the previous work, namely global perturbations and targeted pixel perturbations, however we use input regions that are hundreds of orders of magnitude larger. First, we look at the global perturbation. For these, the input space is created by taking an input image and then allowing a per turbation of +/ pgreyscale units to all of its pixels. In the previous work, the largest perturbation was p=3. Figure 11 compares Neu roDiff andReluDiff+ onp=3all the way up to 8, where the xaxis is the perturbation applied, and the yaxis is the percentage of verification tasks (out of 100) that each can handle. The results show that NeuroDiff can handle perturbations up to +/ 6 units, whereas ReluDiff+ begins to struggle at 4. While the difference between 4 and 6, may seem small, the volume of input space for a perturbation of 6 is 6784/4784≈1.1×10138times larger than 4, or in other words, 138 orders of magnitude larger. Next, we show a comparison of the epsilon verified by a single forward pass for a perturbation of 8 on the MNIST 4x1024 network in Figure 12. Points above the blue line indicate NeuroDiff per formed better. Overall, NeuroDiff is between two and three times more accurate than ReluDiff+ . Finally, we look at the targeted pixel perturbation properties. For these, the input space is created by taking an image, randomly choosing npixels, and setting there bounds to [0,255], i.e., allowing arbitrary changes to the chosen pixels. We again use the 4x1024 MNIST network. The results are summarized in Table 3. The first column shows the number of randomly perturbed pixels. We can again see very large speedups, and a significant increase in the size of the input region that NeuroDiff can handle. 5.3.4 Contribution of Each Technique. Here, we analyze the con tribution of individual techniques, namely convex approximations and symbolic variables, to the overall performance improvement. In Table 4, we present the average ϵthat was able to be verified after a single forward pass on the 4x1024 MNIST network for each of the four techniques: ReluDiff+ (baseline), NeuroDiff withTable 3: Results of the MNIST 4x1024 pixel experiment. Num. PixelsNeuroDiff (new) ReluDiff+Speedupproved undet. time (s) proved undet. time (s) 15 100 0 236.5 100 0 1610.2 6.8 18 100 0 540.8 88 12 34505.8 63.8 21 100 0 1004.0 30 70 145064.5 144.5 24 99 1 7860.1 1 99 179715.9 22.9 27 83 17 49824.0 0 100 180000.0 3.6 Table 4: Evaluating the individual contributions of convex approximation and symbolic variables using the MNIST 4x1024 global perturbation experiment. PerturbAverageϵVerified ReluDiff+ Conv. Approx. Int. Vars. NeuroDiff 3 0.59 0.42 (+1.39x) 0.43 (+1.38x) 0.20 (+2.93x) 4 1.02 0.70 (+1.46x) 0.87 (+1.18x) 0.36 (+2.85x) 5 1.60 1.06 (+1.52x) 1.47 (+1.09x) 0.56 (+2.87x) 6 2.29 1.47 (+1.55x) 2.19 (+1.04x) 0.79 (+2.90x) 7 3.02 1.92 (+1.58x) 2.96 (+1.02x) 1.04 (+2.91x) 8 3.80 2.39 (+1.59x) 3.77 (+1.01x) 1.30 (+2.93x) only convex approximations, NeuroDiff with only intermediate variables, and the full NeuroDiff . Overall, the individual benefits of the two proposed approxima tion techniques are obvious. While convex approximation (alone) consistently provides benefit as perturbation increases, the ben efit of symbolic variables (alone) tends to decrease. In addition, combining the two provides much greater benefit than the sum of their individual contributions. With perturbation of 8, for example, convex approximations alone are 1.59 times more accurate than ReluDiff+ , and intermediate variables alone are 1.01 times more accurate. However, together they are 2.93 times more accurate. The results suggest two things. First, intermediate symbolic vari ables perform well when a significant portion of the network is already in the stable state. We confirm, by manually inspecting the experimental results, that it is indeed the case when we use a per turbation of 3 and 8 in the MNIST experiments. Second, the convex approximations provide the most benefit when the preReLU delta intervals are (1) significantly wide, and (2) still contain a significant 10ASE ’20, September 21–25, 2020, Virtual Event, Australia amount of symbolic information. This is also confirmed by man ually inspecting our MNIST results: increasing the perturbation increases the overall width of the delta intervals. 6 RELATED WORK "
368,SelfKin: Self Adjusted Deep Model For Kinship Verification.txt,"One of the unsolved challenges in the field of biometrics and face
recognition is Kinship Verification. This problem aims to understand if two
people are family-related and how (sisters, brothers, etc.) Solving this
problem can give rise to varied tasks and applications. In the area of homeland
security (HLS) it is crucial to auto-detect if the person questioned is related
to a wanted suspect, In the field of biometrics, kinship-verification can help
to discriminate between families by photos and in the field of predicting or
fashion it can help to predict an older or younger model of people faces.
Lately, and with the advanced deep learning technology, this problem has gained
focus from the research community in matters of data and research. In this
article, we propose using a Deep Learning approach for solving the
Kinship-Verification problem. Further, we offer a novel self-learning deep
model, which learns the essential features from different faces. We show that
our model wins the Recognize Families In the Wild(RFIW2018,FG2018) challenge
and obtains state-of-the-art results. Moreover, we show that our proposed model
can reduce the size of the network by half without loss in performance.","The goal of Kinship Veriﬁcation is to determine if two people are related and how (i.e., brothers, sisters, etc.).An automatic system that will verify the relation between two people can be beneﬁcial in different areas. Such an automated system can help in ﬁnding the family of a known suspect; it can help determine the family of a lost child. In the ﬁeld of biometrics, we can use such a concept in building a uniﬁed identity database per family and so on. Although the potential for solving this problem and Although there is an increasing interest in the computerscience community for it, the progress made so far is limited and usually implemented in spe ciﬁc cases and handpicked scenarios. Recently, the most signiﬁcant dataset for kinship veriﬁcation was introduced [1] The dataset includes different types of kinship which serves as a resource for research, the Faculty of Engineering, Bar Ilan University, Israel. eran.dahan.ee@gmail.com. Faculty of Engineering, Bar Ilan University, Israel. yosi.keller@gmail.com. Fig. 1. Illustration of the method proposed in the paper, usi ng the face features from face recognition for the task of face veri ﬁcation. The weighted face features are selflearned for the task of k inship veriﬁcation as will be explained in the paper dataset was released along with a challenge (Recognize Families In the Wild, FG2018) to build a classiﬁer for different Kinship types automatically. One of the promising methods for solving the Kinship problem is with using deep learning. Deep networks can learn a different representation of faces according to a speciﬁc task (i.e., face recognition, age estimation, etc.). When investigating the Kinship Veriﬁcation task one can see that we do not have a theory for known features to be extracted from the faces for this task. That is unlike the well studied Face Recognition task, where there are known features that were considered and proven to be accurate for building Face Recog nition systems. Furthermore, we can not point what makes us decide that two people are related, so it is impossible to code this knowledge or theory to an automated system. The last justiﬁcation was motivated us to use Deep Learning methods that can be thought by examples, and not by a theory to learn the features and classiﬁer of KinshipVeriﬁcation problem. Deep Learning methods can be implemented in dif ferent scenarios and setups. In the training phase, wecan distinguish between a relaxed scenario and a strict scenario, in the relaxed scenario we have information about each ID (i.e., tag ID for each photo). The ID information can be used to build our dataset, (i.e., building more negative examples or even to ﬁne tune our face recognition model to extract more accurate face features). In literature, it is also called the image unrestricted scenario. In the strict scenario, the dataset is composed of only having the examples of a Kin and nonkin photos. In literature, it is also called the imagerestricted scenario. We can also distinguish between relaxed and strict scenarios in the testing phase. The relaxed scenario is where we have multiple photos per ID, and we want to jointly classify them as related to some other multiple other ID’s photos. The strict scenario is where we need to make a decision only on a pair of unique photos. In our research, we challenged the strict in the notation as explained above. The illustration of our proposed method and concept can be seen in ﬁgure 1 We can summarize our main contribution as follows: 1) We propose a novel method to make the net work selflearn the needed features to complete the task of FaceVeriﬁcation. Furthermore, we show that by learning those speciﬁc features we can reduce the number of parameters that the model uses by half with almost no reduction in performance. 2) We propose a local and global classiﬁer to classify the different weighted features between two feature maps; we explain why this theory is needed in the task of Kinship Veriﬁcation. 3) We train our model in the strict scenario of the KinshipVeriﬁcation problem and show that our trained model wins the RFIW2018 (FG2018) challenge. The rest of the paper is arranged as follows: Section II will summarize the previous work on the area of kinship veriﬁcation; Section III, will describe in details the architecture we propose, focusing on the feature selection layer and the localglobal classiﬁer; Section IV will include computation describing the proposed network, loss function, forward and backward calcu lations; Section V will include training information in details and technical information for training the parameterf our network. Section VI, we describe our results on RFIW’s, and elaborate on showing the results for different setups of the proposed network; Section VII will conclude and discuss future work. II. R ELATED WORK "
338,Large-Scale Image Retrieval with Attentive Deep Local Features.txt,"We propose an attentive local feature descriptor suitable for large-scale
image retrieval, referred to as DELF (DEep Local Feature). The new feature is
based on convolutional neural networks, which are trained only with image-level
annotations on a landmark image dataset. To identify semantically useful local
features for image retrieval, we also propose an attention mechanism for
keypoint selection, which shares most network layers with the descriptor. This
framework can be used for image retrieval as a drop-in replacement for other
keypoint detectors and descriptors, enabling more accurate feature matching and
geometric verification. Our system produces reliable confidence scores to
reject false positives---in particular, it is robust against queries that have
no correct match in the database. To evaluate the proposed descriptor, we
introduce a new large-scale dataset, referred to as Google-Landmarks dataset,
which involves challenges in both database and query such as background
clutter, partial occlusion, multiple landmarks, objects in variable scales,
etc. We show that DELF outperforms the state-of-the-art global and local
descriptors in the large-scale setting by significant margins. Code and dataset
can be found at the project webpage:
https://github.com/tensorflow/models/tree/master/research/delf .","Largescale image retrieval is a fundamental task in com puter vision, since it is directly related to various practical applications, e.g., object detection, visual place recognition, and product recognition. The last decades have witnessed tremendous advances in image retrieval systems—from hand crafted features and indexing algorithms [22, 33, 27, 16] to, more recently, methods based on convolutional neural net works (CNNs) for global descriptor learning [2, 29, 11]. Despite the recent advances in CNNbased global descrip tors for image retrieval in small or mediumsize datasets [ 27, 28], their performance may be hindered by a wide variety DELF Pipeline LargeScaleIndex FeaturesDELF Features Query ImageDELF PipelineIndex  Query NN Features Attention Scores Database ImagesRetrieved ImagesGeometric VerificationFigure 1: Overall architecture of our image retrieval system, us ing DEep Local Features (DELF) and attentionbased keypoint selection. On the left, we illustrate the pipeline for extraction and selection of DELF. The portion highlighted in yellow represents an attention mechanism that is trained to assign high scores to relevant features and select the features with the highest scores. Feature extraction and selection can be performed with a single forward pass using our model. On the right, we illustrate our largescale featurebased retrieval pipeline. DELF for database images are indexed ofﬂine. The index supports querying by retrieving nearest neighbor (NN) features, which can be used to rank database images based on geometrically veriﬁed matches. of challenging conditions observed in largescale datasets, such as clutter, occlusion, and variations in viewpoint and illumination. Global descriptors lack the ability to ﬁnd patch level matches between images. As a result, it is difﬁcult to retrieve images based on partial matching in the presence of occlusion and background clutter. In a recent trend, CNN based local features are proposed for patchlevel matching [12,42,40]. However, these techniques are not optimized speciﬁcally for image retrieval since they lack the ability to detect semantically meaningful features, and show limited accuracy in practice. Most existing image retrieval algorithms have been evalu ated in small to mediumsize datasets with few query images, i.e., only 55 in [ 27,28] and 500 in [ 16], and the images in the datasets have limited diversity in terms of landmark lo cations and types. Therefore, we believe that the imagearXiv:1612.06321v4  [cs.CV]  3 Feb 2018retrieval community can beneﬁt from a largescale dataset, comprising more comprehensive and challenging examples, to improve algorithm performance and evaluation methodol ogy by deriving more statistically meaningful results. The main goal of this work is to develop a largescale image retrieval system based on a novel CNNbased feature descriptor. To this end, we ﬁrst introduce a new largescale dataset, GoogleLandmarks, which contains more than 1M landmark images from almost 13K unique landmarks. This dataset covers a wide area in the world, and is consequently more diverse and comprehensive than existing ones. The query set is composed of an extra 100K images with diverse characteristics; in particular, we include images that have no match in the database, which makes our dataset more challenging. This allows to assess the robustness of retrieval systems when queries do not necessarily depict landmarks. We then propose a CNNbased local feature with atten tion, which is trained with weak supervision using image level class labels only, without the need of object and patch level annotations. This new feature descriptor is referred to as DELF (DEep Local Feature), and Fig. 1 illustrates the overall procedure of feature extraction and image retrieval. In our approach, the attention model is tightly coupled with the proposed descriptor; it reuses the same CNN architecture and generates feature scores using very little extra computa tion (in the spirit of recent advances in object detection [ 30]). This enables the extraction of both local descriptors and key points via one forward pass over the network. We show that our image retrieval system based on DELF achieves the state oftheart performance with signiﬁcant margins compared to methods based on existing global and local descriptors. 2. Related Work "
107,To Frontalize or Not To Frontalize: Do We Really Need Elaborate Pre-processing To Improve Face Recognition?.txt,"Face recognition performance has improved remarkably in the last decade. Much
of this success can be attributed to the development of deep learning
techniques such as convolutional neural networks (CNNs). While CNNs have pushed
the state-of-the-art forward, their training process requires a large amount of
clean and correctly labelled training data. If a CNN is intended to tolerate
facial pose, then we face an important question: should this training data be
diverse in its pose distribution, or should face images be normalized to a
single pose in a pre-processing step? To address this question, we evaluate a
number of popular facial landmarking and pose correction algorithms to
understand their effect on facial recognition performance. Additionally, we
introduce a new, automatic, single-image frontalization scheme that exceeds the
performance of current algorithms. CNNs trained using sets of different
pre-processing methods are used to extract features from the Point and Shoot
Challenge (PaSC) and CMU Multi-PIE datasets. We assert that the subsequent
verification and recognition performance serves to quantify the effectiveness
of each pose correction scheme.","The advent of deep learning [28] methods such as con volutional neural networks (CNNs) has allowed face recog nition performance on hard datasets to improve signiﬁ cantly. For instance, Google FaceNet [39], a CNN based method, achieved over 99% veriﬁcation accuracy on the LFW dataset [19], which was once considered to be ex tremely challenging due to its unconstrained nature. Be cause CNNs possess the ability to automatically learn com * denotes equal contribution Figure 1: Examples of different preprocessing on a sample image (a) from the CASIAWebFace dataset [51]: (b) 2D aligned – no frontalization, (c) Zhu and Ramanan [55] & Hassner et al. [15], (d) Kazemi and Sullivan [23] & Hassner et al., (e) CMR & our frontalization method (OFM), (f) CMR & Hassner et al. [15], (g) Zhu and Ramanan [55] & OFM, and (h) Kazemi and Sullivan [23] & OFM. The left and right images are frontalized asym metrically and symmetrically respectively for (c), (d), (e), (f), (g) and (h). Note how different the results look for each approach. Does this difference impact face recognition performance? We seek to answer this question. plex representations of face data, they systematically out perform older methods based on handcrafted features. Since these representations are learned from the data it self, it is often assumed that we must provide CNNs well labelled, clean, preprocessed data for training [5]. Accord ingly, complex frontalization steps are thought to be inte gral to improving CNN performance [41]. However, with the use of a pose correction method comes many questions: How extreme of a pose can the frontalization method han dle? How high is its yield? Should the method enforce facial symmetry? Does training CNNs with frontalized im ages yield better results, or can they learn robust representa tions invariant of facial pose on their own? To answer these questions, we conducted an extensive comparative study of different facial preprocessing techniques. For this study, we used the CASIAWebFace (CW) [51] dataset for CNN training. Two frontalization techniques were chosen for our training and testing evaluation: the wellestablished method proposed by Hassner et al. (H) [15], and our own newly proposed method. FurtherarXiv:1610.04823v4  [cs.CV]  27 Mar 2018more, to evaluate the effect of facial landmarking on the frontalization process, we used three landmarking tech niques: Zhu and Ramanan (ZR) [55], Kazemi and Sulli van (KS) [23], and our own technique  a Cascade Mixture of Regressors (CMR). Different frontalization results using various combinations of these methods can be seen in Fig. 1. We used the popular VGGFACE [32] as our base archi tecture for training networks using different preprocessing strategies. The PaSC video dataset [38] was used for testing. We extracted face representations from individual video frames in PaSC using a network trained with a particular preprocessing strategy. These features were used for veri ﬁcation and recognition purposes by applying a cosine sim ilarity scorebased face matching procedure. As a set of baselines, we used  1) a simple 2D alignment that corrects for inplane rotation, 2) no preprocessing at all, and 3) a snapshot of the VGGFACE model [32] pre trained on the 2D aligned VGGFACE dataset. This was used to evaluate how much the additional training on CW improved the face representation capability of the CNN model. The effect of each data augmentation is manifested in the performance of each subsequent CNN model. The focus of our study was to evaluate the effect of frontalization on CNNbased face recognition instead of achieving near stateoftheart results on PaSC. Therefore, we chose not to study use any elaborate detection algorithm or scoring scheme like those used by most of the PaSC 2016 Challenge participants [38]. In summary, the contributions of this paper are: The evaluation of popular facial landmarking and frontalization methods to quantify their effect on videobased face recognition tasks using a CNN. A new, effective facial landmarking and frontalization technique for comparison with the other methods. An investigation of frontalization failure rates for each method as a function of facial pose using the CMU MultiPIE dataset [13]. 2. Related Work "
69,HyBNN and FedHyBNN: (Federated) Hybrid Binary Neural Networks.txt,"Binary Neural Networks (BNNs), neural networks with weights and activations
constrained to -1(0) and +1, are an alternative to deep neural networks which
offer faster training, lower memory consumption and lightweight models, ideal
for use in resource constrained devices while being able to utilize the
architecture of their deep neural network counterpart. However, the input
binarization step used in BNNs causes a severe accuracy loss. In this paper, we
introduce a novel hybrid neural network architecture, Hybrid Binary Neural
Network (HyBNN), consisting of a task-independent, general, full-precision
variational autoencoder with a binary latent space and a task specific binary
neural network that is able to greatly limit the accuracy loss due to input
binarization by using the full precision variational autoencoder as a feature
extractor. We use it to combine the state-of-the-art accuracy of deep neural
networks with the much faster training time, quicker test-time inference and
power efficiency of binary neural networks. We show that our proposed system is
able to very significantly outperform a vanilla binary neural network with
input binarization. We also introduce FedHyBNN, a highly communication
efficient federated counterpart to HyBNN and demonstrate that it is able to
reach the same accuracy as its non-federated equivalent. We make our source
code, experimental parameters and models available at:
https://anonymous.4open.science/r/HyBNN.","Deep Neural Networks (DNNs) have been extensively applied to an incredibly wide range of applications due to their ability to generate remarkable results on complex tasks once thought to be almost impossible such as face recognition, selfdriving systems, natural language processing (NLP), image analysis, weather prediction etc Liu et al. [2017]. They form the basis for most stateoftheart machine learning algorithms and are even capable of outperforming humans in some areas Yu et al. [2017]. Deep Neural Networks draw their success from being able to have hundred to thousands of layers consisting of millions or even billions of trainable parameters. For instance, GPT3 has over 175 billion parameters that have been estimated to cost over $12 million to train Floridi and Chiriatti [2020]. This is because training a DNN i.e. Deep Learning requires a long chain of expensive matrix and tensor computation consisting of millions of ﬂoatingpoint operations. Despite their success, a lot of resource constrained devices, such as mobile phones, IoT devices, embedded systems and other edge devices do not have the necessary computing capacity in terms of available processing power, memory, bandwidth or energy to sufﬁciently and quickly train a DNN. As such, training these networks require specialized highperformance hardware, like GPUs, which are expensive and consume an enormous amount of energy to operate. Preprint. Under review.arXiv:2205.09839v1  [cs.LG]  19 May 2022Optimizing deep learning algorithms to be lightweight enough for resourceconstrained devices is still an open and active research problem. There have been numerous proposals for the same, includ ing straightforward solutions like training smaller models, or more complex ones like knowledge distillation Hinton et al. [2015], parameter quantization Gong et al. [2014], parameter pruning Han et al. [2015], transferred convolution ﬁlters Zhang et al. [2018] and lowrank parameter factorization Wen et al. [2018]. Parameter Quantization is especially interesting, since it can greatly reduce the size and computations required for DNNs quickly and lets us reuse existing neural network architectures as is. Binary or Binarized Neural Networks (BNNs) are an extreme form of quantization with weights and activation constrained to 1(0) or +1 at runtime and for gradient computation during training Hubara et al. [2016]. Since all the weights and activations are binary, the runtime can be heavily reduced as the computa tionally expensive matrix operations can be replaced with simple XNOR and popcount operations Rastegari et al. [2016]. These lowlevel operations are highly optimized on most modern processors and can even be applied in parallel on processors that support SIMD (Single Instruction, Multiple Data) instructions. Furthermore, these networks can be easily constructed using electronic circuits via XNOR logic gates, greatly improving performance at a fraction of the cost of a GPU. There are however, challenges that limit the widespread adoption of BNNs in real world scenarios. Since the inputs are realvalued they must be converted to binary using a binarization function, the most common is the sign function. sign (x) =+1;ifx0 "
435,A Security Policy Model Transformation and Verification Approach for Software Defined Networking.txt,"Software defined networking (SDN) has been adopted to enforce the security of
large-scale and complex networks because of its programmable, abstract,
centralized intelligent control and global and real-time traffic view. However,
the current SDN-based security enforcement mechanisms require network managers
to fully understand the underlying configurations of network. Facing the
increasingly complex and huge SDN networks, we urgently need a novel security
policy management mechanism which can be completely transparent to any
underlying information. That is it can permit network managers to define
upper-level security policies without containing any underlying information of
network, and by means of model transformation system, these upper-level
security policies can be transformed into their corresponding lower-level
policies containing underlying information automatically. Moreover, it should
ensure system model updated by the generated lower-level policies can hold all
of security properties defined in upper-level policies. Based on these
insights, we propose a security policy model transformation and verification
approach for SDN in this paper. We first present the formal definition of a
security policy model (SPM) which can be used to specify the security policies
used in SDN. Then, we propose a model transformation system based on SDN system
model and mapping rules, which can enable network managers to convert SPM model
into corresponding underlying network configuration policies automatically,
i.e., flow table model (FTM). In order to verify SDN system model updated by
the generated FTM models can hold the security properties defined in SPM
models, we design a security policy verification system based on model
checking. Finally, we utilize a comprehensive case to illustrate the
feasibility of the proposed approach.",".Currently softwaredened networking (SDN) [1] has been adopted to en force the security of largescale and complex networks because of its pro grammable, abstract, centralized intelligent control and global and realtime trac view. For instances, Garay et al [2] proposed a SDNbased network access control mechanism,  ownac, which is a centralized EAP (Extensible Authentication Protocol) based on terminal security authentication method for IEEE 802.1x wireless local area network (WLAN). Yakasai et al [3] pro posed the  owidentity network access control mechanism. This mechanism integrates EAP security authentication mechanism into SDN controller, and then uses custom rewall policy to control terminal access. Hu et al [4] pro posed a dynamic rewall mechanism based on SDN,  owguard. With this mechanism, network administrators can  exibly customize dierent types of rewall policies and authenticate dierent types of network devices. Ko erner et al [5] proposed a device security authentication mechanism based on MAC address and SDN. This mechanism uses the  oodlight controller to map the MAC address of the laptop to the corresponding VLAN address in the network. Because the MAC address is static, it can ensure the security authentication of the mobile workstation. However, current SDNbased secu rity enforcement mechanisms require network managers to fully understand the detailed underlying congurations of network (such as MAC address, IP address, VLAN ID, network type and etc.) and then load these security poli cies (such as access control policies or rewall policies) containing underlying information of the network into the SDN controller by means of control pro grams or manual input. However, facing the increasingly complex and huge SDN networks, traditional security policy management is becoming more and more dicult because it is nearly impossible for network managers to fully understand all of underlying congurations. Moreover, with the emergence of multicontroller SDN [6], network managers need to manage a variety of heterogeneous SDN controllers at the same time. In this case, the same se 2curity policy often needs to be developed and deployed for dierent types of controllers, which inevitably increases the complexity and diculty of net work management. Therefore, in the face of the increasingly complex and huge SDN networks, we urgently need a novel security policy management mechanism which can be completely transparent to any underlying informa tion of network. That is it can permit network managers to dene upperlevel security policies without containing any underlying information of network, and by means of model transformation system, these upperlevel security policies can be transformed into their corresponding lowerlevel policies con taining underlying information automatically. Moreover, it should ensure SDN system model updated by the generated lowerlevel policies can hold all of security properties dened in upperlevel policies. .Based on these insights, we propose a security policy model transformation and verication approach for SDN in this paper. The main idea of the approach is to specify the current security policies used in SDN, such as access control policies or rewall policies, as a unied security policy model (SPM). SPM is of an upperlevel policy model without containing any underlying information of SDN. Then, we establish SDN system model and use SDN system model to establish the mapping rules between the objects of SPM and the objects of SDN system models, and then use these mapping rules to automatically convert the SPM models into their corresponding lower level policy models containing underlying information of SDN, i.e.,  ow table model (FTM). To be practically useful, we must prove that SDN system model updated by the generated FTM can ensure all of security properties dened in SPM after model transformation. Thus, we design a verication algorithm, which validates the SDN system model updated by FTM by means of the security properties dened in SPM, i.e., validation conditions. If all of given validation conditions are proofed to be true after verication, it proves that the updated SDN system model can hold all of security properties dened in SPM, otherwise it cannot satisfy the security requirements. Hence, the main contributions of this paper can be concluded as follows: .We establish the system model of SDN in this paper, which includes terminal model (TM), OpenFlow switch model (SWM),  ow table model (FTM), network  ow model (NFM) and network topology model (NTM). .Since most of security policies used in SDN can be described as the problem whether the policy subject (user, service or terminal, etc.) can 3access or use the policy object (resource, service, data or terminal, etc.), thus we specify these security policies as a unied security policy model (SPM) in this paper. .Based on the established SDN system model, we further establish the mapping rules between the objects of SPM and the objects of SDN system models. Then, based on these mapping rules, we propose a model transfor mation system which can automatically transform SPM model without con taining any underlying information of network into its corresponding lower level policy model containing underlying information, i.e.,  ow table model (FTM). .In order to prove SDN system model updated by the generated FTM can hold all of security properties dened in SPM after model transformation, we design a security policy verication system based on model checking. The verication algorithm of system validates the SDN system model updated by generated FTM by means of the security properties dened in SPM, i.e., vali dation conditions. If all of given validation conditions are true, it proves that the updated SDN system model can hold all of security properties dened in SPM, otherwise it cannot satisfy the security requirements. .Finally, we utilize a comprehensive case to illustrate the feasibility of the proposed approach. We rst establish SDN system model and SPM of this case, then transform SPM into its corresponding FTM using model transformation algorithm, and verify the generated FTM can hold all of security properties dened in SPM after model transformation using security policy verication algorithm. .The remainder of the paper is structured as follows. Section 2 discuss some related works. Section 3 is the main body of this paper, which includes the framework of the proposed approach, SDN system model, security policy model, model transformation system and security policy verication system using model checking. In section 4, we utilize a comprehensive case to illus trate the feasibility of approach. Finally, Section 5 concludes this paper and presents some future directions. 2. Related Work "
209,A Raspberry Pi-based Traumatic Brain Injury Detection System for Single-Channel Electroencephalogram.txt,"Traumatic Brain Injury (TBI) is a common cause of death and disability.
However, existing tools for TBI diagnosis are either subjective or require
extensive clinical setup and expertise. The increasing affordability and
reduction in size of relatively high-performance computing systems combined
with promising results from TBI related machine learning research make it
possible to create compact and portable systems for early detection of TBI.
This work describes a Raspberry Pi based portable, real-time data acquisition,
and automated processing system that uses machine learning to efficiently
identify TBI and automatically score sleep stages from a single-channel
Electroen-cephalogram (EEG) signal. We discuss the design, implementation, and
verification of the system that can digitize EEG signal using an Analog to
Digital Converter (ADC) and perform real-time signal classification to detect
the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN)
and XGBoost based predictive models to evaluate the performance and demonstrate
the versatility of the system to operate with multiple types of predictive
models. We achieve a peak classification accuracy of more than 90% with a
classification time of less than 1 s across 16 s - 64 s epochs for TBI vs
control conditions. This work can enable development of systems suitable for
field use without requiring specialized medical equipment for early TBI
detection applications and TBI research. Further, this work opens avenues to
implement connected, real-time TBI related health and wellness monitoring
systems.","1.1. Background   Traumatic Brain Injury (TBI) is a form of acquired brain injury caused by external  impact  to the head  that results in damage to the brain [ 1]. It is a common cause of death  and disability in the United States (U.S.)  and can be caused by a variety of factors includ ing falls, motor vehicle crashes, sport s, or combat injuries . TBI affects an estimated 2 mil lion people in the U.S. across all age groups, according to data from U.S.  Centers for Dis ease Control and Prevention  (CDC ) [2] and likely a larger number globally. TBI often leads  to neurological problems in individuals, including cognitive, motor, an d sleep wake dys function [3].   Currently available medical tools for TBI diagnosis are largely subjective  [4] and a  lack of consensus regarding what constitutes mild TBI ( mTBI ) adds to the complication of  the under diagnosis of mTBI [ 5]. TBI is categorized into mild, moderate, or severe based  on the Glasgow coma scale (GCS), Loss of consciousness (LOC), and Post traumatic am nesia (PTA) [ 6] which are qualitative tests rather than quantitative measure s. The World  Health Organization 's (WHO ) definition of mTBI allows for a GCS score of 13 –15 to be     assessed after the typical 30 min timeframe, which accounts f or the expected time of arri val of a qualified healthcare provider [ 7]. However, the GCS has its drawbacks. Being  highly inter observer dependent  makes it necessary to report exact findings rat her than  just the score.  In addition, o ne of the key parameters in GCS  is the eye score which might  be unattainable in case of an eye injury.   Considering the requirements from a medical resource standpoint, e xisting clinical  tools used to  diagnose mTBI such as Magnetic Resonance Imaging (MRI) and Computer  Tomography (CT) [ 4] require an extensive, high cost clinical setup and specialized oper ator skill set  which are not always available at the time and place of an incident , and these  neuroimaging tests may still be negative in many cases of mTBI . As a result of the limita tions of the present day methods used to detect TBI, there is a need for new technology  capable of rapid, accurate, non invasive, and most importantly , field capable detection of  mTBI to bridge the technological gap that exists today. Early, objective, and r eliable mTBI  detection can help affected individuals undergo timely monitoring and therapy and  can  prevent death in severe cases.    Machine learning techniques provide a way to study mTBI and create systems to  help objectively diagnose and monitor mTBI pres ence  and stages in individuals. Recently,  machine learning techniques have been investigated for the purpose of classifying mTBI  from electroencephalogram  (EEG ) data in mice based on models created using the lateral  Fluid Percussion Injury (FPI) method [ 3]. FPI induced mice demonstrate very similar be havioral deficits and pathology to those found in humans afflicted with mTBI, including  sleep disturbances [ 8, 3]. In this work, we use EEG data acquired from the compelling FPI  mouse model of mTBI.  Previous investigations have studied  a variety of classification  techniques, including classical machine learnin g such as SVM [ 9], and deep learning such  as Convolutional Neural Networks (CNN) [ 9, 10]. These  techniques have been shown to  perform TBI classification with more than 80% accuracy  [9]. However, in most investiga tions we reviewed (as described in the subsequent Re lated Works subsection) that imple ment machine learning for TBI detection, the primary focus was the study of classification  techniques and performance of classification models [ 9] rather than portable  deployment .  In cases, where portable deployment was involved, the focus  was application specific im plementation , for example, closed loop robotic c ontrol systems [ 11].  This work focuses on creating  a machine  learning based, fast , portable , and ready to  use EEG classification system for mTBI detection using Raspberry Pi  4 (RPi) . This system  works with a variety of machine learning models and can be used along with live EEG  recording systems  to detect  mTBI. This capability can enable field use  and make early  mTBI d etection possible without the requir ement of  extensive medical setup or special ized medical domain knowledge. Further, this work has the potential to  create avenues  for implementing mTBI related real time connected health monitoring systems [ 12] and  allow further research on realtime mTBI detection using EEG .  The deployment system created in this work incorporates an Analog to Digital Con verter (ADC) front end and utilizes Convolutional Neural Network (CNN) and XGBoost   [13] predictive models to perform sleep staging and detect the presence of mTBI using a  single channel EEG signal. The system captures and classifies EE G epochs int o four target  classes  – Sham Wake, Sham Sleep, mTBI Wake, and mTBI Sleep. We demonstrate that our  system can capture physical EEG signal s and perform feature extraction and prediction  using the XGBoost model in the order of 0.02 s per epoch which makes it possible to  quickly detect the presence of mTBI. We also verify that the cross validation metrics ob tained on the RPi based system are identical to those obtained on a High Performance  Computer (HPC) such as a 64 bit workstation computer runnin g macOS or Windows.   The Related Works subsection covers previous relevant investigations in this area.  We describe the deployment system design, operation, classification model configuration,  and validation techniques in the Methods section. The Results se ction covers the perfor mance comparison of XGBoost with CNN and the deployment system performance eval uation. Finally, we conclude our work and discuss possible future directions in the Con clusions section.     1.2. Related Works   "
474,A novel integrated method of detection-grasping for specific object based on the box coordinate matching.txt,"To better care for the elderly and disabled, it is essential for service
robots to have an effective fusion method of object detection and grasp
estimation. However, limited research has been observed on the combination of
object detection and grasp estimation. To overcome this technical difficulty, a
novel integrated method of detection-grasping for specific object based on the
box coordinate matching is proposed in this paper. Firstly, the SOLOv2 instance
segmentation model is improved by adding channel attention module (CAM) and
spatial attention module (SAM). Then, the atrous spatial pyramid pooling (ASPP)
and CAM are added to the generative residual convolutional neural network
(GR-CNN) model to optimize grasp estimation. Furthermore, a detection-grasping
integrated algorithm based on box coordinate matching (DG-BCM) is proposed to
obtain the fusion model of object detection and grasp estimation. For
verification, experiments on object detection and grasp estimation are
conducted separately to verify the superiority of improved models.
Additionally, grasping tasks for several specific objects are implemented on a
simulation platform, demonstrating the feasibility and effectiveness of DG-BCM
algorithm proposed in this paper.","After decades of development, there are now various types of service robots with  different functions. However, it ’s still challenging to apply them  in home  scenes . It  mainly due to the incomplete technologies related to object detection, grasp estimation,  navigation, and positioning. With the backdrop of accelerated aging around the globe  and the increasing demand for service robots in society, research and developme nt in  the field of home service robots have become an inevitable trend (Hsu et al., 2020). In  order to provide care for disabled and elderly people, service robots combin e with  various technologies  to accurately identify  (Liu et al., 2022 ; Li et al., 2022 ) and track   (Li et al., 20 20; Li et al., 202 0; Cao et al., 2022 ) targets . Therefore, the research on  object detection and grasp technology of service robot in the indoor scenes holds  significance theoretical and practical importance.   Before the emergence of  deep learning, research on object detection and grasp  technology primarily relied on traditional algorithms. In recognition and detection, the  scale invariant feature transform (SIFT) algorithm (Lowe, 2004) was commonly used  to identify key points and ext ract feature descriptors from images. (Dalal et al., 2005)  created the histogram of oriented gradients (HOG) method that proved effective in  detecting pedestrians. Regarding grasp technology, (Li et al., 2015) extracted 2D image  edge features through SIFT and combined them with a depth camera to obtain 3D grasp  pose estimation through iterative closest point algorithm. In contrast  (Fan et al., 2018)  introduced an adaptive grasp strategy that estimate the potential range of pose error for  objects and compens ate through the contact between fingers and objects during  grasping. Furthermore, ( Matsuda  et al., 2022) combined image processing and distance  measurement  to grasp the target object .  The research mentioned above primarily relies on template feature matching and  iterative calculation methods, which can have certain limitations. These approaches  often involve large computational consumption  and poor efficiency, leading to the  difficulty of adapting to the unstructured environment with multiple confounding  factors. Usually, service robots need to operate in dynamic and real time situations. This requires service robots to have the ability to perceive their environment, recognize  objects, a nd interact with them through learning and reasoning processes. Traditional  algorithms may struggle to meet these functional requirements due to their limited  adaptability and scalability (Lee, 2019). With the deep learning developing rapidly,  researchers have trained different convolutional neural networks (CNN) to obtain  impressive  feature extraction and expression capabilities in the field of computer vision.  In this way, the CNN has achieved better results than traditional algorithms in object  detection  and grasp estimation that made their applications more extensive (Liu et al.,  2021). For example, ( Shang  et al., 2020) proposed  grasping posture prediction networks   with CNN for dexterous  robotic  hand to predict optimal grasp postures  and  experimented in simulation . (Yu et al., 2021) used SSD and spatiotemporal long short  term memory (ST LSTM) for space human robot interaction s in realtime hand gesture  detection and identity awareness . (Xie et al., 2021) combined  deep convolutional  generative adversarial network (DCGAN) and YOLOv4 to improve the accuracy of  defect  detection.  (Qu et al., 2022) proposed a minimum bounding rectangle detection  algorithm (MinBRect), which combined RGB and depth maps and calculated the  minimum axis of the rectangle to quickly es timate grasp positions.   However, the above research only focused on improving grasping or detection  separately while  did not consider the detection and grasp process as a merged task ,  resulting in certain difficulty for robots to accomplish recognition and  grasp.  Furthermore, (Zhang et al., 2018) proposed the ROI GD algorithm for grasp estimation  in the ROI region obtained from object detection. However , this method performed well  in stacked scenarios, it heavily relied on the accuracy of the ROI. (Li et al ., 2023)  proposed a two step cascade system that combines YOLOv4 with GGCNN, which can  simultaneously perform object detection and grasp estimation, but there was still room  for improvement to achieve higher accuracy.   In summary, the current research on ob ject detection and grasp estimation is  relatively fragmented, resulting huge difficulties  for robots to seamlessly integrate these  tasks. In addition, due to the significant variations among different indoor scenes and  the presence of interference such as occlusion, the degree of difficulty in precisely detecting and grasping objects is further increased. To enhance the capabilities of object  detection and grasping of service robots  and facilitate their widespread application, this  paper proposed an integra ted solution based on box coordinate matching for object  detection and grasp estimation. The main contributions of our work are as follows   (1) An improved instance segmentation model based on the combination of  convolutional neural networks (CNN) and atten tion mechanism was proposed as the  branch for object detection.   (2) An improved planar grasp estimation model based on the ASPP and channel  attention module that enhance the receptive field and feature extraction ability was built  as the branch for grasp e stimation.   (3) The DG BCM algorithm was proposed to achieve specific object grasp.  Furthermore, we built a robotic arm platform in simulation software and took  experiments to verify the feasibility and effectiveness of improved models and the  proposed algo rithm.   The content of sections are arranged  as follows : In Section 2 we summarized  works related to this paper. In Section 3 we introduced the improved models and  proposed algorithm of this paper while Section 4 described the experiments and results  of abo ve researches. Finally, we concluded the contribution of this paper and discussed  the future direction in Section 5.   2. Relative work   2.1 Object Detection   Object detection algorithms based on deep learning are mainly include two   methods , i.e.Onestage  model  and Twostage  model . Onestage models usually ha ve a  competent  inference  speed, such as YOLOv4 (Bochkovskiy et al., 2020) , EfficientDet  (Tan et al., 2019) , etc. Twostage models  mainly use region proposal algorithm  with a   higher accuracy while  the inference speed is slow er, such as Fast er RCNN (Ren et al.,  2015) , Cascade RCNN (Cai et al., 201 7), etc. With the emergence of fu lly convolutional  networks (FCN ), researchers have proposed pixel level tasks such as semantic segmentation and instance segmentation. For example, (He et al., 2017)  created Mask  RCNN that added FCN branches to Faster RCNN . (Bolya et al., 2019)  used the similar  method on YOLO and proposed YOLACT. However, t he above networks are all anchor  based meth ods. (Wang et al., 2021)  proposed the SOLO network based on anchor free   that without  the generation of bounding  boxes. Instead, pixels in each target  were  directly classified based on their position and size  that transform the detection and  segmentation into classification.   On the basis, SOLOv2 (Wang et al., 2020)  added dynamic convolution that can  change the kernel weights through training and learning, and proposed Matrix NMS to  get faster  speed. The backbone of SOLOv2 is Resnet50 with five residual  layer s for  feature extraction, while the neck takes FPN  (Feature Pyramid Network)  as the features  fusion block for the output of  different layers that can  improve the model's  discriminability for scales. The structure  is shown in Fig. 1.   ResLayer 5 ResLayer 4 ResLayer 3 ResLayer 2 ResLayer 1 Inputupsample P2upsample P3P5 upsample P4Conv 1 x 1  Conv 1 x 1 Conv 1 x 1 Conv 1 x 1  Conv 3 x 3  Conv 3 x 3 Conv 3 x 3 Conv 3 x 3    Fig.1. The structure of backbone and FPN    Fig. 2. The structure of SOLOv2 head   In addition, t he category branch ( 𝐶×𝑆×𝑆 ) and mask branch ( 𝑆2×𝐻×𝑊 ) is  included in the head , and the category branch divides the fused features into a grid of 𝑆× 𝑆, mapped to the  channels  with 𝑆2 in the mask branch, where each channel represents a  predicted instance of grid, as shown in Fig. 2. If the center of an instance is located at the  grid (𝑖,𝑗)  in category branch, the corresponding channel 𝑘  of mask branch is  responsible for predicting the mask of the instance, which can be expressed as:   𝑘=(𝑖−1)∗𝑆+𝑗  (𝑖,𝑗=1,2,3,…,𝑆)              (1)  where 𝑖  and 𝑗  represent the grid indices in the y and x directions, k is the channel  corresponding to mask branch, and S is the number of grids in each direction. The Loss  function of network can be composed of semantic category and mask prediction . The loss  of mask prediction is defined as:   𝐿𝑚𝑎𝑠𝑘 =1 𝑁𝑝𝑜𝑠∑𝛼{𝑝𝑖,𝑗∗>0} 𝑘 ∙𝑑𝑚𝑎𝑠𝑘 (𝑚𝑘,𝑚𝑘∗)             (2)  where 𝑖,𝑗,𝑘 is the same as (1) ，𝑁𝑝𝑜𝑠 is the total positive samples ，𝑝∗ and 𝑚∗ are the  targets of  category and mask respectively. 𝛼 will be 1 if 𝑝𝑖,𝑗∗ > 0. Otherwise  it set to 0 .  𝑑𝑚𝑎𝑠𝑘  is the Dice Loss  (Milletarì et al., 2016) . Then the total loss of training is defined  as：  𝐿=𝐿𝑐𝑎𝑡𝑒 +𝜆𝐿𝑚𝑎𝑠𝑘                        (3)  where 𝐿𝑐𝑎𝑡𝑒 is the loss of semantic category that used the conventional Focal Loss (Lin  et al., 2017) , 𝜆 is set to 3.   What ’s more, mask branch is further divided into two parts: mask kernel branch and  mask feature branch. Mask kernel branch is for dynamical  learn ing the weights of the  kernel G for each layer of features output by FPN. After unifying the feature size of each  layer to 1/4 of the original image and fusing them, input to the mask feature branch for  convolution with G to predict the mask, and then obtain the corresponding in stance mask  through Matrix NMS.   2.2 Grasp Detection   The grasp pose estimation based on deep learning can be categorized into  planar  grasp and 6 DOF  grasps , and this paper  is focus ed on planar grasp. (Lenz et al., 2013)   made a pioneering contribution by  using neural networks to extract gra sping  features,  laying the foundation for gra sping  estimation by deep learning . (Mahler et al., 2017)   established the grasping  dataset Dex Net 2.0 and proposed a Twostage detection  algorithm of Grasp Qua lity Convolutional Neural Network (GQ CNN), which first  sampled candidate grasping poses  and then evaluated the quality . However, this approach  suffers from limitations such as high inference latency and poor performance in stacking   scenarios  of multiple objects.  (Morrison et al., 2018)  proposed a lightweight generative  grasp convolutional neural network (GG CNN), which only used depth image s to achieve  endtoend grasp pose of each pixel , but the accuracy could  be further  improved. (Kumra  et al., 2019)  introduced the  GRCNN , and improve the  ability of  feature extraction through  the residual module. Fig. 3 illustrates the structure of the network .    Fig. 3.  The structure of GRCNN   The network  consists of CBR module, residual module and CTBR module. The  module s mainly include  convolution, batch normalization , activation, transpose  convolution and other s. For the input image, three CBR modules  are used for  pretreatment , and five residual modules  for feature extraction . Finally, to preserve the  spatial features of input data, up sampling is performed by transposing convolution, and  get the gradcam of Quality , Angle , and Width. The Quality is used to describe the grasp   quality of pixels, and its v alue represents the grasp  confidence score at all of pixel s in  image . The range is from 0 to 1, and if the score  is higher , the success rate  of grasp  objects is higher . The Angle represents the grasp  angle at all of pixels in image , ranging  from [  π/2, π/2]. The Width is the width of grasp at all of pixel s in image , ranging from  [0, 𝑊𝑚𝑎𝑥], where 𝑊𝑚𝑎𝑥 is the maximum width that the gripper can reach. Based on  this, the definition of grasping posture is as follows:   𝑔𝑖=(𝑥𝑖,𝑦𝑖,𝜃𝑖,𝑤𝑖,𝑞𝑖)                       (4)  where (𝑥𝑖,𝑦𝑖)  is the center coordinate of the grasping rectangle . 𝜃𝑖  is the rotation  relative to the frame of camera . 𝑤𝑖 is the width in pixel.  𝑞𝑖 is the quality score  of 𝑔𝑖,  and the model will output the highest quality score of grasp pose  in priority .   At the same time, to convert pixel coordinates of the grasping pose into the  coordinates of robot, the following transformation relationship can be used:   𝑔𝑟=𝑇𝑟𝑐(𝑇𝑐𝑖(𝑔𝑖))                        (5)  where 𝑇𝑐𝑖  is the camera intrinsics which is used to convert pixel coordinates into  spatial coordinates under the camera frame. 𝑇𝑟𝑐  is the eye hand  calibration matrix,  which can transfer  the coordinates from  camera frame to robot frame to achieve  grasping.   In loss function, for solving the exploding gradients, GRCNN  used the smooth  L1 loss , defined as:   𝐿(𝑔𝑖,𝑔̂𝑖)=1 𝑛∑ 𝑧𝑖𝑛 𝑖=1                      (6)  𝑧𝑖 is defined as ：  𝑧𝑖={0.5(𝑔𝑖−𝑔̂𝑖)2,   𝑖𝑓 |𝑔𝑖−𝑔̂𝑖|<1 |𝑔𝑖−𝑔̂𝑖|−0.5,    𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒                (7)  where 𝑔𝑖 is the grasp  pose predicted by network ，𝑔̂𝑖 is the ground truth  of grasp  pose. 3. Method   "
63,Fixed Integral Neural Networks.txt,"It is often useful to perform integration over learned functions represented
by neural networks. However, this integration is usually performed numerically,
as analytical integration over learned functions (especially neural networks)
is generally viewed as intractable. In this work, we present a method for
representing the analytical integral of a learned function $f$. This allows the
exact integral of a neural network to be computed, and enables constrained
neural networks to be parametrised by applying constraints directly to the
integral. Crucially, we also introduce a method to constrain $f$ to be
positive, a necessary condition for many applications (e.g. probability
distributions, distance metrics, etc). Finally, we introduce several
applications where our fixed-integral neural network (FINN) can be utilised.","It is often useful to perform integration over learned functions represented by neural networks [1, 2, 3]. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function f. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain f to be positive, a necessary condition for many applications ( e.g.probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixedintegral neural network (FINN) can be utilised. 2 Related Work "
214,Covariance-free Partial Least Squares: An Incremental Dimensionality Reduction Method.txt,"Dimensionality reduction plays an important role in computer vision problems
since it reduces computational cost and is often capable of yielding more
discriminative data representation. In this context, Partial Least Squares
(PLS) has presented notable results in tasks such as image classification and
neural network optimization. However, PLS is infeasible on large datasets, such
as ImageNet, because it requires all the data to be in memory in advance, which
is often impractical due to hardware limitations. Additionally, this
requirement prevents us from employing PLS on streaming applications where the
data are being continuously generated. Motivated by this, we propose a novel
incremental PLS, named Covariance-free Incremental Partial Least Squares
(CIPLS), which learns a low-dimensional representation of the data using a
single sample at a time. In contrast to other state-of-the-art approaches,
instead of adopting a partially-discriminative or SGD-based model, we extend
Nonlinear Iterative Partial Least Squares (NIPALS) -- the standard algorithm
used to compute PLS -- for incremental processing. Among the advantages of this
approach are the preservation of discriminative information across all
components, the possibility of employing its score matrices for feature
selection, and its computational efficiency. We validate CIPLS on face
verification and image classification tasks, where it outperforms several other
incremental dimensionality reduction techniques. In the context of feature
selection, CIPLS achieves comparable results when compared to state-of-the-art
techniques.","Dimensionality reduction is widely used in computer vi sion applications from image classiﬁcation [11] [2] to dete c tion of adversarial images [12]. The idea behind this tech nique is to estimate a transformation matrix that projects the highdimensional feature space onto a lowdimensionallatent space [23][8]. Previous works have demonstrated that dimensionality reduction can improve not only com putational cost but also the effectiveness of the data rep resentation [19] [35] [33]. In this context, Partial Least Squares (PLS) has presented remarkable results when com pared to other dimensionality reduction methods [33]. This is mainly due to the criterion through which PLS ﬁnds the low dimensional space, which is by capturing the relation ship between independent and dependent variables. An other interesting aspect of PLS is that it can operate as a fea  ture selection method, for instance, by employing Variable Importance in Projection (VIP) [24]. The VIP technique employs score matrices yielded by NIPALS (the standard algorithm used for traditional PLS) to compute the impor tance of each feature based on its contribution to the gener ation of the latent space. Despite achieving notable results, PLS is not suitable for large datasets, such as ImageNet [6], since it requires all t he data to be in memory in advance, which is often impractical due to hardware limitations. Additionally, this requireme nt prevents us from employing PLS on streaming applications, where the data are being generated continuously. Such lim itation is not particular to PLS, many dimensionality reduc  tion methods, such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), also suffer from this problem [36, 2, 39]. To handle the aforementioned problem, many works have proposed incremental versions of traditional dimen sionality reduction methods. The idea behind these meth ods is to estimate the projection matrix using a single data sample (or a subset) at a time while keeping some proper ties of the traditional dimensionality reduction methods. A wellknown class of incremental methods is the one based on Stochastic Gradient Descent (SGD) [3] [2]. These meth ods interpret dimensionality reduction as a stochastic opt i mization problem of an unknown distribution. As shown by Weng et al. [36], incremental methods based on SGD are computationally expensive, present convergence problems and require many parameters that depend on the nature of 1(a) IPLS projection. (b) SGDPLS projection. (c) CIPLS (Ours) projection. Figure 1. Projection on the ﬁrst (xaxis) and second (yaxis ) components using different dimensionality reduction tec hniques. Our method (CIPLS) separates the feature space better than IPLS and SGD PLS, which are stateoftheart incremental PLSbased met hods. For IPLS and SGDPLS, the class separability is effective only on a sin gle dimension of the latent space, while for CIPLS it is retai ned on both dimensions. Blue and red points denote positive and negativ e samples, respectively. the data. To address this problem, Zeng et al. [40] proposed an efﬁcient and lowcost incremental PLS (IPLS). In their work, the ﬁrst dimension (component) of the latent space is found incrementally, while the other dimensions are es timated by projecting the ﬁrst component onto the recon structed covariance matrix, which is employed to address the issue of impractical memory requirements of a full co variance matrix. Even though IPLS achieves better performance than SGDbased and other stateoftheart incremental methods , the discriminability of its higherorder components (i.e. , all except the ﬁrst) is not preserved, as shown in Figure 1 (a), where it can be seen that the effectiveness of class separa bility of IPLS is restricted to the ﬁrst dimension of the late nt space. This behavior occurs because the higherorder com ponents are estimated using only the independent variables , that is, they are based on an approximation of the covari ance matrix X⊤X(similar to PCA) instead of X⊤Yem ployed in PLS. This can degrade the discriminability of the latent model since preserving the relationship between in dependent and dependent variables is an important property of the original PLS [8]. It is important to emphasize that, for highdimensional data, employing several components often provides better results [33, 9, 10], hence, IPLS might not be suitable for these cases. Motivated by limitations and drawbacks in incremen tal PLSbased approaches, we propose a novel incremen tal method1. Our method is based on the hypothesis that the estimation of higherorder components using the covarianc e matrix, as proposed by Zeng et al. [40], is inadequate since the relationship between independent and dependent vari ables is lost. Therefore, to preserve this characteristic, we extend NIPALS [1] to avoid the computation of X⊤Yand, consequently, enable it for incremental operation. Since 1https://github.com/arturjordao/IncrementalDimension alityReductionour proposed extension is based on a simple algebraic de composition, we preserve the simplicity and efﬁciency that makes NIPALS attractive, and we ensure that the relation ship between independent and dependent variables is prop agated to all components, differently from other methods. As shown in Figure 1, our method is capable of sepa rating data classes better than IPLS, mainly on the second component (i.e., yaxis). Since the proposed method does not use the covariance matrix ( X⊤X) to estimate higher order components, we refer to it as Covariancefree Incre mental Partial Least Squares (CIPLS). Besides providing superior performance, our method can easily be extended as a feature selection technique since it provides all the re  quirements to perform VIP. Existing incremental PLS meth ods, on the other hand, require more complex techniques to operate as feature selection [24]. We compare the proposed method on the tasks of face veriﬁcation and image classiﬁcation, where it outperforms several other incremental methods in terms of accuracy and efﬁciency. In addition, in the context of feature selection , we evaluate and compare the proposed method to stateof theart methods, where it achieves competitive results. 2. Related Work "
466,Morphological classification of galaxies with deep learning: comparing 3-way and 4-way CNNs.txt,"Classifying the morphologies of galaxies is an important step in
understanding their physical properties and evolutionary histories. The advent
of large-scale surveys has hastened the need to develop techniques for
automated morphological classification. We train and test several convolutional
neural network architectures to classify the morphologies of galaxies in both a
3-class (elliptical, lenticular, spiral) and 4-class (+irregular/miscellaneous)
schema with a dataset of 14034 visually-classified SDSS images. We develop a
new CNN architecture that outperforms existing models in both 3 and 4-way
classification, with overall classification accuracies of 83% and 81%
respectively. We also compare the accuracies of 2-way / binary classifications
between all four classes, showing that ellipticals and spirals are most easily
distinguished (>98% accuracy), while spirals and irregulars are hardest to
differentiate (78% accuracy). Through an analysis of all classified samples, we
find tentative evidence that misclassifications are physically meaningful, with
lenticulars misclassified as ellipticals tending to be more massive, among
other trends. We further combine our binary CNN classifiers to perform a
hierarchical classification of samples, obtaining comparable accuracies (81%)
to the direct 3-class CNN, but considerably worse accuracies in the 4-way case
(65%). As an additional verification, we apply our networks to a small sample
of Galaxy Zoo images, obtaining accuracies of 92%, 82% and 77% for the binary,
3-way and 4-way classifications respectively.","Galaxiesexhibitavarietyofmorphologiesthroughoutcosmictime (Conselice 2014), ranging from broad families of spirals to large ellipticals.Agalaxy’smorphologyisrelatedtoitsinternalphysical processes (Sellwood 2014), such as its dynamical and chemical evolution, star formation activity (Kennicutt 1998), and can reveal insightsintoitspastevolutionaryhistory,includingmergers(Mihos & Hernquist 1996) and interactions with its environment (Alonso et al. 2006). A galaxy’s morphology is often determined by visual inspection, and while this is suﬃcient to delineate a rich array of morphological types in the nearby Universe (Buta 2013), there is the inherent issue of scalability as future surveys yield far greater magnitudes of data. Visually classifying the apparent morphologies of galaxies through the manual inspection of images is a laborious, time consuming task. Recently, the use of citizen science has seen great success in classifying larger datasets (Lintott et al. 2011; Willett etal.2013;Simmonsetal.2016),howeveritisimportanttokeepin mindthatthiswasachievedbyincreasingtheorderofmagnitudeof ★Email: mitchell.cavanagh@icrar.org (MKC)human classiﬁers, while the speed of the actual classiﬁcation itself is more or less unchanged. Future largescale surveys are expected to yield enormous amounts of data, with Euclidalone expected to survey at least to the order of 1 billion galaxies. The scale of sur veyssuchasthesewilloverwhelmthecapacityofhumanvolunteers (Silva et al. 2018), necessitating the development and deployment of techniques for automated classiﬁcation. Automated classiﬁcation techniques, in particular those util ising neural networks (Dieleman et al. 2015; Domínguez Sánchez et al. 2018; Zhu et al. 2019), have the ability to revolutionise the speed at which samples can be individually classiﬁed. Such clas siﬁcation techniques range from analytical approaches, such as the Fourier analysis of luminosity proﬁles (Odewahn et al. 2002) and isophote decompositions (MéndezAbreu et al. 2017), to statistical learning methods (Sreejith et al. 2017), random forest classiﬁers (Beck et al. 2018) and ensemble classiﬁers (Baqui et al. 2021), which are all part of a broader suite of machine learning methods (Chengetal.2020).Neuralnetworkshavebeenutilisedinthisﬁeld forsometime,butonlyrecentlyhastherapidlygrowingﬁeldofdeep learning seen widespread applications in astronomy (Baron 2019). One of the key architectures behind the success of deep learning, particularly in applications involving image recognition and com ©2021 The AuthorsarXiv:2106.01571v1  [astroph.GA]  3 Jun 20212M. K. Cavanagh et al. puter vision, is the convolutional neural network (hereafter CNN), introducedbyLecunetal.(1998).Unlikethetypicalmultiplayerper ceptronspresentinanormalneuralnetwork,CNNsarespeciﬁcally designed to extract features in data through multiple convolutions (LeCun et al. 2015). Each convolution can be thought of as a layer of abstraction, with the extracted highlevel features used to learn a generalised representation of data. Importantly, CNNs provide a modelindependentmeansoffeatureextraction,hencetheirversatile range of applications. Although having originally been developed to recognise handwritten characters, modern CNNs are capable of general image recognition across myriad image types (He et al. 2015).Indeed,imageclassiﬁcationisoneofthemainastronomical applications of CNNs (Cheng et al. 2020). CNNs have been successfully utilised to detect quasars and gravitational lenses (PasquetItam & Pasquet 2018; Schaefer et al. 2018),studybulge/diskdominance(Ghoshetal.2020),detectstellar bars (Abraham et al. 2018) and classify radio morphologies (Wu et al. 2018). CNNs have also been widely used with simulations, includingcosmologicalsimulations(Mustafaetal.2019)andmock surveys(Ntampakaetal.2020),aswellastodeveloptoolsforgalaxy photometric proﬁle analysis (Tuccillo et al. 2017). Recent studies have utilised CNNs for the purpose of binary classiﬁcation (Ghosh et al. 2020), or classifying between general morphological shapes (Zhuetal.2019).Fewerstudieshavelookedat3wayclassiﬁcation between distinct morphological types, though some works have exploredclassifyingbetweenellipticalsandbarred/unbarredspirals (Barchi et al. 2020), or between ellipticals, spirals and irregulars (Calleja & Fuentes 2004) In this work, we train and test several diﬀerent CNN architec tures for the purpose of initially distinguishing between elliptical (E), lenticular (S0) and spiral (Sp) galaxies, before extending the schema to include a fourth irregular and miscellaneous category (Irr+Misc).Webaseourtrainingdataonthevisuallyclassiﬁedcata logueofNair&Abraham(2010),consistingof14034samplesfrom theSDSSDataRelease4(AdelmanMcCarthyetal.2006),andap plyaseriesofdataaugmentationtechniquestoincreasethenumber of training samples for use with the CNN. Our best accuracies are consistent with previous studies, surpassing others when it comes to perclass accuracies. We dedicate a section of our discussion to focusing on the physical implications of the CNN classiﬁcations (both correct and incorrect), as well as how these are reﬂective of theinherentuncertaintyinthetrainingdata,howsuchuncertainties aﬀectCNNaccuracies,andhowCNNscanbeusedtoaddressthese uncertainties. Thestructureofthepaperisasfollows.In§2webrieﬂyoutline the theory of CNNs and its key concepts and terminology, discuss the CNN architectures including our new C2 network, discuss the augmentation and training methodology, and discuss the overall performance of the 3way and 4way classiﬁcation tasks for each of the four CNN architectures. In §3 we present our key results, starting with binary classiﬁcations between morphological classes, before presenting the best results for the 3way and 4way classiﬁ cations and analysing these by considering the physical properties of the samples for each classiﬁcation. In §4 we discuss the accu racy of the CNN. We examine the physical properties of classiﬁed samplesandshowthattherearecommontrendsinthemisclassiﬁed samplesacrossseveralphysicalproperties.Weshowcasebinaryhi erarchical classiﬁcation as an alternative to the direct multiclass CNNs, exploring ﬁve diﬀerent classiﬁers and their key diﬀerences toourmainCNN.Wepresentseveralexamplesofimagesthatwere correctlyandincorrectlyclassiﬁed,commentingoninherentuncer tainties and the challenge of unambiguously classifying samples,whether by eye or by CNN. As a ﬁnal, independent veriﬁcation of our network, we apply our models to a small sample consisting of 1,000 randomly chosen ellipticals and 1,000 randomly chosen spi ralgalaxiesfromtheGalaxyZoodataset.Lastly,wesummariseour ﬁndings in §5. 2 METHODS "
77,Automatic Face Aging in Videos via Deep Reinforcement Learning.txt,"This paper presents a novel approach to synthesize automatically
age-progressed facial images in video sequences using Deep Reinforcement
Learning. The proposed method models facial structures and the longitudinal
face-aging process of given subjects coherently across video frames. The
approach is optimized using a long-term reward, Reinforcement Learning function
with deep feature extraction from Deep Convolutional Neural Network. Unlike
previous age-progression methods that are only able to synthesize an aged
likeness of a face from a single input image, the proposed approach is capable
of age-progressing facial likenesses in videos with consistently synthesized
facial features across frames. In addition, the deep reinforcement learning
method guarantees preservation of the visual identity of input faces after
age-progression. Results on videos of our new collected aging face AGFW-v2
database demonstrate the advantages of the proposed solution in terms of both
quality of age-progressed faces, temporal smoothness, and cross-age face
verification.","Agerelated facial technologies generally address the two areas of age estimation [4, 21, 20, 23, 8, 22] and age progression [6, 29, 46, 30, 41, 35]. The face ageestimation problem is deﬁned as building computer software that has the ability to recognize the ages of individuals in a given photograph. Comparatively, the face ageprogression prob lem necessitates the more complex capability to predict the future facial likeness of people appearing in images [19]. Aside from the innate curiosity of individuals, research of face aging has its origins in cases of missing persons and wanted fugitives, in either case law enforcement de Figure 1: Given an input video, while framebased ap proaches produce inconsistent aging features, our video based method ensures consistency among video frames. sires plausible ageprogressed images to facilitate searches. Accurate face aging also provides beneﬁts for numerous practical applications such as ageinvariant face recogni tion [44, 43, 17]. There have been numerous anthropo logical, forensic, computeraided, and computerautomated approaches to facial ageprogression. However, the results from previous methods for synthesizing aged faces that rep resent accurate physical processes involved in human ag ing are still far from perfect. This is especially so in age progressing videos of faces, due to the usual challenges for 1arXiv:1811.11082v2  [cs.CV]  24 Apr 2019Table 1: The comparison of the properties between our videobased approach and other age progression methods. Ours ICPGAN [41]TNVP [9] CAAE [46] RFA [40] TRBM [7] Modality Videobased Imagebased Imagebased Imagebased Imagebased Imagebased Temporal Consistency Yes No No No No No Aging Mechanism Oneshot Oneshot Multipleshot Oneshot Oneshot Multipleshot Architecture DL + RL DL DL DL DL DL Tractability 3 3 3 3 3 7 face processing involving pose, illumination, and environ ment variation as well as differences between video frames. There have been two key research directions in age pro gression for both conventional computervision approaches and recent deeplearning methods – oneshot synthesis and multipleshot synthesis . Both approaches have used facial image databases with longitudinal sample photos of indi viduals, where the techniques attempt to discover aging pat terns demonstrated over individuals or the population rep resented. In oneshot synthesis approaches, a new face at the target age is directly synthesized via inferring the rela tionships between training images and their corresponding age labels then applying them to generate the aged likeness. These prototyping methods [2, 15, 33] often classify train ing images in facial image databases into age groups ac cording to labels. Then the average faces, or mean faces, are computed to represent the key presentation or archetype of their groups. The variation between the input age and the target age archetypes is complimented to the input image to synthesize the ageprogressed faces at the requested age. In a similar way, Generative Adversarial Networks (GANs) [46, 41] methods present the relationship between semantic representation of input faces and age labels by constructing a deep neural network generator. It is then combined with the target age labels to synthesize output results. Meanwhile, in multipleshot synthesis, the longitudinal aging process is decomposed into multiple steps of aging effects [9, 7, 35, 40, 45]. These methods build on the facial aging transformation between two consecutive age groups. Finally, the progressed faces from one age group to the next are synthesized stepbystep until they reach the target age. These methods can model the longterm sequence of face aging using this strategy. However, these methods still have drawbacks due to the limitations of longterm aging not be ing well represented nor balanced in face databases. Existing ageprogression methods all similarly suffer from problems in both directions. Firstly, they only work on single input images. Supposing there is a need to syn thesize aging faces presented in a captured video, these methods usually have to split the input video into sepa rate frames and synthesize every face in each frame in dependently which may often present inconsistencies be tween synthesized faces. Since face images for each frame are synthesized separately, the aging patterns of generated faces of the same subject are also likely not coherent. Furthermore, most aging methods are unable to produce high resolution images of age progression, important for features such as ﬁne lines that develop fairly early in the aging pro cess. This may be especially true in the latent based meth ods [15, 9, 7, 35, 40, 45]. Contributions of this work: This paper presents a deep Reinforcement Learning (RL) approach to Video Age Pro gression to guarantee the consistency of aging patterns in synthesized faces captured in videos. In this approach, the agetransformation embedding is modeled as the opti mal selection using Convolutional Neural Network (CNN) features under a RL framework. Rather than applying the imagebased age progression to each video frame indepen dently as in previous methods, the proposed approach has the capability of exploiting the temporal relationship be tween two consecutive frames of the video. This prop erty facilitates maintaining consistency of aging informa tion embedded into each frame. In the proposed struc ture, not only can a smoother synthesis be produced across frames in videos, but also the visual ﬁdelity of aging data, i.e. all images of a subject in different or the same age, is preserved for better age transformations. To the best of our knowledge, our framework is one of the ﬁrst face ag ing approaches in videos. Finally, this work contributes a new largescale faceaging database1to support future stud ies related to automated face ageprogression and age esti mation in both images and videos. 2. Related work "
250,Stock Price Prediction using Dynamic Neural Networks.txt,"This paper will analyze and implement a time series dynamic neural network to
predict daily closing stock prices. Neural networks possess unsurpassed
abilities in identifying underlying patterns in chaotic, non-linear, and
seemingly random data, thus providing a mechanism to predict stock price
movements much more precisely than many current techniques. Contemporary
methods for stock analysis, including fundamental, technical, and regression
techniques, are conversed and paralleled with the performance of neural
networks. Also, the Efficient Market Hypothesis (EMH) is presented and
contrasted with Chaos theory using neural networks. This paper will refute the
EMH and support Chaos theory. Finally, recommendations for using neural
networks in stock price prediction will be presented.","HIS paper uses a Dynamic Neural Network to predict a  given stock's future daily closing price . These  are models   used for predictions and non linear filtering by analyzing  data for patterns and ""learning "" from said patterns. T wo  prevailing theories  seek to qualify market behavior; EMH and  Chaos.   The EMH theory  states that no system can reliably 'beat'  the market because a stock 's price ha s alrea dy incorporated  and reflects all factors into its valuation , thus being  random   and unpredictable . In contrast,  the contrary Chaos theory   states that stock prices are made up of deterministic  plus  random  patterns. [1]   Chaos represents deterministic but non linear processes that  appear random because they cannot be easily quantified.  Through the neural networks ' inherent ability to learn non  linear, dynamic, & chaotic systems, it is possible to  outperform traditional stock price analysis ; much  to the liking  of investors who can exploit this ability  for huge profits.   Neural networks enable multivariate models , allowing   parallel processing of input data, thus quickly processing large  amounts of data . Their primary strength is that they can  determine patterns and anomalies within data, but most  importantly, their ability to detect multidimensional & non  linear connections within that data; this makes neural  networks extremely suitable for the dynamic and non linear  data generated by the stoc k market [2].    2. RELATED WORKS   "
195,GPU Acceleration of Sparse Neural Networks.txt,"In this paper, we use graphics processing units(GPU) to accelerate sparse and
arbitrary structured neural networks. Sparse networks have nodes in the network
that are not fully connected with nodes in preceding and following layers, and
arbitrary structure neural networks have different number of nodes in each
layers. Sparse Neural networks with arbitrary structures are generally created
in the processes like neural network pruning and evolutionary machine learning
strategies. We show that we can gain significant speedup for full activation of
such neural networks using graphical processing units. We do a prepossessing
step to determine dependency groups for all the nodes in a network, and use
that information to guide the progression of activation in the neural network.
Then we compute activation for each nodes in its own separate thread in the
GPU, which allows for massive parallelization. We use CUDA framework to
implement our approach and compare the results of sequential and GPU
implementations. Our results show that the activation of sparse neural networks
lends very well to GPU acceleration and can help speed up machine learning
strategies which generate such networks or other processes that have similar
structure.","Artiﬁcial neural networks, ﬁrst proposed by Mcculoch and Pitts in 1943 [1], are universal function approximators loosely based on biological neural networks. Neural networks with back propagation [2] is a robust method in machine learning. Artiﬁcial neural networks(ANN) have interconnected nodes that are separated into three types  inputs, outputs and hidden nodes. Inputs nodes are sensor nodes that take in values from outside the system, output nodes are the nodes that produce the answers from the network and hidden nodes are the nodes which lie in the information propagation path of the neural network. Each node is activated based on the nodes from which it has incoming connections, and the activation is calculated by weighting all the incoming node values with corresponding connection weight and summing all the values. The sum is then thresholded for the ﬁnal activation. Generally the sum is passed through sigmoid function to constrain it within "
134,Speaker verification using attentive multi-scale convolutional recurrent network.txt,"In this paper, we propose a speaker verification method by an Attentive
Multi-scale Convolutional Recurrent Network (AMCRN). The proposed AMCRN can
acquire both local spatial information and global sequential information from
the input speech recordings. In the proposed method, logarithm Mel spectrum is
extracted from each speech recording and then fed to the proposed AMCRN for
learning speaker embedding. Afterwards, the learned speaker embedding is fed to
the back-end classifier (such as cosine similarity metric) for scoring in the
testing stage. The proposed method is compared with state-of-the-art methods
for speaker verification. Experimental data are three public datasets that are
selected from two large-scale speech corpora (VoxCeleb1 and VoxCeleb2).
Experimental results show that our method exceeds baseline methods in terms of
equal error rate and minimal detection cost function, and has advantages over
most of baseline methods in terms of computational complexity and memory
requirement. In addition, our method generalizes well across truncated speech
segments with different durations, and the speaker embedding learned by the
proposed AMCRN has stronger generalization ability across two back-end
classifiers.","Speaker recognition is a task that identifies a person based on  his or her voices [1]. With the wide  application of voiceenabled devices, especially smart phone, s peaker recognition has become an  indispensable component in many applications, such as criminal investigation [2], financial services [3].  For example, in the context of criminal investigation, law enfo rcement agencies often need to know  whether the speech recordings are  uttered by the claimed speake r or not. The law enforcement agencies  want to use speaker recognition as a tool to investigate a susp ect or to determine a judgment of guilt or  innocence [2]. Therefore, speaker  recognition is helpful for th e law enforcement agencies to solve  criminal cases [2]. In addition,  speaker recognition is critica l for successfully implementing other tasks  related to multiple speakers, such as speaker diarization [4], speaker tracking [5], and multispeaker  speech recognition [6].   The task of speaker recognition can be divided into two categor ies: speaker identification and  speaker verification [7]. Speaker identification is to decide w hich enrolled speaker utters a given voice  f r o m  a  s e t  o f  k n o w n  s p e a k e r s  [ 8 ] .  S p e a k e r  v e r i f i c a t i o n  i s  t o  r e ject or accept the identity claim of a  speaker based on the speaker’s utterance [9]. The work in this paper focuses on discussing speaker  verification only.   1.1. Related works  "
114,Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning.txt,"A major challenge in Explainable AI is in correctly interpreting activations
of hidden neurons: accurate interpretations would provide insights into the
question of what a deep learning system has internally detected as relevant on
the input, de-mystifying the otherwise black-box character of deep learning
systems. The state of the art indicates that hidden node activations can, in
some cases, be interpretable in a way that makes sense to humans, but
systematic automated methods that would be able to hypothesize and verify
interpretations of hidden neuron activations are underexplored. In this paper,
we provide such a method and demonstrate that it provides meaningful
interpretations. Our approach is based on using large-scale background
knowledge approximately 2 million classes curated from the Wikipedia concept
hierarchy together with a symbolic reasoning approach called Concept Induction
based on description logics, originally developed for applications in the
Semantic Web field. Our results show that we can automatically attach
meaningful labels from the background knowledge to individual neurons in the
dense layer of a Convolutional Neural Network through a hypothesis and
verification process","Deep learning has led to significant advances in artifi cial intelligence applications including image classifica tion (Ramprasath, Anand, and Hariharan 2018), speech recognition (Graves and Jaitly 2014), translation (Auli et al. 2013), drug design (Segler et al. 2018), medical diagno sis (Choi et al. 2019), climate sciences (Liu et al. 2016), and many more. Despite these successes, the blackbox na ture of deep learning systems remains problematic for some application areas, especially those involving automated de cisions and safetycritical systems. For example, Apple co founder Steve Wozniak accused Apple of gender discrimi nation, claiming that the new Apple Card gave him a credit limit that was ten times higher than that of his wife even though the couple shares all property (Hamilton 2019). In an image search, only 11% of the top image results for “CEOs” were images of women despite the fact that women make up 27% of US CEOs (Silberg and Manyika 2019). Other ap plication areas of particular concern include safetycritical systems such as selfdriving cars (Chen and Huang 2017),drug discovery and treatment recommendations (Rifaioglu et al. 2020; Hariri and Narin 2021), and others, as deep learning systems are prone to adversarial attacks, e.g., by altering classification results by introducing adversarial ex amples (Bau et al. 2020) or simply controlling the order in which training images are presented (Shumailov et al. 2021). Some of these attacks are difficult or impossible to detect af ter the fact (Goldwasser et al. 2022; Clifford et al. 2022). Standard assessments of deep learning performance con sist of statistical evaluation, but do not seem sufficient to ad dress these shortcomings as they cannot provide reasons or explanations for particular system behaviors (Doran, Schulz, and Besold 2018). Consequently, it remains very important to develop strong explanation methods for deep learning sys tems. While there has been significant progress on this front (see Section 2), the current state of the art is mostly re stricted to explanation analyses based on a relatively small number of predefined explanation categories. This is prob lematic from a principled perspective, as this relies on the assumption that explanation categories preselected by hu mans would be viable explanation categories for deep learn ing systems – an asyet unfounded conjecture. Other state of the art explanation systems rely on modified deep learning architectures, usually leading to a decrease in system per formance compared to unmodified systems (Zarlenga et al. 2022). Ideally, we would want strong explanation capabili ties while maintaining the underlying learning architecture. In this paper, we address the aforementioned shortcom ings by using Concept Induction , i.e., formal logical deduc tive reasoning (Lehmann and Hitzler 2010). We show that our approach can indeed provide meaningful explanations for hidden neuron activation in a Convolutional Neural Net work (CNN) architecture for image scene classification (on the ADE20K dataset (Zhou et al. 2019)), using a class hi erarchy consisting of about 2·106classes, derived from Wikipedia, as the pool of categories (Sarker et al. 2020). The benefits of our approach to explainable deep learning are: (a) it can be used on unmodified and pretrained deep learning architectures, (b) it assigns semantic categories (i.e., class labels expressed in formal logic) to hidden neurons such that images related to these labels activate the corresponding neuron with high probability, and (c) it can construct these labels from a very large pool of categories. The rest of this paper is organized as follows. Section 2arXiv:2308.03999v1  [cs.LG]  8 Aug 2023discusses related work on explainable deep learning. Sec tion 3 presents our approach. Section 4 provides evaluation results and Section 5 discussions thereof. Section 6 con cludes and discusses followup research directions. A tech nical appendix provides more complete details of our exper iments and results. Source code, input data, raw result files, and parameter settings for replication are available online.1 2 Related Work "
150,Robust Black-box Watermarking for Deep NeuralNetwork using Inverse Document Frequency.txt,"Deep learning techniques are one of the most significant elements of any
Artificial Intelligence (AI) services. Recently, these Machine Learning (ML)
methods, such as Deep Neural Networks (DNNs), presented exceptional achievement
in implementing human-level capabilities for various predicaments, such as
Natural Processing Language (NLP), voice recognition, and image processing,
etc. Training these models are expensive in terms of computational power and
the existence of enough labelled data. Thus, ML-based models such as DNNs
establish genuine business value and intellectual property (IP) for their
owners. Therefore the trained models need to be protected from any adversary
attacks such as illegal redistribution, reproducing, and derivation.
Watermarking can be considered as an effective technique for securing a DNN
model. However, so far, most of the watermarking algorithm focuses on
watermarking the DNN by adding noise to an image. To this end, we propose a
framework for watermarking a DNN model designed for a textual domain. The
watermark generation scheme provides a secure watermarking method by combining
Term Frequency (TF) and Inverse Document Frequency (IDF) of a particular word.
The proposed embedding procedure takes place in the model's training time,
making the watermark verification stage straightforward by sending the
watermarked document to the trained model. The experimental results show that
watermarked models have the same accuracy as the original ones. The proposed
framework accurately verifies the ownership of all surrogate models without
impairing the performance. The proposed algorithm is robust against well-known
attacks such as parameter pruning and brute force attack.","Deep learning is a variety of machine learning structure that automatically extracts features from training data to perform better on many complicated tasks. Deep learning techniques produce better results when combines with other machine learning approaches such as neural networks. The combination of deep learning and neural networks are called deep neural networks (DNNs). Lately, DNN algorithms have obtained stateoftheart results in multiple ML related domains such as natural processing language, voice recognition, and computer vision. The most important factor that helps DNN to achieve such outstanding results is leveraging numerous labeled training data. Based on the 2016 data science report [1], most data scientists spend 80% of their time collecting, preparing, and managing data. Since collecting a large amount of labeled data and providing powerful hardware for training a DNN can be very expensive, most scientists prefer to employ a pretrained model for their problems. On the other hand, organizations consider a trained DNN as their Intellectual Property (IP) because of the costs of collecting a considerable amount of labeled data and providing powerful hardware for training it. Pretrained models help users to develop their speciﬁc methods, which is called ﬁnetuning. Finetuning obtains the best set of weights for a trained DNN in a particular problem and uses them as initialization weights for a new model in the same domain. Finetuning is an effective technique to speed up the training phase of DNN models, and also it helps overcome the small dataset problem. Finetuning assists scientists to build an accurate and highperformance DNN model. A typical adversary can utilize different ﬁnetuning approaches as a means for redistribution and copyright infringement. Thus, a trained DNN model needs to be protected as an Intellectual Property (IP) from illegal redistribution, reproducing, and derivation. Digital Watermarking is one of the best solutions to protect a trained DNN model from copyright infringement. Digital watermarking is a technique that embeds different types of data, signal, or information into digital media such as digital image, audio, and video [2]. Digital watermarking can have several different purposes. These include merely hiding a piece of meaningful information without modifying the host of the watermark or embedding a piece of speciﬁc information that can ensure the originality of a digital ﬁle by authenticating the embedding content. Various watermarking algorithms utilize several different approaches to distinguish between original or watermarked media. The watermarking algorithms encrypt the content using various encryption techniques such as block ciphers to avoid revealing the watermark’s information to adversaries who have prior knowledge of the watermarking algorithm. Recently, many approaches have been published to watermark the DNNs to protect them from abuse cases. In these methods, the owner of a trained DNN model watermarks his/her model by embedding speciﬁc data into the training dataset or modifying some parameters of the model.arXiv:2103.05590v1  [cs.CR]  9 Mar 20211 To the best of our knowledge, all methods and strategies proposed for watermarking DNNs are focused and evaluated on digital image classiﬁcation tasks because adding noise to a digital image within a dataset can be very straightforward. In this research, we propose a framework to watermark a DNN model that is trained with textual data. The ﬁrst stage of the proposed algorithm includes selecting a random data from the training set and adding a certain amount of random noise to the selected data. This set of data is called the trigger set and considered as the watermark. After generating the trigger set, the model is trained with a speciﬁc combination of this set and original training data. At this step, the trained model is watermarked, which means it returns correct prediction to the ordinary data while it returns a modiﬁed response to the trigger set data. The rest of this paper is organized as follows. Section II summarizes the important related work that have been proposed to protect DNN models and discusses the related research on watermarking DNN models. In Section III, the proposed method for watermarking a textual DNN model is described in detail. The experimental results are presented in Section IV. Section V provides some concluding remarks and discusses future works. II. R ELATED WORK "
43,A Unified View of Piecewise Linear Neural Network Verification.txt,"The success of Deep Learning and its potential use in many safety-critical
applications has motivated research on formal verification of Neural Network
(NN) models. Despite the reputation of learned NN models to behave as black
boxes and the theoretical hardness of proving their properties, researchers
have been successful in verifying some classes of models by exploiting their
piecewise linear structure and taking insights from formal methods such as
Satisifiability Modulo Theory. These methods are however still far from scaling
to realistic neural networks. To facilitate progress on this crucial area, we
make two key contributions. First, we present a unified framework that
encompasses previous methods. This analysis results in the identification of
new methods that combine the strengths of multiple existing approaches,
accomplishing a speedup of two orders of magnitude compared to the previous
state of the art. Second, we propose a new data set of benchmarks which
includes a collection of previously released testcases. We use the benchmark to
provide the first experimental comparison of existing algorithms and identify
the factors impacting the hardness of verification problems.","Despite their success in a wide variety of applications, Dee p Neural Networks have seen limited adoption in safetycritical settings. The main explanatio n for this lies in their reputation for being blackboxes whose behaviour can not be predicted. Current a pproaches to evaluate trained models mostly rely on testing using heldout data sets. However, as Edsger W. Dijkstra said [3], “testing shows the presence, not the absence of bugs”. If deep learnin g models are to be deployed to applica tions such as autonomous driving cars, we need to be able to ve rify safetycritical behaviours. To this end, some researchers have tried to use formal method s. To the best of our knowledge, Zakrzewski [21] was the ﬁrst to propose a method to verify sim ple, one hidden layer neural networks. However, only recently were researchers able to work with no ntrivial models by taking advantage of the structure of ReLUbased networks [5, 11]. Even then, t hese works are not scalable to the large networks encountered in most real world problems. This paper advances the ﬁeld of NN veriﬁcation by making the f ollowing key contributions: 1. We reframe state of the art veriﬁcation methods as special cases of BranchandBound optimization, which provides us with a uniﬁed framework to compare them. 2. We gather a data set of test cases based on the existing lite rature and extend it with new benchmarks. We provide the ﬁrst experimental comparison of veriﬁcation methods. 3. Based on this framework, we identify algorithmic improve ments in the veriﬁcation process, speciﬁcally in the way bounds are computed, the type of branc hing that are considered, as Preprint. Work in progress.well as the strategies guiding the branching. Compared to th e previous state of the art, these improvements lead to speedup of almost two orders of magnitudes . Section 2 and 3 give the speciﬁcation of the problem and forma lise the veriﬁcation process. Section 4 presents our uniﬁed framework, showing that previous meth ods are special cases and highlight ing potential improvements. Section 5 presents our experim ental setup and Section 6 analyses the results. 2 Problem speciﬁcation We now specify the problem of formal veriﬁcation of neural ne tworks. Given a network that imple ments a function ˆ xn=f(x0), a bounded input domain Cand a property P, we want to prove x0∈ C,ˆ xn=f(x0) =⇒P(ˆ xn). (1) For example, the property of robustness to adversarial exam ples inL∞norm around a training sample awith label yawould be encoded by using C/defines{x0|/bardblx0−a/bardbl∞≤ǫ}and P(ˆ xn) =/braceleftbig ∀yˆxn[ya]≥ˆxn[y]/bracerightbig . In this paper, we are going to focus on PiecewiseLinear Neur al Networks (PLNN), that is, networks for which we can decompose Cinto a set of polyhedra Cisuch that C=∪iCi, and the restriction offtoCiis a linear function for each i. While this prevents us from including networks that use activation functions such as sigmoid or tanh, PLNNs all ow the use of linear transformations such as fullyconnected or convolutional layers, pooling u nits such as MaxPooling and activation functions such as ReLUs. In other words, PLNNs represent th e majority of networks used in practice. Operations such as BatchNormalization or Dropo ut also preserve piecewise linearity at testtime. The properties that we are going to consider are Boolean form ulas over linear inequalities. In our robustness to adversarial example above, the property is a c onjunction of linear inequalities, each of which constrains the output of the original label to be great er than the output of another label. The scope of this paper does not include approaches relying o n additional assumptions such as twice differentiability of the network [8, 21], limitation of the activation to binary values [5, 16] or restric tion to a single linear domain [2]. Since they do not provide f ormal guarantees, we also don’t include approximate approaches relying on a limited set of perturba tion [10] or on overapproximation meth ods that potentially lead to undecidable properties [17, 20 ]. 3 Veriﬁcation Formalism 3.1 Veriﬁcation as a Satisﬁability problem The methods we involve in our comparison all leverage the pie cewiselinear structure of PLNN to make the problem more tractable. They all follow the same g eneral principle: given a property to prove, they attempt to discover a counterexample that wou ld make the property false. This is accomplished by deﬁning a set of variables corresponding to the inputs, hidden units and output of the network, and the set of constraints that a counterexampl e would satisfy. To help design a uniﬁed framework, we reduce all instances of veriﬁcation problems to a canoni cal representation. Speciﬁcally, the whole satisﬁability problem will be transformed into a global optimization problem where the decision will be obtained by checking the sign of the minimum. If the property to verify is a simple inequality P(ˆ xn)/definescTˆ xn≥b, it is sufﬁcient to add to the network a ﬁnal fully connected layer with one output, with we ight ofcand a bias of −b. If the global minimum of this network is positive, it indicates tha t for allˆ xnthe original network can output, we have cTˆ xn−b≥0 =⇒cTˆ xn≥b, and as a consequence the property is True. On the other hand, if the global minimum is negative, then the minim izer provides a counterexample. The supplementary material shows that ORandANDclauses in the property can similarly be expressed as additional layers, using MaxPooling units. We can formulate any Boolean formula over linear inequaliti es on the output of the network as a sequence of additional linear and maxpooling layers. T he veriﬁcation problem will be reduced to the problem of ﬁnding whether the scalar output of the potentially modiﬁed net work can reach a negative value. Assuming the network only co ntains ReLU activations be tween each layer, the satisﬁability problem to ﬁnd a counter example can be expressed as: l0≤x0≤u0(2a) ˆxn≤0 (2b)ˆ xi+1=Wi+1xi+bi+1∀i∈ {0, n−1} (2c) xi= max(ˆ xi,0) ∀i∈ {1, n−1}. (2d) 2Eq. (10a) represents the constraints on the input and Eq. (10 h) on the neural network output. Eq. (10b) encodes the linear layers of the network and Eq. (2d ) the ReLU activation functions. If an assignment to all the values can be found, this represents a c ounterexample. If this problem is unsat isﬁable, no counterexample can exist, implying that the pro perty is True. We emphasise that we are required to prove that no counterexamples can exist, and no t simply that none could be found. While for clarity of explanation, we have limited ourselves to the speciﬁc case where only ReLU activation functions are used, this is not restrictive. The supplementary material contains a section detailing how each method speciﬁcally handles MaxPooling u nits, as well as how to convert any MaxPooling operation into a combination of linear layers an d ReLU activation functions. The problem described in (2) is still a hard problem. The addi tion of the ReLU nonlinearities (2d) transforms a problem that would have been solvable by simple Linear Programming into an NPhard problem [11]. Converting a veriﬁcation problem into this ca nonical representation does not make its resolution simpler but it does provide a formalism advan tage. Speciﬁcally, it allows us to prove complex properties, containing several ORclauses, with a single procedure rather than having to decompose the desired property into separate queries as was done in previous work [11]. Operationally, a valid strategy is to impose the constraint s (10a) to (2d) and minimise the value of ˆxn. Finding the exact global minimum is not necessary for veriﬁ cation. However, it provides a measure of satisﬁability or unsatisﬁability. If the value o f the global minimum is positive, it will correspond to the margin by which the property is satisﬁed. 3.2 Mixed Integer Programming formulation A possible way to eliminate the nonlinearities is to encode them with the help of binary variables, transforming the PLNN veriﬁcation problem (2) into a Mixed Integer Linear Program (MIP). This can be done with the use of “bigM” encoding. The following en coding is from Tjeng & Tedrake [19]. Assuming we have access to lower and upper bounds on the values that can be taken by the coordinates of ˆ xi, which we denote liandui, we can replace the nonlinearities: xi= max(ˆ xi,0)⇒δi∈ {0,1}hi,xi≥0,xi≤ui·δi (3a) xi≥ˆ xi,xi≤ˆ xi−li·(1−δi) (3b) It is easy to verify that δi[j]= 0⇔xi[j]= 0 (replacing δi[j]in Eq. (21a)) and δi[j]= 1⇔xi[j]= ˆxi[j](replacing δi[j]in Eq. (21b)). By taking advantage of the feedforward structure of the neu ral network, lower and upper bounds lianduican be obtained by applying interval arithmetic [9] to propa gate the bounds on the inputs, one layer at a time. Thanks to this speciﬁc feedforward structure of the proble m, the generic, nonlinear, nonconvex problem has been rewritten into an MIP. Optimization of MIP i s well studied and highly efﬁcient offtheshelf solvers exist. As solving them is NPhard, pe rformance is going to be dependent on the quality of both the solver used and the encoding. We now as k the following question: how much efﬁciency can be gained by using a bespoke solver rather than a generic one? In order to answer this, we present specialised solvers for the PLNN veriﬁcation tas k. 4 BranchandBound for Veriﬁcation As described in Section 3.1, the veriﬁcation problem can be r ephrased as a global optimization problem. Algorithms such as Stochastic Gradient Descent ar e not appropriate as they have no way of guaranteeing whether or not a minima is global. In this secti on, we present an approach to estimate the global minimum, based on the Branch and Bound paradigm an d show that several published methods, introduced as examples of Satisﬁability Modulo Th eories, ﬁt this framework. Algorithm 1 describes its generic form. The input domain is r epeatedly split into subdomains (line 7), over which lower and upper bounds of the minimum are compu ted (lines 910). The best upper bound found so far serves as a candidate for the global minimu m. Any domain whose lower bound is greater than the current global upper bound can be pruned a way as it cannot contain the global minimum (line 13, lines 1517). By iteratively splitting th e domains, it is possible to compute tighter lower bounds. We keep track of the global lower bound on the mi nimum by taking the minimum over the lower bounds of all subdomains (line 19). When the g lobal upper bound and the global lower bound differ by less than a small scalar ǫ(line 5), we consider that we have converged. Algorithm 1 shows how to optimise and obtain the global minim um. If all that we are interested in is the satisﬁability problem, the procedure can be simpliﬁe d by initialising the global upper bound with 0 (in line 2). Any subdomain with a lower bound greater th an 0 (and therefore not eligible to 3contain a counterexample) will be pruned out (by line 15). Th e computation of the lower bound can therefore be replaced by the feasibility problem (or its relaxation) imposing the constraint that the output is below zero without changing the algorithm. If i t is feasible, there might still be a counterexample and further branching is necessary. If it is infeasible, the subdomain can be pruned out. In addition, if any upper bound improving on 0 is found on a subdomain (line 11), it is possible to stop the algorithm as this already indicates the presence of a counterexample. Algorithm 1 Branch and Bound 1:function BAB(net,domain,ǫ) 2:global _ub←inf 3:global _lb←−inf 4:doms←[(global _lb,domain)] 5: whileglobal _ub−global _lb> ǫdo 6:(_,dom)←pick _out(doms) 7:[subdom _1,...,subdom _s]←split(dom) 8: fori= 1...s do 9: dom_ub←compute _UB(net,subdom _i) 10: dom_lb←compute _LB(net,subdom _i) 11: ifdom_ub<global _ubthen 12: global _ub←dom_ub 13: prune _domains(doms,global _ub) 14: end if 15: ifdom_lb<global _ubthen 16: domains.append((dom_lb,subdom _i)) 17: end if 18: end for 19:global _lb←min{lb|(lb,dom)∈doms} 20: end while 21: returnglobal _ub 22:end functionThe description of the veriﬁcation problem as optimization and the pseudocode of Algo rithm 1 are generic and would apply to veriﬁ cation problems beyond the speciﬁc case of PL NN. To obtain a practical algorithm, it is neces sary to specify several elements. A search strategy , deﬁned by the pick _out function, which chooses the next domain to branch on. Several heuristics are possible, for example those based on the results of previous bound computations. For satisﬁable problems or optimization problems, this allows to dis cover good upper bounds, enabling early prun ing. A branching rule , deﬁned by the split function, which takes a domain dom and return a partition in subdomain such that/uniontext isubdom _i=dom and that (subdom _i∩subdom _j) =∅,∀i/ne}ationslash=j. This will deﬁne the “shape” of the domains, which impacts the hardness of computing bounds. In addition, choosing the right partition can greatly impact the quality of the resulting bounds. Bounding methods , deﬁned by the compute _{UB,LB}functions. These procedures estimate re spectively upper bounds and lower bounds over the minimum ou tput that the network netcan reach over a given input domain. We want the lower bound to be as high as possible, so that this whole domain can be pruned easily. This is usually done by introduc ing convex relaxations of the problem and minimising them. On the other hand, the computed upper bo und should be as small as possi ble, so as to allow pruning out other regions of the space or di scovering counterexamples. As any feasible point corresponds to an upper bound on the minimum, heuristic methods are sufﬁcient. We now demonstrate how some published work in the literature can be understood as special case of the branchandbound framework for veriﬁcation. 4.1 Reluplex Katz et al. [11] present a procedure named Reluplex to verify properties of Neural Network contain ing linear functions and ReLU activation unit, functioning as an SMT solver using the splittingon demand framework [1]. The principle of Reluplex is to always maintain an assignment to all of the variables, even if some of the constraints are violated. Starting from an initial assignment, it attempts to ﬁx some v iolated constraints at each step. It prioritises ﬁxing linear constraints ((10a), (10b), (10h) and some relaxation of (2d)) using a simplex algorithm, even if it leads to violated ReLU constraints. If no solution to this relaxed problem containing only linear constraints exists, the counterexa mple search is unsatisﬁable. Otherwise, either all ReLU are respected, which generates a counterexa mple, or Reluplex attempts to ﬁx one of the violated ReLU; potentially leading to newly violated linear constraints. This process is not guaranteed to converge, so to make progress, nonlineariti es that get ﬁxed too often are split into two cases. Two new problems are generated, each corresponding t o one of the phases of the ReLU. In the worst setting, the problem will be split completely over all possible combinations of activation patterns, at which point the subproblems will all be simple LPs. This algorithm can be mapped to the special case of branchan dbound for satisﬁability. The search strategy is handled by the SMT core and to the best of our knowledge does not prioritise any domain. Thebranching rule is implemented by the ReLUsplitting procedure: when neith er the upper bound search, nor the detection of infeasibility are successful, one nonlinear constraint over the jth neu 4ron of the ith layerxi[j]= max/parenleftbig ˆxi[j],0/parenrightbig is split out into two subdomains: {xi[j]= 0,ˆxi[j]≤0} and{xi[j]= ˆxi[j],ˆxi[j]≥0}. This deﬁnes the type of subdomains produced. The prioritis ation of ReLUs that have been frequently ﬁxed is a heuristic to decide between possible partitions. As Reluplex only deal with satisﬁability, the analogue of th e lower bound computation is an over approximation of the satisﬁability problem. The bounding method used is a convex relaxation, obtained by dropping some of the constraints. The following relaxation is applied to ReLU units for which the sign of the input is unknown ( li[j]≤0andui[j]≥0). xi= max(ˆ xi,0)⇒xi≥ˆ xi(4a) xi≥0 (4b) xi≤ui. (4c) If this relaxation is unsatisﬁable, this indicates that the subdomain cannot contain any counterex ample and can be pruned out. The search for an assignment sati sfying all the ReLU constraints by iteratively attempting to correct the violated ReLUs is a he uristic that is equivalent to the search for an upper bound lower than 0: success implies the end of the pro cedure but no guarantees can be given. 4.2 Planet Ehlers [6] also proposed an approach based on SMT. Unlike Rel uplex, the proposed tool, named Planet, operates by explicitly attempting to ﬁnd an assignm ent to the phase of the nonlinearities. Reusing the notation of Section 3.2, it assigns a value of 0 or 1 to each δi[j]variable, verifying at each step the feasibility of the partial assignment so as to p rune infeasible partial assignment early. As in Reluplex, the search strategy is not explicitly encoded and simply enumerates all the doma ins that have not yet been pruned. The branching rule is the same as for Reluplex, as ﬁxing the decision variableδi[j]= 0 is equivalent to choosing {xi[j]= 0,ˆxi[j]≤0}and ﬁxing δi[j]= 1 is equivalent to{xi[j]= ˆxi[j],ˆxi[j]≥0}. Note however that Planet does not include any heuristic to p rioritise which decision variables should be split over. Planet does not include a mechanism for early termination ba sed on a heuristic search of a feasible point. For satisﬁable problems, only when a full complete as signment is identiﬁed is a solution returned. In order to detect incoherent assignments, Ehler s [6] introduces a global linear approxi mation to a neural network, which is used as a bounding method to overapproximate the set of values that each hidden unit can take. In addition to the exis ting linear constraints ((10a), (10b) and (10h)), the nonlinear constraints are approximated by sets of linear constraints representing the nonlinearities’ convex hull. Speciﬁcally, ReLUs with inp ut of unknown sign are replaced by the set of equations: xi= max(ˆ xi,0)⇒xi≥ˆ xi(5a) xi≥0 (5b) xi[j]≤ui[j]ˆxi[j]−li[j] ui[j]−li[j](5c) wherexi[j]corresponds to the value of the jth coordinate of xi. An illustration of the feasible domain is provided in the supplementary material. Compared with the relaxation of Reluplex (4), the Planet rel axation is tighter. Speciﬁcally, Eq. (4a) and (4b) are identical to Eq. (5a) and (5b) but Eq. (5c) implie s Eq. (4c). Indeed, given that ˆxi[j] is smaller than ui[j], the fraction multiplying ui[j]is necessarily smaller than 1, implying that this provides a tighter bounds on xi[j]. To use this approximation to compute better bounds than the o nes given by simple interval arithmetic, it is possible to leverage the feedforward structure of the neural networks and obtain bounds one layer at a time. Having included all the constraints up until theith layer, it is possible to optimize over the resulting linear program and obtain bounds for all t he units of the ith layer, which in turn will allow us to create the constraints (5) for the next layer . In addition to the pruning obtained by the convex relaxation , both Planet and Reluplex make use of conﬂict analysis [15] to discover combinations of splits th at cannot lead to satisﬁable assignments, allowing them to perform further pruning of the domains. 4.3 Potential improvements As can be seen, previous approaches to neural network veriﬁc ation have relied on methodologies developed in three communities: optimization, for the crea tion of upper and lower bounds; veriﬁca tion, especially SMT; and machine learning, especially the feedforward nature of neural networks for the creation of relaxations. A natural question that ari ses is “Can other existing literature from these domains be exploited to further improve neural networ k veriﬁcation?” Our uniﬁed branchand bound formulation makes it easy to answer this question. To i llustrate its power, we now provide a nonexhaustive list of suggestions to speedup veriﬁcatio n algorithms. 5Better bounding — While the relaxation proposed by Ehlers [6] is tighter than t he one used by Reluplex, it can be improved further still. Speciﬁcally, af ter a splitting operation, on a smaller domain, we can reﬁne all the li,uibounds, to obtain a tither relaxation. We show the importanc e of this in the experiments section with the BaBrelusplit method that performs splitting on the activation like Planet but updates its approximation compl etely at each step. One other possible area of improvement lies in the tightness of the bounds used. Equation (5) is very closely related to the Mixed Integer Formulation of Equ ation (20). Indeed, it corresponds to level 0 of the SheraliAdams hierarchy of relaxations [18]. The proof for this statement can be found in the supplementary material. Stronger relaxations could be obtained by exploring higher levels of the hierarchy. This would jointly constrain group s of ReLUs, rather than linearising them independently. Better branching The decision to split on the activation of the ReLU nonlinea rities made by Planet and Reluplex is intuitive as it provides a clear set of decisi on variables to ﬁx. However, it ignores another natural branching strategy, namely, splitting the input domain. Indeed, it could be argued that since the function encoded by the neural networks are pi ecewise linear in their input, this could result in the computation of highly useful upper and lower bo unds. To demonstrate this, we propose the novel BaBinput algorithm: a branchandbound method that branches over th e input features of the network. Based on a domain with input constrained by Eq . (10a), the split function would return two subdomains where bounds would be identical in all dimension except for the dimension with the largest length, denoted i⋆. The bounds for each subdomain for dimension i⋆are given by l0[i⋆]≤x0[i⋆]≤l0[i⋆]+u0[i⋆] 2andl0[i⋆]+u0[i⋆] 2≤x0[i⋆]≤u0[i⋆]. Based on these tighter input bounds, tighter bounds at all layers can be reevaluated. One of the main advantage of branching over the variables is t hat all subdomains generated by the BaB algorithm when splitting over the input variables end up only having simple bound constraints over the value that input variable can take. In order to explo it this property to the fullest, we use the highly efﬁcient lower bound computation approach of Kol ter & Wong [13]. This approach was initially proposed in the context of robust optimization. H owever, our uniﬁed framework opens the door for its use in veriﬁcation. Speciﬁcally, Kolter & Wo ng [13] identiﬁed an efﬁcient way of computing bounds for the type of problems we encounter, by ge nerating a feasible solution to the dual of the LP generated by the Planet relaxation. While this bound is quite loose compared to the one obtained through actual optimization, they are very fast to evaluate. We propose a smart branching method BaBSB to replace the longest edge heuristic of BaBinput . For all possible splits, we compute fast bounds for each of the resulting subdomain, a nd execute the split resulting in the highest lower bound. The intuition is that despite their loo seness, the fast bounds will still be useful in identifying the promising splits. 5 Experimental setup The problem of PLNN veriﬁcation has been shown to be NPcomp lete [11]. Meaningful compari son between approaches therefore needs to be experimental. 5.1 Methods "
86,Leveraging Systematic Knowledge of 2D Transformations.txt,"The existing deep learning models suffer from out-of-distribution (o.o.d.)
performance drop in computer vision tasks. In comparison, humans have a
remarkable ability to interpret images, even if the scenes in the images are
rare, thanks to the systematicity of acquired knowledge. This work focuses on
1) the acquisition of systematic knowledge of 2D transformations, and 2)
architectural components that can leverage the learned knowledge in image
classification tasks in an o.o.d. setting. With a new training methodology
based on synthetic datasets that are constructed under the causal framework,
the deep neural networks acquire knowledge from semantically different domains
(e.g. even from noise), and exhibit certain level of systematicity in parameter
estimation experiments. Based on this, a novel architecture is devised
consisting of a classifier, an estimator and an identifier (abbreviated as
""CED""). By emulating the ""hypothesis-verification"" process in human visual
perception, CED improves the classification accuracy significantly on test sets
under covariate shift.","Machine learning algorithms based on deep neural networks (DNNs) have made dramatic progress in the eld of computer vision in the last decade. Most of these algorithms strongly rely on the assumption of i.i.d.,i.e., the training data and test data are independent and identically distributed. In practice, how ever, the i.i.d. assumption can be easily violated due to covariate shift in test datasets [1, 4, 15, 17], which causes signicant performance drop of the models learned from the training set. This is investigated as the o.o.d. generalization problem, which has become one of the main challenges that the deep learning community encounters nowadays. One of the common stopgaps for this prob lem is to continuously expand the size of datasets, in order to strengthen the learned invariance of the target objects, by getting rid of other mechanisms or factors of variation. For example, ImageNet [10], which is a typical dataset for training classication and detection algorithms, contains more than 14 mil lion images. Even so, popular classication models trained with ImageNet have 1arXiv:2206.00893v1  [cs.CV]  2 Jun 2022(a)(b)(c)Figure 1: What is in image (a)? There are at least two ways to interpret it, i.e., (b) three black circles partly covered by a white triangle, or (c) three black circles with a notch on each of them. (The former one may have a stronger tendency in perception, according to the Gestalt principles [19].) experienced 40"
253,Fast and Incremental Loop Closure Detection Using Proximity Graphs.txt,"Visual loop closure detection, which can be considered as an image retrieval
task, is an important problem in SLAM (Simultaneous Localization and Mapping)
systems. The frequently used bag-of-words (BoW) models can achieve high
precision and moderate recall. However, the requirement for lower time costs
and fewer memory costs for mobile robot applications is not well satisfied. In
this paper, we propose a novel loop closure detection framework titled `FILD'
(Fast and Incremental Loop closure Detection), which focuses on an on-line and
incremental graph vocabulary construction for fast loop closure detection. The
global and local features of frames are extracted using the Convolutional
Neural Networks (CNN) and SURF on the GPU, which guarantee extremely fast
extraction speeds. The graph vocabulary construction is based on one type of
proximity graph, named Hierarchical Navigable Small World (HNSW) graphs, which
is modified to adapt to this specific application. In addition, this process is
coupled with a novel strategy for real-time geometrical verification, which
only keeps binary hash codes and significantly saves on memory usage. Extensive
experiments on several publicly available datasets show that the proposed
approach can achieve fairly good recall at 100\% precision compared to other
state-of-the-art methods. The source code can be downloaded at
https://github.com/AnshanTJU/FILD for further studies.","A mobile robot should have the ability of exploring unknown places and constructing the reliable map of environ ment while simultaneously using the map for the autonomous localization. The task is deﬁned as the Simultaneous Lo calization And Mapping (SLAM) [1], [2], which is one of the most central topics in robotics research. In SLAM, one major problem is Loop Closure Detection (LCD), that is, the robot must determine whether it has returned to a previously mapped area. With the increase in computing power, the mobile robots not only use range and bearing sensors such as laser scanners [3], radars and sonars [4], but also use single cameras [5] or stereocamera rigs [6]. Exploiting the appearance information of a scene to detect previous visited places is called Visual Loop Closure Detection [7], [8], [9]. The visual loop closure detection problem can be con verted into an online image retrieval task to determine if This work was supported in part by the Chinese National Key Research and Development Plan (2018YFB1305803), Chinese National Natural Sci ence Foundation (61673245), Chinese National Programs for High Tech nology Research and Development (2015AA042307). 1Shan An, Guangfu Che, Fangru Zhou and Yu Chen are with AR/VR department, JD.com, Beijing, China fanshan, cheguangfu1, zhoufangru, chenyu6 g@jd.com 2Xianglong Liu is with School of Computer Science and Engineering, Beihang University, Beijing, China xlliu@buaa.edu.cn 3Xin Ma is with School of Control Science and Engineering, Shandong University, Jinan, China maxin@sdu.edu.cn Corresponding Author 2038 638Fig. 1. The representation of image matching using CasHash [10] and the proposed binary ratio test on Malaga 2009 Parking 6L [11] dataset. (Top Left) The query image captured by the robot. (Top Right) The loop closure image which is returned by our system. (Bottom) The matches of two images are shown, which passed the binary ratio test and the RANSAC algorithm. the current image has been taken from a known location. Conventional methods quantize the descriptor space of local features into Visual Words (VW), whether ﬂoatingpoint features, such as SIFT [12], SURF [13] or binary features, such as BRIEF [14], ORB [15]. The so called BoW [16] employs the widely used term frequencyinverse document frequency (tfidf) technique to create a VW histogram. Pre visited areas can be identiﬁed based on voting techniques [17] for place recognition. The Convolutional Neural Networks (CNN) are designed to beneﬁt and learn from massive amounts of data, which has demonstrated high performance in image classiﬁcation [18] and scene recognition [19]. Recently, with the outstanding discrimination power of CNN features, the landmarks in images are detected and matched for visual place recognition [20], which achieves better recognition accuracy than local features because of their invariance to illumination and their highlevel semantics. In this paper, we present a novel algorithm to detect loop closure, which is realtime and scalable, with the database built online and incrementally. Our approach is based on both the CNN features and SURF features, and using one type of proximity graph, named Hierarchical Nav igable Small World (HNSW) graphs [21]. Several important novelties have been proposed, which make our algorithm much faster than current approaches. The images captured along the trajectory of the mobile robot is ﬁrstly described using the features of the pretrained CNN. These features arearXiv:1911.10752v1  [cs.RO]  25 Nov 2019used to construct the HNSW graphs by adding them into the graphs, and later they will be retrieved to get the top nearest neighbors according to image similarity. Finally, the geomet rical consistency is conﬁrmed using SURF features matched by CasHash [10] and RANSAC. The main contributions of this paper are summarized as follows: A framework which uses CNN features and Hierarchical Navigable Small World graphs [21] to enable the incre mental construction of the searching index and offer extremely fast online retrieval performance. A novel strategy for realtime geometrical veriﬁcation, with the important feature of using Hamming distances instead of Euclidian distances to perform the ratio test. The system only keeps binary hash codes instead of ﬂoatpoint descriptors, which will signiﬁcantly save memory usage. The source code of our implementation will be released to academia to facilitate future studies. The rest of the paper is organized as follows. In Section II, we summarize relevant prior research in loop closure detection. In Section III, the proposed algorithm is described in detail. Our experimental design and comparative results are presented in Section IV . Conclusions and future work are discussed in Section V . II. RELATED WORK "
49,Cost and Effects of Pinning Control for Network Synchronization.txt,"In this paper, the problem of pinning control for synchronization of complex
dynamical networks is discussed. A cost function of the controlled network is
defined by the feedback gain and the coupling strength of the network. An
interesting result is that lower cost is achieved by the control scheme of
pinning nodes with smaller degrees. Some rigorous mathematical analysis is
presented for achieving lower cost in the synchronization of different
star-shaped networks. Numerical simulations on some non-regular complex
networks generated by the Barabasi-Albert model and various star-shaped
networks are shown for verification and illustration.","Complex networks are currently being studied across many ﬁe lds of sciences, including physics, chemistry, biology, mathematics, sociology and engineeri ng [1, 2, 3, 5, 9, 15, 17, 19]. A complex network is a large set of interconnected nodes, in which a nod e is a fundamental unit with speciﬁc contents. Examples of complex networks include the Interne t, food webs, cellular neural networks, biological neural networks, electrical power grids, telep hone cell graphs, etc. Recently, synchro nization of complex networks of dynamical systems has recei ved a great deal of attention from the nonlinear dynamics community [10, 12, 16, 18, 20]. A special control strategy called pinning control is used to achieve synchronization of complex networks; tha t is, only a fraction of the nodes or even a single node is controlled over the whole network [4, 6, 11, 2 1]. This control method has become a common technique for control, stabilization and synchroni zation of coupled dynamical systems. In general, diﬀerent nodes have diﬀerent degrees in a network, th us a natural question is how diﬀerent the eﬀect would be when nodes with diﬀerent degrees are pinned. Consider a dynamical network consisting of N identical and d iﬀusively coupled nodes, with each node being an ndimensional dynamical system. The state equations of the n etwork are ˙xi(t) =f(xi(t),t)+cN/summationdisplay j=1aijΓxj(t), i= 1,2,···,N, (1) wheref(·) is the dynamical function of an isolated node, xi= (xi1,xi2,···,xin)∈Rnare the state variables of node i, constant c >0 represents the coupling strength, and Γ ∈RN×Nis the ∗This work is supported by the National Science Foundation of China under grants 60674093, 60334030. †Corresponding author: lirong@pku.edu.cn 1inner linking matrix. Moreover, the coupling matrix A= (aij)∈RN×Nrepresents the coupling conﬁguration of the network: If there is a connection betwee n nodeiand node j(i/ne}ationslash=j), then aij=aji= 1; otherwise, aij=aji= 0 (i/ne}ationslash=j); the diagonal entries of Aare deﬁned by aii=−N/summationdisplay j=1 j/negationslash=iaij, i= 1,2,···,N. (2) Suppose that the network is connected in the sense of having n o isolated clusters. Then, the coupling matrix Ais irreducible. From Lemma 2 of [20], it can be proved that zer o is an eigenvalue ofAwith multiplicity one and all the other eigenvalues of Aare strictly negative. Network (1) is said to achieve (asymptotical) synchronizat ion if x1(t)→x2(t)→ ··· → xN(t)→s(t),ast→ ∞, (3) where, because of the diﬀusive coupling conﬁguration, s(t) is a solution of an isolated node, which can be an equilibrium, a periodic or a chaotic orbit. As shown in [4, 6, 11, 21], this can be achieved by controlling several nodes (or even only one node) of the ne twork. Without loss of generality, supposethat the controllers are added on the last N−knodes of the network, so that the equations of the controlled network can be written as ˙xi(t) =f(xi(t),t)+c/summationtextN j=1aijΓxj(t), i = 1,2,···,k, ˙xi(t) =f(xi(t),t)+c/summationtextN j=1aijΓxj(t)−cεiΓ(xi(t)−s(t)), i=k+1,k+2,···,N,(4) where the feedback gains εiare positive constants. It can be seen that synchronizing al l statesxi(t) tos(t) is determined by the dynamics of an isolated node, the coupl ing strength c >0, the inner linking matrix Γ, the feedback gains εi≥0, and the coupling matrix A. As discussed in [6, 11, 21], to achieve synchronization of co mplex dynamical networks, the con trollers are generally preferred to be added to the nodes wit h larger degrees. However, it is also known that, to achieve a certain synchronizability of the ne twork, the feedback gains εiusually have to be quite large. In [4], when a single controller is use d, the coupling strength chas to be quite large in general. From the view point of realistic appl ications, these are not expected and sometimes cannot be realized. Practically, a designed cont rol strategy is expected to be eﬀective and also easily implementable. In this paper, for various st arshaped networks and nonregular complex networks, a new concept of cost function is introduc ed to evaluate the eﬃciency of the designed controllers. It is found that surprisingly the cos t can be much lower by controlling nodes with smaller degrees than controlling nodes with larger deg rees. As will beseen, moreover, both the feedback gains εiand the coupling strength ccan be much smaller than those used in [4, 6, 11, 21]. The outline of this paper is as follows. In Section 2, a new deﬁ nition of cost function and some mathematical preliminaries are given. Stability of di ﬀerent starshaped networks controlled by pinning some nodes with small degrees are analyzed in Sect ions 3 and 4, respectively, where some simulated examples of dynamical networks are compared for illustration and veriﬁcation. In Section 5, pinning control of nonregular complex dynami cal networks of chaotic oscillators is studied through numerical simulations. Finally, Section 6 concludes the paper. 22 The cost of pinning control Denoteei(t) =xi(t)−s(t), where s(t) satisﬁes ˙ s(t) =f(s(t)). Then, the error equations of network (1) can be written as ˙ei(t) =f(xi(t),t)−f(s(t))+cN/summationdisplay j=1aijΓej(t), i = 1,2,···,N, while the error equations of the controlled network (4) can b e written as ˙ei(t) =f(xi(t),t)−f(s(t))+cN/summationdisplay j=1˜aijΓej(t), i = 1,2,···,N, (5) where ˜aii=aii−εi, εi>0, i=k+1,k+2,···,N, and ˜aij=aijotherwise. Let ˜A= (˜aij)∈RN×N, and denote e(t) = (e1(t),e2(t),···,eN(t))T. Diﬀerentiating (5) along s(t) gives ˙e(t) =D(f(s(t)))e(t)+cΓe(t)˜AT. (6) By analyzing the matrix ˜AT, it is easy to see that all the eigenvalues of ˜ATare negative, which are denoted by 0> λ1≥ ··· ≥λN. There exists an orthogonal matrix Usuch that ˜AT=UJU−1, whereJ=diag{λ1,λ2,···,λN}. Let ˜e(t) =e(t)U. Then, from (6), one has ˙˜ei(t) = [Df(s(t))+cλiΓ]˜ei(t), i = 1,···,N. (7) Therefore, the local stability problem of network (4) is con verted into the stability problem of the Nindependent linear systems (7). When the system function of an isolated node and the inner linking matrix Γ are ﬁxed, the stability problem of systems ( 7) are dependent on the coupling strength cand the eigenvalues of ˜A. Clearly, the smaller the eigenvalues of the matrix ˜Aare (λi<0, i= 1,···,N), the smaller the coupling strength c >0 is needed to guarantee the same synchronizability of the network (4), if systems (7) have un bounded synchronization regions [18]. In the following, a cost function is introduced to describe t he eﬃciency of the controllers. Deﬁnition 1 (Cost Function) Suppose that the feedback gain matrix is G=diag{ε1,···, εN}, whereεi≥0, i= 1,···,N, are given as in (4). The Cost Function is deﬁned as CF=cN/summationdisplay i=1εi. Remark 1 The smaller the CF, the more eﬃcient a control strategy to achieve the same goal of control, and the easier to be implemented. In order to discuss the eﬀects of pinning control, the followi ng Lemmas are needed. Lemma 2 [8] LetA= [aij]∈Cn×nbe Hermitian, and let ann≤ ··· ≤ a22≤a11be a rearrangement of its diagonal entries in increasing order. Let the eigenvalues of Abe ordered as λmin=λn≤λn−1≤ ··· ≤λ2≤λ1=λmax. (8) Then (i)λn≤aii≤λ1for alli= 1,···,n, (ii)a11+a22≤λ2,ifλ1= 0. 3Remark 2 For diﬀusive networks, the smaller the λ2, the easier the synchronization, if the network has an unbound synchronized region [18]. However, L emma 2 shows that λ2is related to a11+a22, wherea11anda22are determined by two smallest nodes. Therefore, in this cas e, in order to improve the synchronizability, these small nodes should be pinned. Lemma 3 LetA∈Cn×nbe Hermitian, and ˜A=/bracketleftBigg A1A12 AT 21A2+D/bracketrightBigg , whereA1∈Rk×k,A12∈Rk×(N−k),A2∈R(N−k)×(N−k)andD=diag{εk+1,···,εN}. Then, ˜A <−αI, α > 0, if and only if A1<−αI1andA2+D−AT 12(A1+αI1)−1A12<−αI2, whereI1 andI2are identity matrixes with appropriate dimensions. Remark 3 MatrixD, i.e.εican be determined by the LMI method [7]. Suppose the eigenval ues "
93,Correlation Verification for Image Retrieval.txt,"Geometric verification is considered a de facto solution for the re-ranking
task in image retrieval. In this study, we propose a novel image retrieval
re-ranking network named Correlation Verification Networks (CVNet). Our
proposed network, comprising deeply stacked 4D convolutional layers, gradually
compresses dense feature correlation into image similarity while learning
diverse geometric matching patterns from various image pairs. To enable
cross-scale matching, it builds feature pyramids and constructs cross-scale
feature correlations within a single inference, replacing costly multi-scale
inferences. In addition, we use curriculum learning with the hard negative
mining and Hide-and-Seek strategy to handle hard samples without losing
generality. Our proposed re-ranking network shows state-of-the-art performance
on several retrieval benchmarks with a significant margin (+12.6% in mAP on
ROxford-Hard+1M set) over state-of-the-art methods. The source code and models
are available online: https://github.com/sungonce/CVNet.","Image retrieval is a longstanding problem in computer vision. This task aims to sort a database of images based on their similarities to the given query image. For this task, global retrieval through global descriptor matching and ge ometric veriﬁcation after local feature matching are mainly employed. These approaches typically comprise two pri mary components of the image retrieval framework that mu tually complement one another. The global retrieval quickly performs a coarse retrieval across the database, and geo metric veriﬁcation reranks the coarse results by performing precise evaluation only on the potential candidates. Along with deep learning, image retrieval has also advanced sig niﬁcantly. In particular, several studies [8,30,41,45,46,53] have been focused on extracting representative and distinc tive features for global and local representations with deep learning. However, geometric veriﬁcation after local fea ture matching still plays an essential role in the reranking *Corresponding author. 1ststep : Matching 2ndstep: Verification (a) Geometric Verification (b) Correlation Verification (Ours) CVNet Count # of inliers Image Similarity Dense CorrelationFigure 1. Novel image retrieval reranking method named correla tion veriﬁcation that directly predicts image similarity by leverag ing dense feature correlation in a convolutional manner. task in image retrieval, despite its drawbacks. Owing to itsverifyaftermatching structure, geometric veriﬁcation is performed based on only sparse and thresholded feature correspondence. Moreover, it is neither learnable nor dif ferentiable and requires iterative optimization even during testing. In addition, geometric veriﬁcation does not include a component that can handle multiscale operation. Thus, several studies [8, 30, 32, 45] have attempted to solve the scale problem by repeating inference with the image pyra mid to extract multiscale local features. However, this is an extremely expensive process. In this study, we propose an endtoend learnable re ranking network called Correlation Veriﬁcation Networks (CVNet) to replace the role of geometric veriﬁcation in a better way. The proposed network directly evaluates se mantic and geometric relations by leveraging dense feature correlations in a convolutional manner. Following the suc cessful architectural design of representative 2D convolu tional neural networks (CNN), we design a 4D CNN with a pyramid structure of deeply stacked 4D convolution layers. It compresses the correlation between semantic cues into image similarity while learning diverse geometric match ing patterns from a large number of image pairs. To en sure robustness even for large scale difference problems, itarXiv:2204.01458v1  [cs.CV]  4 Apr 2022expands the singlescale feature to a feature pyramid for each image, forming crossscale correlations between fea ture pyramids. This structure enables crossscale matching with a single inference while replacing the multiscale in ference conventionally used in image retrieval. Our model does not require additional inference to extract local infor mation; therefore the feature extraction latency, which sig niﬁcantly affects online retrieval time, is considerably re duced compared with other reranking methods. Similar to several computer vision problems, image retrieval suf fers from the problem of hard samples. We address these challenges through curriculum learning using the hard neg ative mining and HideandSeek [43] strategy in the training phase. This improves the overall performance by focusing on hard samples without losing generality in the case of nor mal ones. Our proposed reranking network shows stateof theart performance on several image retrieval benchmarks with a signiﬁcant margin over several stateoftheart meth ods. Our main contributions are as follows: • We present Correlation Veriﬁcation Networks (CVNet), which is a powerful reranking model that directly pre dicts the similarity of an image pair based on dense fea ture correlation. • To replace expensive multiscale inference, we construct crossscale correlations within the model and perform crossscale matching using a single inference. • We propose curriculum learning using the hard negative mining and HideandSeek strategy to handle hard sam ples without losing generality. • The proposed model achieves new stateoftheart performance on several image retrieval benchmarks: ROxford (+1M),RParis (+1M), and GLDv2retrieval. 2. Related Work "
379,Scalable Quantitative Verification For Deep Neural Networks.txt,"Despite the functional success of deep neural networks (DNNs), their
trustworthiness remains a crucial open challenge. To address this challenge,
both testing and verification techniques have been proposed. But these existing
techniques provide either scalability to large networks or formal guarantees,
not both. In this paper, we propose a scalable quantitative verification
framework for deep neural networks, i.e., a test-driven approach that comes
with formal guarantees that a desired probabilistic property is satisfied. Our
technique performs enough tests until soundness of a formal probabilistic
property can be proven. It can be used to certify properties of both
deterministic and randomized DNNs. We implement our approach in a tool called
PROVERO and apply it in the context of certifying adversarial robustness of
DNNs. In this context, we first show a new attack-agnostic measure of
robustness which offers an alternative to purely attack-based methodology of
evaluating robustness being reported today. Second, PROVERO provides
certificates of robustness for large DNNs, where existing state-of-the-art
verification tools fail to produce conclusive results. Our work paves the way
forward for verifying properties of distributions captured by real-world deep
neural networks, with provable guarantees, even where testers only have
black-box access to the neural network.","The past few years have witnessed an increasing adoption of deep neural networks (DNNs) in domains such as au tonomous vehicles [5], [35], [52], drones [24] or robotics [16], [67], where mispredictions can have serious longterm con sequences. Robustness, privacy, and fairness have emerged as central concerns to be addressed for safe adoption of DNNs [1], [9], [33], [37], [43], [47], [51]. Consequently, there has been a growing attention to testing and veriﬁcation of neural networks for properties of interest. To establish that the resulting DNNs have the desired prop erties, a large body of prior work has focused on techniques based on empirical testing [19], [53], [56], [65] or specialized attack vectors [2], [20], [36], [42], [47], [59]. While such techniques are useful, they do not rigorously quantify how sure we can be that the desired property is true after testing. In contrast to testing approaches, formal veriﬁcation seeks to provide rigorous guarantees of correctness. Inspired by the 1The name is a pun on prover `o(I will prove it) and provero (protruth) in Italian. Code and benchmarks are available at teobaluta.github.io/proverosuccess of model checking in the context of hardware and software veriﬁcation, the earliest formal veriﬁcation method ologies in the context of deep neural networks focused on qualitative veriﬁcation, i.e., whether a system satisﬁes a given speciﬁcation. Prior work in this category has been following the model checking paradigm wherein a given DNN is encoded as a model using constraints grounded in a chosen theory. Then a satisﬁability solver (often modulo the chosen theory) is invoked to check if there exists an execution of the system that violates the given speciﬁcation [14], [26], [39], [44]. The proposed techniques in this category appear to have three limitations. Firstly, they require whitebox access to the models and specialized procedures to transform the DNNs to a speciﬁcation, limiting their generality. Secondly, the perfor mance of the underlying feasibility solver degrades severely with the usage of nonlinear constraints, leading to analyses that do not scale to larger models. Thirdly, prior techniques are limited to deterministic neural networks, while extensive research effort has been invested in designing randomized DNNs, especially to enhance robustness [11], [12], [33]. Such qualitative veriﬁcation considers only two scenarios: either a DNN satisﬁes the property, or it does not. How ever, neural networks are stochastically trained, and more importantly, they may run on inputs drawn from an unknown distribution at inference time. Properties of interest are thus often probabilistic and deﬁned over an input distribution (e.g., fairness [37] or robustness to distributional changes [1]). Hence, qualitative veriﬁcation is unsuitable for such properties. An alternative approach is to check how often a property is satisﬁed by a given DNN under a given input distribution. More speciﬁcally, one can assert that a DNN satisﬁes a property with a desirably high probability 1"
294,Speaker Verification using Convolutional Neural Networks.txt,"In this paper, a novel Convolutional Neural Network architecture has been
developed for speaker verification in order to simultaneously capture and
discard speaker and non-speaker information, respectively. In training phase,
the network is trained to distinguish between different speaker identities for
creating the background model. One of the crucial parts is to create the
speaker models. Most of the previous approaches create speaker models based on
averaging the speaker representations provided by the background model. We
overturn this problem by further fine-tuning the trained model using the
Siamese framework for generating a discriminative feature space to distinguish
between same and different speakers regardless of their identity. This provides
a mechanism which simultaneously captures the speaker-related information and
create robustness to within-speaker variations. It is demonstrated that the
proposed method outperforms the traditional verification methods which create
speaker models directly from the background model.","In speaker veriﬁcation (SV), the identity of a query spoken utterance should be conﬁrmed by comparing to the gallery of known speakers. The speaker veriﬁ cation can be categorized to textdependent and text independent. In textindependent, no restriction is con sidered for the utterances. On the other hand, in text dependent setting, all speakers repeat the same phrase. Due to the variational nature of the former setup, it considers being a more challenging task since the sys tem must be able to clearly distinguish between the speaker and nonspeaker characteristics of the uttered phrases. The general procedure of speaker veriﬁcation consists of three phases: Development, enrollment, and evaluation. For development, a background model must be created for capturing the speakerrelated information. In enrollment, the speaker models are created using the background model. Finally, in the evaluation, thequery utterances are identiﬁed by comparing to existing speaker models created in the enrollment phase. Recently, with the advent of deep learning in different applications such as speech, image recognition and net work pruning [1]–[4], datadriven approaches using Deep Neural Networks (DNNs) have also been proposed for effective feature learning for Automatic Speech Recog nition (ASR) [3] and Speaker Recognition (SR) [5], [6]. Also deep architecture has mostly been treated as black boxes, some approaches based on Information Theory [7], have been presented for multimodal feature extraction and demonstrated promising results [8]. Some traditional successful model for speaker veriﬁca tion are Gaussian Mixture ModelUniversal Background Model (GMMUBM) [9] and ivector [10]. The main disadvantage of these models is their unsupervised na ture since there are not trained objectively for speaker veriﬁcation setup. Some methods have been proposed to supervise the aforementioned models training such as SVMbased GMMUBMs [11] and PLDA for ivectors model [12]. With the advent of Convolutional Neural Networks (CNNs) and their promising results for action recognition [13], scene understanding [14], recently they have been proposed as well for speaker and speech recognition [6], [15]. In this work, we propose to use the Siamese neural networks to operate one traditional speech features such as MFCCs1instead of raw feature for having a higher level representation for speakerrelated characteristics. Moreover, we show the advantage of utilizing an effec tive pair selection method for veriﬁcation purposes. 1Mel Frequency Cepstral CoefﬁcientsarXiv:1803.05427v2  [eess.AS]  10 Aug 2018II. R ELATED WORKS "
126,Polarimetric Thermal to Visible Face Verification via Attribute Preserved Synthesis.txt,"Thermal to visible face verification is a challenging problem due to the
large domain discrepancy between the modalities. Existing approaches either
attempt to synthesize visible faces from thermal faces or extract robust
features from these modalities for cross-modal matching. In this paper, we take
a different approach in which we make use of the attributes extracted from the
visible image to synthesize the attribute-preserved visible image from the
input thermal image for cross-modal matching. A pre-trained VGG-Face network is
used to extract the attributes from the visible image. Then, a novel Attribute
Preserved Generative Adversarial Network (AP-GAN) is proposed to synthesize the
visible image from the thermal image guided by the extracted attributes.
Finally, a deep network is used to extract features from the synthesized image
and the input visible image for verification. Extensive experiments on the ARL
Polarimetric face dataset show that the proposed method achieves significant
improvements over the state-of-the-art methods.","Face Recognition (FR) is one of the most widely studied problems in computer vision and biometrics research com munities due to its applications in authentication, surveil lance and security. Various methods have been developed over the last two decades that speciﬁcally attempt to address the challenges such as aging, occlusion, disguise, variations in pose, expression and illumination. In particular, con volutional neural network (CNN) based FR methods have gained a lot of traction in recent years [24, 23]. Deep CNN based methods [19, 29, 35, 2, 20, 21] have achieved impres sive performances on the current FR benchmarks. Despite the success of CNNbased methods in address ing various challenges in FR, they are fundamentally lim ited to recognize face images that are collected nearvisible V isible S0 DoLP S1 S2  PolarFigure 1. Sample Stokes as well as polarimetric and visible images corresponding to a subject in the ARL dataset [9]. spectrum. In many practical scenarios such as surveil lance in lowlight conditions, one has to detect and rec ognize faces that are captured using thermal modalities [9, 27, 31, 36, 26, 14, 18, 16, 1]. However, the perfor mance of many deep learningbased methods degrades sig niﬁcantly when they are presented with thermal face im ages. For example, it was shown in [36, 26] that simply using deep features extracted from both raw polarimetric thermal and visible facial images are not sufﬁcient enough for crossdomain face recognition. The performance degra dation is mainly due to the signiﬁcant distributional change between the thermal and visible domains as well as a lack of sufﬁcient data for training the deep networks for cross modal matching. In many recent approaches, the polarizationstate infor mation of thermal emissions has been used to achieve im proved crossspectrum face recognition performance [9, 27, 31, 36, 26] since it captures geometric and textural details of faces that are not present in the conventional thermal fa cial images [31, 9]. A polarimetric thermal image consistsarXiv:1901.00889v1  [cs.CV]  3 Jan 2019of four Stokes images: S0,S1,S2, and degreeoflinear polarization (DoLP), where S0indicates the conventional total intensity thermal image, S1captures the horizontal and vertical polarizationstate information, S2captures the di agonal polarizationstate information and DoLP describes the portion of an electromagnetic wave that is linearly po larized [9]. These Stokes images along with the visible and the polarimetric images corresponding to a subject in the ARL dataset [9] are shown in Figure 1. It can be observed thatS1,S2and DoLP tend to preserve more textural details compared to S0. Similar to [36, 26], we also refer to Polar as the three channel polarimetric image with S0,S1andS2 as the three channels. Several attempts have been made to address the polari metric thermalvisible face recognition problem [26, 27, 36]. For instance, Riggan et al. [27] proposed a twostep procedure (visible feature estimation and visible image re construction) to solve this crossmodal matching problem. Zhang et al. [36] proposed an endtoend generative adver sarial network by fusing the different Stokes images as a multichannel input to synthesize the visible image given the corresponding polarimetric signatures. Recently, Rig ganet al. [26] developed a global and local regionbased technique to improve the discriminative quality of the syn thesized visible imagery. Though these methods are able to synthesize photorealistic visible face images to some extent, the synthesized results in [36, 25, 26] are still far from optimal and they tend to lose some semantic attribute information such as mouth open, mustache, etc. Such re constructions may degrade the performance of thermal to visible face veriﬁcation. In this paper, we take a different approach to the prob lem of thermal to visible matching. Figure 2 compares the traditional crossmodal veriﬁcation problem with that of the proposed attributepreserved crossmodal veriﬁcation approach. Given a visible and thermal pair, the traditional approach ﬁrst extracts some features from these images and then veriﬁes the identity based on the extracted features [14] (see Figure 2(b)). In contrast, we propose a novel frame work in which we make use of the attributes extracted from the visible image to synthesize the attributepreserved visi ble image from the input thermal image for matching (see Figure 2(b)). In particular, a pretrained VGGFace model [19] is used to extract the attributes from the visible image. Then, a novel Attribute Preserved Generative Adversarial Network (APGAN) is proposed to synthesize the visible image from the thermal image guided by the extracted at tributes. Finally, a deep network is used to extract features from the synthesized and the input visible images for veri ﬁcation. The proposed APGAN model is inspired by the recent image generation from attributes/text works [25, 36, 3]. The APGAN consists of two parts: (i) a multimodal compact GeneratorFeature ExtractorFeature ExtractorVerify (b) Proposed Heterogeneous Face V eriﬁcation Feature ExtractorFeature Extractor Verify Attribute Predictor (a) T raditional Heterogeneous Face V eriﬁcationthermal face visible face thermal face attributes visible facesynthesized faceFigure 2. (a) Traditional heterogeneous face veriﬁcation ap proaches use the features directly extracted from different modal ities for veriﬁcation [8, 14, 11, 34]. (b) The proposed heteroge neous face veriﬁcation approach uses a thermal face and semantic attributes to synthesize a visible face. Finally, deep features ex tracted from the synthesized and visible faces are used for veriﬁ cation. bilinear (MCB) poolingbased generator [4, 5], and (ii) a tripletpair discriminator. The generator fuses the extracted attribute vector with the image feature vector in the latent space. On the other hand, the discriminator uses triplet pairs (real image/true attributes, fake image/true attributes, real image/wrong attributes) to not only discriminate between real and fake images but also to discriminate between the image and the attributes. In order to generate highquality and attributepreserved images, the generator is optimized by a multipurpose objective function consisting of adver sarial loss [6], L1loss, perceptual loss [12], identity loss [36] and attribute preserving loss. The entire APGAN framework is shown in Figure 3. To summarize, the following are our main contributions: A novel thermalvisible face veriﬁcation framework is proposed in which APGAN is developed for synthe sizing visible faces from thermal (conventional or po larimetric) images using facial attributes. A novel MCB pooling [4, 5] based generator is pro posed to fuse the given attributes with the image fea tures. A novel tripletpair discriminator is proposed, where the discriminator [25] not only learns to discriminate between real/fake images but also to discriminate be tween the image and the corresponding semantic at tributes. Extensive experiments are conducted on the ARL Facial Database [9] and comparisons are performed(a) APGAN Framework (b) Multimodal Compact Bilinear (MCB) pooling Figure 3. (a) A Unet based generator with MCB pooling is proposed to fuse the semantic attribute information with the image feature in the latent space. A tripletpair is adopted for the discriminator in order to discriminate fake/real images as well as the corresponding semantic attributes. In order to generate highquality and attributepreserving images, a multipurpose loss is optimized for training the network. (b) The architecture of MCB. Here, FFT indicates the Fast Fourier Transform and FFT"
402,Efficient Training of Physics-Informed Neural Networks with Direct Grid Refinement Algorithm.txt,"This research presents the development of an innovative algorithm tailored
for the adaptive sampling of residual points within the framework of
Physics-Informed Neural Networks (PINNs). By addressing the limitations
inherent in existing adaptive sampling techniques, our proposed methodology
introduces a direct mesh refinement approach that effectively ensures both
computational efficiency and adaptive point placement. Verification studies
were conducted to evaluate the performance of our algorithm, showcasing
reasonable agreement between the model based on our novel approach and
benchmark model results. Comparative analyses with established adaptive
resampling techniques demonstrated the superior performance of our approach,
particularly when implemented with higher refinement factor. Overall, our
findings highlight the enhancement of simulation accuracy achievable through
the application of our adaptive sampling algorithm for Physics-Informed Neural
Networks.","Physicsinformed neural networks (PINNs) have gained prominence in recent years as a versatile tool for solving partial differential equations (PDEs) governed problems using deep neural networks (DNNs). Although PINNs have demonstrated success, addressing a broad range of increasingly complex PDE problems presents theoretical and practical challenges, necessitating further advance ments to enhance prediction accuracy, computational efficiency, and training robustness [1]. Various techniques, such as loss function metalearning [2], gradientenhanced PINN [3], and adaptive sam pling of residual points [4,5] have been employed to enhance the accuracy of PINNs. Our focus is on improving PINN accuracy through a novel algorithm for adaptive nonuniform sampling. Two common approaches for adaptive sampling methods (ASM) are identified [4]. The first approach (ASM 1) selects points from the original residual set based on a probability mass function (PMF) [6]. Although computationally efficient, ASM 1 only selects the additional point only from the existing set of residual points. Therefore, it does not introduce any new points at different locations within the input space. Previous research has shown that the adaptive location of the resampled points further enhances the accuracy of PINNs [7]. An alternative approach to Adaptive Sampling, reffered as ASM2, considers addition of residual points at new locations within the input space [5]. In ASM2, a random sampling of residual points takes place over the input space, and those with relatively higher PDE residual values are selected. This method facilitates the adaptive positioning of residual points and is intutionally similar to adaptive mesh refinement technique used in numerical methods. However, ASM 2 is computationally expensive as it requires calculation of PDE residual at all randomly chosen points during each resampling period [8]. It is worth noting that recent research studies have introduced various variants of these two adaptive sampling methods [9,10,11, and 12]. In this research, we introduce a novel adaptive sampling scheme for sampling points from new locations in the input space based on the PDE residual of original residual points (ASM 3). The scheme Preprint. Under review.arXiv:2306.08293v1  [cs.LG]  14 Jun 2023consists of three steps. In Step 1, an equispaced grid of residual points is defined as the reference set throughout the training process. In Step 2, new points are sampled from the reference residual points at each resampling period using their probability distribution function (as in ASM 1). In Step 3, a new set of points is added in the neighborhood of each sampled point from Step 2. For a refinement factor of 2, one point is added, while for higher refinement factors, multiple points are added. The mathematical definition of the neighborhood in Step 3 is provided in Section 2. The proposed method exhibits computational efficiency by utilizing the PDE residual on the reference residual points in Step 2, eliminating the need for additional calculations. This efficiency enables a higher frequency of resampling, thereby increasing accuracy. Furthermore, the method achieves adaptive point placement by assigning new points in the vicinity of the sampled points from Step 2, akin to the mesh refinement technique used in numerical studies. In contrast to ASM2, which indirectly refines the grid and is independent of the original set of residual points, this algorithm directly refines the grid formed by the reference set of residual points in Step 1. Additionally, like adaptive mesh refinement, it offers flexibility in assigning higher refinement for improved PINN accuracy. Notably, in order to enhance computational efficiency of ASM3 algorithm, previously sampled points in Step 3 are not retained or utilized in the subsequent resampling events. 2 Direct Grid Refinement Method For the development of ASM 3, we considered a transient PDE case with t∈[0, T]andx∈Ω (where Ω∈RD). In step 1, a set of uniformly spaced residual pointsn ti f, xi foNf i=1are defined with a corresponding grid size of htandhx. At each resampling period, m points ti s, xi s	m i=1are sampled from the set of reference residual points based on a probability mass function (elaborated in section (2.1)). For a refinement factor of 2, a new point ti r, xi r	m i=1is selected in the neighborhood of each sampled point"
116,Verification of Binarized Neural Networks via Inter-Neuron Factoring.txt,"We study the problem of formal verification of Binarized Neural Networks
(BNN), which have recently been proposed as a energy-efficient alternative to
traditional learning networks. The verification of BNNs, using the reduction to
hardware verification, can be even more scalable by factoring computations
among neurons within the same layer. By proving the NP-hardness of finding
optimal factoring as well as the hardness of PTAS approximability, we design
polynomial-time search heuristics to generate factoring solutions. The overall
framework allows applying verification techniques to moderately-sized BNNs for
embedded devices with thousands of neurons and inputs.","Articial neural networks have become essential building blocks in realizing many automated and even autonomous systems. They have successfully been deployed, for example, for perception and scene understanding [17, 21, 26], for control and decision making [7, 14, 19, 29], and also for endtoend solutions of autonomous driving scenarios [5]. Implementations of articial neural networks, however, need to be made much more powerecient in order to deploy them on typical embedded devices with their characteristically limited resources and power constraints. Moreover, the use of neural networks in safetycritical systems poses severe verication and certication challenges [3]. Binarized Neural Networks (BNN) have recently been proposed [9, 16] as a potentially much more powerecient alternative to more traditional feed forward articial neural networks. Their main characteristics are that trained weights, inputs, intermediate signals and outputs, and also activation constraints are binaryvalued. Consequently, forward propagation only relies on bitlevel arithmetic. Since BNNs have also demonstrated good performance on standard datasets in image recognition such as MNIST, CIFAR10 and SVHN [9], they are an attractive and potentially powerecient alternative to current  oatingpoint based implementations of neural networks for embedded applications.arXiv:1710.03107v2  [cs.SE]  19 Jan 2018In this paper we study the verication problem for BNNs. Given a trained BNN and a specication of its intended inputoutput behavior, we develop veri cation procedures for establishing that the given BNN indeed meets its intended specication for all possible inputs. Notice that naively solving verication prob lems for BNNs with, say, 1000 inputs requires investigation of all 21000dierent input congurations. For solving the verication problem of BNNs we build on wellknown meth ods and tools from the hardware verication domain. We rst transform the BNN and its specication into a combinational miter [6], which is then trans formed into a corresponding propositional satisability (SAT) problem. In this process we rely heavily on logic synthesis tools such as ABC[6] from the hardware verication domain. Using such a direct neurontocircuit encoding, however, we were not able to verify BNNs with thousands of inputs and hidden nodes, as encountered in some of our embedded systems case studies. The main challenge therefore is to make the basic verication procedure scale to BNNs as used on current embedded devices. It turns out that one critical ingredient for ecient BNN verication is to factor computations among neurons in the same layer, which is possible due to weights being binary. Such a technique is not applicable within recent works in verication of  oating point neural networks [8, 10, 15, 20, 25]. The key theorem regarding the hardness of nding optimal factoring as well as the hardness of inapproximability leads to the design of polynomial time search heuristics for generating factorings. These factorings substantially increase the scalability of formal verication via SAT solving. The paper is structured as follows. Section 2 denes basic notions and con cepts underlying BNNs. Section 3 presents our verication work ow including the factoring of counting units (Section 3.2). We summarize experimental results with our verication procedure in Section 4, compare our results with related work from the literature in Section 5, and we close with some nal remarks and an outlook in Section 6. Proofs of theorems are listed in the appendix. 2 Preliminaries LetBbe the set of bipolar binaries1, where +1 is interpreted as trueand "
315,Boosting the Robustness Verification of DNN by Identifying the Achilles's Heel.txt,"Deep Neural Network (DNN) is a widely used deep learning technique. How to
ensure the safety of DNN-based system is a critical problem for the research
and application of DNN. Robustness is an important safety property of DNN.
However, existing work of verifying DNN's robustness is time-consuming and hard
to scale to large-scale DNNs. In this paper, we propose a boosting method for
DNN robustness verification, aiming to find counter-examples earlier. Our
observation is DNN's different inputs have different possibilities of existing
counter-examples around them, and the input with a small difference between the
largest output value and the second largest output value tends to be the
achilles's heel of the DNN. We have implemented our method and applied it on
Reluplex, a state-of-the-art DNN verification tool, and four DNN attacking
methods. The results of the extensive experiments on two benchmarks indicate
the effectiveness of our boosting method.","Nowadays, the research and application of deep learning (DL) [9] have achieved tremendous progresses. Deep learning has been widely used in many areas, including speech recog nition [1] [24], autonomous driving [18], image classiﬁcation [20], etc. Deep learning techniques, e.g., Deep Neural Network (DNN) [30], play a crucial role in the products or systems in these areas. When applied in critical areas, such as autonomous cars [18] and airborne collision avoidance systems [15], DL based systems need to assure highquality safety. However, there already exist the cases in which DLbased systems cause disasters, such as the one caused by Tesla car in 2016 [37]. Besides, there exist studies [29] [39] that use adversarial examples [35] to attack DLbased systems. How to ensure the safety of DLbased systems is challenging. DNN is a representative DL classiﬁcation technique. The existing work of safety assurance for DNNbased systems mainly focuses on the robustness of DNN. Existing methods have two categories: 1) attacking methods [10] [21] [28] [25] [4] that generate the adversarial examples [35] of a DNN and retrain the DNN to improve the robustness; 2) defense methods that include verifying the robustness of a DNN [17], detecting the attacking adversarial inputs online [40], etc. To verify the robustness of a DNN, the existing work usually models the DNN, such as symbolic encoding [12] and abstraction [7], and tries to verify the model via symbolic solving or invariant checking. If the veriﬁcation succeeds, theDNN is proved to be robust; otherwise, counterexamples (or adversarial examples) are produced. The existing veriﬁcation methods differ in the aspects including scalability, complete ness, etc. However, veriﬁcation’s cost is usually high, and the veriﬁcation methods are difﬁcult to scale to large DNNs. How to improve the scalability is a key problem for verifying DNNs. Existing studies [10] [21] [28] [25] [4] of adversarial examples [35] indicate that most realworld DNNs tend to have adversarial examples, i.e., they are not robust. Hence, the veriﬁcation methods usually produce counterexamples. Then, boosting counterexample ﬁnding during the process of veriﬁcation directly improves the scalability. Most existing DNN veriﬁcation methods support only the veriﬁcation of local robustness ,i.e., given an initial point pof the input domain, prove that the neighbouring points within a limited rangeare classiﬁed into the same type as p. Hence, the selections of panddirectly inﬂuence the result and the efﬁciency of veriﬁcation. We observe that veriﬁcation tools tend to quickly ﬁnd counterexamples at some initial points. Therefore, if we can select the right points, we can boost ﬁnding counterexamples. In this paper, we ﬁrst prove that the outputs of a DNN using ReLU are continuous w.r.t. the inputs. Then, based on the continuity result, we propose a method for evaluating the possibility of ﬁnding counterexamples around an input. The key idea is an input whose largest and second largest outputs are close is likely to have counterexamples around. As far as we know, it is the ﬁrst evaluation method considering this aspect. Besides, we propose a lightweight preanalysis to boost ﬁnding counterexamples further. We have implemented our boosting method and applied it on Reluplex [17], i.e., a state oftheart robustness veriﬁcation tool for DNN, and representa tive DNN attacking methods. The experimental results on two benchmarks, i.e., ACASXu [15] and MNIST [38], indicate the effectiveness of our method. The main contributions of this paper are as follows: Based the continuity property of DNN, we propose an evaluation method for selecting the inputs around which counterexamples tend to exist. We propose a preanalysis greedy algorithm to speed up counterexample ﬁnding further. We have implemented our boosting method and applied it on Reluplex and four adversarial example generation methods. The extensive experiments on two representaarXiv:1811.07108v1  [cs.LG]  17 Nov 2018tive DNN benchmarks indicate: compared with random method, our boosting method can achieve at least an order of magnitude time speedup in ﬁnding the same amount of counterexamples; under the same time budget, our method can ﬁnd an order of magnitude more counter examples; besides, our method can averagely improve the success rate of the methods for generating adversarial examples by 3.2 times. The remaining of this paper is organized as follows. Section 2 brieﬂy introduces the backgrounds and motivations of our method. Section 3 proves the continuity of the DNNs using ReLU. Section 4 presents our boosting method. Section 5 gives experimental results. Section 6 reviews the related work, and the conclusion is drawn in Section 7. II. P RELIMINARY AND MOTIVATION In this section, we will brieﬂy introduce the basic concepts of DNN, its robustness, and the DNN veriﬁcation tool Relu plex. Then, our boosting method will be motivated. A. DNN Generally, a DNN is comprised of multiple layers, wherein each layer consists of nodes called neurons . The ﬁrst and the last layer are input layer and output layer, respectively. The remaining layers are hidden layers. Here, we focus on the feed forward multilayer neural networks [30]. For each neuron of every layer, it is connected to each neuron of the next layer in a forward direction with a weight . Each neuron in the hidden layers has an incoming value and an outgoing value, which are linked by an activation function , such as ReLU [26], tanh [2] and sigmoid [13]. In this paper, we only consider the DNN using ReLU activation function, i.e.,max (0;x). If a DNN has nhidden layers and the ith layer has mneurons, we use ni;j to denote the jth neuron of the ith layer, where i2f0;:::;n + 1gandj2f0;:::;m"
286,Verifying Safety of Neural Networks from Topological Perspectives.txt,"Neural networks (NNs) are increasingly applied in safety-critical systems
such as autonomous vehicles. However, they are fragile and are often
ill-behaved. Consequently, their behaviors should undergo rigorous guarantees
before deployment in practice. In this paper, we propose a set-boundary
reachability method to investigate the safety verification problem of NNs from
a topological perspective. Given an NN with an input set and a safe set, the
safety verification problem is to determine whether all outputs of the NN
resulting from the input set fall within the safe set. In our method, the
homeomorphism property and the open map property of NNs are mainly exploited,
which establish rigorous guarantees between the boundaries of the input set and
the boundaries of the output set. The exploitation of these two properties
facilitates reachability computations via extracting subsets of the input set
rather than the entire input set, thus controlling the wrapping effect in
reachability analysis and facilitating the reduction of computation burdens for
safety verification. The homeomorphism property exists in some widely used NNs
such as invertible residual networks (i-ResNets) and Neural ordinary
differential equations (Neural ODEs), and the open map is a less strict
property and easier to satisfy compared with the homeomorphism property. For
NNs establishing either of these properties, our set-boundary reachability
method only needs to perform reachability analysis on the boundary of the input
set. Moreover, for NNs that do not feature these properties with respect to the
input set, we explore subsets of the input set for establishing the local
homeomorphism property and then abandon these subsets for reachability
computations. Finally, some examples demonstrate the performance of the
proposed method.","Machine learning has witnessed rapid growth due to the high amount of data produced in many industries and the increase in computation power. NNs have emerged as a leading candidate computation model for machine learning, which promotes the prosperity of artificial intelligence in various fields, such as computer vision [ 8,45], natural language processing [ 26,56] and so on. Recently, NNs are increasingly applied in safetycritical systems. Consequently, to gain users’ trust and ease their concerns, it is of vital importance to ensure that NNs are able to produce safe outputs and satisfy the essential safety requirements before the deployment. Safety verification of NNs, which determines whether all outputs of an NN satisfy specified safety requirements via computing output reachable sets, has attracted a huge attention from different communities such as machine learning [1,33], formal methods [ 21,32,46], and security [ 12,48]. Because NNs are generally large, nonlinear, and nonconvex, exact computation of output reachable sets is challenging. Although there are some methods on exact reachability analysis such as SMTbased [ 27] and polyhedronbased approaches [ 47,50], they are usually timeconsuming and do not scale well. Moreover, these methods are limited to NNs with ReLU activation functions. Consequently, overapproximate reachability analysis, which mainly involves the computation of super sets of output reachable sets, is often resorted to in practice. The overapproximate analysis is usually more efficient and can be applied to more general NNs beyond ReLU ones. Due to these advantages, an increasing attention has been attracted and thus a large amount of computational techniques have been developed for overapproximate reachability analysis [31]. Overly conservative overapproximations, however, often render many safety properties unverifiable in practice. This conservatism mainly results from the wrapping effect, which is the accumulation of overapproximation errors through layerbylayer propagation. As the extent of the wrapping effect correlates strongly with the size of the input set [ 51], techniques that partition the input set and independently compute output reachable sets of the resulting subsets are often adopted to reduce the wrapping effect, especially for the cases of large input sets. Such partitioning may, however, produce a great number of subsets, which is generally exponential in the dimensionality. This will induce extensive demand on computation time and memory, often rendering existing reachability analysis techniques not suitable for safety verification of complex NNs in real applications. Therefore, exploring subsets of the input set rather than the entire input set could help reduce computation burdens and thus accelerate the safety verification tremendously. In this work, we investigate the safety verification problem of NNs from the topological perspective, mainly focusing on the homeomorphism and open map properties. For one thing, we extend the setboundary reachability method, which is originally proposed for verifying safety properties of systems modeled by ODEs in [ 52], to safety verification of NNs. In [52], the setboundary reachability method only performs overapproximate reachability analysis on the initial set’s boundary rather than the entire initial set to address safety verification problems. It was built upon the homeomorphism property of ODEs. This nice property also widely exists in NNs, and representative NNs are invertible NNs such as neural ODEs [ 5] and invertible residual networks [ 4]. Consequently, it is straightforward to extend the setboundary reachability method to safety verification of these NNs, just using the boundary of the input set for reachability analysis which does not involve reachability computations of interior points and thus reducing computation burdens in safety verification. Furthermore, we extend the setboundary reachability method to general NNs (feedforward NNs) via exploiting the local homeomorphism property with respect to the input set. This exploitation is instrumental for Manuscript submitted to ACMVerifying Safety of Neural Networks from Topological Perspectives 3 constructing a subset of the input set for reachability computations, which is gained via removing a set of points in the input set such that the NN is a homeomorphism with respect to them. The above methods of extracting subsets for performing reachability computations can also be applied to intermediate layers of NNs rather than just between the input and output layers. For another thing, since the homeomorphism property has strong constraints on the NN structures, that is, the dimensions of the input and output layers must be the same, which greatly limits the types and scale of applied NNs. Therefore, we consider the more general trapezoidal NN structures, which are characterized by nonstrictly monotonically decreasing layer dimensions, and the open mapping property widely exists in these NNs. Relaxing the homeomorphism property to an open mapping does not guarantee that the input set’s boundary is mapped to the output set’s boundary, however, it ensures that the boundary of the output set must come from the boundary of the input set (there is a redundancy situation where the input set’s boundary is mapped to the interior points of the output set). Subsequently, similar to the homeomorphism property, the safety verification can be carried out with the set boundaries. Finally, we demonstrate the performance of the proposed method on several examples. The main contributions of this paper are listed as follows. •We investigate the safety verification problem of NNs from the topological perspective. More concretely, we exploit the homeomorphism and the open map properties, and aim at extracting a subset of the input set rather than the entire input set for reachability computations. To the best of our knowledge, this is the first work on the utilization of the topological property to address the safety verification problems of NNs. This might on its own open research directions on digging into insightful topological properties of facilitating reachability computations for NNs. •The proposed method is able to enhance the capabilities and performances of existing reachability computation methods for the safety verification of NNs via reducing computation burdens. Based on the homeomorphism and the open map properties, the computation burdens of solving the safety verification problems can be reduced for the NNs featuring these topological properties. We further show that the computation burdens can also be reduced for more general NNs by exploiting the local homeomorphism property established on the subsets of the input set. The remainder of this paper is structured as follows. First, an overview of the closely relevant research is introduced in Section 2. Afterward, we formulate the safety verification problem of interest on NNs in Section 3 and then elucidate our setboundary reachability method for addressing the safety verification problem with either the homeomorphism property or the open map property in Section 4. Following this, we demonstrate the performance of our setboundary reachability method and compare it with existing methods on several examples in Section 5. Finally, we summarize the paper and discuss potential future work in Section 6. 2 RELATED WORK "
377,Towards Repairing Neural Networks Correctly.txt,"Neural networks are increasingly applied to support decision making in
safety-critical applications (like autonomous cars, unmanned aerial vehicles
and face recognition based authentication). While many impressive static
verification techniques have been proposed to tackle the correctness problem of
neural networks, it is possible that static verification may never be
sufficiently scalable to handle real-world neural networks. In this work, we
propose a runtime verification method to ensure the correctness of neural
networks. Given a neural network and a desirable safety property, we adopt
state-of-the-art static verification techniques to identify strategically
locations to introduce additional gates which ""correct"" neural network
behaviors at runtime. Experiment results show that our approach effectively
generates neural networks which are guaranteed to satisfy the properties,
whilst being consistent with the original neural network most of the time.","Deep neural networks (DNNs) are widely applied to a variety of applications thanks to their exceptional performance, such as fa cial recognition [ 22], sentiment analysis [ 26], and malware detec tion [ 31]. In addition, they are increasingly applied in safetycritical systems such as medical diagnosis [ 29], selfdriving cars [ 1] and aircraft collision avoidance for unmanned aircraft (ACAS Xu) [ 17], which highlights the growing importance of DNNs’ safety and reli ability. DNNs are, however, known to be brittle to attacks such as adversarial perturbations [ 5]. That is, a slight perturbation on an input can cause a DNN to make a decision in an unexpected and incorrect way. Worse yet, due to the blackbox nature of DNNs, it is extremely hard to ‘debug’ and repair such erroneous behaviors. Existing efforts on ensuring that DNNs behave correctly roughly fall into two categories. One is on static verification of DNNs, includ ing efforts such as Reluplex [ 8], MIPVerify [ 27] and DeepPoly [ 24]. This group of works focus on statically verifying a specific property of interest, e.g., a reachability condition or local robustness. The aim is to provide a formal guarantee on the correctness if the prop erty is satisfied. This group of works suffer from two limitations. First, these works are not yet scalable enough to handle certain realworld DNNs (which may contain thousands or even millions of neurons). Although more and more sophisticated verification algorithms have been developed [ 23,30], it is entirely possible that they may never be scalable enough. Second, these approaches donot answer the natural question: what the subsequent measure that one should take if the DNN is not verified? While we can typically construct a counterexample and then fix the ‘bug’ in the setting of program verification, how to improve a DNN with the verifica tion result is far more complicated. Note that simply discarding the problematic DNN and training a new one from scratch is not always feasible due to the high time and computing costs. Worst yet, there is no guarantee that the newly trained neural network is safe either. The second group of works focus on improving DNNs by repair ing them [ 16,25]. One idea is to retrain a target model with adver sarial samples which can be generated using attacking tools [ 14,16]. Although the retrained models typically have improved robustness against adversarial perturbations, they do not guarantee that the retrained model is correct. Indeed, it has been shown that such retrained models are often subject to further adaptive adversarial attacks [28]. Another line of work on repairing DNNs [3, 25] is to, given a DNN, modify the weights of neurons which contribute to certain specific behaviours. For instance, Arachne, proposed in [ 25], modifies the weights of certain neurons guided by a fitness func tion to prune misbehaviours caused by certain inputs. However, Arachne similarly does not guarantee the correctness of the re paired model. Goldberger et al. proposed in [ 3] a verificationbased approach to modify neural weights of the output layer such that the DNN satisfies a given property. Different from the previous repairing approaches, this approach provides a formal guarantee on the correctness of the repaired model based on verification tech niques. However, their repair is limited to the output layer, which is shown to have limited capability in handling DNNs trained for realistic safetycritical systems such as ACAS Xu. Further, their ap proach is limited to correcting the DNN’s behavior on one concrete input, which has limited usefulness as there could be many such inputs. Repairing the DNN so that it always behaves correctly with respect to a desirable property is both more useful and technically challenging. In this work, we propose a verificationbased approach for re pairing DNNs. The goal of our work is to repair a DNN such that it is guaranteed to satisfy userprovided properties. Different from [ 3], our approach is not restricted to modifying the output layer (which is often too late). Rather, we propose to identify and modify the most relevant neurons for violating the desired property through the guidance of the verification results. We further incorporate abstraction refinement techniques to minimize the modification so that the modification is only relevant in limited regions of input space where correctness cannot be verified.arXiv:2012.01872v2  [cs.LG]  6 May 2021Guoliang Dong, Jun Sun, Jingyi Wang, XingenWang, Ting Dai, and Xinyu Wang 𝒑Verifier Verified？NoYes Counterexampleg1g2gk...𝑁GradsonneuronsNeuronselection Modify𝒊thneuronofNOutput𝑁VerificationRepair The𝒊thneuronˆN<latexit sha1_base64=""kAYvGQOCbsZmySXPkxMoxZ0cd+I="">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF09SwX5AG8pmu2mXbjZhdyKU0B/hxYMiXv093vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gfDvz209cGxGrR5wk3I/oUIlQMIpWavdGFLP7ab9ccavuHGSVeDmpQI5Gv/zVG8QsjbhCJqkxXc9N0M+oRsEkn5Z6qeEJZWM65F1LFY248bP5uVNyZpUBCWNtSyGZq78nMhoZM4kC2xlRHJllbyb+53VTDK/9TKgkRa7YYlGYSoIxmf1OBkJzhnJiCWVa2FsJG1FNGdqESjYEb/nlVdK6qHq16uVDrVK/yeMowgmcwjl4cAV1uIMGNIHBGJ7hFd6cxHlx3p2PRWvByWeO4Q+czx9zAo+n</latexit> Modifiedneuralnetwork Stoprepairing?YesOutput𝑁𝑜𝑛𝑒 No Figure 1: Overall framework Figure 1 presents the overall workflow of our approach. There are two main parts, i.e., network verification (on the left) and net work repair (on the right). Given a network 𝑁and a userprovided property𝜙, we first check whether network 𝑁satisfies𝜙or not with a static verification engine. If the property is violated and the termination condition, e.g., timeout, has not been met, we identify a counterexample. Afterwards, we first compute the gradient of each neuron with respect to the violation loss of the counterexample, and then select the neuron which is most ‘responsible’ for the violation according to the magnitude of the gradients. Intuitively, the magni tude of the gradients measures the contribution of the neurons to DNN prediction result [ 19]. The gradients of the network’s output with respect to the neurons thus can be regarded as a measure on the neurons’ contribution on violating the property. We then tune the weights of the selected neuron and obtain a modified model ˆ𝑁. Iteratively, we take ˆ𝑁as the new input model and repeat the above process until the network is verified or the termination condition is met. We have implemented our approach as a selfcontained proto type based on the DeepPoly verification engine and evaluate it on 41 models for two kinds of tasks, i.e., aircraft collision avoidance (ACAS Xu) and image classification (MNIST and CIFAR10). For ACAS Xu, we apply our approach to the models which violate at least one of the properties. For the image classification, we focus on the robustness property, i.e., we apply our approach to repair the models so that they are robust against perturbation which are lim ited to certain region of the input images. In total, we have a set of 587 repair tasks. Overall, our approach achieves 98.46%, 94.77% and 96.67% success rate on models of ACAS Xu, MNIST and CIFAR10 respectively. On average, the time overhead to repair a target model is 6.5, 2.7 and 2.07 minutes for models of ACAS Xu, MNIST, and CIFAR10 respectively. To show that the repaired DNNs are faithful to the original DNN, we measure the fidelity of each repaired DNN, and the results show that the repaired models have a high fidelity of 97.51% with respect to the original DNNs. In summary, we make the following technical contributions. •We propose an effective and efficient verificationbased frame work for repairing DNNs. Different from existing approaches, the repaired DNN is guaranteed to satisfy the property.•We propose to repair DNNs based on adjusting weights of neurons which are most responsible for violating the prop erty, through an optimization algorithm. •We implement a selfcontained toolkit for repairing DNNs and evaluate our approach on two tasks over three datasets. The results show our approach can effectively repair DNNs. The remainder of the paper is organized as follows. We review relevant background in Section 2, and present our approach in detail in Section 3. We evaluate our approach and discuss the ex periment results in Section 4. We review related work in Section 5 and conclude in Section 6. 2 BACKGROUND In this section, we briefly review relevant background. Deep neural networks In this work, we focus on feedforward neural networks (FNNs) for various classification tasks. We remark that in theory our approach can be extended to support other kinds of DNNs as long as static verification techniques for those networks are available. A FNN consists of an input layer, multiple hidden layers, and an output layer. Let 𝑁be an FNN. We denote 𝑁= 𝑓0◦𝑓1◦···◦𝑓𝑙where𝑓0is the input layer, 𝑓𝑙is the output layer, and 𝑓𝑖(𝑖∈[1,𝑙−1]) in between is the hidden layer. N can be regarded as a function 𝑁:𝑋→𝑌mapping an input 𝑥∈𝑋to a label𝑐∈𝑌. Given an input 𝑥∈𝑋, for each layer 𝑓, it computes the output as follows. 𝑓0=𝑥 (1) 𝑓𝑖=𝜎(𝑊𝑖𝑓𝑖−1+𝐵𝑖) (2) where𝑊𝑖and𝐵𝑖are the weights and biases of the neurons in 𝑖th layer respectively, and 𝜎is the activation function such as max out [ 4], rectified linear unit (ReLU) [ 18] and hyperbolic tangent (tanh). In this work, we focus on ReLU which coverts any negative input to zero and keep the positive input unchanged. Note that here we use𝑓to denote the layer of neurons or the output of layer 𝑓 depending on the context. The predicted label 𝑐is obtained from the output vector of the final layer 𝑓𝑙:𝑐=argmax𝑖(𝑓𝑖 𝑙)where𝑓𝑖 𝑙 denotes the 𝑖th value in output 𝑓𝑙.Towards Repairing Neural Networks Correctly Table 1: An example of the reachability property Input 𝑥=[𝑥1,𝑥2,𝑥3,𝑥4,𝑥5] Output 𝑦=[𝑦1,𝑦2,𝑦3,𝑦4,𝑦5] Input proposition 𝑥1≥55947.691,𝑥4≥1145 and𝑥5≤60 Output proposition 𝑦1is not the maximal score Properties and verification of neural networks The problem of veri fying neural networks is to provide formal guarantees about if a given network satisfies a certain property of interest. There are a variety of properties addressing different concerns. In this work, we focus on a class of reachability properties which are defined based on an input proposition 𝜙and an output proposition 𝜔. Table 1 exemplifies a property for ACAS Xu models [17], which describes that if the intruder aircraft is distant and is significantly slower than the ownship, the score of a “ClearofConflict” advisory should not be the maximal score. Formally, the problem is to check if the following assertion holds. ∀𝑥⊨𝜙.𝑁(𝑥)⊨𝜔 (3) Intuitively, the property states that if an input satisfies certain con straint𝜙, the neural network output must satisfy 𝜔. A verification algorithm may produce three results. One is that the property is verified. One is that the property is violated and a counterexample is generated (i.e., one input 𝑥such that𝑥⊨𝜙∧𝑁(𝑥)¬⊨𝜔). The last one is that the algorithm fails to verify or falsify the property (e.g., timeout or outputs an ‘unknown’ result), due to the limitation of existing verification techniques. Among many existing verification toolkits for neural networks, we focus on DeepPoly in this work. DeepPoly is a stateofthe art neural network verifier [ 24]. Verification with DeepPoly com prises two steps. Firstly, it adopts abstract interpretation to over approximate the reachable set at each layer, starting with 𝜙at the input layer. Through layerbylayer propagation, DeepPoly obtains an abstract representation of the reachable set for each class. Then, based on the approximation of the output of each label, DeepPoly checks whether 𝜔is satisfied. Let 𝜙=(𝑙,𝑢)be the input constraint where𝑙and𝑢is the lower bound and upper bound of the input respectively, we then formalize the first step of DeepPoly as follows. A(𝑁,𝜙)={(𝑎≤ 1,𝑎≥ 1),...,(𝑎≤ 𝑘,𝑎≥ 𝑘)} (4) where𝑎≤ 𝑖and𝑎≥ 𝑖are the lower bound and upper bound of the abstract domain of label 𝑖. During the second step, DeepPoly checks whether{(𝑎≤ 1,𝑎≥ 1),...,(𝑎≤ 𝑘,𝑎≥ 𝑘)}satisfies𝜔. For example, if 𝜔re quires that the score assigned to label 1 is always the largest among all the labels for any input 𝑥∈𝜙, DeepPoly checks whether the lower bound of label 1 is greater than the upper bound of any of the other labels. If it is the case, the property is verified; otherwise, DeepPoly reports that the verification fails. 3 OUR APPROACH In this section, we present our approach in detail. The goal of our work is to repair a neural network such that the userprovided prop erty is guaranteed to be satisfied. Our approach can be categorized as verificationbased repair. The overall idea is to identify a mini mal set of input regions in which the property is not verified andAlgorithm 1: 𝑟𝑒𝑝𝑎𝑖𝑟 _𝑝𝑟𝑒𝑑𝑖𝑐𝑡(𝑥,𝑁,𝑅) 1for𝑖=1to𝑠𝑖𝑧𝑒(𝑅)do 2 letˆ𝑁be the𝑖th repaired models in 𝑅; 3 letˆ𝜙be the input region corresponding to ˆ𝑁; 4 if𝑥∈ˆ𝜙then 5 return ˆ𝑁(𝑥); 6return𝑁(𝑥) then tune the weights of the neurons which are ‘most’ responsible for violating the property in the regions so that the property is satisfied. In the following, we first define the repair problem, and then present the details of our solution. 3.1 Problem definition Now, we formally describe our repair problem as follows. Definition 1. Let𝑁denote a neural network; let 𝜙denote the input constraint and let 𝜔denote the output constraint. Assuming that𝜙consists of a set of disjoint partitions: 𝜙={𝜙1,𝜙2,...,𝜙 𝑛} (i.e., partitions of the input space), and 𝑁violates𝜔in some of the partitions. Our repair problem is to find a neural network ˆ𝑁for each erroneous partition 𝜙′such that∀𝑥⊨𝜙′,ˆ𝑁(𝑥)⊨𝜔. According to definition 1, our approach returns a set of repaired models and each of them is responsible for a certain input subre gion𝜙′in which the property is violated. That is, when we use the returned models to predict, we first locate the subregion where the input sample belongs to, and then use the corresponding model to perform the prediction. Algorithm 1 shows at a highlevel how to use the repaired models, where 𝑥is the input sample, 𝑁is the original model and 𝑅is the set of repaired models of 𝑁. As afore mentioned in Section 2, we focus on the reachability properties. That is, given an input from a certain input region, any output returned by a repaired model is acceptable as long as it satisfies the output constraint, even the output is different from the one returned by the original model. Note that compared to having a repaired model for all inputs, this way of repairing the neural network allows more flexibility. First, a well trained and tested neural network often behaves cor rectly in many of the regions and thus we should avoid to repair the neural network in those regions. Second, repairs in different regions could be different, which potentially allows us to utilize characteristics which are specific to certain region to assist the repair. Furthermore, we remark that our repair is not the DNN min imal modification problem proposed in [ 3]. More specifically, we do not put any “quality"" constraint, e.g, a minimal distance between the repaired and the buggy network, during repairing because our repair is propertyoriented. That is, we assume that the property is critical and must be satisfied (e.g., collision avoidance for unmanned aircraft); and if a repaired model can satisfy a given property, we say the repair is successful regardless of how many modifications are made. One may concern that without such constraints on the repaired network, our approach could return a new network that is repaired but behaves very differently from the original network. We will address this concern later.Guoliang Dong, Jun Sun, Jingyi Wang, XingenWang, Ting Dai, and Xinyu Wang Algorithm 2: 𝑜𝑣𝑒𝑟𝑎𝑙𝑙(𝑁,𝜙,𝜔,𝛼,𝛽,𝜂) 1verify𝑁with a verifier; 2if𝑁is verified then 3 return𝑁; 4if𝜙can be further partitioned then 5 partition𝜙into𝜙1and𝜙2; 6 letˆ𝑁1be𝑜𝑣𝑒𝑟𝑎𝑙𝑙(𝑁,𝜙 1,𝜔,𝛼,𝛽,𝜂); 7 letˆ𝑁2be𝑜𝑣𝑒𝑟𝑎𝑙𝑙(𝑁,𝜙 2,𝜔,𝛼,𝛽,𝜂); 8 return an assemble of ˆ𝑁1and ˆ𝑁2; 9else 10 ifa counterexample 𝑐𝑡is generated then 11 return𝑟𝑒𝑝𝑎𝑖𝑟(𝑁,𝜙,𝜔,𝑐𝑡,𝛼,𝛽,𝜂); 3.2 From Verification to Optimization Our overall algorithm is shown in Algorithm 2 which takes as inputs the original model 𝑁, the input constraint 𝜙, the output constraint𝜔, a bound on the number of neurons to modify 𝛼, the maximum number of times a single neuron is allowed to be mod ified𝛽and the step size 𝜂. First, we employ a verifier to verify 𝑁 against the property. If the property is satisfied, we return 𝑁with out any modifications. Otherwise, we check if the input domain 𝜙 can be further partitioned. While in theory, it is always possible to partition the input domain, in practice, existing verifiers often have restrictions on the form of input constraints (e.g., a range constraint subject to certain further restriction in DeepPoly [ 24]) and thus it is not always possible to partition. If affirmative, we partition𝜙into two nonoverlapping constraints 𝜙1and𝜙2and repair𝑁for each partition separately. At line 8, we assemble a neural network based on the repaired results ˆ𝑁1and ˆ𝑁2, i.e., by adding a gate which directs the input to ˆ𝑁1if𝜙1is satisfied or ˆ𝑁2 otherwise. The partition strategy differs according to the verifier engine. One simple and general (and importantly efficient) strategy is bisection (i.e., by bisecting a range into two equalsized ranges). If𝜙cannot be further partitioned, a counterexample 𝑐𝑡is generated and we invoke Algorithm 3 to repair 𝑁. Given a particular input region (i.e., 𝜙) in which the property is violated, Algorithm 3 aims to repair the neural network by tuning the weights of the relevant neurons. The question is then: which neurons do we tune (so that the neuron network is repaired by tuning a minimal set of neurons) and how do we tune the neurons? Our answer to the question is to solve an optimization problem. That is, given the property and the counterexample 𝑐𝑡, we define a loss function and then minimize the violation loss of 𝑐𝑡by modifying the outputs of neurons. Formally, min ˆ𝑁𝑙𝑜𝑠𝑠(𝑐𝑡,ˆ𝑁) (5) where ˆ𝑁is a repaired model satisfying the property. To reduce the search space, we restrict ˆ𝑁such that ˆ𝑁and𝑁have the same structure and weights except that some neurons ˆ𝑁have a constant activation value. Note that the above loss function is defined based on one single counterexample. That is, one single counterexample is adequate to guide the repair. The intuition is that after rounds of input partitioning, we typically end up repairing a small region eachtime. Within the small region, counterexamples closely resemble each other, and as a result, once a counterexample is repaired, others are often repaired as well. This is evidenced empirically in our experiments. The key to design the loss function 𝑙𝑜𝑠𝑠(𝑐𝑡,𝑁)is that the loss of an input𝑐𝑡should measure how far 𝑐𝑡is from being satisfied. The general idea is to adapt existing established loss functions such as crossentropy loss for classification tasks and mean squared errors for regression tasks. In this work, we focus on classification tasks and thus a general form of loss functions is as follows. 𝑙𝑜𝑠𝑠(𝑐𝑡,ˆ𝑁)=∑︁ 𝑖∈𝑆𝑠𝑖𝑔𝑛(𝑖)·𝑒𝑥𝑝(𝑦𝑖)Í 𝑖∈𝐿𝑒𝑥𝑝(𝑦𝑖)(6) where𝑆⊆𝐿is the set of desired labels (usually a singleton set) specified by the output constraint (i.e., 𝜔) and𝐿is the set of all labels in classification; 𝑦𝑖where𝑖∈𝐿is the score of label 𝑖given 𝑐𝑡and ˆ𝑁; and𝑠𝑖𝑔𝑛(𝑖)for a label𝑖∈𝑆is defined as follows. 𝑠𝑖𝑔𝑛(𝑖)=( 1,if the score of label 𝑖should be smaller −1,otherwise(7) Intuitively,𝑠𝑖𝑔𝑛(𝑖)is 1 for the undesired label and 1 for the other labels. The idea is thus to use the loss value as a guideline to search for a repaired neural network ˆ𝑁such that ˆ𝑁produces the desired output, i.e., ˆ𝑁(𝑐𝑡)⊨𝜔. For example, in terms of the property shown in Table 1, the output constraint requires that the score of the first label is not the maximum. Let 𝐿={𝑎,𝑏,𝑐,𝑑,𝑒}be the corresponding labels of the output, i.e.,𝑎is the first label corresponding to the first dimension of the output. Then we have 𝑆={𝑎}and𝑠𝑖𝑔𝑛(𝑎)=1according to Formula 7. Consequently, the loss function is defined as follows. 𝑙𝑜𝑠𝑠=𝑒𝑥𝑝(𝑦𝑎)Í 𝑖∈{𝑎,𝑏,𝑐,𝑑,𝑒}𝑒𝑥𝑝(𝑦𝑖)(8) 3.3 Solving the Optimization Problem We adopt a greedy strategy to solve the the optimization problem defined by 5. Algorithm 3 shows the details on how the neural network is repaired. First, we initialize an empty dictionary Υat line 1 to record which neurons have been modified and the number of times they have been modified. From line 2 to line 14, we iteratively modify the neurons in 𝑁. During each iteration, we first compute the gradient of each neuron with respect to 𝑐𝑡and the value of the loss function as described in 6. Afterwards, we select the neuron which is most ‘responsible’ for the loss by invoking Algorithm 4 at line 4. When a neuron is identified, we obtain its gradient and output with respect to 𝑐𝑡at line 7. Afterwards, we modify the neuron by tuning its output according to the gradient at line 8. That is, like in the case of stochastic gradient descent [ 21], we alter the neuron’s output towards the opposite direction of its gradient to decrease the violation loss. Note that we tune the neuron’s output rather than its weights to repair the neural network for the sake of efficiency. Next, we check if the modified model is repaired at line 9, i.e., whether the modified N satisfies the property within the input region, using the static verifier. Note that line 9 first checks whether the counterexample has been eliminated, which is logically speaking redundant. In practice, it serves as an efficient sanity checkTowards Repairing Neural Networks Correctly Algorithm 3: 𝑟𝑒𝑝𝑎𝑖𝑟(𝑁,𝜙,𝜔,𝑐𝑡,𝛼,𝛽,𝜂) 1letΥbe a dictionary recording the number of times each neuron has been modified, which is initialised as empty; 2while𝑠𝑖𝑧𝑒(Υ)<𝛼or𝑡𝑖𝑚𝑒𝑜𝑢𝑡 do 3𝐺←compute the gradient of each neuron w.r.t. 𝑐𝑡and the loss function; 4 let𝑜be the neuron returned by 𝑠𝑒𝑙𝑒𝑐𝑡 _𝑛𝑒𝑢𝑟𝑜𝑛(𝐺,Υ,𝛽); 5 if𝑜is None then 6 return None; 7 let∇be the gradient of neuron 𝑜, and𝜁be the output of 𝑜w.r.t.𝑐𝑡; 8 modify𝑁by setting𝑜’s output as 𝜁−𝜂·∇; 9 if𝑁(𝑐𝑡)⊨𝜔and𝑁satisfies the property then 10 return𝑁; 11 if𝑜not in Υthen 12 Υ[𝑜]=1; 13 else 14 Υ[𝑜]=Υ[𝑜]+1; 15return None; Algorithm 4: 𝑠𝑒𝑙𝑒𝑐𝑡 _𝑛𝑒𝑢𝑟𝑜𝑛(𝐺,Υ,𝛽) 1Γ←sort the neurons according to the gradients 𝐺in a descending order; 2foreach neuron 𝑜inΓdo 3 ifΥ[𝑜]<𝛽then 4 return o; 5return None ; and helps to reduce the number of times the verifier is called. Once a fix is found, we return the fixed model at line 10. From line 11 to line 14, we record the number of modifications on the selected neuron𝑜. Concretely, we first check if 𝑜has ever been modified at line 11. If not, we record neuron 𝑜inΥand initialize its number of modifications to 1 at line 12. Otherwise, we increment its record by 1 at line 14. Note that when more than one repair is performed, i.e., the loop (line 2 to line 14) executed multiple times, subsequent repairs typically do not undo the earlier ones. There are two cases. In the first case, the repairs are for disjoint regions of inputs, and thus the repairs are by definition independent from each other. In the second case, if multiple repairs take place in the same input region, because we take one counterexample from the region and solve the optimization according to 5 during each repair, the loss typically reduces throughout for the optimization for each repair as well as cross different repairs. Algorithm 3 terminates when one of the three criteria is met. •We reach the threshold of neurons we are allowed to modify at line 2. •No qualified neuron is returned or timeout at line 5. •The model is repaired successfully at line 9.Algorithm 3 either returns a repaired model which is guaranteed to satisfy the property (at line 10) or returns None at line 6 or at line 15 when the repair fails. Algorithm 4 shows the details about how a neuron is selected. In Algorithm 4, we first sort all neurons of network 𝑁according to their gradients. Intuitively, the bigger the magnitude of a neuron’s gradient is, the more likely modifying the corresponding neuron would repair the model. Note that a neuron may be selected many times. To avoid the scenario that the optimization is stuck with a single neuron (because a single neuron can have limited influence on the loss), we limit the number of times that a neuron can be selected to no more than 𝛽times at line 3. We remark that the greedy strategy cannot guarantee that the number of modified neurons is always the minimal. Our optimiza tion method is derived from the standard gradient descent method, which potentially suffers from the problem of local optima. The overall complexity of our approach depends on the complexity of the verification algorithm and thus we evaluate it empirically in the next section. Overall, because DeepPoly relies on abstraction interpretation techniques, it is always rather efficient. 4 EVALUATION We have implemented our approach as a selfcontained toolkit called nRepair with about 3k source lines of code based on PyTorch, ONNX1and the stateoftheart analyzer ERAN2. The nRepair toolkit as well as all the data used in our experiments is available online3. In the following, we evaluate our approach to answer four research questions (RQs). 4.1 Experimental Setup Our experiment subjects include 42 feedforward networks for two kinds of two tasks, i.e., aircraft collision avoidance and image classification. The details are shown as follows. •ACAS Xu . ACAS Xu [ 17] is an aircraft collision avoidance system developed for unmanned aircraft. This system issues appropriate navigation actions to avoid collision with an intruder aircraft based on dynamic programming, which results in a large numeric lookup table. To compress the lookup table without loss of performance, neural networks are adopted to mimic the behaviors of the lookup table [ 7]. Recently, an array of 45 DNNs was developed to reduce the lookup time [ 8], which are our experiment subjects in this part of the experiment. Each of the fully connected networks has 6 hidden layers and each layer is equipped with 50 ReLU nodes. The inputs of these DNNs consists of five variables describing the speed and relative position of both the intruder and ownership, and the outputs are the scores of five advisories. These models are subject to a set of 10 safetycritical properties [ 8], which we adopt to test our approach. •MNIST . MNIST [ 13] is a dataset for image classification, which consists of 70k handwritten digits. Each digit is repre sented by a 28×28pixel greyscale image and the range of 1https://onnx.ai/ 2https://github.com/ethsri/eran 3omitted for anonymityGuoliang Dong, Jun Sun, Jingyi Wang, XingenWang, Ting Dai, and Xinyu Wang Table 2: Neural networks of image classification Dataset ModelArchitecture (layers×units)Training Accu.Test Accu. MNISTFNNSmall 3×100 0.9743 0.9668 FNNMed 5×100 0.9803 0.9662 FNNBig 7×100 0.9671 0.9386 CIFAR10FNNSmall 3×100 0.6474 0.5222 FNNMed 5×100 0.5689 0.4966 FNNBig 7×100 0.6327 0.4095 digits is from 0 to 9. We train three forward neural networks over this dataset (size of training/test set is 60000/10000) and then evaluate our approach on these models. We refer the three neural networks as FNNSmall (3 hidden layers), FNNMed (5 hidden layers) and FNNBig (7 hidden layers) respectively, and the hidden layer size is 100 for all of the three models. •CIFAR10 . CIFAR10 [ 12] is another widely used dataset. It contains 60000 32×32colour images in 10 categories. The dataset is well split, i.e., 50000 training images and 10000 test images. We train three different forward neural networks over this dataset, and the architectures of the tree networks are same with the architectures of MNIST models described above. Table 2 shows the details of the image classification models used in our experiments. For the ACAS Xu models, among the 45 models and 10 properties, 34 models fail property 2 and 1 model fails property 7 and 1 model fails property 8, which constitutes a total of 36 repairing tasks. For the MNIST and CIFAR10 models, we randomly select multiple images and verify whether they satisfy local robustness in the 𝐿∞norm [ 2], i.e., given an image 𝑥with label𝑐, the𝐿∞norm robustness is satisfied if the model outputs label 𝑐for any input in [𝑥−𝜏,𝑥+𝜏].𝜏used in our experiments is 0.03 for MNIST and 0.0012 for CIFAR10 (i.e., maximum values in DeepPoly). We manage to identify 100 images which violate the property for both datasets and all models except FNNBig for MNIST, for which a total of 51 images are identified. Note that for the verification of local robustness, each image together its label is regarded as an independent property, as a result, we total have 551 repair tasks. For each repair task, we set the timeout to 1 hour. In the following, we report all experiment results. All experiment results are obtained on a workstation with 1 i99900 processor and 64GB system memory. The threshold 𝛼is set to 5% of the number of neurons in the original model, i.e., for all ACAS Xu models, no more than 15 neurons can be modified and for both MNIST and CIFAR10, the number is 15, 25 and 35 for FNNSmall, FNNMed and FNNBig respectively. The threshold 𝛽is set to 50 for all experiments, i.e., a neuron can be modified at most 50 times. The values of these two parameters are identified empirically. 4.2 Research Questions In this section, we aim to answer the following research questions by multiple experiments.RQ1: Is nRepair effective at repairing neural networks? To answer this question, we apply nRepair to the 36 models of ACAS Xu, 3 models of MNIST and 3 models of CIFAR10. The step size 𝜂used in this research question is 0.35 for the ACAS Xu models, 0.05 for MNIST and 0.1 for CIFAR10 models. 𝜂is the key hyperparameter in our approach and we explain how to set the value of 𝜂later. The results on repairing the ACAS Xu models are summarized in Table 3, where the third column shows the number of partitions that need to be repaired. Note that the number of partitions ranges from 2 to 22. For each partition, we generate a counterexample (by collecting the constraints that a counterexample must satisfy, i.e., the partitioned input constraint as well as the negation of the property, and solving the constraint using mixed integer linear programming solver Gurobi [ 6]), and then solve the optimization problem as described in Section 3. We then report the percentage of partitions which are successfully repaired in the fourth column, and the number of neurons which are modified on average to repair each partition. We observe that nRepair successfully repairs 98.46% of the partitions, which means that for almost all of the regions, the neural network is made such that it is guaranteed to satisfy the property. We remark that for the few regions where we fail to repair, it is always possible to raise an alarm when an input in that region is received at runtime. Furthermore, the number of modified neurons are kept relatively small, i.e., 3.07 on average, or equivalently 1.02% on average. This suggests that only minor modifications are required to repair the neural networks. The results on repairing the MNIST and CIFAR10 models are summarized in Table 4. Note that due to the many repair tasks (i.e., 551 in total), we only show the average success rate and the number of modified neurones. Our approach achieves 95.72% success rate on the two datasets on average. Concretely, it can be observed that the success rate of most models is around 98%, and for FNNBig of MNIST the success rate drops but still remains at a relatively high level, i.e, 84.31%. This suggests that our approach has the good scalability, in terms of the success rate, and thus has the potential to be applied in practice. In addition, we observe that the number of modified neurons for those models which we successfully repair is larger than those for the ACAS Xu models, i.e., on average 7.04 and 7.13 neurons are modified. This is reasonable because for a simple model like ACAS Xu, a few neurons can dominate a certain result, whereas for a more complex model like MNIST and CIFAR10, the classification results are typically the joint results of multiple (if not many) neurons. We thus have the following answer to RQ1. Answer to RQ1: nRepair repairs neural network models with a high success rate and modifies a small number of neurons. RQ2: Does nRepair ’s repair undermine the overall performance of the model? The question asks whether the repaired model, while satisfying the property, carries the same level of performance , i.e., whether the repaired model has an accuracy close to the original model on those inputs whose prediction results given by the origi nal model satisfy the property. In other words, the repair should not undermine the performance of the model on those inputs which are not counterexamples to the property. For the ACAS Xu models,Towards Repairing Neural Networks Correctly Table 3: Results of repairing on ACAS Xu models Property Model #partitionsSuccess rate(%)Avg.m Avg.f(%) 𝑝2𝑁2,1 11 100 2.18 95.53 𝑁2,2 20 100 5 92.91 𝑁2,3 20 100 2.75 98.12 𝑁2,4 10 100 2.2 95.56 𝑁2,5 20 100 1.9 97.63 𝑁2,6 9 100 2.22 97.69 𝑁2,7 9 100 3.67 96.98 𝑁2,8 10 100 2.9 97.23 𝑁2,9 6 100 2.67 99.25 𝑁3,1 9 100 2.67 96.65 𝑁3,2 3 100 2 99.33 𝑁3,4 6 100 2.17 98.44 𝑁3,5 7 100 2.43 97.56 𝑁3,6 9 100 2.22 98.53 𝑁3,7 8 100 4 96.06 𝑁3,8 10 100 3.6 96.65 𝑁3,9 20 95 5.32 97.86 𝑁4,1 5 100 2.6 98.78 𝑁4,3 20 100 4.05 95.47 𝑁4,4 9 100 3.22 96.56 𝑁4,5 9 100 8.11 97.09 𝑁4,6 10 90 4.11 98.59 𝑁4,7 9 100 1.22 98.69 𝑁4,8 9 77.78 3.57 99.07 𝑁4,9 4 100 4.5 98.61 𝑁5,1 22 100 3.64 91.67 𝑁5,2 20 100 2.3 97.34 𝑁5,3 2 100 2 99.97 𝑁5,4 9 100 3.56 96.93 𝑁5,5 16 100 2.25 98.36 𝑁5,6 16 100 1.44 98.80 𝑁5,7 10 100 2 98.24 𝑁5,8 9 100 2.78 97.43 𝑁5,9 9 100 1.33 97.97 𝑝7 𝑁1,9 11 81.82 4.1 99.38 𝑝8 𝑁2,9 11 100 3.8 99.33 Avg 98.46 3.07 97.51 Table 4: Results of repairing on image classification Dataset Model#Repair casesSuccess rate(%)Avg.mAvg.acc Local(%)Avg.accR Global(%) MNISTFNNSmall 100 100 2.92 100 98.69 FNNMed 100 100 4.34 100 95.46 FNNBig 51 84.31 13.86 100 65.52 Avg 94.77 7.04 100 86.56 CIFAR10FNNSmall 100 95 4.77 100 84.98 FNNMed 100 98 5.71 100 75.39 FNNBig 100 97 10.92 100 60.42 Avg 96.67 7.13 100 73.6 because the training or test dataset are not available, we cannot measure the accuracy of the repaired model directly. We thus an swer this question by measuring the fidelity of the repaired model with respect to the original model, which is defined as follows. 𝐹𝑖𝑑𝑒𝑙𝑖𝑡𝑦 =Í 𝑥∈𝑇I(ˆ𝑁(𝑥)=𝑁(𝑥)) |𝑇|(9) where𝑇is a test set and|𝑇|is the number of samples in 𝑇; andI(𝑦) is an indicator function which equals 1 if 𝑦holds and 0 otherwise. That is, we synthesize (based on Gaussian sampling) a test set with 10000 samples for each repair case. Algorithm 5 shows the details about how the test set is synthesized. The algorithm takes three parameters: the original model 𝑁, the input constraint 𝜙and output constraint𝜔of a specified property. We first yield a sample 𝑥(lineAlgorithm 5: 𝑠𝑦𝑛𝑡ℎ𝑒𝑠𝑖𝑧𝑒 _𝑎𝑐𝑎𝑠𝑥𝑢(𝑁,𝜙,𝜔) 1Let(𝑙1,𝑢1),(𝑙2,𝑢2),...,(𝑙5,𝑢5)be the lower bound and upper bound of variable 𝑣1,𝑣2,...,𝑣 5defined by input constraint𝜙; 2Let(𝜇1,𝜎1),(𝜇2,𝜎2),...,(𝜇5,𝜎5)be the mean and standard deviation of variable 𝑣1,𝑣2,...,𝑣 5respectively; 3Let𝑋={}be the set of generated samples; 4while True do 5 Let𝑥be an empty array with length 5.; 6 for𝑖=1to5do 7𝑥[𝑖]←𝐺𝑎𝑢𝑠𝑠𝑖𝑎𝑛(𝑙𝑖,𝑢𝑖,𝜇𝑖,𝜎𝑖); 8 if𝑁(𝑥)⊨𝜔then 9𝑋←𝑋∪{𝑥}; 10 if|𝑋|==10000 then 11 return X; 67), and then check if the generated sample satisfies the output constraint𝜔(line 8). If yes, we then add it into the test set 𝑋. Note that we filter those samples which fail the given property since the prediction on these samples are supposed to be modified in order to satisfy the property. After that, we adopt Algorithm 1 to perform the prediction with the repaired models. The results are shown in the last column of Table 3. It can be observed that the fidelity remains across all models, i.e., with an average of 97.51%. For MNIST and CIFAR10, our goal of repairing is to make the original model satisfy the local robustness property. That is, we repair the given model around a set of selected images. To evaluate the the repair results on the two image classification tasks, we report the accuracy of the model with respects to a set of testing data containing images which are sampled within the norm, i.e., those which satisfy the input condition. Note that all samples within the norm should have the same label as the selected image (at the center of the norm). The results are shown in the second last column “Avg.acc Local"" in Table 4. We can observe that the result is always 100%. This is expected as the repaired model is guaranteed to label the images within the norm correctly. Out of curiosity, we conduct an additional experiment as follows. We sample a set of samples throughout the input space and test the accuracy of the repaired model. Note that this is not how the repaired model is meant to be used since the repair is meant to take effect only for those inputs which satisfy the input constraint of the property. Rather, the goal is to see whether a local repair applies globally. Concretely, we use the test set to measure the accuracy of those repaired models relative to that of their original model, which is defined as follows. 𝑎𝑐𝑐𝑅=accuracy of repaired model accuracy of original model(10) The results are shown in the last column “Avg.accR Global"" of Ta ble 4. It can be observed that the accuracy is high for small networks and drops significantly as the size of the model increases. It suggests that a local repair may work globally only if the model is simple. Our interpretation is that the correlation among different neurons are complicated in large neural networks and a minor modificationGuoliang Dong, Jun Sun, Jingyi Wang, XingenWang, Ting Dai, and Xinyu Wang Figure 2: Average time overhead 99.98%  97.78%90.07%99.99% 98.87%89.73%99.99%97.38%84.85%0.002.004.006.008.0010.00 ACAS XuMNISTCIFAR10Avg.time(minutes)p2/FNNSmallp7/FNNMedp8/FNNBig to some neurons may be easily magnified through the network. This suggests that a localized repair, like in our approach, is more likely to be successful in practice. We thus have the following answer to RQ2. Answer to RQ2: nRepair maintains high level of fidelity/accuracy and a local repair generated by nRepair does not apply globally. RQ3: What is the time overhead of our approach? The most time consuming step of our approach is the verification part, which depends on the verification algorithm, the target model and the property. The current implement of nRepair employs DeepPoly as the verifier since it is the stateoftheart. nRepair could be easily refactored to take advantage of improved neural network verifiers which we foresee will be developed in the future. In the following, we report the time taken by nRepair to repair the models (with the same setting as in RQ1).The results are shown in Figure 2, where each bar represents the average time (in minutes) of a successful repair and the percentage shown at the top of each bar is the proportion of the verification time among the total time. For the ACAS Xu task, we show the results on three properties (i.e., property 2, 7 and 8), and for the image classification task, we show the results on three kinds of models (i.e., FNNSmall, FNNMed and FNNBig). We observe that the time overhead varies across different cases. Concretely, the ACAS Xu models take the most time to repair, i.e., 6.05 minutes on average. One possible reason is that the verified properties are more complex than the local robustness property for MNIST and CIFAR10. In general, the more complex the property is, the more difficult it is to verify the model, and thus the more time nRepair takes. This hypothesis, to some extent, can be evidenced by the time spent on repairing property 7. According to the definition in [17], property 7 is more complex than property 2 and 8 since the verification space (defined by the input constraint) of property 7 is the largest compared with that of property 2 and 8. We can observe that the repair of property 7 takes the most time, i.e., 9.74 minutes.Furthermore, it can be observed that the verification time takes up 99.98% of the repair time for ACAS Xu, 98.01% for MNIST and 88.22% for CIFAR10. As expected, the verification time dominates the total execution time. In general, these models are repaired within minutes, which we believe are acceptable as model training typically takes considerable time and our goal to repair a model offline before the model is deployed. We thus have the following answer to RQ3. Answer to RQ3: nRepair ’s execution time depends on the underlying verifier and nRepair is able to repair benchmark models within minutes. RQ4: How does the value of 𝜂influence the repair results? At line 8 of Algorithm 3, we ‘repair’ a neuron by subtracting 𝜂·∇from its output𝜁. The parameter 𝜂has a great impact on the repair results in many ways. Intuitively, the larger 𝜂is, the bigger the modification is, which may have consequences on the fidelity or accuracy as well as the success rate of the repair. Deciding the optimal value for𝜂is highly nontrivial. In the following, we apply nRepair with different 𝜂to understand its impact and subsequently provide practical guidelines on how to set the value of 𝜂. Concretely, for the ACAS Xu models, we evaluate the influence of𝜂on three representative models, one for each of the three prop erties, i.e., model 𝑁3,8,𝑁1,9and𝑁2,9for property 2, property 7 and property 8 respectively. Note that the model for property 2, i.e., 𝑁3,8, is selected according to the median of the number of partitions. For MNIST and CIFAR10, we take all the three models, i.e., FNNSmall, FNNMed and FNNBig, and for each target model, to evaluate the effect of𝜂. For each model, we set 𝜂to 10 different values, i.e., from 0.05 to 0.5 with a step size of 0.05, and then evaluate the effects from three aspects, i.e., success rate, fidelity/accR and time overhead. In the following, we show the results in the three aspects separately. Influence on success rate . The results of the success rate are shown in Table 5. It can be observed that the sensitivity to the value of 𝜂 varies across different repair cases. Concretely, for the models of ACAS Xu and CIFAR10, the success rate firstly increases, e.g., from 88% to 97% for FNNBig of CIFAR10, and then fluctuates at a high level with the increasing of 𝜂, while for the FNNMed and FNNBig of MNIST, the success rate drops gradually with the increasing of 𝜂. For FNNSmall of MNIST, 𝜂has limited impact and the success rate remains unchanged, i.e. 100%. The results show that in some cases, a smaller 𝜂may not be adequate to effectively repair a model due to the limited modifi cation allowed on the selected neurons, and a bigger 𝜂may be necessary. Furthermore, increasing the value of 𝜂does not always lead to better repair since the success rate tends to remain stable or fluctuates within a small range. Our hypothesis is that there is often a threshold on 𝜂such that the magnitude of modification is adequate to “repair” the neuron. Imagine a case where the model could be repaired as long as one neuron is deactivated (i.e., its out put is set to be zero). In such a case, as long as 𝜂is large enough to reduce the neuron’s output to zero, the success rate would remain unchanged. In the case where the success rate drops with a larger 𝜂, an even larger 𝜂is unlikely to work as the optimization process would probably not converge.Towards Repairing Neural Networks Correctly Table 5: Success rate (%) with different 𝜂 Task Model𝜂 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 ACAS Xu𝑁3,8(p2) 80.00 90.00 100.00 100.00 100.00 90.00 100.00 90.00 100.00 100.00 𝑁1,9(p7) 63.64 72.73 81.82 81.82 81.82 81.82 81.82 72.73 72.73 81.82 𝑁2,9(p8) 63.64 81.82 90.91 90.91 90.91 100.00 100.00 100.00 100.00 100.00 MNISTFNNSmall 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 FNNMed 100.00 100.00 98.00 97.00 96.00 97.00 94.00 96.00 94.00 91.00 FNNBig 84.31 82.35 74.51 78.43 74.51 64.71 64.71 58.82 50.98 54.90 CIFAT10FNNSmall 90.00 95.00 93.00 96.00 96.00 96.00 96.00 96.00 96.00 97.00 FNNMed 95.00 98.00 98.00 98.00 98.00 99.00 98.00 99.00 98.00 98.00 FNNBig 88.00 97.00 96.00 95.00 96.00 97.00 98.00 96.00 95.00 97.00 Figure 3: Performance of the repaired models with different 𝜂 97.496.8596.896.8296.8297.4796.6597.2196.7496.7899.8799.7999.7399.3399.3399.3899.3899.4599.3899.499.7499.5699.4399.3899.3599.3299.3399.3399.3399.33 9696.59797.59898.59999.5100 0.050.10.150.20.250.30.350.40.450.5Fidelity(%) ηACASXup2n38p7n19p8n29 98.6998.698.4998.4898.4198.3898.3998.3198.1998.195.4694.3491.5488.4284.0381.9476.3672.5368.9265.5165.5255.449.6142.4244.4434.0839.1535.4325.9225.682030405060708090100 0.050.10.150.20.250.30.350.40.450.5Avg.accRGlobal(%) ηMNISTFNNSmallFNNMedFNNBig 84.5884.9884.2283.9383.7182.7782.682.5382.7482.5977.2475.3974.3973.5973.1372.7671.8370.8969.7968.5568.4460.4257.2253.3249.5347.3244.543.4343.3440.982030405060708090100 0.050.10.150.20.250.30.350.40.450.5Avg.accRGlobal (%) ηCIFAR10FNNSmallFNNMedFNNBig Table 6: Average costs(minutes) with different 𝜂 Task Model𝜂 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 ACAS Xu𝑁3,8(p2) 3.33 2.33 2.57 13.40 6.68 2.25 2.14 3.91 1.74 1.52 𝑁1,9(p7) 35.60 14.26 17.67 9.04 16.63 15.37 9.74 6.32 11.73 10.19 𝑁2,9(p8) 6.77 6.90 7.31 4.73 3.30 4.61 2.98 2.57 2.49 22.81 MNISTFNNSmall 0.07 0.05 0.04 0.03 0.03 0.03 0.03 0.03 0.03 0.02 FNNMed 0.53 0.38 0.47 0.57 1.15 1.34 1.43 1.55 1.55 1.11 FNNBig 7.49 8.65 7.87 6.74 6.08 4.56 6.14 4.20 1.81 4.45 CIFAR10FNNSmall 1.10 0.85 0.70 0.65 0.60 0.50 0.46 0.43 0.38 0.36 FNNMed 1.81 1.35 1.13 0.87 0.79 0.75 0.69 0.39 0.38 0.50 FNNBig 5.92 4.01 3.48 1.84 2.12 1.49 0.94 1.30 1.64 0.72 Influence on the performance of repaired models . We also explore the impact of 𝜂on the “generalizability” of the repaired models, i.e., how big an impact on the global accuracy the repair would lead to with different 𝜂. The results are shown in Figure 3. As expected, the performance, i.e., fidelity or accR, decreases with the increasing of 𝜂in general, although the magnitude of the decline differs from model to model. Concretely, for small models, e.g., all models of ACAS Xu and FNNSmall of both MNIST and CIFAR10, are insensitive to the value of 𝜂. That is, their performance decreases slightly (0.59% decline for MNIST and 1.9% decline for CIFAR10). For big models, the performance of repaired results drops significantlywhen the value of 𝜂increases, i.e., nearly 40% declines for MNIST and 28% declines for CIFAR10. This result is expected as a larger 𝜂 means a greater modification during each iteration in Algorithm 3. Furthermore, as discussed before, deeper and larger networks tend to magnify small modification. Influence on time overhead . Intuitively, a larger 𝜂may acceler ate the repair. We show the average time of successfully repair in Table 6. We can observe that in most cases, the time overhead decreases when an increased 𝜂value. Specially, for the FNNBig of CIFAR10, the time overhead decreases significantly, e.g., the repair with 0.5 on FNNBig is 8.2 times faster than that with 0.05. However,Guoliang Dong, Jun Sun, Jingyi Wang, XingenWang, Ting Dai, and Xinyu Wang there is an exception where the costs increase, i.e., FNNMed of MNIST. This is because that the a large 𝜂may lead to over modi fication on a single neuron and make the neuron “jump over"" the “solution"". In this case, more neurons will be involved, and thus more time is spent. We thus have the following answer to RQ4. Answer to RQ4: In general, a small 𝜂leads to a repaired model with a high fidelity and more time spent on repairing. Our practical guideline is thus to have a small 𝜖as long as the model can be repaired. 4.3 Threats to Validity The dependence on the verifier We only evaluate our approach with the DeepPoly verifier. We choose DeepPoly as it is the stateofthe art at the time of writing. Different verifiers may lead to different performance. In fact, our approach is orthogonal to the rapid de velopment of neural network verification techniques. As long as the verifier used in our framework is sound (i.e., when the veri fier returns holds, the property actually holds), our approach works. The size of repaired network As illustrated in Algorithm 2, our ap proach returns an repaired network which is the result of assem bling the repaired result of each erroneous partition. Thus, the size of the final repaired network depends on the number of erroneous partitions. In our experiments, for the ACAS Xu DNNs, the number of partitions varies from 2 to 20, and thus the size of the repaired network is 2 times to 20 times bigger than the original one. The blowup could be reduced by combining common parts of the re paired network, which we will study in the future work. Limited number of images We evaluate our approach with 100 im ages in fixing the local robustness property. The amount of images may not be adequate. Our experimental setting is largely adopted from existing work, i.e., Goldberger et al. [3] and Singh et al. [24], both used 100 images for evaluation. In our experiments, we take the limited number of images because that it is difficult to obtain qualified test cases. Images selected from the testing dataset must be correctly predicted by the original model but fail to be verified. Meanwhile, at least one counterexample can be found when one image can not be verified. Under these constraints, the total number of qualified images is small. For example, we only found 51 quali fied images for the FNNBig on the whole MNIST testing set (10000 images in total). However, our approach can be easily extended to large scale datasets if more qualified images are available. 5 RELATED WORKS "
371,"DNN Verification, Reachability, and the Exponential Function Problem.txt","Deep neural networks (DNNs) are increasingly being deployed to perform
safety-critical tasks. The opacity of DNNs, which prevents humans from
reasoning about them, presents new safety and security challenges. To address
these challenges, the verification community has begun developing techniques
for rigorously analyzing DNNs, with numerous verification algorithms proposed
in recent years. While a significant amount of work has gone into developing
these verification algorithms, little work has been devoted to rigorously
studying the computability and complexity of the underlying theoretical
problems. Here, we seek to contribute to the bridging of this gap. We focus on
two kinds of DNNs: those that employ piecewise-linear activation functions
(e.g., ReLU), and those that employ piecewise-smooth activation functions
(e.g., Sigmoids). We prove the two following theorems: 1) The decidability of
verifying DNNs with a particular set of piecewise-smooth activation functions
is equivalent to a well-known, open problem formulated by Tarski; and 2) The
DNN verification problem for any quantifier-free linear arithmetic
specification can be reduced to the DNN reachability problem, whose
approximation is NP-complete. These results answer two fundamental questions
about the computability and complexity of DNN verification, and the ways it is
affected by the network's activation functions and error tolerance; and could
help guide future efforts in developing DNN verification tools.","The use of artificial intelligence, and specifically that of deep neural networks (DNNs), is becoming extremely widespread — as DNNs are often able to solve complex tasks more successfully than any other computational approach. These include critical tasks in healthcare [ 16], autonomous driving [ 9], communication networks [ 12], and also the task of communicating with humans through text [ 11] — which seems to bring DNNs closer and closer to passing the famous Turing test [57]. However, it has been shown that even stateoftheart DNNs are susceptible to various errors. Inoneinfamousexample, knownas adversarial perturbations ,smallinputperturbations that are imperceivable to the human eye are crafted to fool modern DNNs, causing them to output incorrect results selected by an attacker [ 23]. Adversarial perturbations thus constitute a safety and security threat, to which most DNNs are susceptible [ 52]. Other issues, such as privacy concerns and bias against various groups, have also been observed, making it clear that a high bar of trustworthiness must be met before stakeholders can fully accept DNNs [30].arXiv:2305.06064v2  [cs.LO]  10 Jul 20232 DNN Verification, Reachability, and the Exponential Function Problem Overcoming these weaknesses of DNNs is a significant challenge, due to their size and complexity. This is further aggravated by the fact that DNNs are machinegenerated (automatically trainedover many examples). Consequently, they are opaque to human engineers, and often fail to generalize their results to examples sufficiently different from the set of examples used for training [ 33]. This has sparked much interest in the verification community, which began studying verification techniques for DNNs, in order to guarantee their compliance with given specifications. In recent years, the verification community has designed and implemented multiple verification algorithms for DNNs, relying on techniques such as SMT solving [ 27,29,61], abstract interpretation [ 20,24], convex relaxation [ 34], adversarial search [ 25], and many others [ 63,5,7,19,2,42,48,50,56,59,41,15,51,4]. Indeed, DNN verification technology has been making great strides recently [36]. Modern verification algorithms depend heavily on the structure of the DNN being verified, and specifically on the type of its activation functions . Initial efforts at DNN verification focused almost exclusively on DNNs with piecewiselinear (PWL) activation functions. It has been shown that the verification of such networks is an NPcomplete problem [ 29,46], and multiple algorithms have been proposed for solving it [ 32,29,20]. Although more recent approaches can handle DNNs with smooth activation functions, these algorithms are often approximate and/or incomplete [ 37,25,50]; in fact, to the best of our knowledge, there is not a single algorithm that is guaranteed to terminate with a correct answer when verifying such DNNs. This raises two important questions: 1.Does there exists a nonapproximating algorithm that can alwayssolve verification queries involving DNNs with nonPWL activation functions? Or, in other words, is the verification problem of DNNs with smooth and piecewisesmooth activation functions decidable ? and 2.When introducing approximations, how difficult does the verification problem become with respect to the DNN, the specification, and the size of the approximation? In other words, what is the computational complexity of DNN verification with smooth and piecewisesmooth activation functions and with ϵerror tolerance? In this paper, we provide a partial answer for the first question, by showing that the verification problem of DNNs with smooth and piecewisesmooth activation functions is equivalent to a wellknown, open problem from the field of model theory — Tarski’s exponential function problem [ 54]. We do so by introducing a constructive bijection between verification queries of such DNNs, and instances of Tarski’s open problem. In addition, we provide a partial answer to the second question, by studying the relations between DNN verification and DNN reachability problems, and ultimately proving that they are equivalent. Even though this equivalence result was previously used [ 14], as far as we know, we are the first to provide a formal reduction. This enables further investigation of DNN verification with any quantifierfree linear arithmetic specification formula as a specific case of DNN reachability, without loss of generality. The latter problem is known to be NPcomplete when ϵerror tolerance is introduced in the result [45]. Formally, we prove the two following theorems: 1.The DNN verification problem for DNNs with smooth and piecewisesmooth activation functions is equivalent to Tarski’s exponential function problem [ 54], which is a wellknown open problem. 2.The DNN verification problem, with any quantifierfree linear arithmetic specification formula, can be reduced to the DNN reachability problem, which is NPcomplete when someϵerror tolerance is allowed [45]. Our results imply a fundamental difference between the hardness of verification of DNNs with piecewisesmooth and with piecewiselinear activation functions. As far as we know, weO. Isac, Y. Zohar, C. Barrett and G. Katz 3 are the first to provide any proof of this difference. The rest of the paper is organized as follows. In Section 2 we provide background on DNNs, verification and other necessary mathematical concepts. In Section 3 and Section 4 we formally prove the two main results mentioned above. In Section 5 we discuss related work, and in Section 6 we describe our conclusions and directions for future work. 2 Background 2.1 Deep Neural Networks Deep neural networks (DNNs) [ 22] are directed graphs whose nodes (neurons) are organized into layers, and whose nodes and edges are labeled with rational numbers. Nodes in the first layer, called the input layer , are assigned values matching the input to the DNN; and then the values of nodes in each of the subsequent layers are computed as functions of the values assigned to neurons in the preceding layer. More specifically, each node value is computed by first applying an affine transformation (linear transformation and addition of a constant) to the values from the preceding layer, and then applying a nonlinear activation function [13] to the result. The final (output) layer, which corresponds to the output of the network, is computed without applying an activation function. Three of the most common activation functions are the rectified linear unit (ReLU), which is defined as: ReLU (x) =/braceleftigg x x> 0 0otherwise; theSigmoid function, defined as: σ(x) :R→(0,1) =exp(x) exp(x)+1 where expis the exponential function; and the hyperbolic tangent , defined as: tanh(x) :R→(−1,1) =exp(x)−exp(−x) exp(x)+exp(−x) The latter two activation functions are both injective, and their inverses are defined as follows: ∀x∈(0,1) :σ−1(x) = ln(x 1−x) ∀x∈(−1,1) : tanh−1(x) =1 2ln(1+x 1−x) where lnis the natural logarithm function. In addition, we consider the NLReLU activation function [31, 13], denoted τfor short, which is defined as: NLReLU (x) =τ(x) = ln(ReLU (x) + 1) A simple DNN with four layers appears in Figure 1, where all biases are set to zero and are ignored. For input ⟨1,2,1⟩, the first node in the second layer evaluates to ReLU (1·1 + 2·(−1)) =ReLU (−1) = 0; the second node in the second layer evaluates to ReLU (2·1 + 1· (−1)) =ReLU (1) = 1; and the node in the third layer evaluates to σ(0−1) =σ(−1). Thus, the node in the fourth (output) layer evaluates to 4·σ(−1). Formally, a DNN N:Rm→Rkis a sequence of nlayersL0,...,Ln−1where each layer Liconsists ofsi∈Nnodes, denoted v1 i,...,vsi i, and biases pj i∈Qfor eachvj i. Each directed edge in the DNN is of the form (vl i−1,vj i)and is labeled with wi,j,l∈Q. The assignment to the nodes in the input layer is defined by vj 0=xj, wherex∈Rmis the input vector, and the assignment for the jthnode in the 1≤i<n−1layer is computed as4 DNN Verification, Reachability, and the Exponential Function Problem x1 x2 x3v1 v2v3y1 −1 1 −11 −14ReLU ReLUσ Figure 1 A toy DNN. vj i=fj i/parenleftbiggsi−1/summationtext l=1wi,j,l·vl i−1+pj i/parenrightbigg for some activation function fj i:R→R. Finally, neurons in the output layer are computed as: vj n−1=sn−2/summationtext l=1wn−1,j,l·vl n−2+pj n−1 wherewi,j,landpj iare (respectively) the predetermined weights and biases of N. The size of a network|N|is defined as the overall number of its neurons. 2.2 Formal Analysis of DNNs The formal methods community has tackled the formal analysis of DNNs primarily along two axes: DNN verification , and DNN reachability . These are two related formulations, as reachability problems may be expressed as verification problems in a straightforward manner. In this paper, we further study the connections between these two formulations. DNN Verification. LetN:Rm→Rkbe a DNN and P:Rm+k→{⊤,⊥}be a property, where⊤,⊥represent the values for which the property does and does not hold, respectively. TheDNN verification problem is to decide whether there exist x∈Rmandy∈Rksuch that (N(x) =y)∧P(x,y)holds. In particular, a verification query is always expressed as an existential formula. If such xandyexist, we say that the verification query ⟨N,P⟩is satisfiable (SAT); and otherwise, we say that it is unsatisfiable (UNSAT). A verification algorithm is soundif it does not return UNSATfor satisfiable queries, and does not return SATfor unsatisfiable queries (in other words, if its answers are always correct); and iscomplete if it always terminates, for any query. So far, there have been several efforts at studying the complexitytheoretical aspects of DNN verification [ 17,26,46,47,45,28,29]. Most previous work was focused on DNNs with piecewiselinear activation function (specifically, ReLUs), while leaving many open questions about the complexitytheoretical aspects of verifying DNNs with smooth and piecewisesmooth activation functions. DNN Reachability. Given a DNNN:Rm→R, a function o:R→Rand an input set X ⊆ [0,1]m, theDNN reachability problem is to compute sup x∈Xo(N(x))and inf x∈Xo(N(x)), perhaps up to some ϵerror tolerance. For DNNs with Lipschitzcontinuous activationO. Isac, Y. Zohar, C. Barrett and G. Katz 5 functions, either smooth or piecewiselinear, (such as σandReLU), the reachability problem with some ϵerror tolerance is NPcomplete, in the size of the network and ϵ[45]. In this work, we consider a decision version for the problem where ois the identity, deciding whether sup x∈XN(x)≥0is achieved for some x∈X. This decision version with some ϵerror tolerance is then to decide whether sup x∈XN(x)≥−ϵis achieved for some x∈X. In addition, we may assume that the input of Nis within [0,1]m, as the input domain may be normalized before the network is evaluated. For example, consider the DNN depicted in Figure 1. A possible verification query for this DNN is given by a property Pthat returns⊤if and only if (x1,x2,x3)∈[0,1]3∧ (y∈[0.5,0.75]∨y∈[0,0.25]); i.e., if there exists an input in the [0,1]3cube, for which y∈[0.5,0.75]∨y∈[0,0.25]. A possible reachability query is to check whether there exists an input in the domain [0,1]×[0,0.5]×[0.5,1], for which y≥0. This reachability query can trivially be represented as a verification property P′, which returns⊤if and only if (x1,x2,x3)∈[0,1]×[0,0.5]×[0.5,1]∧y≥0. For anyϵ>0, the equivalent reachability query withϵtolerance is to decide if there exists an input in the domain [0,1]×[0,0.5]×[0.5,1], for whichy≥−ϵ. 2.3 Decidability and Mathematical Logic Mathematical Logic. In mathematical logic, a signature Σis a set of symbols, representing functions and relations. A Σformula is a formula, comprised of atoms and relations that appear in Σ, the usual logical operators (∧,¬,∨,→,↔), and the quantifiers ∀(universal) and∃(existential). A variable affixed with a quantification symbol is a bounded variable ; and otherwise it is a free variable . A formula without free variables is called a sentence, and a formula without bounded variables is called a quantifierfree formula . A formula with variablesx= (x1,...,xn)of the form∃(x).φwhereφis quantifierfree is called an existential formula. AΣtheoryis a set of Σsentences. A ΣmodelMis comprised of a set of elements, denoted|M|, and an interpretation for all Σfunctions and relations; that is, a definition fM:|M|n→|M|for every nary function f∈Σ, and a definition rM⊆|M|mor every mary relation r∈Σ. If the interpretation of a Σsentenceφis true within a model M, we say thatMsatisfiesφ, and denoteM|=φ. IfMsatisfies all sentences in a ΣtheoryT, thenMis aTmodel, denotedM|=T. Given some model Mover signature Σ, we define the theory Th(M)as the set of all Σsentencesφsuch thatM|=φ. It is then trivial that M|=Th(M). For example, let Σbe the set{+,−,·,0,1,<}where +,−,·are 2ary functions, 0,1are 0ary functions and <is a 2ary relation. Let Mbe a model defined over Rwith addition, subtraction, multiplication, the constants 0,1 and the usual order. Then Th(M)is the set of allΣsentences thatMsatisfies, such as∀x:x·0 = 0. Theory Decidability. For a ΣtheoryTand a Σsentenceφ, we say that φisTvalidand denoteT ⊢φ, if every model of Tsatisfiesφ. Furthermore, we say that φisTsatisfiable if there exists a model MofT, for whichM|=φ; and that φisTunsatisfiable ifM̸|=φ for all modelsMofT. Satisfiability and validity are closely connected, as φisTvalid if and only if¬φisTunsatisfiable. A theory Tisdecidable if there exists an algorithm that, for any sentence φ, decides whether T ⊢φ, within a finite number of steps. If φis valid, the algorithm returns ⊤; and otherwise, it returns ⊥. Due to the connection of satisfiability and validity, validitychecking algorithms may also be used to decide satisfiability, and vice versa. In particular, for any theory Th(M)for some modelM, allTh(M)models satisfy6 DNN Verification, Reachability, and the Exponential Function Problem exactly the same sentences, so validity and satisfiability are equivalent. Thus, throughout this paper we use decision procedures to decide the Th(M)satisfiability of formulas. In addition, when considering quantifierfree formulas (i.e., a formula where all variables are free), all of the formula’s variables are implicitly existentially quantified. In this case, for any quantifierfree formula φ(x)with variable vector x, the satisfiability problem of φ(x) with respect to a model Mis equivalent to deciding whether M|=∃x.φ(x). Similarly, the satisfiability problem of φ(x)with respect to a theory Tis equivalent to deciding whether ∃x.φ(x)isTsatisfiable. It has previously been shown that the theory of the real field Th(R,+,−,·,0,1,<) is decidable [ 53], and that the theory of the real field with the transcendental functions exp,sinand the constants log2,πis undecidable [ 43]. The question of the decidability of Th(R,+,−,·,0,1,<,exp)has remained an open problem since the 1950’s [ 54], and it is commonly known as Tarksi’s exponential function problem . A theoryTisstablyinfinite if for every quantifierfree formula φ, the satisfiability of φ inTimplies that φis satisfiable in some infinite model of T. It is then immediate that for any infinite model M,Th(M)is stablyinfinite. Given two decidable, stablyinfinite theories T1andT2defined over disjoint sets of symbols, for any quantifierfree formulas F1∈T1,F2∈T2, the formula F1∧F2is decidable as well [ 39]. TheNelsonOppen method [39] is a wellknown method for combining two decision procedures for two theories into a decision procedure for the quantifierfree fragment of their union. Equisatisfiability. Two formulas φandψareequisatisfiable ifφis satisfiable if and only ifψis satisfiable. For example, the formulas φ:= (a+b)∗(a−b) = 0andψ:=c∗d= 0∧c=a+b∧d=a−bare equisatisfiable. Note that φandψmay be formulated in different theories,T1andT2, respectively. In this case, we say that the formulas are equisatisfiable if φisT1satisfiable if and only if ψisT2satisfiable. Function Definability. For any signature Σand an nary function f, not necessarily in Σ, we say that fisdefinable in aΣmodelMif there exists a Σformulaψ(x1,...,xn,y,z 1,...,zm) over the variables x1,...,xn,ysuch that for any elements a1,...,an,binMwe have that M|=∃z1,...,zm.ψ(a1,...,an,b,z 1,...,zm)if and only if b=f(a1,...,an). We say that fis definable in a ΣtheoryTif it is definable in all models of T. ModelCompleteness. In model theory, the concept of modelcompleteness has several equivalent definitions. For our purposes, a theory Tis modelcomplete if and only if any formula in the theory has an equivalent existential formula (modulo T). This means that the existential formulas in Tcan express all the formulas in it. Th(R,+,−,·,0,1,<,exp)is known to be modelcomplete [60]. 3 Decidability of DNN Verification In this section, we prove our first main result: the decidability of verifying a DNN with the activation functions ReLU,σ,tanhandτis equivalent to the decidability of Th(R,+,−,·,0,1,<,exp). The decidability of this theory is an open problem [ 54]. Thus, the equivalence implies that the decidability of DNN verification for DNNs with the activation functions ReLU, σ,tanhandτis an open problem as well. For simplicity, we denote TR=Th(R,+,−,·,0,1,<),Texp=Th(R,+,−,·,0,1,<,exp), andTσ=Th(R,+,−,·q∈Q,0,1,<,σ, tanh,τ), where·qis an unary function, interpreted as the multiplication with a constant q∈Q. We use Σσ,ΣexpandΣRto denote the signatures ofTσ,TexpandTR, respectively. Note that for any DNN, weights and biases are in Q, and canO. Isac, Y. Zohar, C. Barrett and G. Katz 7 thus be expressed as TRterms. Therefore, we can express the affine constraints of the network asTRformulas. In addition, any constraint of the form f=ReLU (b)can be expressed as the formula (f=b↔b>0)∧(f= 0↔b≤0), and thus ReLUis definable inTR,TexpandTσ. Therefore, without loss of generality, we need not add a function symbol to Σσto express DNNs with ReLUactivation functions (or any other piecewiselinear function). Our goal is then to show that the decidability of Texpis equivalent to the decidability of all existential formulas ofTσ. Example: We begin with an example that illustrates this equivalence. For the first direction, consider the toy DNN depicted in Figure 1, and let x1,x2,x3,b1,f1,b2,f2,b3,f3, andybe the variables of the network. Variables x1,x2,x3represent the input variables, variables b1,f1,b2,f2,b3,f3represent the inputs and outputs of nodes v1,v2,v3, respectively, and variableyrepresents the network’s output. Let Pbe the property restricting the input to be within [0,1]3and the output to be in [1,2]. The verification query for the network in Figure 1 and Pis then: /logicalandtext i∈1,2,3[(xi≥0)∧(xi≤1)]∧ (x1−x2=b1)∧(x2−x3=b2)∧/logicalandtext i∈1,2[(fi=bi)↔(bi>0)]∧[(fi= 0)↔(bi≤0)]∧ (f1−f2=b3)∧(f3=σ(b3))∧(4·f3=y)∧ (1≤y)∧(y≤2) This is aTσquery, which can be expressed as a query in Texp, sinceσ(x) =exp(x) 1+exp(x). The equivalentTexpquery is: /logicalandtext i∈1,2,3[(xi≥0)∧(xi≤1)]∧ (x1−x2=b1)∧(x2−x3=b2)∧/logicalandtext i∈1,2[(fi=bi)↔(bi>0)]∧[(fi= 0)↔(bi≤0)]∧ (f1−f2=b3)∧[(exp(b3) + 1)·f3= exp(b3)]∧(4·f3=y) (1≤y)∧(y≤2) For the second direction, we begin by demonstrating a purification process of a given formulaφ:=exp(a+b) =exp(a)·exp(b)into a formula inTσ. We assume that we can define ψc=a·bandψy=exp(x)inΣσ, which are defined over the variables a,b,candx,y, respectively, and that witness the definability of the functions ·andexpinTσ. Therefore, the formula ψp=exp(a)∧ψq=exp(b)∧ψr=exp(a+b)∧ψr=p·q is equisatisfiable to exp(a+b) = exp(a)·exp(b). Formally, we prove the following theorem: ▶Theorem 1. The decidability of verifying DNNs with σ,tanh,τandReLUactivation functions is equivalent to the decidability of Th(R,+,−,·,0,1,<,exp). Proof.The first direction of the proof is similar to a technique proposed by Ivanov et al. [ 28]. Assume there exists a decision procedure for Texp, and letF∈Σσbe a DNN verification query. For any appearance of σ(t)for some term t, we replace t,σ(t)with the fresh variables x,y, respectively and add the conjunction: (exp(x) + 1)·y= exp(x)∧x=t8 DNN Verification, Reachability, and the Exponential Function Problem to the resulting formula. This is done in a way similar to the one described in the example. Similarly, for any appearance of tanh(t)for some term t, we replace t,tanh(t)with the fresh variablesx,y, respectively and add the conjunction: (exp(x) + exp(−x))·y= exp(x)−exp(−x)∧x=t to the resulting formula. For defining τ, we first define: ψf=ReLU (b):= (f=b↔b>0)∧(f= 0↔b≤0) Now, for any appearance of τ(t) =ln(ReLU (t) + 1)for some term t, we replace t,τ(t)with the fresh variables x,y, respectively and add the conjunction: ψz=ReLU (x)∧exp(y) =z+ 1∧x=t to the resulting formula, where zis an additional fresh variable. After repeating this process iteratively, we convert any F∈Σσto an equisatisfiable formula F′∈Σexp. We then use the decision procedure to decide the satisfiability of F′. The second direction of the proof is more complex. Assume we have a sound and complete verification procedure for DNNs with σ,tanhandτactivation functions; that is, a decision procedure for deciding the satisfiability of quantifierfree Σσformulas inTσ. SinceTexpis modelcomplete, it is tempting to try and construct a decision procedure for theexistentialformulasof Texp. However, tothebestofourknowledge, givenageneralformula inTexpit is not known how to effectively derive its equivalent existential formula. In order to circumvent this issue, we consider instead a fourth theory, Te=Th(R,+,−,·,0,1,<,e ), defined over the signature Σe, wheree:R→Rwithe(x) =exp(1 1+x2)is therestricted exponential function. It has been shown by Macintyre and Wilkie [ 35] that the decidability of this theory implies the decidability of Texp, and that given any formula in the language of Te, one can effectively find an equivalent existential formula (in Te). Therefore, it is enough for our purpose to consider any existential formula ∃x.φ∈Σe, and decide the satisfiability of φinTe. Let∃x.φ∈Σebe an existential formula, where φis a quantifierfree formula. We construct a Σσformulaψ, equisatisfiable to φ. In this construction, all variables are implicitly existentially quantified. In order to do so, it is enough to define formulas ψc=a·b andψy=e(x)over the variables a,b,candx,yrespectively, and witness the definability of the functions·andeinTσ. In this case, given any formula φ∈Σe, we can iteratively replace any occurrence of terms of the form t·swith the fresh variable pand add the conjunction ψp=t·s, and occurrences of terms of the form e(x)with the fresh variable qand add the conjunction ψq=e(x). This process terminates with a Σσformulaψequisatisfiable to φ, allowing us to apply the decision procedure to ψ. To complete the proof, it remains to show how ψc=a·bandψy=e(x)can be defined using the formula ψy=ln(x). We show the construction of ψy=ln(x)later, in Lemma 2, and we use it here to define both ψc=a·bandψy=e(x). We start by defining ψc=a·b. Note that∀a,b> 0, it holds that ln(a·b) =ln(a) +ln(b); and soa·b=exp(ln(a) +ln(b)), assuminga,b> 0. This equality can be expressed using the formula: θc=a·b:=ψp=ln(a)∧ψq=ln(b)∧ψp+q=ln(c), wherecrepresents the value of a·b, andp,qare fresh variables. For defining ψc=a·bfor all a,b∈R, we split into cases, and write:O. Isac, Y. Zohar, C. Barrett and G. Katz 9 ψc=a·b:= [(a>0∧b>0)→θc=a·b]∧ [(a<0∧b>0)→θ−c=−a·b]∧ [(a>0∧b<0)→θ−c=a·−b]∧ [(a<0∧b<0)→θc=−a·−b]∧ [(a= 0∨b= 0)↔c= 0], which represents the function ·and witnesses its definability in Tσ. We now define ψy=e(x). Recall that e(x) =exp(1 x2+1), so in order to define ψy=e(x)we use bothψc=a·bandψy=ln(x): ψy=e(x):=ψa=ln(y)∧ψ1=a·(b+1)∧ψb=x·x wherea,bare fresh variables. We have defined both ψc=a·bandψy=e(x), which concludes our proof. ◀ For the completeness of this section, we provide now the proof of Lemma 2, which shows the construction of ψy=ln(x): ▶Lemma 2. The natural logarithm function lnis definable inTσ. Proof.First, observe that for any x≥1we have that τ(x−1) = ln(ReLU (x−1)+ 1) = ln(x−1 + 1) = ln(x). Second, observe that ∀x∈(0,1), the inverses of σandtanhare defined and are equal to: σ−1(x) = ln(x 1−x) = ln(x)−ln(1−x) and tanh−1(x) =1 2ln(1 +x 1−x) =1 2(ln(1 +x)−ln(1−x)) We conclude that: ∀x∈(0,1) :σ−1(x)−2 tanh−1(x)+τ(x) = ln(x)−ln(1−x)−ln(1+x)+ln(1−x)+ln(x+1) = ln(x) We can express this relation using the following formula, and the fresh variables a,b,c: θx,y:= [x=σ(a)]∧[x= tanh(b)]∧[c=τ(x)]∧[y=a−2b+c] Where 2bis syntactic sugar for ·2(b). Thus, we can define: ψy=ln(x):= [(1<x)→(y=τ(x−1))]∧[(x= 1)↔(y= 0)]∧[(0<x< 1)→θx,y]∧[0<x] which concludes the proof. ◀ 4 DNN Verification is DNN Reachability The two main formal analysis approaches for DNNs, verification and reachability, are closely connected: a DNN reachability instance can be formulated as DNN verification in a straightforward manner, as in the example in Section 2.2. Presently, DNN analysis algorithms and tools typically support one of the two formulations. Here, we prove that DNN verification and DNN reachability are in fact equivalent. In this part, we consider DNNs that use both piecewiselinear and Sigmoidal activation functions. We formally prove that any instance of the DNN verification problem, with any specification expressible by a quantifierfree linear arithmetic formula, can be reduced to an10 DNN Verification, Reachability, and the Exponential Function Problem instance of the DNN reachability problem. Since the reachability problem is a specific case of verification, we ultimately prove that for DNNs, reachability and verification are equivalent. Since it was shown that approximation of DNN reachability queries with Lipschitzcontinuous activation functions (such as σandReLU) is NPcomplete [ 45], we deduce that the DNN verification problem is reducible to a problem whose approximation is NPcomplete. The reduction involves adding an additional input, denoted ϵ, and we use (x,ϵ)to denote the concatenation of ϵto the input vector x. Formally, we prove the following theorem: ▶Theorem 3. LetN:Rm→Rkbe a neural network, let φbe a quantifierfree property with atoms expressing affine constraints over variables yiofN, and letX⊆Rm. There exists a neural network N′:Rm+1→R, with|N′|=O(|N|+|φ|)such that the two following conditions are equivalent: ∃x∈X.N(x)|=φ ∃(x,ϵ)∈X×(0,1].N′(x,ϵ)≥0 Example. We begin with an example for constructing N′, given some DNN N,and a propertyφ. Consider firstN:R4→R2as depicted in Figure 2a and φ:= (y1>0)∧(y1≥y2). We denote θ:=y1≥y2andψ:=y1>0≡¬(−y1≥0). In Figure 2 we start with the initial DNNN, and then iteratively add new nodes to N. In particular, we show how to add neurons that are active if and only if N |=θandN |=ψ, respectively in Figure 2b and Figure 2c. Lastly, in Figure 2d we show how to add the output neuron, such that ∃x∈X.N(x)|=φif and only if∃(x,ϵ)∈X×(0,1].N′(x,ϵ)≥0. This concludes our example. We now prove the theorem by induction on the generating sequence of φ; that is, a sequence of subformulas of φ:φ1,...,φnsuch that∀i,jifj >ithenφjcannot be a sub formula ofφi, andφn=φ. This allows inductive proofs over the formulas [ 49]. For example, a generating sequence for the formula φ:=∃x,y.(3x≥7)∧¬(y≥x) is: y≥x,3x≥7,¬(y≥x),(3x≥7)∧¬(y≥x),∃x,y.(3x≥7)∧¬(y≥x) Proof.Without loss of generality, assume that φis composed of atoms, negations, and conjunctions. In addition, assume that each variable yjis an output variable (otherwise, we may add neurons with the identity as activation function from the neuron outputting yjto the output layer). For every step iin the generating sequence φ1,...,φk=φ, we add a constant number of output neurons, such that for any x∈Rm, the resulting DNN N′ i, satisfiesN′ i≥0(for the last constructed output neuron) if and only if N(x)|=φi. Below we explain the construction and prove its correctness; and in Figure 3 we show its visual representation. Base Cases: 1.Letφ:=⊤. In this case,N′is constructed from Nby adding a single affine neuron with no activation function, and with its input edges with weight 0from all output nodes of N. This also maintains the convention that the output layer does not have an activation function. Therefore, ∀x∈Rm:N′(x,ϵ)≥0if and only if/summationtext i0≥0, which is equivalent to ⊤. The case of⊥is covered by our handling of negations.O. Isac, Y. Zohar, C. Barrett and G. Katz 11 x1 x2 x3 x4v1 v2 v3y1 y21 −1 1 −1 1 −15 −3 3 −5ReLU ReLU ReLU (a)The initial network.x1 x2 x3 x4v1 v2 v3y1 y2yθ1 −1 1 −1 1 −15 −3 3 −51 −1ReLU ReLU ReLU (b)Adding a construct for y1≥y2. x1 x2 x3 x4v1 v2 v3y1 y2yθ yψ ϵ1 −1 1 −1 1 −15 −3 3 −51 −1 1 11−1ReLU ReLU ReLU (c)Adding a construct for y2>0. x1 x2 x3 x4v1 v2 v3y1 y2yθ yψ ϵyφ1 −1 1 −1 1 −15 −3 3 −51 −1 1 11−1−1 −1−1 −1ReLU ReLU ReLUReLU ReLUReLU (d)Adding a construct for φ, as a conjunction. Figure 2 Construction of a reachability problem for N|=φ.12 DNN Verification, Reachability, and the Exponential Function Problem 2.Letφ:=/summationtext ici·yi+b≥0. In this case,N′is constructed from Nby adding a single affine neuron with no activation function, with its input edges with weight cifrom every output neuron yiofN, and a bias b. Therefore,∀x∈RmandN(x) =ywe have that/summationtext ici·yi+b≥0if and only ifN′(x,ϵ)≥0. We note that equality can be handled using conjunctions, while strict inequalities can be handled using negations. Inductive step: 1.Letφ:=ψ∧θ, and letyψ,yθbe the values of the neurons such that yψ≥0,yθ≥0if and only ifN(x)|=ψ,θ, respectively. Consider: yφ=−ReLU (−yψ)−ReLU (−yθ) In this case, we have that yφ≥0if and only if yψ≥0∧yθ≥0. We can see this since ifyψ≥0∧yθ≥0then both−ReLU (−yψ)and−ReLU (−yθ)equal zero. Otherwise, at least one of−ReLU (−yψ)and−ReLU (−yθ)is negative (and the other is nonpositive). Thus, we add two ReLUneurons, with a single −1input edge from each of the nodes corresponding to yψ,yθ, respectively. We then add a third neuron with two −1edges from the ReLU nodes and no activation function. 2.Letφ:=¬ψ, and letyψbe the value of the neuron such that yψ≥0if and only if N(x)|=ψ. In this case, we first need to add a new ϵφinput neuron, and restrict it to ϵφ>0. Then, observe that ¬(yψ≥0)≡yψ<0if and only if there exists some ϵ>0 s.t.ϵ+yψ≤0, or equivalently −ϵ−yψ≥0. Therefore, we add a new neuron with no activation function, with a skip connection from the ϵφneuron with weight −1, and with a−1weight from yψ, resulting in yφ=−ϵφ−yψ. We note that since there are finitely many such constructions, we can choose the minimal ϵimplied by all of them, and again choose the minimum of it and 1. Thus, a single ϵ∈(0,1]suffices. In addition, the use of the skip connections can be replaced with a line of ReLUneurons, starting with the ϵ neuron and feedforwarding to a neuron on every layer. This construction does not affect the asymptotic size of N′. On every step of the recursion, we added a constant number of neurons to the network, such that∀x∈Rm,N′(x,ϵ)≥0if and only ifN(x)|=φi. This concludes our proof. ◀ y1 yny⊤0 0 0 (a)An atom predicate of the form ⊤.y1 ynyPc1 ci cn +b (b)An atom predicate of the form P:=/summationtext ici·yi+b≥0. yψ yθyφ−1 −1−1 −1ReLU ReLU (c)A formula of the form φ=ψ∧θ.yψ ϵyφ−1 −1 (d)A formula of the form φ=¬ψ. Figure 3 Constructs for each step of the induction.O. Isac, Y. Zohar, C. Barrett and G. Katz 13 5 Related Work "
527,Global Optimization of Objective Functions Represented by ReLU Networks.txt,"Neural networks can learn complex, non-convex functions, and it is
challenging to guarantee their correct behavior in safety-critical contexts.
Many approaches exist to find failures in networks (e.g., adversarial
examples), but these cannot guarantee the absence of failures. Verification
algorithms address this need and provide formal guarantees about a neural
network by answering ""yes or no"" questions. For example, they can answer
whether a violation exists within certain bounds. However, individual ""yes or
no"" questions cannot answer qualitative questions such as ""what is the largest
error within these bounds""; the answers to these lie in the domain of
optimization. Therefore, we propose strategies to extend existing verifiers to
perform optimization and find: (i) the most extreme failure in a given input
region and (ii) the minimum input perturbation required to cause a failure. A
naive approach using a bisection search with an off-the-shelf verifier results
in many expensive and overlapping calls to the verifier. Instead, we propose an
approach that tightly integrates the optimization process into the verification
procedure, achieving better runtime performance than the naive approach. We
evaluate our approach implemented as an extension of Marabou, a
state-of-the-art neural network verifier, and compare its performance with the
bisection approach and MIPVerify, an optimization-based verifier. We observe
complementary performance between our extension of Marabou and MIPVerify.","Articial deep neural networks (DNNs) have demonstrated great promise in a wide variety of applications (Schmidhuber, 2015; Liu et al., 2017). These applications include image recognition (Krizhevsky et al., 2012), control (Hunt et al., 1992), and natural language processing (Otter et al., 2020), among many others. Because of these successes, there is naturally interest in incorporating DNNs into other applications, including safetycritical systems (Bojarski et al., 2016; Julian et al., 2016). Although DNNs are obtaining unprecedented results, their opacity poses signicant challenges | especially in the context of safetycritical systems, where mistakes can endanger lives and cause signicant damage. A notable example in cludes DNNs in autonomous driving systems, where unexpected behavior of the DNN could harm passengers or pedestrians. Consequently, it is especially desir able to formally reason about DNNs, providing rigorous guarantees about their behaviors. Recent research has focused on neural network verication (Huang et al., 2017; Katz et al., 2017; Gehr et al., 2018; Wang et al., 2018b). Verication involves an swering \yes or no"" questions about DNNs, and can be used to rule out undesirable behaviors. For example, a verication query for an autonomous driving DNN could ask whether an input exists that encodes a situation in which the autonomous ve hicle is approaching an obstacle, but for which the DNN advises the vehicle to maintain the current course. If the verication engine answers no, we are guar anteed that this particular behavior can never happen for any possible input . If it answers yes, then it returns an input that leads to the undesirable outcome. The verication problem has been shown to be NPcomplete (Katz et al., 2017); however, large strides have been made in recent years in solving networks that arise in practice (Wang et al., 2018a; Weng et al., 2018; Katz et al., 2019; Tran et al., 2020b; Wu et al., 2020). Although tremendous eort has been put into answering yes or no questions about DNNs, formally answering quantitative questions about them has received less attention. Such questions can be highly important when verifying a system: for example, we may want to know how close an obstacle can be before the DNN controller turns the vehicle, or how much the steering of the car can be aected by small errors in the input image (e.g., caused by a malfunctioning camera). Finding answers to these questions requires an optimization process. Existing techniques can provide bounds on the answers to these questions, but the bounds may be too loose to eectively reason about the performance of the system (Singh et al., 2018a; Wang et al., 2018a,b; Weng et al., 2018; Zhang et al., 2018; Boopathy et al., 2019; Liu et al., 2021). For example, knowing that the most extreme steering angle for aGlobal Optimization of Objective Functions Represented by ReLU Networks 3 selfdriving car is somewhere between "
11,Credit Assignment for Trained Neural Networks Based on Koopman Operator Theory.txt,"Credit assignment problem of neural networks refers to evaluating the credit
of each network component to the final outputs. For an untrained neural
network, approaches to tackling it have made great contributions to parameter
update and model revolution during the training phase. This problem on trained
neural networks receives rare attention, nevertheless, it plays an increasingly
important role in neural network patch, specification and verification. Based
on Koopman operator theory, this paper presents an alternative perspective of
linear dynamics on dealing with the credit assignment problem for trained
neural networks. Regarding a neural network as the composition of sub-dynamics
series, we utilize step-delay embedding to capture snapshots of each component,
characterizing the established mapping as exactly as possible. To circumvent
the dimension-difference problem encountered during the embedding, a
composition and decomposition of an auxiliary linear layer, termed minimal
linear dimension alignment, is carefully designed with rigorous formal
guarantee. Afterwards, each component is approximated by a Koopman operator and
we derive the Jacobian matrix and its corresponding determinant, similar to
backward propagation. Then, we can define a metric with algebraic
interpretability for the credit assignment of each network component. Moreover,
experiments conducted on typical neural networks demonstrate the effectiveness
of the proposed method.","Artiﬁcial neural networks (ANN), also known as neural net works (NN) for short, have recently emerged as leading can didate models for deep learning (DL), popularly used in a variety of areas, such as computer vision (Dahnert et al. 2021; Esser, Rombach, and Ommer 2021; Tian, Yang, and Wang 2021), natural language processing (Karch et al. 2021; Wang et al. 2021; Yuan, Neubig, and Liu 2021) and so on. Behind the enormous success, ANNs are generally with complicated structures, meaning that there is an intricate data ﬂow through multiple linear or nonlinear components from an input sample to its corresponding output. Since each component works for the information processing and affects *This work is supported by the National Natural Science Foun dation of China under Grant No.61872371, No.61836005 and No.62032024 and the CAS Pioneer Hundred Talents Program.the subsequent components, there is a pressing need to eval uate how much a certain component contributes for the ﬁnal output. This problem is termed Credit Assignment Problem (CAP) (Minsky 1961), which has being at the heart of the training methods of ANNs since proposed. For an untrained neural network consisting of a large quantity of parameters, CAP coincides with the requirement to change parameters at different levels during its train ing phase. Backward Propagation algorithm (BP) (Werbos 1974) provides a powerful and popular approach to CAP, which computes partial derivatives representing the sensitiv ity of the neurons in a certain layer on the ﬁnal loss and indi rectly reﬂects the credits of different neurons to the network output. It is widely utilized in the mainstream training meth ods of ANNs, such as SGD (Nemirovski et al. 2009), RM SProp (Tieleman, Hinton et al. 2012) and Adam (Kingma and Ba 2015), promoting the prosperity of neural networks and deep learning greatly. Whereas, instead of training phases, the problem on trained neural networks, i.e., evaluating the credit of each component to the network capability, receives little focus. As a matter of fact, the CAP on trained ANNs is of vi tal signiﬁcance in neural network related research. Minimal network patch (Kauschke and F ¨urnkranz 2018; Goldberger et al. 2020) requires to ﬁnd out the minimal modiﬁcation on network parameters to satisfy certain given properties which do not hold before. Furthermore, formal speciﬁcation and veriﬁcation on ANNs (Liu et al. 2020; Akintunde et al. 2019; Vengertsev and Sherman 2020; Liu et al. 2021) demands to do speciﬁcations or veriﬁcation on the most sensitive inputs related to robustness, reachability and so on. Visualization seems to be a candidate solution to CAP on trained ANNs and some researchers resort to feature maps to describe the contribution of newly designed neural net work components (or, layers) (Han et al. 2015; Lengerich et al. 2017; Tang et al. 2018). However, it is mainly lim ited in computer vision domains and lacks of formal anal ysis and guarantee. Meanwhile, it is difﬁcult for human to recognize the feature maps as the process goes in networks, let alone matching them with the learning credit, as illus trated in Fig. 4(a). Consequently, it is instrumental and ur gent to deal with the credit assignment problem for trained neural networks formally and rigorously. For simplicity, thearXiv:2212.00998v1  [cs.LG]  2 Dec 2022following referred CAPs are all on trained neural networks. In fact, an ANN can be regarded as a special class of non linear dynamical system evolved from big data and there ex ist mature datadriven theories of dynamics analysis, which makes it possible to tackle the credit assignment problem from a dynamics perspective. More appealingly, Koopman operator theory could accomplish linear approximation of nonlinear systems, which greatly simpliﬁes the analysis on the base of corresponding Koopman operators. Though the Koopman operator is a linear transformation on inﬁnite dimensions, an approximation transformation on ﬁnite di mensions is adopted generally, which can be obtained with the representative datadriven algorithms, such as Dynamic Mode Decomposition (DMD) (Kutz et al. 2016), Extended Dynamic Mode Decomposition (EDMD) (Li et al. 2017), Kernel Dynamic Mode Decomposition (KDMD) (Kawahara 2016) and so on. When a nonlinear dynamical system is associated with a linear transformation, its credit analysis would be tractable with the linear algebra theory then. In this paper, inspired by the Koopman operator theory and backward propagation process, we propose an alterna tive approach to resolving the credit assignment problem for trained neural networks from a linear dynamical system per spective. We treat an ANN as a nonlinear system, consisted of a series of subdynamics (i.e., components) to be assigned credit. With the aid of Koopman linearization of each sub system, the established function of an ANN is approximately characterized by the composition of a linear transformation series. Then the CAP of an ANN component is reduced to the contribution of its corresponding Koopman operator to the whole transformation. Similar to the partial derivatives of BP algorithm, we leverage the concepts of Jacobian ma trix/determinant to derive and deﬁne a metric with algebraic interpretation and further to assign the credit to each network component subsequently. Main contributions of this paper are listed as follows: • Migrating traditional CAP to trained neural networks and regarding the input/output of an ANN (or, component) as the evolution series of a dynamical system, we utilize stepdelay embedding to obtain a more precise represen tation of the component function and further a more ac curate linear approximation with Koopman operator the ory. • To deal with the dimensiongap between the input and output layers of a component, encountered in the step delay embedding, we present a minimal linear dimension alignment approach via composing and decomposing a linear network layer onto a network component, which provides a novel insight into the dimension difference. • Resorting to linear transformation analysis, we derive the sensitivity of a network component to the whole network capability with Jacobian matrices and the corresponding determinants, and deﬁne a credit metric with compre hensible algebraic explanation for assigning the learning credits to network components. The remainder of this paper is organized as follows. We ﬁrst brieﬂy introduce related preliminaries. Then we give the technique details for solving CAP on trained ANNs,including block partition, minimal linear dimension align ment, Koopman approximation and credit assignment. Sub sequently, we apply the presented approach on some typical neural networks to justify its effectiveness with experimen tal demonstration. We summarize this paper and discuss the possible future work at the end. II Preliminaries In this paper, we let Rm;nbe the space consists of all mby nreal matrices, and let Rkbe the space constituted by real vectors with length k. Given two matrices A= (ai;j)im;jn2Rm;nandB2 Rk;`, then the Kronecker product ofAandB, denoted as A B, is the matrix (ai;jB)im;jn2Rmk;n`. Let Vec(A)be the vector (a1;1;:::;a 1;n;a2;1;:::;a 2;n;:::;a m;1;:::;a m;n)T: We in what follows use Vec"
200,Scalar Invariant Networks with Zero Bias.txt,"Just like weights, bias terms are the learnable parameters of many popular
machine learning models, including neural networks. Biases are thought to
enhance the representational power of neural networks, enabling them to solve a
variety of tasks in computer vision. However, we argue that biases can be
disregarded for some image-related tasks such as image classification, by
considering the intrinsic distribution of images in the input space and desired
model properties from first principles. Our findings suggest that zero-bias
neural networks can perform comparably to biased networks for practical image
classification tasks. We demonstrate that zero-bias neural networks possess a
valuable property called scalar (multiplication) invariance. This means that
the prediction of the network remains unchanged when the contrast of the input
image is altered. We extend scalar invariance to more general cases, enabling
formal verification of certain convex regions of the input space. Additionally,
we prove that zero-bias neural networks are fair in predicting the zero image.
Unlike state-of-the-art models that may exhibit bias toward certain labels,
zero-bias networks have uniform belief in all labels. We believe dropping bias
terms can be considered as a geometric prior in designing neural network
architecture for image classification, which shares the spirit of adapting
convolutions as the transnational invariance prior. The robustness and fairness
advantages of zero-bias neural networks may also indicate a promising path
towards trustworthy and ethical AI.","Using bias terms in neural networks is a common practice. Its theoretical foundation goes back to the invention of artificial neural networks, which are loosely inspired by biological neurons. Biological neurons have some thresholds to determine whether they should ""fire” (produce an output that goes to other neurons) [ 27,50,19]. These thresholds are essentially the same thing as bias terms. From the representation learning perspective, the bias term is widely believed to increase the representational power of neural networks and thus is always needed when designing neural networks to solve a broad array of tasks in computer vision [ 46,39,2]. In this work, we challenge the commonlyheld beliefs of the necessity of including bias terms in neural networks to solve image classification tasks. Our geometric observations suggest the intrinsic distribution of images should incorporate directionality , as suggested in Figure 1. With this property holding, bias terms should not Preprint. Under review.arXiv:2211.08486v4  [cs.CV]  29 May 2023(a) Image classes can be thought of as manifolds in ndimensional input space. (b) Image classes can be concep tualized as cornshaped if consid ering the varying contrast. (c) An image specifes a direction in input space, and copies of that image with vary ing contrast lie along the same direction. Figure 1: The directionality (varying contrast) manifests in the intrinsic distribution of images. affect models’ representational power and performance, even for large modern CNN models such as ResNets [ 20]. Indeed, several recent works like SphereFace [ 31] and SphereNet [ 32] achieve strong performance in realworld tasks by ignoring the bias term and designing angularinspired losses. Moreover, Hesse et al. [ 22] report that removing bias terms only has a minor impact on predictive accuracy. Our thorough experimental results also support this argument. In addition, we show that neural networks will possess an intriguing property  scalar (multiplication) invariance after dropping bias terms. We then extend scalar invariance to CNNs as well as ResNets. This property allows zerobias networks to perfectly generalize to inputs with different levels of contrast without any data augmentation, which normal neural networks (with biases) usually fail to do so. Based on the scalar invariance property, we further derive more general robustness guarantees that could verify even certain convex regions of input space. In contrast, normal neural networks are highly combinatorial in nature, making such guarantees hardly exist. We also discover that scalar invariant neural networks exhibit complete fairness when predicting zero images. Considering the current issue of bias in AI systems towards certain genders and races [ 38,30], we believe that scalar invariant neural networks provide a promising solution to eliminate such biases and move towards ethical AI. We summarize our contributions as follows: (1) We show that the basic building blocks of neural networks are scalar multiplication associative if the bias is ignored. This, in turn, assures the scalar invariant property of convolutional neural networks. By adapting batch normalizationfree methods, we can extend scalar invariance to ResNets. We also conduct experiments on a few popular image classification benchmarks to validate the scalar invariant property; (2) Based on the scalar invariant property, we propose two additional robustness properties that verify inputs along certain lines (interpolations) and convex regions of the input space. Empirical validation of the interpolation robustness guarantee is done using image examples from benchmarks such as MNIST and CIFAR10; (3) We show that scalar invariant neural networks1share the same inductive bias as humans, which is a uniform belief in all labels when observing the zero/black images. On the other hand, stateof theart models tend to have a strong preference for specific labels; (4) We demonstrate through both empirical results and the theoretical tool known as Neural Tangent Kernel [ 26] that zerobias neural networks and normal neural networks exhibit nearly identical training dynamics; (5) Our geometric observations suggest the intrinsic distribution of images should incorporate directionality . Under this property, scalar invariant neural networks should have the same representational power as normal neural networks, thus delivering comparable performances. 1We use terms scalar invariant, zerobias, without bias interchangeably to describe the same variant of neural network. 22 Scalar invariant neural networks 2.1 Preliminary A neural network consists of an input layer, hidden layers, and an output layer. For convolutional neural networks, some of the hidden layers are called convolution layers which perform convolution operations on their input tensors with convolution kernels. The outputted tensors are passed to an activation function, commonly ReLU , before downsampling through pooling layers. After that, the input tensor is flattened out so that a fully connected network can process it and calculate the final prediction. For classification tasks, the final prediction is represented by a probability distribution over all classes using some activation functions such as Softmax . To further investigate the scalar invariant property, we formally denote the input tensor as Xand a convolutional neural network as N. Then Nis composed of convolutional layers Fi, pooling layers Pi, and fully connected layers Lj, where i, j∈N. And we denote the final activation function as AandReLU asR. We think of layers and activation functions as transformations on the input X, then the output of the network before the final activation function Ais represented by: O(X) =Lj◦ R ◦ ...◦ R ◦ L 1| {z } j Linear layers◦Pi◦ R ◦ F i...◦ P1◦ R ◦ F 1| {z } i Convolutional layers◦X (1) And the final prediction class is determined by the one with the highest probability over all classes C, that is: N(X) = argmax c∈C{A ◦ O (X)} (2) 2.2 Scalar associative transformations We consider the operation inside a convolution layer Fwith a kernel K, it is easy to show the associative property with scalar multiplication hold for convolution operations. More formally, let s be a positive scalar s.t. s∈R+, then we have: F ◦(sX) =X mX nsX(i+m, j+n)K(m, n) =sX mX nX(i+m, j+n)K(m, n) =s(F ◦X) (3) In addition, the above property also holds for pooling layers P, including max pooling and average pooling. Since both the max and average operation should preserve the scalar multiplication. The same argument also applies to the ReLU function. So we have: P ◦(sX) =s(P ◦X)andR ◦(sX) =s(R ◦X) (4) Finally, passing the input Xto a fully connected layer Lcan be thought of as applying a linear transformation ( W,B) onX. If we set the bias term Bto0. We will have the scalar associative property. That is: L ◦(sX) = (sX)WT=sXWT=s(L ◦X) (5) Note our proofs also use the commutative property which generally holds for matrix and vector multi plications with a scalar. Put together, by setting biases to zeros, we have the scalar (multiplication) associative property holds for the output function, i.e., ( O(sX) =sO(X)). 2.3 Scalar invariant convolutional neural networks Now we consider how to calculate the final prediction of the network N. For classification tasks, the last activation function Ais usually Softmax . If we multiply the input Xwith a scalar s(s∈R+) and pass the product to Softmax , it is equivalent to changing the temperature of the distribution. Note that the rank of candidate classes remains the same despite the change in the shape of the distribution. Or in other words, the predicted class by the network Nis scalar (multiplication) invariant: argmax cesO(X)c X c∈CesO(X)c= argmax ceO(X)c X c∈CeO(X)c(6) 3Put together with the scalar associative property of the output function O(·), we have a scalar invariant neural network: N(sX) = argmax c{A ◦ O (sX)}= argmax c{A ◦ O (X)}=N(X)(7) The concept of scalar invariant neural networks generalizes beyond just convolutional neural networks. In fact, as long as hidden layers perform scalar associative (and commutative) transformations and the last activation function preserves the highest probable candidate under scalar multiplication, the neural network will be scalar invariant. Since an image input Xrepresents a direction in the input space and we have proved that zerobias neural networks could yield the same prediction along that direction, we could restate this property as directional robustness property. Lemma 1 (Directional robustness property) For any input Xto a zerobias neural network N, the prediction remains the same when Xis multiplied by any positive scalar s. Formally, we have N(sX) =N(X)∀s∈R+. 2.4 Scalar invariant ResNet We briefly discussed the most simple architecture of convolutional neural networks in the previous section. However, in addition to those basic layers we mention before, modern powerful CNNs also employ extra layers and techniques to address overfitting and gradient exploding/vanishing issues. For example, ResNet [ 20] adopts Dropout [43],Additive Skip Connection [20] and Batch Normalization [25] which contributes enormously to its success. First, as dropout layers are disabled during the inference phase, it has no impact on the scalar invariant property. Second, it is trivial to show skip connection is also scalar multiplication associative if the corresponding residual branch G is also scalar multiplication associative. sX+G(sX) =s(X+G(X))∀s∈R+(8) Lastly, we consider Batch Normalization, which is performed through a normalization transformation that fixes the means and variances of inputs to each layer. Let us use XBto denote a minibatch of the entire training set. Then we have the batch normalization transformation as follows: BN(XB) =γˆXB+β (9) where γandβare learnable parameters, and ˆXBis the normalized input, represented by ˆXB= XB−µBq (σB)2+ϵ,ϵis an arbitrarily small constant. Clearly, we observe that the scalar associative/invariant property doesn’t hold for the normalization step, because: γ(sX) +β=γ(sX)−µBq (σB)2+ϵ+β̸=s(γX+β) (10) Thus, in order to achieve scalar invariance, we can adopt two approaches. Firstly, for small neural networks that do not have severe gradient explosion/vanishing issues, we can drop BN layers. Secondly, for larger neural networks, we can consider some alternatives to batch normalization. There exists a line of work on exploring efficient residual learning without normalization such as Instance Normalization [ 45], Fixup [ 53],XDNNs [ 22], and NFNets [ 5,6]. The majority of these approaches can be easily adapted to achieve scalar invariance, further information can be found in Appendix B. 2.5 Scalar invariance evaluation In this section, we conduct a series of experiments to verify the scalar invariance property of zerobias neural networks and their normally trained counterparts. We train both types of neural networks using the same configuration, except for the option of using bias, on several popular image classification benchmarks. More training details can be founded in Appendix C. We further demonstrate the effect of scalar invariance by evaluating their accuracy on test sets multiplied by different scalars, ranging from 1to0.0001 . The results, which are presented in Table 1, suggest that zerobias networks and normal networks achieve similar accuracies when the scalar is set to 1. However, when the contrast/scalar multiplier of the input image decreases, normal networks show a lack of robustness 4Table 1: As expected, zerobias neural networks achieve perfect scalar invariance, while normal neural networks are generally not robust against decreasing the contrast of the input image. Results are replicated thrice and averaged to reduce stochasticity effects, with all variances being below 0.5. Scalar multiplier 1 0.25 0.15 0.125 0.1 0.075 0.05 0.025 0.01 0.001 0.0001 MNIST FCNw/ bias 88.12 87.07 84.46 82.57 79.52 74.76 65.82 42.84 16.34 10.28 10.28 w/o bias 88.27 88.27 88.27 88.27 88.27 88.27 88.27 88.27 88.27 88.27 88.27 FashionMNIST CNNw/ bias 89.10 67.10 40.12 32.52 24.16 17.91 12.46 10.12 10.00 10.00 10.00 w/o bias 89.02 89.02 89.02 89.02 89.02 89.02 89.02 89.02 89.02 89.02 89.02 CIFAR100 ResNet18w/ bias 67.62 19.86 8.20 6.11 4.16 2.58 1.69 1.06 1.01 1.01 1.01 w/o bias 67.33 67.33 67.33 67.33 67.33 67.33 67.33 67.33 67.33 67.33 67.33 ImageNet [11] ResNet50w/ bias 75.37 66.72 57.84 53.62 47.27 37.61 21.81 3.39 0.21 0.10 0.10 w/o bias 73.82 73.82 73.82 73.82 73.82 73.82 73.82 73.82 73.82 73.82 73.82 (a) Coreshape unbounded regions of zerobias neural networks are formed by hyperplanes that pass through the origin. (b) Any point lying on the line between x1andx2 should yield the same pre diction as x1andx2. (c) Any point lying inside the convex region formed by in putsxishould yield the same prediction as xi. Figure 2: Zerobias neural networks exhibit unique robustness properties when inputs share the same neural activation pattern and are predicted identically by the neural network. as their accuracy declines at varying rates. In contrast, zerobias networks achieve scalar invariance as expected, and their performance remains unchanged regardless of the varying contrast of input images. We also train both types of models using augmented training sets that involve multiplication of the scalars used in test evaluation. We find that withbias models trained on augmented data still perform poorly when the scalar multiplier is extremely small, such as 0.001and0.0001 . For larger scalar ranges from 0.25to0.01, withbias models are merely comparable to zerobias models. These results demonstrate a significant advantage of zerobias networks in terms of data efficiency. 3 Interesting robustness properties Despite achieving remarkable success in a wide range of tasks, neural networks have been proven not robust under even small perturbations to the input [ 9,1], which accelerates the study of neural network verification and attacks. We find that zerobias networks exhibit some interesting robustness guarantees that are rarely identified in common neural networks. Given that these guarantees of robustness are closely tied to specific regions within the input space, it is pertinent to explore how zerobias neural networks divide the geometry of the input space. When the bias terms are eliminated, the hyperplanes defined by each neuron will originate from the origin. When these hyperplane arrange together, they create multiple coreshaped unbounded regions that differ from the typical convex regions formed by normal neural networks, as illustrated in Figure 2a. To better illustrate the interesting robustness properties of zerobias networks, we first introduce the notion of neural activation patterns [15]. Definition 1 (Neural Activation Pattern) ANeural Activation Pattern (NAP) of a neural network Nis a tuple Px:= (A, D), where AandDare collections of all activated and deactivated neurons respectively when passing xthrough N. Theorem 2 (Interpolation robustness property) For any two inputs X1andX2that have the same prediction and neural activation pattern by network N, i.e.,N(X1) =N(X2)andPX1=PX2, their 5linear interpolation also yield the same prediction, that is, N(λX1+(1−λ)X2) =N(X1) =N(X2), where λ∈[0,1]. Assuming that two points share the same prediction and neural activation pattern, it can be proven that their interpolation will also share the same prediction and neural activation pattern. Please refer to Appendix A for detailed proof. What’s even more interesting is that this property can be extended to the multiple inputs setting, where a convex region can provide robustness assurance. Theorem 3 (Convex region robustness property) Let{Xi|i∈ {1,2, . . . , n }}be a collection of inputs that have the same prediction and neural activation pattern by network N, we denote the convex polygon formed by vertices XiasM. Then, for any point mthat lies inside the polygon M, malso yield the same prediction as Xi, that is, N(m) =N(Xi)∀m∈ M ∀ i∈ {1,2, ..., n}. Asmcan always be represented by some linear combination of vertices {Xi}, the convex region robustness property holds as the direct result of Theorem 2. In contrast, such guarantees hardly exist on normal neural networks due to their highly combinatorial nature. Furthermore, recent research has shown that ignoring bias can enhance the robustness of models, as demonstrated in [13, 12]. To test the interpolation robustness property, we conducted experiments using visual examples sourced from MNIST and CIFAR10. Following neural network training, we search for image pairs that shared the same prediction and neural activation pattern. We then interpolate 1000 images between each pair and confirm that each interpolation yields the same prediction, as expected. Figure 3 presents some examples of our findings. = 0.00 pred=0  = 0.20 pred=0  = 0.40 pred=0  = 0.50 pred=0  = 0.60 pred=0  = 0.80 pred=0  = 1.00 pred=0 = 0.00 pred=8  = 0.20 pred=8  = 0.40 pred=8  = 0.50 pred=8  = 0.60 pred=8  = 0.80 pred=8  = 1.00 pred=8 = 0.00 pred=dog  = 0.20 pred=dog  = 0.40 pred=dog  = 0.50 pred=dog  = 0.60 pred=dog  = 0.80 pred=dog  = 1.00 pred=dog = 0.00 pred=truck  = 0.20 pred=truck  = 0.40 pred=truck  = 0.50 pred=truck  = 0.60 pred=truck  = 0.80 pred=truck  = 1.00 pred=truck Figure 3: The leftmost and rightmost images are from the original MNIST (the first two rows) and CIFAR10 (the last two rows), whereas synthesized/interpolated images are in the middle. We find that any interpolated images will yield the same prediction as a result of the interpolation robustness property ( Theorem 2). Please refer to Appendix D for additional examples. However, it is important to note that such robustness guarantees are rarely identified in larger and more accurate neural networks. For example, with the small neural networks used in our experiments comprising only 30 neurons and achieving accuracies of 32.27% and 29.6% on MNIST and CIFAR 10, respectively, we can easily identify many qualified pairs. In larger networks with accuracies of around 80%, we still find a few qualified pairs. However, in even larger networks with accuracies of over 90%, we are unable to find any examples of the interpolation robustness property. This is due to the fact that as the number of neurons grows, the input space becomes more scattered, reducing the 60 200 400 600 800 1000 Class0.00000.00250.00500.00750.01000.01250.0150ProbabiliyResNet34 ResNet52 ResNet101 ResNet152 W/o bias nets(a) The predicted probability of ResNets on the zero image. 0 200 400 600 800 1000 Class0.00050.00100.00150.0020ProbabiliyEfficientNetB0 W/o bias nets(b) The predicted probability of Ef ficientNet on the zero image. 0 200 400 600 800 1000 Class0.00000.02000.04000.0600ProbabiliyViT W/o bias nets(c) The predicted probability of ViT on the zero image. Figure 4: SOTA models are biased towards to certain classes when predicting the zero image, whereas scalar invariant neural networks are unbiased, i.e, having a uniform belief in all classes like humans. likelihood of two or more inputs sharing the same neural activation pattern. While this seems like a new No Free Lunch Theorem in terms of the tradeoff between interpolation robustness and accuracy, we believe there are methods to improve model accuracy while still maintaining these robustness guarantees. For example, we could design new training objectives to control the diminishing margin between hyperplanes of networks. We leave this as a direction for future work. 4 Fairness on the zero image This section investigates the fairness of models when predicting the zero image, which refers to an image where all pixel values are set to zero. From a human perspective, the zero image contains no discernible information, resulting in maximum information entropy. As such, it is equally likely for the zero image to belong to any class, meaning that it follows a uniform distribution. It is easy to demonstrate that scalar invariant neural networks possess the same inductive bias as humans, since: N(0) = argmax c{A ◦ O (0)}= argmax ce0 X c∈Ce0= argmax c1 |C| (11) Nevertheless, this may not hold for normal neural networks with bias terms, even those that are considered stateoftheart models, as they may exhibit bias towards certain classes. Figure 4 presents selective results on models’ bias/fairness when predicting the zero image. It is noteworthy that all selected models display some level of bias. Specifically, among the three stateoftheart models, EfficientNet [44] shows less bias, while ViT [14] is heavily biased towards certain classes. However, this is a significant concern when applying AI to realworld applications such as gender classification, as current AI systems have shown to have problems with bias, including gender and racial bias [ 42,8]. We believe that this issue is deeply rooted in the inductive bias of normal neural networks and may not be easily addressed by using augmented data or changing training objectives. In contrast, zerobias neural networks exhibit a uniform belief in all potential classes, which cannot be altered even by training with imbalanced data, as it serves as an inductive bias of the model. While our study focuses on the fairness of zerobias neural networks in the context of the zero image, we believe that this fairness property has the potential to be extended to other scenarios. For instance, it may help address gender or racial bias, providing a promising path towards achieving ethical AI. We intend to explore this direction in our future work. 5 Training dynamics and expressiveness 5.1 Training dynamics In this section, we demonstrate that removing the bias terms does not affect the training dynamics of neural networks. We begin by demonstrating our observation through the use of a theoretical tool known as the Neural Tangent Kernel (NTK). The NTK is a kernel that explains how neural networks evolve during training through gradient descent [ 26]. It provides valuable insights into why sufficiently wide neural networks can converge to a global minimum when trained to minimize an empirical loss. We extend the two key results from the original paper to zerobias cases, as follows: 70 5 10 15 20 Epoch0.51.01.52.0Lossw/o bias train loss w/ bias train loss w/o bias test loss w/ bias test loss(a) Loss curves of FCNs on MNIST. 0 5 10 15 20 Epoch0.40.60.81.0Lossw/o bias train loss w/ bias train loss w/o bias test loss w/ bias test loss(b) Loss curves of CNNs on FashionMNIST. 0 10 20 30 40 Epoch0.51.01.52.0Lossw/o bias train loss w/ bias train loss w/o bias test loss w/ bias test loss(c) Loss curves of ResNet9 on CIFAR10. 0 10 20 30 40 Epoch0.01.02.03.04.0Lossw/o bias train loss w/ bias train loss w/o bias test loss w/ bias test loss(d) Loss curves of ResNet18 on CIFAR100. Figure 5: Loss curves of normally trained neural networks, and their scalar invariant counterparts are almost identical, which supports our argument that removing bias doesn’t impact the training dynamics and generalization of models on image classification tasks. Remark 1 When the width of networks goes to infinite, both NTKs of zerobias neural networks and normal neural networks converge in probability to the same deterministic limit. Remark 2 In the infinitewidth limit, both NTKs of zerobias neural networks and normal neural networks stay asymptotically the same constant during training. To summarize, from the perspective of NTK formulation, the bias terms do not have an impact on the training dynamics of models. We provide further details for supporting these two remarks in the Appendix. E.5. While successful neural networks are not typically operated in the kernel regime, the Neural Tangent Kernel provides valuable insights that bias terms do not play a key role in the training dynamics of neural networks. In addition, we empirically evaluate and compare both zerobias networks and their normally trained counterparts on some popular image classification benchmarks, as shown in Figure 5. Highly overlapped training loss curves indicate that both types of models have almost identical training dynamics, which aligns with our analysis results using NTK. More surprisingly, the two different types of models also exhibit very similar generalization capabilities on unseen datasets. We also aim to dive deeper into the generalization behavior of zerobias networks in future work. 5.2 Geometric insights on expressiveness (a) Dir1 and Dir2 stay within unbounded regions of the zerobias network. (b) Dir1 and Dir2 traverse multiple convex regions of the withbias network. +  + + + Dir10.465 0.372 0.279 0.186 0.093 0.0000.0930.1860.2790.3720.465Logitwith_bias model zero_bias model(c) Normal networks can fit data in Dir1 whereas zero bias networks fail to do so. Dir20.465 0.372 0.279 0.186 0.093 0.0000.0930.1860.2790.3720.465Logitwith_bias model zero_bias model(d) Both normal and zero bias networks can fit data in Dir2 (directionality holds). Figure 6: The first two subfigures show Dir1 and Dir2 on the input space for both zerobias networks and normal networks, respectively. The latter two subfigures represent the prediction logit function of zerobias networks and normal networks along Dir1 and Dir2, respectively. It is a widely held belief that eliminating bias from neural networks can diminish their representational power, ultimately affecting the accuracy of models. For instance, Xu et al. [ 48] show that neural networks linearize outside of their training regime once omitting the biases. To this end, we provide geometric insights to show that zerobias networks are comparable with normal networks in expressive capabilities when solving image classification tasks. Since a neural network can be thought of as a piecewise (linear) function defined over many convex polytopes [ 17,18], we plot linear regions of a simple 3layer neural network and its zerobias 8counterpart on a simple 2D input space to study their representational power in Figure 6. Our aim is to illustrate how these networks perform on two simple binary classification tasks characterized by Dir (Direction) 1 and Dir2. In Dir1, points along the same direction are not labelled identically, whereas points along Dir2 are assigned to the same class, i.e., satisfying directionality . Note that in this study, we say a model can fit a specific point if its prediction logit function (before Sigmoid ) is negative for •and positive for +. While zerobias networks have a more limited expressive capacity compared to normal networks, being restricted to linear functions originating from the origin, they can effortlessly fit Dir2, as shown in Figure 6d. This is due to the fact that all points in the predicted logit fall below 0. However, in the absence of directionality  Dir1, shown in Figure 6c, the points in the predicted logit scatter across the 0 line (as they belong to different classes). A linear function starting from the origin could never fit this case. On the other hand, normal networks, with their highly expressive piecewise functions, can fit both Dir1 and Dir2. In conclusion, our observations indicate that both types of neural networks achieve similar accuracies in image classification tasks (because of directionality), which is consistent with our experimental findings in Section 2.5. Moreover, we propose that directionality can serve as a powerful geometric prior in image classification, akin to the translational invariance prior employed in CNNs. 6 Related work "
422,Dynamics of Deep Neural Networks and Neural Tangent Hierarchy.txt,"The evolution of a deep neural network trained by the gradient descent can be
described by its neural tangent kernel (NTK) as introduced in [20], where it
was proven that in the infinite width limit the NTK converges to an explicit
limiting kernel and it stays constant during training. The NTK was also
implicit in some other recent papers [6,13,14]. In the overparametrization
regime, a fully-trained deep neural network is indeed equivalent to the kernel
regression predictor using the limiting NTK. And the gradient descent achieves
zero training loss for a deep overparameterized neural network. However, it was
observed in [5] that there is a performance gap between the kernel regression
using the limiting NTK and the deep neural networks. This performance gap is
likely to originate from the change of the NTK along training due to the finite
width effect. The change of the NTK along the training is central to describe
the generalization features of deep neural networks.
  In the current paper, we study the dynamic of the NTK for finite width deep
fully-connected neural networks. We derive an infinite hierarchy of ordinary
differential equations, the neural tangent hierarchy (NTH) which captures the
gradient descent dynamic of the deep neural network. Moreover, under certain
conditions on the neural network width and the data set dimension, we prove
that the truncated hierarchy of NTH approximates the dynamic of the NTK up to
arbitrary precision. This description makes it possible to directly study the
change of the NTK for deep neural networks, and sheds light on the observation
that deep neural networks outperform kernel regressions using the corresponding
limiting NTK.","Deep neural networks have become popular due to their unpreced ented success in a variety of machine learning tasks. Image recognition [ 25,26,42], speech recognition [ 19,34], playing Go [ 35,36] and natural language understanding [ 10,12,44] are just a few of the recent achievements. However, one aspec t of deep The work of H.T. Y. is partially supported by NSF Grants DMS 1606305 and DMS1855509, and a Simons Investigator award. 1neural networks that is not well understood is training. Training a d eep neural network is usually done via a gradient decent based algorithm. Analyzing such training dynam ics is challenging. Firstly, as highly nonlinear structures, deep neural networks usually involve a large number of parameters. Secondly, as highly nonconvex optimization problems, there is no guarantee that a gr adient based algorithm will be able to ﬁnd the optimal parameters eﬃciently during the training of neural net works.One question then arises: given such complexities, is it possible to obtain a succinct descr iption of the training dynamics? Inthispaper, wefocusontheempiricalriskminimizationproblemwith t hequadraticlossfunction min θL(θ) =1 2nn/summationdisplay α=1(f(xα,θ)−yα)2, where{xα}n α=1are the training inputs, {yα}n α=1are the labels, and the dependence is modeled by a deep fullyconnected feedforward neural network with Hhidden layers. The network has dinput nodes, and the input vector is given by x∈Rd. For 1 /lessorequalslantℓ/lessorequalslantH, theℓth hidden layer has mneurons. Let x(ℓ)be the output of the ℓth layer with x(0)=x. Then the feedforward neural network is given by the set of recu rsive equations: x(ℓ)=1√mσ(W(ℓ)x(ℓ−1)), ℓ= 1,2,···,H, (1.1) whereW(ℓ)∈Rm×difℓ= 1 and W(ℓ)∈Rm×mif 2/lessorequalslantℓ/lessorequalslantHare the weight matrices, and σis the activation unit, which is applied coordinatewise to its input. The output of the n eural network is f(x,θ) =a⊤x(H)∈R, (1.2) wherea∈Rmis the weightmatrix for the output layer. We denote the vector con tainingall trainableparam eters by θ= (vec(W(1)),vec(W(2))...,vec(W(H)),a). We remark that this parametrization is nonstandard because of those 1 /√mfactors. However, it has already been adopted in several recent works [13,14,20,27]. We note that the predictions and training dynamics of ( 1.1) are identical to those of standard networks, up to a scaling factor 1 /√min the learning rate for each parameter. We initialize the neural networkwith randomGaussian weightsfollowing the Xavierinitialization scheme [18]. More precisely, we set the initial parameter vector θ0asW(ℓ) ij∼ N(0,σ2 w),ai∼ N(0,σ2 a). In this way, for the randomly initialized neural network, we have that the L2norms of the output of each layer are of order one, i.e. /ba∇dblx(ℓ)/ba∇dbl2 2= O(1) for 0 /lessorequalslantℓ/lessorequalslantH, andf(x,θ0) = O(1) with high probability. In this paper, we train all layers of the neural network with continuous time gradient descent (gradient ﬂow): for any time t/greaterorequalslant0 ∂tW(ℓ) t=−∂W(ℓ)L(θt), ℓ= 1,2,···,H, ∂ tat=−∂aL(θt), (1.3) whereθt= (vec(W(1) t),vec(W(2) t)...,vec(W(H) t),at). For simplicity of notations, we write σ(W(ℓ)x(ℓ−1)) asσℓ(x), or simply σℓif the context is clear. We write its derivative diag( σ′(W(ℓ)x(ℓ−1))) asσ′ ℓ(x) =σ(1) ℓ(x), andrth derivative diag( σ(r)(W(ℓ)x(ℓ−1))) as σ(r) ℓ(x), orσ(r) ℓforr/greaterorequalslant1. In this notation, σ(r) ℓ(x) are diagonal matrices. With those notations, explicitly, 2the continuous time gradient descent dynamic ( 1.3) is ∂tW(ℓ) t=−∂W(ℓ)L(θt) =−1 nn/summationdisplay β=1/parenleftBigg σ′ ℓ(xβ)(W(ℓ+1) t)⊤ √m···σ′ H(xβ)at√m/parenrightBigg ⊗(x(ℓ−1) β)⊤(f(xβ,θt)−yβ),(1.4) forℓ= 1,2,···,H, and ∂tat=−∂aL(θt) =−1 nn/summationdisplay β=1x(H) β(f(xβ,θt)−yβ). (1.5) 1.1 Neural Tangent Kernel A recent paper [ 20] introduced the Neural Tangent Kernel (NTK) and proved the limit ing NTK captures the behavioroffullyconnecteddeepneuralnetworksintheinﬁnitewid thlimittrainedbygradientdescent: ∂tf(x,θt) =∂θf(x,θt)∂tθt=−∂θf(x,θt)∂θL(θt) =−1 n∂θf(x,θt)n/summationdisplay β=1∂θf(xβ,θt)(f(xβ,θt)−yβ) =−1 nn/summationdisplay β=1K(2) t(x,xβ)(f(xβ,θt)−yβ),(1.6) where the NTK K(2) t(·,·) is given by K(2) t(xα,xβ) =/an}b∇acketle{t∂θf(xα,θt),∂θf(xβ,θt)/an}b∇acket∇i}ht=H+1/summationdisplay ℓ=1G(ℓ) t(xα,xβ) (1.7) and for 1 /lessorequalslantℓ/lessorequalslantH, G(ℓ) t(xα,xβ) =/an}b∇acketle{t∂W(ℓ)f(xα,θt),∂W(ℓ)f(xβ,θt)/an}b∇acket∇i}ht =/angbracketleftBigg σ′ ℓ(xα)(W(ℓ+1) t)⊤ √m···σ′ H(xα)at√m,σ′ ℓ(xβ)(W(ℓ+1) t)⊤ √m···σ′ H(xβ)at√m/angbracketrightBigg /an}b∇acketle{tx(ℓ−1) α,x(ℓ−1) β/an}b∇acket∇i}ht and G(H+1) t=/an}b∇acketle{t∂af(xα,θt),∂af(xβ,θt)/an}b∇acket∇i}ht=/an}b∇acketle{tx(H) α,x(H) β/an}b∇acket∇i}ht. The NTK K(2) t(·,·) varies along training. However, in the inﬁnite width limit, the training d ynamic is very simple: The NTK does not change along training, K(2) t(·,·) =K(2) ∞(·,·). The network function f(x,θt) follows a linear diﬀerential equation [ 20]: ∂tf(x,θt) =−1 nn/summationdisplay β=1K(2) ∞(x,xβ)(f(xβ,θt)−yβ), (1.8) 3which becomes analytically tractable. In other words, the training d ynamic is equivalent to the kernel regression using the limiting NTK K(2) ∞(·,·). While the linearization ( 1.8) is only exact in the inﬁnite width limit, for a suﬃciently wide deep neural network, ( 1.8) still provides a good approximation of the learning dynamic for the corresponding deep neural network [ 13,14,27]. As a consequence, it was proven in [ 13,14] that, for a fullyconnected wide neural network with m/greaterorsimilarn4under certain assumptions on the data set, the gradient descent converges to zero training loss at a linear rate. A lthough highly overparametrized neural networks is equivalent to the kernel regression, it is possible to sho w that the class of ﬁnite width neural networksis more expressivethan the limiting NTK. It has been const ructed in [ 1,17,46] that there are simple functions that can be eﬃciently learnt by ﬁnite width neural networ ks, but not the kernel regression using the limiting NTK. 1.2 Contribution There is a performance gap between the kernel regression ( 1.8) using the limiting NTK and the deep neural networks. It wasobservedin [ 5] that the convolutionalneural networksoutperformtheir corr espondinglimit ing NTK by 5%  6%. This performance gap is likely to originate from the c hange of the NTK along training due to the ﬁnite width eﬀect. The change of the NTK alongtraining ha s its beneﬁts on generalization. In the current paper, we study the dynamic of the NTK for ﬁnite wid th deep fullyconnected neural networks. Here we summarize our main contributions: •We show the gradient descent dynamic is captured by an inﬁnite hiera rchy of ordinary diﬀerential equations, the neural tangent hierarchy (NTH). Diﬀerent from t he limiting NTK ( 1.7), which depends only on the neural network architecture, the NTH is data depende nt and capable of learning data dependent features. •We derive a priori estimates of the higher order kernels involved in th e NTH. Using these a priori estimates as input, we conﬁrm a numerical observation in [ 27] that the NTK varies at a rate of order O(1/m). As a corollary, this implies that for a fullyconnected wide neural n etwork with m/greaterorsimilarn3, the gradient descent converges to zero training loss at a linear rate, w hich improves the results in [ 13]. •The NTH is just an inﬁnite sequence of relationship. Without truncat ion, it cannot be used to de termine the dynamic of the NTK. Using the a priori estimates of the h igher order kernels as input, we construct a truncated hierarchy of ordinary diﬀerential equa tions, the truncated NTH. We show that this system of truncated equations approximates the dynam ic of the NTK to certain time up to arbitrary precision. This description makes it possible to directly stu dy the change of the NTK for deep neural networks. 1.3 Notations In the paper, we ﬁx a large constant p >0, which appears in Assumptions ( 2.1) and (2.2). We use c,Cto represent universal constants, which might be diﬀerent from line t o line. In the paper, we write a= O(b) ora/lessorsimilarbif there exists some large universal constant Csuch that |a|/lessorequalslantCb. We write a/greaterorsimilarbif there exists some small universal constant c>0 such that a/greaterorequalslantcb. We write a≍bif there exist universal constants c,C such that cb/lessorequalslant|a|/lessorequalslantCb. We reserve nfor the number of input samples and mfor the width of the neural network. For practical neural networks, we always have that m/lessorsimilarpoly(n) andn/lessorsimilarpoly(m). We denote 4the set of input samples as X={x1,x2,···,xn}. For simplicity of notations, we write the output of the neural network as fβ(t) =f(xβ,θt). We denote vector L2norm as /ba∇dbl · /ba∇dbl2, vector or function L∞norm as /ba∇dbl·/ba∇dbl∞, matrix spectral norm as /ba∇dbl·/ba∇dbl2→2, and matrix Frobenius norm as /ba∇dbl·/ba∇dblF. We say that an event holds with high probability, if it holds with probability at least 1 −e−mcfor some c>0. Then the intersection of poly(n,m) many high probability events is still a high probability event, provided mis large enough. In the paper, we treat cr,Crin Assumption 2.1and2.2, and the depth Has constants. We will not keep track of them. 1.4 Related Work "
354,SpeakerNet: 1D Depth-wise Separable Convolutional Network for Text-Independent Speaker Recognition and Verification.txt,"We propose SpeakerNet - a new neural architecture for speaker recognition and
speaker verification tasks. It is composed of residual blocks with 1D
depth-wise separable convolutions, batch-normalization, and ReLU layers. This
architecture uses x-vector based statistics pooling layer to map
variable-length utterances to a fixed-length embedding (q-vector). SpeakerNet-M
is a simple lightweight model with just 5M parameters. It doesn't use voice
activity detection (VAD) and achieves close to state-of-the-art performance
scoring an Equal Error Rate (EER) of 2.10% on the VoxCeleb1 cleaned and 2.29%
on the VoxCeleb1 trial files.","Speaker Recognition (SR) is a broad research area that solves two major tasks: speaker identiﬁcation (who is speaking) and speaker veriﬁcation (is the speaker whom they claim to be) [1]. In this work, we focus on the close, textindependent speaker recognition when the identity of the speaker is based on how speech is spoken, not necessarily in what is being said. Typically such SR systems operate on unconstrained speech utterances, which are converted to a vector of ﬁxed length, called speaker embeddings. Speaker embeddings are also used in automatic speech recognition (ASR) [2] and speech synthesis [3]. In this paper, we propose SpeakerNet – a new neural networkbased architecture for the speaker recognition do main. SpeakerNet consists of three major parts: Encoder, Pooling layer, and Decoder. The Encoder is based on the QuartzNet architecture developed for ASR [4]. It is composed of residual blocks, where each block consists of 1D depth wise separable convolutions, batch norm, ReLU, and dropout layers. The Encoder converts audio of variable length into a sequence of acoustic features, which can be used to extract top level features, E. The Pooling layer maps the temporal sequence of acoustic features into a vector of ﬁxed length by computing statistics on acoustic features. The Decoder, consisting of a series of fully connected layers, maps ﬁxed length vectors of dimension Dto a number of speakers Nto compute the probability that the current segment belongs to a speaker from the training set. This way the network ex tracts ﬁxedlength representation from variable length speech segments. The network was trained endtoend for speaker classiﬁcation using the V oxCeleb1 and V oxCeleb2 dev [5] datasets. The advantage of the proposed speaker embedding model is that it can be easily integrated with endtoend deep ASR models [4] since both models use the same architecture. The following are the main contributions of this paper: 1. A new SR model: SpeakerNet based on the QuartzNet architecture with xvector based pooling and without V AD 2. Investigation of tradeoff between training time, length of utterance, and Equal Error Rate 3. Lighter model than current stateoftheart (SOTA) model with similar performance The paper is organized as follows: In Section 2, we re view related work on Neural Network based speaker recogni tion. In Section 3, we describe the QuartzNet architecture for speaker identiﬁcation and the extraction of QuartzNet vectors (qvectors) for speaker embedding. In Section 4, we describe the training and evaluation methodology along with results on V oxCeleb1 trial ﬁles. Pretrained models and code are open sourced in NeMo, a conversational AI toolkit.1 2. RELATED WORK "
395,A Fuzzy System based Approach to Extend Network Lifetime for En-Route Filtering Schemes in WSNs.txt,"Wireless sensor networks suffer from false report injection attacks. This
results in energy drain over sensor nodes on the event traversal route. Novel
en-route filtering schemes counter this problem by filtering these attacks on
designated verification nodes. However, these filtering schemes among other
limitations inherently are network lifetime inefficient. Generally, report
traversal paths and verification nodes are also fixed. In this paper, we cater
these limitations in our proposed scheme. Simulation experiments results show
that proposed schemes outperforms existing en-route filtering schemes in
networks lifetime. We employed a Fuzzy Logic System to select forwarding nodes
from candidate nodes based on current network conditions. Proposed scheme gains
in network lifetime, and energy-efficiency while having comparable false report
filtering efficiency.","Wireless sensor networks (WSNs) have witnessed mushroom growth due to recent advances in  microelectromechanical systems (MEMS) [1]. En route filtering schemes [2 7] have demonstrated  several limitations; fix path routing, energy hole problem, and often underlying routing [8] is not  suitable for WSNs. Energy hole problem in static sink based WSNs is caused by fasted rate of  energy consump tion around based station and critical paths. This results in network partitioning  and deteriorated network lifetime.   In WSNs, false report injection attacks [9] are prevalent. An adversary with an intent to drain  the scarce of energy of sensor nodes gene rate false reports from compromised node(s) and forward  it to the base station. In order to counter these attacks, en route filtering schemes use filtering nodes  on the pre determined path(s). Since, path and filtering nodes are fixed, making it vulnerable . In  general, in these schemes, underlying routing is based on greedy or shorted path routing which  does not perform well with regard to network lifetime.   Yet, another limitation of en route filtering schemes is energy hole problem [10]. This problem  occu rs since in static sink based WSNs, the rate of energy consumption around sink and on critical                                                                1 MKS ( mkshahzad@ieee.org)  is with IEEE Seoul Section, LN (e mail: nkenyele@sejong.ac.kr)   and SMRI (e mail: riaz@sejong.ac.kr ) are with Sejong University, Seoul, South Korea.  paths is faster as compare of other nodes and paths. This results in network partition(s) and severely  effects network lifetime. In [11], a light weight routing was presented to safe energy in sensor  networks. In this work we use energy consumption model widely used [12]. Energy consumption  values are taken from [13] assuming that underlying sensing platforms  are Mica2 sensor nodes  [14].   In order to extend network  lifetime without compromising security, we propose fuzzy based  dynamic en route filtering. In this approach, since the paths are not fixed communication loads are  routed to several paths resulting in network lifetime extension. In the proposed scheme, dyn amic  path routing, there can exit several path since each time forwarding node is selected based on  current energy level of candidate nodes, in addition to message authentication codes (MACs) and  attacks frequency. In traditional schemes, attacks response is based on probabilistic method;  independent of attacks intensity, security response is fix. However, in real scenarios, attacks  frequency does vary with time. Therefore, a countermeasure which respond to current attack  frequency is proposed in this paper . Main contribution of proposed scheme against existing  schemes among other are:    Dynamic path routing .   Countermeasure against false report attacks based on current attacks frequency .   Fuzzy Logic System based forwarding node selecti on.   Avoiding energy hole problem .  2. METHODOLOGY AND MODELS   "
444,Modular Control Plane Verification via Temporal Invariants.txt,"Monolithic control plane verification cannot scale to hyperscale network
architectures with tens of thousands of nodes, heterogeneous network policies
and thousands of network changes a day. Instead, modular verification offers
improved scalability, reasoning over diverse behaviors, and robustness
following policy updates. We introduce Timepiece, a new modular control plane
verification system. While one class of verifiers, starting with Minesweeper,
were based on analysis of stable paths, we show that such models, when deployed
naively for modular verification, are unsound. To rectify the situation, we
adopt a routing model based around a logical notion of time and develop a
sound, expressive, and scalable verification engine.
  Our system requires that a user specifies interfaces between module
components. We develop methods for defining these interfaces using predicates
inspired by temporal logic, and show how to use those interfaces to verify a
range of network-wide properties such as reachability or access control.
Verifying a prefix-filtering policy using a non-modular verification engine
times out on an 80-node fattree network after 2 hours. However, Timepiece
verifies a 2,000-node fattree in 2.37 minutes on a 96-core virtual machine.
Modular verification of individual routers is embarrassingly parallel and
completes in seconds, which allows verification to scale beyond non-modular
engines, while still allowing the full power of SMT-based symbolic reasoning.","Major cloud providers are seeing sustained financial growth in response to mounting demand for reliable networking [Miller 2022]. This demand suggests a commensurate network infrastructure growth will take place to accommodate more and more users. These networks can already have hundreds of data centers, each with hundreds of thousands of devices running thousands of heterogeneous policies, and receiving thousands of updates a day [Jayaraman et al .2019b]. Network operators program this infrastructure using distributed routing protocols, where each router in a network may run thousands of lines of configuration code. Despite operators’ care, routine Authors’ addresses: Timothy Alberdingk Thijm, Princeton University, Princeton, NJ, United States, tthijm@cs.princeton.edu; Ryan Beckett, Microsoft Research, Redmond, WA, United States, ryan.beckett@microsoft.com; Aarti Gupta, Princeton University, Princeton, NJ, United States, aartig@cs.princeton.edu; David Walker, Princeton University, Princeton, NJ, United States, dpw@cs.princeton.edu. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). ©2023 Copyright held by the owner/author(s). 24751421/2023/6ART108 https://doi.org/10.1145/3591222 Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.arXiv:2204.10303v2  [cs.LO]  8 Apr 2023108:2 Timothy Alberdingk Thijm, Ryan Beckett, Aarti Gupta, and David Walker configuration updates have inadvertently rendered routers unreachable [Strickx and Hartman 2022] or violated isolation requirements that prevent flooding [Vigliarolo 2022]. To prevent costly errors, operators can use control plane verification to analyze their networks [Ab hashkumar et al .2020; Beckett et al .2017a, 2018, 2019; Fayaz et al .2016; GemberJacobson et al .2016; Lopes and Rybalchenko 2019; Prabhu et al .2020; Weitz et al .2016; Ye et al .2020]. Until recently, research has focused on monolithic verification of the entire network at once, which is infeasible for large cloud provider networks. Such networks demand modular techniques that divide the network into components to verify in isolation. This approach has proven successful for software verification [Alur and Henzinger 1999; Flanagan and Qadeer 2003; Giannakopoulou et al .2018; Grumberg and Long 1994; Henzinger et al .1998] and network data plane verification [Jayaraman et al.2019a]. We annotate the interfaces between network components with invariants that describe each component’s routing behavior. Given the interfaces of a component’s neighbors, we can verify that the component respects its own interface. When the interfaces imply a useful property, e.g., reachability or access control, we can conclude that the monolithic network satisfies that property. We propose Timepiece , the first modular technique with abstract network interfaces to verify a wide range of properties (including route reachability). Kirigami [Alberdingk Thijm et al .2022a] proposed an architecture for modular control plane verification, but restricted its interfaces to only exact routes. Lightyear [Tang et al .2022] presented an alternative verification technique with more expressive interfaces, but can only check that a network never receives a route ( e.g.,for access control properties) — it cannot check reachability, a keen property of interest. A temporal model. The basis of Timepiece ’s approach is a temporal model of network execution , where we reason over the states of nodes at all times . This model came as a surprise to us: one branch of prior work, starting with Minesweeper [Beckett et al .2017a], sought to avoid the burden of reasoning over all transient states of the network by focusing on the stable states of the routing protocol once routing converges. Unfortunately, a naïve combination of modular reasoning and Minesweeperstyle analysis of stable states is unsound . We discovered that the best way to recover soundness, while maintaining the system’s generality, is to move to a temporal model. 0 1,000 2,00002,0004,0006,000timeout Topology Size (Nodes)Verification time[s] Modular Monolithic Fig. 1. Verification time comparison between Timepiece and Minesweeperstyle verification.This temporal model appears to ask the verification engine to do a lot more work: the system must verify that all the messages produced at all times are consis tent with a usersupplied interface for each network component. Nevertheless, because reasoning is modu lar, ensuring individual problems are small, the system scales with the size of the largest component rather than the size of the network. This modular reasoning is general and any symbolic method ( e.g.,symbolic simulation, model checking) could use it to verify in dividual components. We use a Satisfiability Modulo Theories (SMT)based method in this work [Barrett and Tinelli 2018]. As a preview of modularity’s ben efits, Figure 1 shows the time it takes Timepiece to verify connectivity for variablesized fattree topolo gies [AlFares et al .2008] with external route announcements using the eBGP routing protocol, compared with a Minesweeperstyle networkwide stable paths encoding. Timepiece does require more work of users than monolithic, nonmodular systems: users must supply interfaces that characterize the routes each network component may generate at each time. Still, these interfaces, once constructed, provide the typical benefits of interfaces in any Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.Modular Control Plane Verification via Temporal Invariants 108:3 software engineering context. First, they localize exactly where an error occurs: if a component is not consistent with its interface, then one must search only that component for the mistake, and a counterexample from the SMT solver can help pinpoint it. Second, router configurations change rapidly, and these changes are often the source of networkwide problems [Zhang et al . 2022]. Welldefined interfaces will be stable over time. As users update their configurations, they may easily recheck them against the stable, local interface for problems. Inspired by temporal logic [Pnueli 1984], we developed a simple language to help users specify their interfaces. Through this language, users may state that they expect to see certain sets of routes always ,eventually (by some specified time 𝑡, to be more precise), or until (some approximate specified time). Moreover, the interface language allows users to write abstract specifications that need not characterize irrelevant features of routes, and instead only what is necessary to prove a desired property. For instance, a user might specify a reachability property simply by stating a node must “eventually receive some route at time 𝑡,” without saying which route it must receive. Our formal model is based on a synchronous semantics of time, where nodes receive updates in lockstep. As discussed in prior work [Daggitt et al .2018], this simplifies reasoning over the routing behavior of networks which converge to unique solutions. One may extend this model to consider a bounded number of steps of delay at the cost of increasing the complexity of our invariants. To summarize, the key contributions of this paper are: •We demonstrate in depth why a natural, but naïve modular control plane analysis based on an analysis of stable states is unsound (§2). •We develop a new theory for modular control plane analysis based on time (§3). We prove it sound with respect to the semantics of a network simulator, and complete with respect to the closed network semantics (starting from fixed initial values). This theory is general, and can verify individual components using any verification method. •We define an SMTbased verification procedure to reason about all possible routes at all times, which can analyze networks with symbolic representations of, e.g.,external announcements or destination routers. (§4) •We design and implement a new, modular control plane verification tool, Timepiece , based directly on this procedure (§5). We evaluate Timepiece and check a variety of policies at individual nodes in hundreds of milliseconds. Thanks to its embarrassingly parallel modular procedure, Timepiece scales to networks with thousands of nodes (§6). 2 KEY IDEAS This section introduces the stable routing model of network control planes, which serves as a foundation for many past network verification tools [Beckett et al .2017a, 2018, 2019; Prabhu et al . 2020]. It illustrates in depth why naïvely adopting this model for modular verification is unsound. It then introduces a new temporal model for control plane verification and provides the intuition for why the revised model is superior. This section is long but contains a substantial payoff: the essence of why a sound and general modular control plane analysis should be based off a temporal model of control plane behavior. 2.1 Background To determine how to deliver traffic between two endpoints, routers (also called nodes) run distributed routing protocols such as BGP [Lougheed and Rekhter 1991], OSPF [Moy 1998], RIP [Hedrick 1988], or ISIS [Oran 1990]. Each node participating in a protocol receives messages (also called routes ) from its neighboring nodes. After receiving routes from its neighbors, a node will select its “best” route—the route it will use to forward traffic. Different protocols use different metrics to compare Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.108:4 Timothy Alberdingk Thijm, Ryan Beckett, Aarti Gupta, and David Walker 𝑛𝑤 𝑣𝑑𝑒 Neighbor WAN Data Centerfiltertag allowRouting policies: filter :drop all routes tag:tag routes internal allow :drop external routes Fig. 2. Our idealized example cloud provider network. routes and select the best among those received. For instance, RIP compares hop count; OSPF uses the shortest weightedlength path; and BGP uses a complex, userconfigurable combination of metrics. Finally, each router sends its chosen route to its neighbors, possibly modifying the route along the way (for instance, by prepending its identifier to the path represented by the route). Routing algebras. Routing algebras [Griffin and Sobrinho 2005; Sobrinho 2005] are abstract models that capture the similarities between different distributed routing protocols. Prior work on control plane verification [Beckett et al .2017a; Giannarakis et al .2020; Griffin et al .2002] uses similar abstract models to formalize route computation. We adopt this standard abstract model of routing protocols, which specifies the following components. •A directed graph 𝐺that defines the network topology’s nodes ( 𝑉) and edges ( 𝐸). We use lowercase letters ( 𝑢,𝑣,𝑤,etc.) for nodes and pairs ( 𝑢𝑣) to indicate directed edges. •A set𝑆ofroutes that communicate routing information between nodes. Routes abstract the routing announcements and metarouting information used in different routing protocols. Depending on the problem under consideration, 𝑆may be the set of Booleans Bor natural numbers N(e.g.,checking reachability or path length properties), a set of flags ( e.g.,checking access control), or a record with multiple fields (more complex policies and/or properties). •An initialization function Ithat provides an initial route I𝑣∈𝑆for each node 𝑣. •A function Fthat maps edges to transfer functions. Each transfer function F(𝑒)=f𝑒transforms routes as they traverse the edge 𝑒. •A binary associative and commutative function ⊕(a.k.a. merge or the selection function ) selects the best route between two options. An idealized example. Many large cloud providers deploy data center networks to scale up their compute capacity. They connect those data centers to each other and the rest of the Internet via a widearea network (WAN). To illustrate the challenges of modular network verification, we will explore verification of an idealized cloud provider network with WAN and data center components. Figure 2 presents a highly abstracted view of our network’s topology. The data center network contains routers dandewhere dconnects to the corporate WAN and econnects to data center servers. The WAN consists of routers wandv. Router vconnects to the data center as well as to a neighboring network n, which is not controlled by our cloud provider.1 The default routing policy uses shortestpaths. However, in addition, the network administrators want eto be reachable from all cloudproviderowned devices ( i.e., w ,v,d), but not to be reachable from outsiders ( i.e., n ). They intend to enforce this property by tagging all routes originating from their network ( 𝑤) as “internal” ( e.g.,using BGP community tags [CISCO 2005]) and allowing those 1Any of the edges could be bidirectional, allowing routes to pass in both directions, but for pedagogic reasons we strip down the example to the barest minimum, retaining edges that flow from lefttoright except for at vanddwhere routes may flow back and forth. Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.Modular Control Plane Verification via Temporal Invariants 108:5 routes to traverse the deedge. Doing so should allow eto communicate with internal machines but not external machines. Furthermore, to protect nodes from outside interference, the cloud provider applies route filters to external peers to drop erroneous advertised routes that may “hijack” [Feamster and Balakrishnan 2005] internal routing. Modelling the example. To model our example network, we define the network topology as the graph𝐺pictured earlier. We assume all routers participate in an idealized variant of eBGP [Lougheed and Rekhter 1991], which is commonly used in both widearea networks and data centers [Ab hashkumar et al .2021]. We abstract away some of the fields of eBGP routing announcements to define the set of routes 𝑆as records with 3 fields: (i)an integer “local preference” that lets users overwrite default preferences, (ii)an integer path length, and (iii)a boolean tag field that is set to true if a route comes from an internal source and false otherwise. 𝑆also includes∞, a message that indicates absence of a route. Let’s consider what happens when starting with a specific route at WAN node w,⟨100,0,false⟩ (local preference 100, path length of 0, not tagged internal). The Ifunction assigns wthat route, and assigns the∞route to all other nodes. The transfer function f𝑒increments the length field of every route by one across every edge 𝑒. In addition, edge wvsets the internal tag field to true and edge nvdrops all routes (transforms them into∞). Finally, edge dedrops all routes not tagged internal/true. The merge function ⊕always prefers some route over the ∞route, and prefers routes with higher local preference over lower local preference. If the local preference is the same, it chooses a route with a shorter path length. ⊕ignores the tag field. For example, ⊕operates as follows: ⟨100,2,false⟩ ⊕ ∞ =⟨100,2,false⟩ ⟨100,2,false⟩ ⊕ ⟨ 200,5,true⟩=⟨200,5,true⟩ ⟨200,2,false⟩ ⊕ ⟨ 200,5,true⟩=⟨200,2,false⟩ Network simulation. Astate of a network is a mapping from nodes to the “best routes” they have computed so far. One may simulate a network by starting in its initial state and repeatedly computing new states ( i.e.,new “best routes” for particular nodes). Wellbehaved networks eventually converge tostable states where no node can compute a better route, given the routes provided by its neighbors. To compute a new best route at a particular node, say v, we apply the ffunction to each best route computed so far at its neighbors w,n, and d, and then select the best route among the results and the initial value at v, using the merge (⊕) function. More precisely: vnew=f𝑤𝑣(wold)⊕f𝑛𝑣(nold)⊕f𝑑𝑣(dold)⊕I𝑣 The table in Figure 3 presents an example simulation. At each time step, all nodes compute their best route given the routes sent by their neighbors at the previous time step. Our model assumes a synchronous time semantics for simplicity: this simulation is hence one possible asynchronous execution.2After time step 3, no node computes a better route—the system has reached a stable state. The picture in Figure 3 annotates each node in the diagram with the stable route it computes. Network verification. Since the edge from dtoeonly allows routes tagged internal, 𝑤’s route would not reach eifvwere to receive a better route from n(e.g.,if the route filter from 𝑛was implemented incorrectly). In other words, the simulation demonstrates that the network correctly operates when nsends no route (∞). But what about other routes? Will 𝑓𝑛𝑣filter all routes from n correctly? SMTbased tools like Minesweeper [Beckett et al .2017a] and Bagpipe [Weitz et al .2016] can answer such questions by translating the routing problem into constraints for a Satisfiability Modulo Theory (SMT) solver to solve. An SMTbased encoding of our network could represent 2See §4 for a discussion of how we can extend our model to consider networks with delay. Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.108:6 Timothy Alberdingk Thijm, Ryan Beckett, Aarti Gupta, and David Walker filtertag allow 𝑛𝑤 𝑣𝑑𝑒∞⟨100,0,false⟩ ⟨100,1,true⟩⟨100,2,true⟩ ⟨100,3,true⟩ time n w v d e 0∞ ⟨ 100,0,false⟩ ∞ ∞ ∞ 1∞ ⟨ 100,0,false⟩ ⟨ 100,1,true⟩ ∞ ∞ 2∞ ⟨ 100,0,false⟩ ⟨ 100,1,true⟩ ⟨ 100,2,true⟩ ∞ 3∞ ⟨ 100,0,false⟩ ⟨ 100,1,true⟩ ⟨ 100,2,true⟩ ⟨ 100,3,true⟩ 4∞ ⟨ 100,0,false⟩ ⟨ 100,1,true⟩ ⟨ 100,2,true⟩ ⟨ 100,3,true⟩ Fig. 3. Simulation of the example network for a fixed set of initial routes. Node ereceives a route from dsince its route is tagged as internal, and the network stabilizes at time 3. any possible external route announcement from nby representing its initial value with a symbolic variable: the solver can then search for a concrete route captured by this variable that violates our desired property, i.e.,a stable state where enever receives a route from w. 2.2 The Challenge of Modular Verification A system for modular verification will partition a network into components and verify each component separately, possibly in parallel. However, since routes computed at a node in one component depend on the routes sent by nodes in neighboring components, each component must make some assumptions about the routes produced by its neighbors. Interfaces. In our case, for simplicity (though this is not necessary), we place every node in its own component and define for it an interface that attempts to overapproximate (or equal) the set of routes that the node might produce in a stable state. The interface for the network as a whole is a function𝐴from nodes to sets of routes where 𝐴(𝑥)is the interface for node 𝑥. The person attempting to verify the network will supply these interfaces. Of course, interfaces may be wrong —that is, they might not include some route computed by a simulation (and hence might not be a proper overapproximation). Indeed, when there are bugs in the network, the interfaces a user supplies are likely to be wrong! The user expects the network to behave one way, producing a certain set of routes, but the network behaves differently due to an error in its configuration. A sound modular verification procedure must detect such errors and indicate if we must strengthen the interface to prove the property. On the other hand, a useful modular verification procedure should allow interfaces to overapproximate the routes produced, when users find it convenient. Overapproximations are sound for verifying properties over all routing behaviors of a network, and they often simplify reasoning, allowing users to think more abstractly. Throughout the paper, we use predicates 𝜑to define interfaces, where 𝜑stands in for the set of routes{𝑠|𝑠∈𝑆,𝜑(𝑠)}. Returning to our running example, one might define the interface for wusing the predicate 𝑠.lp=100∧𝑠.len=0∧¬𝑠.tag. Such an interface would include exactly the one route generated by win our example:⟨100,0,false⟩. However, path length is unimportant in the current context; to avoid thinking about it, a user could instead provide a weaker interface representing infinitely many possible routes, such as 𝑠.lp=100∧¬𝑠.tag. This interface relieves the user of having to figure out the exact path length (not so hard in this simple example, but potentially challenging in an arbitrary widearea network), and instead specifies only the local Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.Modular Control Plane Verification via Temporal Invariants 108:7 preference and the tag. In general, admitting overapproximations make it possible for users to ignore any features of routing that are not actually relevant for analyzing the properties of interest. The strawperson verification procedure. For a given node x, the component centered at x is the subgraph of the network that includes node xand all edges that end at x. Given a network interface 𝐴, our strawperson verification procedure ( SV) will consider the component centered at each node xindependently. Suppose a node xhas neighbors 𝑛1,...,𝑛𝑘. For that node x,SVchecks that ∀𝑠1∈𝐴(𝑛1),...,∀𝑠𝑘∈𝐴(𝑛𝑘),f𝑛1𝑥(𝑠1)⊕···⊕ f𝑛𝑘𝑥(𝑠𝑘)⊕I𝑥∈𝐴(𝑥) (1) This check is akin to performing one local step of simulation, checking that all possible inputs from neighbors produce an output route satisfying the interface. We might hope that by performing such a check on allcomponents independently, we could guarantee that all nodes converge to stable states satisfying their interfaces. If that were the case, then we could verify properties by: (1)Checking that all components guarantee their interfaces, under the assumption their neigh bors do as well; and (2)Checking that the interfaces imply the network property of interest ( e.g.,reachability, access control, no transit). The problem: execution interference. It turns out this simple and natural verification procedure is unsound: users can supply interfaces that, when analyzed in isolation, satisfy equation (1)above, butexclude stable states computed by simulation. Hence, the second verification step is pointless: a destination that appears reachable according to an interface may not be; conversely, a route that appears blocked may not be. Let us reconsider the running example, where we assign wan initial route with local preference 100, and assume the external neighbor 𝑛can send us any route ( true). A user could provide the interfaces shown in Figure 4 to falsely conclude that 𝑒will not receive a route from 𝑤. filtertag allow 𝑛𝑤 𝑣𝑑𝑒 true𝑠.lp=100 𝑠.lp=200∧¬𝑠.tag𝑠.lp=200∧¬𝑠.tag 𝑠=∞ Fig. 4. Running example with bad interfaces. Here, it is easy to check that nodes nandwsatisfy equation (1). Node n’s interface is simply any route. Node w’s route can be any route with a local preference of 100.3 The surprise comes at node vwhere its interface only includes routes that satisfy ¬𝑠.tag,i.e., routes not tagged as internal. Those routes have 𝑠.lp=200and may have any path length. But the route from wis tagged truealong the edge 𝑤𝑣— why is such a route erroneously excluded from v’s interface? We show the component centered at vin Figure 5. 3It could be any route ( true), as the edge 𝑤𝑣applies the default preference of 100, but for clarity we label the routes at w with preference 100. Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.108:8 Timothy Alberdingk Thijm, Ryan Beckett, Aarti Gupta, and David Walker filtertag allow 𝑛𝑤 𝑣𝑑𝑒 true𝑠.lp=100 𝑠.lp=200∧¬𝑠.tag𝑠.lp=200∧¬𝑠.tag 𝑠=∞ Fig. 5. Running example centered on v’s component. When computing its stable state, vwill compare the routes it receives from wandd: because all routes from whave a local preference set to 100 by f𝑤𝑣, whereas all routes from dhave a better local preference of 200, vwill always wind up selecting the route from dover the route from w. But how then did dacquire these preferential routes tagged false? Such routes came in turn from v’s interface. Figure 6 shows the component centered at d. filtertag allow 𝑛𝑤 𝑣𝑑𝑒 true𝑠.lp=100 𝑠.lp=200∧¬𝑠.tag𝑠.lp=200∧¬𝑠.tag 𝑠=∞ Fig. 6. Running example centered on d’s component. What has happened is that vtransmits its spurious routes to d, enabling dto justify its own spurious routes. dtransmits these back again to v, where d’s routes interact with the legitimate routes from w. Since w’s routes have lower local preference, vdiscards them during computation of stable states. In a nutshell, our interface proposed routes that do notsoundly overapproximate the legitimate routes from the true simulation, but our verification procedure accepted this bad interface as it circularly justified itself at vandd. How might we prevent this execution interference ? Other approaches. We can modify this verification procedure to make it sound, but these solutions will limit the verification procedure’s power or the expressiveness of the properties it can prove. One approach is to limit every interface to exactly one route . Doing so avoids introducing any imaginary executions in the first place. Kirigami [Alberdingk Thijm et al .2022a] takes this approach, but the cost is that a user analyzing their network must know exactly which routes appear at which locations. Computing routes exactly can be difficult in practice, and would seem unnecessary if all one cares about is a highlevel property such as reachability. Moreover, it makes the interfaces brittle in the face of change—any change in network configuration likely necessitates a change in interface. A superior system would allow operators to define durable andabstract interfaces that imply key properties, and to check configuration updates against those interfaces. Another approach is to limit the set of properties that the system can check to only those that say what does not happen in the network rather than what does happen. This is the approach Lightyear [Tang et al .2022] takes. For instance, Lightyear can check that node awillnotbe able to reach node b, but not that aandbwill have connectivity — a common requirement in networks. A final approach is to statically order the components, and verify each component according to this ordering, using no information from the notyetverified components. By ordering vbefore d, we would need to satisfy v’s invariant without routes from d(treating d’s invariant as false) — this would fail for the bad invariant 𝑠.lp=200∧¬𝑠.tagusing only routes from nandw. In practice, Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.Modular Control Plane Verification via Temporal Invariants 108:9 this is still unnecessarily conservative. The running example is overly simple as it shows routes propagated through a network in a single direction from left to right. In realistic networks, multiple destinations may broadcast routes in multiple directions at once. In such situations, there may be no way to order the components, and verification may not be possible. 2.3 The Solution: A Temporal Model Our key insight is to change the model: rather than focus exclusively on the final stable states of a system, as a Minesweeperstyle verifier would, we ensure that the model preserves the entirety of every stepbystep execution. To make this work, we need to add information to the model: a notion of logical time . By associating every route with the time at which a node computes it, we can (i)ensure that allroutes at a particular time are properly considered, and their executions extended a time step, and (ii)ensure that we avoid collisions between routes computed at different times. To verify such routing systems modularly, we once again must specify interfaces, but this time the interface for each node will specify the set of routes that may appear at any time . We write this now as an interface 𝐴(𝑥)(𝑡)that takes both a node 𝑥and a time𝑡and returns an overapproximation of the set of routes that may appear at 𝑥at that time𝑡. To check the interfaces, we use a verification procedure structured inductively with respect to time, as follows: •At every node x, check I𝑥is included in 𝐴(𝑥)(0) •Consider each node xwith neighbors 𝑛1,...,𝑛𝑘. At time𝑡+1, check that merging any combination of routes 𝑠1∈𝐴(𝑛1)(𝑡),...,𝑠𝑘∈𝐴(𝑛𝑘)(𝑡)from neighbors’ interfaces at time 𝑡 produces a route in 𝐴(𝑥)(𝑡+1): f𝑛1𝑥(𝑠1)⊕···⊕ f𝑛𝑘𝑥(𝑠𝑘)⊕I𝑥∈𝐴(𝑥)(𝑡+1) (2) Because this procedure is structured inductively, we can prove, by induction on time, that all states at all times are included in their respective interfaces—the procedure is sound . For brevity, we specify our interfaces using temporal operators . These operators are functions that take a time 𝑡as an argument, compare it to an explicit time variable 𝜏, and return a predicate. We writeG(𝑃)(“globally𝑃”) when a node’s interface includes the routes that satisfy predicate 𝑃 for all times 𝑡. We write𝑃1U𝜏𝑄2(“𝑃1until𝑄2”) when a node may have routes satisfying 𝑃1until time𝜏−1and operator 𝑄2(𝜏)holds afterwards. Finally, we write F𝜏(𝑄)(“finally𝑄”) to mean that eventually at time 𝜏routes start satisfying 𝑄(𝜏). Verifying correct interfaces. Figure 7 below presents an interface we may verify with this model. filtertag allow 𝑛𝑤 𝑣𝑑𝑒 G(true)G(𝑠.lp=100) G(𝑠=∞∨𝑠.tag)G(𝑠=∞∨𝑠.tag) G(𝑠=∞∨𝑠.tag) Fig. 7. Running example with interfaces proving that if ehas a route, it is tagged. We again assume that nsends any route at any time, denoted by the interface: G(true). We assume whas a route with default local preference: G(𝑠.lp=100). The interesting part is at nodes vanddwhere the interfaces state that there is always no route ( e.g.,at time 0), or a tagged route: G(𝑠=∞∨𝑠.tag). We can then prove a weak property about node e:if it receives a route , then the route will be tagged internal. We can prove node v’s route satisfies its interface since f𝑤𝑣tags routes Proc. ACM Program. Lang., Vol. 7, No. PLDI, Article 108. Publication date: June 2023.108:10 Timothy Alberdingk Thijm, Ryan Beckett, Aarti Gupta, and David Walker on import from node w, routes from nare correctly dropped, and routes from dmust also have a tag per its interface. In fact, all the nodes satisfy their interface given their neighbors’ interfaces. Proving reachability. Figure 7’s interfaces were too weak to prove that wcan reach e. The problem is that they reason about alltimes ( i.e.,from time 0 onward), yet ewill only eventually have a route from wat some time in the future. Consider now the stronger interfaces shown in Figure 8: filtertag allow 𝑛𝑤 𝑣𝑑𝑒 G(true)G(𝑠.lp=100) 𝑠=∞U1G(𝑠.tag)𝑠=∞U2G(𝑠.tag) F3G(𝑠≠∞) Fig. 8. Running example with interfaces proving ecan reach w. As before, we allow nandwto send any route. However, now nodes vandddeclare that they will not have a route until a specified (logical) time, at which point they receive a tagged route. We give precise witness times for vandd’s interfaces, as otherwise vcould give da nonnull route (or viceversa) that would violate the interface before its witness time. e’s interface simply requires that ereceives some route at the witness time (allowing arbitrary routes before the witness time). These interfaces are sufficient to prove that ewill eventually receive a route to w, since dwill eventually have a route tagged as internal, and hence ewill allow it. Debugging erroneous interfaces. Let us revisit the example where a user gave unsound interfaces using spurious routes with local preference 200. Figure 9 presents the equivalent temporal interfaces. filtertag allow 𝑛𝑤 𝑣𝑑𝑒 G(true)G(𝑠.lp=100) G(𝑠.lp=200∧¬𝑠.tag)G(𝑠.lp=200∧¬𝑠.tag) G(𝑠=∞) Fig. 9. Running example with bad temporal interfaces. Unlike before, the verification procedure detects an error: the interfaces at nodes vandddo not include the initial route ∞at time 0. As a result, the user will receive a counterexample for time 𝑡=0when verifying vord. Suppose our imaginative user tries to circumvent this issue by also including the initial route in the interfaces for vanddwith the interface: G"
121,Algorithm Selection for Software Verification using Graph Attention Networks.txt,"The field of software verification has produced a wide array of algorithmic
techniques that can prove a variety of properties of a given program. It has
been demonstrated that the performance of these techniques can vary up to 4
orders of magnitude on the same verification problem. Even for verification
experts, it is difficult to decide which tool will perform best on a given
problem. For general users, deciding the best tool for their verification
problem is effectively impossible.
  In this work, we present Graves, a selection strategy based on graph neural
networks (GNNs). Graves generates a graph representation of a program from
which a GNN predicts a score for a verifier that indicates its performance on
the program.
  We evaluate Graves on a set of 10 verification tools and over 8000
verification problems and find that it improves the state-of-the-art in
verification algorithm selection by 11\%. We conjecture this is in part due to
Graves' use of GNNs with attention mechanisms. Through a qualitative study on
model interpretability, we find strong evidence that the Graves' GNN-based
model learns to base its predictions on factors that relate to the unique
features of the algorithmic techniques.","Given a program, 𝑃, and a correctness specification, 𝜙, formal verification seeks to determine whether the executable program behavior is consistent with the specification, 𝑃|=𝜙. In practice, 𝜙 can take many forms, such as that all assertions hold, that memory is used safely, or a guarantee that a program will terminate. Verification tools must check all feasible program executions to ensure that𝜙cannot be violated. As programs grow in size and complexity, it is increasingly hard to verify them. To address this, a variety of different verification algorithms and tools have been created, with different tools excelling in scenarios where others fail. In recent competition settings, 19 of the 20 competing tools built for verifying C programs were able to uniquely solve somewhere between 4 and 500 verification instances [ 9]. Of the 15000 verification problems in the competition, nearly 10% were solved by a single verifier. Deciding which tool is best suited to verify a specific piece of software can be difficult for an expert in the field of formal software verification, let alone an nonexpert software developer. Algorithm selection determines which algorithm from a suite of algorithms can best solve a instance of a specific problem class, e.g. the variety of sorting algorithms. Since it is rare for a single algorithm to dominate all others in terms of performance, algorithm selection is generally challenging. Even with the overhead of deciding which algorithm to employ and then executing it, algorithm selectors have been shown to be superior over single tools in various competitions [ 52,66]. Recently, machine learning techniques have been shown to be effective algorithm selectors for several problem classes [ 37,46,51]. To relieve developers of the burden of deciding between verification tools, algorithm selectors have been created to determine which tool in a suite of tools is best suited for a given problem. Authors’ addresses: Will Leeson, willleeson@virginia.edu, University of Virginia, 85 Engineer’s Way, Charlottesville, Virginia, USA, 22903; Matthew B. Dwyer, matthewbdwyer@virginia.edu, University of Virginia, 85 Engineer’s Way, Charlottesville, Virginia, USA, 22903.arXiv:2201.11711v2  [cs.SE]  5 Feb 20222 Leeson and Dwyer In this paper, we introduce a graph neural network approach to algorithm selection for verification of software, Graves . Given an arbitrary program 𝑃and a specification 𝜙, our approach can be used to rank a portfolio of verification tools based on their ability to accurately and efficiently verify𝑃. Through an automated process, 𝑃is converted into a graph, 𝐺, which is constructed from the program’s abstract syntax tree, control flow edges, and data flow edges. Using 𝐺, a GNN, consisting of graph attention layers, a jumping knowledge layer, and an attention based pooling layer, produces a graph feature vector. A simple neural network uses this vector to predict a fitness score for each verifier in question. The contributions of this paper are the following: •We introduce an approach to algorithm selection using stateoftheart graph neural networks techniques, which we have implemented in our verification tool GravesCPA •We provide a broad empirical evaluation of verification tool prediction that demonstrates the improvement over the stateoftheart of the proposed technique across a range of benchmarks, baselines, and metrics •We perform a qualitative study into the interpretability of the model our technique employs which identifies portions of programs that an expert would use to select a verification technique 2 BACKGROUND In this section, we present background information on both automated software verification and graph neural networks. 2.1 Automated Software Verification Tools Developments in the field of automated software verification have led to a diverse field of verification techniques. Each technique has strengths and weaknesses. Many model checker based tools convert programs into an SMT formula in an attempt to prove no values of the free, or input, variables lead to a property violation. Thus, the power of the tool hinges on the SMT solver’s ability to check the complex formula they provide it. Abstract interpreters use abstract domains to characterize the variables and paths in programs. If an abstract domain is not precise enough to capture the behavior of the program, it can lead to the tool reporting unknown, or worse, giving an incorrect answer. Most modern tools do not implement a single verification technique. Instead, they combine techniques to make a more sophisticated verifier. The CPAChecker framework allows developers to build their own verification tool by combining preimplemented techniques using a configuration file [ 10]. Because tools implement different sets of techniques, there is an algorithmic diversity which allows some verifiers to excel where others fail. The Competition on Software Verification (SVComp) is an annual event that evaluates verifica tion tools on a diverse set of benchmarks, covering many program behaviors and several verification properties. In the most recent competition, SVComp 2021, 26 different verification tools competed, using some subset of over 20 techniques [ 9]. Table 1 provides an abbreviated look at the diversity of algorithmic implementations of tools at SVComp 2018, which we use in our study. While there are “winners” of the overall competition and the different categories, there is no single verifier that does best on all programs. This has motivated the creation of algorithm selectors using suites of tools from the competition. In fact, there are tools which compete using algorithm selectors [17, 52].Algorithm Selection for Software Verification using Graph Attention Networks 3 Table 1. Techniques implemented by several tools for the 2018 SVComp Verification ToolCEGARPredicate AbstractionSymbolic ExecutionBounded Model CheckingkInductionInterval AnalysisLazy AbstractionInterpolationAutomata Based AnalysisRanking Function 2LS "" "" "" "" CBMC "" CPASeq "" "" "" "" "" "" "" "" DepthK "" "" ESBMCKind "" "" ESBMCIncr "" Symbiotic "" U. Automizer "" "" "" "" "" U. Kojak "" "" "" "" U. Taipan "" "" "" "" "" 2.2 Machine Learning Machine learning is used to solve tasks that would normally require rigorous, often impossible, programming. For example, autonomous driving cars would require a complex series of conditionals to handle any given scenario a driver may encounter on the road. Instead of codifying a system of rules an autonomous car should follow when driving, machine learning has been used to learn how the car should react to a given scenario [ 40,56]. The power of machine learning techniques are their ability to learn complex patterns in large corpora of data to make accurate predictions. A simple, yet effective machine learning technique for classification is the support vector machine (SVM) [ 12]. SVMs use training data to learn a boundary which maximizes the margin between the boundary and the data points of the two classes. When a new data point is presented to the SVM, it is classified based on which side of the boundary it lies. SVMs can be generalized to multiclass classification problems as well. One of the core concepts in machine learning is the idea of an artificial neural network (ANN) [ 43]. These networks are a series of layers of nodes with connections between layers. Data is input into the network and flows through the network. As the data passes through the layers, calculations are performed on the data until the final layer is reached. The output is then used to answer the task the network is meant to solve. These networks go through a training phase where the calculations the networks perform are iteratively tuned, typically through a process called backpropagation [55]. Traditional machine learning techniques leverage the fact that the data they operate on is of a consistent size. For example, bitmapped image encodings have a consistent dimension and ordering. The networks learn to make calculations accordingly. Recurrent neural networks (RNNs) allow for variable sized input, typically streams of data, but they still leverage the fact that data has a set ordering or pattern. RNNs maintain a state which is updated as data is input to it. Graph data, in general, has no set ordering or size which makes it problematic for SVMs, ANNs and RNNs. Introduced in Scarselli et al. [ 57], graph neural networks aim not only to capture the information in the nodes of the graph, but also the connections, or edges, between them. An interesting observation in the foundational work is that GNNs can be thought of as a generalization of RNNs. RNNs operate on data that can be thought of as a linear, acyclic graph. Each input to the RNN is a node in the graph, with an edge from the last input and to the next input. If this restriction on the data can be relaxed, these networks can operate on arbitrary graphs. In an RNN, data flows in a linear fashion. As data is fed through, calculations are made and the state is updated. With GNNs, this must be augmented. Instead of a single state, they have a set of states, one for each node in the graph. Each node in the graph is updated in parallel using the values of the nodes adjacent to it during a process known as message passing. After several iterations4 Leeson and Dwyer of message passing, each node in the graph has a value computed from itself and its neighboring nodes, effectively allowing the network to learn about each node and where it lies in the graph. 2.3 Model Interpretability Model interpretability—the process of discerning why a machine learning model makes a certain decision for a given input—is a sought after property amongst machine learning researchers [ 21,25, 49]. Interpretability can help ensure the model is learning to make predictions based on features a domain expert would recognize as important to a given problem. For example, the tool RISE[ 49] produces heat maps of images based on how important a pixel is to the model’s prediction. If the model is supposed to predict whether or not an image is a tire, the heat map could identify circle shapes are important to pictures. It could also show that the model looks at images that typically have tires, e.g. a car or bike. Ying et al. present a blackbox approach to GNN interpretability, GNNExplainer, based on the idea of masking [ 67]. Masking is the approach of removing certain data points to see how it affects the models prediction. GNNExplainer operates by masking edges in a given graph and giving the altered graph to the model. They can then determine the edges that most influence the model’s prediction. 3 RELATED WORK "
301,Overview frequency principle_spectral bias in deep learning.txt,"Understanding deep learning is increasingly emergent as it penetrates more
and more into industry and science. In recent years, a research line from
Fourier analysis sheds lights into this magical ""black box"" by showing a
Frequency Principle (F-Principle or spectral bias) of the training behavior of
deep neural networks (DNNs) -- DNNs often fit functions from low to high
frequency during the training. The F-Principle is first demonstrated by
one-dimensional synthetic data followed by the verification in high-dimensional
real datasets. A series of works subsequently enhance the validity of the
F-Principle. This low-frequency implicit bias reveals the strength of neural
network in learning low-frequency functions as well as its deficiency in
learning high-frequency functions. Such understanding inspires the design of
DNN-based algorithms in practical problems, explains experimental phenomena
emerging in various scenarios, and further advances the study of deep learning
from the frequency perspective. Although incomplete, we provide an overview of
F-Principle and propose some open problems for future research.","1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Frequency principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2 Empirical study of FPrinciple 7 2.1 Onedimensional experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Twodimensional experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Frequency principle in highdimensional problems . . . . . . . . . . . . . . . . . 8 2.3.1 Projection method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3.2 Filtering method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3 Theoretical study of FPrinciple 10 3.1 Intuitive analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.1.1 Idealized setting for analyzing activation function . . . . . . . . . . . . . . 11 3.1.2 Frequency weight in the loss function . . . . . . . . . . . . . . . . . . . . 12 3.1.3 The joint effect of activation and loss . . . . . . . . . . . . . . . . . . . . 14 3.2 General setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.3 NTK setting and linear FPrinciple . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.3.1 NTK dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.3.2 Eigen analysis for twolayer DNN with dense data distribution . . . . . . . 16 3.3.3 Linear FPrinciple for twolayer neural network with arbitrary data distribution 16 4 Generalization 17 4.1 DNN without FPrinciple produces oscillated output . . . . . . . . . . . . . . . . 18 4.2 Strength and weakness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.3 Early stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.4 Quantitative understanding in NTK regime . . . . . . . . . . . . . . . . . . . . . . 21 4.5 Runge’s phenomenon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5 FPrinciple for scientiﬁc computing 21 5.1 Parameterize the solution of a PDE . . . . . . . . . . . . . . . . . . . . . . . . . . 22 5.2 Difference from conventional algorithms . . . . . . . . . . . . . . . . . . . . . . . 23 5.2.1 Iterative methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.2.2 RitzGalerkin (RG) method . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.3 Understanding FPrinciple by comparing the differential operator and the integrator operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.4 Algorithm design to overcome the curse of highfrequency . . . . . . . . . . . . . 31 6 Application of the FPrinciple 32 6.1 Frequency perspective for understanding experimental phenomena . . . . . . . . . 32 6.2 Inspiring the design of algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 27 AntiFPrinciple 34 7.1 Derivative w.r.t. input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 7.2 Large weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 8 Conclusion 35 31. Introduction 1.1 Motivation In practice, deep learning, often realized by deep neural networks (DNNs), has achieved tremendous success in many applications, such as computer vision, speech recognition, speech translation, and natural language processing, etc. It also has become an indispensable method for solving a variety of scientiﬁc problems. On the other hand, DNN sometimes fails and causes critical issues in applications. In theory, DNN remains a black box for decades. Many researchers make the analogy between the practical study of DNN and the alchemy. Due to the booming application of DNNs, it has become an important and urgent mission to establish a better theoretical understanding of DNNs. In recent years, theoretical study of DNNs ﬂourishes. Yet, we still need to demonstrate clearly how these theoretical results provides key insight and guidance to practical study of DNNs. An insightful theory usually provides guidance to practice from two aspects–capability and limitation. For example, the conservation of mass in chemistry informs the fundamental limitation of chemical reactions that they cannot turn one element into another, e.g., turning bronze into gold. On the other hand, they may combine elementary substances into their compounds. These understandings are extremely valuable, with which, the study of alchemy transforms into modern chemistry. In this work, we overview the discovery and studies about the frequency principle of deep learning (Xu et al., 2019, 2020; Rahaman et al., 2019; Zhang et al., 2021a), by which we obtain a basic understanding of the capability and limitation of deep learning, i.e., the difﬁculty in learning and achieving good generalization for high frequency data and the easiness and intrinsic preference for low frequency data. Based on this guideline of frequency principle, many algorithms has been developed to either employ this low frequency bias of DNN to well ﬁt smooth data or design special tricks/architectures to alleviate the difﬁculty of DNN in ﬁtting data known to be highly oscillatory (Liu et al., 2020; Jagtap et al., 2020; Cai et al., 2020; Tancik et al., 2020). Hopefully, by the development of frequency principle and theories from other perspectives, the practical study of deep learning would become a real science in the near future. The discovery of frequency principle is made confronting the following open puzzle central for DNN theories: why overparameterized DNNs generalize well in many problems, such as natural image classiﬁcation. In 1995, Vladimir Vapnik and Larry Jackel made a bet, where Yann LeCun was the witness, that is, Larry claimed that by 2000 we will have a theoretical understanding of why big neural nets work well (in the form of a bound similar to what we have for SVMs). Also in 1995, Leo Breiman published a reﬂection after refereeing papers for NIPS (Breiman, 1995), where he raised many important questions regarding DNNs including “why don’t heavily parameterized neural networks overﬁt the data”. In 2016, an empirical study (Zhang et al., 2017) raises much attention again on this nonoverﬁtting puzzle with systematic demonstration on modern DNN architectures and datasets. This nonoverﬁtting puzzle contradicts the conventional generalization theory and traditional wisdom in modeling, which suggests that a model of too many parameters easily overﬁt the data. This is exempliﬁed by von Neumann’s famous quote “with four parameters I can ﬁt an elephant"" (Dyson, 2004). Establishing a good theoretical understanding of this nonoverﬁtting puzzle has since become more and more crucial as modern DNN architectures incorporates increasingly more parameters, e.g., 108for VGG19 (Simonyan and Zisserman, 2015), 1011for GPT3 (Brown et al., 2020), which indeed achieve huge success in practice. To address this puzzle, a notable line of works, starting from the conventional complexitybased generalization theory, attempt to propose novel normbased complexity measures suitable for DNNs. 4However, a recent empirical study shows that many normbased complexity measures not only perform poorly, but negatively correlate with generalization, speciﬁcally, when the optimization procedure injects some stochasticity (Jiang et al., 2019). Another line of works start from a variety of idealized models of DNNs, e.g., deep linear network (Saxe et al., 2014, 2019; Lampinen and Ganguli, 2019), committee machine (Engel and Broeck, 2001; Aubin et al., 2018), spin glass model (Choromanska et al., 2015), meanﬁeld model (Mei et al., 2018; Rotskoff and VandenEijnden, 2018; Chizat and Bach, 2018; Sirignano and Spiliopoulos, 2020), neural tangent kernel (Jacot et al., 2018; Lee et al., 2019). These works emphasize on fully rigorous mathematical proofs and have difﬁculties in providing a satisfactory explanation for general DNNs (Zdeborová, 2020). The frequency principle overviewed in this paper takes a phenomenological approach to the deep learning theory, which serves as an important approach over the history to understand complex systems, black boxes at ﬁrst glance, in science and especially in physics. Taking this approach, the ﬁrst difﬁculty we encounter is the extreme complexity of deep learning experiments in practice. For example, the MNIST dataset is a wellknown simple (if not too simple) benchmark for testing a DNN. However, the learned DNN is already a very high dimensional (784dimensional) mapping, which is impossible to be visualized and analyzed exactly. In face of such difﬁculty, an important step we take is to carefully design synthetic problems simple enough for thorough analysis and visualization of the DNN learning process, but complicated enough for reproducing interested phenomena. We train DNNs to ﬁt a function with onedimensional input and onedimensional output like sin(x) + sin(5x) shown in Fig. 1. Luckily, a clear phenomenon emerges from the thorough visualization of the DNN training process that the DNN ﬁrst captures a coarse and relatively “ﬂat” landscape of the target function, followed by more and more oscillatory details. It seems that the training of a DNN gives priority to the ﬂat functions, which should generalize better by intuition, over the oscillatory functions. By the phenomenological approach, we next quantify this phenomenon by the Fourier analysis, which is a natural tool to quantify ﬂatness and oscillation. As shown latter, by transforming the DNN output function into the frequency domain, the differences in convergence rate between ﬂat and oscillatory components become apparent. We concludes this phenomenon of implicit lowfrequency bias by the frequency principle/spectral bias (Xu et al., 2019, 2020; Rahaman et al., 2019; Zhang et al., 2021a), i.e., DNNs often ﬁt target functions from low to high frequencies during the training, followed by extended experimental studies for real datasets and a series of theoretical studies detailed in the main text. In the end, as a reﬂection, we note that the specialness of the discovery of frequency principle lies in our faith and insistence in performing systematic DNN experiments on simple 1d synthetic problems, which is clearly not understood and simple for observation and analysis. From the perspec tive of phenomenological study, such simple cases serve as an excellent starting point, however, they are rarely considered in the experimental studies of DNNs. Some researchers even deems MNIST experiments as too simple for an empirical study without realizing that even phenomenon regarding the training of DNN on 1d problems is not well studied. In addition, because Fourier analysis is not naturally considered for high dimensional problems due to the curse of dimensionality, it is difﬁcult even to think about Fourier transform for DNNs on real datasets as done in Sec. 2 without making a direct observation of DNN learning from ﬂat to oscillatory in 1d problems. Therefore, by overviewing the discovery and studies of frequency principle, we advocate for the phenomenological approach to the deep learning theory, by which systematic experimental study on simple problems should be encouraged and serve as a key step for developing the theory of deep learning. 5(a)  (b)  (c) Figure 1: Illustration of the training process of a DNN. Training data are sampled from target function sin(x) + sin(5x). Red, green and black curves indicates DNN output, sin(x), andsin(x) + sin(5x)respectively. 1.2 Frequency principle To visualize or characterize the training process in frequency domain, it requires a Fourier transform of the training data. However, the Fourier transform of highdimensional data suffers from the curse of dimensionality and the visualization of highdimensional data is difﬁcult. Alternatively, one can study the problem of onedimensional synthetic data. A series of experiments on synthetic lowdimensional data show that DNNs often ﬁt target functions from low to high frequencies during the training. This implicit frequency bias is named as frequency principle (FPrinciple) (Xu et al., 2019, 2020; Zhang et al., 2021a) or spectral bias (Rahaman et al., 2019) and can be robustly observed no matter how overparameterized DNNs are. More experiments on real datasets are designed to conﬁrm this observation (Xu et al., 2020). It is worthy to note that the frequency used here is a response frequency characterizing how the output is affected by the input. This frequency is easy to be confused in imaging classiﬁcation problems. For example, in MNIST dataset, the frequency domain is also 784dimensional but not 2dimensional, i.e., the frequency of the classiﬁcation function but not the image frequency w.r.t. 2dimensional space. Xu et al. (2020) proposed a key mechanism of the FPrinciple that the regularity of the activation function converts into the decay rate of a loss function in the frequency domain. Theoretical studies subsequently show that the FPrinciple holds in general setting with inﬁnite samples (Luo et al., 2021a) and in the regime of wide DNNs (Neural Tangent Kernel (NTK) regime (Jacot et al., 2018)) with ﬁnite samples (Zhang et al., 2019, 2021a; Luo et al., 2020a) or samples distributed uniformly on sphere (Cao et al., 2021; Yang and Salman, 2019; Ronen et al., 2019; Bordelon et al., 2020). E et al. (2020) show that the integral equation would naturally leads to the FPrinciple. In addition to characterizing the training speed of DNNs, the FPrinciple also implicates that DNNs prefer lowfrequency function and generalize well for lowfrequency functions (Xu et al., 2020; Zhang et al., 2019, 2021a; Luo et al., 2020a). The FPrinciple further inspires the design of DNNs to fast learn a function with high frequency, such as in scientiﬁc computing and image or point cloud ﬁtting problems (Liu et al., 2020; Jagtap 6et al., 2020; Cai et al., 2020; Tancik et al., 2020). In addition, the FPrinciple provides a mechanism to understand many phenomena in applications and inspires a series of study on deep learning from frequency perspective. The study of deep learning is a highly interdisciplinary problem. As an example, the Fourier analysis, an approach of signal processing, is an useful tool to understand better deep learning (Giryes and Bruna, 2020). A comprehensive understanding of deep learning remains an excited research subject calling for more fusion of existing approaches and developing new methods. 2. Empirical study of FPrinciple Before the discovery of the FPrinciple, some works have suggested the learning of the DNNs may follow a order from simple to complex (Arpit et al., 2017). However, previous empirical studies focus on the real dataset, which is highdimensional, thus, it is difﬁcult to ﬁnd a suitable quantity to characterize such intuition. In this section, we review the empirical study of the FPrinciple, which ﬁrst presents a clear picture from the onedimensional data and then carefully designs experiments to verify the FPrinciple in highdimensional data (Xu et al., 2019, 2020; Rahaman et al., 2019). 2.1 Onedimensional experiments To clearly illustrate the phenomenon of FPrinciple, one can use 1d synthetic data to show the relative error of different frequencies during the training of DNN. The following shows an example from Xu et al. (2020). Training samples are drawn from a 1d target function f(x) = sin(x) + sin(3x) + sin(5x)with three important frequency components and even space in ["
342,Feature Weight Tuning for Recursive Neural Networks.txt,"This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform ""weight tuning"" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.","Recursive neural network models [1] constitute one type of neural structure for obtaining higher level representations beyond wordlevel such as phrases or sentences. It works in a bottomup fashion on tree structures (e.g., parse trees) in which longterm dependency can be to some extent captured. Figure 1 gives a brief illustration about how recursive neural models work to obtain the distributed representation for the short sentence “ The movie is wonderful ”. Supposehisand hwonderful are the embeddings for tokens isandwonderful . The representation for their parent node VP at second layer is given by: hVP=f(W[his,hwonderful ] +b) (1) whereWandbdenote parameters involved in the convolutional function. f()is the activation function, usually tanh orsigmod or the rectiﬁer linear function. For NLP tasks, the obtained embeddings could be further fed into taskspeciﬁc machine learning models1, through which parameters are to be optimized. Take sentiment analysis as an example, we could feed the aforementioned sentence embedding into a logistic regression model to classify it as either positive or negative. Embeddings are sometimes more capable of capturing latent semantic meanings or syntactic rules within the text than manually developed features, from which many NLP tasks would beneﬁt (e.g., [2, 3]). Such a type of structure suffers some sorts of intrinsic drawbacks. Revisit Figure 1, common sense tells us that tokens like “ the”, “movie ” and “ is” do not contribute much to the sentiment decision but word “ wonderful ” is the key part (and a good machine learning model should have the ability of learning these rules). Unfortunately, the intrinsic structure of recursive neural networks makes it less ﬂexible to get rid of the inﬂuence from less sentimentrelated tokens. If the keyword “wonderful” hides too deep in the parse tree, for example, as in the sentence “ I studied Russia in Moscow, where 1Of course, embeddings could also be optimized through the taskspeciﬁc objective functions. 1arXiv:1412.3714v2  [cs.NE]  13 Dec 2014Figure 1: Illustration of Standard Recursive Neural Network for Sentencelevel Representation Cal culation. Model Accuracy unigram SVM 0.743 Recursive Neural Net 0.730 Table 1: A brief comparison between SVM and standard neural network models for sentencelevel sentiment classiﬁcation using date set from [4]. Neural network models are trained with L2 reg ularization, using AdaGrad [5] with minibatches (for details about implementations of recursive networks, please see Section 2). Parameters are trained based on 5fold cross validation on the train ing data. We report the best performance searching optimum regularization parameter, optimum batch size for minibatches and convolutional function. Word embeddings are borrowed from Glove [6] with dimensionality of 300, which generates better performance than word2vect, SENNA [7] and RNNLM [8]. all my family think the winter is wonderful ”, it will takes quite a few convolution steps before the keyword ‘wonderful” comes up to the surface, with the consequence that its inﬂuence on the ﬁnal sentence representation could be very trivial. Such an issue, usually referred to as gradient vanishing [9]. is not speciﬁc for recursive models, but for most deep learning architectures. When we compare neural models with SVM, one notable weakness of bigofword based SVM is its inability of considering how words are combined to form meanings (or order information in other words) [10]. But interestingly, such downside of SVM comes with the advantage of resilience in feature managing as the optimization is “ﬂatexpanded”. Low weights will be assigned to less informative evidence, which could further be pushed to zero by regularization. Table 1 gives a brief comparison between unigram based SVM and neural network models for sentencelevel sentiment prediction on Pang et al.’s dataset [4], and as can be seen, in this task, standard neural network models underperform SVM2. Revisit the form of Equ.1, there are two straws we can grasp at to deal with the aforementioned problem: (1) expecting the learned feature embeddings for less useful words such as the3exert very little inﬂuence (for example, a zero vector for the best) (2) expecting the compositional parameters Wandbare extremely powerful. For the former, it is sometimes hard, as mostly we borrow (or initialize) word embeddings from those trained from large corpus (e.g., word2vec, RNNLM [8, 11], SENNA [7]), rather than training embeddings from taskspeciﬁc objective functions as neural models can be easily over ﬁtted given the small amount of training data4. Regarding the latter issue, several alternative compositional functions have been proposed to enable more varieties in composition to cater. Recent proposed approaches include, for example, Matrix Vector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which 2To note, results here are not comparable with Socher et al.’s work [2] which obtains stateofart perfor mance in sentiment classiﬁcation, as here labels at sentencelevel constitute only sort of supervision for both SVM and neural network models (for details, see footnote 7). 3We just use this example for illustration. Practically, themight be a good sentiment indicator as it usually coappears with superlatives. 4There are cases, for example, [2], where taskspeciﬁc word embeddings are learned. But it requires suf ﬁcient training data to avoid over ﬁtting. For example, Socher et al.’s work labels every single node as posi tive/negative/neutral along parse trees (with a total number of more than 200,000 phrases). 2associates different labels (e.g., POS tags, relation tags) with different sets of compositional param eters. These approaches to some extent enlarge the power of compositional functions. In this paper, we borrow the idea of “weight tuning” from feature based SVM and try to incorporate such idea into neural architectures. To achieve this goal, we propose two recursive neural archi tectures, Weighted Neural Network (WNN) and BinaryExpectation Neural Network (BENN). The major idea involved in the proposed approaches is to associate each node in the recursive network with additional parameters, indicating how important it is for ﬁnal decision. For example, we would expect such type of a structure would dilute the inﬂuence of tokens like “the” and “movie” but mag niﬁes the impact of tokens like “wonderful” and “great” in sentiment analysis tasks. Parameters associated with proposed models are automatically optimized through the objective function man ifested by the data. The proposed model combines the capability of neural models to capture the local compositional meanings with weight tuning approach to reduce the inﬂuence of undesirable information at the same time, and yield better performances in a range of different NLP tasks when compared with standard neural models. The rest of this paper is organized as follows: Section 2 brieﬂy describes the related work. The details of WNN and BENN are illustrated in Section 4 and experimental results are presented in Section 5, followed by a brief conclusion. 2 Related Work "
264,Verifying Low-dimensional Input Neural Networks via Input Quantization.txt,"Deep neural networks are an attractive tool for compressing the control
policy lookup tables in systems such as the Airborne Collision Avoidance System
(ACAS). It is vital to ensure the safety of such neural controllers via
verification techniques. The problem of analyzing ACAS Xu networks has
motivated many successful neural network verifiers. These verifiers typically
analyze the internal computation of neural networks to decide whether a
property regarding the input/output holds. The intrinsic complexity of neural
network computation renders such verifiers slow to run and vulnerable to
floating-point error.
  This paper revisits the original problem of verifying ACAS Xu networks. The
networks take low-dimensional sensory inputs with training data provided by a
precomputed lookup table. We propose to prepend an input quantization layer to
the network. Quantization allows efficient verification via input state
enumeration, whose complexity is bounded by the size of the quantization space.
Quantization is equivalent to nearest-neighbor interpolation at run time, which
has been shown to provide acceptable accuracy for ACAS in simulation. Moreover,
our technique can deliver exact verification results immune to floating-point
error if we directly enumerate the network outputs on the target inference
implementation or on an accurate simulation of the target implementation.","The Airborne Collision Avoidance System (ACAS) is crucial for aircraft safety [11]. This system aims to avoid collision with intruding aircraft via automatically controlling the aircraft or advising a human operator to take action. The ACAS typically takes lowdimensional sensory inputs, including distance, direction, and speed for the intruder and ownship aircraft, and provides a control policy which is a valuation for a set of candidate actions such as \weak left"" or \strong right"". Recent work has formulated aircraft dynamics under uncertainties such as advi sory response delay as a partially observable Markov decision process for which dynamic programming can be used to compute values for dierent actions [10]. The value function computed via dynamic programming is often stored in aarXiv:2108.07961v1  [cs.LG]  18 Aug 20212 Kai Jia and Martin Rinard lookup table with millions of entries [12] that require gigabytes of storage. While this table could, in principle, be used to implement the ACAS, the high storage demand makes it too costly to be embedded in practical  ight control systems. This situation has motivated the development of table compression techniques, including block compression with reduced  oatingpoint precision [13] and deci sion trees [7]. Recently, neural networks have emerged as an ecient alternative for com pressing the lookup tables in ACAS Xu (ACAS X for unmanned aircraft) by approximating the value function with small neural networks. Specically, Ju lian et al. [7] compresses the twogigabyte lookup table into 45 neural networks with 2.4MB of storage, where each network handles a partition of the input space. Katz et al. [9] proposes a set of safety properties for the ACAS Xu net works, such as that a \strong right"" advisory should be given when a nearby intruder is approaching from the left. These safety properties have served as a valuable benchmark to motivate and evaluate multiple verication algorithms [1, 9, 15, 17, 19]. Such veriers typically need to perform exact or conserva tive analysis of the internal neural network computation [14, 18]. Unfortunately, neural network verication is an NPComplete problem [9], and therefore the veriers need exponential running time in the worst case and can be very slow in practice. In particular, Bak et al. [1] recently presented the rst verier that is able to analyze the properties 1to4in the ACAS Xu benchmarks with a time limit of 10 minutes for each case, but their verier still needs 1.7 hours to analyze the property 7. In summary, previous techniques perform the following steps to obtain and verify their neural network controllers for ACAS: 1. Compute a lookup table containing the scores of dierent actions given sen sory states via dynamic programming. 2. Train neural networks to approximate the lookup table. 3. In deployed systems, use the neural networks to provide control advisories. {At run time, the networks give interpolated scores for states not present in the original lookup table. {Neural network veriers that analyze the internal computing of neural networks are adopted to check if the networks meet certain safety spec ications. We propose instead to verify neural networks with lowdimensional inputs, such as the ACAS Xu networks, via input quantization and state enumeration. Specically, we prepend a quantization layer to the network so that all the internal computation is performed on the discretized input space. Our proposed technique performs the following steps to obtain and verify a quantized neural network: 1. We take a pretrained network and prepend an input quantization layer to the network. The input quantization should be compatible with the original lookup table, i.e., preserving the grid points in the lookup table.Verifying Lowdimensional Input Neural Networks via Input Quantization 3 2. In deployed systems, sensory inputs are rst quantized by the input quanti zation layer. The original network then computes the scores for the quantized input. {At run time, the quantization process is equivalent to nearestneighbor interpolation. {To verify the network for any specication, we enumerate all quantized states within the constraint of the specication and check if the network outputs meet the specication. Our method provides the following desirable features: 1. Our method provides acceptable runtime accuracy for ACAS Xu. Our input quantization is equivalent to nearestneighbor interpolation and gives identi cal results on the table grid points as the original continuous network. Julian et al. [7] has shown that nearestneighbor interpolation on the lookup table for runtime sensory inputs provides eective collision avoidance advisories in simulation. 2. Our method enables ecient verication. Verifying the inputquantized net works for any safety specication takes nearly constant time bounded by evaluating the network on all the grid points in the quantized space. Multi ple specications can be veried simultaneously by evaluating the network on the grid once and checking the input and output conditions for each property. Our method provides a verication speedup of tens of thousands of times compared to the ReluVal [19] verier. 3. Many existing veriers do not accurately model  oatingpoint arithmetic due to eciency considerations, thus giving potentially incorrect verication re sults [5]. For example, Wang et al. [19] reports that Reluplex [9] occasionally produces false adversarial examples due to  oatingpoint error. By contrast, our verication result is exact (i.e., complete and sound) and does not suer from  oatingpoint error because we combine input quantiza tion and complete enumeration of the eective input space. Moreover, input quantization allows directly verifying on the target implementation or an ac curate simulation of the implementation, and therefore provides trustworthy safety guarantees for given neural network inference implementations. 4. Our technique allows easily verifying more complicated network architec tures, such as continuousdepth models [2]. Our verication only needs an ecient inference implementation for the networks. By contrast, extending other neural network veriers to new network architectures requires signi cant eort. We recommend input quantization for neural networks with lowdimensional inputs as long as the quantization provides sucient accuracy for the target application and the quantization space is small enough to allow ecient enu meration. This technique enables ecient, exact, and robust verication and provides reliable performance on the deployed platform.4 Kai Jia and Martin Rinard 2 Method "
326,Certifying Robustness of Convolutional Neural Networks with Tight Linear Approximation.txt,"The robustness of neural network classifiers is becoming important in the
safety-critical domain and can be quantified by robustness verification.
However, at present, efficient and scalable verification techniques are always
sound but incomplete. Therefore, the improvement of certified robustness bounds
is the key criterion to evaluate the superiority of robustness verification
approaches. In this paper, we present a Tight Linear approximation approach for
robustness verification of Convolutional Neural Networks(Ti-Lin). For general
CNNs, we first provide a new linear constraints for S-shaped activation
functions, which is better than both existing Neuron-wise Tightest and
Network-wise Tightest tools. We then propose Neuron-wise Tightest linear bounds
for Maxpool function. We implement Ti-Lin, the resulting verification method.
We evaluate it with 48 different CNNs trained on MNIST, CIFAR-10, and Tiny
ImageNet datasets. Experimental results show that Ti-Lin significantly
outperforms other five state-of-the-art methods(CNN-Cert, DeepPoly, DeepCert,
VeriNet, Newise). Concretely, Ti-Lin certifies much more precise robustness
bounds on pure CNNs with Sigmoid/Tanh/Arctan functions and CNNs with Maxpooling
function with at most 63.70% and 253.54% improvement, respectively.","Although neural networks achieve remarkable success in many complex classification tasks, such as speech and image recognition, researchers discover that nonrobust neural networks are vulnera ble to the perturbation from environment and adversarial attacks [10,11,20,21]. In some safetycritical and securitysensitive domain, such as selfdriving [ 12] and face recognition [ 13], some subtle ad versarial perturbation is extremely imperceptible and harmful, and may cause disastrous consequences. Therefore, there is a great need to certify model robustness guarantees against adversarial attacks, which can provide certified defense against any possible attack [2, 6–8]. The methodology of neural network robustness verification could be divided into two categories: complete verifier andincom plete verifier [26].Complete verifier [28–31] which can get accu rate robustness bounds, is timeconsuming and only applies to ReLUbased networks. In contrast, incomplete verifier [32–34] risks verification precision loss due to overapproximation but is morescalable and efficient. Thus, to evaluate the robustness of other types of CNNs, acceleration techniques, such as linear approximation [1,5,9] or abstract interpretation [ 8,14] is necessary. As approxi mation technique inevitably introduces overestimation, certified lower bound is the key criterion to evaluate robustness verification methods’ performances. Regarding the tightness of the linear approximation technique, there is a plenty of works focusing on narrowing the inputoutput zone of nonlinear functions in the network to gain larger certi fied robustness bounds [ 1–5,9]. Some works propose tight linear bounds of nonlinear functions by producing the minimal over approximation zone, resulting in much more precise certified robust ness bounds. Zhang et al. [ 22] define the linear bounds producing the minimal overapproximation area as Neuronwise Tightest linear bounds. Concretely, POPQORN [ 3], CNNCert [ 2], and VeriNet [ 15] propose Neuronwise Tightest robustness bounds of 𝜎(𝑥)𝑡𝑎𝑛ℎ(𝑦),𝑥· 𝜎(𝑦)functions, ReLU function, and Sigmoid/Tanh/Arctan function, respectively. Recently, Zhang et al. [ 22] introduce the notion of Networkwise Tightest and propose Networkwise Tightest linear bounds for monotonous CNNs with Sigmoid/Tanh/Arctan func tion. Unfortunately, regarding the approximation methods’ perfor mances on CNNs with Sigmoid/Tanh/Arctan function, Neuronwise Tightest technique(VeriNet) is limited in precision while Network wise Tightest technique(Newise [ 22]) is limited in scalability. More over, Maxpooling function is far more complex to verified as it is a multivariate function. Some attempts have been made to verify Maxpooling function but result in loose certified robustness bounds [2, 4]. To address the above challenges, in this work, we propose a Tight Linear approximation method called TiLin for general CNNs. By introducing the notion of Neuronwise Tightest , we discover that Neuronwise Tightest is equivalent to Layerwise Tightest , which can explain the reason why minimizing the area/volume between upper and lower constraints of nonlinear functions can get a tighter ro bustness bound. We then analyze the limitations of the Neuronwise Tightest andNetworkwise Tightest techniques(VeriNet, Newise). To overcome these limitations, we make our first contribution by proposing the Tightest linear bounds for Sigmoid/Tanh/Arctan function among three stateoftheart baseline methods. Regard ing our second contribution, as mentioned above, Maxpooling is a complex function to verified. To the best of our knowledge, wearXiv:2211.09810v1  [cs.LG]  13 Nov 2022Conference’17, July 2017, Washington, DC, USA Yuan Xiao, Tongtong Bai, Mingzheng Gu, Chunrong Fang, and Zhenyu Chen are the first to propose Neuronwise Tightest linear constraints for Maxpooling function. We implement TiLin atop the stateoftheart robustness veri fication framework CNNCert [ 2]. We evaluate it with neural net works trained on MNIST, CIFAR10, and Tiny ImageNet datasets. For CNNs with Sigmoid/Tanh/Arctan functions, experimental re sults reveal that VeriNet [ 15] is superior to DeepCert [ 1] in 78.33% cases and TiLin outperforms these two stateoftheart techniques in 96.67% cases. Furthermore, TiLin outperforms Newise, which isNetworkwise Tightest approach with up to 63.70% improvement to certified robustness bounds. For CNNs with Maxpooling func tion, TiLin outperforms CNNCert [ 2] and DeepPoly [ 4] with up to 235.54%, 43.14% improvement to certified robustness bounds, respectively. Finally, we conduct Wilcoxon ranksum test [ 24] and the results confirm that TiLin significantly outperform the five stateoftheart baseline methods in precision with the same time consumption. Compared to other related works, TiLin has four significant ad vantages: (i) effective, it provides much tighter robustness bounds than stateoftheart approaches,(ii) efficient, it has almost the same time consumption as other stateoftheart techniques, (iii) versa tile, it applies to general CNNs, (iv)detailed analysis and proof, it provides proofs to all theorems mentioned in this work. In summary, our work makes the following contributions: •We propose a TightLinear approximation technique( Ti Lin), which applies to general CNNs with ReLU, Sigmoid, Tanh, Arctan, and Maxpooling functions. •We identity the limitations of existing Neuronwise Tight estand Networkwise Tightest methods. We then propose novel linear constraints for Sigmoid/Tanh/Arctan function, which stands for the Tightest among the existing Neuron wise Tightest andNetworkwise Tightest tools. •We first propose provably Neuronwise Tightest linear con straints for Maxpool function, which significantly outper forms prior works in precision. •We evaluate our methods on 35 different CNNs with Sig moid/Tanh/Arctan functions and 13 types of CNNs with Maxpooling, which are both trained on MNIST, CIFAR10 and Tiny ImageNet. The results confirm that TiLin certi fies more precise robustness bounds in an efficient man ner than other five stateoftheart baseline methods. We make our implementation and appendix available at https: //github.com/TiLin/TiLin The rest of this paper is organized as follows. In Section 2, we introduce some necessary preliminaries of our approach, including notations of TiLin, some notions of linear approximation technique and notions of the tightness of the linear approximation technique. In Section 3, we present a technical description of TiLin. We first introduce the limitationd of the existing Neuronwise Tightest and Networkwise Tightest technique which motivates us to find a tighter constraints for Sigmoid/Tanh/Arctan function. Then, we propose Neuronwise Tightest linear bounds for Maxpool function and finally we present the whole computing process of our approach. In Section 4, we first introduce our experiment settings, including datasets, models and research questions and then we analyse ourNotation Definition 𝐹:R𝑛0→R𝑛𝐾 network classifier 𝑛𝑘 number of Neurons in the 𝑘𝑡ℎlayer [𝐾] the set{1,···,𝐾} 𝐹𝑘 𝑗(𝑥):R𝑛0→R the𝑗𝑡ℎoutput of the 𝑘𝑡ℎlayer 𝐿𝑗(𝑥):R𝑛0→R linear lower bound of 𝐹𝐾 𝑗(𝑥) 𝑈𝑗(𝑥):R𝑛0→R linear upper bound of 𝐹𝐾 𝑗(𝑥) 𝐿𝑘 𝑗(𝑥):R𝑛0→R linear lower bound of 𝐹𝑘 𝑗(𝑥) 𝑈𝑘 𝑗(𝑥):R𝑛0→R linear upper bound of 𝐹𝑘 𝑗(𝑥) 𝑎𝑘 𝑢,𝑖the slope of linear upper bound 𝑏𝑘𝑢 the intercept of linear upper bound 𝐴𝑘 𝑈,𝑖the slope of𝑈𝑘(𝑋) 𝐵𝑘 𝑈the intercept of 𝑈𝑘(𝑋) 𝑙𝑘 𝑗lower bound of 𝐹𝑘 𝑗(𝑥) 𝑢𝑘 𝑗upper bound of 𝐹𝑘 𝑗(𝑥) 𝑥𝑘∈[𝑙𝑘−1,𝑢𝑘−1] values of neurons at the 𝑘𝑡ℎlayer 𝑓𝑘(𝑥𝑘−1):𝑅𝑛𝑘−1→𝑅𝑛𝑘the function of the 𝑘𝑡ℎlayer 𝛾𝐿,𝑗 global lower bound of 𝐹𝐾 𝑗(𝑥) 𝛾𝑈,𝑗 global upper bound of 𝐹𝐾 𝑗(𝑥) Table 1: Notations experimental results. Section 5 and 6 are threats to validity and related works. Section 7 is the summary of this paper. 2 PRELIMINARIES This section briefly introduces some related concepts and notations to understand our approach. Specially, we present the linear up per and lower bounds of convolutional and fullyconnected layers, which is fundamental to our approach. 2.1 Notations Let𝐹(𝑥):R𝑛0→R𝑛𝐾be a neural network classifier function with (K+1) layers and 𝑥0be an input data point. We use 𝜎(·)to denote all nonlinear functions in 𝐹(𝑥). In our work, nonlinear functions can be Tanh: 𝜎(𝑥)=𝑒𝑥−𝑒−𝑥 𝑒𝑥+𝑒−𝑥, Arctan:𝜎(𝑥)=𝑡𝑎𝑛−1(𝑥), Sigmoid: 𝜎(𝑥)=1 1+𝑒−𝑥, ReLU:𝜎(𝑥)=𝑚𝑎𝑥(0,𝑥), Maxpool:𝜎(𝑥1,···,𝑥𝑛)= 𝑚𝑎𝑥{𝑥1,···,𝑥𝑛}. We use superscripts 𝑘to denote the index of layers, subscripts 𝑢,𝑙to denote upper and lower and subscripts 𝑗(𝑖,𝑞,respectively) denotes the 𝑗𝑡ℎ(𝑖𝑡ℎ,𝑞𝑡ℎ,respectively) Neuron of current layer. The symbols 𝑢𝑗(·),𝑙𝑗(·)denote the linear approxi mation bounding lines/planes of the 𝑗𝑡ℎNeuron at current layer and𝑎,𝑏denotes the slope and intercept of linear approximation function, for example, 𝑢𝑗(𝑥)=𝑎𝑢,𝑗𝑥+𝑏𝑢,𝑗,𝑙𝑗(𝑥)=𝑎𝑙,𝑗𝑥+𝑏𝑙,𝑗. We use superscripts+,−to denote the positive and negative values, for example, 𝑎+=𝑚𝑎𝑥(𝑎,0),𝑎−=𝑚𝑖𝑛(𝑎,0). In our approach, the global linear bounds of 𝐹𝑘(𝑥)are the final linear bounds to compute 𝑙𝑘,𝑢𝑘. Other notations are in Table 1. LetB𝑝(𝑥0,𝜖)denotes𝑥0perturbed within an 𝑙𝑝normed ball with radius 𝜖, that is B𝑝(𝑥0,𝜖)={𝑥|∥𝑥−𝑥0∥𝑝≤𝜖}.𝑡denotes the true label of 𝑥0in𝐹, that is𝑡=𝑎𝑟𝑔𝑚𝑎𝑥𝑖𝐹𝑖(𝑥0) Definition 2.1 (Local robustness bound). if𝜖𝑟(𝜖𝑟≥0)is the local robustness bound of an input 𝑥0in neural network, if and only ifCertifying Robustness of Convolutional Neural Networks with Tight Linear Approximation Conference’17, July 2017, Washington, DC, USA these two conditions (i) 𝑎𝑟𝑔𝑚𝑎𝑥𝑖𝐹𝑖(𝑥)=𝑡,∀𝑥∈B𝑝(𝑥0,𝜖𝑟)and (ii) ∀𝛿>0,∃𝑥𝑎∈B𝑝(𝑥0,𝜖+𝛿)𝑠.𝑡.𝑎𝑟𝑔𝑚𝑎𝑥 𝑖𝐹𝑖(𝑥𝑎)≠𝑐are satisfied. Definition 2.2 (Certified lower bound). if𝜖𝑐𝑒𝑟𝑡is a certified lower bound of input 𝑥0in a neural network, if and only if these two conditions (i) 𝜖𝑐𝑒𝑟𝑡≤𝜖𝑟and (ii)𝑎𝑟𝑔𝑚𝑎𝑥𝑖𝐹𝑖(𝑥)=𝑡,∀𝑥∈B𝑝(𝑥0,𝜖𝑟). That is to say, the local robustness bound of input 𝑥0is the maximum absolute safe radius of 𝑥0, while the certified lower bound of𝑥0is the absolute safe radius of 𝑥0. 2.2 Linear approximation The key idea of the linear approximation technique is to give con strains𝑙(𝑥) ≤𝑓(𝑥) ≤𝑢(𝑥),𝑥∈ [𝑙,𝑢]of every layer’s function 𝑓(𝑥), including nonlinear, fullyconnected and convolutional lay ers. Through propagation, the linear constraints must ensure the value of output will not exceed the upper and lower bounds, which is the key to keeping the soundness of the linear approximation technique. In our work, we fully exploit the CNNs with Maxpool and Sigmoid/Tanh/Arctan functions to give the Tightest linear con straints and make use of the general framework in CNNCert [2] The procedure of computing robustness bounds is a layerby layer process computing from the first hidden layer to the last output layer. To compute global robustness bounds, we need to compute the linear bounds of every layer in CNNs. For details of the whole computing procedure, readers can refer to the work [ 4]. Definition 2.3 (Upper/Lower linear bounds). Let𝑓𝑘 𝑖(𝑥)be the func tion of the𝑖𝑡ℎNeuron in the 𝑘𝑡ℎlayer of neural network 𝐹, with 𝑥∈[𝑙,𝑢]⊂R𝑛,𝑎𝑢,𝑎𝑙,𝑏𝑢,𝑏𝑙∈R𝑛and 𝑢𝑖(𝑥)=𝑎𝑢𝑥+𝑏𝑢,𝑙𝑖(𝑥)=𝑎𝑙𝑥+𝑏𝑙 𝑢𝑖(𝑥)and𝑙𝑖(𝑥)are called linear upper and lower bounds of 𝑓𝑘 𝑖(𝑥) if𝑙𝑖(𝑥)≤𝑓𝑘 𝑖(𝑥)≤𝑢𝑖(𝑥),∀𝑥∈[𝑙,𝑢] It is worth mentioning that 𝑛is determined by the type of 𝑓𝑘 𝑖(𝑥). When𝑓𝑘 𝑖(𝑥)is Maxpool function, 𝑛is equal to the size of the input to be pooled. When 𝑓𝑘 𝑖(𝑥)is ReLU/Sigmoid/Tanh/Arctan function, 𝑛=1. When the𝑘𝑡ℎlayer is a convolutional layer, 𝑛corresponds to the size of the weight filter, and the linear constraints are 𝑢𝑖(𝑥)=𝑤∗𝑥+𝑏,𝑙𝑖(𝑥)=𝑤∗𝑥+𝑏. When the𝑘𝑡ℎlayer is a fullyconnected layer, 𝑛=𝑛𝑘−1and the linear constraints are 𝑢(𝑥)=𝑤𝑥+𝑏,𝑙(𝑥)=𝑤𝑥+𝑏. 2.3 Tightness of linear approximation techniques Linear approximation techniques offer an overapproximation zone of every layer’s function in essence. DeepPoly [ 4] first discovers that minimizing the overapproximation zone can give rise to a more precise certified robustness bound. However, empirical analysis in Newise [ 22] shows that the linear constraints that producing the minimal overapproximation zone could not lead to the most precise robustness result all the time, which motivates us to find the relation between minimizing the overapproximation zone and Tightest robustness bound. Therefore, we introduce the notion of Neuronwise Tightest Linear Bounds from Newise.Definition 2.4 (Neuronwise Tightest). upper linear approxima tion𝑢𝑘 𝑖:×𝑞∈[𝑛𝑘−1][𝑙𝑘−1𝑞,𝑢𝑘−1𝑞] → R, lower linear approxima tion𝑙𝑘 𝑖:×𝑞∈[𝑛𝑘−1][𝑙𝑘−1𝑞,𝑢𝑘−1𝑞]→Ris Neuronwise Tightest iff 𝐷𝑘−1=×𝑞∈[𝑛𝑘−1][𝑙𝑘−1𝑞,𝑢𝑘−1𝑞],∀𝑗∈ [𝑛𝑘+1],∬ 𝐷𝑘−1𝑢𝑘 𝑖(𝑥)𝑑𝑥and∬ 𝐷𝑘−1−𝑙𝑘 𝑖(𝑥)𝑑𝑥are minimal We discover Neuronwise Tightest linear bounds can keep the Tightest through onelayer propagation(proved in appendix). There fore, Neuronwise Tightest can explain why narrowing the over approximation zone can increase certified robustness bounds. Fur thermore, Newise defines the notion of Networkwise Tightest , that is,Networkwise Tightest linear bounds can keep the Tightest through the whole network propagation. Furthermore, Newise provides the Networkwise Tightest linear bounds for monotonous networks, such as nonnegative networks [22]. Definition 2.5 (Networkwise Tightest). global upper linear ap proximation 𝑈𝐾 𝑖:×𝑞∈[𝑛0][𝑥0,𝑞−𝜖,𝑥0,𝑞+𝜖]→R, global lower linear approximation 𝐿𝑖:×𝑞∈[𝑛0][𝑥0,𝑞−𝜖,𝑥0,𝑞+𝜖]→Ris network wise Tightest iff for any different global linear constraints ˆ𝑈𝐾 𝑖,ˆ𝐿𝐾 𝑖, 𝑚𝑎𝑥𝑈𝐾 𝑖(𝑥)≤𝑚𝑎𝑥 ˆ𝑈𝐾 𝑖(𝑥), 𝑚𝑖𝑛𝐿𝐾 𝑖(𝑥)≥𝑚𝑖𝑛ˆ𝐿𝐾 𝑖(𝑥),𝑥∈B𝑝(𝑥0,𝜖) 3 METHODOLOGY "
349,DeepObliviate: A Powerful Charm for Erasing Data Residual Memory in Deep Neural Networks.txt,"Machine unlearning has great significance in guaranteeing model security and
protecting user privacy. Additionally, many legal provisions clearly stipulate
that users have the right to demand model providers to delete their own data
from training set, that is, the right to be forgotten. The naive way of
unlearning data is to retrain the model without it from scratch, which becomes
extremely time and resource consuming at the modern scale of deep neural
networks. Other unlearning approaches by refactoring model or training data
struggle to gain a balance between overhead and model usability.
  In this paper, we propose an approach, dubbed as DeepObliviate, to implement
machine unlearning efficiently, without modifying the normal training mode. Our
approach improves the original training process by storing intermediate models
on the hard disk. Given a data point to unlearn, we first quantify its temporal
residual memory left in stored models. The influenced models will be retrained
and we decide when to terminate the retraining based on the trend of residual
memory on-the-fly. Last, we stitch an unlearned model by combining the
retrained models and uninfluenced models. We extensively evaluate our approach
on five datasets and deep learning models. Compared to the method of retraining
from scratch, our approach can achieve 99.0%, 95.0%, 91.9%, 96.7%, 74.1%
accuracy rates and 66.7$\times$, 75.0$\times$, 33.3$\times$, 29.4$\times$,
13.7$\times$ speedups on the MNIST, SVHN, CIFAR-10, Purchase, and ImageNet
datasets, respectively. Compared to the state-of-the-art unlearning approach,
we improve 5.8% accuracy, 32.5$\times$ prediction speedup, and reach a
comparable retrain speedup under identical settings on average on these
datasets. Additionally, DeepObliviate can also pass the backdoor-based
unlearning verification.","In recent years, deep learning has gained extensive progress in image classiﬁcation, speech recognition, natural language processing, and etc. To handle more complex tasks and vari able decisionmaking scenarios, deep learning models (DLMs) have evolved from LeNet [34] of simple network structures to AlexNet [33], ResNet [22], VGGNet [50], GoogLeNet [54] and other deeper models. Additionally, a massive amount of high quality data is required and collected by model providers from multitudinous data providers. With the increasing de mand of privacy and security, there emerges a new require ment of erasing trained data, dubbed as machine unlearning conventionally, from a welltrained model [10]. In a deep learning task, data providers offer their data and model providers collect the data to train DLMs. The requirements for unlearning data come from both two sources. The ﬁrst and more important requirement comes from data providers, who want their offered data removed from themodel to prevent privacy leak and abuse. DLMs may learn private information from the training data. For instance, an im age of house number likely exposes its owner’s home address, building material, housing color and other information [40]. A medical record that elaborates a patient’s medical history can be revealed from a membership inference attack [49]. Attackers can infer whether a user purchased a product based on shopping records [46]. More importantly, data owners have the legal right of removing their private data from the trained model, that is, the right to be forgotten . There are many bills to guarantee this right such as the General Data Protection Regulation (GDPR) [1], the California Consumer Privacy Act (CCPA) [3] and Amended Act on the Protection of Personal Information (APPI) [2]. They incur a mandatory legal obligation of model providers to unlearn data. The unlearning requirements from model providers include removing polluted or outdated data. On one hand, DLMs are suffering from poisoning attacks [27, 48, 57], where training data is polluted by crafted attacking data samples. Poisonous samples can undermine the usability of DLMs [27, 60], and implant a backdoor into the model [48]. Consequently, model providers need to remove poisonous samples as well as the caused inﬂuences to model. On the other hand, some training data is timesensitive and will become outofdate or even wrong in future, i.e., concept drift [58]. It may degrade the performance of DLMs [56]. Model providers have to eliminate the inﬂuence imposed by the outdated data for usability. Attributed to above requirements, the technology of machine unlearning (hereafter we use deep unlearning to represent the unlearning techniques towards DLMs) appears and in creasingly gains researchers and practitioners’ concern. Con ceptually, a deep unlearning process can be interpreted as eliminating the inﬂuence to model of data points requested for unlearning. If the normal learning performs an “addition” operation, then unlearning performs a “subtraction” operation to the model. A naive unlearning (baseline) can be accom plished by removing the unlearned data, and retraining the remaining data points from scratch [9, 16, 17]. As we all know, it is never trivial to train a DLM at the modern scale of dataset and model. Under such circumstances, the naive unlearning undoubtedly requires a lot of computing resources and retraining time. Frequent unlearning is even unaffordable in reality. Therefore, it motivates researchers to develop fast yet costeffective approaches of deep unlearning. 1arXiv:2105.06209v1  [cs.LG]  13 May 2021D1D2… Dd1DdDd+1… Dd+tDd+t+1… DBUnseen AreaDeleted AreaAffected AreaUnaffected AreaFig. 1: An illustration example for unlearning in D EEP OBLIVIATE , where one data block Diis requested to be deleted from the training dataset fD1; : : : ; D Bg. Prior studies for improving the efﬁciency of machine or deep unlearning roughly fall into two categories: parameters manipulation , that is to directly update model parameters to offset the impact of deleted data on the model [8, 19, 36], and; dataset reorganizing , where model providers reorganize the training data and train one or several new models, and these models work collaboratively for a consensus prediction [9, 10, 17]. Parameters manipulation is effective in machine learning models like Kmeans [37], decision tree [45], and SVM [53]. The approach of model sharding (SISA) [9] can apply to DLMs, but it loses much model accuracy with the increase of shard number. Liu et al. [36] also propose an unlearning method on DLMs, but it only works in federated learning. To this end, we make the ﬁrst attempt to investigate and quantify the inﬂuence to model parameters of unlearned data, termed as temporal residual memory , in an iterative training process. Through an empirical study, we observe that the temporal residual memory is subject to exponential decay which fades at an increasing rate over time (see Section IIIB). Based on this phenomenon, we develop a D EEPOBLIVIATE approach1for a fast yet costeffective deep unlearning. Our approach can offset the impact of unlearning data on the model, reduce retraining overhead efﬁciently, and make no signiﬁcant changes to the original model without introducing additional security risks and maintenance cost. More specif ically, we retain the intermediate models for training each block (detailed deﬁnition in Section IIIA), and divide them into four areas as per temporal residual memory, as shown in Figure 1. The ﬁrst unseen area contains all the models before the arrival of unlearned data. The second deleted area contains the unlearned data. The third affected area covers the models with prominent residual memory which need to be retrained, and the fourth unaffected area is where the residual memory extinguishes. To determine the affected area, we introduce the parameter change vector to measure the residual memory. We adopt the detrended ﬂuctuation analysis [41] to calculate when this memory can be ignored and to terminate our retraining. As a result, an unlearned model can be stitched by reusing unseen and unaffected models, and retraining the affected areas. Our approach is extensively evaluated on multiple datasets, including MNIST [35], SVHN [40], CIFAR10 [32], Pur chase [46], and ImageNet [44], and a variety of DLMs such as LeNet [34], ResNet [22], VGG [50]. When unlearning from 1 data point to 1% data of training set, results show that 1“Obliviate” is a powerful charm to wipe out human’s memory used by Hermione Granger in J. K. Rowling’s Harry Potter series. Our D EEPOBLIVI  ATE aims to effectively unlearn data from deep neural networks.DEEPOBLIVIATE can reduce over 98 95, 9882, 9785, 9694, 9286 (%) retraining costs, and achieve 99.0 96.9, 95.093.1, 91.990.8, 96.795.7, 74.168.7 (%) accuracy on these ﬁve datasets (top1 accuracy for ImageNet), re spectively. Under the identical experiment setting, D EEP OBLIVIATE outperforms SISA [9] by an improvement of 5.8% accuracy, 1.01 retrain speedup, and 32.5 prediction speedup under the same storage overhead on average on these datasets. In addition, our approach can apply to varying unlearning scenarios and achieve superior results to date in deep unlearning. It takes only 19% of naive unlearning efforts for multiple data deletions, and supports the operation of data deletion at any time. We also adopt the backdoorbased veriﬁcation [51] to guarantee that our method has already unlearned the data. Contributions . We make the following contributions. We make the ﬁrst attempt to quantify the temporal residual memory of unlearned data in a gradientbased training. We measure this residual memory through the difference vector between parameter change vectors on two models. It reﬂects how training data impacts on model parameters, and how this inﬂuence changes over time. We develop a simple, fast and cost effective approach, DEEPOBLIVIATE , that makes no signiﬁcant changes to original models, requires less additional computation, and can handle multiple unlearning requests such as single, bulk, and frequent deletions. We conduct an extensive evaluation with ﬁve deep learn ing models on ﬁve datasets. D EEPOBLIVIATE provides a superior ability of unlearning data compared to thestate oftheart in the same setting. Additionally, we implement a backdoorbased unlearning veriﬁcation that further proves the successfulness of deep unlearning. II. B ACKGROUND In this section, we give a brief introduction of machine unlearning and four evaluation criteria. A. Machine Unlearning Different from machine learning that builds a mathematical model from data, machine unlearning can be regarded as a reverse process that drops data from the model. It is one of bionic technologies to imitate the function of the human brain. Humans have an efﬁcient way to unlearn memory, especially unpleasant memory [52], as the experience that is seldom or never recalled is more likely to be forgotten. It used to be a defect for artiﬁcial neural networks since forgetting the previously seen information can lead to “catastrophic inter ference” [39, 43]. The defect of amnesia in neural networks can be countered by technologies like latent learning [20] and selfrefreshing memory [6]. However, in this study, machine unlearning is one type of ability of DNNs. It is one subjective and intentional behavior of model providers to eliminate all the possible effects brought by speciﬁc data on the model. Since the training data undergoes a number of complicated and even nondeterministic transformations before shaping a 2model, it is not easy at all to accurately measure the effects of part of data. The difﬁculty is further increased in the scenario of DNNs. Cook and Weisberg [13, 14] ﬁrst propose inﬂuence functions to approximate the leaveoneout cross validation estimation of prediction variance. Koh and Liang use an inﬂuence function to correlate training data with the corresponding predictions [30]. To the best of our knowledge, there is no quantitative analysis yet of the inﬂuence exerted by training data to the model in a machine unlearning task. B. Formalization of Machine Unlearning Given a training process, we assume D:fx1; x2; : : : ; x ng as the training data, Fas a speciﬁc learning algorithm, and Mas the trained model. As such, we have F:D!M, or F(D) =Mfor simplicity, which means the model Mis built with the learning algorithm Fon dataset D. A deep unlearning operation on deleted xd(xd2D) can be represented as U(xd;M). Let M0be the unlearned model, and we get M0=U(xd;M). Deﬁnition 1: (Naive Unlearning) Supposing the model never sees the data point xd, and thereby performs the retraining on the remaining n"
241,Deep Speaker Embedding Learning with Multi-Level Pooling for Text-Independent Speaker Verification.txt,"This paper aims to improve the widely used deep speaker embedding x-vector
model. We propose the following improvements: (1) a hybrid neural network
structure using both time delay neural network (TDNN) and long short-term
memory neural networks (LSTM) to generate complementary speaker information at
different levels; (2) a multi-level pooling strategy to collect speaker
information from both TDNN and LSTM layers; (3) a regularization scheme on the
speaker embedding extraction layer to make the extracted embeddings suitable
for the following fusion step. The synergy of these improvements are shown on
the NIST SRE 2016 eval test (with a 19% EER reduction) and SRE 2018 dev test
(with a 9% EER reduction), as well as more than 10% DCF scores reduction on
these two test sets over the x-vector baseline.","Speaker veriﬁcation (SV)[1] is one of the key components for human machine interface and initially was widely used in per son identiﬁcation for security purposes. Nowadays with in telligent speech assistants such as Alexa, Google Home, Siri and Cortana being used in home environments as well as on smartphones, the demand for SV technology is rising. This is especially true for robust speaker veriﬁcation in challenging acoustic conditions and different population of speakers. The speaker veriﬁcation problem usually falls into two categories: textdependent (TD) SV and textindependent (TI) SV . In the TD SV system, the transcriptions for the test utterances and enrollment utterances are the same and usually limited to a small set. In the TI SV system, there is no constraint on tran scriptions for both test and enrollment utterances. Hence the TI case is more difﬁcult than the TD case due to larger varia tions introduced by different utterance transcriptions and du ration. In this study, we will focus on the more challenging TI speaker veriﬁcation system. Recently, more attention has been drawn to the use of deep neural networks (DNN) to generate speaker embeddingrepresentations [2] [3] [4]. These deep speaker embedding systems are shown to have large improvements over the i vector [5] based methods, especially the recently proposed xvector system with data augmentation [4]. The DNN based speaker embedding extraction system usually consists of three components: frame level feature processing, utterance (speaker) level feature processing and training loss. Frame level processing deals with local short span of acoustic fea tures. It can be done via recurrent neural networks [2] or con volutional neural networks [3][6]. Utterance level processing forms speaker representation based on the frame level output. A pooling layer is used to gather frame level information to form utterance level representation. Methods such as statisti cal pooling [3], max pooling[7], attentive statistical pooling [8], multiheaded attentive statistical pooling [9] are popular choices. Cross entropy and triplet loss are two widely used training losses. Cross entropy based methods are focused on reducing the confusion for all speakers in the training data [3], while triplet loss based methods [10][11][12][13] are focused on increasing the margin between similar speakers. In this paper, we propose a novel deep speaker embed ding learning framework to improve upon the xvector sys tem. We ﬁrst add LSTM layers to the xvector’s TDNN struc ture, because sequential modeling of an utterance would gen erate different speaker information from that of TDNN. In order to aggregate these different information, we propose a multilevel pooling strategy to fuse different information from the different frame level models, one from TDNN and one from LSTM. We also add a regularization term on the speaker embedding extraction layer in order to make the sys tem output more suitable for the backend processing. Over all, the proposed new speaker embedding system improves the equal error rate (EER) by 19% and the detection cost function(DCF)[1] score by 12%, compared to the previous xvector baseline on the NIST SRE 2016 eval test. This is to our knowledge the lowest EERs (9.2% on Tagalog and 3.1% on Cantonese) on SRE 2016 eval test in publications so far. The rest of this paper is organized as follows. Section 2 brieﬂy describes prior work, including the baseline xvector system and related work. The proposed new system is introarXiv:1902.07821v1  [cs.CL]  21 Feb 2019duced in Section 3 on gathering speaker information from dif ferent modeling levels. The experimental set up, results and analysis are described in Section 4. Finally, the conclusion is given at Section 5. 2. PRIOR WORK 2.1. Baseline xvector System Figure 1 depicts the deep neural network conﬁguration for x vector system [3][4]. The blue, yellow and green blocks rep resent the frame level, utterance level and training loss used in the system training. Black arrows indicate a ReLU activation function. Batch normalization layers are added in two adja cent layers, while the red arrows indicate there is no nonlinear mapping added between layers. Fig. 1 . Model structure of baseline xvector system. The frame level model consists of three TDNN layers to extract speaker information at the frame level. XYZ in the block represents the number of ﬁlters, ﬁlter window size and dilation number at this layer. At the utterance level, high or der stats pooling methods such statistical pooling or attentive statistical pooling can be used. The output (concatenation of the mean and standard deviation vectors) is fed into another three fully connected layers. Speaker embeddings are then extracted from the output of the ﬁrst linear projection layer. Cross entropy loss is used to train the system and reduce the confusion between all speakers in the training set. Once speaker embeddings are extracted, LDA and PLDA [14] are used as backend scoring, as in the standard Kaldi xvector recipe. 2.2. Related work "
382,Attention based Writer Independent Handwriting Verification.txt,"The task of writer verification is to provide a likelihood score for whether
the queried and known handwritten image samples belong to the same writer or
not. Such a task calls for the neural network to make it's outcome
interpretable, i.e. provide a view into the network's decision making process.
We implement and integrate cross-attention and soft-attention mechanisms to
capture the highly correlated and salient points in feature space of 2D inputs.
The attention maps serve as an explanation premise for the network's output
likelihood score. The attention mechanism also allows the network to focus more
on relevant areas of the input, thus improving the classification performance.
Our proposed approach achieves a precision of 86\% for detecting intra-writer
cases in CEDAR cursive ""AND"" dataset. Furthermore, we generate meaningful
explanations for the provided decision by extracting attention maps from
multiple levels of the network.","Writer independent veriﬁcation is the task to measure the similarity of two given handwritten samples as how likely is it that the samples were written by the same, without having any knowledge of writers identity. There have been many efforts in this ﬁeld to provide an automated hint to streamline the job of manual handwriting examiners, making this an interesting research problem. A general intuition is that samples from a single source tend to be similar while samples from different sources tend to show bigger variances. The premise for ﬁnding unique characteristics is based on the hypothesis that every individual has a unique way of writing [1]. Further studies show that two different writers may also happen to have a similar writing style. This makes the problem of handwriting veriﬁcation challenging. With the advent of automated pattern learning methods; es pecially with models that can be trained to focus on key areas, it is possible to design a robust system to assist a Forensic Document Examiner (FDE). In this work we propose attention based approaches for the task of handwriting veriﬁcation. We produce two kind of attention maps1which highlights (i) the important corresponding pixel regions between two images that the network deemed similar (ii) the high correlated pixel regions, in the feature space of given samples, that the network attends to provide its decision. Such visualizations render the desired interpretability for forensic veriﬁcation systems. 1Code is publicly available on: https://github.com/mshaikh2/AttentionHandwritingVeriﬁcationII. R ELATED WORK "
232,Specialized Re-Ranking: A Novel Retrieval-Verification Framework for Cloth Changing Person Re-Identification.txt,"Cloth changing person re-identification(Re-ID) can work under more
complicated scenarios with higher security than normal Re-ID and biometric
techniques and is therefore extremely valuable in applications. Meanwhile,
higher flexibility in appearance always leads to more similar-looking confusing
images, which is the weakness of the widely used retrieval methods. In this
work, we shed light on how to handle these similar images. Specifically, we
propose a novel retrieval-verification framework. Given an image, the retrieval
module can search for similar images quickly. Our proposed verification network
will then compare the input image and the candidate images by contrasting those
local details and give a similarity score. An innovative ranking strategy is
also introduced to take a good balance between retrieval and verification
results. Comprehensive experiments are conducted to show the effectiveness of
our framework and its capability in improving the state-of-the-art methods
remarkably on both synthetic and realistic datasets.","PERSON reidentiﬁcation (ReID) aims to ﬁnd out images of the same person across multiple images taken in different scenes. Essentially, ReID is a kind of unobtrusive identiﬁcation. It neither collects detailed biological traits such as ﬁngerprints and facial appearance nor requires the cooperation of users, which makes differences from traditional identiﬁcation methods. It is also an extremely valuable computer vision task for its use in public safety and security area such as person localization, multiobject tracking, and video surveillance. Recently proposed benchmarks [1, 2] and works [3, 4, 5] have greatly promoted the research in this community. R. Zhang, Y. Fang, and H. Kato are with the Division of Information Science, Nara Institute of Science and Technology, Nara, Japan (Email:zhang.renjie.zq0@is.naist.jp; fang.yu.fq0@is.naist.jp; kato@is.naist.jp). H. Song is with the Department of Mathematical Informatics, Tokyo University, Japan(email: songh@g.ecc.utokyo.ac.jp). F. Wan and Y. Fu are with the School of Data Science, Fudan University, Shanghai, China((email: fbwan18@fudan.edu.cn; yan weifu@fudan.edu.cn). Y. Wu is with the ARC Lab, Tencent PCG, Shenzhen, China (email: dylanywu@tencent.com).arXiv:2210.03592v1  [cs.CV]  7 Oct 20222 However, these methods are limited by a strong assumption that the same people do not change their clothes. We categorize such models as the clothconsistent ReID models. In contrast, cloth changing ReID models aim to recognize the person of interest under drastic appearance changes, which is another important component of a ReID system. Compared with the clothconsistent ReID models, cloth changing ReID models can work in more complicated scenarios for a longer time span while with a lower risk of data abuse since identiﬁcation by human eyes is also harder. These advantages make cloth changing ReID models more effective and practical in various applications. On the other hand, higher appearance ﬂexibility also inquires models to make the correct selection from a large number of similar candidate images. Since the person of interest is no longer limited by his/her color appearance, more candidate images that may share similar gestures, viewpoints, or body shapes are collected in the dataset. Distinguishing these similar images is always a tough task even by humans, not to mention a model that is not specially designed for this. Nevertheless, we ﬁnd that if we can learn how these similar images differ, it will help the decisionmaking among vast similar images. Unfortunately, many researchers have underestimated the value of this task as well as the importance of details in this problem. Inspired by this, we try to solve this potential and challenging problem by better analyzing those local details. The biggest challenge in cloth changing ReID is that there are no features as intuitive and decisive as color appearance. The system should pay more attention to other features like body shapes, faces, and gestures than before. Relying too much on one speciﬁc type of features will lead to a biased result. Generally speaking, such auxiliary features are not that representative and can only handle some easy works[6, 7]. Following this idea, many cloth changing ReID works[6, 8, 9] devotes their efforts to incorporating various auxiliary features, especially body shape features. Unfortunately, these works fail to use these features in a correct manner. Almost all these works take auxiliary features as a kind of general features(i.e. features given by retrieval models). Though such design is common in clothconsistent ReID, it helps little when it comes to cloth changing ReID(see Table 1 and Table 2). In clothconsistent ReID, candidate images usually share similar color appearances but with remarkable differences in other aspects such as body shape and accessories. In this case, auxiliary features can help eliminate some unwanted images and bring a better result. In contrast, candidate images in cloth changing ReID datasets are always hard to discern even by human eyes. They usually share similar global features with just some inconspicuous distinctions in local parts. Auxiliary features are typically not representative enough for discerning these hard samples. To solve this problem, the key point is to reactivate auxiliary features. We notice that the malfunction of auxiliary features is caused by the difference in data distribution. For example, supposing auxiliary features are trained to tell whether a person is strong or not, but when ana lyzing the similar candidate images, we are comparing two strong people with tiny differences. Such difference in data distribution causes the performance reduction of auxiliary features. To re activate auxiliary features, we should make them representative enough for distinguishing those3 hard samples. One nature idea is to learn auxiliary features that can discern similar images. To reach this target, one should train auxiliary features to be highgranularity. On the other hand, learning such allmighty features with limited samples is extremely difﬁcult. This restrictions forces us to seek other possibilities. Alternatively, we propose verifying similar images through auxiliary features, which mainly differs from the former method in two aspects: 1). The target is changed to verify whether two images are from the same person. This is a more directive way of fulﬁlling the responsibility of auxiliary features, i.e. eliminating unwanted images through speciﬁc features. The veriﬁcation score is adopted for inference. Compared with feature distance, the veriﬁcation score given by the network can better balance the contribution of different auxiliary features and is thus more stable and reliable. 2). Auxiliary features are jointly decided by two input images after adequate information exchange. Such a setting enables the model to ﬂexibly adjust its expression according to different combinations of the input images. In other words, features of the same image are dynamically changing according to different images to be compared with. We categorize this kind of features as specialized features. With the help of specialized features, the system can actively and precisely detect the important clues in the input image pair and produce more representative features. In this work, we introduce a novel retrievalveriﬁcation scheme for solving this problem. Instead of using general features extracted by ReID models to directly give a rank, we utilize these features for ﬁnding a shot list of reliable candidate images in the retrieval part of the scheme. These candidate images typically have inconspicuous while decisive differences in various points. Then, a specially designed veriﬁcation network is proposed to learn the specialized features of these candidate images. Different from the retrieval part, the veriﬁcation network takes a pair of candidate image and query image as its inputs and outputs a similarity score of the image pair. Such a setting forces the veriﬁcation network to automatically detect, judge, and balance the similarity of detailed local image parts. The combination of image retrieval and veriﬁcation makes proper use of both general features and specialized features. Intuitively, general features given by the retrieval model provide a basic sketch to the person of interest, while distinctive details in the image are in fact the ones decisive. In contrast, specialized features are induced jointly by two input images and can better represent the difference between the images. Therefore, we utilize general features in the retrieval part as a highspeed and highquality similar image ﬁltering. To further distinguish these similar images, we apply an image veriﬁcation network to explore the specialized features of those inconspicuous yet decisive differences in local parts. To better contrast the candidate images with the query images, hint locating becomes the key to a successful veriﬁcation network. In this work, we incorporate human partbased segmentation to parse the image. By parsing the image into human body parts, parttopart matching can be accomplished automatically. This greatly help narrow the area for hint searching and improve the efﬁciency of comparison. Another biggest advantage of human partbased segmentation is its pixellevel accuracy. This allows us to analyze the input image pairs from the viewpoint of part shape, which is overlooked by many cloth changing ReID works[10, 11, 12] but is actually very4 important for hard sample identiﬁcation. Furthermore, highorder features including the length ratio of limbs and body proportions, are also extremely crucial for image veriﬁcation. Fusing the above ideas, we propose a novel veriﬁcation network named Local Clues oriented Veriﬁcation Network(LCVN) for similar image veriﬁcation. The structure of our LCVN is shown in Fig. 2. We also discuss another key point for establishing a successful retrievalveriﬁcation frame work, which is to know when to stop. Actually, building an allmighty image veriﬁcation network is almost impractical, as it works on an exponentially boosted input space. Without enough data and computing power, it is impossible to compare all images for similarity calculation. Including more ”dissimilar” images usually leads to more meaningless comparisons and drastic performance drops. To best exert the effectiveness of our veriﬁcation network, we propose a new ranking strategy for our retrievalveriﬁcation framework, which takes a good balance between comparing more images and reducing the risk of performance drops. In summary, our contributions are listed as follows: (1). We propose a novel twostep retrievalveriﬁcation strategy to use metric learning results for candidate image ﬁltering and another professional veriﬁcation network for similar image judgment. To the best of our knowledge, this is the ﬁrst work that learning specialized features for solving the similar images problem in clothchanging ReID. (2). A specially designed image veriﬁcation network named Local Clues oriented Veriﬁcation Network(LCVN) is proposed to learn the specialized features of similar images in cloth changing ReID. It takes both visual clues and interparts relationships for giving comprehensive compar isons to similar image pairs. Furthermore, we also introduce a new ranking strategy that can best exert the potential of the veriﬁcation module. (3). Comprehensive experiments are given to verify the effectiveness of our model on both the VCClothes[9] dataset and the PRCC[11] dataset. Our method improves the stateofthe art methods by a large margin on both datasets, showing the effectiveness of our proposed framework. 2 R ELATED WORK "
