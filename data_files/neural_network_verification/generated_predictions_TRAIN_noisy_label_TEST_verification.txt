This paper presents McNetKAT, a new tool for reasoning about probabilistic network programs written in the guarded and history-free fragment of Probabilistic NetKAT (ProbNetKAT). ProbNetKAT is an expressive programming language capable of modeling a variety of probabilistic behaviors and properties including randomized routing, uncertainty about demands, and failures. Although state-of-the-art tools can easily handle deterministic net works with hundreds of thousands of nodes, state-of-the-art tools for probabilistic networks are currently orders of magnitude behind.
Deep learning has been explored for a wide range of different financial services applications, including credit card default prediction, exchange rate prediction, and anti-money laundering. However, one question that often arises as a barrier to adopting deep learning for financial services in real-world practice is whether the developed financial deep learning models are fair in their predictions. While explorations into fairness for financial deep learning have been conducted through aspects such as explainability, a fundamental aspect of fairness that has not been explored in financial deep learning is the concept of trust. In this study, we conduct proof-of-
This paper presents a novel technique for modular verification of control planes. We start from an existing general model for distributed routing, the Stable Routing Problem (SRP). In an SRP, each node of the network exchanges routes with its neighbors to compute a locally-stable solution. We then extend this model to include a new set of input nodes and output nodes, called “open SRPs”. We then implement this extension as a Kirigami extension to the NV network verification language and tool. We present a new technique for modular verification of the control plane
Automatic speaker verification (ASV) is a widely used biometric method that verifies the speaker identity of an input utterance with enrollment data. With the development of deep neural networks (DNN), DNN-based ASV systems have been successfully applied to both text-dependent and text-independent tasks. The state-of-the-art DNN-based ASV systems can be broadly categorized into two groups: stage-wise approaches and fully end-to-end (E2E) approaches. The stage-wise approaches mainly consist of a speaker encoder for extractings speaker
Handwritten signature is a biometric authentication method widely used for personal documents and legal contract valida tions. However, this approach can be expensive and time-consuming, given the amount of data accumulated in institutions that use a handwritten signature as a means of identification and authentication. Several approaches have been developed in the field of machine learning and statistical methods to perform the signature detection and verification tasks automatically. However, such approaches use public databases, which present images with a light background and dark signatures. On the other hand, document images captured by smartphones are usually presented with distortions and background noise
Payment Channel Networks are a second layer on top of a blockchain that processes transactions without writing each transaction to the blockchain. The security model for payment channels requires that honest parties cannot loose their funds even if all other users behave adversarially. To avoid financial loss caused by design flaws in a payment channel protocol, it should be verified that the protocol is secure. In this paper, we analyze the security of the protocol of the Lightning Network, a payment channel network for Bitcoin. To concretize, we formalize an ideal functionality of a
Network policy enforcement has been and continues to be a challenging and error-prone task. Recent efforts on network testing and verification (e.g., [26,42,43,44]) offer a promising alternative to existing expensive and manual debugging efforts. However, there are fundamental gaps between the intent of network operators and the capabilities of existing network checking mechanisms. Specifically, we identify three key limitations with respect to expressiveness and scalability (2): Packets are cumbersome and insufficient: While located packets, they miss key stateful semantics (e.
Deep neural network based fingerprint recognition has achieved tremendous success in various applications. However, the choice of data representation plays a critical role in determining the success of a machine learning model because different representations can highlight and/or support different factors of variation underlying the data. In this work, we propose a state-of-the-art (SOTA) deep neural network based fingerprint recognition model called DeepPrint (DP) that can learn an ensemble of five fingerprint representations from a single image. The DP architecture is based on a combination of domain knowledge (minu
Quantum Key Distribution (QKD) is the study of securely distributing keys between two parties (say, Alice and Bob) using properties of photons (quanta of light). The major application for LRNG in QKD comes into the picture for prepare-and-measure (P&M) protocols. In this work, we aim to obtain a LRNG bit stream, which is implementable on low cost, low powered FPGA boards suitable for BB84 protocol. The bit stream is varied for differing parameters, say input seed value and post-processed
The Beacon Chain is the backbone component of Ethereum 2.0. It is responsible for enforcing consensus among the nodes participating in the network. The Beacon Chain is the reference implementation for any Beacon Chain client implementer. The reference implementation is written in Python and provides a detailed operational description of the state machine each Beacon Chain participant (node) must implement. As a result, inaccuracies, ambiguities, or bugs in the reference implementation will lead to erroneous and/buggy clients that could compromise the integrity, or performance of
Deep neural networks (DNNs) have been used to solve a wide range of computationally challenging problems, including safety critical tasks in aviation, healthcare, and autonomous driving. However, the Achilles heel of DNNs lies in generalizing their predictions from the finite training set to an infinite input domain. In this work, we propose a novel proof production mechanism for a broad family of search-based DNN verifiers. Whenever the search procedure returns UNSAT (indicating that no counterexample exists), our mechanism produces a proof certificate that can be easily checked using simple,
Quantum computers are a promising tool for many applications, including machine learning. However, the application of quantum computers in real-world problems is less known so far due to the current limitation of computational resource for quantum computers and their simulators. In this work, we consider a real-world financial problem, namely prediction of stock returns, employing quantum and quantuminspired machine learning algorithms. We conduct a backtesting experiment on a set of stocks in the Japanese stock market, and compare the performance of quantum neural network, tensor network, and neural network. We perform backtesting over 10 years
Deep neural networks (DNNs) have achieved unprecedented success in many complex tasks. However, they are also vulnerable to various attacks, such as adversarial attacks. To verify a DNN, we need to estimate the change of the prediction given an input perturbation. To achieve this, we need to over-approximate the computation of the network. To achieve this, we propose a symbolic reasoning framework for DNN verification. This framework consists of symbolic domains and quadratic relations. Symbolic domains have flexible semantics, and compositional, and compositional. We will
In this paper, we study the problems of reachable set estimation and safety verification for closed-loop systems equipped with neural network controllers. The key step is to soundly estimate the output set of a feedforward neural network controller. Then, as the estimated output set is the input set to the plant described by an ordinary difference/differential equation (ODE), reachable set estimation for the closed-loop system can be obtained by employing sophisticated reachability analysis methods for ODE models. The reachable set estimation problem is solved by using a finite number of hyper rectangular sets to
Hypofractionated lung radiotherapy is an increasingly used modality for the treatment of primary and secondary lung cancers. However, the precision requirement of this therapy is high. Consequently, it is absolutely critical to effectively monitor the target to ensure maximal irradiation of the tumor with minimal irradiation of surrounding normal tissue. The major uncertainty in treating lung cancer is the respiratory lung tumor motion, which can be clinically significant for some patients (e.g., of the order of 2 – 3 cm). During treatment, a sequence of cine EPID images was developed to measure displacement vectors
Graph neural networks (GNNs) are a generalization of neural networks that operate on graph structured data, typically in the form of an adjacency matrix or graph laplacian, and feature vectors defined for the nodes. They have found success in tasks such as link prediction, node classification, and graph classification. Recent theoretical work has elucidated properties on their depth (Oono & Suzuki, 2019), architectural alignment with algorithms (Xu et al., 2019), and their discriminative power (Grathwohl Grathwohl,
Reachability analysis is a key property of control systems where the plant model is given as a nonlinear ordinary differential equation (ODE) and the controller is implemented by a neural network. In principle, reachability analysis for NNCS can be implemented by chaining two off-the-shelf tools for analyzing the ODE and the neural network. The output set of one tool is the input set of the other tool, and this process is repeated for each control cycle. While correct, such an approach often yields sets that are too conservative to be useful in practice. This incurs an app
Deep Neural Networks (DNNs) have emerged as an effective approach for solving challenging real-world problems. However, they can also have "bugs", e.g., producing unexpected results on inputs that are different from those in training data, and be attacked, e.g., small perturbations to the inputs by a malicious adversary or even sensorial imperfections result in misclassification. To address this question, researchers have developed a variety of techniques and tools to verify DNNs. However, the state-of-of-of-
Deep neural networks (DNNs) have been developed for a variety of tasks, including malware detection, abnormal network activity detection, and self-driving cars. While the accuracy of DNNs has greatly improved, they are susceptible to adversarial examples, which pose a number of safety-critical challenges. In this paper, we propose a novel game-based approach for safety verification of DNNs. We consider two pointwise robustness problems, referred to as the maximum safe radius problem and feature robustness problem, respectively. We show that the
Deep neural networks (DNNs) have been widely adopted in many applications for their high effectiveness and efficiency. However, recent studies revealed that adversaries can obtain a functionsimilar copy model of the well-trained victim model to ‘steal’ it. This attack is called model stealing. Currently, there are some methods to defend against model stealing, including the active defenses and verification-based defenses. However, active defenses may lead to poor performance of the victim model and could even be bypassed by advanced adaptive attacks; while verification-based defenses introduce some
In this paper, we propose a novel approach to tackle the generic reachability problem, which, for a given DNN, an input subspace and a function over the outputs of the network, computes the upper and lower bounds over the values of the function. The function is generic, with the only requirement that it is Lipschitz continuous. Existing approaches for analysing DNNs with a guarantee suffer from two major weaknesses. Firstly, their subjects of study are restricted. More specifically, they can only work with linear transformations, and cannot scale to work with large networks.
Deep neural networks (DNNs) are a powerful tool for solving a variety of challenging problems, including image classification, function approximation, and natural language translation. However, their behavior can be unpredictable due to slight perturbations in their inputs (i.e., adversarial perturbations). In this paper, we introduce the NNV (Neural Network Verification) tool, which is a software framework that performs set-based verification for DNNs and learning-enabled CPS, known collotopes. NNV declares a DNN or
Spinal fusion surgery is a high-risk surgery with high economic burden and high risk of cortical breach. Despite substantial improvements in operative technique, intraoperative 3D conebeam CT (CBCT) imaging is not currently being used for spinal fusion, because compared to CT, Carm CBCT images suffer from substantially stronger metal artifacts around the highly-attenuating titanium implants. This compromises the value of intraoperative CBCT for assessing cortical breach. In this work, we propose to extend the traditional shortscan trajectory by autonomously adjusting
This paper presents two fault injection (FI) frameworks for TensorFlow. The frameworks allow the user to model soft errors during the execution of a neural network, analyze the effects of such errors, and identify the most critical parts of the network. The results show which erroneous operations or layers have the largest impact on the accuracy of the neural network. The FI frameworks are designed in a modular way and can therefore easily be expanded with additional features. The FI frameworks are designed in a modular way and can be easily extended with additional features.
Topology design optimization is a mathematical method to solve a material layout problem constrained for a given design domain, loading, and boundary conditions. The method determines the optimal distribution of material such that the structure has desired properties (e.g. minimizing the compliance of structure) while satisfying the design constraints. The conventional topology optimization approach is in general impractical or computationally unattainable for real world applications due to over thousands of design iterations are often required for just a few design variables. To this end, we have developed a new conditional Wasserstein generative adversarial network (
Face anti-spoofing is a challenging problem in computer vision especially in remote scenarios without specific hardware equipped. Recent methods mainly rely on hardwares embedded with structured light, light field or LIDAR to reconstruct 3D shape. However, these methods suffer from the lack of strong prior of object shapes, leading to the lack of generalization capability. In this paper, we propose a simple yet effective face anti-spoofing system termed Aurora Guard (AG). Its principle is using light reflection to disentangle two auxiliary information, i.
Deep neural network (DNN) is a powerful machine learning tool that has been widely used in computer vision, natural language processing and data mining tasks. However, successful applications of DNN come at the cost of the expensive training process and the need to protect the valuable training data and the trained DNN models from being illegally copied, redistributed or misused. To address this need, a secure federated learning (SFL) framework has been proposed to collaboratively train a federated deep neural network (FedDNN) without sharing of private training data. However, this requirement has
Convolutional neural networks (CNNs) have achieved state-of-the-art performance in various computer vision tasks, such as object detection, detection, segmentation, and so on. In the field of face verification, the most common pipeline involves face detection, landmark detection, feature extraction, and feature comparison. In the feature comparison step, the cosine similarity or equivalently L2normalized Euclidean distance is used to measure the similarities between features. In practice, the inner product without normalization is the most widely-used similarity measure during training. To illustrate this, we performed an experiment
In this work, we address the challenge of cross-resolution face recognition with a novel metric learning approach called octuplet loss finetuning. This objective constitutes fine-tuning an existing network to increase its robustness against image resolution while maintaining its performance in controlled scenarios. We propose a novel loss function called octuplet loss that leverages four triplet loss terms to capture the relationship between high-resolution and low-resolution images and identity labels. We demonstrate that fine-tuning several state-of-of-of-art networks with our
Deep neural networks (DNNs) are a powerful tool to identify patterns in large amounts of data and to make predictions on new, previously unseen data. However, relying solely on the predictions made by a DNN component might be dangerous, as there is always some uncertainty about the correctness of the prediction. The uncertainty intrinsic with DNNs is either caused by entropy in the input or by inadequate training. In this paper, we propose that Deep Learning based Systems (DLS) include a supervisor, which monitors the DNN uncertainty for any given input at runtime
In this work, we propose a hybrid quantum-classical algorithm for verifying the robustness of ReLU neural networks. The verification problem is a mixed-integer program (MIP) with a quadratic unconstrained binary optimization (QUBO) and a linear program (LP). The MIP is then partitioned into a quadratic unconstrained binary optimization (QUBO) and a linear program (LP). The LP generates cuts for the QUBO which is solved by a quantum computer or a quantum computer. We derive bounds for the
Convolutional neural networks (CNNs) have been widely used in machine learning and artificial intelligence. However, the standard approach to training a CNN is based on solving a nonconvex optimization problem that is known to be NP-hard. In practice, researchers use some form of stochastic gradient method, in which gradients are computed via backpropagation. However, this approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity, and (i
Deep Learning (DL) is a class of machine learning models that require a substantial quantity of computational resources (often in a distributed fashion) for training. At the same time, the DL model building process is increasingly outsourced to the cloud. However, the majority of these investigations are limited in that they only address the computational integrity at the test phase while depending on the application context. To address these limitations, we introduce GOAT (Goat for Integrity-preserving Learning as a Service), a framework for integrity-preserving learning as a service that provides integrity guarantees in outsourced DL
Feedforward deep neural networks have been successfully used for controlling physical systems, such as self-driving cars and unmanned aerial vehicles. In the case of a safety-critical system, such as a self-driving car, a particular effort needs to be made to demonstrate its safety. More precisely, one has to show evidence that the system fulfills a set of safety requirements, such as, in aeronautics, A catastrophic failure shall occur with a probability less than 10. In this paper, we propose a
Face aging has raised considerable attentions in computer vision and machine learning communities recently. However, the synthesized results in these previous approaches are still far from perfect due to various challeng ing factors, such as heredity, living styles, etc. In this paper, we propose a novel face aging progression method, namely the SDAP. This method directly synthesizes a face to the target age using the relation ships between training images and their corresponding age labels. In contrast, the step-by-step approach aging transform embedd between two consecutive development stages. In addition
We propose a novel cross-modal image fusion method based on the human visual perception system. Our method combines the enhanced vision system (EVS) and synthetic vision system (SVS) images in CVS image fusion data set to achieve a good fusion effect for image details. Our method uses the effective combination of attention mechanism and nonlinear neural network to simulate the feature selection characteristics and nonlinear combination characteristics of human visual perception system. We also use the effective combination of attention mechanism and nonlinear neural network to simulate the nonlinear combination of attention mechanism of human visual perception system. We
Recurrent neural networks (RNNs) are widely used to model long-term dependencies in lengthy sequential signals. However, prior work has shown the susceptibility of RNNs to adversarial perturbations of its inputs, exposing their safe deployment. As a result, certifying the robustness of recurrent architectures is critical for their safe deployment. In this paper, we address both these challenges and present the first precise and scalable verifier for recurrent neural networks based on abstract interpretation. Our method, called Prover, we first compute a polyhedral abstraction
Deep neural networks are susceptible to small perturbations applied to their inputs, i.e. it is possible to misguide the model output by applying a designed perturbation to a given input. To overcome such vulnerability many approaches have been previously employed. To reduce the vulnerability of the neural networks, some formal procedures have been proposed to certify or guarantee that the model behaves as expected under some circumstances or within a specied domain region. However, these procedures remained vulnerable to adversarial attacks. In this paper, we propose three novel reachability algorithms: APNM and
Full-waveform inversion (FWI) is a high resolution inversion method in exploration seismology. It is commonly used for estimating subsurface velocity structure based on seismic waves recorded at the surface. Although FWI can achieve high accuracy when the full seismic waveform is matched, the nonlinearity of the objective function poses a challenge to the optimization process. One solution is to recover or predict the missing low frequency content of the seismic waveform data, such as through envelope inversion, sparse blind deconvolution, l2norm penalty (total variation),
Handwriting evidences provided by expert forensic document examiners (FDE) has long been admissible in the court of law. The premise for finding unique characteristics of the handwritten samples is based on the hypothesis that every individual has a unique way of writing. The task of handwriting verification is to find a measure of confidence whether the questioned and known handwritten samples are written by the same writer or different writer. The output of handwriting verification systems is difficult to interpret because the system does not provide explanations for the decision. We propose a new approach wherein
In this paper, we are interested in identifying fake news by testing location consistency  whether an image is taken at a claimed location. Currently, existing methods rely on GPS estimation to make this decision, but they are unsuitable for the context of image-to-GPS verification because the critical camera information like shooting angle and focal length is unknown, and this raises a problem to retrieve an appropriate reference image to compare against the query image. In this paper, we propose a novel Bottom-Up Pattern Matching (BUPM) based verification network to compare a query image and a
The problem of person re-identification (re-ID) is a challenging task that requires the identification of a person from a large-scale dataset. The problem is usually viewed as an image retrieval problem, which matches pedestrians from different cameras. Recent works have shown that the convolutional neural network (CNN) has the potential to learn state-of-the-art feature embeddings or deep metrics. However, the two models are different in terms of input, feature extraction and loss function for training. In this work, the two types of models, the
Deep neural networks (DNNs) have been widely used in various domains, including image classification, natural language processing, and game playing. However, for safety-critical applications, it is required that the DNNs are certified against properties related to its safety. Unfortunately, DNNs have been found lack of robustness. For example, it is possible to add a small, or even imperceptible, perturbation to a correctly classified input and make it misclassified. Such adversarial examples have raised serious concerns on the safety of DNNs.
Deep neural networks (DNNs) have been used to improve the performance of speaker verification (SV) systems. However, SV systems are vulnerable to various presentation attacks, such as replay attacks, voice conversion, and speech synthesis. These attacks have inspired research into presentation attack detection (PAD), which classifies given utterances as spoofed or not spoofed. In this paper, we propose two spoofing-aware frameworks for the ISV task. The first proposes a monolithic end-to-end-to-end (E2E) architecture
Face verification in unconstrained settings is a challenging problem. Recent face verification systems have achieved excellent performance on datasets like Labeled Faces in the Wild (LFW). However, it is still difficult to achieve similar accuracy on faces with extreme variations in viewpoints, resolution, occlusion and image quality. Most of the DCNN based methods trained with softmax loss tend to overfit to the high quality data and fail to correctly classify faces acquired in difficult conditions. In this paper, we propose the Crystal loss function that adds a constraint on the features during training such that their L2
Deep convolutional networks (CNNs) have been widely used in face recognition tasks. However, face recognition is a challenging task due to the lack of large scale datasets and the emergence of powerful and scalable network architectures such as Inception ResNet to train on large scale datasets. In this paper, we employ all three attributes associated with face recognition. We use a large scale publicly available dataset, VGGFace2, to train the powerful Inception ResNet V1 network. We propose a novel loss function named Git loss to enhance the discriminative power of deep features. The name
This paper presents a machine learning-supported methodology for the measurement and verification of energy savings in industrial facilities. The methodology is based on the IPMVP, which is a widely recognised and well-established methodology for the measurement and verification of energy savings. The IPMVP defines a methodological approach that can be applied to quantify uncertainty in a project. The IPMVP is a widely recognised and well-established methodology for the measurement and verification of energy savings. However, the lack of a prescribed, analytical process has been highlighted as a significant shortcoming in the industry.
Handwriting recognition is a numeric process of translating handwritten text images into strings of characters. The recognition process traditionally involves two steps: optical character recognition and linguistic processing. Optical character recognition is a hard task due to the variability of shapes in handwritten texts. Linguistic processing aims at combining the characters hypotheses together so as to provide the most likely sequence of words in accordance with some high level linguistic rules. There is currently no efficient alternative to the use of lexicon driven recognition methods, either for isolated words or for text recognition. The contribution of the
In this paper, we propose a novel deep neural network-based speaker verification (TISV) system which extracts speaker features from a large number of utterances. The TISV system uses a deep neural network (DNN) to extract speaker features. The DNN model takes both the enrollment utterance and the test utterance as input, and generates corresponding speaker embeddings. Then, the two speaker embeddings are fed into a backend model, such as probabilistic linear discriminant analysis (PLDNN) to compute a similarity
Authorship verification (AV) is a branch of forensic authorship analysis. Traditionally, the problem of AV considered a closed and limited set of authors and a closed set of documents written by those authors. During the training step some of these documents were observed. Then, the problem was to identify whether the authors of a pair of documents from the rest of the document set were identical. However, this structure is static and not compatible with new future unseen authors. In this paper, we propose two different schemas to study the AV problem. We propose two models:
Face recognition networks have significantly improved over the last few years. However, this has raised concerns about bias against sensitive attributes such as gender, skintone and age. In this paper, we propose two novel non-adversarial techniques to mitigate bias in face recognition. More specifically, we propose two novel know-edge distillation-based techniques called D&D and D&D++ to incrementally learn different categories of a given sensitive attribute while significantly reducing bias with respect to that attribute. We observe that face recognition networks attend to different regions of the skintone attribute. We show that our methods can be used to
Deep learning models are treated as valuable intellectual properties. In the future, licensed models will be distributed only to licensed model users as a charged software or service. In such a situation, one concern is that some malicious model user who obtains a high performance model might redistribute licensed models illegally or provide a prediction service using the licensed model without permission. To deal with such leakage, we require a method that allows us to verify the ownership of models externally. One promising solution is digital watermarking, to embed a mechanism into the model
SuperResolution is a new paradigm in image processing that aims to improve the accuracy of face recognition. It is a powerful tool for image classification and image processing, and it is a powerful tool for image super-resolution. It is a powerful tool for image super-resolution, and it is a powerful tool for image super-resolution.