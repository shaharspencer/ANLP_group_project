Unnamed: 0,titles,abstract,introduction
105,Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training.txt,"User-generated reviews can be decomposed into fine-grained segments (e.g.,
sentences, clauses), each evaluating a different aspect of the principal entity
(e.g., price, quality, appearance). Automatically detecting these aspects can
be useful for both users and downstream opinion mining applications. Current
supervised approaches for learning aspect classifiers require many fine-grained
aspect labels, which are labor-intensive to obtain. And, unfortunately,
unsupervised topic models often fail to capture the aspects of interest. In
this work, we consider weakly supervised approaches for training aspect
classifiers that only require the user to provide a small set of seed words
(i.e., weakly positive indicators) for the aspects of interest. First, we show
that current weakly supervised approaches do not effectively leverage the
predictive power of seed words for aspect detection. Next, we propose a
student-teacher approach that effectively leverages seed words in a
bag-of-words classifier (teacher); in turn, we use the teacher to train a
second model (student) that is potentially more powerful (e.g., a neural
network that uses pre-trained word embeddings). Finally, we show that iterative
co-training can be used to cope with noisy seed words, leading to both improved
teacher and student models. Our proposed approach consistently outperforms
previous weakly supervised approaches (by 14.1 absolute F1 points on average)
in six different domains of product reviews and six multilingual datasets of
restaurant reviews.","A typical review of an entity on platforms such as Yelp and Amazon discusses multiple aspects of the entity (e.g., price, quality) in individual re view segments (e.g., sentences, clauses). Consider for example the Amazon product review in Fig ure 1. The text discusses various aspects of the Great price for an excellent LED TVGreat Tv for the price.  Easy to setup.  The audio is ok for the tiny speakers.  The picture is just as good as my panasonic viera 42"" plasma tv.  Much better than the 20"" tube tv. PriceEase of UseImageSound QualityGeneralAspectSentenceFigure 1: Example of product review with aspect an notations: each individual sentence of the review dis cusses a different aspect (e.g., price) of the TV . TV such as price, ease of use, and sound quality. Given the vast number of online reviews, both sell ers and customers would beneﬁt from automatic methods for detecting ﬁnegrained segments that discuss particular aspects of interest. Finegrained aspect detection is also a key task in downstream applications such as aspectbased sentiment anal ysis and multidocument summarization (Hu and Liu, 2004; Liu, 2012; Pontiki et al., 2016; Ange lidis and Lapata, 2018). In this work, we consider the problem of clas sifying individual segments of reviews to pre deﬁned aspect classes when ground truth aspect labels are not available. Indeed, reviews are of ten entered as unstructured, freeform text and do not come with aspect labels. Also, it is infeasi ble to manually obtain segment annotations for re tail stores like Amazon with millions of different products. Unfortunately, fully supervised neural networks cannot be applied without aspect labels. Moreover, the topics learned by unsupervised neu ral topic models are not perfectly aligned with the users’ aspects of interest, so substantial human ef fort is required for interpreting and mapping the learned topics to meaningful aspects. Here, we investigate whether neural networks can be effectively trained under this challenging setting when only a small number of descriptive keywords, or seed words , are available for eacharXiv:1909.00415v1  [cs.LG]  1 Sep 2019Aspect Seed Words Price (EN) price, value, money, worth, paid Image (EN) picture, color, quality, black, bright Food (EN) food, delicious, pizza, cheese, sushi Drinks (FR) vin, bière, verre, bouteille, cocktail Ambience (SP) ambiente, mesas, terraza, acogedor, ruido Table 1: Examples of aspects and ﬁve of their corre sponding seed words in various domains (electronic products, restaurants) and languages (“EN” for En glish, “FR” for French, “SP” for Spanish). aspect class. Table 1 shows examples of aspects and ﬁve of their corresponding seed words from our experimental datasets (described later in more detail). In contrast to a classiﬁcation label, which is only relevant for a single segment, a seed word can implicitly provide aspect supervision to poten tially many segments. We assume that the seed words have already been collected either manually or automatically. Indeed, collecting a small1set of seed words per aspect is typically easier than man ually annotating thousands of segments for train ing neural networks. As we will see, even noisy seed words that are only weakly predictive of the aspect will be useful for aspect detection. Training neural networks for segmentlevel as pect detection using just a few seed words is a challenging task. Indeed, as a contribution of this paper, we observe that current weakly supervised networks do not effectively leverage the predic tive power of the available seed words. To address the shortcomings of previous seed wordbased ap proaches, we propose a novel weakly supervised approach, which uses the available seed words in a more effective way. In particular, we con sider a studentteacher framework, according to which a bagofseedwords classiﬁer (teacher) is applied on unlabeled segments to supervise a sec ond model (student), which can be any supervised model, including neural networks. Our approach introduces several important con tributions. First, our teacher model considers each individual seed word as a (noisy) aspect indicator, which as we will show, is more effective than pre viously proposed weakly supervised approaches. Second, by using only the teacher’s aspect prob abilities, our student generalizes better than the teacher and, as a result, the student outperforms both the teacher and previously proposed weakly 1In our experiments, we only consider around 30 seed words per aspect. For comparison, the vocabulary of the datasets has more than 10,000 terms.supervised models. Finally, we show how iterative cotraining can be used to cope with noisy seed words: the teacher effectively estimates the pre dictive quality of the noisy seed words in an unsu pervised manner using the associated predictions by the student. Iterative cotraining then leads to both improved teacher and student models. Over all, our approach consistently outperforms exist ing weakly supervised approaches, as we show with an experimental evaluation over six domains of product reviews and six multilingual datasets of restaurant reviews. The rest of this paper is organized as follows. In Section 2 we review relevant work. In Section 3 we describe our proposed weakly supervised ap proach. In Section 4 we present our experimen tal setup and ﬁndings. Finally, in Section 5 we conclude and suggest future work. A preliminary version of this work was presented at the Sec ond Learning from Limited Labeled Data Work shop (Karamanolakis et al., 2019). 2 Related Work and Problem Deﬁnition "
149,Unsupervised Neural Aspect Search with Related Terms Extraction.txt,"The tasks of aspect identification and term extraction remain challenging in
natural language processing. While supervised methods achieve high scores, it
is hard to use them in real-world applications due to the lack of labelled
datasets. Unsupervised approaches outperform these methods on several tasks,
but it is still a challenge to extract both an aspect and a corresponding term,
particularly in the multi-aspect setting. In this work, we present a novel
unsupervised neural network with convolutional multi-attention mechanism, that
allows extracting pairs (aspect, term) simultaneously, and demonstrate the
effectiveness on the real-world dataset. We apply a special loss aimed to
improve the quality of multi-aspect extraction. The experimental study
demonstrates, what with this loss we increase the precision not only on this
joint setting but also on aspect prediction only.","Unsupervised aspect extraction is an essential part of natu  ral language processing and usually solved using topic mod elling approaches, which have proven themselves in this tas k. In general, aspect extraction aims to identify the category or multiple categories of a given text. The aspect can be a globa l context of the sentence or a speciﬁc term in this sentence; a term, in turn, can be either a single word or a collocation. Pr e vious unsupervised approaches achieved signiﬁcant improv e ment in the task of aspect extraction. The joint task of the aspect and the aspect term pairs extraction is still a challe nge for natural language processing. For example: in the senten ce ”Best Pastrami I ever had and great portion without being ridiculous” the aspect and aspect term pairs ”Food Quality: Pastrami” and ”Food Style option: portion”. Most of the existing approaches apply twostage extrac tion: aspect extraction ﬁrst and then aspect term extractio n based on the known aspect. We propose a conjoint solu tion based on the convolutional multiattention mechanism (CMAM). The CMAM was inspired by Inceptionblock in computer vision, where kernels of different sizes allow inc or porating features from different levels of localisation. T hesentence representations built with CMAM capture the fea tures, which are used for aspect predictions, while the at tention detects related terms. Also, the convolutional att en tion does not require much additional time to infer the re sult, which is vital for the realworld application. In orde r to increase the quality of multiaspect extraction, we prop ose a novel loss function  tripletlike aspect spreading (TLAS ), which maximises the distance between topN aspectbased sentence representations and minimises the distance betwe en these representations and corresponding aspect vectors. T his approach allows achieving close to the stateoftheart re sults in aspect extraction with the ability to extract their terms . In summary, the contributions of this paper are: •CMAM; convolutional multiattention mechanism, which is aimed to build sentence vector representation and to extract aspect terms. •TLAS  loss function for aspect probabilities distribution modifying. •The experimental study of the proposed model on SemEval2016 Restaurant dataset and Citysearch cor pus. 2 Related Work "
584,"HyperGAN: A Generative Model for Diverse, Performant Neural Networks.txt","Standard neural networks are often overconfident when presented with data
outside the training distribution. We introduce HyperGAN, a new generative
model for learning a distribution of neural network parameters. HyperGAN does
not require restrictive assumptions on priors, and networks sampled from it can
be used to quickly create very large and diverse ensembles. HyperGAN employs a
novel mixer to project prior samples to a latent space with correlated
dimensions, and samples from the latent space are then used to generate weights
for each layer of a deep neural network. We show that HyperGAN can learn to
generate parameters which label the MNIST and CIFAR-10 datasets with
competitive performance to fully supervised learning, while learning a rich
distribution of effective parameters. We also show that HyperGAN can also
provide better uncertainty estimates than standard ensembles by evaluating on
out of distribution data as well as adversarial examples.","It is well known that it is possible to train deep neu ral networks from different random initializations and ob tain models that, albeit having quite different parameters, achieve similar loss values (Freeman & Bruna, 2016). It has further been found that ensembles of deep networks that are trained in such a way have signiﬁcant performance advantages over single models (Maclin & Opitz, 2011), similar to the classical bagging approach in statistics. En sembles are also more robust to outliers, and can provide uncertainty estimates over their inputs (Lakshminarayanan et al., 2017). In Bayesian deep learning, there is a signiﬁcant interest in 1School of Electrical Engineering and Computer Science, Ore gon State University. Correspondence to: Neale Ratzlaff <rat zlafn@oregonstate.edu >, Li Fuxin<lif@oregonstate.edu >. Proceedings of the 36thInternational Conference on Machine Learning , Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).learning approximate posterior distributions over network parameters. Past approaches mostly leverage variational in ference to model this likely intractable distribution. (Gal & Ghahramani, 2016) formulated dropout as a Bayesian ap proximation, and showed that networks with dropout fol lowing each layer are equivalent to a deep Gaussian pro cess (Damianou & Lawrence, 2013) marginalized over its covariance functions. They proposed MC dropout as a sim ple way to estimate model uncertainty. Applying dropout to every layer however, may result in underﬁtting of the data. Moreover, dropout only integrates over the space of mod els reachable from a single (random initialization). As an other interesting direction, hypernetworks (Ha et al., 2016) are neural networks which output parameters for a target neural network. The hypernetwork and the target network together form a single model which is trained jointly. Orig inally, hypernetwork produced the target weights as a deter ministic function of its own weights, but Bayesian Hyper networks (BHNs) (Krueger et al., 2017), and Multiplica tive Normalizing Flows (MNF) (Louizos & Welling, 2016) learn variational approximations by transforming samples from a Gaussian prior through a normalizing ﬂow. Normal izing ﬂows can model complicated posteriors, but they are composed of invertible bijections, which limits their scala bility and the variety of learnable functions. In this paper we explore an approach which generates all the parameters of a neural network in a single pass, with out assuming any ﬁxed noise models or functional form of the generating function. To keep our method scalable, we do not restrict ourselves to invertible functions as in ﬂow based approaches. We instead utilize ideas from genera tive adversarial networks (GANs). We are especially moti vated by recent Wasserstein Autoencoder (Tolstikhin et al., 2017) approaches, which have demonstrated an impressive capability to model complicated, multimodal distributions. One of the issues in generating weights for every layer is the connectivity of the network. Namely, the output of the previous layer becomes the input of the next layer, hence the network weights must be correspondent in or der to generate valid results. In our approach, we sam ple from a simple multidimensional Gaussian distribution, and propose to transform this sample into multiple differ ent vectors. We call this procedure a mixer since it in troduces correlation to the otherwise independent randomarXiv:1901.11058v3  [cs.LG]  14 Jul 2020HyperGAN noise. Then each random vector is used to generate all the weights within one layer of a deep network. The genera tor is then trained with conventional maximum likelihood (classiﬁcation/regression) on the weights that it generates, and an adversarial regularization keeps it from collapsing onto only one mode. In this way, it is possible to generate much larger networks than the dimensionality of the latent code, making our approach capable of generating all the weights of a deep network with a single GPU. Somewhat surprisingly, with just this approach we can already generate complete, multilayer convolutional net works which do not require additional ﬁnetuning. We are able to easily sample many welltrained networks from the generator which each achieve low loss on the target dataset. Moreover, our diversity constraints result in models sig niﬁcantly more diverse than training with multiple random starts (ensembles) or past variational methods. We believe our approach is widely applicable to a variety of tasks. One area where populations of diverse networks show promise is in uncertainty estimation and anomaly de tection. We show through a variety of experiments that populations of diverse networks sampled from our model are able to generate reasonable uncertainty estimates by calculating the entropy of the predictive distribution of sampled networks. Such uncertainty estimates allow us to detect out of distribution samples as well as adversarial ex amples. Our method is straightforward, as well as easy to train and sample from. We hope that we can inspire future work in the estimation of the manifold of neural networks. We summarize our contribution as follows: We propose HyperGAN, a novel approach to approx imating the posterior of neural network parameters for a target architecture. HyperGAN contains a novel mixer that mixes input noise into separate vectors that generate each layer of the network respectively. Different from prior GANs, HyperGAN does not re quire repeated samples to start with (e.g. no need to train 1;000networks as a training set) but trains di rectly using maximum likelihood. This signiﬁcantly improve training efﬁciency. The generated networks perform well without need for further ﬁnetuning. On classiﬁcation experiments, 100 network ensem bles generated by HyperGAN signiﬁcantly improves accuracy. To validate the uncertainty estimates given by ensembles from HyperGAN, we performed experi ments on a synthetic regression task, an opencategory classiﬁcation task and an adversarial detection task. 2. Related Work "
192,MEAL: Multi-Model Ensemble via Adversarial Learning.txt,"Often the best performing deep neural models are ensembles of multiple
base-level networks. Unfortunately, the space required to store these many
networks, and the time required to execute them at test-time, prohibits their
use in applications where test sets are large (e.g., ImageNet). In this paper,
we present a method for compressing large, complex trained ensembles into a
single network, where knowledge from a variety of trained deep neural networks
(DNNs) is distilled and transferred to a single DNN. In order to distill
diverse knowledge from different trained (teacher) models, we propose to use
adversarial-based learning strategy where we define a block-wise training loss
to guide and optimize the predefined student network to recover the knowledge
in teacher models, and to promote the discriminator network to distinguish
teacher vs. student features simultaneously. The proposed ensemble method
(MEAL) of transferring distilled knowledge with adversarial learning exhibits
three important advantages: (1) the student network that learns the distilled
knowledge with discriminators is optimized better than the original model; (2)
fast inference is realized by a single forward pass, while the performance is
even better than traditional ensembles from multi-original models; (3) the
student network can learn the distilled knowledge from a teacher model that has
arbitrary structures. Extensive experiments on CIFAR-10/100, SVHN and ImageNet
datasets demonstrate the effectiveness of our MEAL method. On ImageNet, our
ResNet-50 based MEAL achieves top-1/5 21.79%/5.99% val error, which outperforms
the original model by 2.06%/1.14%. Code and models are available at:
https://github.com/AaronHeee/MEAL","The ensemble approach is a collection of neural networks whose predictions are combined at test stage by weighted averaging or voting. It has been long observed that en sembles of multiple networks are generally much more ro bust and accurate than a single network. This beneﬁt has also been exploited indirectly when training a single net work through Dropout (Srivastava et al. 2014), Dropcon nect (Wan et al. 2013), Stochastic Depth (Huang et al. 2016), Equal contribution. This work was done when Zhankui He was a research intern at University of Illinois at UrbanaChampaign. Copyright c 2019, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. 1 2 3 4 5 # of ensembles0×1×2×3×4×5×6×FLOPs FLOPs at Inference Time Snapshot Ensemble (Huang et al. 2017) Our FLOPs at Test TimeFigure 1: Comparison of FLOPs at inference time. Huang et al. (Huang et al. 2017a) employ models at different lo cal minimum for ensembling, which enables no additional training cost, but the computational FLOPs at test time lin early increase with more ensembles. In contrast, our method use only one model during inference time throughout, so the testing cost is independent of # ensembles. Swapout (Singh, Hoiem, and Forsyth 2016), etc. We extend this idea by forming ensemble predictions during training, using the outputs of different network architectures with dif ferent or identical augmented input. Our testing still operates on a single network, but the supervision labels made on dif ferent pretrained networks correspond to an ensemble pre diction of a group of individual reference networks. The traditional ensemble, or called true ensemble, has some disadvantages that are often overlooked. 1) Redun dancy: The information or knowledge contained in the trained neural networks are always redundant and has over laps between with each other. Directly combining the pre dictions often requires extra computational cost but the gain is limited. 2) Ensemble is always large and slow: Ensem ble requires more computing operations than an individual network, which makes it unusable for applications with lim ited memory, storage space, or computational power such as desktop, mobile and even embedded devices, and for appli cations in which realtime predictions are needed. To address the aforementioned shortcomings, in this paarXiv:1812.02425v2  [cs.CV]  25 Jul 2019librarybookshopconfectionerygrocerystoretobaccoshoptoyshopFigure 2: Left is a training example of class “tobacco shop” from ImageNet. Right are soft distributions from different trained architectures. The soft labels are more informative and can provide more coverage for visuallyrelated scenes. per we propose to use a learningbased ensemble method. Our goal is to learn an ensemble of multiple neural networks without incurring any additional testing costs . We achieve this goal by leveraging the combination of diverse outputs from different neural networks as supervisions to guide the target network training. The reference networks are called Teachers and the target networks are called Students . Instead of using the traditional onehot vector labels, we use the soft labels that provide more coverage for cooccurring and visu ally related objects and scenes. We argue that labels should be informative for the speciﬁc image. In other words, the labels should not be identical for all the given images with the same class. More speciﬁcally, as shown in Fig. 2, an im age of “tobacco shop” has similar appearance to “library” should have a different label distribution than an image of “tobacco shop” but is more similar to “grocery store”. It can also be observed that soft labels can provide the additional intra and intercategory relations of datasets. To further improve the robustness of student networks, we introduce an adversarial learning strategy to force the student to generate similar outputs as teachers. Our exper iments show that MEAL consistently improves the accu racy across a variety of popular network architectures on different datasets. For instance, our shakeshake (Gastaldi 2017) based MEAL achieves 2.54% test error on CIFAR10, which is a relative 11:2%improvement1. On ImageNet, our ResNet50 based MEAL achieves 21.79%/5.99% val error, which outperforms the baseline by a large margin. In summary, our contribution in this paper is three fold. An endtoend framework with adversarial learning is de signed based on the teacherstudent learning paradigm for deep neural network ensembling. The proposed method can achieve the goal of ensembling multiple neural networks with no additional testing cost . The proposed method improves the stateoftheart accu racy on CIFAR10/100, SVHN, ImageNet for a variety of existing network architectures. 2. Related Work "
113,Augmentation Strategies for Learning with Noisy Labels.txt,"Imperfect labels are ubiquitous in real-world datasets. Several recent
successful methods for training deep neural networks (DNNs) robust to label
noise have used two primary techniques: filtering samples based on loss during
a warm-up phase to curate an initial set of cleanly labeled samples, and using
the output of a network as a pseudo-label for subsequent loss calculations. In
this paper, we evaluate different augmentation strategies for algorithms
tackling the ""learning with noisy labels"" problem. We propose and examine
multiple augmentation strategies and evaluate them using synthetic datasets
based on CIFAR-10 and CIFAR-100, as well as on the real-world dataset
Clothing1M. Due to several commonalities in these algorithms, we find that
using one set of augmentations for loss modeling tasks and another set for
learning is the most effective, improving results on the state-of-the-art and
other previous methods. Furthermore, we find that applying augmentation during
the warm-up period can negatively impact the loss convergence behavior of
correctly versus incorrectly labeled samples. We introduce this augmentation
strategy to the state-of-the-art technique and demonstrate that we can improve
performance across all evaluated noise levels. In particular, we improve
accuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in
absolute accuracy, and we also improve performance on the Clothing1M dataset.
  (K. Nishi and Y. Ding contributed equally to this work)","Data augmentation is a common method used to expand datasets and has been applied successfully in many com puter vision problems such as image classiﬁcation [32] and object detection [28], among many others. In particular, *Equal contribution Source code is available at https://github.com/KentoNishi/ AugmentationforLNL .there has been much success using learned augmentations such as AutoAugment [6] and RandAugment [7] which do not require an expert who knows the dataset to curate aug mentation policies. It has been shown that incorporating augmentation policies during training can improve gener alization and robustness [12, 8]. However, few works have explored their efﬁcacy for the domain of learning with noisy labels (LNL) [21]. Many techniques which tackle the LNL problem make use of the network memorization effect, where correctly la beled data ﬁt before incorrectly labeled data as discovered by Arpit et al. [2]. This phenomenon was successfully ex plored in Deep Neural Networks (DNNs) through model ing the loss function and the training process, leading to the development of approaches such as loss correction [29] and sample selection [10]. Recently, the incorporation of MixUp augmentation [35] has dramatically improved the ability for algorithms to tolerate higher noise levels [1, 14]. While many existing works use the common random ﬂip and crop image augmentation which we refer to as weak augmentation , to the best of our knowledge, no work at the time of writing has explored using more aggressive aug mentation from learned policies such as AutoAugment dur ing training for LNL algorithms. These stronger augmenta tion policies include transformations such as rotate, invert, sheer, etc. We propose to incorporate these stronger aug mentation policies into existing architectures in a strategic way to improve performance. Our intuition is that for any augmentation technique to succeed, they must (1)improve the generalization of the dataset and (2)not negatively im pact the loss modeling and loss convergence behavior that LNL techniques rely on. With this in mind, we propose an augmentation strategy we call Augmented Descent (A UGDESC) to beneﬁt from data augmentation without negatively impacting the net work memorization effect. Our idea for A UGDESC is to use two different augmentations: a weak augmentation for any loss modeling and pseudolabeling task, and a strong augmentation for the backpropagation step to improve genarXiv:2103.02130v3  [cs.CV]  1 Apr 2021eralization. In this paper, we propose and examine how we can incor porate stronger augmentation into existing LNL algorithms to yield improved results. We provide some answers to this problem through the following contributions: • We propose an augmentation strategy, Augmented Descent, which demonstrates stateoftheart perfor mance on synthetic and realworld datasets under noisy label scenarios. We show empirically that this can increase performance across all evaluated noise levels (Section 4.4). In particular, we improve accu racy on the CIFAR10 benchmark at 90% symmetric noise by more than 15% in absolute accuracy, and we also improve performance on the realworld dataset Clothing1M (Section 4.5). • We show that there is a large effect on performance de pending on how augmentation is incorporated into the training process (Section 4.2). We empirically deter mine that it is best to use weaker augmentation during earlier epochs followed by stronger augmentations to not adversely affect the memorization effect. We ana lyze the behavior of loss distribution to yield insight to guide effective incorporation of augmentation in future work (Section 4.3). • We evaluate the effectiveness of our augmentation methodology by performing generalization studies on existing techniques (Section 4.7). Without tuning any hyperparameters, we were able to improve existing techniques with only the addition of our proposed aug mentation strategy by up to 5% in absolute accuracy. 2. Related Work "
109,Diverse Ensembles Improve Calibration.txt,"Modern deep neural networks can produce badly calibrated predictions,
especially when train and test distributions are mismatched. Training an
ensemble of models and averaging their predictions can help alleviate these
issues. We propose a simple technique to improve calibration, using a different
data augmentation for each ensemble member. We additionally use the idea of
`mixing' un-augmented and augmented inputs to improve calibration when test and
training distributions are the same. These simple techniques improve
calibration and accuracy over strong baselines on the CIFAR10 and CIFAR100
benchmarks, and out-of-domain data from their corrupted versions.","Modern neural network models can produce overconﬁdent or miscalibrated predictions, even when training examples are independent and identically distributed (i.i.d.) to th e test distribution. This miscalibration is exacerbated when the training and testing distributions are different. In safet y critical scenarios, the ability to accurately represent mo del uncertainty is valuable. Such model miscalibration has been shown to reduce when we train an ensemble of models and average their predic tions ( Lakshminarayanan et al. ,2017 ;Ovadia et al. ,2019 ). Ensembles have long been known to improve generalisa tion ( Hansen & Salamon ,1990 ), especially when an en semble is diverse, which is promoted with various tech niques such as using latent variables ( Sinha et al. ,2020 ) or diversityencouraging losses and architecture changes (Kim et al. ,2018 ;Lee et al. ,2016 ;Pang et al. ,2019 ). Re cent work proposes ‘cheap’ ensembles by sharing most of the model parameters across all ensembles, and using rank 1 factors to modify the linear layers in an ensemble member (Wen et al. ,2020 ), making ensembles easier to train and 1School of Informatics, University of Edinburgh. Correspon  dence to: Asa Cooper Stickland <a.cooper.stickland@ed.ac.uk >. Presented at the ICML 2020 Workshop on Uncertainty and Ro bustness in Deep Learning. Copyright 2020 by the author(s).store. Another longstanding way to improve generalization isData Augmentation , i.e. expanding our training set with modiﬁed copies ( Zhang et al. ,2018 ;Yun et al. ,2019 ; Cubuk et al. ,2019 ). Recent examples include work by Hendrycks et al. (2020 ) and Xie et al. (2019 ). These ap proaches exploit the intuition that a blurry or rotated imag e should have the same class as the original image. This work extends and combines recent work on cheap en sembles and data augmentation. We increase ensemble di versity by applying different augmentations to each ensem ble member. This method improves calibration on an i.i.d. test set, and accuracy and calibration on outofdistribut ion test sets for the CIFAR10 and CIFAR100 datasets. We ad ditionally simplify the idea of ‘mixing’ unaugmented and augmented inputs introduced by Hendrycks et al. (2020 ), and explore adversarial perturbations, which apply more generally and result in better performance on i.i.d. data. 2. Methods "
128,Are you wearing a mask? Improving mask detection from speech using augmentation by cycle-consistent GANs.txt,"The task of detecting whether a person wears a face mask from speech is
useful in modelling speech in forensic investigations, communication between
surgeons or people protecting themselves against infectious diseases such as
COVID-19. In this paper, we propose a novel data augmentation approach for mask
detection from speech. Our approach is based on (i) training Generative
Adversarial Networks (GANs) with cycle-consistency loss to translate unpaired
utterances between two classes (with mask and without mask), and on (ii)
generating new training utterances using the cycle-consistent GANs, assigning
opposite labels to each translated utterance. Original and translated
utterances are converted into spectrograms which are provided as input to a set
of ResNet neural networks with various depths. The networks are combined into
an ensemble through a Support Vector Machines (SVM) classifier. With this
system, we participated in the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020
Computational Paralinguistics Challenge, surpassing the baseline proposed by
the organizers by 2.8%. Our data augmentation technique provided a performance
boost of 0.9% on the private test set. Furthermore, we show that our data
augmentation approach yields better results than other baseline and
state-of-the-art augmentation methods.","In this paper, we describe our system for the Mask Sub Challenge (MSC) of the INTERSPEECH 2020 Computational Paralinguistics Challenge (ComParE) [1]. In MSC, the task is to determine if an utterance belongs to a person wearing a face mask or not. As noted by Schuller et al. [1], the task of detect ing whether a speaker wears a face mask is useful in modelling speech in forensics or communication between surgeons. In the context of the COVID19 pandemic, another potential applica tion is to verify if people wear surgical masks. We propose a system based on Support Vector Machines (SVM) [2] applied on top of feature embeddings concate nated from multiple ResNet [3] convolutional neural networks (CNNs). In order to improve our mask detection performance, we propose a novel data augmentation technique that is aimed at eliminating biases in the training data distribution. Our data augmentation method is based on (i)training Genera tive Adversarial Networks (GANs) with cycleconsistency loss [4, 5] for unpaired utterancetoutterance translation among two classes (with mask and without mask), and on (ii)generating new training utterances using the cycleconsistent GANs, as signing opposite labels to each translated utterance. While deep neural networks attain stateoftheart results in various domains [3, 6, 7, 8, 9], such models can easily succumbto the pitfall of overﬁtting [10]. This means that deep models can take decisions based on various biases existing in training data. A notorious example is an image of a wolf being correctly labeled, but only because of the snowy background [11]. In our case, the training samples belonging to one class may have different gender and age distribution than the training samples belonging to the other class, among other unknown biases. In stead of ﬁnding relevant features to discriminate utterances with and without mask, a neural network might consider features for gender prediction or age estimation, which is undesired. With our data augmentation approach, all utterances with mask are translated to utterances without mask and the other way around, as shown in Figure 1. Any potential bias in the distribution of training data samples is eliminated through the compensation that comes with the augmented data samples from the opposite class. This forces the neural networks to discover features that discriminate the training data with respect to the desired task, i.e. classiﬁcation into mask versus nonmask . We conduct experiments on the Mask Augsburg Speech Corpus (MASC), showing that our data augmentation approach attains superior results in comparison to a set of baselines, e.g. noise perturbation and time shifting, and a set of stateofthe art data augmentation techniques, e.g. speed perturbation [12], conditional GANs [13] and SpecAugment [14]. 2. Related Work "
358,Panoptic Lifting for 3D Scene Understanding with Neural Fields.txt,"We propose Panoptic Lifting, a novel approach for learning panoptic 3D
volumetric representations from images of in-the-wild scenes. Once trained, our
model can render color images together with 3D-consistent panoptic segmentation
from novel viewpoints.
  Unlike existing approaches which use 3D input directly or indirectly, our
method requires only machine-generated 2D panoptic segmentation masks inferred
from a pre-trained network. Our core contribution is a panoptic lifting scheme
based on a neural field representation that generates a unified and multi-view
consistent, 3D panoptic representation of the scene. To account for
inconsistencies of 2D instance identifiers across views, we solve a linear
assignment with a cost based on the model's current predictions and the
machine-generated segmentation masks, thus enabling us to lift 2D instances to
3D in a consistent way. We further propose and ablate contributions that make
our method more robust to noisy, machine-generated labels, including test-time
augmentations for confidence estimates, segment consistency loss, bounded
segmentation fields, and gradient stopping.
  Experimental results validate our approach on the challenging Hypersim,
Replica, and ScanNet datasets, improving by 8.4, 13.8, and 10.6% in scene-level
PQ over state of the art.","Robust panoptic 3D scene understanding models are key to enabling applications such as VR, robot navigation, or selfdriving, and more. Panoptic image understanding – the task of segmenting a 2D image into categorical “stuff” areas and individual “thing” instances – has experienced tremen dous progress over the past years. These advances can be at tributed to continuously improved model architectures and the availability of largescale labeled 2D datasets, leading to stateoftheart 2D panoptic segmentation models [6,21,45] that generalize well to unseen images captured in the wild. Singleimage panoptic segmentation, unfortunately, is still insufﬁcient for tasks requiring coherency and consis tency across multiple views. In fact, panoptic masks often contain viewspeciﬁc imperfections and inconsistent clas siﬁcations, and singleimage 2D models naturally lack the ability to track unique object identities across views (see Fig. 2). Ideally, such consistency would stem from a full, 3D understanding of the environment, but lifting machine generated 2D panoptic segmentations into a coherent 3D panoptic scene representation remains a challenging task. Project page: nihalsid.github.io/panopticlifting/ 1arXiv:2212.09802v1  [cs.CV]  19 Dec 2022Recent works [11,19,42,47] have addressed panoptic 3D scene understanding from 2D images by leveraging Neu ral Radiance Fields (NeRFs) [24], gathering semantic scene data from multiple sources. Some works [11, 42] rely on ground truth 2D and 3D labels, which are expensive and timeconsuming to acquire. The work of Kundu et al. [19] instead exploits machinegenerated 3D bounding box detec tion and tracking together with 2D semantic segmentation, both computed using offtheshelf models. However, this approach is limited by the fact that 3D detection models, when compared to 2D panoptic segmentation ones, strug gle to generalize beyond the data they were trained on. This is in large part due to the large difference in scale between 2D and 3D training datasets. Furthermore, dependence on multiple pretrained models increases complexity and intro duces potentially compounding sources of error. In this work we introduce Panoptic Lifting, a novel for mulation which represents a static 3D scene as a panoptic radiance ﬁeld (see Sec. 3.2). Panoptic Lifting supports ap plications like novel panoptic view synthesis and scene edit ing, while maintaining robustness to a variety of diverse in put data. Our model is trained from only 2D posed images and corresponding, machinegenerated panoptic segmenta tion masks, and can render color, depth, semantics, and 3D consistent instance information for novel views of the scene. Starting from a TensoRF [4] architecture that encodes density and viewdependent color information, we intro duce lightweight output heads for learning semantic and instance ﬁelds. The semantic ﬁeld, represented as a small MLP, is directly supervised with the machinegenerated 2D labels. An additional segment consistency loss guides this supervision to avoid optima that fragment objects in the presence of label inconsistencies. The 3D instance ﬁeld is modelled by a separate MLP, holding a ﬁxed number of classagnostic, 3Dconsistent surrogate object identiﬁers. Losses for both the ﬁelds are weighted by conﬁdence esti mates obtained by testtime augmentation on the 2D panop tic segmentation model. Finally, we discuss speciﬁc tech niques, e.g. bounded segmentation ﬁelds and stopping semanticstogeometry gradients (see Sec. 3.3), to further limit inconsistent segmentations. In summary, our contributions are: • A novel approach to panoptic radiance ﬁeld represen tation that models the radiance, semantic class and in stance id for each point in the space for a scene by directly lifting machinegenerated 2D panoptic labels. • A robust formulation to handle inherent noise and in consistencies in machinegenerated labels, resulting in a clean, coherent and viewconsistent panoptic seg mentations from novel views, across diverse data. Figure 2. Predictions from stateoftheart 2D panoptic segmen tation methods such as Mask2Former [6] are typically noisy and inconsistent when compared across views of the same scene. Typ ical failure modes include conﬂicting labels (e.g. sofa predicted as a bed above) and segmentations (e.g. labeled void above). Fur thermore, instance identities are not preserved across frames (rep resented as different colors). 2. Related Work "
479,Bayesian Deep Learning and a Probabilistic Perspective of Generalization.txt,"The key distinguishing property of a Bayesian approach is marginalization,
rather than using a single setting of weights. Bayesian marginalization can
particularly improve the accuracy and calibration of modern deep neural
networks, which are typically underspecified by the data, and can represent
many compelling but different solutions. We show that deep ensembles provide an
effective mechanism for approximate Bayesian marginalization, and propose a
related approach that further improves the predictive distribution by
marginalizing within basins of attraction, without significant overhead. We
also investigate the prior over functions implied by a vague distribution over
neural network weights, explaining the generalization properties of such models
from a probabilistic perspective. From this perspective, we explain results
that have been presented as mysterious and distinct to neural network
generalization, such as the ability to fit images with random labels, and show
that these results can be reproduced with Gaussian processes. We also show that
Bayesian model averaging alleviates double descent, resulting in monotonic
performance improvements with increased flexibility. Finally, we provide a
Bayesian perspective on tempering for calibrating predictive distributions.","Imagine ﬁtting the airline passenger data in Figure 1. Which model would you choose: (1) f1(x) =w0+w1x, (2) f2(x) =P3 j=0wjxj, or (3)f3(x) =P104 j=0wjxj? Put this way, most audiences overwhelmingly favour choices (1) and (2), for fear of overﬁtting. But of these options, choice (3) most honestly represents our beliefs. Indeed, it is likely that the ground truth explanation for the data is out of class for any of these choices, but there is some setting of the coefﬁcientsfwjgin choice (3) which provides a better description of reality than could be managed by choices (1) and (2), which are special cases of choice (3). Moreover, our beliefs about the generative processes for our observations, 1949 1951 1953 1955 1957 1959 Year100k200k300k400k500kAirline Passengers Figure 1. Airline passenger numbers recorded monthly. which are often very sophisticated, typically ought to be independent of how many data points we happen to observe. And in modern practice, we are implicitly favouring choice (3): we often use neural networks with millions of param eters to ﬁt datasets with thousands of points. Furthermore, nonparametric methods such as Gaussian processes often involve inﬁnitely many parameters, enabling the ﬂexibil ity for universal approximation (Rasmussen & Williams, 2006), yet in many cases provide very simple predictive distributions. Indeed, parameter counting is a poor proxy for understanding generalization behaviour. From a probabilistic perspective, we argue that generaliza tion depends largely on twoproperties, the support and the inductive biases of a model. Consider Figure 2(a), where on the horizontal axis we have a conceptualization of all possible datasets, and on the vertical axis the Bayesian ev idence for a model. The evidence, or marginal likelihood, p(DjM ) =R p(DjM;w)p(w)dw, is the probability we would generate a dataset if we were to randomly sample from the prior over functions p(f(x))induced by a prior over parameters p(w). We deﬁne the support as the range of datasets for which p(DjM )>0. We deﬁne the inductive biases as the relative prior probabilities of different datasets — the distribution of support given byp(DjM ). A similar schematic to Figure 2(a) was used by MacKay (1992) to understand an Occam’s razor effect in using the evidence for model selection; we believe it can also be used to reason about model construction and generalization.arXiv:2002.08791v4  [cs.LG]  30 Mar 2022Bayesian Deep Learning and a Probabilistic Perspective of Generalization p(D|M ) Corrupted CIFAR10CIFAR10 MNISTDataset Structured Image DatasetsComplex Model Poor Inductive Biases Example: MLPSimple Model Poor Inductive Biases Example: Linear FunctionWellSpeciﬁed Model Calibrated Inductive Biases Example: CNN (a) True ModelPrior Hypothesis Space Posterior (b) True ModelPrior Hypothesis Space Posterior (c) True ModelPrior Hypothesis Space Posterior (d) Figure 2. A probabilistic perspective of generalization. (a) Ideally, a model supports a wide range of datasets, but with inductive biases that provide high prior probability to a particular class of problems being considered. Here, the CNN is preferred over the linear model and the fullyconnected MLP for CIFAR10 (while we do not consider MLP models to in general have poor inductive biases, here we are considering a hypothetical example involving images and a very large MLP). (b) By representing a large hypothesis space, a model can contract around a true solution, which in the realworld is often very sophisticated. (c) With truncated support, a model will converge to an erroneous solution. (d) Even if the hypothesis space contains the truth, a model will not efﬁciently contract unless it also has reasonable inductive biases. From this perspective, we want the support of the model to be large so that we can represent any hypothesis we believe to be possible, even if it is unlikely. We would even want the model to be able to represent pure noise, such as noisy CIFAR (Zhang et al., 2016), as long as we honestly believe there is some nonzero, but potentially arbitrarily small, probability that the data are simply noise. Crucially, we also need the inductive biases to carefully represent which hypotheses we believe to be a priori likely for a particular problem class. If we are modelling images, then our model should have statistical properties, such as convolutional structure, which are good descriptions of images. Figure 2(a) illustrates three models. We can imagine the blue curve as a simple linear function, f(x) =w0+w1x, combined with a distribution over parameters p(w0;w1), e.g.,N(0;I), which induces a distribution over functions p(f(x)). Parameters we sample from our prior p(w0;w1) give rise to functions f(x)that correspond to straight lines with different slopes and intercepts. This model thus has truncated support: it cannot even represent a quadratic func tion. But because the marginal likelihood must normal ize over datasetsD, this model assigns much mass to the datasets it does support. The red curve could represent a large fullyconnected MLP. This model is highly ﬂexible, but distributes its support across datasets too evenly to be particularly compelling for many image datasets. The green curve could represent a convolutional neural network, which represents a compelling speciﬁcation of support and induc tive biases for image recognition: this model has the ﬂexibil ity to represent many solutions, but its structural properties provide particularly good support for many image problems. With large support, we cast a wide enough net that the poste rior can contract around the true solution to a given problemas in Figure 2(b), which in reality we often believe to be very sophisticated. On the other hand, the simple model will have a posterior that contracts around an erroneous solution if it is not contained in the hypothesis space as in Figure 2(c). Moreover, in Figure 2(d), the model has wide support, but does not contract around a good solution because its support is too evenly distributed. Returning to the opening example, we can justify the high order polynomial by wanting large support. But we would still have to carefully choose the prior on the coefﬁcients to induce a distribution over functions that would have rea sonable inductive biases. Indeed, this Bayesian notion of generalization is not based on a single number, but is a two dimensional concept. From this probabilistic perspective, it is crucial not to conﬂate the ﬂexibility of a model with thecomplexity of a model class. Indeed Gaussian processes with RBF kernels have large support, and are thus ﬂexible, but have inductive biases towards very simple solutions. We also see that parameter counting has no signiﬁcance in this perspective of generalization: what matters is how a distri bution over parameters combines with a functional form of a model, to induce a distribution over solutions. Rademacher complexity (Mohri & Rostamizadeh, 2009), VC dimension (Vapnik, 1998), and many conventional metrics, are by con trast one dimensional notions , corresponding roughly to the support of the model, which is why they have been found to provide an incomplete picture of generalization in deep learning (Zhang et al., 2016). In this paper we reason about Bayesian deep learning from a probabilistic perspective of generalization. The key dis tinguishing property of a Bayesian approach is marginaliza tion instead of optimization, where we represent solutions given by all settings of parameters weighted by their posBayesian Deep Learning and a Probabilistic Perspective of Generalization terior probabilities, rather than bet everything on a single setting of parameters. Neural networks are typically under speciﬁed by the data, and can represent many different but high performing models corresponding to different settings of parameters, which is exactly when marginalization will make the biggest difference for accuracy and calibration. Moreover, we clarify that the recent deep ensembles (Lak shminarayanan et al., 2017) are not a competing approach to Bayesian inference, but can be viewed as a compelling mechanism for Bayesian marginalization. Indeed, we em pirically demonstrate that deep ensembles can provide a better approximation to the Bayesian predictive distribution than standard Bayesian approaches. We further propose a new method, MultiSWAG, inspired by deep ensembles, which marginalizes within basins of attraction — achieving signiﬁcantly improved performance, with a similar training time. We then investigate the properties of priors over functions induced by priors over the weights of neural networks, show ing that they have reasonable inductive biases. We also show that the mysterious generalization properties recently pre sented in Zhang et al. (2016) can be understood by reasoning about prior distributions over functions, and are not speciﬁc to neural networks. Indeed, we show Gaussian processes can also perfectly ﬁt images with random labels, yet generalize on the noisefree problem. These results are a consequence of large support but reasonable inductive biases for com mon problem settings. We further show that while Bayesian neural networks can ﬁt the noisy datasets, the marginal like lihood has much better support for the noise free datasets, in line with Figure 2. We additionally show that the mul timodal marginalization in MultiSWAG alleviates double descent, so as to achieve monotonic improvements in per formance with model ﬂexibility, in line with our perspective of generalization. MultiSWAG also provides signiﬁcant im provements in both accuracy and NLL over SGD training and unimodal marginalization. Finally we provide several perspectives on tempering in Bayesian deep learning. In the Appendix we provide several additional experiments and results. We also provide code at https://github. com/izmailovpavel/understandingbdl . 2. Related Work "
309,Unsupervised Domain Adaptation with Random Walks on Target Labelings.txt,"Unsupervised Domain Adaptation (DA) is used to automatize the task of
labeling data: an unlabeled dataset (target) is annotated using a labeled
dataset (source) from a related domain. We cast domain adaptation as the
problem of finding stable labels for target examples. A new definition of label
stability is proposed, motivated by a generalization error bound for large
margin linear classifiers: a target labeling is stable when, with high
probability, a classifier trained on a random subsample of the target with that
labeling yields the same labeling. We find stable labelings using a random walk
on a directed graph with transition probabilities based on labeling stability.
The majority vote of those labelings visited by the walk yields a stable label
for each target example. The resulting domain adaptation algorithm is
strikingly easy to implement and apply: It does not rely on data
transformations, which are in general computational prohibitive in the presence
of many input features, and does not need to access the source data, which is
advantageous when data sharing is restricted. By acting on the original feature
space, our method is able to take full advantage of deep features from external
pre-trained neural networks, as demonstrated by the results of our experiments.","Unsupervised domain adaptation (DA) addresses the prob lem of building a good predictor for a target domain using labeled training data from a related source domain and tar get unlabeled training data. A typical example in visual ob ject recognition involves two different datasets consisti ng of images taken under different cameras or conditions: for in stance, one dataset consists of images taken at home with a digital camera while another dataset contains images taken in a controlled environment with studio lightning conditions . In some cases, the source domain is related to the target one, but predictive features for the target domain may not even be present in the source domain as illustrated in the toy example in the ﬁgure. For instance this phenomenon can hap pen in natural language processing, where different genreshs (a) Source domainht (b) Target domain Figure 1: A simple dataset for DA, the vertical dimension is r elevant for the target domain, but not for the source. often use very different vocabulary to described similar co n cepts. Here the target domain is rotated 45deg compared to the source domain. A linear classiﬁer hsfor the source do main will have an accuracy of only around 84% on the target domain. If we perform feature selection on the source data, then we lose a feature that is relevant to the target domain and we will not be able to improve the accuracy. However, the two classes are well separated in the target domain, and i t should be possible to ﬁnd a largemargin classiﬁer separati ng the classes. Just trying to separate the classes in the target domain is no t enough, mainly because this does not tell us which class is which, since no labeled target data are available. For that w e need to use the relation to the source domain. More generally , there is a tradeoff between having a classiﬁer that separat es the classes in the target domain, and a classiﬁer that stays close to the knowledge from the source domain. We propose to model such tradeoff by casting domain adaptation as the problem of ﬁnding a ‘stable’ label for each target example. We introduce the notion of labeling stability, motivated by a generalization bound for linear large margin classiﬁers: a target labeling is stable when, with high expectation, a tar get hypothesis trained on a random subsample of the target data with that labeling yields the same labeling. To ﬁnd stable ta r get labelings we use a formalization based on random walks. We deﬁne a Markov chain with states equal to labelings from large margin linear classiﬁers and onestep transition pro ba bilities deﬁned using the proposed notion of labeling stabi lity. Then we perform a random walk starting at the labeling obtained from the source hypothesis. The walk will be attracte d toward more stable labelings, which will be visited more of ten. The majority vote of the labelings visited by the walk provides our ﬁnal estimated label for each target example. We call the resulting unsupervised adaptation algorithm RWA ( Random Walk based Adaptation). RWA is strikingly simple to implement and apply. It does not rely on data trans formations, which are in general computational prohibitiv e in the presence of many input features. RWA does not need to access the source data. It acts on the original feature spa ce, hence can take full advantage of the use of deep features from external pretrained deep neural networks, as demonstrate d by the results of our experiments. Results of extensive expe r iments on sentiment analysis and image object recognition show stateoftheart performance of RWA across adaptatio n datasets with diverse nature and characteristics. Notably , us ing deep learning features from pretrained deep neural net  works RWA outperforms much more involved endtoend DA methods based on deep learning. Our contributions can be summarized as follows: (1) a new deﬁnition of stability of a target labeling inspired by a gen er alization bound for linear large margin classiﬁers; (2) a ne w representation of the DA problem based on random walks; (3) a strikingly simple method for unsupervised DA; (4) a direct and effective way to exploit deep features from pre trained deep neural networks for visual adaptation tasks; ( 5) new stateoftheart results on hard adaptation tasks with im age as well as text data. 2 Related work "
582,Automated Detection of Label Errors in Semantic Segmentation Datasets via Deep Learning and Uncertainty Quantification.txt,"In this work, we for the first time present a method for detecting label
errors in image datasets with semantic segmentation, i.e., pixel-wise class
labels. Annotation acquisition for semantic segmentation datasets is
time-consuming and requires plenty of human labor. In particular, review
processes are time consuming and label errors can easily be overlooked by
humans. The consequences are biased benchmarks and in extreme cases also
performance degradation of deep neural networks (DNNs) trained on such
datasets. DNNs for semantic segmentation yield pixel-wise predictions, which
makes detection of label errors via uncertainty quantification a complex task.
Uncertainty is particularly pronounced at the transitions between connected
components of the prediction. By lifting the consideration of uncertainty to
the level of predicted components, we enable the usage of DNNs together with
component-level uncertainty quantification for the detection of label errors.
We present a principled approach to benchmarking the task of label error
detection by dropping labels from the Cityscapes dataset as well from a dataset
extracted from the CARLA driving simulator, where in the latter case we have
the labels under control. Our experiments show that our approach is able to
detect the vast majority of label errors while controlling the number of false
label error detections. Furthermore, we apply our method to semantic
segmentation datasets frequently used by the computer vision community and
present a collection of label errors along with sample statistics.","In many applications such as automated driving and medical imaging, large amounts of data are collected and labeled with the longterm goal of obtaining a strong pre dictor for such labels via artiﬁcial intelligence, in particular via deep learning [13, 19, 21, 24, 25, 29]. Acquisition of socalled semantic segmentation ground truth, i.e., the pixel wise annotation within a chosen set of classes on which we focus in this work, involves huge amounts of human labor. A German study states an effort of about 1.5 working hours per high deﬁnition street scene image [33]. Typically, in dustrial and scientiﬁc labelling processes consist of an iter ative cycle of data labeling and quality assessment. Since the longterm goal of acquiring enough data to train e.g. deep neural networks (DNNs) to close to ground truth per formance requires huge amounts of data, partial automa tion of the labeling cycle is desirable. Two research direc tions aiming at this goal are active learning, which aims at labeling only those data points that leverage the model performance a lot (see e.g. [6, 22, 34]), and the automated detection of label errors (see [9, 28]). Currently, in active learning for semantic segmentation, a moderate number of methods exists. This is also due to the fact that active learn ing comes with an increased computational cost as a DNN has to be trained several times over the course of the ac tive learning iterations [6, 22]. Typically, these methods as sume that perfect ground truth can be obtained by an ora cle/teacher in each active learning iteration. In practice this is not the case and annotations are subject to multiple review loops. In that regard, current methods mostly study how noisy labels affect the model performance [16,44], with the insight that DNNs can deal with a certain amount of label noise quite well. Methods for modeling label uncertainty in medical image segmentation, semantic street scene seg mentation and everyday scene segmentation were proposed in [18, 23, 38, 42, 45]. For image classiﬁcation tasks, the detection of label er rors was studied in [28]. Importantly, it was pointed out that label errors harm the stability of machine learning bench marks [27]. This stresses the importance of being able to detect label errors, which will help to improve model bench marks and speed up dataset review processes. In this work, we for the ﬁrst time study the task of de tecting label errors in semantic segmentation in settings of low inter and intraobserver variability. While DNNs proarXiv:2207.06104v1  [cs.CV]  13 Jul 2022vide predictions on pixel level, we assess DNN predictions on the level of connected components belonging to a given class by utilizing [31]. Note that this is crucial since a connected component has uncertain labels at its boundary, which makes label error detection on pixel level a complex task. For each connected component, we estimate the prob ability of that prediction being correct. If a connected com ponent has a high estimated probability of being correct, while it is signaled to be false positive w.r.t. ground truth, we consider that component as a potential label error. We study the performance of our label error detection method on syn thetic image data from the driving simulator CARLA [10] and on Cityscapes [8]. CARLA gives us a guarantee of be ing per se free of label errors such that we can provide a clean evaluation. To this end, we remove objects from the ground truth and study whether our method is able to iden tify these components as overlooked by the ground truth. Cityscapes provides high quality ground truth with only a small amount of label errors. The ground truth is available in terms of polygons such that we can drop connected com ponents as well. In both cases it turns out that our method is able to detect most of the dropped labels while keeping the amount of false positive label errors under control. We be lieve that our method offers huge potential to make labeling processes more efﬁcient. Our contribution can be summa rized as follows: • We for the ﬁrst time present a method that detects label errors in semantic segmentation. • Utilizing [31] we detect label errors on the level of con nected components. • We introduce a principled benchmark for the detection of label errors based on [10] and [8]. • We apply our method to additional datasets [1, 12, 47] and provide examples of label errors that we found. By manually assessing samples of that data we evaluate the precision of our method on those datasets. For all of those four realworld datasets we studied, we achieved a precision between 47:5%and67:5%of correctly predicted label errors. We show that our method is able to ﬁnd both overlooked and classwise ﬂipped labels while keeping the amount of prediction to review considerably low. 2. Related Work "
420,SemiGNN-PPI: Self-Ensembling Multi-Graph Neural Network for Efficient and Generalizable Protein-Protein Interaction Prediction.txt,"Protein-protein interactions (PPIs) are crucial in various biological
processes and their study has significant implications for drug development and
disease diagnosis. Existing deep learning methods suffer from significant
performance degradation under complex real-world scenarios due to various
factors, e.g., label scarcity and domain shift. In this paper, we propose a
self-ensembling multigraph neural network (SemiGNN-PPI) that can effectively
predict PPIs while being both efficient and generalizable. In SemiGNN-PPI, we
not only model the protein correlations but explore the label dependencies by
constructing and processing multiple graphs from the perspectives of both
features and labels in the graph learning process. We further marry GNN with
Mean Teacher to effectively leverage unlabeled graph-structured PPI data for
self-ensemble graph learning. We also design multiple graph consistency
constraints to align the student and teacher graphs in the feature embedding
space, enabling the student model to better learn from the teacher model by
incorporating more relationships. Extensive experiments on PPI datasets of
different scales with different evaluation settings demonstrate that
SemiGNN-PPI outperforms state-of-the-art PPI prediction methods, particularly
in challenging scenarios such as training with limited annotations and testing
on unseen data.","Proteinprotein Interactions (PPIs) are central to vari ous cellular functions and processes, such as signal transduction, cellcycle progression, and metabolic path ways [Acuner Ozbabacan et al. , 2011 ]. Therefore, the identi ﬁcation and characterization of PPIs are of great importance for understanding protein functions and disease occurrence, which can potentially facilitate therapeutic target identiﬁca tion[Petta et al. , 2016 ]and the novel drug design [Skrabanek equal contributionet al. , 2008 ]. In past decades, highthroughput experimen tal methods, e.g., yeast twohybrid screens (Y2H) [Fields and Song, 1989 ], and mass spectrometric protein complex identi ﬁcation (MSPCI) [Hoet al. , 2002 ]have been developed to identify PPIs. Nevertheless, genomescale experiments are expensive, tedious, and timeconsuming while suffering from high error rates and low coverage [Luoet al. , 2015 ]. As such, there is an urgent need to establish reliable computational methods to identify PPIs with high quality and accuracy. In recent years, a large variety of highthroughput compu tational approaches for PPI prediction have been proposed, which can be broadly divided into two groups: classic ma chine learning (ML)based methods [Browne et al. , 2007; Lin and Chen, 2013; Guo et al. , 2008; Wong et al. , 2015; Chen and Liu, 2005 ]and deep learning (DL)based meth ods[Sunet al. , 2017; Du et al. , 2017; Hashemifar et al. , 2018; Chen et al. , 2019a; Lv et al. , 2021 ]. Compared to clas sic ML methods, DL algorithms are capable of processing complicated and largescale data and extracting useful fea tures automatically, achieving signiﬁcant success in a di verse range of bioinformatics applications [Min et al. , 2017; Soleymani et al. , 2022 ], including PPI prediction [Soleymani et al. , 2022 ]. Most existing DLbased methods treat inter actions as independent instances, ignoring protein correla tions. PPI can be naturally formulated as graph networks with proteins and interactions represented as nodes and edges, respectively [Margolin et al. , 2006; Pio et al. , 2020 ]. To improve PPI prediction performance, recent works [Yang et al., 2020; Lv et al. , 2021 ]have been proposed to investi gate the correlations between PPIs using various graph neu ral network (GNN) architectures [Kipf and Welling, 2016; Xuet al. , 2019 ]. However, they are limited by ignoring learning label dependencies for multitype PPI prediction. It has recently become common practice to employ Graph Convolutional Networks (GCNs) to capture label correlation in a wide range of multilabel tasks [Chen et al. , 2019b; Wang et al. , 2020 ]. Nevertheless, multilabel learning uti lizing label graphs predominantly works in the visual domain and has yet to be extended to PPI prediction tasks. In general, a desired PPI prediction framework should be efﬁcient, transferable, and generalizable, whereas two maarXiv:2305.08316v1  [qbio.MN]  15 May 2023jor bottlenecks deriving from imperfect datasets have hin dered the development of such models. Label scarcity: De spite the tremendous progress in PPI research using various computational and experimental methods, many interactions still need to be annotated from experimental data. Conse quently, only a small portion of labeled samples can be used for model training. It can be a signiﬁcant bottleneck in ob taining robust and accurate PPI prediction models. Domain shift: Most existing methods are only developed and val idated using indistribution data ( i.e., trainsethomologous testsets), receiving severe performance degradation when be ing deployed to unseen data with different distributions ( i.e., trainsetheterologous testsets). Although [Lvet al. , 2021 ]de sign new evaluations to better reﬂect model generalization, giving instructive and consistent assessment across datasets, the domain shift issue still needs to be fully explored for PPI prediction. Therefore, how to deal with imperfect data for improving model efﬁciency and generalization remains a vital issue in PPI prediction. Recent studies [Zhang et al. , 2021; Zhao et al. , 2022 ]show that selfensemble methods with semisupervised learning (SSL) [Laine and Aila, 2017; Tarvainen and Valpola, 2017 ]have demonstrated effective ness in addressing both label scarcity and domain shift. In this work, to tackle the above challenges and limita tions, we propose an efﬁcient and generalizable PPI predic tion framework, referred to as Selfensembling multiGraph Neural Network ( SemiGNNPPI ). Firstly, we propose lever aging graph structure to model protein correlations and label dependencies for multigraph learning. Speciﬁcally, we learn interdependent classiﬁers to extract information from the la bel graph, which are then applied to the protein representa tions aggregated by neighbors in the protein graph for multi type PPI prediction. Secondly, we propose combining GNN with Mean Teacher [Tarvainen and Valpola, 2017 ], a power ful SSL model, to explore unlabeled data for selfensemble graph learning. In our framework, the student model learns to classify the labeled data accurately and also distills the knowledge beneath unlabeled data from the teacher model with multiple graph consistency constraints for improving the model performance under complex scenarios. To the best of our knowledge, this is the ﬁrst study to explore efﬁcient and generalizable multitype PPI prediction. Precisely, the main contributions of the work can be summarized as follows: • For multitype PPI prediction, we ﬁrst investigate the limitations and challenges of existing methods under complex but realistic scenarios, and then propose an ef fective Selfensembling multiGraph Neural Network based PPI prediction ( SemiGNNPPI ) framework for improving model efﬁciency and generalization. • In SemiGNNPPI, we construct multiple graphs to learn correlations between proteins and label dependencies si multaneously. We further advance GNN with Mean Teacher to effectively utilize unlabeled data by consis tency regularization with multiple constraints. • Extensive experiments on three PPI datasets with dif ferent settings demonstrate that SemiGNNPPI outper forms other stateoftheart methods for multilabel PPI prediction under various challenging scenarios.2 Related Work "
530,Robust Optimization for Fairness with Noisy Protected Groups.txt,"Many existing fairness criteria for machine learning involve equalizing some
metric across protected groups such as race or gender. However, practitioners
trying to audit or enforce such group-based criteria can easily face the
problem of noisy or biased protected group information. First, we study the
consequences of naively relying on noisy protected group labels: we provide an
upper bound on the fairness violations on the true groups G when the fairness
criteria are satisfied on noisy groups $\hat{G}$. Second, we introduce two new
approaches using robust optimization that, unlike the naive approach of only
relying on $\hat{G}$, are guaranteed to satisfy fairness criteria on the true
protected groups G while minimizing a training objective. We provide
theoretical guarantees that one such approach converges to an optimal feasible
solution. Using two case studies, we show empirically that the robust
approaches achieve better true group fairness guarantees than the naive
approach.","As machine learning becomes increasingly pervasive in real world decision making, the question of ensuring fairness of ML models becomes increasingly important. The deﬁnition of what it means to be “fair” is highly context dependent. Much work has been d one on developing mathematical fairness criteria according to various societal and ethica l notions of fairness, as well as methods for building machinelearning models that satisfy those fairn ess criteria [see, e.g., 21, 32, 53, 41, 58, 14, 25, 55]. Many of these mathematical fairness criteria are groupbased , where a target metric is equalized or enforced over subpopulations in the data, also known as protected groups . For example, the equality of opportunity criterion introduced by Hardt et al. [32] speciﬁes that the t rue positive rates for a binary classiﬁer are equalized across protected group s. The demographic parity [21] criterion requires that a classiﬁer’s positive prediction rates are e qual for all protected groups. ∗First two authors have equal contributions. 34th Conference on Neural Information Processing Systems ( NeurIPS 2020), Vancouver, Canada.One important practical question is whether or not these fai rness notions can be reliably measured or enforced if the protected group information is noisy, mis sing, or unreliable. For example, survey participants may be incentivized to obfuscate their respon ses for fear of disclosure or discrimination, or may be subject to other forms of response bias. Social desi rability response bias may affect par ticipants’ answers regarding religion, political afﬁliat ion, or sexual orientation [40]. The collected data may also be outdated: census data collected ten years ag o may not an accurate representation for measuring fairness today. Another source of noise arises from estimating the labels of the protected groups. For various image recognition tasks (e.g., face detection), one may want to me asure fairness across protected groups such as gender or race. However, many large image corpora do n ot include protected group labels, and one might instead use a separately trained classiﬁer to e stimate group labels, which is likely to be noisy [12]. Similarly, zip codes can act as a noisy indicat or for socioeconomic groups. In this paper, we focus on the problem of training binary clas siﬁers with fairness constraints when only noisy labels, ˆG∈{1,...,ˆm}, are available for mtrue protected groups, G∈{1,...,m}, of interest. We study two aspects: First, if one satisﬁes fairn ess constraints for noisy protected groups ˆG, what can one say with respect to those fairness constraints for the true groups G? Second, how can side information about the noise model between ˆGandGbe leveraged to better enforce fairness with respect to the true groups G? Contributions: Our contributions are threefold: 1. We provide a bound on the fairness violations with respect to the true groups Gwhen the fairness criteria are satisﬁed for the noisy groups ˆG. 2. We introduce two new robustoptimization methodologies that satisfy fairness criteria on the true protected groups Gwhile minimizing a training objective. These methodologie s differ in convergence properties, conservatism, and noise model speciﬁcation. 3. We show empirically that unlike the naïve approach, our tw o proposed approaches are able to satisfy fairness criteria with respect to the true groups Gon average. The ﬁrst approach we propose (Section 5) is based on distribu tionally robust optimization (DRO) [19, 8]. Let pdenotes the full distribution of the data X,Y∼p. Letpjbe the distribution of the data conditioned on the true groups being j, soX,Y|G=j∼pj; andˆpjbe the distribution of X,Y conditioned on the noisy groups. Given an upper bound on the t otal variation (TV) distance γj≥TV(pj,ˆpj)for each j∈ {1,...,m}, we deﬁne ˜pjsuch that the conditional distributions (X,Y|˜G=j∼˜pj) fall within the bounds γiwith respect to ˆG. Therefore, the set of all such ˜pjis guaranteed to include the unknown true group distribution pj,∀j∈G. Because it is based on the wellstudied DRO setting, this approach has the advantage o f being easy to analyze. However, the results may be overly conservative unless tight bounds {γj}m j=1can be given. Our second robust optimization strategy (Section 6) uses a r obust reweighting of the data from soft protected group assignments, inspired by criteria pro posed by Kallus et al. [37] for auditing the fairness of ML models given imperfect group information. Ex tending their work, we optimize a con strained problem to achieve their robust fairness criteria , and provide a theoretically ideal algorithm that is guaranteed to converge to an optimal feasible point, as well as an alternative practical version that is more computationally tractable. Compared to DRO, th is second approach uses a more precise noise model, P(ˆG=k|G=j), between ˆGandGfor all pairs of group labels j,k, that can be esti mated from a small auxiliary dataset containing groundtru th labels for both GandˆG. An advantage of this more detailed noise model is that a practitioner can i ncorporate knowledge of any bias in the relationship between GandˆG(for instance, survey respondents favoring one socially pr eferable response over others), which causes it to be less likely than DRO to result in an overlyconservative model. Notably, this approach does notrequire that ˆGbe a direct approximation of G—in fact, G andˆGcan represent distinct (but related) groupings, or even gro upings of different sizes, with the noise model tying them together. For example, if Grepresents “language spoken at home,” then ˆG could be a noisy estimate of “country of residence.” 22 Related work "
400,Embedded Ensembles: Infinite Width Limit and Operating Regimes.txt,"A memory efficient approach to ensembling neural networks is to share most
weights among the ensembled models by means of a single reference network. We
refer to this strategy as Embedded Ensembling (EE); its particular examples are
BatchEnsembles and Monte-Carlo dropout ensembles. In this paper we perform a
systematic theoretical and empirical analysis of embedded ensembles with
different number of models. Theoretically, we use a Neural-Tangent-Kernel-based
approach to derive the wide network limit of the gradient descent dynamics. In
this limit, we identify two ensemble regimes - independent and collective -
depending on the architecture and initialization strategy of ensemble models.
We prove that in the independent regime the embedded ensemble behaves as an
ensemble of independent models. We confirm our theoretical prediction with a
wide range of experiments with finite networks, and further study empirically
various effects such as transition between the two regimes, scaling of ensemble
performance with the network width and number of models, and dependence of
performance on a number of architecture and hyperparameter choices.","A common strategy of improving accuracy of predic tive models is model ensembling [Dietterich, 2000]. In Proceedings of the 25thInternational Conference on Arti cial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022 by the au thor(s).its simplest form, several models are constructed inde pendently, and their outputs are averaged. Despite its simplicity, this strategy is very reliable and ecient, almost invariably improving the accuracy and robust ness of predictions [Dusenberry et al., 2020]. However, a major downside of this strategy is a sub stantial increase of resources required for model train ing and execution: training time, inference time, and required storage scale linearly with the number of models in the ensemble. This downside is especially acute for deep neural networks (DNNs) since they are already complex { as a result, DNN ensembles be come challenging or even infeasible in many applica tions [Schwenk and Bengio, 2000,Huang et al., 2017]. Recently signicant attention was paid to the con struction of \lightweight"" ensembles that mitigate this issue [Wen et al., 2019, Havasi et al., 2020, Ram e et al., 2021, Wenzel et al., 2020, Zhang et al., 2021]. A lightweight ensemble attempts to retain the accu racy gain from ensembling while relaxing requirements for a particular resource. For example, snapshot en sembles [Huang et al., 2017, Garipov et al., 2018] re duce the ensemble training time (without signicantly aecting the storage and inference time). Lightweight ensembles typically have a lower accuracy than the standard independent ensembles of the same size, be cause of a lower diversity of their members. In this work we address what we call Embedded Ensem bles(EE). Their common idea is to construct dierent models by some kind of perturbation of a single refer ence neural network. Examples of EE include Monte Carlo (MC) dropout ensembles [Gal and Ghahramani, 2016] and BatchEnsembles [Wen et al., 2019]. Most weights in an embedded ensemble are just the shared reference network weights, so this ensemble requires much less storage than a respective ensemble of inde pendent reference networks. Furthermore, if the per turbation is restricted to the last layers, then network computations can be eciently reused among ensemarXiv:2202.12297v1  [stat.ML]  24 Feb 2022Embedded Ensembles: Innite Width Limit and Operating Regimes bled models making computation time comparable to that of a single model. The price one pays for this eciency is the lower ac curacy of embedded ensembles. In fact, while the ac curacy of the usual independent ensembles only in creases with additional models, it was observed em pirically [Havasi et al., 2020] that the accuracy of em bedded ensembles can degrade when the number of ensemble members is large. The primary purpose of this work is to systemati cally explore how performance of embedded ensembles scales with the number of models. An additional im portant factor that we consider is the size of the ref erence network. Intuitively, larger reference networks can accommodate more uncorrelated models and so provide higher ensemble accuracy. We conrm this intuition, both theoretically and empirically. Our contribution. We perform an extensive theo retical and empirical study of Embedded Ensembles. •We describe the behaviour of Embedded Ensem bles in the limit of innite reference network width. Particularly, we derive dynamic equation of EE model outputs describing their evolution under gradient descent. Also, we characterize at initialization the distribution of ensemble outputs and Neural Tangent Kernel of the ensemble. •In the innite width limit we identify indepen dent and collective operating regimes of Embed ded Ensembles. In the independent regime we show that EE is fully identical to the ensemble of independent reference networks. Also, we propose to use dierent gradient scalings for independent and collective regimes to ensure proper behavior of EEs with large number of ensemble models. We show that the operating regime of Embedded En semble is determined by the structure of individ ual parameters and their initialization strategy. •We perform extensive experiments with embed ded ensembles on the CIFAR100 data set. We empirically observe the collective and independent regimes and demonstrate the transition between them. We observe that nitewidth EEs in the independent regime have an optimal number of models at which the highest accuracy is achieved; in agreement with our theory this optimal number increases with the network width. We further ex plore, both empirically and theoretically, a num ber of architecture modications and the scaling of the learning rates.   model 1 model 2 model 3 common Figure 1: All models in the BatchEnsemble have com mon fullyconnected (or convolutional) weights (col ored black), and a small number of pre and post ac tivation modulations (colored red, green or blue) that dier for each model. For each model, only the respec tive family of modulations (i.e., red, green or blue) is active on a forward pass. 2 Related Work "
211,Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification.txt,"Many unsupervised domain adaptive (UDA) person re-identification (ReID)
approaches combine clustering-based pseudo-label prediction with feature
fine-tuning. However, because of domain gap, the pseudo-labels are not always
reliable and there are noisy/incorrect labels. This would mislead the feature
representation learning and deteriorate the performance. In this paper, we
propose to estimate and exploit the credibility of the assigned pseudo-label of
each sample to alleviate the influence of noisy labels, by suppressing the
contribution of noisy samples. We build our baseline framework using the mean
teacher method together with an additional contrastive loss. We have observed
that a sample with a wrong pseudo-label through clustering in general has a
weaker consistency between the output of the mean teacher model and the student
model. Based on this finding, we propose to exploit the uncertainty (measured
by consistency levels) to evaluate the reliability of the pseudo-label of a
sample and incorporate the uncertainty to re-weight its contribution within
various ReID losses, including the identity (ID) classification loss per
sample, the triplet loss, and the contrastive loss. Our uncertainty-guided
optimization brings significant improvement and achieves the state-of-the-art
performance on benchmark datasets.","Person reidentiﬁcation (ReID) is an important task that matches person images across times/spaces/cameras, which has many applications such as people tracking in smart re tail, image retrieval for ﬁnding lost children. Existing ap proaches achieve remarkable performance when the train ing and testing data are from the same dataset/domain. But they usually fail to generalize well to other datasets where there are domain gaps (Ge, Chen, and Li 2020). To ad dress this practical problem, unsupervised domain adaptive (UDA) person ReID attracts much attention for both the academic and industrial communities, where labeled source domain and unlabeled target domain data are exploited for training. *This work was done when Kecheng Zheng was an intern at MSRA. †Corresponding Author Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. Figure 1: Observations on the relations between the correct ness of pseudo labels and the uncertainty (which we measure by the inconsistency level of the output features of two mod els,i.e., the student model and the teacher model based on the mean teacher method (Tarvainen and Valpola 2017) for the target domain samples (obtained from Duke !Market). We found the uncertainty for samples with wrong/noisy pseudo labels (red curve) is usually larger than those (green curve) with correct/clean pseudo labels. Typical UDA person ReID approaches (Ge, Chen, and Li 2020; Zhai et al. 2020a; Zhong et al. 2019; Zheng et al. 2020; Song et al. 2020) include three steps: feature pretraining with labeled source domain data, clusteringbased pseudo label prediction for the target domain data, and feature rep resentation learning/ﬁnetuning with the pseudolabels. The last two steps are usually iteratively conducted to promote each other. However, the pseudolabels obtained/assigned through clustering usually contain noisy (wrong) labels due to the divergence/domain gap between the source and target data, and the imperfect results of the clustering algorithm. Such noisy labels would mislead the feature learning and harm the domain adaptation performance. Thus, alleviating the negative effects of those samples with unreliable/noisy pseudo labels is important for the success of domain adap tation .arXiv:2012.08733v2  [cs.CV]  17 Dec 2020The challenge lies in 1) how to identify samples that are prone to have noisy pseudo labels; 2) how to alleviate their negative effects during the optimization. In this paper, to an swer the ﬁrst question, we have observed abundant samples and analyzed the relationship between the characteristics of the samples and the correctness of pseudo labels. Based on the theory on uncertainty (Kendall and Gal 2017), a model has uncertainty on its prediction of an input sample. Here, we measure the inconsistency level of the output features of two models (the student model and the teacher model based on the mean teacher method (Tarvainen and Valpola 2017)) and take it as the estimated uncertainty of a target domain sample. As shown in Fig. 1, we observe the distribution of the uncertainty (inconsistency levels) for correct/clean pseudo labels and wrong pseudo labels. We found that the uncertainty values for the samples with wrong pseudo labels are usually larger than those with correct pseudo labels. This motivates us to estimate and exploit the uncertainty of sam ples to alleviate the negative effects of noisy pseudo labels, enabling effective domain adaptation. We answer the second question by carefully incorporating the uncertainty of sam ples into classiﬁcation loss, triplet loss, and contrastive loss, respectively. We summarize our main contributions as follows: • We propose a network named Uncertaintyguided Noise Resilient Network (UNRN) to explore the credibility of the predicted pseudo labels of target domain samples for effective domain adaptive person ReID. • We develop an uncertainty estimation strategy by calcu lating the inconsistency of two models in terms of their predicted soft multilabels. • We incorporate the uncertainty of samples to the ID clas siﬁcation loss, triplet loss, and contrastive loss through reweighting to alleviate the negative inﬂuence of noisy pseudo labels. Extensive experiments demonstrate the effectiveness of our framework and the designed components on unsu pervised person ReID benchmark datasets. Our scheme achieves the stateoftheart performance on all the bench mark datasets. 2 Related Work "
421,Self-Supervised Learning for Group Equivariant Neural Networks.txt,"This paper proposes a method to construct pretext tasks for self-supervised
learning on group equivariant neural networks. Group equivariant neural
networks are the models whose structure is restricted to commute with the
transformations on the input. Therefore, it is important to construct pretext
tasks for self-supervised learning that do not contradict this equivariance. To
ensure that training is consistent with the equivariance, we propose two
concepts for self-supervised tasks: equivariant pretext labels and invariant
contrastive loss. Equivariant pretext labels use a set of labels on which we
can define the transformations that correspond to the input change. Invariant
contrastive loss uses a modified contrastive loss that absorbs the effect of
transformations on each input. Experiments on standard image recognition
benchmarks demonstrate that the equivariant neural networks exploit the
proposed equivariant self-supervised tasks.","Selfsupervised learning is the method by which we deﬁne pretext tasks based on our prior knowledge about the input data and train the feature extractor using the pretext tasks without supervised labels. Pretext tasks include tasks to solve illposed problems such as image completion, tasks to predict image context, and tasks to make the features of the augmented images from the same image close to each other. Currently, selfsupervised learning demonstrates comparative accuracy to supervised learning and is an effective framework for learning features in an unsupervised manner. Another direction for utilizing prior knowledge is to incorporate the knowledge into the model structure of the feature extractor. For example, the convolutional layer is designed to be robust to the local translation of the object in an image. Group equivariant neural networks are the effective framework for utilizing the knowledge of invariance for the required transformations such as image rotations and image ﬂipping in the neural networks structure. Given the input datax, the transformation on the input Tin(g), and the transformations on the output Tout(g), the group equivariant neural networks fare constructed to satisfy Tout(g)(f(x)) =f(Tin(g)(x))for any transformation parameter g, input datax, and model parameter . Because the structure of group equivariant neural networks are restricted to satisfy equivariance, this restriction regularizes the model, and the learned model generally demonstrates better accuracy than the standard nonequivariant neural networks. Therefore, we expect that we can obtain the effective feature learning method by combining these two ideas to utilize the prior knowledge. When we combine the idea of selfsupervised learning and the group equivariant neural networks, it is important to design the method such that these two components do not adversely affect each other. The functions that the group equivariant neural networks can learn are restricted to the mappings that preserve equivariance. Therefore, when a pretext task requires the mapping to violate the equivariance, it is difﬁcult to learn the function through this pretext task. In this paper, we propose a selfsupervised task that is suitable for group equivariant neural networks. The idea is to construct a selfsupervised loss that does not change under the transformations on the input data. This invariance guarantees that we can learn the same equivariant model even when we apply the considered transformations to thearXiv:2303.04427v1  [cs.CV]  8 Mar 2023SelfSupervised Learning for Group Equivariant Neural Networks A P REPRINT input. To construct a selfsupervised loss that satisﬁes this condition, we propose two concepts for selfsupervised loss: equivariant pretext labels and invariant contrastive loss. Equivariant pretext labels are constructed such that they are consistent with the considered transformations on the input. This indicates that when we apply the transformations on the input, the corresponding pretext labels are also changed according to the considered transformations. The invariant contrastive loss is a loss that does not change when we apply transformations to each input data. We extend several existing selfsupervised pretext tasks to satisfy these concepts. We apply the proposed loss to the image classiﬁcation model on the ImageNet dataset and evaluate the model using several image recognition benchmarks. We demonstrate that the trained group equivariant neural networks demonstrate good classiﬁcation accuracy when we use the proposed loss. The contributions of the paper are as follows: •We propose the concepts of equivariant pretext labels and invariant contrastive loss to train the group equivariant neural networks in a selfsupervised manner. • We propose an equivariant extension to several existing selfsupervised tasks. •We apply our method to standard image recognition benchmarks and demonstrate the effectiveness of the proposed loss. 2 Related Work "
142,Segmental Convolutional Neural Networks for Detection of Cardiac Abnormality With Noisy Heart Sound Recordings.txt,"Heart diseases constitute a global health burden, and the problem is
exacerbated by the error-prone nature of listening to and interpreting heart
sounds. This motivates the development of automated classification to screen
for abnormal heart sounds. Existing machine learning-based systems achieve
accurate classification of heart sound recordings but rely on expert features
that have not been thoroughly evaluated on noisy recordings. Here we propose a
segmental convolutional neural network architecture that achieves automatic
feature learning from noisy heart sound recordings. Our experiments show that
our best model, trained on noisy recording segments acquired with an existing
hidden semi-markov model-based approach, attains a classification accuracy of
87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6%
accuracy of the state-of-the-art statistical classifier trained and evaluated
on the same dataset. Our results indicate the potential of using neural
network-based methods to increase the accuracy of automated classification of
heart sound recordings for improved screening of heart diseases.","Heart diseases constitute a signicant global health burden. Just one subset of these dis eases, valvular heart disease (VHD) resulting from rheumatic fever, causes 300,000500,000 preventable deaths each year globally, primarily in developing countries.1,2Early detection of many heart diseases is crucial for optimal treatment management to prevent disease pro gression.3,4In developing countries, the standard practice for screening of heart diseases such as VHD and cardiac arrhythmia is cardiac auscultation to listen for abnormal heart sounds. Patients found to have suspicious abnormalities are then referred to specialists for proper diagnosis by a much more expensive echocardiographic procedure.3Although cardiac auscul tation has been replaced by echocardiography for screening in industrialized countries, the costeectiveness and procedural simplicity of auscultation make it an important screening tool for primary care providers and clinicians in underresourced communities.5,6 The main challenge in cardiac auscultation is the diculty of detecting and interpreting subtle acoustic features associated with heart sound abnormalities. Manual classication of heart sounds suers from high intraobserver variability,7{14causing false positive and false negative results. Much work has been done in trying to improve screening accuracy, including eorts to design devices to record heart sounds and automatically classify them. However, the biggest challenge for this task remains in developing an accurate classier for heart sound This work was nished in May 2016, and remains unpublished until December 2016 due to a request from the data provider.arXiv:1612.01943v1  [cs.SD]  6 Dec 20162 recordings, which are often obtained in noisy environments. Here, we propose a novel approach based on segmental convolutional neural networks to classication of heart sound recordings. Our approach achieves automatic feature learning together with accurate prediction of the abnormality. On noisy recordings, this approach outperforms prior classiers using a stateof theart feature set developed for noiseless recordings. The rest of this paper is organized as follows. In Section 2, we discuss related previous research. In Section 3, we introduce the methods that we used to classify noisy heart sound recordings, including preprocessing of data, the use of traditional classiers, and our segmental convolutional neural network models. Next, in Section 4, we present the performance of our classiers, along with our analysis of these results. We discuss the limitations of our work and future directions in Section 5 and conclude our work in Section 6. 2. Related Work "
233,Encoding Event-Based Data With a Hybrid SNN Guided Variational Auto-encoder in Neuromorphic Hardware.txt,"Neuromorphic hardware equipped with learning capabilities can adapt to new,
real-time data. While models of Spiking Neural Networks (SNNs) can now be
trained using gradient descent to reach an accuracy comparable to equivalent
conventional neural networks, such learning often relies on external labels.
However, real-world data is unlabeled which can make supervised methods
inapplicable. To solve this problem, we propose a Hybrid Guided Variational
Autoencoder (VAE) which encodes event based data sensed by a Dynamic Vision
Sensor (DVS) into a latent space representation using an SNN. These
representations can be used as an embedding to measure data similarity and
predict labels in real-world data. We show that the Hybrid Guided-VAE achieves
87% classification accuracy on the DVSGesture dataset and it can encode the
sparse, noisy inputs into an interpretable latent space representation,
visualized through T-SNE plots. We also implement the encoder component of the
model on neuromorphic hardware and discuss the potential for our algorithm to
enable real-time learning from real-world event data.","Prior work has demonstrated how online supervised learning with labeled data can be used for tasks such as rapid, eventdriven learn ing from neuromorphic sensor data [ 26]. However, realworld data is unlabeled and, in the case of classification, can have classes that were not anticipated. Therefore, to leverage realworld data, labels must be generated by a supervisor in realtime without a priori knowledge of the number of classes. Additionally, while a trained classifier is trained to generate class labels, it cannot generalize to new classes. This is compounded by the fact that neural network classifiers trained using gradient de scent are usually overconfident of their classification, making the learning of new classes impractical. An alternative approach is to use the intermediate layers of a trained neural network classifier as pseudolabels or features for learning classes. In this work, we formalize this idea using an eventdriven guided Variational Auto Encoder (VAE) which is trained to generate an embedding that disentangles according to labels in a labeled dataset and that gener alizes to new data. The resulting embedding space can then either be used for (pseudo)labels for supervised learning or for measuring data similarity. We focus our demonstration of the guided VAE on a Dynamic Vision Sensor (DVS) gesture learning problem because of the availability of an eventbased gesture dataset and the high relevance of gesture recognition use cases [1]. Neuromorphic Dynamic Vision Sensors (DVS) inspired by the biological retina capture temporal, pixelwise intensity changes as a sparse stream of binary events [ 8]. This approach has key advantages over traditional RGB cameras, such as faster response times, better temporal resolution, and invariance to static image features like lighting and background. Thus, raw DVS sensor data intrinsically emphasizes the dynamic movements that comprise most natural gestures. However, effectively processing DVS event streams remains an open challenge. Events are asynchronous and spatially sparse, making it challenging to directly apply conven tional vision algorithms [8, 9]. Spiking Neural Networks (SNNs) can efficiently process and learn from eventbased data while taking advantage of temporalarXiv:2104.00165v2  [cs.NE]  8 Mar 2022NICE 2022, March 28April 1, 2022, Virtual Event, USA Kenneth Stewart, Andreea Danielescu, Timothy M Shea, and Emre O Neftci Figure 1: The Hybrid GuidedVAE architecture. Streams of gesture events recorded using a Dynamic Vision Sensor (DVS) are input into a Spiking Neural Network (SNN) that encodes the spatiotemporal features of the input data into a latent structure 𝑧.𝑃and𝑄are presynaptic traces and 𝑈is the membrane potential of the spiking neuron. For clarity, only a single layer of the SNN is shown here and refractory states𝑅are omitted. To help disentangle the latent space, a portion of the 𝑧equal to the number of target features 𝑦∗is input into a classifier that trains each latent variable to en code these features (Exc. Loss). The remaining 𝑧, noted\𝑚 are input into a different classifier that adversarially trains the latent variables to not encode the target features so they encode for other features instead (Inh. Loss). The latent state 𝑧is decoded back into 𝑥∗using the conventional deconvolu tional decoder layers. information [ 22]. SNN models emulate the properties of biological neurons and can be used for hierarchical feature extraction from the precise timing of events through eventbyevent processing [ 11]. Recent work demonstrated how SNNs can be trained endtoend using gradient backpropagation in time and standard autodifferen tiation tools, making the integration of SNNs possible as part of modern machine learning and deep learning methods [2, 25, 30]. Here, we take advantage of this capability by incorporating a convolutional SNN into a Variational Autoencoder (VAE) to encode spatiotemporal streams of events recorded by the DVS (Figure 1). The goal of the VAE is to embed the streams of DVS events into a latent space which facilitates the evaluation of event data similarity for semisupervised learning from realworld data. To best use the underlying hardware, we implement a hybrid VAE to process the DVS data, with an SNNbased encoder and a conventional (non spiking) convolutional network decoder. To ensure the latent space represents features which are perceptually salient and useful for recognition, we use a guided VAE to disentangle the features that account for variation in the underlying structure of the data. Our Hybrid GuidedVAE encodes and disentangles the variations of the structure of event data allowing for the clustering of similar patterns, such as similar looking gestures, and assigning of pseudo labels to novel samples. The key contributions of this work are:(1)Endtoend trainable eventbased SNNs for processing neu romorphic sensor data eventbyevent and embedding them in a latent space. (2)A Hybrid GuidedVAE that encodes eventbased camera data in a latent space representation of salient features for clus tering and pseudolabeling. (3)A proofofconcept implementation of the Hybrid Guided VAE on Intel’s Loihi Neuromorphic Research Processor. The ability to encode event data into a disentangled latent repre sentation is a key feature to enable learning from realworld data for tasks such as midair gesture recognition systems that are less rigid and more natural because they can adapt to each user. 2 RELATED WORK "
300,Learning to Bootstrap for Combating Label Noise.txt,"Deep neural networks are powerful tools for representation learning, but can
easily overfit to noisy labels which are prevalent in many real-world
scenarios. Generally, noisy supervision could stem from variation among
labelers, label corruption by adversaries, etc. To combat such label noises,
one popular line of approach is to apply customized weights to the training
instances, so that the corrupted examples contribute less to the model
learning. However, such learning mechanisms potentially erase important
information about the data distribution and therefore yield suboptimal results.
To leverage useful information from the corrupted instances, an alternative is
the bootstrapping loss, which reconstructs new training targets on-the-fly by
incorporating the network's own predictions (i.e., pseudo-labels).
  In this paper, we propose a more generic learnable loss objective which
enables a joint reweighting of instances and labels at once. Specifically, our
method dynamically adjusts the per-sample importance weight between the real
observed labels and pseudo-labels, where the weights are efficiently determined
in a meta process. Compared to the previous instance reweighting methods, our
approach concurrently conducts implicit relabeling, and thereby yield
substantial improvements with almost no extra cost. Extensive experimental
results demonstrated the strengths of our approach over existing methods on
multiple natural and medical image benchmark datasets, including CIFAR-10,
CIFAR-100, ISIC2019 and Clothing 1M. The code is publicly available at
https://github.com/yuyinzhou/L2B.","Recent advances in deep learning have achieved great success on various computer vision applications, where largescale clean datasets are available. However, 1arXiv:2202.04291v1  [cs.CV]  9 Feb 2022noisy labels or intentional label corruption by an adversarial rival could easily cause dramatic performance drop [ 24]. This problem is even more crucial in the medical ﬁeld, given that the annotation quality requires great expertise. Therefore, understanding, modeling, and learning with noisy labels has gained great momentum in recent research eﬀorts [ 5,23,8,17,20,11,28,40,16,34, 47, 41, 49, 36, 48]. Existing methods of learning with noisy labels primarily take a loss correction strategy. One popular direction is to ﬁrst estimate the noise corruption matrix and then use it to correct the loss function [ 26,6]. However, correctly estimating the noise corruption matrix is usually challenging and often involves assumptions about the noise generation process [ 37,21,10]. Other research eﬀorts focus on selecting clean samples from the noisy data [ 11,7,44,4] by treating samples with small loss as clean ones [ 2]. Instead of directly discarding those “unclean” examples, an extension of this idea is focusing on assigning learnable weights to each example in the noisy training set [ 28,30], where noisy samples have low weights. However, discarding or attending less to a subset of the training data (e.g., noisy samples) can erase important information about the data distribution. To fully exploit the corrupted training samples, another direction is to leverage the network predictions (i.e., pseudolabels [ 14]) to correct or reweight the original labels [ 27,31], so that the holistic data distribution information could be preserved during network training. One representative work is the bootstrapping loss [ 27], which introduces a perceptual consistency term in the learning objective that assigns a weight to the pseudolabels to compensate for the erroneous guiding of noisy samples. While in this strategy, the weight for the pseudolabels is manually selected and remains the same for all training samples, which does not prevent ﬁtting the noisy ones and can even lead to lowquality label correction [ 1]. To tackle this challenge, Arazo et al. [ 1] designed a dynamic bootstrapping strategy to adjusts the label weight by ﬁtting a mixture model. Instead of separately reweighting labels or instances, in this paper, we propose a more generic learning strategy to enable a joint instance and label reweighting. We term our method as Learning to Bootstrap (L2B), where we aim to leverage the learner’s own predictions to bootstrap itself up for combating label noise from a metalearning perspective. During each training iteration, L2B learns to dynamically rebalance the importance between the real observed labels and pseudolabels, where the per sample weights are determined by the validation performance on a separated clean set in a meta network. Unlike the bootstrapping loss used in [ 27,1,46] which explicitly conducts relabeling by taking a weighted sum of the pseudo and the real label, L2B reweights the two losses associated with the pseudo and the real label instead (where the weights need not be summed as 1). In addition, we theoretically prove that our formulation, which reweights diﬀerent loss terms, can be reduced to the original bootstrapping loss and therefore conducts an implicit relabeling instead. By learning these weights in a metaprocess, our L2B yields substantial improvement (e.g., +8.9%improvement on CIFAR100 with 50% noise) compared with the instance reweighting baseline with almost no extra cost. We conduct extensive experiments on public natural image datasets 2(i.e., CIFAR10, CIFAR100, and Clothing 1M) and medical image dataset (i.e., ISIC2019), under diﬀerent types of simulated noise and realworld noise. Our method outperforms various existing explicit label correction and instance reweighting works, demonstrating the strengths of our approach. Our main contributions are as follows: •We propose a generic learnable loss objective which enables a joint instance and label reweighting, for combating label noise in deep learning models. •We prove that our new objective is, in fact, a more general form of the bootstrapping loss, and propose L2B to eﬃciently solve for the weights in a metalearning framework. •Compared with previous instance reweighting methods, L2B exploits noisy examples more eﬀectively without discarding them by jointly rebalancing the contribution of real and pseudo labels. •We show the theoretical convergence guarantees for L2B, and demonstrate its superior results on natural and medical image recognition tasks under both synthetic and realworld noise. 2 Related Works "
511,Efficient Stochastic Inference of Bitwise Deep Neural Networks.txt,"Recently published methods enable training of bitwise neural networks which
allow reduced representation of down to a single bit per weight. We present a
method that exploits ensemble decisions based on multiple stochastically
sampled network models to increase performance figures of bitwise neural
networks in terms of classification accuracy at inference. Our experiments with
the CIFAR-10 and GTSRB datasets show that the performance of such network
ensembles surpasses the performance of the high-precision base model. With this
technique we achieve 5.81% best classification error on CIFAR-10 test set using
bitwise networks. Concerning inference on embedded systems we evaluate these
bitwise networks using a hardware efficient stochastic rounding procedure. Our
work contributes to efficient embedded bitwise neural networks.","Research results in recent years have shown tremendous advances in solving complex problems using deep learning approaches. Especially classiﬁcation tasks based on image data have been a major target for deep neural networks ( DNN s) [8,14]. A challenge for leveraging the strengths of deep learning methods in embedded systems is their massive computational cost. Even relatively small DNN s often require millions of parameters and billions of operations for performing a single classiﬁcation. Model compression approaches can help to relax memory requirements as well as to reduce the number of required operations of DNN s. While some approaches consider special network topologies [ 8,11], another stream of research focuses on precision reduction of the model parameters. Recent publications of bitwise neural networks ( BNN s) have shown that network weights and activations can be reduced from a highprecision ﬂoatingpoint down to a binary representation, while maintaining classiﬁcation accuracy on benchmark datasets [ 5]. Stochastic projection of the network weights during training is a key component that enables this strong quantization. Studies These authors contributed equally to this work. yProfessor Gerd Ascheid is Senior Member IEEE. Submitted to 1st International Workshop on Efﬁcient Methods for Deep Neural Networks at 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Copyright c 2016 Robert Bosch GmbH. Rights reserved.arXiv:1611.06539v1  [cs.NE]  20 Nov 2016which employed this training method have so far only analyzed deterministic projections during testtime [4, 5, 15]. With techniques presented in this paper, we contribute to stochastic inference of bitwise neural networks on hardware. We show that stochastic rounding at testtime improves classiﬁcation accuracy of networks that were trained with stochastic weight projections (Section 3). Furthermore, we present a method which efﬁciently realizes stochastic rounding of network weights in a dedicated hardware accelerator (Section 4). We start off with a brief review of the literature on weight discretization (Section 2). 2 Related Work "
526,Epistemic Neural Networks.txt,"Intelligence relies on an agent's knowledge of what it does not know. This
capability can be assessed based on the quality of joint predictions of labels
across multiple inputs. In principle, ensemble-based approaches produce
effective joint predictions, but the computational costs of training large
ensembles can become prohibitive. We introduce the epinet: an architecture that
can supplement any conventional neural network, including large pretrained
models, and can be trained with modest incremental computation to estimate
uncertainty. With an epinet, conventional neural networks outperform very large
ensembles, consisting of hundreds or more particles, with orders of magnitude
less computation. The epinet does not fit the traditional framework of Bayesian
neural networks. To accommodate development of approaches beyond BNNs, such as
the epinet, we introduce the epistemic neural network (ENN) as an interface for
models that produce joint predictions.","Consider a conventional neural network trained to predict whether a random person would classify a drawing as a ‘rabbit’ or a ‘duck’. As illustrated in Figure 1, given a single drawing, the network outputs a marginal prediction that assigns probabilities to the two classes. If the probabilities are each 0.5, it remains unclear whether this is because labels sampled from random people are equally likely, or whether the neural network would learn a single class if trained on more data. Conventional neural networks do not distinguish these cases, even though it can be critical for decision making systems to know what they do not know. This capability can be assessed through the quality of jointpredictions (Wen et al., 2022). The two tables to the right of Figure 1 represent possible joint predictions that are each consistent with the network’s uniform marginal prediction. These joint predictions are over pairsof labels for the same image, (y1,y2)∈{R,D}×{R,D}. For any joint prediction, Bayes’ rule deﬁnes a conditional prediction for y2giveny1. The ﬁrst table indicates inevitable uncertainty that would not be resolved through training on additional data; conditioning on the ﬁrst label does not alter the prediction for the second. The second table indicates that additional training should resolve uncertainty; conditioned on the ﬁrst label, the prediction for the second label assigns all probability to the same outcome as the ﬁrst. Figure 1: Conventional neural nets generate marginal predictions, which do not distinguish genuine ambiguity from insuﬃciency of data. Joint predictions can make this distinction. ∗Contact iosband@deepmind.com Preprint. Under review.arXiv:2107.08924v8  [cs.LG]  17 May 2023Figure 1 presents the toy problem of predictions across two identical images as a simple illustration of these types of uncertainty. The observation that joint distributions express whether uncertainty is resolvable extends more generally to practical cases, where the inputs diﬀer, or where there are more than two simultaneous predictions (Osband et al., 2022a). Bayesian neural networks (BNNs) oﬀer a statisticallyprincipled way to make eﬀective joint predictions, by maintaining an approximate posterior over the weights of a base neural network. Assymptotically these can recover the exact posterior, but the computational costs are prohibitive for large models (Welling and Teh, 2011). Ensemblebased BNNs oﬀer a more practical approach by approximating the posterior distribution with an ensemble of statistically plausible networks that we call particles (Osband and Van Roy, 2015; Lakshmi narayanan et al., 2017). While the quality of joint predictions improves with more particles, practical implementations are often limited to ten or fewer due to computational constraints. In this paper, we introduce an approach that outperforms ensembles of hundreds of particles at a computational cost less than that of two particles . Our key innovation is the epinet: a network architecture that can be added to any conventional neural network to estimate uncertainty. Figure 2 oﬀers a preview of results presented in Section 6, where we compare these approaches on ImageNet. The quality of the ResNet’s marginal predictions – measured by classiﬁcation error or marginal logloss – does not change much if supplemented with an epinet. However the epinetenhanced ResNet dramatically improves the quality of jointpredictions, as measured by the joint logloss, outperforming the ensemble of 100 particles, with total parameters less than 2 particles. Prior work has shown the importance of jointpredictions in driving eﬀective decisions (Wen et al., 2022; Osband et al., 2022a). 3e7 1e8 3e8 1e9 3e90.210.220.23lossclassification error enn resnet ensemble epinet 3e7 1e8 3e8 1e9 3e90.800.850.90marginal logloss 3e7 1e8 3e8 1e9 3e9 model size (number of parameters)457joint logloss Figure 2: Quality of marginal and joint predictions across models on ImageNet (Section 6). The epinet does not ﬁt into the traditional framework of BNNs. In particular, it does not represent a distribution over base neural network parameters. To accommodate development of the epinet and other approaches that do not ﬁt the BNN framework, we introduce the concept of epistemic neural networks (ENNs). We establish that all BNNs are ENNs, but there are useful ENNs such as the epinet, that are not BNNs. 2 Related work "
356,Network-based Biased Tree Ensembles (NetBiTE) for Drug Sensitivity Prediction and Drug Sensitivity Biomarker Identification in Cancer.txt,"We present the Network-based Biased Tree Ensembles (NetBiTE) method for drug
sensitivity prediction and drug sensitivity biomarker identification in cancer
using a combination of prior knowledge and gene expression data. Our devised
method consists of a biased tree ensemble that is built according to a
probabilistic bias weight distribution. The bias weight distribution is
obtained from the assignment of high weights to the drug targets and
propagating the assigned weights over a protein-protein interaction network
such as STRING. The propagation of weights, defines neighborhoods of influence
around the drug targets and as such simulates the spread of perturbations
within the cell, following drug administration. Using a synthetic dataset, we
showcase how application of biased tree ensembles (BiTE) results in significant
accuracy gains at a much lower computational cost compared to the unbiased
random forests (RF) algorithm. We then apply NetBiTE to the Genomics of Drug
Sensitivity in Cancer (GDSC) dataset and demonstrate that NetBiTE outperforms
RF in predicting IC50 drug sensitivity, only for drugs that target membrane
receptor pathways (MRPs): RTK, EGFR and IGFR signaling pathways. We propose
based on the NetBiTE results, that for drugs that inhibit MRPs, the expression
of target genes prior to drug administration is a biomarker for IC50 drug
sensitivity following drug administration. We further verify and reinforce this
proposition through control studies on, PI3K/MTOR signaling pathway inhibitors,
a drug category that does not target MRPs, and through assignment of dummy
targets to MRP inhibiting drugs and investigating the variation in NetBiTE
accuracy.","There is strong evidence that the tumor’s genetic makeup can influence the outcome of anticancer drug treatments (1, 2), resulting in heterogeneity in patient clinical response to therapeutic drugs (3). This varied clinical response has led to the promise of personalized (or precision) medicine in cancer, where molecular biomarkers, e.g. gene expression, obtained from a patient’s tumor profiling may be used to design a personalized course of treatment. Targeted treatments have been shown to improve survival rates, for instance, in treating chronic myeloid leukemia (BCR–ABL) and malignant melanoma (BRAF) (4,5). Despite these success stories, variability in drug response still remains an open challenge and the link between genetic and epigenetic alterations and drug response is not appropriately characterized for a large number of cancer drugs (6,7). As large datasets emerge containing genetic profiles of tumors and their associated drug sensitivity, there is a need for computational methods that can effectively harness the available data and link genetic profiles with drug sensitivity through identification of important biomarkers (8–13). The Sanger Institute’s Genomics of Drug Sensitivity in Cancer (GDSC) database is a vast resource of over 200 cancer compounds screened with over a thousand genetically profiled pancancer cell lines (6). The dataset has been of particular interest for drug sensitivity prediction and biomarker identification efforts (3,8,14–18). These include a number of works employing quantitative, statistical and machine learning methods such as : Cell linesimilarity and drugsimilarity based models (19) multilevel mixed effect models using all drugcell line combinations (20), quantitative structureactivity relationship (QSAR) analysis using kernelized Bayesian matrix factorization (21), lasso and elastic net models for drug sensitivity prediction and target identification (8,22,23), as well as logic models for predictor identification (24).3In this work, we introduce a novel machine learning method that enables us to predict IC50 values and identify informative predictors for drug sensitivity using the GDSC dataset. Our approach is based on constructing a biased tree ensemble, where bias is elaborately designed to recapitulate the prior knowledge of drug targets and their highconfidence biomolecular interactions extracted from the STRING database of molecular interactions (25). Tree ensemble methods (26,27) such as the popular random forests (RF) (28) algorithm consist of an aggregation of decision trees and are suitable for dealing with highdimensionality (29) (i.e. small number of samples and large number of features) that is often encountered in biomolecular datasets. In addition, unlike regularized linear methods such as lasso and elastic net (30), regression trees can capture nonlinear relationships. Furthermore, tree ensemble methods are robust and have few tuning parameters (number of trees, mtry , and tree depth) and as such are easy to train. Due to these favorable attributes, tree ensembles, and in particular the random forests algorithm, have been used extensively for the analysis of biomolecular data (31–36). In this paper, we first introduce the Biased Tree Ensembles (BiTE) approach, where the classification and regression trees (CART) (37) are constructed according to prior knowledge. Unlike random tree ensembles (i.e., RF) in which all features have an equal probability of being selected as split variables in a tree (figure 1A), in BiTE, features that are more important or informative according to the available prior knowledge are given a higher probability (figure 1B). We demonstrate that BiTE is a more transparent and interpretable algorithm compared to RF, as it is immediately clear which set of features contributed the most to the model performance. For instance, if a set of features results in BiTE’s loss of accuracy, it can be deduced that the features were uninformative predictors; conversely, an improved accuracy can be attributed to the set of features towards which we biased the model. In this manner, BiTE may be used to examine the predictive power of various features in a transparent and controllable manner.4Building upon BiTE, we propose the Networkbased Biased Tree Ensembles (NetBiTE) algorithm, where two layers of prior knowledge – instead of one in BiTE – are fed into the model. First, drug target proteins are determined from drug databases and the literature and are assigned an initial bias weight. Second, the initial bias weights are propagated over STRING, a network of proteinproteininteraction (PPI) comprising the entire gene set. A number of networkbased methods have been previously put forward that take advantage of PPI networks in combination with biomolecular profiles of cells, in order to identify subnetworks that represent a pathway or a functional complex (38,39). Network propagation (or diffusion) over PPI networks has been previously used to identify pathways, subnetworks or associations that represent a disease, a tumor type or a patient (40–42). Network propagation in essence defines a “neighborhood of influence” surrounding an entity of interest, for instance a mutated gene (42). NetBiTE utilizes network diffusion over STRING PPI network in order to establish a neighborhood of influence surrounding the drug target proteins and construct tree ensembles that are biased towards this neighborhood. Even though several modified random forests or tree ensembles algorithms have been previously proposed (43–46), to the best of our knowledge, NetBiTE is the first algorithm in which multiple layers of prior knowledge are quantitatively and systematically combined and utilized in constructing biased tree ensembles. In the following sections, we demonstrate that BiTE and NetBiTE outperform RF in predicting IC50 drug sensitivity using both a synthetic dataset and the GDSC dataset. In addition, we showcase how NetBiTE in conjunction with the GDSC dataset and the STRING PPI network can identify important biomarkers for drug sensitivity. The organization of this paper is as follows. In section 2.1, we compare BiTE versus RF using a synthetic dataset. We showcase that BiTE can achieve a superior performance and stability at a significantly lower computational cost. In section 2.2, we apply NetBiTE to the GDSC data for a panel of 50 cancer drugs5and compare the predictive performance with that of RF. We demonstrate that NetBiTE achieves significant accuracy gains over RF for drugs than inhibit membrane receptor pathways (MRPs), suggesting that the expression of their reported target genes is an informative biomarker for drug sensitivity. We further investigate this hypothesis by studying all drugs within the GDSC database that target MRPs and by performing two control experiments. In section 3, we discuss the possible reasons behind our observations in the context of prior findings related to the role of oncogenes in cancer development as well as drug sensitivity and resistance. Figure 1. Working principles of biased tree ensembles (BiTE) compared against random forests (RF). A, B) diagrams describe the standard random forests (RF) algorithm and our devised biased tree ensembles (BiTE) approach. At each node of the tree, both algorithms draw a subset of features (di) from which they select a split feature through optimization of a loss function. In RF, the feature subset is selected at random while BiTE biases the selection towards more informative features according to prior knowledge. C, D) Tuning parameters for RF and BiTE algorithms. There are three tuning parameters for RF: the number of trees, ntree, the target partition size ( TPS) – the minimum number of samples in the leaf nodes of the tree, and the number of features to consider when looking for the best split, mtry . In BiTE, there is an additional tuning parameter, the bias weight distribution that controls the probability of each 6feature being included in the feature subset ( di) at each split of the tree. If weights are assigned according to informative prior knowledge, BiTE results in significant performance gains at a lower computational cost. 2. Methods and Materials "
54,Robust Training with Ensemble Consensus.txt,"Since deep neural networks are over-parameterized, they can memorize noisy
examples. We address such a memorization issue in the presence of label noise.
From the fact that deep neural networks cannot generalize to neighborhoods of
memorized features, we hypothesize that noisy examples do not consistently
incur small losses on the network under a certain perturbation. Based on this,
we propose a novel training method called Learning with Ensemble Consensus
(LEC) that prevents overfitting to noisy examples by removing them based on the
consensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC
outperforms the current state-of-the-art methods on noisy MNIST, CIFAR-10, and
CIFAR-100 in an efficient manner.","Deep neural networks (DNNs) have shown excellent performance (Krizhevsky et al., 2012; He et al., 2016) on visual recognition datasets (Deng et al., 2009). However, it is difﬁcult to obtain high quality labeled datasets in practice (Wang et al., 2018a). Even worse, DNNs might not learn patterns from the training data in the presence of noisy examples (Zhang et al., 2016). Therefore, there is an increasing demand for robust training methods. In general, DNNs optimized with SGD ﬁrst learn patterns relevant to clean examples under label noise (Arpit et al., 2017). Based on this, recent studies regard examples that incur small losses on the network that does not overﬁt noisy examples as clean (Han et al., 2018; Shen & Sanghavi, 2019). However, such smallloss examples could be noisy, especially under a high level of noise. Therefore, sampling trainable examples from a noisy dataset by relying on smallloss criteria might be impractical. To address this, we ﬁnd the method to identify noisy examples among smallloss ones based on well known observations: (i) noisy examples are learned via memorization rather than via pattern learning and (ii) under a certain perturbation, network predictions for memorized features easily ﬂuctuate, while those for generalized features do not. Based on these two observations, we hypothesize that out of smallloss examples, training losses of noisy examples would increase by injecting certain perturbation to network parameters, while those of clean examples would not. This suggests that examples that consistently incur small losses under multiple perturbations can be regarded as clean. This idea comes from an artifact of SGD optimization, thereby being applicable to any architecture optimized with SGD. In this work, we introduce a method to perturb parameters to distinguish noisy examples from small loss examples. We then propose a method to robustly train neural networks under label noise, which is termed learning with ensemble consensus (LEC). In LEC, the network is initially trained on the entire training set for a while and then trained on the intersection of smallloss examples of the ensemble of perturbed networks. We present three LECs with different perturbations and evaluate their effectiveness on three benchmark datasets with random label noise (Goldberger & BenReuven, 2016; Ma et al., 2018), openset noise (Wang et al., 2018b), and semantic noise. Our proposed LEC outperforms existing robust training methods by efﬁciently removing noisy examples from training batches. 1arXiv:1910.09792v3  [cs.LG]  11 Nov 2020Published as a conference paper at ICLR 2020 2 R ELATED WORK "
313,Deep Self-Paced Learning for Person Re-Identification.txt,"Person re-identification (Re-ID) usually suffers from noisy samples with
background clutter and mutual occlusion, which makes it extremely difficult to
distinguish different individuals across the disjoint camera views. In this
paper, we propose a novel deep self-paced learning (DSPL) algorithm to
alleviate this problem, in which we apply a self-paced constraint and symmetric
regularization to help the relative distance metric training the deep neural
network, so as to learn the stable and discriminative features for person
Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive
the adaptive weights to samples based on both the training loss and model age.
As a result, the high-confidence fidelity samples will be emphasized and the
low-confidence noisy samples will be suppressed at early stage of the whole
training process. Such a learning regime is naturally implemented under a
self-paced learning (SPL) framework, in which samples weights are adaptively
updated based on both model age and sample loss using an alternative
optimization method. Secondly, we introduce a symmetric regularizer term to
revise the asymmetric gradient back-propagation derived by the relative
distance metric, so as to simultaneously minimize the intra-class distance and
maximize the inter-class distance in each triplet unit. Finally, we build a
part-based deep neural network, in which the features of different body parts
are first discriminately learned in the lower convolutional layers and then
fused in the higher fully connected layers. Experiments on several benchmark
datasets have demonstrated the superior performance of our method as compared
with the state-of-the-art approaches.","Person reidentiﬁcation (ReID) has become an active re search topic in the ﬁeld of computer vision, because of its wide application in the video surveillance community. Given one single shot or multiple shots of a target, person ReID concerns the problem of matching the same person among a set of gallery candidates captured from the disjoint camera views [1–4]. It is a very challenging task due to noisy samples with mutual oc clusion and background clutter that makes the large appearance variations across different camera views [5, 6]. Therefore, the Corresponding author: Tel.: +8602983395146; Fax: +8602983395175; Email address: sanpingzhou@stu.xjtu.edu.cn (Sanping Zhou)key to improve the identiﬁcation performance is to learn the stable and discriminative features for representation. The fundamental person ReID problem is to compare an im age of each interested target seen in a probe camera view to a large number of candidates captured from a gallery camera view which has no overlap with the probe one [7]. If a true match to the probe exists in the gallery, it should have a higher similarity score as compared with the incorrect matches. Pre vious efforts for solving this problem primarily focus on the following two aspects: 1) developing robust feature descriptors to handle the variations in person’s appearance, and 2) design ing discriminative distance metrics to measure the similarity of person’s images. For the ﬁrst category, different cues are Preprint submitted to Journal of Pattern Recognition October 17, 2017arXiv:1710.05711v1  [cs.CV]  7 Oct 2017employed for the stable and discriminative features. Represen tative descriptors include the Local Binary Pattern (LBP) [8], Ensemble of Local Feature (ELF) [9] and Local Maximal Oc currence (LOMO) [10]. For the second category, labeled im ages are used to train a distance metric, in which the intraclass distance is minimized while the interclass distance is maxi mized. Typical metric learning methods include the Locally Adaptive Decision Function (LADF) [11], Large Margin Near est Neighbor (LMNN) [12] and Information Theoretic Metric Learning (ITML) [13]. Since both line of works regard the feature extraction and metric learning processes as two disjoint steps, their performances are limited. In the past two years, the deep convolutional neural network (CNN) based methods [14–19] have been proposed to combine the feature extraction and metric learning into an endtoend learning framework, in which a neural network is built to ex tract the stable and discriminative features under the supervi sion of a suitable distance metric. Beneﬁt from the powerful representation capability of the deep CNN, this line of meth ods have achieved promising results on the benchmark datasets for person ReID. The relative distance metric [20] has been widely used as loss function in the deep learning based methods for visual recognition. Compared with the wellknown softmax loss [21], it is a better choice for the zeroshot recognition prob lem, because of the training set doesn’t have the same identity with the testing set. The relative distance metric aims to maxi mize the relative distance between the positive pair and negative pair in each triplet unit, which can generate a large number of triplet inputs even using a small number of training samples. Therefore, it is very suitable choice for the person ReID prob lem which not only is a zeroshot problem but also can only provide the smallscale dataset for training. To further improve the identiﬁcation performance, our ob servation shows that the following three issues should also be addressed in the learning process. Firstly, the order and weight of training samples should be considered, as shown in Fig. 1, otherwise it might be easy to cause the unstable learning due to the noisy samples or outliers with mutual occlusion and back 1 2 3 4 5 6Similarity 1 2 3 4 5 6 1 2 3 4 5 6Training the deep model in SPL Manner Weight = 0.32Weight = 0.29 Weight = 0.96Weight = 0.84 Weight = 0.84 Weight = 0.09Weight = 0.98Weight = 0.87Weight = 0.26 Weight = 0.19Weight = 0.02 Weight = 0.86Anchor AnchorAnchor     Candidates in GalleryFigure 1: Illustration of our SPL motivations in dealing with the noisy training samples or outliers. The left column shows some typical positive candidates to two anchor images, in which the similarity scores of these positive candidates to the anchor vary from large to small with the incensement of indexes. The right column shows the SPL training strategy , in which the derived weighting scheme will adaptively update the sample weights according to the training loss and model age. Therefore, highconﬁdence ﬁdelity samples will be emphasized and the the lowconﬁdence noisy samples will be suppressed at early stage of the whole learning process. ground clutter. Secondly, it is unsuitable to directly apply the distance metric to supervise the training process of deep CNN without any regularization to the gradient backpropagation. Because most of the deep learning tools, such as Caffe [22] and Tensorﬂow [23], take the gradient backpropagation algorithm to optimize the deep parameters. Thirdly, the neural network should be relatively small and include the part processing mod ule, due to the person ReID is a ﬁnegrained problem and the dataset for person ReID is usually in small size. As a conse quence, it is very urgent to study the three aspects of problems in the training process. In this paper, we propose a novel deep selfpaced learning (DSPL) algorithm to adaptively update the weights to samples and regularize the gradient backpropagation of relative dis tance metric [20] in the learning process, so as to further im prove the identiﬁcation performance of deep neural network for person ReID. In order to extract the stable and discrimi native features, we ﬁrstly build a partbased deep neural net work, in which the features of different body parts are discrim inately learned in the lower convolutional layers and then fused in the higher fully connected layers. Then, we introduce the 2selfpaced learning (SPL) theory [24] into the training frame work, in which samples can be ranked in a selfpaced manner by applying a novel soft polynomial regularizer term to adap tively update the weights according to both the model age and sample loss in each iteration. Specially, the highconﬁdence ﬁ delity samples will be emphasized and the the lowconﬁdence noisy samples will be suppressed at early stage of the whole learning process. Therefore, the neural network can be trained in a stable process by gradually involving the faithful samples from easy to hard. In addition, a symmetric regularizer term is introduced to overcome the drawback of relative distance met ric in gradient backpropagation. As a result, the intraclass dis tance is minimized and the interclass distance is maximized by regularizing the asymmetric gradient backpropagation in each triplet unit. Extensive experimental results on several bench mark datasets have shown that our method performs much bet ter than the stateoftheart approaches. In summary, the main contributions of this paper can be high lighted as follows: We propose a novel DSPL algorithm to supervise the learning of deep neural network, in which a soft polyno mial regularizer term is proposed to gradually involve the faithful samples into training process in a selfpaced man ner. We optimize the gradient backpropagation of relative dis tance metric by introducing a symmetric regularizer term, which can convert the backpropagation from the asym metric mode to a symmetric one. We build an effective partbased deep neural network, in which features of different body parts are ﬁrst discrimi nately learned in the lower convolutional layers and then fused in the higher fully connected layers. The rest of our paper is organized as follows: Section 2 re views some of the related works. In Section 3, we describe the proposed method, including the DSPL algorithm and deep neu ral network. The experimental results and corresponding anal ysis are presented in Section 4. Conclusion comes in Section 5.2. Related work "
575,Enhancing Privacy against Inversion Attacks in Federated Learning by using Mixing Gradients Strategies.txt,"Federated learning reduces the risk of information leakage, but remains
vulnerable to attacks. We investigate how several neural network design
decisions can defend against gradients inversion attacks. We show that
overlapping gradients provides numerical resistance to gradient inversion on
the highly vulnerable dense layer. Specifically, we propose to leverage
batching to maximise mixing of gradients by choosing an appropriate loss
function and drawing identical labels. We show that otherwise it is possible to
directly recover all vectors in a mini-batch without any numerical optimisation
due to the de-mixing nature of the cross entropy loss. To accurately assess
data recovery, we introduce an absolute variation distance (AVD) metric for
information leakage in images, derived from total variation. In contrast to
standard metrics, e.g. Mean Squared Error or Structural Similarity Index, AVD
offers a continuous metric for extracting information in noisy images. Finally,
our empirical results on information recovery from various inversion attacks
and training performance supports our defense strategies. These strategies are
also shown to be useful for deep convolutional neural networks such as LeNET
for image recognition. We hope that this study will help guide the development
of further strategies that achieve a trustful federation policy.","Federated learning (FL) enables distributed client nodes to contribute to the training of a centralised global model without exposing their private data (McMahan et al., 2017; Kairouz et al., 2021; Yang et al., 2019). The promises of federated learning are signiﬁcant and have wide applicabil ity in industry. For example through federated learning it is possible for hospitals to collaborate on training a centralised model around the globe, without sharing or moving the ac tual private patient information across institutions (Rieke et al., 2020). As it potentially protects sensitive data, it can better align with data protection regulations such as GDPR (Commission, 2018). For example, FL has been already applied to prediction of treatment side effects in medicine (Jochems et al., 2016) or learning a predictive keyboard for smartphones (Bonawitz et al., 2019; Kone ˇcn´y et al., 2016). The reduction of data movement is an addi tional important advantage, as it is costly and time consum ing for large industrial applications. Given the potential impact of FL, many authors have since examined the se curity and privacy of FL (Zhao et al., 2020; Geiping et al., 2020; Yin et al., 2021; Zhu et al., 2019; Huang et al., 2020; Phong et al., 2018; Carlini et al., 2020; Shokri et al., 2017; Melis et al., 2019). A standard FL conﬁguration is typically achieved with a central aggregator node which exchanges gradients for centralised aggregation. At each training step (t), a client node receives neural network model weights,F(Wt), from the aggregator server and calculates loss ( l) with a local data xt;ytfor a minibatch, B, which generates gradients with respect to the model weights: Wt="
386,Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification.txt,"Hierarchical multi-granularity classification (HMC) assigns hierarchical
multi-granularity labels to each object and focuses on encoding the label
hierarchy, e.g., [""Albatross"", ""Laysan Albatross""] from coarse-to-fine levels.
However, the definition of what is fine-grained is subjective, and the image
quality may affect the identification. Thus, samples could be observed at any
level of the hierarchy, e.g., [""Albatross""] or [""Albatross"", ""Laysan
Albatross""], and examples discerned at coarse categories are often neglected in
the conventional setting of HMC. In this paper, we study the HMC problem in
which objects are labeled at any level of the hierarchy. The essential designs
of the proposed method are derived from two motivations: (1) learning with
objects labeled at various levels should transfer hierarchical knowledge
between levels; (2) lower-level classes should inherit attributes related to
upper-level superclasses. The proposed combinatorial loss maximizes the
marginal probability of the observed ground truth label by aggregating
information from related labels defined in the tree hierarchy. If the observed
label is at the leaf level, the combinatorial loss further imposes the
multi-class cross-entropy loss to increase the weight of fine-grained
classification loss. Considering the hierarchical feature interaction, we
propose a hierarchical residual network (HRN), in which granularity-specific
features from parent levels acting as residual connections are added to
features of children levels. Experiments on three commonly used datasets
demonstrate the effectiveness of our approach compared to the state-of-the-art
HMC approaches and fine-grained visual classification (FGVC) methods exploiting
the label hierarchy.","Traditional singlegranularity classiﬁcation usually as signs a single label to a given object from a set of mu *Corresponding author (a) Differences in domain knowledge and interference from the image occlusion. (b) Large variations of image resolutions. Figure 1. Different objects can be discerned at various levels in the label hierarchy due to differences in domain knowledge or image quality such as occlusion or resolution. tually exclusive class labels. For instance, FGVC aims at distinguishing objects from different subordinatelevel cat egories within a given object category, e.g., subcategories of birds [32], cars [16], aircraft [20]. However, the deﬁnition of what is ﬁnegrained is subjective, and the image quality may affect the identiﬁcation, as illustrated in Fig. 1. A bird can be discerned as Albatross or Laysan Albatross due to differences in domain knowledge. Moreover, a bird expert recognizes a bird as Albatross rather than Blackfooted Al batross because of the occlusion of key parts. Airborne or satellite image resolutions often have large variations, caus ing objects to be recognized at different levels. These chal lenges increase the difﬁculty of constructing a dataset for singlegranularity classiﬁcation, while images annotated as coarse categories are also overlooked. Compared to singlegranularity classiﬁcation, a more preferable solution is to employ hierarchical multi granularity labels to describe an object, which provides more ﬂexible options for annotators with different knowl edge backgrounds [4]. HMC [15] aims to exploit hierarchi cal multigranularity labels and embeds the label hierarchy in loss function or network architecture. Whereas conven 1arXiv:2201.03194v2  [cs.CV]  11 Jan 2022tional HMC usually evaluates each sample with complete hierarchical labels from the coarsest to the ﬁnest granular ity. A more robust HMC model should effectively utilize examples observed at various levels in the hierarchy, e.g., making use of bird images annotated as [“Albatross”] and [“Albatross”, “Laysan Albatross”]. In this paper, we study the HMC problem in which sam ples are labeled at any level of the hierarchy. We factor ize this problem into two aspects: (1) how to effectively use instances labeled at different levels; (2) how to perform hierarchical feature interaction in the network architecture. For the ﬁrst problem, we adopt a tree hierarchy that deﬁnes two kinds of semantic relationships between labels: parent child correlations between levels and mutual exclusion at the same level. Inspired by the work of [7], if an instance is discerned at a label in the hierarchy, we maximize its marginal probability in the probability space constrained by the tree hierarchy. Such marginalization enjoys two bene ﬁts: learning with the coarselevel label could impact de cisions of ﬁnegrained subclasses while learning with the ﬁnelevel label aids the prediction of coarsegrained super classes. Moreover, if the ground truth label is observed at the leaf level, we further impose the multiclass cross entropy loss to enhance the discriminative power among ﬁnegrained categories. Another critical issue is to design appropriate hierarchi cal feature interaction that reﬂects the label hierarchy. A distinct characteristic of hierarchical categories is that from coarsetoﬁne levels, ﬁnelevel classes not only have unique attributes but also inherit attributes related to coarselevel superclasses. Based on this property, we propose a hierar chical residual network (HRN) illustrated in Fig. 2. We ﬁrst set up granularityspeciﬁc layers to disentangle hierarchical features from the trunk network. Then, these hierarchical features interact via residual connections [11–14,19,30,34], i.e., features from parent levels acting as skip connections are added to features of children levels. Experiments on three commonly used FGVC datasets demonstrate the ef fectiveness of our approach compared to the stateoftheart HMC approaches and FGVC methods exploiting hierarchi cal knowledge under two evaluation metrics [31]. 2. Related Work "
271,Recurrent Neural Networks for Person Re-identification Revisited.txt,"The task of person re-identification has recently received rising attention
due to the high performance achieved by new methods based on deep learning. In
particular, in the context of video-based re-identification, many
state-of-the-art works have explored the use of Recurrent Neural Networks
(RNNs) to process input sequences. In this work, we revisit this tool by
deriving an approximation which reveals the small effect of recurrent
connections, leading to a much simpler feed-forward architecture. Using the
same parameters as the recurrent version, our proposed feed-forward
architecture obtains very similar accuracy. More importantly, our model can be
combined with a new training process to significantly improve re-identification
performance. Our experiments demonstrate that the proposed models converge
substantially faster than recurrent ones, with accuracy improvements by up to
5% on two datasets. The performance achieved is better or on par with other
RNN-based person re-identification techniques.","Person reidentification consists of associating different tracks of a person as they are captured across a scene by different cameras. There are many applications for this task. The most obvious one is videosurveillance. It is common in public spaces to deploy net works of cameras with nonoverlapping field of views that capture different areas. These networks produce large amount of data and it can be very timeconsuming to manually analyze the video feeds to keep track of the actions of a single person as they move across the various fields of view. Person reidentification allows this task to be automated and makes it scalable to keep track of the trajectories of a high number of different identities. Solving this problem can also be critical for home automation, where it is important to keep track of the location of a user as they move across the different rooms, for singlecamera person tracking in order to recover from occlusions, or for crowd dynamics understanding, among other tasks. The challenges inherent to this task are the variations in background, body pose, illumination and viewpoint. It is important to represent a person using a descriptor that is as robust as possible to these variations, while still being discriminative enough to be characteristic of a single person’s identity. A subclass of this problem is videobased reidentification, where the goal is to match a video of a person against a gallery of videos captured by different cameras, by opposition to imagebased (or singleshot) reidentification, where only a single view of a person is provided.Person reidentification has recently received rising attention due to the much improved performance achieved by methods based on deep learning. For videobased reidentification, it has been shown that representing videos by aggregating visual informa tion across the temporal dimension was particularly effective. Re current Neural Networks (RNNs) have shown promising results for performing this aggregation in multiple independent works [3,20,27,29,30,32,38]. In this paper, we analyze one type of archi tecture that uses RNNs for video representation. The contributions of this work are the following. We show that the recurrent network architecture can be replaced with a simpler nonrecurrent architec ture, without sacrificing the performance. Not only does this lower the complexity of the forward pass through the network, making the feature extraction easier to parallelize, but we also show that this model can be trained with an improved process that boosts the final performance while converging substantially faster. Finally, we obtain results that are on par or better than other published work based on RNN, but with a much simpler technique. 2 RELATED WORK "
22,Deep Attributes Driven Multi-Camera Person Re-identification.txt,"The visual appearance of a person is easily affected by many factors like
pose variations, viewpoint changes and camera parameter differences. This makes
person Re-Identification (ReID) among multiple cameras a very challenging task.
This work is motivated to learn mid-level human attributes which are robust to
such visual appearance variations. And we propose a semi-supervised attribute
learning framework which progressively boosts the accuracy of attributes only
using a limited number of labeled data. Specifically, this framework involves a
three-stage training. A deep Convolutional Neural Network (dCNN) is first
trained on an independent dataset labeled with attributes. Then it is
fine-tuned on another dataset only labeled with person IDs using our defined
triplet loss. Finally, the updated dCNN predicts attribute labels for the
target dataset, which is combined with the independent dataset for the final
round of fine-tuning. The predicted attributes, namely \emph{deep attributes}
exhibit superior generalization ability across different datasets. By directly
using the deep attributes with simple Cosine distance, we have obtained
surprisingly good accuracy on four person ReID datasets. Experiments also show
that a simple metric learning modular further boosts our method, making it
significantly outperform many recent works.","Person ReIdentication (ReID) targets to identify the same person from dif ferent cameras, datasets, or time stamps. As illustrated in Fig. 1, factors like viewpoint variations, illumination conditions, camera parameter dierences, as well as body pose changes make person ReID a very challenging task. Due to its important applications in public security, e.g., cross camera pedestrian searching, tracking, and event detection, person ReID has attracted lots of attention from both the academic and industrial communities. Currently, research on this topic mainly focus on two aspects: a) extracting and coding local invariant features to arXiv:1605.03259v2  [cs.CV]  9 Aug 20162 authors running upperBodyLongSleeve upperBodyBlacklowerBodyJeanspersonalFemalehairBlackpersonalFemale upperBodyRedfootwearBlacklowerBodyBlackupperBodyOtherhairShort upperBodyWhitelowerBodyGreypersonalMalelowerBodyTrousers (a) (b) (c) Fig. 1. Example images of the same person taken by two cameras from three datasets: (a)VIPeR [26], (b) PRID [27], and (c) GRID [28]. This gure also shows ve of our predicted attributes shared by these two images. represent the visual appearance of a person [1,2,3,4,5,6,7] and b) learning a dis criminative distance metric hence the distance of features from the same person can be smaller [8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]. Although signicant progress has been made from previous studies, person ReID methods are still not mature enough for real applications. Local features mostly describe the lowlevel visual appearance, hence are not robust to variances of viewpoints, body poses, etc. On the other side, distance metric learning suers from the poor generalization ability and the quadratic computational complexity, e.g., dierent datasets present dierent visual characteristics corresponding to dierent metrics. Compared with lowlevel visual feature, human attributes like long hair, blue shirt, etc., represent midlevel semantics of a person. As illustrated in Fig. 1, attributes are more consistent for the same person and are more robust to the above mentioned variances. Some recent works hence have started to use attributes for person ReID [29,30,31,32,33,34]. Because human attributes are expensive for manual annotation, it is dicult to acquire enough training data for a large set of attributes. This limits the performance of current attribute features. Consequently, lowlevel visual features still play a key role and attributes are mostly used as auxiliary features [31,32,33,34]. Recently, deep learning has exhibited promising performance and general ization ability in various visual tasks. For example in [35], an eightlayer deep Convolutional Neural Network (dCNN) is trained with largescale images for visual classication. The modied versions of this network also perform impres sively in object detection [36] and segmentation [37]. Motivated by the issues of low level visual features and the success of dCNN, our work targets to learn a dCNN to detect a large set of human attributes discriminative enough for person ReID. Due to the diversity and complexity of human attributes, it is a laborious task to manually label enough of attributes for dCNN training. The key issuestitle running 3 … … … Attributes Triplet  Loss …… …dCNN dCNN Sigmoid Cross Entropy  LossFc7:  4096 nodes Fc8:  Knodes Fc7:  4096 nodesFc8: K nodes … …Anchor Positive Negative … 111… 101… 110… 001… 101… 011…Stage 1: Fullysupervised dCNN training Independent dataset  with attribute labels Dataset with person ID labels Predicted attributesStage 2: Fine tuning using attributes triplet loss Stage 3:Final fine tuning on the combined dataset … … … ……dCNN Sigmoid Cross Entropy  LossFc7:  4096 nodes Fc8:  Knodes … … …101… 110… 110… 001… 101… 101…Independent dataset Dataset with refined attributesAnchor Positive Negative Fig. 2. Illustration of Semisupervised Deep Attribute Learning (SSDAL). are hence how to train this dCNN from a partiallylabeled dataset and ensure its discriminative power and generalization ability in the person ReID tasks. To address these issues, we propose a Semisupervised Deep Attribute Learn ing (SSDAL) algorithm. As illustrated in Fig. 2, this algorithm involves three stages. The rst stage uses an independent dataset with attribute labels to per form fullysupervised dCNN training. The resulting dCNN produces initial at tribute labels for the target dataset. To improve the discriminative power of these attributes for ReID task, we start the second stage of training, i.e., ne tuning the network using the person ID labels and our dened attributes triplet loss. The training data for netuning can be easily collected because the person ID labels are readily accessible in many person tracking datasets. The attributes triplet loss updates the network to enforce that the same person has more similar attributes and vice versa. This netuned dCNN hence predicts initial attribute labels for target datasets. Finally in the third stage, the initially labeled target dataset plus the original independent dataset are combined for the nal stage of netuning. The attributes predicted by the nal dCNN model are named as deep attributes . In this manner, the dCNN is rstly trained with the indepen dent dataset, then is rened to acquire more discriminative power for person ReID task. Because this procedure involves one dataset with attribute labels and another without attribute labels, we call it a semisupervised learning. To validate the performance of deep attributes, we test them on four popular person ReID datasets without combining with the local visual features. The4 authors running experimental results show that deep attributes perform impressively, e.g., they signicantly outperform many recent works combining both attributes and local features [31,32,33,34]. Note that, predicting and matching deep attributes make person ReID system signicantly faster, because it no longer needs to extract and code local features, compute distance metric, and fuse with other features. Our contributions can be summarized as follows: 1) we propose a three stage semisupervised deep attribute learning algorithm, which makes learning a large set of human attributes from a limited number of labeled attribute data possible, 2) deep attributes achieve promising performance and generalization ability on four person ReID datasets, and 3) deep attributes release the previous dependencies on local features, thus make the person ReID system more robust and ecient. To the best of our knowledge, this is an original work predicting human attributes using dCNN for person ReID tasks. The promising results of this work guarantees further investigation in this direction. 2 Related Work "
470,Triplet-based Deep Similarity Learning for Person Re-Identification.txt,"In recent years, person re-identification (re-id) catches great attention in
both computer vision community and industry. In this paper, we propose a new
framework for person re-identification with a triplet-based deep similarity
learning using convolutional neural networks (CNNs). The network is trained
with triplet input: two of them have the same class labels and the other one is
different. It aims to learn the deep feature representation, with which the
distance within the same class is decreased, while the distance between the
different classes is increased as much as possible. Moreover, we trained the
model jointly on six different datasets, which differs from common practice -
one model is just trained on one dataset and tested also on the same one.
However, the enormous number of possible triplet data among the large number of
training samples makes the training impossible. To address this challenge, a
double-sampling scheme is proposed to generate triplets of images as effective
as possible. The proposed framework is evaluated on several benchmark datasets.
The experimental results show that, our method is effective for the task of
person re-identification and it is comparable or even outperforms the
state-of-the-art methods.","Recently, person reidentiﬁcation (reid) catches great at tention in both computer vision community and industry be cause of its potential practical applications, such as surveil lance security [31], person tracking in crosscamera scenes, and retrieval of lost children. The goal is to ﬁnd a query per son among a gallery of people images [34, 11]. Inﬂuenced by illumination condition, widely varying person poses, res olution, partial occlusion, etc., reid is still an open chal lenging and popular task. Since the milestone work [18], deep learning has great achievements in computer vision for different tasks,such as object recognition [26, 12], semantic segmentation [23, 3], artist style transform [10, 16], and the reid task [30, 32, 35,29, 33]. However, it is well known that, a largescale dataset (e.g. ImageNet [18], which have 1.2 million images with 1000 categories) is the prerequisite for sufﬁciently training a deep learning model [20]. It often lacks of such largescale dataset in many speciﬁc areas. But many smaller datasets are published by different research groups. Jointly training a deep learning model with all these small datasets is worth trying to alleviate the grate demand of dataset. Furthermore, a dataset which is collected by a research group doesn’t vary too much because of limited condition of collecting scene and custom of the collector. For example, the CUHK01 [21] (as shown in Fig. 1(a)) and CUHK03 [22] datasets are cap tured in a university, where most of the collected person samples are students. The iLID [36] (Fig. 1(d)) dataset is captured in an airport and the many person are taking lug gages. PRID [14] (Fig. 1(c)) is taken from street views, where crosswalks are the main actors. The image reso lution in VIPeR [13] changes violently with varying cam era views. Combing these data together make the training dataset discrepant a lot, and then the model is trained to learn more general and robust feature representation. Typical person reid framework contains two major com ponents: a feature extractor to describe each sample of the dataset and a metric to measure the distance between the the query image and the gallery images. Many existing works research these two components separately and most of them pay more attention on the ﬁrst one [6, 9]. After extracting the features, a standard distance measure such as L2 dis tance [8], Bhattacharyya distance (Bhat) [25], etc. is ap plied to calculate their similarity. Our framework is mainly inspired by [22, 19], which learn features and distance met ric jointly by designing a reasonable loss function. Further more, different from usual way, more than one image can be feed into the system to learn discriminative feature rep resentations simultaneously. In this paper, we propose a triplet CNN framework to learn the deep similarity representation, with which the dis tance within the same person identity is decreased and be tween different persons is increased. Six datasets are com bined together to make the training data vary widely, and 1 arXiv:1802.03254v1  [cs.CV]  9 Feb 2018(a) CUHK1  (b) 3DPES (c) PRID  (d) iLIDS Figure 1: Examples of multiple person reidentiﬁcation datasets. Each dataset has its certain trait. The green bounding box indicates the query person image and the image in a red box is the corresponding matched person in the gallery. then the framework is able to learn more general and ro bust representation for the task of person reid. While the framework is feed 3 images: two of them have the same per son identity and the 3rd is a different person, there is huge number of such possible combinations in the combined dataset, which makes the training impossible. To address this problem, we introduce a double sampling scheme for training the framework efﬁciently. An overview of the pro posed framework is depicted in Fig. 2. The main technical contributions of this paper are threefold: First, a double sampling method is proposed to address the challenge of numerous possible combinations of triplet input for training the proposed deep convolution network without loss of gen erality. Second, a triplet ranking loss function for making the distance within the same person samples smaller while the distance between the samples of different persons larger. Third, the model is jointly trained on six different datasets. 2. Related Work "
363,Prototypical Classifier for Robust Class-Imbalanced Learning.txt,"Deep neural networks have been shown to be very powerful methods for many
supervised learning tasks. However, they can also easily overfit to training
set biases, i.e., label noise and class imbalance. While both learning with
noisy labels and class-imbalanced learning have received tremendous attention,
existing works mainly focus on one of these two training set biases. To fill
the gap, we propose \textit{Prototypical Classifier}, which does not require
fitting additional parameters given the embedding network. Unlike conventional
classifiers that are biased towards head classes, Prototypical Classifier
produces balanced and comparable predictions for all classes even though the
training set is class-imbalanced. By leveraging this appealing property, we can
easily detect noisy labels by thresholding the confidence scores predicted by
Prototypical Classifier, where the threshold is dynamically adjusted through
the iteration. A sample reweghting strategy is then applied to mitigate the
influence of noisy labels. We test our method on CIFAR-10-LT, CIFAR-100-LT and
Webvision datasets, observing that Prototypical Classifier obtains substaintial
improvements compared with state of the arts.","Deep neural networks (DNNs) have been widely used for machine learning applications. Despite of their success, it has been shown that the training of DNNs requires largescale labeled and unbiased data. However, in many realworld applications, training set biases are prevalent [21, 27, 28, 9], which typically have two types: i) classimbalanced data distribution; and ii) noisy labels. For example, in autonomous driving, the vast majority of the training data is composed of standard vehicles but models also need to recognize rarely seen classes such as emergency vehicles or animals with very high accuracy. This will sometime lead to biased training models that do not perform well in practice. Moreover, largescale highquality data annotations are expensive and timeconsuming to obtain. Although coarse labels are cheap and of high availability, the presence of noise will hurt the model performance. Therefore, it is desirable to develop machine learning algorithms that can accommodate not only classimbalanced training set, but also the presence of label noise. Both learning with noisy labels and classimbalanced learning (a.k.a. longtailed learning) have been studied for many years. When dealing with label noise, the most popular approach is sample selection where correctlylabeled examples are identiﬁed by capturing the training dynamics of DNNs [ 11,29]. When dealing with class imbalance, many existing works propose to reweight examples or design unbiased loss functions by taking into account the class distribution of training set [26, 3, 8]. However, most existing methods focus on only one of these two training set biases. In this paper, we address both training set biases simultaneously. As shown in Figure 1a, it is known that the classiﬁer directly learned on classimbalanced data is biased towards head classes [ 8,32] which results in poor generalization on tail classes. Moreover, using sample loss/conﬁdence produced by biased classiﬁers fails to detect label noise,arXiv:2110.11553v1  [cs.CV]  22 Oct 2021Prototypical Classiﬁer for Robust ClassImbalanced Learning A P REPRINT 𝑥𝑥1 (a) Normal Classiﬁer 𝑐𝑐2𝑐𝑐1 𝑐𝑐3𝑥𝑥2 (b) Prototypical 1NN 𝑐𝑐2𝑐𝑐1 𝑐𝑐3𝑥𝑥2𝑥𝑥1 (c) Prototypical Classiﬁer Figure 1: Illustration of normal classiﬁer and Prototypical Classiﬁer. because both clean and noisy samples of tail classes have large loss and low conﬁdence. To solve this problem, we propose to use Prototypical Classiﬁer which is demonstrated to produce balanced predictions even through the training set is classimbalanced. Our basic idea is that there exists an embedding in which examples cluster around a single prototype representation for each class. In order to do this, we learn a nonlinear mapping of the input into an embedding space using a neural network and take a class’s prototype to be the normalized mean vector of examples in the embedding space. Classiﬁcation is then performed for an embedded test example by simply ﬁnding the nearest class prototype. Notably, Prototypical Classiﬁer does not need additional learnable parameters given embedding of examples. Unfortunately, it is easy to observe that simply using prototypes for classiﬁcation may lead to many wrong predictions for samples of head classes as shown in Figure 1b. The reason is that the representations are supposed to be modiﬁed when the classiﬁcation boundaries of tail classes expand. We therefore train the neural networks to pull together embedding of examples and the prototype of their class, while pushing apart examples from prototypes of other classes. By doing this, it can avoid many misclassiﬁcations for samples of head classes, as shown in Figure 1c. Subsequently, we ﬁnd that the conﬁdence scores produced by Prototypical Classiﬁer is balanced and comparable across classes. By leveraging this property, we can simply detect noisy labels via thresholding where the threshold is dynamically adjusted, followed by a sample reweighting strategy. In summary, our key contributions of this work are: • We propose to learn from training set with mixed biases, which is practical but has been understudied; •Our approach, Prototype Classiﬁer, is simple yet powerful. It produces more balanced predictions over all classes than normal classiﬁers even when the training set is classimbalanced. This property further beneﬁts the detection of label noise. •On both simulated datasets and a realworld dataset Webvision with label noise, Prototype Classiﬁer achieves substaintial performance improvement. 2 Related Work "
471,Person re-identification across different datasets with multi-task learning.txt,"This paper presents an approach to tackle the re-identification problem. This
is a challenging problem due to the large variation of pose, illumination or
camera view. More and more datasets are available to train machine learning
models for person re-identification. These datasets vary in conditions: cameras
numbers, camera positions, location, season, in size, i.e. number of images,
number of different identities. Finally in labeling: there are datasets
annotated with attributes while others are not. To deal with this variety of
datasets we present in this paper an approach to take information from
different datasets to build a system which performs well on all of them. Our
model is based on a Convolutional Neural Network (CNN) and trained using
multitask learning. Several losses are used to extract the different
information available in the different datasets. Our main task is learned with
a classification loss. To reduce the intra-class variation we experiment with
the center loss. Our paper ends with a performance evaluation in which we
discuss the influence of the different losses on the global re-identification
performance. We show that with our method, we are able to build a system that
performs well on different datasets and simultaneously extracts attributes. We
also show that our system outperforms recent re-identification works on two
datasets.","In many domains, such as surveillance or digital signage, being able to auto matically recognize a person across diﬀerent, nonoverlapping cam eras, without the help of a human operator is very valuable. This task is known as pe rson reidentiﬁcation and can be extremely challenging since great variat ions can oc cur between the diﬀerent cameras. Figure 1 shows two images, tak en from two diﬀerent cameras from three academic datasets: VIPeR [2], CUHK0 1 [3] and CUHK03 [4]. Variation can be large between two pictures belonging to t he same dataset such as body pose, luminosity, view angle or background. In many works,person reidentiﬁcation is based on a similarity score between a pairofimages.If the twoimagesrepresentthe same person,the similarityscore2 Matthieu Ospici, Antoine Cecchi (a) VIPeR  (b) CUHK01  (c) CUHK03 Fig.1: Three reidentiﬁcation datasets used in our work. is high. Two aspects are usually studied. The ﬁrst one consists in ext racting robust invariant features to represent the appearance of a per son [5,6,7]. The second is metric learning [8,9]: it consists in learning the best possible m etric to discriminate between positive and negative samples. Recently, convolutional neural networks demonstrated very hig h eﬃciency in several computer vision problems such as image segmentation [10] or object recognition [11]. Many research projects have proved that deep n eural networks are also extremely eﬃcient for reidentiﬁcation [12,13,14,15]. To train such a deep neural network, large datasets are mandato ry. Re cently, reidentiﬁcation datasets largeenough to train deep mode ls haveemerged [13,4,16]. In many works [17,15], a neural network is trained on a large d ataset and then ﬁne tuned on a smaller one. Consequently, for the perfor mance evalu ation, a speciﬁc ﬁne tuned model is used to evaluate its correspond ing dataset. For an industrial purpose, having a single model able to perform well on many datasets is extremely important. It means that the model can han dle diﬀer ent situations, which enables deploying the same model on cameras in stalled in diﬀerent environments. Reidentiﬁcation with CNNs is usually performed using features extr acted by the neural network from identities during the training phase. At tributes, that are more highlevel features, like gender, clothes length, ha ndbag may be extremely valuable for reidentiﬁcation since such features are tr uly robust to viewangle and cameras change. Schumann et al. [15] demonstrated that using only attributes leads to low performance compared to the feature s learned by a CNN from the identities. A good approach is therefore to use a comb ination of attributes and features extracted from identities. To have a system able to make use of attributes, an access to a larg e dataset annotated with attributes is required. Nevertheless, it is diﬃcult to acquire large training datafora set ofattributes sincemanual annotationsis ex tremely expen sive.Thus,only asubsetofreidentiﬁcationdatasetsis annotate dwith attributes and many of them will remain attributeless. It is therefore a problem to build a general system that performs well on several datasets and ma ke use of atPerson reidentiﬁcation across diﬀerent datasets with mul titask learning 3 tributes. To deal with the variation of size and annotation of reide ntiﬁcation datasets, we present in this paper a multitask learning approach w hich learns the reidentiﬁcation task from a combination of several datasets . Furthermore, our system is able to take advantage of attribute information in dat aset anno tated with. Two main strategies are used in the reidentiﬁcation community for t raining deep neural networks. We will describe them in more detail in the nex t section. The ﬁrstone[18,19,20]isbasedonsiamesenetworks,contractiveo rtriplet losses. The second one [21,14], used in our work, is based on classiﬁcation loss es. Since the last layer is a linear classiﬁer, classiﬁcation methods ensure that features are linearly separable. Consequently, the distance between featu res belonging to two diﬀerent classes increases. Nevertheless, with this approa ch, the intra class variation is not controlled. Intuitively, reducing the intraclas s variations can make the features more discriminant and then increase the re identiﬁcation performance.Inthis work,weaddonetaskinourmultitasklearnin gobjective:a task designed to force the features of same identities to be the clo sest as possible. For the implementation, we employ a method described in [1] which prop oses a loss called center loss . We then evaluate the interest of this center loss for our reidentiﬁcation system. The contributions of our work are three folds: –We build a model that learns a generic representation of the person using several datasets for reidentiﬁcation (CHUK01 [3], CHUK03 [4], MAR S [16], ViPER [2], Market1501with attributes [22]) to build a system that perf orms well on all of these datasets without performing a speciﬁc ﬁne tunin g. –We take advantage of attributes available in some reidentiﬁcation d atasets such as hair length, top/bottom color, clothes length. We have a mu ltitask learning objective: the reidentiﬁcation task, learned from all the datasets and the attribute classiﬁcation tasks, learned from a subset of th e available datasets. –We evaluate an auxiliary task designed to control the intraclass var iation of the reidentiﬁcation features. This task is based on the center los s described in [1]. 2 Related work "
16,Ensembling Neural Networks for Improved Prediction and Privacy in Early Diagnosis of Sepsis.txt,"Ensembling neural networks is a long-standing technique for improving the
generalization error of neural networks by combining networks with orthogonal
properties via a committee decision. We show that this technique is an ideal
fit for machine learning on medical data: First, ensembles are amenable to
parallel and asynchronous learning, thus enabling efficient training of
patient-specific component neural networks. Second, building on the idea of
minimizing generalization error by selecting uncorrelated patient-specific
networks, we show that one can build an ensemble of a few selected
patient-specific models that outperforms a single model trained on much larger
pooled datasets. Third, the non-iterative ensemble combination step is an
optimal low-dimensional entry point to apply output perturbation to guarantee
the privacy of the patient-specific networks. We exemplify our framework of
differentially private ensembles on the task of early prediction of sepsis,
using real-life intensive care unit data labeled by clinical experts.","Ensembling describes a family of algorithms that train mult iple learners to solve the same problem, and exploit their heterogeneous properties to per form a committeebased predic tion that achieves higher accuracy than any single componen t learner. These techniques are welltried in machine learning practice and have led to t heoretically wellfounded algo rithms such as stacking ( Wolpert,1992), boosting ( Freund and Schapire ,1995), or bagging (Breiman,1996). Research on ensembling has very early tackled the problem of reducing variance of neural networks while keeping bias low at the sam e time. In the wide spectrum of approaches, ranging from sophisticated techniques to jo intly train component networks (Liu and Yao ,1999;Buschj¨ ager et al. ,2020) to building ensembles from model parameters of a single training trajectory ( Huang et al. ,2017;Izmailov et al. ,2018), we are speciﬁ cally interested in approaches where component models are t rained independently and then smartly combined. A key insight in this area, ﬁrst formulated in Perrone and Cooper (1992), is that the generalization error of theweighted average of prediction sof individualcomponent networks can be formalized as the weighted correlation between the co mponent neural networks participating in the ensemble. This formulation opens seve ral possibilities for eﬃcient and ©2022 S. Schamoni, M. Hagmann & S. Riezler.Ensembling Neural Networks for Improved Prediction and Pri vacy eﬀective machine learning: First, the bulk of the machine lea rning cost, namely the cost of training individual component networks, can be triviall y parallelized or even be done asynchronously, thus providing an eﬃcient way of enhancing the representational power of the ensemble by training multiple classiﬁers at once. Sec ond, optimizing combination weights to minimize the weighted correlation between compo nent networks provides a direct avenue to minimize the generalization error of the ensemble , or to build a sparse ensemble from the optimal subset of component networks with small err or and small correlation with other component networks. A further advantage of weightedaveraging ensembles that h as been investigated much less than their generalization performance is the possibil ity to seamlessly integrate privacy protection into machine learning. In the case of machine lea rning models trained on medical data, the privacy to be protected might concern the membersh ip of patientspeciﬁc data in the training data for a particular disease. As argued by Dinur and Nissim (2003), removal of “identifying” attributes such as patients’ names is not e nough, but instead random per turbations have to be applied to the outputs in order to prote ct privacy even in the simplest case of “statistical” queries such as averages over databas es. The framework of diﬀerential privacy ( Dwork and Roth ,2014) allows giving strong guarantees on the information deriv able from private training data when querying a machine lear ning algorithm. We show that weightedaveraging ensembles do possess small sensitivit y by tightly bounded output ranges and do not accumulate privacy budget via iterative training , thus they are ideally suited for privacy protection at small noise scales. Furthermore, we prove that uniform weights are optimal to protect privacy in a weightedaveraging ense mble. Specifying guarantees on privacy protection is of increasi ng importance for medical re search. National laws and regulations such as the US HIPAA Pr ivacy rule1requiremeasures to protect the privacy of health information. On the hospita l level, protecting a patient’s privacy is crucial especially when information is shared ac ross institutions. Our method demonstrates the beneﬁt of output sharing where hospitals keep their inhouse model in a secured area and only share the output with other institutio ns, thus avoiding the challenges and diﬃculties of model sharing techniques such as federated learning ( Rieke et al. ,2020). On the patient level, a recent survey has shown that more than 30% of the participants are comfortable with sharing their electronic health data f or personalized healthcare, while less than 5% are very uncomfortable with sharing ( Garett and Young ,2022). This means more than 60% do not have a strong opinion on this topic, thus w e hope that an increasing number of people will share their data if stronger privacy gu arantees can be given. Generalizable Insights about Machine Learning in the Context of Healthcare Expert labels and neural networks are a powerful combinatio n for early sepsis prediction. However, patient data for this task is scarce as expert label s are diﬃcult to obtain, while the protection of privacy is crucial to encourage patients t o contribute with their personal private data. We show how to train individual personalized m odels and how to combine a small number of patient models in an ensemble that has more de sirable properties in the ﬁeld of medical data analysis than a standard full model, i.e ., a single model that is trained on all available patient data. 1.www.hhs.gov/hipaa/forprofessionals/privacy/ (accessed 07/06/2022) 2Ensembling Neural Networks for Improved Prediction and Pri vacy •We present theoretical results that an ensemble of models wh ich was trained on a fraction of the available data can be better than a full model , and we verify this empirically. •Our training method not only exposes fewer patients in the pr edictor than a full model, but also protects the privacy better: we apply a stron g membership attack and show that the ensemble successfully prevents privacy le akage. •We show that an ensemble of several models is favorable to a si ngle model due to its reduced sensitivity in theory, and we experimentally ve rify that our ensemble maintains its accuracy at privacy budgets almost two orders of magnitude smaller than a full model. Furthermore, our ensemble can be easily updated by modelgr owing without the need of retraining the whole system when new patient’s data becomes available. 2. Related Work "
540,Meta Self-Refinement for Robust Learning with Weak Supervision.txt,"Training deep neural networks (DNNs) under weak supervision has attracted
increasing research attention as it can significantly reduce the annotation
cost. However, labels from weak supervision can be noisy, and the high capacity
of DNNs enables them to easily overfit the label noise, resulting in poor
generalization. Recent methods leverage self-training to build noise-resistant
models, in which a teacher trained under weak supervision is used to provide
highly confident labels for teaching the students. Nevertheless, the teacher
derived from such frameworks may have fitted a substantial amount of noise and
therefore produce incorrect pseudo-labels with high confidence, leading to
severe error propagation. In this work, we propose Meta Self-Refinement (MSR),
a noise-resistant learning framework, to effectively combat label noise from
weak supervision. Instead of relying on a fixed teacher trained with noisy
labels, we encourage the teacher to refine its pseudo-labels. At each training
step, MSR performs a meta gradient descent on the current mini-batch to
maximize the student performance on a clean validation set. Extensive
experimentation on eight NLP benchmarks demonstrates that MSR is robust against
label noise in all settings and outperforms state-of-the-art methods by up to
11.4% in accuracy and 9.26% in F1 score.","Finetuning pretrained language models has led to great success across NLP tasks. Nonetheless, it still requires a substantial amount of manual labels to achieve satisfying performance on many tasks. In reality, obtaining large amounts of highquality labels is costly and laborintensive (Davis et al., 2013). For certain domains, it is even infeasible due to legal issues and lack of data or domain experts. Weak supervision is a widelyused approach for reudcing such cost by leveraging labels from weak sources, e,g., heuristic rules, knowledge bases orlowerquality inexpensive crowdsourcing (Ratner et al., 2017; Zhou et al., 2020; Lison et al., 2020). It has raised increasing attention in recent years, and efforts have been made to quantify the progress on weakly supervised learning, like the WRENCH benchmark (Zhang et al., 2021). Although weak labels are inexpensive to ob tain, they are often noisy and inherit biases from weak sources. Training neural networks with weak labels is challenging because of their immense capacity, which leads them to heavily overﬁt to the noise distribution, resulting in inferior gener alization performance (Zhang et al., 2017). Vari ous approaches have been proposed to tackle this challenge. Earlier research focused primarily on simulated noise (Bekker and Goldberger, 2016; Hendrycks et al., 2018), required prior knowl edge (Ren et al., 2020; Awasthi et al., 2020) or relied on contextfree aggregation rules without leveraging modern pretrained language models (Ratner et al., 2017; Fu et al., 2020). Recently, Yu et al. (2021) proposed a contrastive regularized selftraining framework that achieved stateoftheart (SOTA) performance in several NLP tasks from the WRENCH benchmark. It trains a teacher network on weak labels, then it eratively applies the teacher to produce pseudo labels for training a new student model. To pre vent error propagation, it ﬁlters the pseudolabels with the model conﬁdence scores and adds con trastive feature regularization to enforce more dis tinguishable representations. However, we ﬁnd that this approach is effective on easy tasks but fragile on challenging ones , where the initial teacher model already have memorized a substan tial amount of biases with high conﬁdence. Con sequently, conﬁdencebased ﬁltering is misleading and all future students will be reinforced with these initial wrong biases from the teacher. To address this weakness, one strategy is learn ing to reweight the pseudolabels with meta learnarXiv:2205.07290v2  [cs.CL]  30 Apr 2023ing (Ren et al., 2018; Shu et al., 2019; Wang et al., 2020). By this means, sample weights are dynam ically adjusted to minimize the validation loss in stead of preﬁxed with potentially misleading conﬁ dence scores. Nevertheless, if the initial teacher is weak and mostly produces incorrect pseudolabels, simply reweighting the labels does not sufﬁce to extract enough useful training signals. In this paper, we propose Meta SelfReﬁnement (MSR) to go one step further. The teacher is jointly trained with a meta objective such that the student, after one gradient step, can achieve better perfor mance on the validation set. In each training step, a copy of the current student performs one step of gradient descent based on the teacher predic tions. The teacher will then update itself towards the gradient direction that minimizes the validation loss of the student. Finally, the actual student is trained by the updated teacher. In MSR, teacher’s predictions are iteratively reﬁned , instead of only “reweighted”, based on the meta objective. This will enable more efﬁcient data utilization since the teacher still has the opportunity to reﬁne itself to provide the proper training signal, even if its initial output label is wrong. To further stabilize the train ing, we enhance our framework with conﬁdence ﬁltering when teaching the student and apply a lin early scaled learning rate scheduler to the teacher. In summary, the main contributions are as fol lows: 1)We propose a metalearning based self reﬁnement framework, MSR, that allows robust learning with label noise induced by weak supervi sion. 2)We analyze and quantify how label noise impacts model predictions and representation learn ing. We ﬁnd existing methods become less effec tive in challenging cases when the label noise can be easily ﬁtted. In contrast, MSR is more stable and learns better representation. 3)Extensive experi ments demonstrate that MSR consistently reduces the negative impact of the label noise, matching or outperforming SOTAs on six sequence classiﬁca tion and two sequence labeling tasks.1 2 Related Work "
241,Deep Learning is Robust to Massive Label Noise.txt,"Deep neural networks trained on large supervised datasets have led to
impressive results in image classification and other tasks. However,
well-annotated datasets can be time-consuming and expensive to collect, lending
increased interest to larger but noisy datasets that are more easily obtained.
In this paper, we show that deep neural networks are capable of generalizing
from training data for which true labels are massively outnumbered by incorrect
labels. We demonstrate remarkably high test performance after training on
corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain
test accuracy above 90 percent even after each clean training example has been
diluted with 100 randomly-labeled examples. Such behavior holds across multiple
patterns of label noise, even when erroneous labels are biased towards
confusing classes. We show that training in this regime requires a significant
but manageable increase in dataset size that is related to the factor by which
correct labels have been diluted. Finally, we provide an analysis of our
results that shows how increasing noise decreases the effective batch size.","Deep neural networks are typically trained using supervised learning on large, carefully annotated datasets. However, the need for such datasets restricts the space of problems that can be addressed. This has led to a proliferation of deep learning results on the same tasks using the same wellknown datasets. However, carefully annotated data is difﬁcult to obtain, especially for classiﬁcation tasks with large numbers of classes (requiring extensive annotation) or with ﬁnegrained classes (requiring skilled annotation). *Equal contribution1Department of Mathematics, Mas sachusetts Institute of Technology, Cambridge, MA USA 2Department of Computer Science & Cornell Tech, Cornell Uni versity, New York, NY USA3Department of Computer Science, Massachusetts Institute of Technology, Cambridge, MA USA. Cor respondence to: David Rolnick <drolnick@mit.edu >, Andreas Veit<av443@cornell.edu >.Thus, annotation can be expensive and, for tasks requiring expert knowledge, may simply be unattainable at scale. To address this limitation, other training paradigms have been investigated to alleviate the need for expensive an notations, such as unsupervised learning (Le, 2013), self supervised learning (Pinto et al., 2016; Wang & Gupta, 2015) and learning from noisy annotations (Joulin et al., 2016; Natarajan et al., 2013; Veit et al., 2017). Very large datasets (e.g., Krasin et al. (2016); Thomee et al. (2016)) can often be obtained, for example from web sources, with partial or unreliable annotation. This can allow neural net works to be trained on a much wider variety of tasks or classes and with less manual effort. The good performance obtained from these large, noisy datasets indicates that deep learning approaches can tolerate modest amounts of noise in the training set. In this work, we study the behavior of deep neural networks under extremely low label reliability, only slightly above chance. The insights from our study can help guide future settings in which arbitrarily large amounts of data are easily obtainable, but in which labels come without any guarantee of validity and may merely be biased towards the correct distribution. The key takeaways from this paper may be summarized as follows: •Deep neural networks are able to generalize after training on massively noisy data, instead of merely memorizing noise. We demonstrate that standard deep neural networks still perform well even on training sets in which label accuracy is as low as 1 percent above chance. On MNIST, for example, performance still exceeds 90 percent even with this level of label noise (see Figure 1). This behavior holds, to varying extents, across datasets as well as patterns of label noise, including when noisy labels are biased towards confused classes. •A sufﬁciently large training set can accommodate a wide range of noise levels. We ﬁnd that the minimum dataset size required for effective training increases with the noise level (see Figure 9). A large enough training set can accommodate a wide range of noise levels. Increasing the dataset size further, however, arXiv:1705.10694v3  [cs.LG]  26 Feb 2018Deep Learning is Robust to Massive Label Noise does not appreciably increase accuracy (see Figure 8). •High levels of label noise decrease the effective batch size , as noisy labels roughly cancel out and only a small learning signal remains. As such, dataset noise can be partly compensated for by larger batch sizes and by scaling the learning rate with the effective batch size. 2. Related Work "
311,Semi-Supervised Learning for Sparsely-Labeled Sequential Data: Application to Healthcare Video Processing.txt,"Labeled data is a critical resource for training and evaluating machine
learning models. However, many real-life datasets are only partially labeled.
We propose a semi-supervised machine learning training strategy to improve
event detection performance on sequential data, such as video recordings, when
only sparse labels are available, such as event start times without their
corresponding end times. Our method uses noisy guesses of the events' end times
to train event detection models. Depending on how conservative these guesses
are, mislabeled samples may be introduced into the training set. We further
propose a mathematical model for explaining and estimating the evolution of the
classification performance for increasingly noisier end time estimates. We show
that neural networks can improve their detection performance by leveraging more
training data with less conservative approximations despite the higher
proportion of incorrect labels. We adapt sequential versions of CIFAR-10 and
MNIST, and use the Berkeley MHAD and HMBD51 video datasets to empirically
evaluate our method, and find that our risk-tolerant strategy outperforms
conservative estimates by 3.5 points of mean average precision for CIFAR, 30
points for MNIST, 3 points for MHAD, and 14 points for HMBD51. Then, we
leverage the proposed training strategy to tackle a real-life application:
processing continuous video recordings of epilepsy patients, and show that our
method outperforms baseline labeling methods by 17 points of average precision,
and reaches a classification performance similar to that of fully supervised
models. We share part of the code for this article.","Labeled image and video datasets are crucial for training and evaluating machine learning models. As a result, com puter vision researchers have compiled a number of labeled benchmark datasets, such as MNIST [20], ImageNet [8], MSCOCO [22], Kinetics [17], CIFAR [18], and Cityscapes [7]. However, many application areas still remain poorly covered, such as medical imaging data, despite recent ini tiatives such as the UK Biobank [28]. Although medical institutions often possess large amounts of data, most of it remains unlabeled and underutilized. For example, for research purposes, some hospitals record hours of videos of patients in intensive care, but those videos remain only poorly labeled in the clinical routine, with at best, the sparse event labels. Weaklysupervised learning aims to leverage datasets with either incomplete or incorrect labels. Zhou et al. [32] identified two subtypes of weak supervision schemes: in complete and inaccurate supervision. Incomplete super vision applies when only a portion of the training sam ples are labeled. For example, semisupervised learning methods are designed to leverage unlabeled samples next to labeled samples. Inaccurate supervision applies when the given labels are not necessarily correct (e.g., crowd sourcing [23, 6]). The works of Hao et al. [12] on mam mograms and Karimi et al. [16] on brain MRIs are also examples of inaccurate supervision with deep learning for medical data. In this work, we propose a method which combines semisupervised learning and inaccurate supervision to leverage sparselylabeled sequential data. The main task is to detect sequences of events, given only sparse training la bels, i.e., the start times of these events. The end times and the duration of these events remain unknown, which pre vents sampling any positives events with certainty (FigurearXiv:2011.14101v5  [cs.CV]  1 Oct 2022Figure 1. Leveraging sparse video labels. During training time, only event start times are annotated. Event end times have to be guessed, which determine the number Nof elements that can be used as positives during training. A conservative model only uses a single element (N= 1) as positive per sparse label, while a risktolerant model uses multiple elements, e.g. N= 3. Higherrisk labeling strategies may result in more incorrect labels during training (negative segments being mislabeled as positives). However, these higherrisk strategies can provide more training data, which may result in better detection performance despite training with incorrect labels. 1). For example, in a cooking videos dataset, sparse training labels could indicate when cooking an ingredient started at time T, but without any information about when the cooking of that ingredient stopped. To address this problem, we propose making a noisy ap proximation of event end times. For each sparse label, we choose a fixed number of consecutive elements in sequence that follow the sparse label, and use them as positive train ing samples (essentially providing a noisy estimate of dura tion). In the above example with cooking videos, we could guess that the cooking of the ingredient lasts one minute, or 1500 frames at 25 frames per second, and use all 1500 frames as positive samples. The longer the estimated guess, the more likely it is that we introduce potential incorrectly labeled samples (false positives in the training set). We further propose a mathematical model for explain ing and estimating the evolution of the classification per formance for increasingly noisier end time estimates. This model include two sigmoidbased components respectively describing the positive and negative impact of additional noisy labeled sequence elements on the performance. We empirically evaluate our method on sparselylabeled sequences of CIFAR10 images, MNIST images, Berke ley MHAD videos, and HMDB51, and show an improve ment of 3.5 points of mean average precision for CIFAR, 30 points for MNIST, 3 points for MHAD, and 14 points for HMDB51 over the baseline method. Finally, we demonstrate our method on a reallife se quential analysis task—video monitoring of epileptic hos pital patients. Electroencephalography (EEG) is a com mon modality for recording brain activity and monitoring patients. Automated methods have been developed to au tomatically detect seizures from EEG activity [9, 24] but can fail to discern seizures from artifacts caused by distur bances in EEG measurement (e.g., patting on the back or rocking neonatal patients can trigger false positive seizure detections). We address EEG artifacts by automatically detecting five artifact–suctioning of neonates, chewing, rocking, cares by nurse and patting of neonates–from continuous videorecordings acquired during clinical routine. Those events are annotated with sparse labels (only start times, no end times), which is common practice for the labeling of con tinuous recordings in clinical routine [26]. Our method for learning from sparselylabeled sequences can leverage those sparse labels, outperforming baseline methods by 17 points of mean average precision. We show that our semi supervised model can reach the classification of fully super vised models. We also give insight into estimating the pa rameters of the proposed model in case of merging classes. To summarize, our main contributions are: • A training strategy for semisupervised learning with sparselylabeled sequential data. • A mathematical model for explaining and estimating the evolution of the classification performance for in creasingly noisier end time estimates. • A method that automatically detects events from sparselylabeled continuous video recordings of hos pital neonates. 2. Related works "
319,Smooth Neighbors on Teacher Graphs for Semi-supervised Learning.txt,"The recently proposed self-ensembling methods have achieved promising results
in deep semi-supervised learning, which penalize inconsistent predictions of
unlabeled data under different perturbations. However, they only consider
adding perturbations to each single data point, while ignoring the connections
between data samples. In this paper, we propose a novel method, called Smooth
Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on
the predictions of the teacher model, i.e., the implicit self-ensemble of
models. Then the graph serves as a similarity measure with respect to which the
representations of ""similar"" neighboring points are learned to be smooth on the
low-dimensional manifold. We achieve state-of-the-art results on
semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for
CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,
the improvements are significant when the labels are fewer. For the
non-augmented MNIST with only 20 labels, the error rate is reduced from
previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.","As collecting a fully labeled dataset is often expensive and timeconsuming, semisupervised learning (SSL) has been extensively studied in computer vision to improve general ization performance of the classiﬁer by leveraging limited labeled data and a large amount of unlabeled data [ 9]. The success of SSL relies on the key smoothness assumption, i.e., data points close to each other are likely to have the same label. It has a special case named cluster orlow density sep aration assumption, which states that the decision boundary should lie in low density regions, not crossing high density regions [ 10]. Based on these assumptions, many traditional methods have been developed [22, 54, 51, 10, 4]. Recently due to the great advances of deep learning [ 25], remarkable results have been achieved on SSL [ 24,35,40, Corresponding author.27]. Among these works, perturbationbased methods [ 37, 2,35,39,27] have demonstrated great promise. Adding noise to the deep model is important to reduce overﬁtting and learn more robust abstractions, e.g., dropout [ 21] and randomized data augmentation [ 13]. In SSL, perturbation regularization aids by exploring the smoothness assumption. For example, the Manifold Tangent Classiﬁer (MTC) [ 37] trains contrastive autoencoders to learn the data manifold and regularizes the predictions to be insensitive to local perturbations along the lowdimensional manifold. Pseudo Ensemble [ 2] and "
216,Correlated Input-Dependent Label Noise in Large-Scale Image Classification.txt,"Large scale image classification datasets often contain noisy labels. We take
a principled probabilistic approach to modelling input-dependent, also known as
heteroscedastic, label noise in these datasets. We place a multivariate Normal
distributed latent variable on the final hidden layer of a neural network
classifier. The covariance matrix of this latent variable, models the aleatoric
uncertainty due to label noise. We demonstrate that the learned covariance
structure captures known sources of label noise between semantically similar
and co-occurring classes. Compared to standard neural network training and
other baselines, we show significantly improved accuracy on Imagenet ILSVRC
2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a
new state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These
datasets range from over 1M to over 300M training examples and from 1k classes
to more than 21k classes. Our method is simple to use, and we provide an
implementation that is a drop-in replacement for the final fully-connected
layer in a deep classifier.","Image classiﬁcation datasets with many classes and large training sets often have noisy labels [ 2,30]. For example, Imagenet contains many visually similar classes that are hard for human annotators to distinguish [ 10,2]. Datasets such as WebVision where labels are generated automatically by look ing at cooccuring text to images on the Web, contain label noise as this automated process is not 100% reliable [30]. A wide range of techniques for classiﬁcation under label noise already exist [ 29,23,16,37,24,6,9,36,18]. When an image is mislabeled it is more likely that it gets confused with other related classes, rather than a random class [ 2]. Therefore it is important to take interclass correlation into account when modelling label noise in image classiﬁcation. Figure 1: Spot the difference? An Appenzeller (left) and EntleBucher (right). Two visually similar Imagenet classes our method learns have highly correlated label noise (aver age validation set covariance of 0.24) given only the stan dard Imagenet ILSVRC12 training labels. We take a principled probabilistic approach to modelling label noise. We assume a generative process for noisy labels with a multivariate Normal distributed latent variable at the ﬁnal hidden layer of a neural network classiﬁer. The mean and covariance parameters of this Normal distribution are inputdependent (aka heteroscedastic), being computed from a shared representation of the input image. By modelling the interclass noise correlations our method can learn which class pairs are substitutes or commonly cooccur, resulting in noisy labels. See Fig. (1) for an example of two Imagenet classes which our model learns have correlated label noise. We evaluate our method on four largescale image clas siﬁcation datasets, Imagenet ILSVRC12 and Imagenet 21k[10], WebVision 1.0 [ 30] and JFT [ 21]. These datasets range from over 1M training examples (ILSVRC12) to 300M training examples (JFT) and from 1kclasses (ILSVRC12 & WebVision) to over 21kclasses (Imagenet 21k). We demon strate improved accuracy and negative loglikelihood on all datasets relative to (a) standard neural network training, (b) methods which only model the diagonal of the covariance matrix and (c) methods from the noisy labels literature. We evaluate the effect of our probabilistic label noise model on the representations learned by the network. We show that our method, when pretrained on JFT, learns image 1arXiv:2105.10305v1  [cs.LG]  19 May 2021representations which transfer better to the 19 datasets from the Visual Task Adaptation Benchmark (VTAB) [47]. Contributions. In summary our contributions are: 1.A new method which models interclass correlated label noise and scales to largescale datasets. 2.We evaluate our method on four largescale image clas siﬁcation datasets, showing signiﬁcantly improved per formance compared to standard neural network training and diagonal covariance methods. 3.We demonstrate that the learned covariance matrices model correlations between semantically similar or commonly cooccurring classes. 4.On VTAB our method learns more general representa tions which transfer better to 19 downstream tasks. 2. Method "
546,ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal Self-Ensemble for Active Learning.txt,"Modern deep learning has achieved great success in various fields. However,
it requires the labeling of huge amounts of data, which is expensive and
labor-intensive. Active learning (AL), which identifies the most informative
samples to be labeled, is becoming increasingly important to maximize the
efficiency of the training process. The existing AL methods mostly use only a
single final fixed model for acquiring the samples to be labeled. This strategy
may not be good enough in that the structural uncertainty of a model for given
training data is not considered to acquire the samples. In this study, we
propose a novel acquisition criterion based on temporal self-ensemble generated
by conventional stochastic gradient descent (SGD) optimization. These
self-ensemble models are obtained by capturing the intermediate network weights
obtained through SGD iterations. Our acquisition function relies on a
consistency measure between the student and teacher models. The student models
are given a fixed number of temporal self-ensemble models, and the teacher
model is constructed by averaging the weights of the student models. Using the
proposed acquisition criterion, we present an AL algorithm, namely
student-teacher consistency-based AL (ST-CoNAL). Experiments conducted for
image classification tasks on CIFAR-10, CIFAR-100, Caltech-256, and Tiny
ImageNet datasets demonstrate that the proposed ST-CoNAL achieves significantly
better performance than the existing acquisition methods. Furthermore,
extensive experiments show the robustness and effectiveness of our methods.","Deep neural networks (DNNs) require a large amount of training data to opti mize millions of weights. In particular, for supervisedlearning tasks, labeling of training data by human annotators is expensive and timeconsuming. The label ingcostcanbeamajorconcernformachinelearningapplications,whichrequires a collection of realworld data on a massive scale (e.g., autonomous driving) or ?Corresponding AuthorarXiv:2207.02182v2  [cs.CV]  16 Oct 2022the knowledge of highly trained experts for annotation (e.g., automatic medical diagnosis). Active learning (AL) is a promising machine learning framework that maximizes the eﬃciency of labeling tasks within a ﬁxed labeling budget [35]. ThisstudyfocusesonthepoolbasedALproblem,wherethedatainstancesto belabeledareselectedfromapoolofunlabeleddata.InapoolbasedALmethod, the decision to label a data instance is based on a sample acquisition function . The acquisition function, a(x;f), takes the input instance xand the currently trained model fand produces a score to decide if xshould be labeled. Till date, various types of AL methods have been proposed [9–11,20,35,36,38,46,47]. The predictive uncertaintybased methods [9,10,36,46] used wellstudied theo reticmeasuressuchasentropyandmutualinformation.Recently,representation based methods [20,34,38,47] have been widely used as a promising AL approach to exploit highquality representation of DNNs. However, the acquisition used for these methods rely on a single trained model f, failing to account for model uncertainty arising given a limited labeled dataset. To solve this problem, sev eral AL methods [2,35] attempted to utilize an ensemble of DNNs and design acquisition functions based on them. However, these methods require signiﬁcant computational costs to train multiple networks. Whentheamountoflabeleddataislimited,semisupervisedlearning(SSL)is another promising machine learning approach to improve performance with low labelingcosts.SSLimprovesthemodelperformancebyleveragingalargenumber of unlabeled examples [31]. Consistency regularization is one of the most suc cessful approach to SSL [11,22,30,40]. In a typical semisupervised learning, the modelistrainedusingtheconsistencyregularizedlossfunction Ece+Econ,where Ecedenotes the crossentropy loss and Econdenotes the consistencyregularized loss. Minimization of Econregularizes the model to produce consistent predic tions over the training process, improving the performance for a given task. model [22] applied a random perturbation to the input of the DNN and measured the consistency between the model outputs. Mean Teacher (MT) [40] produced the temporal selfensemble through SGD iterations and measured the consistency between the model being trained and the teacher model obtained by taking the exponential moving average (EMA) of the selfensemble. These meth ods successfully regularized the model to produce consistent predictions while using perturbed predictions on unlabeled samples. The objective of this study is to improve the sample acquisition criterion for poolbased AL methods. Inspired by the consistency regularization for SSL, we build a new sample acquisition criterion that measures consistency between multiple ensemble models obtained during the training phase. The proposed ac quisition function generates the temporal selfensemble by sampling the models at the intermediate checkpoints of the weight trajectory formed by the SGD optimization. This provides a better acquisition performance and eliminates the additionalcomputationalcostrequiredbypreviousensemblebasedALmethods. In [1,17,27], the aforementioned method has shown to produce good and diverse selfensembles, which were used to improve the inference model via stochastic weight averaging (SWA) [1,17,27]. We derive the acquisition criterion based onFig.1:Acquisition criterion : Both labeled and unlabeled samples are repre sented in the feature space in the binary classiﬁcation problem. Low consistency samples produce the predictions which signiﬁcantly diﬀer between the student and teacher models. Low conﬁdence samples are found near the decision bound aries of the student models. The proposed STCoNAL evaluates the consistency measure for each data sample and selects those with the highest consistency score. the temporal selfensemble models used in SWA. We present the AL method, referred to as studentteacher consistencybased AL (STCoNAL) , which mea sures the consistency between the student and teacher models. The selfensemble model constitutes a student model, and a teacher model is formed by taking an equallyweighted average (EWA) of the parameters of the student models. Treat ing the output of the teacher model as a desired supervisory signal, STCoNAL measures the Kullback–Leibler (KL) divergence of each teacherstudent output pairs. The acquisition function of STCoNAL acquires the samples to be labeled that yield the highest inconsistency. Though STCoNAL was inspired by the consistency regularization between student and teacher models of the MT, these two methods are quite diﬀerent in the following aspects. MT constructs the teacher model by assigning larger weights to more recent model weights obtained through SGD iterations. Due to this constraint, as training progresses, the teacher model in MT tends to be correlated with the student models, making it diﬃcult to measure good enough consistency measure for AL acquisition. To address this problem, STCoNAL generatesabetterteachermodelbytakinganequallyweightedaveraging(EWA) of the weights of the student models instead of EMA used in MT. Similar to previousALmethods[9,10]thatutilizeensemblemodelstocapturetheposterior distribution of model weights for a given data set, the use of student model weights allows our acquisition criterion to account for model uncertainty.We further improve our STCoNAL method by adopting the principle of entropyminimizationusedforSSL[3,4,12,24,39,44].Weapplyoneoftheentropy minimization methods, sharpening to the output of the teacher model. When the sharpened output is used for our KL divergence, our acquisition criterion can measure the uncertainty of the prediction for the given sample. Our evaluation shows that the STCoNAL is superior to other AL methods on various image classiﬁcation benchmark datasets. Fig.1 illustrates that these low consistency samples lie in the region of the feature space where the student models produce the predictions of a larger vari ation. Note that these samples are not necessarily identical to the low conﬁdence samples located near the decision boundary speciﬁed by the teacher model. The proposed STCoNAL prefers the acquisition of the inconsistent samples rather than the lowconﬁdence samples. The main contributions of our study are summarized as follows: –We propose a new acquisition criterion based on the temporal selfensemble. Temporal selfensemble models are generated by sampling DNN weights through SGD optimization. STCoNAL measures the consistency between these selfensemble models and acquires the most inconsistent samples for labeling. We evaluated the performance of STCoNAL on four diﬀerent pub lic datasets for multiclass image classiﬁcation tasks. We observe that the proposed STCoNAL method achieves the signiﬁcant performance gains over other AL methods. –We identiﬁed a work relevant to ours [11]. While both ours and their work aim to exploit consistency regularization for AL, our work diﬀers from theirs inthefollowingaspects.WhileCSSAL[11]reliesontheinputperturbationto a single ﬁxedmodel, STCoNAL utilizes the selfensemble models tomeasure the consistency. Note that the beneﬁts of using model ensembles for AL have been demonstrated in [2] and our ﬁndings about the superior performance of STCoNAL over CSSAL are consistent with the results of these studies. 2 Related Work "
73,Robust Learning by Self-Transition for Handling Noisy Labels.txt,"Real-world data inevitably contains noisy labels, which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus, to counter the label noise challenge, we propose a novel
self-transitional learning method called MORPH, which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase, the network is updated using all the samples to collect a seed
of clean samples. Then, in the evolution phase, the network is updated using
only the set of arguably clean samples, which precisely keeps expanding by the
updated network. Thus, MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.","In supervised learning for data analysis tasks, deep neural net works (DNNs) have become one of the most popular methods in that traditional machine learning is successfully superseded by recent deep learning in numerous applications [ 13,37]. However, their success is conditioned on the availability of massive data with carefully annotated human labels, which are expensive and timeconsuming to obtain in practice. Some substitutable sources, ∗JaeGil Lee is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ’21, August 14–18, 2021, Virtual Event, Singapore ©2021 Association for Computing Machinery. ACM ISBN 9781450383325/21/08. . . $15.00 https://doi.org/10.1145/3447548.3467222such as Amazon’s Mechanical Turk and surrounding tags of col lected data, have been widely used to mitigate the high labeling cost, but they often yield samples with noisy labels that may not be true [ 35]. In addition, data labels can be extremely complex even for experts [ 7] and adversarially manipulated by a labelflipping attack [41], thereby being vulnerable to label noise. Modern DNNs are typically trained in an overparameterized regime where the number of the parameters of a DNN far exceeds the size of the training data [ 19]. In principle, such DNNs have the capacity to overfit to any given set of labels, even though part of the lables are significantly corrupted. In the presence of noisy labels, DNNs easily overfit to the entire training data regardless of the ratio of noisy labels, eventually resulting in poor generalization on test data [ 44]. Thus, in this paper, we address an important issue of “learning from noisy labels.” One of the most common approaches is sample selection , which involves training a DNN for a possibly clean subset of noisy training data [ 5,9,11,12,31,34]. Typically, in each training iteration, a certain number of smallloss training samples are selected as clean ones and subsequently used to robustly update the DNN. This small loss trick is satisfactorily justified by the memorization effect [2] that DNNs tend to first learn simple and generalized patterns and then gradually memorize all the patterns including irregular ones such as outliers and falselabeled samples. Although this family of methods has achieved better general ization by training with selected smallloss training samples, they commonly have the following twoproblems: 1.Discarding Hard Sample : Truelabeled samples with large losses are simply discarded by the smallloss trick, though they have a great impact on generalization [ 4]. This issue can be exac erbated by realworld and asymmetric label noises, where the loss distributions of true and falselabeled samples are overlapped closely [35]. 2.Inefficient Learning : The smallloss trick suffers from confir mation bias [ 38], which is a hazard of favoring the samples se lected at the beginning of training. Hence, recent approaches of ten leverage multiple DNNs to cooperate with one another [ 9,42] or run multiple training rounds to iteratively refine their selected set of clean samples [ 31,33], thus adding heavy computational overhead. In this regard, we have thoroughly investigated the memoriza tion of a DNN on realworld noisy training samples and, conse quently, observed the existence of twolearning periods in Figure 1(a):(i)the “noiserobust” period where the memorization of false labeled samples is insignificant because the DNN prefers memo rizing easy samples at an early stage and (ii)the “noiseprone” period where the memorization of falselabeled samples rapidly increases because the DNN eventually begins memorizing all the noisy samples at a late stage of training.arXiv:2012.04337v2  [cs.LG]  7 Jun 2021False Labeled Sample Default MORPH Generalization Improvement 0 40 Epochs 0 40 0 120 40 80 EpochsTrueLabeled Sample Noise Prone Noise Robust Evolution Seeding 120 80 Epochs0%25%50%75%100%Memorization Ratio 120 800%25%50%75%100%Memorization Ratio 50%60%70%80%Test ErrorTransition PointNoise Prone Noise Robust(a) Default. (b) MORPH. (c) Test Error Convergence. Figure 1: Key idea of MORPH: (a) and (b) show the memorization ratio when training a WideResNet168 on a subset of FOOD101N1with the realworld noise of 18.4%, where the memorization ratio is the number of memorized (see Definition 3.1) true or falselabeled samples to the total number of true or falselabeled training samples at each epoch. “Default” is a standard training method, and “MORPH” is our proposed one; (c) contrasts the convergence of their test error. These findings motivate us to come up with an approach that leverages the transitional memorization nature in a single train ing round. In this paper, we propose MORPH , which is a self transitional learning approach that automatically transitions its learning phase when a DNN enters the “noiseprone” period after the “noiserobust” period (i.e., the dashed line in Figure 1(b)). Thus, corresponding to these two periods, our key idea divides the train ing process into two learning phases, namely seeding andevolution : 1.Seeding : Owing to the negligible memorization of falselabeled samples, the model update is initiated using allthe training samples in the noiserobust period. As the samples memorized at this time are mostly easy samples with true labels, they are accumulated as a clean seed to derive a maximal safe set in the next phase. Note that MORPH automatically estimates the best phase transition point without anysupervision. 2.Evolution : Without memorizing falselabeled samples, the DNN evolves by being updated only for the maximal safe set in the noiseprone period. Then, the updated DNN recognizes more truelabeled samples previously hard to distinguish and filters out falselabeled samples incorrectly included. This alternating process repeats per iteration so that the maximal safe set is ex panded and refined in the remaining noiseprone period. Through selftransitional learning, MORPH avoids the confirma tion bias by exploiting the noiserobust period with all the training samples, thus eliminating the need for additional DNNs and training rounds. In addition, it incrementally expands the clean seed towards the maximal safe set, which can cover even hard truelabeled sam ples, not just throwing them away. The alternating process in the second phase minimizes the risk of misclassifying falselabeled sam ples as truelabeled ones or vice versa. Hence, as shown in Figure 1(b), MORPH prevents the memorization of falselabeled samples by training with the maximal safe set during the noiseprone period, while increasing the memorization of truelabeled samples. As a result, as shown in Figure 1(c), the generalization performance of a DNN improves remarkably even in realworld noise. Our main contributions are summarized as follows: •No Supervision for Transition : MORPH performs self transitional learning without any supervision such as a true noise rate and a clean validation set, which are usually hard to acquire in realworld scenarios. 1We used the subset in which correct labels are identified.•Noise Robustness : Compared with stateoftheart methods, MORPH identifies truelabeled samples from noisy data with much higher recall and precision. Thus, MORPH improved the test (or validation) error by up to 27.0𝑝𝑝2for three datasets with two synthetic noises and by up to 8.90𝑝𝑝and3.85𝑝𝑝for WebVi sion 1.0 and FOOD101N with realworld noise. •Learning Efficiency : Differently from other methods, MORPH requires neither additional DNNs nor training rounds. Thus, it was significantly faster than others by up to 3.08times. 2 RELATED WORK "
559,RH-Net: Improving Neural Relation Extraction via Reinforcement Learning and Hierarchical Relational Searching.txt,"Distant supervision (DS) aims to generate large-scale heuristic labeling
corpus, which is widely used for neural relation extraction currently. However,
it heavily suffers from noisy labeling and long-tail distributions problem.
Many advanced approaches usually separately address two problems, which ignore
their mutual interactions. In this paper, we propose a novel framework named
RH-Net, which utilizes Reinforcement learning and Hierarchical relational
searching module to improve relation extraction. We leverage reinforcement
learning to instruct the model to select high-quality instances. We then
propose the hierarchical relational searching module to share the semantics
from correlative instances between data-rich and data-poor classes. During the
iterative process, the two modules keep interacting to alleviate the noisy and
long-tail problem simultaneously. Extensive experiments on widely used NYT data
set clearly show that our method significant improvements over state-of-the-art
baselines.","Relation extraction (RE) is a preliminary task in natural lan guage processing (NLP), which aims to capture the relation between two target entities. Recently, RE based on conven tional supervised learning has made a great success. How ever, it heavily relies on human annotations. In order to obtain largescale training corpus, distant su pervision relation extraction (DSRE) [Mintz et al. , 2009 ]was proposed to generate heuristic labeling data by aligning en tity pairs in raw text. It assumes that if two target entities have a semantic relation in KG, all the raw text containing the two entities can be labeled as this relation class. How ever, this solution makes an overstrong assumption and in evitably brings in massive wrong labeling data. For exam ple, as shown in Figure 1, given a fact ( Obama ,born in , US.) from existing KG, DS will regard all sentences with two linked entities Obama andUS.express the relation born in . In consequence, only the ﬁrst sentence is correct, but actually the second expresses the relation president of while the third cannot ﬁnd the predeﬁned relation. Additionally, according (Obama, born in, US.) Aligned Sentences Real Label In 1961, [Obama]h [was born in]r Hawaii, [US.]t born in [Barack Obama]h takes the Oath of Office as the 44th  [president of]r the [United State]tpresident of Last night, [Obama]h gave a radical speech at  McCormick Place, Chicago, [US.]tunknown ... ...Figure 1: The example of sentence alignment from fact ( Obama , born in ,US.) by distance supervision. It shows that only the ﬁrst is correct labeling data and others are noise. to a series works [Liet al. , 2020; Xu and Barbosa, 2019; Hanet al. , 2018b; Zhang et al. , 2019 ], DS always suffers from longtail distribution problem. We analyze that there are two main reasons: 1) existing knowledge bases are far from com pletion and they contain the overlapping problem, 2) the num ber of noisy labeling sentences in some of the relation labels is larger than correct data, which causes the semantics or data insufﬁcient. Inevitably, the ﬁrst factor relies on the quality of KG, which is ﬁxed before alignment with plain text. There fore, we only devote to ﬁnd the target solution corresponding to the second factor. By intuition, if there’re a lot of noisy sentences under a relation class, fewer highquality sentences can be sufﬁciently used to train the model, which results in the longtail. In other words, To improve the relation extrac tion, the noisy labeling and longtail problem should be con sidered simultaneously. Recently, most approaches have been presented to solve the noisy labeling problem [Hoffmann et al., 2011; Zeng et al. , 2015; Jat et al. , 2018; Ji et al. , 2017; Feng et al. , 2018; Qin et al. , 2018b; Qin et al. , 2018a; Zeng et al. , 2018 ]and longtail problems [Vashishth et al. , 2018; Liet al. , 2020; Xu and Barbosa, 2019; Han et al. , 2018b; Zhang et al. , 2019 ]. Despite the success and popularity of these methods, little works handle both two problems simul taneously, which ignore the mutual interactions. In this paper, in order to jointly solve two problems, we propose a novel framework named RHNet, which incor porates Reinforcement learning and Hierarchical relational searching module. At ﬁrst, we leverage reinforcement learnarXiv:2010.14255v2  [cs.CL]  2 Feb 2021ing to select highquality data. Concretely, given an original bag1, the agent splits its into the correct set and noisy set, and we train the downstream module only on the correct set. This idea is motivated by the previous work [Feng et al. , 2018 ], but the difference is that we enhance the agent by integrating pre trained implicit relation information. For the second problem, we regard that the semantics of datarich can be shared with similar datapoor relations. For example, the datarich rela tion /people/person/place ofbirth in NYT corpus can rep resent a fourlayers tree, from top to down are root,/peo ple,/people/person and /people/person/place ofbirth , re spectively, where root is virtual node, /people and /peo ple/person are subrelations. When given a datapoor rela tionpeople/person/religion , it can be integrated with related instances at the layer of root,/people , and /people/person . In contrast to [Han et al. , 2018b ]and[Zhang et al. , 2019 ], we view RE as a tree search task from the root to the leaf node. During the search processing, we leverage the gating mecha nism to save and combine the semantics of related instances at the current node, and calculate the score of each candidate child nodes and choose the maximum one. The two main components joint training at each iterative stage to capture the interactions. The contributions of this paper are as follows: • We are the ﬁrst to transform the relation extraction into a tree search task. We propose the hierarchical relational searching strategy to share the correlated instance se mantics at each node. • We propose a novel framework RHNet, which is ca pable of simultaneously solving the noisy labeling and longtail problem. At the iterative training stage, our method takes advantage of the mutual interactions be tween them. • Extensive experiments on the NYT data set demonstrate that if we consider both two problems, the proposed method outperforms stateoftheart baselines. 2 Related Work "
266,Viewpoint-Aware Channel-Wise Attentive Network for Vehicle Re-Identification.txt,"Vehicle re-identification (re-ID) matches images of the same vehicle across
different cameras. It is fundamentally challenging because the dramatically
different appearance caused by different viewpoints would make the framework
fail to match two vehicles of the same identity. Most existing works solved the
problem by extracting viewpoint-aware feature via spatial attention mechanism,
which, yet, usually suffers from noisy generated attention map or otherwise
requires expensive keypoint labels to improve the quality. In this work, we
propose Viewpoint-aware Channel-wise Attention Mechanism (VCAM) by observing
the attention mechanism from a different aspect. Our VCAM enables the feature
learning framework channel-wisely reweighing the importance of each feature
maps according to the ""viewpoint"" of input vehicle. Extensive experiments
validate the effectiveness of the proposed method and show that we perform
favorably against state-of-the-arts methods on the public VeRi-776 dataset and
obtain promising results on the 2020 AI City Challenge. We also conduct other
experiments to demonstrate the interpretability of how our VCAM practically
assists the learning framework.","Vehicle reidentiﬁcation (reID) aims to match images of the same vehicle captured by a camera network. Re cently, this task has drawn increasing attention because of its wide applications such as analyzing and predict ing trafﬁc ﬂow. While several existing works obtained great success with the aid of Convolutional Neural Net work (CNN) [15, 16, 24], various challenges still hinder the performance of vehicle reID. One of them is that a ve hicle captured from different viewpoints usually has dra matically different visual appearances. To reduce this intra class variation, some works [25, 11, 34] guide the feature learning framework by spatial attention mechanism to ex tract viewpointaware features on the meaningful spatial lo Figure 1: Illustration of Viewpointaware Channelwise Attention Mechanism (VCAM). In the vehicle reID task, the channelwise feature maps are essentially the detectors for speciﬁc vehicle parts, such as Rear Windshield andTires . Our VCAM enables the framework to empha size (i.e. attentive weight >0:5) the features extracted from the clearly visible vehicle parts which are usually helpful for reID matching while ignore (i.e. attentive weight < 0:5) the others which are usually meaningless. cation. However, the underlying drawback is that the ca pability of the learned network usually suffers from noisy generated spatial attention maps. Moreover, the more pow erful spatial attentive model may rely on expensive pixel level annotations, such as vehicle keypoint labels, which are 1arXiv:2010.05810v1  [cs.CV]  12 Oct 2020impractical in realworld scenario. In view of the above ob servations, we choose to explore another type of attention mechanism in our framework that is only related to high level vehicle semantics. Recently, a number of works adopt channelwise atten tion mechanism [8, 3, 26, 29] and achieve great success in several different tasks. Since a channelwise feature map is essentially a detector of the corresponding semantic at tributes, channelwise attention can be viewed as the pro cess of selecting semantic attributes which are meaningful or potentially helpful for achieving the goal. Such char acteristic could be favorable in the task of vehicle reID. Speciﬁcally, channelwise feature maps usually represent the detectors of discriminative parts of vehicle, such as rear windshield or tires. Considering that the vehicle parts are not always clearly visible in the image, with the aid of channelwise attention mechanism, the framework should therefore learn to assign larger attentive weight and, con sequently, emphasize on the channelwise feature maps ex tracted from the visible parts in the image. Nonetheless, the typical implementation of channelwise attention mech anism [8, 3] generates the attentive weight of each stage, explicitly each bottleneck, based on the representation ex tracted from that stage in the CNN backbone. We ﬁnd that the lack of semantic information in the lowlevel represen tations extracted from the former stages may result in unde sirable attentive weight, which would limit the performance in vehicle reID. As an alternative solution, in this paper, we pro pose a novel attentive mechanism, named Viewpointaware Channelwise Attention Mechanism (VCAM) , which adopts highlevel information, the “viewpoint” of captured image, to generate the attentive weight. The motivation is that the visibility of vehicle part usually depends on the viewpoint of the vehicle image. As shown in Fig. 1, with our VCAM, the framework successfully focuses on the clearly visible vehicle parts which are relatively beneﬁcial to reID match ing. Combined with VCAM, our feature learning frame work is as follows. For every given image, our framework ﬁrst estimates the viewpoint of input vehicle image. After wards, based on the viewpoint information, VCAM accord ingly generates the attentive weight of each channel of con volutional feature. ReID feature extraction module is then incorporated with the channelwise attention mechanism to ﬁnally extract viewpointaware feature for reID matching. Extensive experiments prove that our method outper forms stateofthearts on the largescale vehicle reID benchmark: VeRi776 [15, 16] and achieves promising re sults in the 2020 Nvidia AI City Challenge1, which holds competition on the other largescale benchmark, CityFlow ReID [24]. We additionally analyze the attentive weights generated by VCAM in interpretability study to explain how 1https://www.aicitychallenge.org/VCAM helps to solve reID problem in practice. We now highlight our contributions as follows: • We propose a novel framework which can beneﬁt from channelwise attention mechanism and extract viewpointaware feature for vehicle reID matching. • To the best of our knowledge, we are the ﬁrst to show that viewpointaware channelwise attention mecha nism can obtain great improvement in the vehicle reID problem. • Extensive experiments on public datasets increase the interpretability of our method and also demon strate that the proposed framework performs favorably against stateoftheart approaches. 2. Related Work "
44,Ordered or Orderless: A Revisit for Video based Person Re-Identification.txt,"Is recurrent network really necessary for learning a good visual
representation for video based person re-identification (VPRe-id)? In this
paper, we first show that the common practice of employing recurrent neural
networks (RNNs) to aggregate temporal spatial features may not be optimal.
Specifically, with a diagnostic analysis, we show that the recurrent structure
may not be effective to learn temporal dependencies than what we expected and
implicitly yields an orderless representation. Based on this observation, we
then present a simple yet surprisingly powerful approach for VPRe-id, where we
treat VPRe-id as an efficient orderless ensemble of image based person
re-identification problem. More specifically, we divide videos into individual
images and re-identify person with ensemble of image based rankers. Under the
i.i.d. assumption, we provide an error bound that sheds light upon how could we
improve VPRe-id. Our work also presents a promising way to bridge the gap
between video and image based person re-identification. Comprehensive
experimental evaluations demonstrate that the proposed solution achieves
state-of-the-art performances on multiple widely used datasets (iLIDS-VID, PRID
2011, and MARS).","PERSON reidentiﬁcation (Reid) addresses the problem of reassociation persons across disjoint camera views. In this paper, we consider more practical scenarios of video based person reidentiﬁcation ( VPReid ), in which a video of a person, as seen in one camera, must be matched against a gallery of videos captured by a different non overlapping camera. VPReid is an active research topic in computer vision due to its wideranging applications in problems, including visual surveillance and forensics. Since the pioneering work [1], several visual features [2], [3], and learning methods [4]–[6] have consistently improved the matching performance, leading the research community to address more challenging scenarios in complex datasets [4], [7], [8]. However, signiﬁcant hurdles due to variations in appearance, viewpoint, illumination, and occlusion come in the way of solving the problem. Recently, deep convolutional neural networks (Con vNets) stand at the forefront of several vision tasks, includ ing image classiﬁcation [9]–[12], segmentation [13], pose es timation [14], face recognition [15], crowd counting [16], and image based person reidentiﬁcation [17], just to mention a few. Deep learning for VPReid , however, is also witnessing L. Zhang and Z. Zeng are with the Institute for Infocomm Research, the Agency for Science, Technology and Research (A*STAR), Singapore. J. T. Zhou is with the Institute of High Performance Computing, the Agency for Science, Technology and Research (A*STAR), Singapore. Z. Shi is with the University of Amsterdam, Netherlands. J. W. Bian and C. Shen is with the School of Computer Science, The University of Adelaide, Australia. M. M. Cheng and Y Liu are with the TKLNDST, College of Computer Science, Nankai University, China. The ﬁrst two authors are the joint ﬁrst author, and Joey Tianyi Zhou is the corresponding author (Email: joey.tianyi.zhou@gmail.com). Manuscript received April 19, 2005; revised August 26, 2015. t1 t2 t3 t4 t5 t6 t7 t1 t3 t2 t5 t4 t7 t6 t2 t3 t4 t5 t6 t7 t1 True ？Fig. 1. Motivation. First two rows: existing methods adopts RNNs to model temporal dependencies for VPReid . Last two rows: human be ings can easily performs this on image sequences with a random order. such vast popularity. Typically, the prior methods [6], [18]– [23] process videos by using RNNs to temporally aggregate spatial information extracted from ConvNets. However, unlike other sequential modelling tasks [24], it is littleknown about the beneﬁts of using the RNN for mod elling temporally extracted spatial information for VPRe id. Although theoretically fascinating, we show a typical recurrent structure may be less effective to capture temporal dependencies as they assumed. To simplify our analysis, we ﬁnd that the pioneering work [18] of using RNN and its followups for VPReid leads to an orderless representation. This motivates us to reponder over the task of VPReid as illustrated in Fig. 1. According to the separable visual pathway hypothesis [25], the human visual cortex contains the ventral stream and the dorsal stream, which recognize objects and how they move, respectively. Existing methodsarXiv:1912.11236v1  [cs.CV]  24 Dec 2019IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2 [18] employing RNNs overemphasize the temporal depen dencies related to person’s motion. Unfortunately, these behavioral biometrics suffer from large intra/interclass variations, making existing RNNs difﬁcult to generalize. In this work we postulate that the appearance information from a single still image plays a more important role in re associating persons from different cameras. Considering an example in Fig. 1, human beings can easily classify whether the images in two cameras come from the same identity, even the image frames in the second camera has been manually shufﬂed to remove temporal dependencies. Declaring that orderless encoding’s beneﬁts in video analytic is noncontroversial, and indeed we are certainly not the ﬁrst to inject orderless encoding into video based tasks. Even in other tasks such as human action recognition where the temporal dependency between each video frame is considered to be vital, orderless representations [26]–[28] can still demonstrate their superiority over recurrent struc tures [29]. While we take inspiration from these works and the recent success of image based person reidentiﬁcation, we are the ﬁrst to dive deep into the effect of modelling temporal information for VPReid . More speciﬁcally, we present a simple yet surprisingly powerful approach by dividing videos into individual images and perform VPReid with an orderless ensemble of shared image based rankers. We show that, under the i.i.d. assumption, the ensemble of rankers are consistent and the error bound of VPReid decreases exponentially with the number of image frames, when each base ranker is better than a random guess. Our work opens a path towards putting more emphasis on ap pearance information for VPReid . It not only achieves state oftheart performance on multiple benchmarks (iLIDSVID, PRID 2011, and MARS) but also bridges the gap between video and image based person reidentiﬁcation. 2 R ELATED WORK "
563,DBLFace: Domain-Based Labels for NIR-VIS Heterogeneous Face Recognition.txt,"Deep learning-based domain-invariant feature learning methods are advancing
in near-infrared and visible (NIR-VIS) heterogeneous face recognition. However,
these methods are prone to overfitting due to the large intra-class variation
and the lack of NIR images for training. In this paper, we introduce
Domain-Based Label Face (DBLFace), a learning approach based on the assumption
that a subject is not represented by a single label but by a set of labels.
Each label represents images of a specific domain. In particular, a set of two
labels per subject, one for the NIR images and one for the VIS images, are used
for training a NIR-VIS face recognition model. The classification of images
into different domains reduces the intra-class variation and lessens the
negative impact of data imbalance in training. To train a network with sets of
labels, we introduce a domain-based angular margin loss and a maximum angular
loss to maintain the inter-class discrepancy and to enforce the close
relationship of labels in a set. Quantitative experiments confirm that DBLFace
significantly improves the rank-1 identification rate by 6.7% on the EDGE20
dataset and achieves state-of-the-art performance on the CASIA NIR-VIS 2.0
dataset.","NIRVIS heterogeneous face recognition refers to the problem of matching face images across the two visual domains. It has been widely adopted to various applica tions, such as video surveillance and user authentication in deﬁcient lighting conditions. With the evolution of deep learning models, a number of deep learningbased methods [10, 7, 17] are presented and achieved signiﬁcant improve ment on the popular benchmarks ( e.g., CASIA NIRVIS 2.0 [23] and OuluCASIA NIRVIS [18]). However, NIR VIS heterogeneous face recognition remains a challenging problem due to the large crossmodality gap and the lack of largescale training data with both VIS and NIR images. Datasets of VIS images ( e.g., MS1M [13] and Glint [5]) (a)  (b) Figure 1: Depiction of a set of two labels of a subject: (a) the VIS images with label 1 and (b) the NIR images with label 2. The images depicted on (a) and (b) belong to the same subject, but they are labeled as two separate classes. that contain millions of faces with various face poses and illuminations play a vital role in the success of face recog nition algorithms in the VIS domain. In comparison to these datasets, crossdomain face datasets contain a signiﬁ cantly fewer number of subjects and images. For instance, CASIA NIRVIS 2.0, one of the largest crossspectral face datasets, contains only 17,580 images of 725 subjects cap tured in a constrained environment. The amount of data from CASIA NIRVIS 2.0 is insufﬁcient for training a face recognition system that can accurately identify images in both visual domains. Several deep learningbased domain invariant feature learning methods [19, 15, 17] have been proposed to overcome the high intraclass variation by train ing on a largescale dataset of VIS images and ﬁnetuning on a smallscale dataset of both VIS and NIR images. Al though training or ﬁnetuning on a smallscale dataset alle viates the gap between the source and the target domains,arXiv:2010.03771v1  [cs.CV]  8 Oct 2020it also reduces the generalization capability of the trained models. The lack of generalization leads to poor perfor mance on new data with much difference to the training data. To avoid ﬁnetuning on a smallscale dataset, we pro pose to train a face recognition model on a joint dataset of a largescale dataset of VIS images and a smallscale dataset of both VIS and NIR images. The number of NIR images in the joint dataset is signiﬁcantly fewer than the number of VIS images. Therefore, the NIR images become outliers in their classes. To enhance the contribution of NIR images to the training process, we assume that a subject is not repre sented by a single label but by a set of labels. Each label represents images of a speciﬁc visual domain. In particular, a set of two labels per subject, one for the NIR images and one for the VIS images, are used for training a NIRVIS face recognition model. Based on this assumption, NIR im ages can contribute to the learning of their class representa tions without being dominated by VIS images. In addition, the classiﬁcation of images of each subject into two differ ent labels reduces the intraclass variation since each class contains images of only one visual domain. Inspired by the additive angular margin loss (ArcFace) [6], we introduce a domainbased angular margin loss (DAML) to maintain the interclass discrepancy. Based on our assumption, the relationship between classes is not equal. It is redundant to enforce the discrepancy between classes that correspond to the same subject. Therefore, the DAML is designed to enforce the margin between classes of different subjects only. In addition, the classes that cor respond to the same subject should be close to each other in the representation space. Therefore, we introduce a max imum angular loss (MAL) to minimize the angle between class representations that correspond to the same subject. The geometrical interpretation of our DAML and MAL is depicted in Fig. 2. The key contributions of this work are as follows: • We introduce DBLFace, a learning approach based on the assumption that a subject is represented by a set of two labels, one for the VIS images and one for the NIR images. The classiﬁcation of images reduces the intra class variation and lessens the negative impact of data imbalance in training (Section 1). • We propose a DAML to enforce the angular margin between classes of different subjects (Section 3.1). • We propose a MAL that controls the close relationship of class representations of the same subject. The MAL allows class representations of the same subject to be close in the representation space (Section 3.2).2. Related Work "
360,Deep Miner: A Deep and Multi-branch Network which Mines Rich and Diverse Features for Person Re-identification.txt,"Most recent person re-identification approaches are based on the use of deep
convolutional neural networks (CNNs). These networks, although effective in
multiple tasks such as classification or object detection, tend to focus on the
most discriminative part of an object rather than retrieving all its relevant
features. This behavior penalizes the performance of a CNN for the
re-identification task, since it should identify diverse and fine grained
features. It is then essential to make the network learn a wide variety of
finer characteristics in order to make the re-identification process of people
effective and robust to finer changes. In this article, we introduce Deep
Miner, a method that allows CNNs to ""mine"" richer and more diverse features
about people for their re-identification. Deep Miner is specifically composed
of three types of branches: a Global branch (G-branch), a Local branch
(L-branch) and an Input-Erased branch (IE-branch). G-branch corresponds to the
initial backbone which predicts global characteristics, while L-branch
retrieves part level resolution features. The IE-branch for its part, receives
partially suppressed feature maps as input thereby allowing the network to
""mine"" new features (those ignored by G-branch) as output. For this special
purpose, a dedicated suppression procedure for identifying and removing
features within a given CNN is introduced. This suppression procedure has the
major benefit of being simple, while it produces a model that significantly
outperforms state-of-the-art (SOTA) re-identification methods. Specifically, we
conduct experiments on four standard person re-identification benchmarks and
witness an absolute performance gain up to 6.5% mAP compared to SOTA.","In recent years, person reidentiﬁcation (ReID) has at tracted major interest due to its important role in various computer vision applications: video surveillance, human authentication, humanmachine interaction etc. The main 𝐴𝐴𝐴𝐴𝐴𝐴 𝐴𝐴𝐴𝐴𝐴𝐴 Mined  Features 𝑓𝑓𝑒𝑒2 Global  Features 𝑓𝑓𝑔𝑔 Erasing Operator𝑌𝑌1𝐴𝐴𝐴𝐴𝐴𝐴 𝑌𝑌2𝐴𝐴𝐴𝐴𝐴𝐴𝑌𝑌2𝑌𝑌3 𝐴𝐴𝐴𝐴𝐴𝐴𝑌𝑌3𝑌𝑌4𝑌𝑌3𝑒𝑒2 𝑌𝑌4𝑒𝑒2 𝑌𝑌2𝑒𝑒1𝑌𝑌3𝑒𝑒1𝐴𝐴𝐴𝐴𝐴𝐴(𝑌𝑌3𝑒𝑒1)𝑌𝑌4𝑒𝑒1𝑌𝑌4𝑙𝑙Local  Features 𝑓𝑓𝑙𝑙 Mined  Features 𝑓𝑓𝑒𝑒1 𝐴𝐴𝐴𝐴𝐴𝐴AttentionModule Figure 1: Deep Miner Model Architecture. Given a stan dard CNN backbone, several branches are created to en rich and diversify the features for the purpose of person re identiﬁcation. The proposed Deep Miner model is made of three types of branches: (i)The main branch G (in or ange ) is the original backbone and predicts the standard global features fg;(ii)Several InputErased (IE) branches (ingreen ) that takes as input erased feature maps and pre dictmined features fe1andfe2;(iii)The local branch (in blue) that outputs local features fland in which a uniform partition strategy is employed for part level feature reso lution as proposed by [38]. In the global branch, and the bottom IE Branch, attention modules are used in order to improve their feature representation. objective of person ReID is to determine whether a given person has already appeared over a network of cameras, which technically implies a robust modelling of the global appearance of individuals. The ReID task is particularly challenging because of signiﬁcant appearance changes – of ten caused by variations in the background, the lightening conditions, the body pose and the subject orientation w.r.t. the recording camera. In order to overcome these issues, one of the main goals of person ReID models is to produce rich representations of any input image for person matching. Notably, CNNs are known to be robust to appearance changes and spatial loca tion variations, as their global features are invariant to such 1arXiv:2102.09321v1  [cs.CV]  18 Feb 2021Input  Global/tildelow IE 1  IE 2  Local Figure 2: Feature visualization for three examples. Warmer color denotes higher value. The global branch (second column) focuses only on some features but ignores other important ones. Thanks to the Input Erased branches (third and fourth columns), Deep Miner discovers new im portant features (localized by the black boxes). For in stance, in the ﬁrst row, the IEbranches are more attentive to the person pant. In the second row, they discover some patterns on the coat and get attentive to its cap. In the third row, they ﬁnd out the plastic bag. The local branch (last column) helps the network to focus on local features such as the shoes of the subject or the object handled by the sub ject in the second row. transformations. Nevertheless, the aforementioned global features are prone to ignore detailed and potentially relevant information for identifying speciﬁc person representations. To enforce the learning of detailed features, attention mech anisms and aggregating global partbased representations were introduced in the literature, yielding very promising results [5, 7, 31]. Speciﬁcally, attention mechanisms allow to reduce the inﬂuence of background noise and to focus on relevant features, while partbased models divide feature maps into spatial horizontal parts thereby allowing the net work to focus on ﬁnegrained and local features. Despite their observed effectiveness in various tasks, these approaches do not provide ways to enrich and diver sify an individual’s representation. In fact, deep learning models are shown to exhibit a biased learning behavior [4, 3, 32]; in the sense that they retrieve sufﬁciently partial attributes concepts which contribute to reduce the training loss over the seen classes, rather than learning allsideddetails and concepts. Basically, deep networks tend to focus on surface statistical regularities rather than more general abstract concepts. This behavior is problematic in the context of reidentiﬁcation, since the network is required to provide the richest and most diverse possible representations. In this paper, we propose to address this problem by adding Input Erased Branches (IEBranch) into a standard backbone. Precisely, an IEBranch takes partially removed feature maps as input in the aim of producing (that is “mining”) more diversiﬁed features as output (as depicted in Figure 2). In particular, the removed regions correspond quite intuitively to areas where the network has strong activations and are determined by a simple suppression operation (see subsection 3.2). The proposed Deep Miner model is therefore made as the combination of IEbranches with local and global branches. The multibranch architec ture of Deep Miner is depicted on Figure 1. The main contributions brought by this work may be summarized in the following items: (i)We provide a multi branch model allowing the mining of rich and diverse fea tures for people reidentiﬁcation. The proposed model in cludes three types of branches: a Global branch (Gbranch), a Local branch (Lbranch) and an InputErased Branch (IE Branch); the latter being responsible of mining extra fea tures; (ii)IEBranches are constructed by adding an erase operation on the global branch feature maps, thereby allow ing the network to discover new relevant features; (iii)Ex tensive experiments were conducted on Market1501 [44], DukeMTMCReID [24], CUHK03 [17] and MSMT17[37]. We demonstrate that our model signiﬁcantly outperforms the existing SOTA methods on all benchmarks. 2. Related Work "
462,Distilling Effective Supervision from Severe Label Noise.txt,"Collecting large-scale data with clean labels for supervised training of
neural networks is practically challenging. Although noisy labels are usually
cheap to acquire, existing methods suffer a lot from label noise. This paper
targets at the challenge of robust training at high label noise regimes. The
key insight to achieve this goal is to wisely leverage a small trusted set to
estimate exemplar weights and pseudo labels for noisy data in order to reuse
them for supervised training. We present a holistic framework to train deep
neural networks in a way that is highly invulnerable to label noise. Our method
sets the new state of the art on various types of label noise and achieves
excellent performance on large-scale datasets with real-world label noise. For
instance, on CIFAR100 with a $40\%$ uniform noise ratio and only 10 trusted
labeled data per class, our method achieves $80.2{\pm}0.3\%$ classification
accuracy, where the error rate is only $1.4\%$ higher than a neural network
trained without label noise. Moreover, increasing the noise ratio to $80\%$,
our method still maintains a high accuracy of $75.5{\pm}0.2\%$, compared to the
previous best accuracy $48.2\%$.
  Source code available:
https://github.com/google-research/google-research/tree/master/ieg","Training deep neural networks usually requires large scale labeled data. However, the process of data labeling by humans is challenging and expensive in practice, espe cially in domains where expert annotators are needed such as medical imaging. Noisy labels are much cheaper to ac quire (e.g., by crowdsourcing, web search, etc.). Thus, a great number of methods have been proposed to improve neural network training from datasets with noisy labels to take advantage of the cheap labeling practices [48]. How ever, deep neural networks have high capacity for memo rization. When noisy labels become prominent, deep neural networks inevitably overﬁt noisy labeled data [46, 37]. To overcome this problem, we argue that building the dataset wisely is necessary. Most methods consider the set ting where the entire training dataset is acquired with the 1Source code available: https://github.com/ googleresearch/googleresearch/tree/master/ieg 0.0 0.2 0.4 0.6 0.8 1.0 Noise ratio55606570758085Accuracy (%) Fullysupervised Semisupervised (1000 labels) Noiserobust (Prev. best) Noiserobust (Ours)Ratio 0.85 0.9 0.93 0.95 0.96 0.98 0.99 mean 74.7 70.9 68.8 64.8 62.6 58.4 54.4 Figure 1: Image classiﬁcation results on CIFAR100. Fully supervised denotes a model trained with all data without label noise. Noiserobust (prev. best) denotes the previ ous best results for noisy labels (50 trusted data per class are used by this method). 10 trusted data per class are available forSemisupervised andNoiserobust (ours) . The bot tom table provides the accuracy of settings over 80% noise ratios. Semisupervised is our improved version of Mix Match [4]. Our method outperforms Semisupervised at up to a 95% noise ratio. The bottom table shows mean ac curacy of three runs. See Section 5.4 for more details. same labeling quality. However, it is often practically fea sible to construct a small dataset with humanveriﬁed la bels, in addition to a largescale noisy training dataset. If the methods based on this setting can demonstrate high ro bustness to noisy labels, new horizons can be opened in data labeling practices [21, 42]. There are a few recent methods that demonstrate good performance by leverag ing a small trusted dataset while training on a large noisy dataset, including learning weights of training data [17, 33], loss correction [16], and knowledge graph [25]. However, these methods either require a substantially large trusted set or become ineffective at high noise regimes. In contrast, our method maintains superior performance with remark 1arXiv:1910.00701v5  [cs.LG]  12 Jun 2020ably smaller size of the trusted set (e.g., the previous best method [17] uses up to 10% of the total training data while our method achieves superior results with as low as 0.2%). Given a small trusted dataset and large noisy dataset, there are two common machine learning approaches to train neural networks. The ﬁrst is noiserobust training, which needs to handle label noise effects as well as distill correct supervision from the large noisy dataset. Considering the possible harmful effects from label noise, the second ap proach is semisupervised learning, which discards noisy labels and treats the noisy dataset as a largescale unlabeled dataset. In Figure 1, we compare methods of the two direc tions under such setting. We can observe that the advanced noiserobust method is inferior to semisupervised meth ods even with a 50% noise ratio (i.e., they cannot utilize the many correct labels from the other data), motivating the necessity for further investigation of noiserobust training. This also raises a practically interesting question: Should we discard noisy labels and opt in semisupervised training at high noise regimes for model deployment? Contributions: In response to this question, we propose a highly effective method for noiserobust training. Our method wisely takes advantage of a small trusted dataset to optimize exemplar weights and labels of mislabeled data in order to distill effective supervision from them for su pervised training. To this end, we generalize a meta re weighting framework and propose a new meta relabeling extension, which incorporates conventional pseudo labeling into meta optimization. We further utilize the probe data as anchors to reconstruct the entire noisy dataset using learned data weights and labels and thereby perform supervised training. Comprehensive experiments show that even with extremely noisy labels, our method demonstrates greatly su perior robustness compared to previous methods (Figure 1). Furthermore, our method is designed to be modelagnostic and generalizable to a variety of label noise types as val idated in experiments. Our method sets new state of the art on CIFAR10 and CIFAR100 by a signiﬁcant margin and achieves excellent performance on the largescale WebVi sion, Clothing1M, and Food101N datasets with realworld label noise. 2. Related Work "
273,Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling.txt,"Social media has effectively become the prime hub of communication and
digital marketing. As these platforms enable the free manifestation of thoughts
and facts in text, images and video, there is an extensive need to screen them
to protect individuals and groups from offensive content targeted at them. Our
work intends to classify codemixed social media comments/posts in the Dravidian
languages of Tamil, Kannada, and Malayalam. We intend to improve offensive
language identification by generating pseudo-labels on the dataset. A custom
dataset is constructed by transliterating all the code-mixed texts into the
respective Dravidian language, either Kannada, Malayalam, or Tamil and then
generating pseudo-labels for the transliterated dataset. The two datasets are
combined using the generated pseudo-labels to create a custom dataset called
CMTRA. As Dravidian languages are under-resourced, our approach increases the
amount of training data for the language models. We fine-tune several recent
pretrained language models on the newly constructed dataset. We extract the
pretrained language embeddings and pass them onto recurrent neural networks. We
observe that fine-tuning ULMFiT on the custom dataset yields the best results
on the code-mixed test sets of all three languages. Our approach yields the
best results among the benchmarked models on Tamil-English, achieving a
weighted F1-Score of 0.7934 while scoring competitive weighted F1-Scores of
0.9624 and 0.7306 on the code-mixed test sets of Malayalam-English and
Kannada-English, respectively.","Socialmediahasbecomeapopularcontrivanceofcommunicationinthe21stcenturyandisthe“democratisationof information” by converting people into publishers from the conventional readers (Nasir Ansari et al., 2018). 53.6% of the world’s population use social media (Chaﬀey, 2021) which comprises a vivid structure between users from various backgrounds (Kapoor et al., 2018). With its free expressing environment, it witnesses much content, including images, videosandcommentsfromvariousagegroupsbelongingtodiverseregions,languagesandinterests. Whilethebasic ideaofsocialmediaremainstobecommunicationandentertainment,usersareseenusingrudeanddefamatorylanguage to express their views. Users might not appreciate such comments or posts and might be inﬂuential on teenagers. Oﬀensive posts targeted on a group or an individual can lead to frustration, depression and distress (Kawate and Patil, 1https://github.com/adeepH/DravidianOLI <Equal Contribution < <Corresponding Author adeeph18c@iiitt.ac.in (A. Hande); karthikp18c@iiitt.ac.in (K. Puranik); konthalay18c@iiitt.ac.in (K. Yasaswini); rubapriyadharshini.a@gmail.com (R. Priyadharshini); sajeethas@esn.ac.lk (S. Thavareesan); anbu.1318@gmail.com (A. Sampath); kogilavani.sv@gmail.com (K. Shanmugavadivel); theni_d@ssn.edu.in (D. Thenmozhi); bharathi.raja@insightcentre.org (B.R. Chakravarthi) ORCID(s):0000000220034836 (A. Hande); 0000000255362258 (K. Puranik); 0000000158454759 (K. Yasaswini); 0000000323231701 (R. Priyadharshini); 0000000262525393 (S. Thavareesan); 0000000302268150 (A. Sampath); 000000020715143X (K. Shanmugavadivel); 0000000306816628 (D. Thenmozhi); 0000000245757934 (B.R. Chakravarthi) Hande et al.: Preprint submitted to Elsevier Page 1 of 27arXiv:2108.12177v1  [cs.CL]  27 Aug 2021Oﬀensive Language Identiﬁcation 2017; Puranik et al., 2021). Researchers recognised the need to detect and remove oﬀensive content from social media platformsfor along period. However,there werea fewchallengesfaced inthis ﬁeld. Thoughautomating thisprocess with the help of supervised machine learning models gave better accuracy than human moderators (Zampieri et al., 2020),thelatterwerepreferredastheycouldjustifytheirdecisioninremovingthecomment/postfromtheplatform (Rischetal.,2020). Secondly,mostofthecommentsandpostsmadewereincodemixedunderresourcedlanguages (Chakravarthi et al., 2020a). There was an absence of enough datasets and tools to produce stateoftheart results to be implementedintheseplatforms. OurpaperpresentssomeuniqueapproachestogiveexcellentF1scoresforcodemixed Dravidian languages, mainly Tamil, Malayalam, and Kannada. Social mediacreates awhole newopportunity in theﬁeld ofresearch. NonEnglish speakerstend touse phonetic typing,Romanscripts,transliteration,codemixingandmixingseverallanguagesinsteadofUnicode1. Codemixed sentences for Dravidian languages can be Intersentential which consists of pure Dravidian languages written in Latin script, Codeswitching at a morphological level when it is written in both Latin and the Dravidian language and an IntrasententialmixofEnglishandtheDravidianlanguagewritteninLatinscript(Yasaswinietal.,2021). Theforemost stepinanalysingcodemixedorcodeswitcheddataincludeslanguagetagging,which,ifnotaccurate,canaﬀectthe results of other tasks. Language tagging has evolved over the years but is not yet satisfactory for analysing codemixed data(MandalandSingh,2018). Thepastyearshaveseenenormousencouragementandresearchincodemixingof underresourcedlanguagesduetooverﬁtting. Oneofthemainissuesofdealingwithcodemixedlanguagesarethe lackofannotateddatasetsandlanguagesmodelsbeingpretrainedoncodemixedtexts. Thelackofcodemixeddata resultedinconstriction ofdatacrisis,which aﬀectedtheperformanceofvarious tasks. Transliterationofcodemixed data can increase the size of the input dataset. Transliteration refers to converting a word from one language to another while protecting the semantic meaning of the utterance and obeying the syntactic structure in the target language. The pronunciation of the source word is maintained as much as possible. While trying to get the critical features from a text or translating from one language to another, some language pairs like English/Spanish might not encounter any issue as Porfavor iswrittenas Porfavor inEnglishastheysharethesameLatinscript. However,performingsuchtaskson Dravidianlanguagesmightposeproblems, andtransliterationcansolvethemtoanextent(KnightandGraehl,1997). Supervised learning on small datasets or languages with limited resources can be complex. Thus, pseudolabeling (Lee, 2013) can be employed to increase the performance considerably. In pseudolabeling, the model is trained on labeled data to predict labels for a batch of unlabeled data. The predictions are then fed into the model as pseudo labelled data. 1.1. Research Questions In this paper, we attempt to address the following research questions: 1.What architectures can be employed for eﬀective crosslingual knowledge transfer among codemixed languages for oﬀensive language identiﬁcation? We evaluate several recent approaches for oﬀensive language identiﬁcation, primarily focusing on crosslingual transfer due to the persistence of codemixed instances in the dataset. We have also used several stateoftheart pretrained multilingual language models for three languages, Kannada, Malayalam, and Tamil. 2.How do we break the curse of the lack of data for underresourced Dravidian languages? To overcome the barrier of lack of data, we revisit pseudolabeling. We transliterate the dataset to the respective Dravidian language for our multilingual dataset and generate labels by using approaches such as pseudolabeling. We combine the two datasets to form a larger dataset. 1.2. Contribution 1.Weproposeanapproachtoimproveoﬀensivelanguageidentiﬁcationbyemphasisingmoreonconstructinga biggerdataset,generatingthepseudolabelsonthetransliterateddataset,andcombiningthelatterwiththeformer to have extensive amounts of data for training. 2.We experiment withmultilingual languages models separately on theprimarydataset, the transliterated dataset, and the newly constructed combination of both the datasets to examine if an increase in training data would improve the overall performance of the language models. We observe that this approach yields the bestweighted F1Scores on all three languages concerning its counterparts. 3.Wehaveshownthatourmethodworksforthreeunderresourcedlanguages,namelyKannada,Malayalamand Tamil, in a codemixed setting. We also have compared our approaches to all other models that have been benchmarked on the datasets. 1https://amitavadas.com/CodeMixing.html Hande et al.: Preprint submitted to Elsevier Page 2 of 27Oﬀensive Language Identiﬁcation The rest of our work is organised as follows. Section 2 talks about the related work on oﬀensive language identiﬁcation, while Section 3 entails a discussion about the Dravidian languages and their histories. Section 4 introduces the dataset usedforthetaskathand. Section5discussestheseveralmodelsandapproachestotesttheirﬁdelityontheoriginalcode mixeddataset,pseudolabelsprocuredforthetransliterateddataandthecombinationoftheboth. Section6comprisesa detailedanalysisconcerningthebehaviourandresultsofthepretrainedmodelswhenﬁnetunedoncodemixedand transliterated data, and the results are compared with other approaches (Chakravarthi et al., 2021a) from the shared task conducted by DravidianLangTech20212at EACL 2021. We also perform the error analysis on the Kannada and Tamil predictions. Finally,Section7concludesourworkandtalksaboutpotentialdirectionsforfutureworkonOﬀensive Language Identiﬁcation in Dravidian languages. 2. Related Work "
388,Learning Boolean Circuits with Neural Networks.txt,"While on some natural distributions, neural-networks are trained efficiently
using gradient-based algorithms, it is known that learning them is
computationally hard in the worst-case. To separate hard from easy to learn
distributions, we observe the property of local correlation: correlation
between local patterns of the input and the target label. We focus on learning
deep neural-networks using a gradient-based algorithm, when the target function
is a tree-structured Boolean circuit. We show that in this case, the existence
of correlation between the gates of the circuit and the target label determines
whether the optimization succeeds or fails. Using this result, we show that
neural-networks can learn the (log n)-parity problem for most product
distributions. These results hint that local correlation may play an important
role in separating easy/hard to learn distributions. We also obtain a novel
depth separation result, in which we show that a shallow network cannot express
some functions, while there exists an efficient gradient-based algorithm that
can learn the very same functions using a deep network. The negative
expressivity result for shallow networks is obtained by a reduction from
results in communication complexity, that may be of independent interest.","It is well known (e.g. Livni et al. (2014)) that while deep neuralnetworks can express any function that can be run efﬁciently on a computer, in the general case, learning neuralnetworks is compu tationally hard. Despite this theoretic pessimism, in practice, deep neural networks are successfully trained on real world datasets. Bridging this theoreticalpractical gap seems to be the holy grail of theoretical machine learning nowadays. Maybe the most natural direction to bridge this gap is to ﬁnd a property of data distributions that determines whether learning them is computationally easy or hard. The goal of this paper is to propose such a property. To motivate this, we ﬁrst recall the kparity problem: the input is nbits, there is a subset of k relevant bits (which are unknown to the learner), and the output should be 1if the number of"
136,Co-learning: Learning from Noisy Labels with Self-supervision.txt,"Noisy labels, resulting from mistakes in manual labeling or webly data
collecting for supervised learning, can cause neural networks to overfit the
misleading information and degrade the generalization performance.
Self-supervised learning works in the absence of labels and thus eliminates the
negative impact of noisy labels. Motivated by co-training with both supervised
learning view and self-supervised learning view, we propose a simple yet
effective method called Co-learning for learning with noisy labels. Co-learning
performs supervised learning and self-supervised learning in a cooperative way.
The constraints of intrinsic similarity with the self-supervised module and the
structural similarity with the noisily-supervised module are imposed on a
shared common feature encoder to regularize the network to maximize the
agreement between the two constraints. Co-learning is compared with peer
methods on corrupted data from benchmark datasets fairly, and extensive results
are provided which demonstrate that Co-learning is superior to many
state-of-the-art approaches.","The success of deep learning relies on carefully labeled data in largescale. However, precise annotations are extremely expensive and timeconsuming. To alleviate the problem, inexpensive alter natives are often used, such as web crawling [ 35] or completing ∗Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ’21, October 20–24, 2021, Virtual Event, China ©2021 Association for Computing Machinery. ACM ISBN 9781450386517/21/10. . . $15.00 https://doi.org/10.1145/3474085.3475622annotations with crowdsourcing and online queries. Unfortunately, these alternative methods can often leads to noisy labels. However, deep neural networks can easily overfit to noisy labels, as shown in recent research [ 18,22,34,40], and this can dramatically degrade the generalization performance. Minibatch 1Minibatch 2Minibatch 3DecouplingCoteachingCoteaching+JoCoRColearning!=!=!=ABBABAABABABABABABABABAB!=!=!=ABABAB Figure 1: Comparison of leading methods in dealing with noisy labels. All the methods use two cooperative modules (A & B). As the training goes, the two become more and more in agreement with each other, but in different ways. Decoupling updates the parameters of the two networks with predictiondisagreed (!=) samples from a minibatch. Coteaching [11] uses smallloss samples of one network to teach its peer network for further training. Coteaching+ [39] first predicts each minibatch with the two networks but uses disagreed (!=) samples only to compute the train ing loss. JoCoR [37] trains the two networks as a whole with a joint loss of weighted goals: making the two predictions agree with each other, and making the predictions stick to groundtrue labels as far as possible. Colearning trains a single shared encoder network with two heads (the self supervised and the noisilysupervised) that constrain each other and maximizes the agreement between them in latent space. Several studies have been conducted to investigate learning with noisy labels, where semisupervised learning frameworks play cru cial roles [ 1,8,17,25,38]. Most of these deploy unsupervised learn ing to obtain information about labelindependent features and then do further training using noisy labels in a supervised learning manner. These methods commonly leverage labels through two views: (1) supervised learning taking advantage of direct supervi sion with labels and (2) unsupervised learning exploiting intrinsic features from data to combat label noise. The core challenge here is how to combine the two views effectively. Arazo et al. [ 1] design a beta mixture model as an unsupervised generative model of sample loss values and then adopt MixUp [ 41] augmentation to assure reli able convergence under extreme noise levels. Li et al. [ 17] models sample loss with Gaussian mixtures to divide training data into unlabeled data and labeled data, and then apply the stateoftheartarXiv:2108.04063v4  [cs.LG]  30 Oct 2021semisupervised learning method MixMatch [ 2] to deal with the divided sets. However, the above methods are not endtoend and the stepbystep learning manner increases complexity in both time and space. Cotraining and model ensembling have been shown to be ben eficial in learning with noisy labels [ 11,23,37,39]. Decoupling [23] trains two networks simultaneously and updates them using instances with different predictions. Coteaching [ 11] also trains two networks simultaneously and selects smallloss data to teach the peer network during the training process. Coteaching+ [ 39] follows a similar scheme as Coteaching but with a scheme which selects smallloss data from disagreement data. JoCoR [ 37] main tains two networks but trains them as a whole with a joint loss to make predictions of them get closer. These methods are based on the assumption that two networks can provide two different views of the data, but the extra information gain they bring is very limited since the differences between two networks of the same architecture mainly come from random initialization. A compar ison between Colearning and other cotraininglike methods is illustrated in Figure 1. Motivated by semisupervised learning and cotraining, we pro pose a simple yet effective learning paradigm called "" Colearning "" to combat the problem with noisy labels, in which selfsupervised learning is introduced as a featuredependent view to assist su pervised learning. Different from those cotraininglike methods, Colearning has a shared feature encoder with two exclusive heads. While the projection head performs a selfsupervised learning and exploits featuredependent information through the intrinsic sim ilarity, the classifier head performs a vanilla supervised learning and learns from labeldependent information. In addition, a struc tural similarity loss is utilized between the two exclusive heads to regularize the classifier and avoid bias caused by noisy labels. This is based on the constraint that output from the projection head and that from the classifier head should share similar structure pairwise. Unlike the other methods, Colearning can be implemented conve niently without the need for such prior knowledge as noise rates, data distributions, and additional clean samples, thus avoiding the needed hyperparameters thereof. We conducted experiments on simulated and realworld noisy datasets, including CIFAR10, CIFAR100, Animal10N [ 32], and Food101N [ 16] datasets. Among these datasets, Animal10N is a noisy dataset that contains confusing images for manual annota tions, and Food101N is a webly dataset that directly collects data from the Web. Comparative results demonstrate that Colearning is superior to stateoftheart methods and robust to highlevel noise in labels. 2 RELATED WORK "
405,CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images.txt,"We present a simple yet efficient approach capable of training deep neural
networks on large-scale weakly-supervised web images, which are crawled raw
from the Internet by using text queries, without any human annotation. We
develop a principled learning strategy by leveraging curriculum learning, with
the goal of handling a massive amount of noisy labels and data imbalance
effectively. We design a new learning curriculum by measuring the complexity of
data using its distribution density in a feature space, and rank the complexity
in an unsupervised manner. This allows for an efficient implementation of
curriculum learning on large-scale web images, resulting in a high-performance
CNN model, where the negative impact of noisy labels is reduced substantially.
Importantly, we show by experiments that those images with highly noisy labels
can surprisingly improve the generalization capability of the model, by serving
as a manner of regularization. Our approaches obtain state-of-the-art
performance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101.
With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on
the WebVision challenge for 1000-category classification. This result was the
top performance by a wide margin, outperforming second place by a nearly 50%
relative error rate. Code and models are available at:
https://github.com/MalongTech/CurriculumNet .","Deep convolutional networks have rapidly advanced numerous computer vision tasks, providing stateoftheart performance on image classication [9,31,34,14,37,8], object detection [28,27,22,20], sematic segmentation [23,11,4,10], etc. They pro duce strong visual features by training the networks in a fullysupervised manner using largescale manually annotated datasets, such as ImageNet [5], MSCOCO [21] and PASCAL VOC [6]. Full and clean human annotations are of crucial ?Weilin Huang is the corresponding author (email:whuang@malong.com).arXiv:1808.01097v4  [cs.CV]  18 Oct 20182 S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R. Scott, D. Huang Fig. 1. Image samples of the WebVision dataset [19] from the categories of Carton , Dog,Taxi andBanana . The dataset was collected from the Internet by using text queries generated from the 1, 000 semantic concepts of the ImageNet benchmark [5]. Each category includes a number of mislabeled images as shown on the right. importance to achieving a highperformance model, and better results can be reasonably expected if a larger dataset is provided with noisefree annotations. However, obtaining massive and clean annotations are extremely expensive and timeconsuming, rendering the capability of deep models unscalable to the size of collected data. Furthermore, it is particularly hard to collect clean annotations for tasks where expert knowledge is required, and labels provided by dierent annotators are possibly inconsistent. An alternative solution is to use the web as a source of data and supervision, where a large amount of web images can be collected automatically from the Internet by using input queries, such as text information. These queries can be considered as natural annotations of the images, providing weak supervision of the collected data, which is a cheap way to increase the scale of the dataset nearinnitely. However, such annotations are highly unreliable, and often in clude a massive amount of noisy labels. Past work has shown that these noisy labels could signicantly aect the performance of deep neural networks on im age classication [39]. To address this problem, recent approaches have been developed by proposing robust algorithms against noisy labels [30]. Another so lution is to develop noisecleaning methods that aim to remove or correct the mislabelled examples in training data [32]. However, the noisecleansing meth ods often suer from the main diculty in distinguishing mislabeled samples from hard samples, which are critical to improving model capability. Besides, semisupervised methods have also been introduced by using a small subset of manuallylabeled images, and then the models trained on this subset are gen eralized to a larger dataset with unlabelled or weaklylabelled data [36]. UnlikeCurriculumNet: Weakly Supervised Learning from LargeScale Web Images 3 these approaches, we do not aim to propose a noisecleaning, noiserobust or semisupervised algorithm. Instead, we investigate improving model capability of standard neural networks by introducing a new training strategy. In this work, we study the problem of learning convolutional networks from largescale images with a massive amount of noisy labels, such as the WebVision challenge [18], which is a 1000category image classication task having the same categories as ImageNet [5]. The labels are provided by simply using the queries text generated from the 1,000 semantic concepts of ImageNet [5], without any manual annotation . Several image samples are presented in Fig. 1. Our goal is to provide a solution able to handle massive noisy labels and data imbalance eectively. We design a series of experiments to investigate the impact of noisy labels on the performance of deep networks, when the amount of training im ages is suciently large. We develop a simple but surprisingly ecient training strategy that allows for improving model generalization and overall capability of the standard deep networks, by leveraging highly noisy labels. We observe that training a CNN from scratch using both clean and noisy data is more eective than just using the clean one. The contributions of this work are threefold: { We propose CurriculumNet by developing an ecient learning strategy with curriculum learning. This allows us to train highperformance CNN models from largescale web images with massive noisy labels, which are obtained without any human annotation. { We design a new learning curriculum by ranking data complexity using dis tribution density in an unsupervised manner. This allows for an ecient implementation of curriculum learning tailored for this task, by directly ex ploring highly noisy labels. { We conduct extensive experiments on a number of benchmarks, including WebVision [19], ImageNet [5], Clothing1M [39] and Food101 [2], where the proposed CurriculumNet obtains stateoftheart performance. The Curricu lumNet, with an ensemble of multiple models, archived the top performance with a top5 error rate of 5.2%, on the WebVision Challenge at CVPR 2017, outperforming the other results by a large margin. 2 Related work "
252,Model-based 3D Hand Reconstruction via Self-Supervised Learning.txt,"Reconstructing a 3D hand from a single-view RGB image is challenging due to
various hand configurations and depth ambiguity. To reliably reconstruct a 3D
hand from a monocular image, most state-of-the-art methods heavily rely on 3D
annotations at the training stage, but obtaining 3D annotations is expensive.
To alleviate reliance on labeled training data, we propose S2HAND, a
self-supervised 3D hand reconstruction network that can jointly estimate pose,
shape, texture, and the camera viewpoint. Specifically, we obtain geometric
cues from the input image through easily accessible 2D detected keypoints. To
learn an accurate hand reconstruction model from these noisy geometric cues, we
utilize the consistency between 2D and 3D representations and propose a set of
novel losses to rationalize outputs of the neural network. For the first time,
we demonstrate the feasibility of training an accurate 3D hand reconstruction
network without relying on manual annotations. Our experiments show that the
proposed method achieves comparable performance with recent fully-supervised
methods while using fewer supervision data.","Reconstructing 3D human hands from a single image is important for computer vision tasks such as handrelated action recognition, augmented reality, sign language trans lation, and humancomputer interaction [21, 33, 43]. How ever, due to the diversity of hands and the depth ambiguity in monocular 3D reconstruction, imagebased 3D hand re construction remains a challenging problem. In recent years, we have witnessed fast progress in re covering 3D representations of human hands from images. In this ﬁeld, most methods were proposed to predict 3D hand pose from the depth image [1, 10, 15, 22, 49] or the *Work done during an internship at Tencent AI Lab. †Corresponding author: tuzhigang@whu.edu.cn Training Phase Testing Phase Input Image Output Joints 3D ReconstructionSingle View Images Detected 2D Keypoints Figure 1: Given a collection of unlabeled hand images, we learn a 3D hand reconstruction network in a selfsupervised manner. Top: the training uses a collection of unlabeled hand images and their corresponding noisy detected 2D keypoints. Bottom: our model outputs accurate hand joints and shapes, as well as vivid textures. RGB image [2, 8, 24, 37, 52]. However, the surface infor mation is needed in some applications such as grasping an object by a virtual hand [21], where the 3D hand pose rep resented by sparse joints is not sufﬁcient. To better display the surface information of the hand, previous studies pre dict the triangle mesh either via regressing pervertex coor dinate [16, 29] or by deforming a parametric hand model [19, 20]. Outputting such highdimensional representation from 2D input is challenging for neural networks to learn, thus resulting in the training process relying heavily on 3D annotations such as dense hand scans, modelﬁtted paramet ric hand mesh, or humanannotated 3D joints. Besides, the hand texture is important in some applications, such as vivid hands reconstruction in immersive virtual reality. But only recently has a study exploring parametric texture estimation in a learningbased hand recovery system [35], while most 1arXiv:2103.11703v1  [cs.CV]  22 Mar 2021Approach Supervision Outputs [35] 3DM, 3DJ, 2DKP, I, TI 3DM, 3DJ, Tex [20, 30] 3DM, 3DJ 3DM, 3DJ [48] 3DM*, 3DJ, 2DKP, 2DS, Syn 3DM, 3DJ [16] 3DM*, 3DJ, 2DKP, D* 3DM, 3DJ [29] 3DM*, D2DKP 3DM, 3DJ [51] 3DJ, 2DKP, Mo 3DM, 3DJ [3, 7, 50] 3DJ, 2DKP, 2DS 3DM, 3DJ [52] 3DJ, 2DKP, 2DS 3DJ [24] 3DJ, 2DKP 3DJ [4] 3DJ*, 2DKP, 2DS 3DM, 3DJ, Tex [37] 3DJ*, 2DKP 3DJ [8] 2DKP, D 3DJ Ours D2DKP, I 3DM, 3DJ, Tex Table 1: A comparison of some representative 3D hand recovery approaches with highlighting the differences between the supervi sion and the outputs. We use the weakest degree of supervision and output the most representations. 3DM: 3D mesh, 3DJ: 3D joints, I: input image, TI: an additional set of images with clear hand tex ture, Tex: texture, 2DKP: 2D keypoints, 2DS: 2D silhouette, D: depth, D2DKP: detected 2D keypoints, Syn: extra synthetic se quence data, Mo: extra motion capture data. * indicates that the study uses multiple datasets for training, and at least one dataset used the supervision item. previous works do not consider texture modeling. Our key observation is that the 2D cues in the image space are closely related to the 3D hand model in the real world. The 2D hand keypoints contain rich structural infor mation, and the image contains texture information. Both are important for reducing the use of expensive 3D annota tions but have not been investigated much. In this way, we could directly use 2D annotations and the input image to learn the structural and texture representations without us ing 3D annotations. However, it is still laborconsuming to annotate 2D hand keypoints. To completely save the cost of manual annotation, we propose to extract some geomet ric representations from the unlabeled image to help shape reconstruction and use the texture information contained in the input image to help texture modeling. Motivated by the above observations, this work seeks to train an accurate and robust 3D hand reconstruction net work only using supervision signals obtained from the in put images and eliminate all manual annotations, which is the ﬁrst attempt in this task. To this end, we use an offthe shelf 2D keypoint detector [9] to produce some noisy 2D keypoints and supervise the hand reconstruction by these noisy detected 2D keypoints and the input image. To bet ter achieve this goal, there are several issues that need to be addressed. First, how to efﬁciently use jointwise 2D keypoints to supervise the illposed monocular 3D hand re construction? Second, since our setting does not use any ground truth annotation, how do we handle the noise in the 2D detection output? To address the ﬁrst issue, a modelbased autoencoder is presented to estimate 3D joints and shape, where the output3D joints are projected into image space and forced to align with the detected keypoints during training. However, if we only align keypoints in image space, invalid hand poses of ten occur. This may be an invalid 3D hand conﬁgure that could be projected to be the correct 2D keypoints. Also, 2D keypoints cannot reduce the scale ambiguity of the pre dicted 3D hand. Thus, we design a series of priors em bedded in the modelbased hand representations to help the neural network output hand with a reasonable pose and size. To address the second issue, a trainable 2D keypoint es timator and a novel 2D3D consistency loss are proposed. The 2D keypoint estimator outputs jointwise 2D keypoints and the 2D3D consistency loss links the 2D keypoint esti mator and the 3D reconstruction network to make the two mutually beneﬁcial to each other during the training. In ad dition, we ﬁnd that the detection accuracy of different sam ples varies greatly, thus we propose to distinguish each de tection item to weigh its supervision strength accordingly. In summary, we present a S2HAND (selfsupervised 3D hand reconstruction) model which enables us to train a neural network that can predict 3D pose, shape, texture, and camera viewpoint from a hand image without any ground truth annotation, except that we use the outputs from a 2D keypoint detector (Fig. 1). Our main contributions are summarized as follows: We present the ﬁrst selfsupervised 3D hand recon struction network, which accurately outputs 3D joints, mesh, and texture from a single image, without using any annotated training data. We exploit an additional trainable 2D keypoint estima tor to boost the 3D reconstruction through a mutual improvement manner, in which a novel 2D3D consis tency loss is proposed. We introduce a hand texture estimation module to learn vivid hand texture through selfsupervision. We benchmark selfsupervised 3D hand reconstruction on some currently challenging datasets, where our self supervised method achieves comparable performance to previous fullysupervised methods. 2. Related Work "
468,Multiplicative Reweighting for Robust Neural Network Optimization.txt,"Yet, their performance degrades in the presence of noisy labels at train
time. Inspired by the setting of learning with expert advice, where
multiplicative weights (MW) updates were recently shown to be robust to
moderate data corruptions in expert advice, we propose to use MW for
reweighting examples during neural networks optimization. We theoretically
establish the convergence of our method when used with gradient descent and
prove its advantage for label noise in 1d cases. We then validate empirically
our findings for the general case by showing that MW improves neural networks
accuracy in the presence of label noise on CIFAR-10, CIFAR-100 and Clothing1M.
We also show the impact of our approach on adversarial robustness.","Deep neural networks (DNNs) have gained massive popularity due to their success in a variety of applications. Large accurately labeled data are required to train a DNN to achieve good prediction performance. Yet, such data is costly and require a signiﬁcant amount of human attention. To reduce costs, one may train a network using annotated datasets that were created with lesser eﬀorts, but these may contain label noise which impedes the training process. We study methods to mitigate the harmful eﬀect of noise in the training process. Motivated by learning with expert advice, we employ MultiplicativeWeight (MW) updates [ 33,12] for promoting robustness in the training process. In online learning, it was shown theoretically that MW achieves optimal regret in a variety of scenarios, e.g, with losses drawn stochastically [ 42] or adversarially [ 33,12], and also in an intermediate setting with stochastic losses that are moderately corrupted by an adaptive adversary [ 3]. Thus, it is natural to consider this technique in the context of DNN training with noise, such as label noise in train data. To employ MW, we interpret the examples as experts and the predictions induced by the network as theiradvice. Wethenencounteralossfunctionoverthepredictions, whichfacilitatestheuseofMW.Thus, instead of learning by minimizing a uniform average of the losses, we propose to learn a weighted version of thelatterinwhichnotallexamplesaﬀectthetrainingprocessuniformly. WeemployaMWupdaterulefor reweighting the examples during the learning process, which we denote as multiplicative reweighting(MR). It alternates between SGD steps for optimizing the DNN’s parameters and MW updates for reweighting. ∗Blavatnik School of Computer Science, Tel Aviv University; nogabar@mail.tau.ac.il . †Blavatnik School of Computer Science, Tel Aviv University, and Google Research, Tel Aviv; tkoren@tauex.tau.ac.il . ‡School of Electrical Engineering, Tel Aviv University; raja@tauex.tau.ac.il . 1arXiv:2102.12192v3  [cs.LG]  29 Nov 20210 20 40 60 80 100 120 140 160 epoch0.00.20.40.60.81.0weighting sumnoisy clean 40% noisy 20% noisyFigure 1. Evolution of the multiplicative weights sum for clean and noisy examples with 20% and 40% artiﬁcial label noise when trained with CIFAR10. The sum of all weights is 1. Note how the weight of the noisy examples decreases throughout the training and thus they less aﬀect the network.Fig. 1 shows the weighting evolution during train ing where noisy examples have signiﬁcantly lower weightsthanthereratiointhetrainingdata. MR is simple, generic and can ﬁt easily in most train ing procedures. We establish the convergence of a simpliﬁed version of MR under mild conditions and prove the eﬃcacy of MR for 1d input data. Motivated by our theoretical ﬁndings, we show empirically that MR is also beneﬁcial in the multidimensional case for training with noisy labels. We demonstrate its advantage using two popular DNN optimizers: SGD (with momen tum) and Adam. We evaluate our approach both on artiﬁcial label noise in CIFAR10 and CIFAR100, and real noise in Clothing1M [ 72]. We compare to common techniques: Mixup [ 78] and labelsmoothing [ 63], which are known to improve accuracy with label noise, and the state oftheart sparse regularization [ 82]. We improve performance when we combine MR with them. We further compare to [4], which also suggested an unsupervised weighting method. Since MW is known to be optimal with worstcase adversarial loss, we also tested our method for adversarial attacks. We show how MR can improve adversarial training, which is known to be one of the best approaches against adversarial attacks. Speciﬁcally, we demonstrate the MR advantage with Free Adversarial Training [53] and TRADES [77]. Overall, we show that our method can ﬁt into the deep learning optimization toolbox as an easy to use tool that can help improving network robustness in a variety of scenarios and training pipelines. Our code appears in https://github.com/NogaBar/mr_robust_optim . 2 Related Work "
590,GLAD: GLocalized Anomaly Detection via Human-in-the-Loop Learning.txt,"Human analysts that use anomaly detection systems in practice want to retain
the use of simple and explainable global anomaly detectors. In this paper, we
propose a novel human-in-the-loop learning algorithm called GLAD (GLocalized
Anomaly Detection) that supports global anomaly detectors. GLAD automatically
learns their local relevance to specific data instances using label feedback
from human analysts. The key idea is to place a uniform prior on the relevance
of each member of the anomaly detection ensemble over the input feature space
via a neural network trained on unlabeled instances. Subsequently, weights of
the neural network are tuned to adjust the local relevance of each ensemble
member using all labeled instances. GLAD also provides explanations which can
improve the understanding of end-users about anomalies. Our experiments on
synthetic and real-world data show the effectiveness of GLAD in learning the
local relevance of ensemble members and discovering anomalies via label
feedback.","Deﬁnition 1 ( Glocal )Reﬂecting or characterized by both local and global considerations1. Endusers ﬁnd it easier to trust algorithms they understand and are familiar with. Such algorithms are typically built on broadly general and simplifying assumptions over the entire *Equal contribution from ﬁrst two authors.1School of EECS, Washington State University, Pullman, WA 99163 2School of Engineering & Computer Science, The Uni versity of Texas at Dallas, TX 75080. Correspondence to: Md Rakibul Islam <mdrakibul.islam@wsu.edu>, Shub homoy Das <shubhomoy.das@wsu.edu>, Janardhan Rao Doppa <jana.doppa@wsu.edu>, Sriraam Natarajan <Sri raam.Natarajan@utdallas.edu>. Workshop on Human in the Loop Learning at 37thInternational Conference on Machine Learning , Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s). 1https://en.wikipedia.org/wiki/Glocal (retrieved on May21 2020)feature space (i.e., global behavior), which may not be ap plicable universally (i.e., not relevant locally in some parts of the feature space) in an application domain. This observa tion is true of most machine learning algorithms including those for anomaly detection. We propose a principled tech nique referred as GLocalized Anomaly Detection (GLAD) which allows a human analyst to continue using anomaly detection ensembles with global behavior by learning their local relevance in different parts of the feature space via label feedback. Ensembles of anomaly detectors often outperform single detectors (Aggarwal & Sathe, 2017). Additionally, anoma lous instances can be discovered faster when the ensembles are used in conjunction with active learning, where a hu man analyst labels the queried instance(s) as nominal or anomaly (Veeramachaneni et al., 2016; Das et al., 2016; 2018; Siddiqui et al., 2018). A majority of the active learn ing techniques for discovering anomalies employ a weighted linear combination of the anomaly scores from the ensem ble members. This approach works well when the members are themselves highly localized, such as the leaf nodes of treebased detectors (Das et al., 2018). However, when the members of the ensemble are global (such as LODA pro jections (Pevny, 2015)), it is highly likely that individual detectors are incorrect in at least some local parts of the input feature space. To overcome this drawback, our GLAD algorithm automati cally learns the local relevance of each ensemble member in the feature space via a neural network using the label feed back from a human analyst. One interesting observation related to the key insight behind active learning with tree based models (TreeAAD) (Das et al., 2018) and GLAD is as follows: uniform prior over weights of each subspace (leaf node) in TreeAAD and uniform prior over input feature space for the relevance of each ensemble member in GLAD are highly beneﬁcial for labelefﬁcient active learning. We can consider GLAD as very similar to the TreeAAD ap proach. TreeAAD partitions the input feature space into discrete subspaces and then places a uniform prior over those subspaces (i.e., the uniform weight vector to combine ensemble scores). If we take this view to an extreme by imagining that each instance in feature space represents a subspace, we can see the connection to GLAD. While TreeAAD assigns the scores of discrete subspaces to inarXiv:1810.01403v4  [cs.LG]  15 Jul 2020GLAD: GLocalized Anomaly Detection via HumanintheLoop Learning stances (e.g., node depths for Isolation Forest), the scores assigned by GLAD are continuous, deﬁned by the global ensemble members. The relevance in GLAD is analogous to the learned weights in TreeAAD. Our GLAD technique is similar in spirit to dynamic ensem ble weighting (Jimenez, 1998). However, since we are in an active learning setting for anomaly detection, we need to consider two important aspects: (a)Number of labeled examples is very small (possibly none), and (b)To reduce the effort of the human analyst, the algorithm needs to be primed so that the likelihood of discovering anomalies is very high from the ﬁrst feedback iteration itself. Speciﬁcally, we employ a neural network to predict the local relevance of each ensemble member. This network is primed with unlabeled data such that it places a uniform prior for the relevance of each ensemble member over the input feature space. In each iteration of the active learning loop, we se lect one unlabeled instance for querying, and update the weights of the neural network to adjust the local relevance of each ensemble member based on all the labeled instances. Our code and datasets are publicly available at https: //github.com/shubhomoydas/ad_examples . 2. Related Work "
26,Building a Noisy Audio Dataset to Evaluate Machine Learning Approaches for Automatic Speech Recognition Systems.txt,"Automatic speech recognition systems are part of people's daily lives,
embedded in personal assistants and mobile phones, helping as a facilitator for
human-machine interaction while allowing access to information in a practically
intuitive way. Such systems are usually implemented using machine learning
techniques, especially with deep neural networks. Even with its high
performance in the task of transcribing text from speech, few works address the
issue of its recognition in noisy environments and, usually, the datasets used
do not contain noisy audio examples, while only mitigating this issue using
data augmentation techniques. This work aims to present the process of building
a dataset of noisy audios, in a specific case of degenerated audios due to
interference, commonly present in radio transmissions. Additionally, we present
initial results of a classifier that uses such data for evaluation, indicating
the benefits of using this dataset in the recognizer's training process. Such
recognizer achieves an average result of 0.4116 in terms of character error
rate in the noisy set (SNR = 30).",2 Related Work 2 
531,Human-In-The-Loop Person Re-Identification.txt,"Current person re-identification (re-id) methods assume that (1) pre-labelled
training data are available for every camera pair, (2) the gallery size for
re-identification is moderate. Both assumptions scale poorly to real-world
applications when camera network size increases and gallery size becomes large.
Human verification of automatic model ranked re-id results becomes inevitable.
In this work, a novel human-in-the-loop re-id model based on Human Verification
Incremental Learning (HVIL) is formulated which does not require any
pre-labelled training data to learn a model, therefore readily scalable to new
camera pairs. This HVIL model learns cumulatively from human feedback to
provide instant improvement to re-id ranking of each probe on-the-fly enabling
the model scalable to large gallery sizes. We further formulate a Regularised
Metric Ensemble Learning (RMEL) model to combine a series of incrementally
learned HVIL models into a single ensemble model to be used when human feedback
becomes unavailable.","PERSON reidentiﬁcation (reid) is the problem of matching people across nonoverlapping camera views distributed in open spaces at different locations, typically achieved by matching detected bounding box images of people [23]. This is an inher ently challenging problem due to the potentially dramatic visual appearance changes caused by uncontrolled variations in human pose and unknown viewing conditions on illumination, occlusion, and background clutter (Fig. 1). A reid model is required to differentiate images of different categories (persons) with similar appearances, which can be considered as solving a ﬁnegrained visual categorisation problem [14,78], whilst also able to recognise a same category (person) with visually dissimilar appearances. Unlike conventional biometrics identiﬁcation problems, e.g. face recognition, a person reid model has no labelled training data on target classes , i.e. similar to a zeroshot learning problem [31] that requires the model to perform inherently transfer learning between a training population (seen) and a target population (unseen). Moreover, person reid requires implicitly a model to perform crossdomain transfer learning [53] if each camera view is considered as a speciﬁc domain of potentially signiﬁcant difference to other domains (views). This is more difﬁcult than a standard zeroshot learning problem. Current reid methods are dominated by supervised learning techniques [23,34,38,39,40,50,55,79,80,88,94,97], which typically employ a “ trainonceanddeploy ” scheme (Fig. 2(a)). That is, a prelabelled training dataset of pairwise true and falsematching Hanxiao Wang is with the Electrical and Computer Engineering De partment, Boston University, US. Email: hxw@bu.edu. Shaogang Gong and Tao Xiang are with the School of Electronic Engineering and Computer Science, Queen Mary University of London, UK. Email: s.gong@qmul.ac.uk, t.xiang@qmul.ac.uk. Xiatian Zhu is with Vision Se mantics Ltd., London, UK. Email: eddy@visionsemantics.com. (a)Crossview appearance variations  (b)Similar appearance among different people Fig. 1. Person reidentiﬁcation Challenges. (a) A signiﬁcant visual ap pearance change of the same person across camera views. (b) Strong appearance similarities among different people. identities (training population) is collected by human annotators for every pair of cameras through manually examining a vast pool of image/video data. This training dataset is used to train an ofﬂine reid model. It is tacitly assumed by most that such a trained model can be deployed plausibly as a fully automated solution to reidentify target (unseen during model training) person images at test time, without any human assistance nor model adaptation. Based on this assumption, the reid community has witnessed everincreased matching accuracies on increasingly larger sized benchmarks of more training identity classes over the past two years. For instance, the CUHK03 benchmark [38] contains 13,164 images of 1,360 identities, of which 1,260 are used for training and 100 for testing, signiﬁcantly larger than the earlier VIPeR [25] (1,264 images of 632 people with 316 for training) and iLIDS [96] (476 images for 119 people with 69 for training). The stateofthe art Rank1 accuracy on CUHK03 has exceeded 70% [86], tripling the best performance reported only two years ago [38]. Despite such rapid progresses, current automatic reid solu tions remain illsuited for a practical deployment. This is because: (1)A manually prelabelled pairwise training data set for every camera pair does not exist, due to either being prohibitively expensive to collect in the realworld as there are a quadratic number of camera pairs, or nonexistence of sufﬁciently large number of training people reappearing in every pair of cameraarXiv:1612.01345v2  [cs.CV]  4 May 20182 Deployable to further  population   Large training  dataset  Exhaustively label   ALL camera pairs   (a) Human OutoftheLoop re id scheme  Offline   model training  ReId model   Deploy to the same camera pairs  Annotation Stage   Deployment Stage   (c) HVIL: Human Verification Incremental Learning  Probe  population  Gallery   population   rank/re rank  user feedback   reid models   optimised in isolation   Training Stage   Gallery   population   Model N Human intheloop  (b) POP: Post rank optimisation  Model 1  rank/re rank  user feedback   rank/re rank  user feedback  Model 1  Strong  model   Human intheloop  reid models   optimised incrementally   limited labour budget   Not deployable to  further population   Model 2 Model 2  Fig. 2. Illustration of two person reid schemes. (a)The conventional HumanOutoftheLoop (HOL) reid scheme requires exhaustive prelabelled training data collection for supervised ofﬂine model learning. The learned model is assumed sufﬁciently generalisable and then deployed to perform fully automated person reid tasks without human in the loop. (b)POP [44]: A recent HumanIntheLoop (HIL) reid approach which optimises probespeciﬁc models in isolation from human feedback veriﬁcations in the deployment time. All probe people requires human in the loop. (c)HVIL: The proposed new incremental HIL reid model capable of not only progressively learning a generalised model from human veriﬁcations across all probed people while carrying out the HIL reid tasks, but also performing the HOL reid tasks when human effort becomes unavailable. views. (2)Assuming the size of the training population is either signiﬁcantly greater or no less than that of the target population is unrealistic. For instance, the standard CUHK03 benchmark test deﬁnes the training set having paired images of 1,260 people from six different camera views (on average 4.8 image samples per person per camera view), whilst the test set having only 100 identities each with a single image. The test population is thus 10 times smaller than the training population, with approximately 50 times less images. In practice, any deployment gallery size (test population) is almost always much greater than any labelled training data size even if such training data were available. In a public space such as an underground station, there are easily thousands of people passing through a camera view every hour [1], with a typical gallery population size of over 10,000 per day. We observed on the CUHK03 dataset that, only a 10fold increase in gallery size leads to a 10fold decrease in reid Rank1 performance, leading to a singledigit Rank1 score, even when the stateoftheart reid models were trained from sufﬁciently sized labelled data. Given such low Rank1 scores, in practice human operators (users) would still be required to verify any true match of a probe from an automatically generated ranking list. In this work, we explore an alternative approach to person re id by formulating a hybrid humancomputer learning paradigm with humans in the model matching loop (Fig. 2(c)). We call this semiautomated scheme HumanIntheLoop (HIL) reid, de signed to optimise reid performance given a small number of human veriﬁcation feedback and a largersized test population, as compared to the conventional HumanOutoftheLoop (HOL) reid models that are mostly designed to optimise reid given a largersized prelabelled training data and a smallsized test population. This HIL reid scheme has three signiﬁcant advantages over the conventional HOL models: (1)Less human labelling effort : HIL reid requires much less human labelling effort, since it does not necessarily require the expensive construction of a prelabelled training set. More importantly, it prioritises directly the human labour effort on each given reid task in deployment, rather than optimising the model learning error on an independent training set. More speciﬁcally, the number of feedback from human veriﬁcation is typically in tens as compared to thousands of ofﬂine prelabelled training data required by HOL methods. (2)Model transfer learning : Our HIL model is able to achieve greater transferability with better reid performance in test domains. This is because a HIL model focuses on reid matching optimisation directly in the deployment gallery population, rather than learning a distance metric from a separate training set and assuming its blind transferability to independent (unseen) test data. It enables a human operator to interactively validate model matching results for each reid task and inform on model mistakes (similar in spirit to hard negative mining). (3)Reinforcing visual consistency : As computer vision algorithms are intrinsically very different from the human visual system, a reid model can make mistakes that generate “unexpected” (visually inconsistent) reid ranking results, readily identiﬁable by a human observer. By learning directly from the inconsistency between a computer vision model and human observation, a HIL reid model is guided to maximise visually more consistent ranking lists favoured by human observations, and thus rendering the learned model more discriminative and capable of avoiding future mistakes that seem insensible to human observation. The main contribution of this work is a novel HIL reid model that enables a user to reidentify rapidly a given probe person image after only a handful of feedback veriﬁcations even when the search gallery size is large. More speciﬁcally, a Human Veriﬁcation Incremental Learning (HVIL) model (Fig. 2(c)) is formulated to simultaneously minimise humanintheloop feed back and maximise model reid accuracy by incorporating: (1) Sparse feedback  HVIL allows for easier human feedback on a few dissimilar matching results without the need for exhaustive eyeball search of true/false in the entire rank list. It aims to rectify rapidly unexpected model mistakes by focusing only on minimising visually obvious errors (hard negatives) identiﬁed by human observation. This is reminiscent to learning by hard negative mining but with human in the loop, so to improve model learning with less training data. (2)Immediate beneﬁt  HVIL introduces a new online incremental distance metric learning algorithm, which enables realtime model response to human feedback by rapidly presenting a freshly optimised ranking list for further human feedback, quickly leading to identifying a true match. (3)The older the wiser  HVIL is updated cumulatively on theﬂy utilising multiple user feedback per probe, with incremental3 … … A large person image pool … ID #i  (a) Exhuastive labelling Actively/randomly selected image pairs  … (b) True or false pairwise labelling Probe shorts backpack blue sneakers (c) Attribute labelling Topk Ranked Gallery Images  Probe (d) Top ranks labelling (true match, negative) Fig. 3. Different human labelling processes are employed in person reid model training and deployment. (a)Large size ofﬂine labelling of crossview positive and negativepairs of training data with identity labels [40,52,75,91]. (b)Selective or random sampling of person image pairs for human veriﬁcation in either model training [48] or deployment [15]. (c)Finegrained attribute labelling in either training [64] or deployment [15]. (d)True match veriﬁcation among the top ranked sublist in model deployment [26,44,76], or veriﬁcation of both visually dissimilar and similar wrong matches in top ranks (strong/hard and weak negative mining) in model deployment [44,76]. model optimisation for each new probe given what have been learned from all previous probes. (4)A strong ensemble model An additional Regularised Metric Ensemble Learning (RMEL) model is introduced by taking all the incrementally optimised per probe models as a set of “weak” models [4,59] and constructing a “strong” ensemble model for performing HOL reid tasks when human feedback becomes unavailable. Extensive comparative ex periments on three benchmark datasets (CUHK03 [38], Market 1501 [95], and VIPeR [25]) demonstrate that this HVIL model outperforms the stateoftheart methods for both the proposed new HIL and the conventional HOL reid deployments. 2 R ELATED WORK "
593,Detecting human and non-human vocal productions in large scale audio recordings.txt,"We propose an automatic data processing pipeline to extract vocal productions
from large-scale natural audio recordings. Through a series of computational
steps (windowing, creation of a noise class, data augmentation, re-sampling,
transfer learning, Bayesian optimisation), it automatically trains a neural
network for detecting various types of natural vocal productions in a noisy
data stream without requiring a large sample of labeled data. We test it on two
different data sets, one from a group of Guinea baboons recorded from a primate
research center and one from human babies recorded at home. The pipeline trains
a model on 72 and 77 minutes of labeled audio recordings, with an accuracy of
94.58% and 99.76%. It is then used to process 443 and 174 hours of natural
continuous recordings and it creates two new databases of 38.8 and 35.2 hours,
respectively. We discuss the strengths and limitations of this approach that
can be applied to any massive audio recording.","There is a growing number of massive continuous audio recordings made in natural environments that aim to study the vocal productions of dierent animal species. The need to collect such data is particularly important in comparative approaches. It is essential notably to further progress on the issue of language evo lution [1]. In particular, studies on the vocal productions of dierent species allow us to question hypotheses that were no longer discussed for decades [2, 3]. Beyond wild ecosystems and the study of nonhuman animals, Gilkerson et al. [4] note the importance of studying the vocal productions of human children in their natural environment with their parents. By quantifying these vocalizations over time, one can estimate the relationship between certain covariates and child development. Cabon et al. [5] have shown, for example, the value of recording and retrieving the cries of newborns in neonatal wards to ensure the proper development of these children. Similarly, ter Haar et al. [6] showed that forms of babbling, an important phase in human language development, can be found in species other than humans. For all these questions, there is a need for new methods to eciently and rapidly analyze massive audio data to further study these critical developmental phases in detail. To deal with this type of problem, deep learning approaches have proven their eciency in dierent areas to treat massive data, possibly noisy, with increasingly good results [7]. However, one problem with deep learning approaches is the need for huge amount of data and computational resources to learn the relevant information. Here we propose a complete pipeline based on deep learning neural networks that has been Corresponding Author Email address : guillem.bonafos@univamu.fr Preprint submitted to ElsevierarXiv:2302.07640v1  [cs.SD]  14 Feb 2023designed to quickly detect the target signal (i.e., vocalizations from a given species), to treat massive and noisy data, and to minimise the loss of information. The originality of this approach is multiple. The pipeline is complete and truly endtoend, from learning to prediction. It has been tested on real data and its generalizability has been evaluated on two qualitatively distinct data sets from two animal species, Guinea baboons ( Papio papio ) and human infants ( Homo sapiens ). In each case, training of the model has been done on a restricted labeled data set while predictions were done on massive records. Computations were relatively fast and done on a laptop. Finally, the model provides supplementary information about the class of each detected vocalization. In the following sections, we rst provide a review of the pattern recognition literature that address the issue of detecting and classifying vocalizations from large scale audio recordings. Second, we present the proposed pipeline. Third, we test it on two completely dierent data sets. Fourth, we show that the results reach stateoftheart performances on comparable data sets and how the method can be easily applied to other data sets. 2. Related work "
83,Asymmetric Co-Teaching for Unsupervised Cross Domain Person Re-Identification.txt,"Person re-identification (re-ID), is a challenging task due to the high
variance within identity samples and imaging conditions. Although recent
advances in deep learning have achieved remarkable accuracy in settled scenes,
i.e., source domain, few works can generalize well on the unseen target domain.
One popular solution is assigning unlabeled target images with pseudo labels by
clustering, and then retraining the model. However, clustering methods tend to
introduce noisy labels and discard low confidence samples as outliers, which
may hinder the retraining process and thus limit the generalization ability. In
this study, we argue that by explicitly adding a sample filtering procedure
after the clustering, the mined examples can be much more efficiently used. To
this end, we design an asymmetric co-teaching framework, which resists noisy
labels by cooperating two models to select data with possibly clean labels for
each other. Meanwhile, one of the models receives samples as pure as possible,
while the other takes in samples as diverse as possible. This procedure
encourages that the selected training samples can be both clean and
miscellaneous, and that the two models can promote each other iteratively.
Extensive experiments show that the proposed framework can consistently benefit
most clustering-based methods, and boost the state-of-the-art adaptation
accuracy. Our code is available at
https://github.com/FlyingRoastDuck/ACT_AAAI20.","Person reidentiﬁcation (reID) (Sun et al. 2018; Zheng, Yang, and Hauptmann 2016; Li, Zhu, and Gong 2018b) aims to locate the target person in surveillance videos with a given probe image. With the rapid evolution of deep learning mod els, the accuracy of person reID has been greatly boosted in the public datasets. However, models trained on the source domain often suffer from domain shifts, leading to a perfor mance decline on a different target domain. To alleviate this issue, recent works (Zhong et al. 2019b; Zhong et al. 2018b) make efforts on the unsupervised do This work was done when Fengxiang Yang was an intern at Youtu Lab (yangfx@stu.xmu.edu.cn). yCorresponding Author (zhiming.luo@xmu.edu.cn, winfred sun@tencent.com) Copyright c 2020, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.main adaptation (UDA), which aims to transfer the knowl edge from the labeled source domain to the unlabeled tar get domain. These works mainly lie in two aspects, distri bution aligning (Wei et al. 2018; Deng et al. 2018; Chang et al. 2019; Lin et al. 2018; Wang et al. 2018) and tar get pseudo label discovering (Fan, Zheng, and Yang 2018; Song et al. 2018; Li, Zhu, and Gong 2018a). The former aims to reduce the distribution gap between domains in a common space, such as imagelevel (Wei et al. 2018; Deng et al. 2018) and attributelevel (Chang et al. 2019; Lin et al. 2018; Wang et al. 2018) spaces. The latter attempts to leverage the underlying relations among target samples and predict pseudo labels for model retraining, e.g.assign ing pseudo labels based on clustering (Fan, Zheng, and Yang 2018; Song et al. 2018; Li, Zhu, and Gong 2018a) and k nearest neighbors (Zhong et al. 2019a; Yang et al. 2018). Among them, clustering based methods have reported very competitive accuracy for UDA in person reID. These meth ods usually employ an iterative process of predicting pseudo identities for unlabeled target samples according to the clus ters and ﬁnetuning the model with those predicted samples. Despite their promising results, clustering based methods are restricted by two main drawbacks. On the one hand, the clustering accuracy can not be guaranteed even using the modern approaches, so that pseudo labels assigned by clusters can be noisy. Training the model with noisy labels that assigned to wrong identities will undoubtedly damage the reID performance. On the other hand, most clustering methods tend to leave low conﬁdence samples as outliers and do not assign cluster labels to them, e.g., DBSCAN (Es ter et al. 1996). These outliers are usually hard samples that encounter high image variations. Without considering such samples during training, the model may have a problem in discriminating high variation testing samples. However, di rectly assigning them to the nearest cluster will bring more noisy labels, hindering the retraining of the model. CoTeaching (CT) (Han et al. 2018) is a commonly used algorithm for training model with noisy labels, which learns two networks by feeding samples with small losses of one network to another. However, most coteaching frameworks utilize symmetric inputs for both networks, which do not effectively apply to the context of clustering based crossarXiv:1912.01349v1  [cs.CV]  3 Dec 20197DUJHW'DWD«Ltri 6PDOO/RVV6DPSOHV/DEHOHG2XWOLHUV Ltri /DEHOHG,QOLHUV& &0 00 6PDOO/RVV6DPSOHV6WHS,,QOLHUV2XWOLHUV*HQHUDWLRQ 6WHS,,$V\PPHWULF&R7HD FKLQJ 2XWOLHUV ,QOLHUV Figure 1: The proposed asymmetric coteaching framework (ACT). “M” and “C” denote the main model and the collaborator model, respectively. We ﬁrst train CNN on the source labeled data and ﬁnetune it on target data with pseudo labels predicted by clustering to get initial weights for “M” and “C”. “M” receives samples as diverse as possible from inliers and outliers, while “C” takes in samples as pure as possible from inliers during ACT. This process encourages the two models to mutually promote the discriminative ability of each other. More details can be found at Sec. 3.4. domain reID. This is because that the training samples with lowconﬁdence commonly have large losses during training. Using symmetric inputs leads the model to always select easy samples and ignore the lowconﬁdence samples within the training minibatch. As a consequence, the second short coming mentioned above will still remain and will lead re ID model to a local minimum. To this end, we choose the stateoftheart clustering based method proposed in (Song et al. 2018) as our baseline, and propose an asymmetric coteaching framework to elim inate the negative effects of the above two shortcomings. Speciﬁcally, we ﬁrst divide the target samples into inliers and outliers, according to the clustering results (as shown in Fig. 1). In this paper, we regard the lowconﬁdent samples recognized by the clustering method as outliers while re maining as inliers. After that, our framework is trained with two models. The ﬁrst one is the main model which aims to infer samples with small losses from the inliers, while the second one is the collaborator model that estimates sam ples with small losses from the outliers. The samples in ferred/estimated by the certain model are selected for the training of another model. This training process is similar to the traditional coteaching, except that the inputs of the two models are asymmetric, i.e.the data for training the two models comes from two different data ﬂows. In this manner, selecting samples with small losses ensure that the models can be trained with possibly clean data. Moreover, these two models are iteratively promoted by each other. On the one hand, the main model attempts to mine as pure as possible samples from the inliers for maintaining the basic represen tation of the collaborator model. On the other hand, the col laborator model tries to select as diverse as possible samples from the outliers for further improving the discriminative ability of the main model. Our contributions are summarized in threefold:We introduce to employ coteaching technique for re sisting noisy labels generated by clustering in the cross domain person reID. Experiments show that learning with ﬁltered data can consistently improve adaptation ac curacy. We divide the unlabeled target data into inliers and out liers and design an asymmetric coteaching (ACT) frame work to make reID model see hard samples at the early stage of adaptation. Experiments demonstrate that the asymmetric approach is more effective in handling hard samples than the symmetric one. Experiments conducted on three largescale datasets show that our method can apply to various clustering based methods and produces stateoftheart adaptation accu racy in person reID. 2 Related Work "
219,GMNN: Graph Markov Neural Networks.txt,"This paper studies semi-supervised object classification in relational data,
which is a fundamental problem in relational data modeling. The problem has
been extensively studied in the literature of both statistical relational
learning (e.g. relational Markov networks) and graph neural networks (e.g.
graph convolutional networks). Statistical relational learning methods can
effectively model the dependency of object labels through conditional random
fields for collective classification, whereas graph neural networks learn
effective object representations for classification through end-to-end
training. In this paper, we propose the Graph Markov Neural Network (GMNN) that
combines the advantages of both worlds. A GMNN models the joint distribution of
object labels with a conditional random field, which can be effectively trained
with the variational EM algorithm. In the E-step, one graph neural network
learns effective object representations for approximating the posterior
distributions of object labels. In the M-step, another graph neural network is
used to model the local label dependency. Experiments on object classification,
link classification, and unsupervised node representation learning show that
GMNN achieves state-of-the-art results.","We live in an interconnected world, where entities are con nected through various relations. For example, web pages are linked by hyperlinks; social media users are connected through friendship relations. Modeling such relational data is an important topic in machine learning, covering a va riety of applications such as entity classiﬁcation (Perozzi et al., 2014), link prediction (Taskar et al., 2004) and link 1Mila  Qu ´ebec AI Institute2University of Montr ´eal3Canadian Institute for Advanced Research (CIFAR)4HEC Montr ´eal. Cor respondence to: Meng Qu <meng.qu@umontreal.ca >, Jian Tang <jian.tang@hec.ca >. Proceedings of the 36thInternational Conference on Machine Learning , Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).classiﬁcation (Dettmers et al., 2018). Many of these applications can be boiled down to the fundamental problem of semisupervised object classiﬁca tion (Taskar et al., 2007). Speciﬁcally, objects1are inter connected and associated with some attributes. Given the labels of a few objects, the goal is to infer the labels of other objects. This problem has been extensively studied in the literature of statistical relational learning (SRL), which develops statistical methods to model relational data. Some representative methods include relational Markov networks (RMN) (Taskar et al., 2002) and Markov logic networks (MLN) (Richardson & Domingos, 2006). Generally, these methods model the dependency of object labels using con ditional random ﬁelds (Lafferty et al., 2001). Because of their effectiveness for modeling label dependencies, these methods achieve compelling results on semisupervised ob ject classiﬁcation. However, several limitations still remain. (1) These methods typically deﬁne potential functions in conditional random ﬁelds as linear combinations of some handcrafted feature functions, which are quite heuristic. Moreover, the capacity of such models is usually insufﬁcient. (2) Due to the complexity of relational structures between objects, inferring the posterior distributions of object labels for unlabeled objects remains a challenging problem. Another line of research is based on the recent progress of graph neural networks (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018). Graph neural networks approach object classiﬁcation by learning effective object representations with nonlinear neu ral architectures, and the whole framework can be trained in an endtoend fashion. For example, the graph convolu tional network (GCN) (Kipf & Welling, 2017) iteratively updates the representation of each object by combining its own representation and the representations of the surround ing objects. These approaches have been shown to achieve stateoftheart performance because of their effectiveness in learning object representations on relational data. How ever, one critical limitation is that the labels of objects are independently predicted based on their representations. In other words, the joint dependency of object labels is ignored. 1In this paper, we will use “object” and “node” interchangeably to refer to entities in graphs, because they are different terminolo gies used in the literature of statistical relational learning and graph neural networks.arXiv:1905.06214v3  [cs.LG]  23 Jul 2020GMNN: Graph Markov Neural Networks In this paper, we propose a new approach called the Graph Markov Neural Network (GMNN), which combines the ad vantages of both statistical relational learning and graph neural networks. A GMNN is able to learn effective object representations as well as model label dependency between different objects. Similar to SRL methods, a GMNN in cludes a conditional random ﬁeld (Lafferty et al., 2001) to model the joint distribution of object labels conditioned on object attributes. This framework can be effectively and efﬁ ciently optimized with the variational EM framework (Neal & Hinton, 1998), alternating between an inference proce dure (Estep) and a learning procedure (Mstep). In the learning procedure, instead of maximizing the likelihood function, the training procedure for GMNNs optimizes the pseudolikelihood function (Besag, 1975) and parameterizes the local conditional distributions of object labels with a graph neural network. Such a graph neural network can well model the dependency of object labels, and no handcrafted potential functions are required. For inference, since exact inference is intractable, we use a meanﬁeld approxima tion (Opper & Saad, 2001). Inspired by the idea of amor tized inference (Gershman & Goodman, 2014; Kingma & Welling, 2014), we further parameterize the posterior dis tributions of object labels with another graph neural net work, which is able to learn useful object representations for predicting object labels. With a graph neural network for inference, the number of parameters can be signiﬁcantly reduced, and the statistical evidence can be shared across different objects in inference (Kingma & Welling, 2014). Our GMNN approach is very general. Though it is designed for object classiﬁcation, it can be naturally applied to many other applications, such as unsupervised node representation learning and link classiﬁcation. Experiment results show that GMNNs achieve stateoftheart results on object clas siﬁcation and unsupervised node representation learning, as well as very competitive results on link classiﬁcation. 2. Related Work "
354,Transductive CLIP with Class-Conditional Contrastive Learning.txt,"Inspired by the remarkable zero-shot generalization capacity of
vision-language pre-trained model, we seek to leverage the supervision from
CLIP model to alleviate the burden of data labeling. However, such supervision
inevitably contains the label noise, which significantly degrades the
discriminative power of the classification model. In this work, we propose
Transductive CLIP, a novel framework for learning a classification network with
noisy labels from scratch. Firstly, a class-conditional contrastive learning
mechanism is proposed to mitigate the reliance on pseudo labels and boost the
tolerance to noisy labels. Secondly, ensemble labels is adopted as a pseudo
label updating strategy to stabilize the training of deep neural networks with
noisy labels. This framework can reduce the impact of noisy labels from CLIP
model effectively by combining both techniques. Experiments on multiple
benchmark datasets demonstrate the substantial improvements over other
state-of-the-art methods.","The revolutionized successes of deep neural networks in a variety of computer vision applications are conferred by large databases with accurate annotation [1, 2, 3]. In many real world scenarios, data labeling is very costly in terms of re source and time consumption. Several efforts had been made for unsupervised model optimization in the past [4, 5, 6, 7, 8]. Recently, Contrastive LanguageImage Pretraining (CLIP) [9] has emerged as a promising alternative for generalizing vision tasks. To alleviate the burden of data labeling, there is a strong motivation to leverage the unlabeled data supervised from the CLIP model in a transductive learning manner. How ever, it cannot achieve satisfactory performance by directly learning from the pseudo labels predicted by CLIP model since the classiﬁcation model is prone to ﬁt and memorize the label noise [10], leading to the performance degeneration. * This work is done when Junchu Huang was an intern in Hikvision Research Institute. "
350,Improving Robustness and Generality of NLP Models Using Disentangled Representations.txt,"Supervised neural networks, which first map an input $x$ to a single
representation $z$, and then map $z$ to the output label $y$, have achieved
remarkable success in a wide range of natural language processing (NLP) tasks.
Despite their success, neural models lack for both robustness and generality:
small perturbations to inputs can result in absolutely different outputs; the
performance of a model trained on one domain drops drastically when tested on
another domain.
  In this paper, we present methods to improve robustness and generality of NLP
models from the standpoint of disentangled representation learning. Instead of
mapping $x$ to a single representation $z$, the proposed strategy maps $x$ to a
set of representations $\{z_1,z_2,...,z_K\}$ while forcing them to be
disentangled. These representations are then mapped to different logits $l$s,
the ensemble of which is used to make the final prediction $y$. We propose
different methods to incorporate this idea into currently widely-used models,
including adding an $L$2 regularizer on $z$s or adding Total Correlation (TC)
under the framework of variational information bottleneck (VIB). We show that
models trained with the proposed criteria provide better robustness and domain
adaptation ability in a wide range of supervised learning tasks.","Supervised neural networks have achieved remark able success in a wide range of NLP tasks, such as language modeling (Xie et al., 2017; Devlin et al., 2018a; Liu et al., 2019; Joshi et al., 2020; Meng et al., 2019b), machine reading comprehen sion (Seo et al., 2016; Yu et al., 2018), and machine translation (Sutskever et al., 2014; Vaswani et al., 2017b; Meng et al., 2019a). Despite the success,neural models lack for both robustness and gener ality and are extremely fragile: the output label can be changed with a minor change of a single pixel (Szegedy et al., 2013; Goodfellow et al., 2014b; Nguyen et al., 2015; Papernot et al., 2017; Yuan et al., 2019) in an image or a token in a document (Li et al., 2016; Papernot et al., 2016; Jia and Liang, 2017; Zhao et al., 2017; Ebrahimi et al., 2017; Jia et al., 2019b); The model lacks for domain adapta tion abilities (Mou et al., 2016; Daumé III, 2009): a model trained on one domain can hardly generalize to new test distributions (Fisch et al., 2019; Levy et al., 2017). Despite that different avenues have been proposed to address model robustness such as augmenting the training data using rulebased lexical substitutions (Liang et al., 2017; Ribeiro et al., 2018) or paraphrase models (Iyyer et al., 2018), building robust and domainadaptive neural models remains a challenge. In a standard supervised learning setup, a neural network model ﬁrst maps an input xto a single vectorz=f(x).zcan be viewed as the hidden feature to represent x, and is transformed to its logitlfollowed by a softmax operator to output the target label y. At training time, parameters in volved in mapping from x2Xtozthen toyare learned. At test time, the pretrained model makes a prediction when presented with a new instance x02X0. This methodology works well if Xand X0come from exactly the same distribution, but signiﬁcantly suffers if not. This is because the implicit representation learned through supervised signals can easily and overﬁt to the training do mainX, and the mapping function f(x), which is trained only based on X, can be confused with outofdomain features in x0, such as a lexical, prag matic, and syntactic variation not seen in the train ing set (Ettinger et al., 2017). We can also interpret the weakness of this methodology from a domainarXiv:2009.09587v1  [cs.CL]  21 Sep 2020adaptation point of view (Daume III and Marcu, 2006; Daumé III, 2009; Tan et al., 2009; Patel et al., 2014): it is crucial to separate sourcespeciﬁc fea tures, targetspeciﬁc features and general features (features shared by sources and targets). One of the most naive strategies for domain adaptation is to ask the model to only use general features for test. In the standard x!z!ysetup, all features, including sourcespeciﬁc, targetspeciﬁc and gen eral features, are entangled in z. Due to the lack of interpretability (Li et al., 2015; Linzen et al., 2016; Lei et al., 2016; Koh and Liang, 2017) of neural models, it is impossible to disentangle them. Inspired by recent work in disentangled representa tion learning (Bengio et al., 2013; Kim and Mnih, 2018; Hjelm et al., 2018; Kumar et al., 2018; Lo catello et al., 2019), we propose to improve robust ness and generality of NLP models using disen tangled representations. Different from mapping xto a single representation zand then to y, the proposed strategy ﬁrst maps xto a set of distinct representations Z=fz1;;zKg, which are then individually projected to logits l1;;lK.ls are ensembled to make the ﬁnal prediction of y. In this setup, we wish to make zs orls to be disentangled from each other as much as possible, which poten tially improves both robustness and generality: For the former, the decision of yis more immune to small changes in xsince even though small changes lead to signiﬁcant changes in some zs orls, others may remain invariant. The ultimate inﬂuence on ycan be further regulated when ls are combined. For the latter, different ls have the potential to dis entangle or partially disentangle sourcespeciﬁc, targetspeciﬁc and general features. Practically, we propose two ways to disentangle representations: adding an L2 regularizer or adding Total Correlation (TC) (Cover and Thomas, 2012; Ver Steeg and Galstyan, 2015; Steeg, 2017; Gao et al., 2018; Chen et al., 2018) under the frame work of variational information bottleneck (VIB). We show that models trained with the proposed criteria provide better robustness and domain adap tation ability in a wide range of NLP tasks, with tiny or nonsigniﬁcant sacriﬁce on taskspeciﬁc ac curacies. In summary, the contributions of this paper are: We present two methods to improve the ro bustness and generality of NLP models in theview of disentangled representation learning and the information bottleneck theory. Extensive experiments on domain adaptation and defense against adversarial attacks show that the proposed methods are able to provide better robustness compared with conventional taskspeciﬁc models, which indicates the ef fectiveness of the theory of information bottle neck and disentangled representation learning for NLP tasks. The rest of this paper is organized as follows: we present related work in Section 2. Models are de tailed in Section 3 and Section 4. We present exper imental results and analysis in Section 5, followed by a brief conclusion in Section 6. 2 Related Work "
602,Stochastic Generalized Adversarial Label Learning.txt,"The usage of machine learning models has grown substantially and is spreading
into several application domains. A common need in using machine learning
models is collecting the data required to train these models. In some cases,
labeling a massive dataset can be a crippling bottleneck, so there is need to
develop models that work when training labels for large amounts of data are not
easily obtained. A possible solution is weak supervision, which uses noisy
labels that are easily obtained from multiple sources. The challenge is how
best to combine these noisy labels and train a model to perform well given a
task. In this paper, we propose stochastic generalized adversarial label
learning (Stoch-GALL), a framework for training machine learning models that
perform well when noisy and possibly correlated labels are provided. Our
framework allows users to provide different weak labels and multiple
constraints on these labels. Our model then attempts to learn parameters for
the data by solving a non-zero sum game optimization. The game is between an
adversary that chooses labels for the data and a model that minimizes the error
made by the adversarial labels. We test our method on three datasets by
training convolutional neural network models that learn to classify image
objects with limited access to training labels. Our approach is able to learn
even in settings where the weak supervision confounds state-of-the-art weakly
supervised learning methods. The results of our experiments demonstrate the
applicability of this approach to general classification tasks.","Recent success of deep learning has seen an explosion in interest towards building largescale models for various ap plications. Training deep models often involves using mas sive amounts of training data whose labels are not easily obtained or available. Collecting labeled data for training these largescale models is a major bottleneck since these labels are usually provided by expert annotators and can be expensive to gather. Weak supervision offers an alternative for training machine learning models because it relies on ap proximate labels that are easily obtained. Weakly supervised learning alleviates some of the difﬁculties and cost associ ated with supervised learning by only requiring annotators Copyright c 2020, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.to provide rules or approximate indicators that automatically label the data. Ideally, these annotators or human experts provide several weak rules in the form of feature annotations, heuristic patterns, or programmatically generated labels. The difﬁculty lies in combining multiple weak supervision sig nals that may be from various sources, make dependent er rors, or sometimes have different noise levels within them. Researchers have developed methods (Ratner et al .2017; Ratner et al .2016; Arachie and Huang 2019) to combine the different weak supervision signals to train models robust to redundancies and errors in the weak supervision. These ap proaches are limited in ways such as being overly optimistic about the independence of weak signals, being only able to handle binary classiﬁcation, or having poor scalability. We address these limitations in this paper. We develop a general framework that encodes multiple linear constraints on the weak supervision signals. Our algo rithm learns from the weak supervision by solving a nonzero sum game between an adversary and the model. By using a nonzero sum game, our formulation provides more ﬂex ibility than previous methods and allows for different loss functions to be used. Thus, we enable learning for other forms such as multiclass classiﬁcation, multilabel classiﬁcation, and structured prediction. Our framework is stochastic and uses different forms of weak supervision, thus making it possi ble to train large scale models like deep neural networks. In summary, we make the following contributions : We deﬁne a general framework for adversarial label learn ing that uses multiple linear constraint forms. We develop a stochastic approach that enables training for largescale models, e.g., deep neural networks. Our algorithm provides greater ﬂexibility by solving a non zero sum game thus allowing learning of different forms. Our evaluation tests weak supervision provided in different forms, either as weak rules from human experts, heuristic patterns, or programmatically generated labels. We validate our approach on three image classiﬁcation datasets. We use error and precision constraints to solve mul ticlass image classiﬁcation tasks for deep neural network models. In the experiments, we provide weak supervision sig nals that are both generated by humans and programmaticallyarXiv:1906.00512v2  [cs.LG]  18 Sep 2019generated. Our results show that our approach outperforms stateoftheart methods on deep image classiﬁcation tasks. Our experiments also highlight the difﬁculties in providing adequate weak supervision signals for solving multiclass image classiﬁcation tasks. 2 Related Work "
399,DST: Data Selection and joint Training for Learning with Noisy Labels.txt,"Training a deep neural network heavily relies on a large amount of training
data with accurate annotations. To alleviate this problem, various methods have
been proposed to annotate the data automatically. However, automatically
generating annotations will inevitably yields noisy labels. In this paper, we
propose a Data Selection and joint Training (DST) method to automatically
select training samples with accurate annotations. Specifically, DST fits a
mixture model according to the original annotation as well as the predicted
label for each training sample, and the mixture model is utilized to
dynamically divide the training dataset into a correctly labeled dataset, a
correctly predicted set and a wrong dataset. Then, DST is trained with these
datasets in a supervised manner. Due to confirmation bias problem, we train the
two networks alternately, and each network is tasked to establish the data
division to teach another network. For each iteration, the correctly labeled
and predicted labels are reweighted respectively by the probabilities from the
mixture model, and a uniform distribution is used to generate the probabilities
of the wrong samples. Experiments on CIFAR-10, CIFAR-100 and Clothing1M
demonstrate that DST is the comparable or superior to the state-of-the-art
methods.","The remarkable success on training deep neural networks (DNNs) in various tasks relies on a largescale dataset with the correctly labels. However, labeling large amounts of data with highquality annotations is expensive and time consuming. Although there are some alternative and inex pensive methods such as crowdsourcing [ 36,39], online queries [ 4] and labelling samples with the annotator [ 27] that can annotate the largescale datasets easily to alleviate this problem, the samples with noisy labels are yielded by these alternative methods. A recent study [ 40] shows that a dataset with noisy labels can be overﬁtted by DNNs and *Corresponding author. (a) Warmup for 15 epochs  (b) DST for 50 epochs Figure 1. Distributions of the normalized loss on CIFAR10 with 80% symmetric noise. We use GMM to select samples for small loss and DST. Top: smallloss; bottom: DST. (a) 3007 samples (2816 correct samples) selected by smallloss and 7771 samples (6964 correct samples) selected by DST; (b) 10503 samples (9889 correct samples) selected by smallloss and 17897 samples (17054 correct samples) selected by DST. leads to poor generalization performance of the model. As this problem generally exists in the neural network training process and makes models get poor generalization, there are many algorithms developed for Learning with Noisy Labels(LNL). Some of methods attempt to estimate the latent noise transition matrix to express noisy labels and correct the loss function [ 8,19,21]. However, how to cor rectly construct noise transition matrix is challenging. Some researches modify labels correctly by predictions of models for improving the model performance [ 23,26]. Because of training labels from the DNN, the model would easily lead to overﬁtting under a high noise ratio. The recent research [ 1] adopts MixUp [ 41] to address this problem. Another ap proach reduces the inﬂuence of noise on the training process by selecting or weighting samples [ 24]. Many methods se lect clean samples with small loss [ 1,12]. Coteaching [ 9], 1arXiv:2103.00813v1  [cs.CV]  1 Mar 2021Coteaching +[38] and JoCoR [ 34] use two networks to select smallloss samples to train each other. Despite smallloss is a good method to choose correct samples from the noise samples, the samples which is pre dicted correctly by the models are ignored in training pro cess. In this work, we propose DST (Data Selection and joint Training), which can leverage correctly predicted samples and avoid overﬁtting new labels chosen by the model under a high level of the noise ratio. Compared with other methods using small loss [ 1,9,38], we propose another method based on two kinds of the loss on each sample to distinguish sam ples with correctly labels for training networks. We provide experiments to demonstrate the feasibility of our approach, which is superior to many related approaches. Our main contributions are as follows: •We propose two kinds of the sample loss (1. loss of the label from the dataset; 2. loss of the label predicted by model.), which can be used to distinguish correctly la beled and predicted samples. We ﬁt a Gaussian Mixture Model (GMM) dynamically on dataset loss distribu tion to divide the dataset into correctly labeled samples, correctly predicted samples and wrong samples with wrong labels and predictions. •We train two networks to generate losses of samples. For each network, we use GMM to get correct samples, which is then used to train another network. This can ﬁlter different types of error and avoid conﬁrmation bias in selftraining [16]. 2. Related Work "
95,SSR: An Efficient and Robust Framework for Learning with Unknown Label Noise.txt,"Despite the large progress in supervised learning with neural networks, there
are significant challenges in obtaining high-quality, large-scale and
accurately labelled datasets. In such a context, how to learn in the presence
of noisy labels has received more and more attention. As a relatively complex
problem, in order to achieve good results, current approaches often integrate
components from several fields, such as supervised learning, semi-supervised
learning, transfer learning and resulting in complicated methods. Furthermore,
they often make multiple assumptions about the type of noise of the data. This
affects the model robustness and limits its performance under different noise
conditions. In this paper, we consider a novel problem setting, Learning with
Unknown Label Noise}(LULN), that is, learning when both the degree and the type
of noise are unknown. Under this setting, unlike previous methods that often
introduce multiple assumptions and lead to complex solutions, we propose a
simple, efficient and robust framework named Sample Selection and
Relabelling(SSR), that with a minimal number of hyperparameters achieves SOTA
results in various conditions. At the heart of our method is a sample selection
and relabelling mechanism based on a non-parametric KNN classifier~(NPK) $g_q$
and a parametric model classifier~(PMC) $g_p$, respectively, to select the
clean samples and gradually relabel the noisy samples. Without bells and
whistles, such as model co-training, self-supervised pre-training and
semi-supervised learning, and with robustness concerning the settings of its
few hyper-parameters, our method significantly surpasses previous methods on
both CIFAR10/CIFAR100 with synthetic noise and real-world noisy datasets such
as WebVision, Clothing1M and ANIMAL-10N. Code is available at
https://github.com/MrChenFeng/SSR_BMVC2022.","It is now commonly accepted that supervised learning with deep neural networks can provide excellent solutions for a wide range of problems, so long as there is sufﬁcient availability of labelled training data and computational resources. However, these results have been mostly obtained using wellcurated datasets in which the labels are of high quality. In the real world, it is often costly to obtain highquality labels, especially for largescale datasets. A common © 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2111.11288v2  [cs.CV]  7 Oct 20222 CHEN ET AL: SSR: AN EFFICIENT AND ROBUST FRAMEWORK FOR LULN approach is to use semiautomatic methods to obtain the labels (e.g. “weblylabelled” images where the images and labels are obtained by webcrawling). While such methods can greatly reduce the time and cost of manual labelling, they also lead to lowquality noisy labels. In such settings, noise is one of the following two types: closedset noise where the true labels belong to one of the given classes (Set B in ﬁg. 1) and openset noise where the true labels do not belong to the set of labels of the classiﬁcation problem (Set C in ﬁg. 1). To deal with different types of noise, two main types of methods have been proposed, which we name here as probabilityconsistent methods and probabilityapproximate meth ods. Figure 1: Different “tigers”.Probabilityconsistent methods usually model noise patterns directly and propose corresponding proba bilistic adjustment techniques, e.g., robust loss func tions [10, 33, 44] and noise corrections based on noise transition matrix [11]. However, accurate modelling of noise patterns is nontrivial, and of ten cannot even model openset noise. Also, due to the necessary simpliﬁcations of probabilistic model ling, such methods often perform poorly with heavy and complex noise. More recently, probability approximate methods, that is methods that do not model the noise patterns explicitly become perhaps the dominant paradigm, especially ones that are based on sample selection. Earlier methods often reduce the inﬂuence of noise samples by selecting a clean subset and training only with it [12, 16, 22, 41]. Recent meth ods tend to further employ semisupervised learning methods, such as MixMatch [4], to fully explore the entire dataset by treating the selected clean subset as labelled samples and the nonselected subset as unlabeled samples [18, 24]. These methods, generally, do not consider the presence of openset noise in the dataset, since most current semisupervised learning methods can not deal with openset noise appropriately. To address this, several methods [27, 35] extend the sample selection idea by further identifying the openset noise and excluding it from the semisupervised training. In general, the above methods make assumptions about the pattern of the noise, such as the conﬁdence penalty speciﬁcally for asymmetric noise in DivideMix [18]. However, these mechanisms are often detrimental when the noise pattern does not meet the assumptions – for example, explicitly ﬁltering openset noise in the absence of openset noise may result in clean hard samples being removed. Furthermore, due to the complexity of combining multi ple modules, the above methods usually need to adjust complex hyperparameters according to the type and degree of noise. In this paper, we consider a novel problem setting — Learning with Unknown Label Noise (LULN ), that is, learning when both the degree and the type of noise are unknown. Striving for simplicity and robustness, we propose a simple method for LUNL , namely Sample Selection and Relabelling (SSR) (section 3.2), with two components that are clearly decoupled: a selection mechanism that identiﬁes clean samples with correct labels, and a relabelling mechanism that aims to recover correct labels of wrongly labelled noisy samples. These two major components are based on the two simple and necessary assumptions for LULN , namely, that samples with highlyconsistent annotations with their neighbours are often clean, and that very conﬁdent model predictions are often trustworthy. Once a well labelled subset is constructed this way we use the most basic supervised training scheme with a crossentropy loss. Optionally, a feature consistency loss can be used for all data soCHEN ET AL: SSR: AN EFFICIENT AND ROBUST FRAMEWORK FOR LULN 3 as to deal better with openset noise. Without bells and whistles, such as semisupervised learning, selfsupervised model pre training and model cotraining, our method is shown to be robust to the values of its very few hyperparameters through extensive experiments and ablation studies and to consistently outperform the stateoftheart by a large margin in various datasets. 2 Related Works "
321,Building a Competitive Associative Classifier.txt,"With the huge success of deep learning, other machine learning paradigms have
had to take back seat. Yet other models, particularly rule-based, are more
readable and explainable and can even be competitive when labelled data is not
abundant. However, most of the existing rule-based classifiers suffer from the
production of a large number of classification rules, affecting the model
readability. This hampers the classification accuracy as noisy rules might not
add any useful informationfor classification and also lead to longer
classification time. In this study, we propose SigD2 which uses a novel,
two-stage pruning strategy which prunes most of the noisy, redundant and
uninteresting rules and makes the classification model more accurate and
readable. To make SigDirect more competitive with the most prevalent but
uninterpretable machine learning-based classifiers like neural networks and
support vector machines, we propose bagging and boosting on the ensemble of the
SigDirect classifier. The results of the proposed algorithms are quite
promising and we are able to obtain a minimal set of statistically significant
rules for classification without jeopardizing the classification accuracy. We
use 15 UCI datasets and compare our approach with eight existing systems.The
SigD2 and boosted SigDirect (ACboost) ensemble model outperform various
state-of-the-art classifiers not only in terms of classification accuracy but
also in terms of the number of rules.","Classication is dened as a process of predicting the class label of new data points, given a set of labeled data points in the training set. The association rule mining is a rulebased approach that helps in identifying patterns in the data in the form of rules, by nding the relationships between the items in the dataset. The association rules are in the form X →Y, where X is the antecedent and Y is the consequent [1]. Associative classiers combine the concept of association rule mining and classication to build a classication model. In an associative classier, we choose the consequent of the rule to be the class label and the antecedent set is a set of attributevalue pairs for the associated class label. In the literature, various associative classiers have been proposed till now namely,arXiv:2007.01972v1  [cs.LG]  4 Jul 20202 Sood et al. CBA [18], CMAR [17], CPAR [20], ARC [3] etc. Although these classiers are easily understandable,  exible and do not assume independence among the at tributes, they require prior knowledge for choosing appropriate parameter values (support and condence). Furthermore, the rules generated may include noisy and meaningless rules, which might hinder the classication. A rule is said to be noisy if it does not add any new information for prediction and instead misleads the classication model. In other terms, a noisy rule would participate more often in misclassications than in correct classications. The authors in [16] proposed SigDirect, an associative classier which mines statistically signicant rules without the need for the support and condence values. However, in this paper, we propose SigD2 where we introduce a more ef fective two stage pruning strategy to obtain a more accurate classication model. The proposed method reduces the number of rules to be used for classication without compromising on the prediction performance. In fact, the performance is improved. Most of the prevalent supervised classication techniques like Ar ticial Neural Networks (ANN), Support Vector Machines (SVM) etc, although provide very high classication accuracy, they act as a black box. The models produced by such classiers are not straight forwardly explainable. However, the proposed associative classier makes the model more explainable by produc ing only a minimal set of classication association rules (CARs). The proposed technique nds its immense usage in various healthcare related applications, where the explanation of proposed models along with the classication accuracy are highly signicant [22]. In healthcare, incorrect predictions may have catas trophic eect, so doctors nd it hard to trust AI unless they can validate the obtained results. Furthermore, we also propose ACboost, which uses an ensemble of classica tion models obtained from the weak version of SigDirect, for boosting. Our goal is to strengthen the classier using less number of rules for prediction. Since, SigDirect is a strong learner and produces already a lesser number of rules for prediction, we form a weak version of SigDirect called wSigDirect, by further re ducing the number of rules to be used for classication as explained later in Sec tion 3. Moreover, in the proposed approach we use Adaboost [11] based boosting strategy over the ensemble of wSigDirect. The wSigDirect's classication model is learnt by running it multiple times on a reweighted data, thereafter performs voting over the learned classiers. We also propose ACbag which is dened as bagging on an ensemble of wSigDirect classiers. Motivated by the approach proposed by Breiman in [5], we use an ensemble model of wSigDirect classiers trained in parallel over dierent training datasets, and perform a majority vot ing over the ensemble for prediction. With the use of this strategy of combining weak learners, the goal is to decrease the variance in the prediction and improve the classication performance henceforth. It was found that for most of the datasets ACboost performs better than SigD2, ACbag, SVM, or ANN; ANN which performs similarly to deep neural network on these reasonably sized datasets. The main aim of this study is to make associative classiers more competitive and to highlight their signicanceBuilding a Competitive Associative Classier 3 as opposed to the other machine learning based classiers like neural networks which do not produce explainable predictions. Deep Learning has garnered all the attention lately, but the inability to produce transparent explanations for the decisions motivates us towards the domain of explainable articial intelligence using rulebased models which have fallen out of favour of late. Our contribution in this study is as follows: {We propose SigD2, an associative classier, which uses an eective two stage pruning strategy for pruning the rules to be used for classication. Using the proposed approach, the number of rules used for classication are reduced notably, without compromising on the classication performance. {We propose ACbag, an ensemble based classier founded on wSigDirect. {We also propose ACboost, which is boosting the wSigDirect classier, to im prove the classication accuracy with an explainable base model. Therefore, making SigDirect more competitive for classication tasks. The rest of the paper is organized as follows: Section 2 gives a literature review about some previously proposed associative classiers, Section 3 explains the methodologies we have adapted in SigD2, ACbag and ACboost, Section 4 shows the evaluation results of our proposed classier on UCI datasets and lastly, Section 5 gives the conclusion of the work and directions about future investi gations. 2 Related Work "
580,Adaptive Early-Learning Correction for Segmentation from Noisy Annotations.txt,"Deep learning in the presence of noisy annotations has been studied
extensively in classification, but much less in segmentation tasks. In this
work, we study the learning dynamics of deep segmentation networks trained on
inaccurately-annotated data. We discover a phenomenon that has been previously
reported in the context of classification: the networks tend to first fit the
clean pixel-level labels during an ""early-learning"" phase, before eventually
memorizing the false annotations. However, in contrast to classification,
memorization in segmentation does not arise simultaneously for all semantic
categories. Inspired by these findings, we propose a new method for
segmentation from noisy annotations with two key elements. First, we detect the
beginning of the memorization phase separately for each category during
training. This allows us to adaptively correct the noisy annotations in order
to exploit early learning. Second, we incorporate a regularization term that
enforces consistency across scales to boost robustness against annotation
noise. Our method outperforms standard approaches on a medical-imaging
segmentation task where noises are synthesized to mimic human annotation
errors. It also provides robustness to realistic noisy annotations present in
weakly-supervised semantic segmentation, achieving state-of-the-art results on
PASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE","Semantic segmentation is a fundamental problem in com puter vision. The goal is to assign a label to each pixel in an image, indicating its semantic category. Deep learn ing models based on convolutional neural networks (CNNs) achieve stateoftheart performance [9, 39, 51, 65]. These models are typically trained in a supervised fashion, which requires pixellevel annotations. Unfortunately, gathering pixellevel annotations is very costly, and may require signif icant domain expertise in some applications [17, 32, 40, 48]. *The ﬁrst two authors contribute equally, order decided by coin ﬂipping. 1Code is available at https://github.com/Kangningthu/ ADELEInput Ground Truth Baseline Baseline+ADELE Figure 1. Visualization of the segmentation results of the base line method SEAM [52] and the baseline combined with the pro posed ADaptive EarlyLearning corrEction (ADELE). Our pro posed ADELE improves segmentation quality. More examples can be found in Appendix A.1. Furthermore, annotation noise is inevitable in some appli cations. For example, in medical imaging, segmentation annotation may suffer from interreader annotation varia tions [22, 63]. Learning to perform semantic segmentation from noisy annotations is thus an important topic in practice. Prior works on learning from noisy labels focus on clas siﬁcation tasks [33, 46, 57]. There are comparatively fewer works on segmentation, where existing works focus on de signing noiserobust network architecture [50] or incorpo rating domain speciﬁc prior knowledge [42]. We instead focus on improving the performance in a more general per spective by studying the learning dynamics. We observe that the networks tend to ﬁrst ﬁt the clean annotations during an “earlylearning” phase, before eventually memorizing the false annotations, thus jeopardizing generalization perfor mance. This phenomenon has been reported in the context of classiﬁcation [33]. However, this phenomenon in seman tic segmentation differs signiﬁcantly from its counterpart in classiﬁcation in the following ways: •The noise in segmentation labels is often spatially de pendent. Therefore, it is beneﬁcial to leverage spatial information during training. 1arXiv:2110.03740v2  [cs.CV]  6 Mar 2022ImagesCategory Labels person   dog    sofabird Pixelwise initial annotationsCAM Segmentation modelClassification modelNoisy Segmentation Training imagesPixelwise noisy annotations bike Figure 2. A prevailing pipeline for training WSSS. We aim to improve the segmentation model from noisy annotations. •In semantic segmentation, early learning and memoriza tion do not occur simultaneously for all semantic cate gories due to pixelwise imbalanced labels. Previous meth ods [28,33] in noisy label classiﬁcation often assume class balanced data and thus either detecting or handling wrong labels for different classes at the same time. •The annotation noise in semantic segmentation can be ubiquitous (all examples have some errors) while the state oftheart methods in classiﬁcation [28,33,67] assume that some samples are completely clean. Inspired by these observations, we propose a new method, ADELE (ADaptive EarlyLearning corrEction), that is de signed for segmentation from noisy annotations. Our method detects the beginning of the memorization phase by monitor ing the Intersection over Union (IoU) curve for each category during training. This allows it to adaptively correct the noisy annotations in order to exploit earlylearning for individual classes. We also incorporate a regularization term to promote spatial consistency, which further improves the robustness of segmentation networks to annotation noise. To verify the effectiveness of our method, we consider a setting where noisy annotations are synthesized and con trollable. We also consider a practical setting – Weakly Supervised Semantic Segmentation (WSSS), which aims to perform segmentation based on weak supervision signals, such as imagelevel labels [24, 54], bounding box [11, 44], or scribbles [30]. We focus on a popular pipeline in WSSS. This pipeline consists of two steps (See Figure 2). First, a classiﬁcation model is used to generate pixellevel annota tions. This is often achieved by applying variations of Class Activation Maps (CAM) [66] combined with postprocessing techniques [3, 25]. Second, these pixellevel annotations are used to train a segmentation model (such as deeplabv1 [8]). Generated by a classiﬁcation model, the pixelwise anno tations supplied to the segmentation model are inevitably noisy, thus the second step is indeed a noisy segmentation problem. We therefore apply ADELE to the second step. In summary, our main contributions are:•We analyze the behavior of segmentation networks when trained with noisy pixellevel annotations. We show that the training dynamics can be separated into an early learning and a memorization stage in segmentation with annotation noise. Crucially, we discover that these dynam ics differ across each semantic category. •We propose a novel approach (ADELE) to perform se mantic segmentation with noisy pixellevel annotations, which exploits early learning by adaptively correcting the annotations using the model output. •We evaluate ADELE on the thoracic organ segmentation task where annotations are corrupted to resemble human errors. ADELE is able to avoid memorization, outper forming standard baselines. We also perform extensive experiments to study ADELE on various types and levels of noises. •ADELE achieves the state of the art on PASCAL VOC 2012 for WSSS. We show that ADELE can be combined with several different existing methods for extracting pixel level annotations [3,14,52] in WSSS, consistently improv ing the segmentation performance by a substantial margin. 2. Methodology "
516,Leaf Identification Using a Deep Convolutional Neural Network.txt,"Convolutional neural networks (CNNs) have become popular especially in
computer vision in the last few years because they achieved outstanding
performance on different tasks, such as image classifications. We propose a
nine-layer CNN for leaf identification using the famous Flavia and Foliage
datasets. Usually the supervised learning of deep CNNs requires huge datasets
for training. However, the used datasets contain only a few examples per plant
species. Therefore, we apply data augmentation and transfer learning to prevent
our network from overfitting. The trained CNNs achieve recognition rates above
99% on the Flavia and Foliage datasets, and slightly outperform current methods
for leaf classification.","Currently, supervised learning of convolutional neural networks (CNNs) for classiﬁca tion tasks achieve stateoftheart performances on a wide range of datasets, e.g. MNIST [13] and ImageNet [5]. Even though these datasets are usually huge in the amount of ex amples per class optimum values are mostly achieved by using data augmentations. The Flavia [22] and Foliage [9] datasets used in this paper include approximately 60 images per class in Flavia and 120 in Foliage, which is why data augmentation is extremely im portant to obtain a reliable generalization of the trained networks. Moreover, we apply further techniques that help prevent overﬁtting of the network. Those are dropout [20] and transfer learning which provides an initial guess for the weights of the network, e.g. [6]. As a result, the trained 9layer CNNs achieve outstanding recognition rates above 99% that also outperform slightly the current stateoftheart published by Sulc and Matas [21], who utilize a texturebased leaf identiﬁcation based on local feature histograms. The following paper is structured as follows. At ﬁrst, we introduce similar work which is also used to compare our results. Next, we present the model we used. This includes preprocessing, the network structure, batch generation, data augmentation, pre training, and the execution parameters of our experiments. Finally, we demonstrate the inﬂuence of augmentations, pretraining, and dropout on the accuracy and compare our results to other published values.arXiv:1712.00967v1  [cs.CV]  4 Dec 20172 Related Work "
568,Learning Robust Kernel Ensembles with Kernel Average Pooling.txt,"Model ensembles have long been used in machine learning to reduce the
variance in individual model predictions, making them more robust to input
perturbations. Pseudo-ensemble methods like dropout have also been commonly
used in deep learning models to improve generalization. However, the
application of these techniques to improve neural networks' robustness against
input perturbations remains underexplored. We introduce Kernel Average Pooling
(KAP), a neural network building block that applies the mean filter along the
kernel dimension of the layer activation tensor. We show that ensembles of
kernels with similar functionality naturally emerge in convolutional neural
networks equipped with KAP and trained with backpropagation. Moreover, we show
that when trained on inputs perturbed with additive Gaussian noise, KAP models
are remarkably robust against various forms of adversarial attacks. Empirical
evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show
substantial improvements in robustness against strong adversarial attacks such
as AutoAttack without training on any adversarial examples.","Model ensembles have long been used to improve robustness in the presence of noise. Classic methods like bagging Breiman (1996), boosting Freund (1995); Freund et al. (1996), and random forests Breiman (2001) are established approaches for reducing the variance in estimated prediction functions that build on the idea of constructing strong predictor models by combining many weaker ones. As a result, performance of these ensemble models (especially random forests) is surprisingly robust to noise variables (i.e. features) Hastie et al. (2009). Model ensembling has also been applied in deep learning Zhou et al. (2001); Agarwal et al. (2021); Liu et al. (2021); Wen et al. (2020); Horváth et al. (2022). However, the high computational cost of training multiple neural networks and averaging their outputs at test time can quickly become prohibitively expensive (also see work on averaging network weights across multiple finetuned versions Wortsman et al. (2022)). To tackle these challenges, alternative approaches have been proposed to allow learning pseudoensembles of models by allowing individual models within the ensemble to share parameters Bachman et al. (2014); Srivastava et al. (2014); Hinton et al. (2012); Goodfellow et al. (2013). Most notably, dropoutHinton et al. (2012); Srivastava et al. (2014) was introduced to approximate the process of combining exponentially many different neural networks by “dropping out” a portion of units from layers of the neural network for each batch. While these techniques often improve generalization for i.i.d. sample sets, they are not as effective in improving the network’s robustness against input perturbations and in particular against adversarial attacks 1arXiv:2210.00062v2  [cs.LG]  30 May 2023Wang et al. (2018). Adversarial attacks Szegedy et al. (2013); Biggio et al. (2013); Goodfellow et al. (2014), slight but carefully constructed input perturbations that can significantly impair the network’s performance, are one of the major challenges to the reliability of modern neural networks. Despite numerous works on this topic in recent years, the problem remains largely unsolved Kannan et al. (2018); Madry et al. (2017); Zhang et al. (2019); Sarkar et al. (2021); Pang et al. (2020); Bashivan et al. (2021); Rebuffi et al. (2021); Gowal et al. (2021). Moreover, the most effective empirical defense methods against adversarial attacks (e.g. adversarial training Madry et al. (2017) and TRADES Zhang et al. (2019)) are extremely computationally demanding (although see more recent work on reducing their computational cost Wong et al. (2019); Shafahi et al. (2019)). Our central premise in this work is that if ensembles can be learned at the level of features (the unit activity at the intermediate layers of the network; in contrast to class likelihoods), the resulting hierarchy of ensembles in the neural network could potentially lead to a much more robust classifier . To this end, we propose a simple method for learning ensembles of kernels in deep neural networks that significantly improves the network’s robustness against adversarial attacks. In contrast to prior methods such as dropoutthat focus on minimizing feature coadaptation and improving the individual features’ utility in the absence of others, our method focuses on learning feature ensembles that form local “committees” similar to those used in Boosting and Random Forests. To create these committees in layers of a neural network, we introduce the Kernel Average Pooling (KAP) operation that computes the average activity in nearby kernels within each layer – similar to how spatial Average Pooling layer computes the locally averaged activity within each spatial window, but instead along the kernel dimension. We show that incorporating KAP into convolutional networks leads to learning kernel ensembles that are topographically organized across the tensor dimensions over which the kernels are arranged (i.e. kernels are arranged in a vector or matrix according to their functional similarity). When such networks are trained on inputs perturbed by additive Gaussian noise, these networks demonstrate a substantial boost in robustness against adversarial attacks. In contrast to other ensemble approaches to adversarial robustness, our approach does not seek to train multiple independent neural network models and instead focuses on learning kernel ensembles within a single neural network. Moreover, compared to neural network robustness methods such as Adversarial Training Madry et al. (2017) and TRADES Zhang et al. (2019), training on Gaussian noise is about an order of magnitude more computationally efficient. Our contributions are as follows: •We introduce kernel average pooling as a simple method for learning kernel ensembles in deep neural networks. •We demonstrate how kernel average pooling leads to learning topographically organized kernel en sembles that in turn substantially improve model robustness against input noise. •Through extensive experiments on a wide range of benchmarks, we demonstrate the effectiveness of kernel average pooling on robustness against strong adversarial attacks. 2 Related works and background "
377,Contrastive Credibility Propagation for Reliable Semi-Supervised Learning.txt,"Inferencing unlabeled data from labeled data is an error-prone process.
Conventional neural network training is highly sensitive to supervision errors.
These two realities make semi-supervised learning (SSL) troublesome. In
practice, SSL approaches often fail to outperform their fully supervised
baseline. Proposed is a novel framework for deep SSL via transductive
pseudo-label refinement called Contrastive Credibility Propagation (CCP).
Through an iterative process of refining soft pseudo-labels, CCP unifies a
novel contrastive approach for generating pseudo-labels and a powerful
technique to overcome instance-dependent label noise. The result is an SSL
classification framework explicitly designed to overcome inevitable
pseudo-label errors. Using standard text and image benchmark classification
datasets, we show CCP reliably boosts or matches performance over a supervised
baseline in four common real-world SSL scenarios: few-label, open-set,
noisy-label, and class distribution misalignment.","Leveraging massive datasets that are only partially labeled is critical for countless applications (Wang et al., 2020; Zhou et al., 2022; Ding et al., 2022; de Carvalho et al., 2022; Gao et al., 2022; Liu et al., 2022; Lugo et al., 2022). Generating and utilizing pseudolabels for unlabeled data is a powerful but errorprone approach. Common issues often challenge the resulting classiﬁer’s ability to reliably perform better than a fully supervised baseline (Oliver et al., 2018). Some of these issues, illustrated in Fig. 1, and how our work addresses them, are described below. A) Error propagation: An incorrect pseudolabel can cause a cascade of future errors. In the original label propa 1AI Research, Palo Alto Networks, Santa Clara, CA. Corre spondence to: Brody Kutt <bkutt@paloaltonetworks.com >. A B CD E F?Figure 1: Six common issues of pseudolabeling algorithms. The color of the interior of each circle indicates the predicted class. The outline of each circle indicates the true class and black outlines indicate a labeled sample. Black arrows indicate label propagation inﬂuence. In C, repulsive (dark red) and attractive (green) arrows indicate gradient inﬂuence. InE, the background color indicates a decision surface. gation algorithm (LPA) (Zhu & Ghahramani, 2002), upon which several more recent techniques are based (Malhotra & Chug, 2021; Xie et al., 2011; Li et al., 2021a; Chin & Ratnavelu, 2017; Li et al., 2020; Dong et al., 2020), label information iteratively diffuses across unlabeled samples whether the initial propagation is correct or not. Our ap proach eliminates error propagation at the batch level by focusing exclusively on ﬁrstorder, i.e. singlehop, simi larities. Highorder propagation is made possible through multiple iterations of Algorithm 1. B) Ambiguity: The true class of unlabeled samples is sometimes unknowable e.g. if it is highly similar to two dif ferent classes. In a similar fashion to (Dong et al., 2020), our approach uses what we call credibility vectors (see Sec. 3.1), as opposed to traditional label vectors, to help reﬂect low prediction weight in this scenario. The output credibility vector for such a sample, qi, will feature nearzero scores for the equally similar classes and <0scores for every other class. qivalues of0induce no effect throughout the framework and nearzero values induce a minimal effect. In general, credibility adjustments help softly differentiate uncontested estimated similarities by giving them higher weight.arXiv:2211.09929v2  [cs.LG]  27 Jan 2023Contrastive Credibility Propagation for Reliable SemiSupervised Learning C) Inappropriate similarity metrics: Learned similarity metrics for SSL often suffer from conﬁrmation bias (Arazo et al., 2020). This occurs when a network is optimized to produce similarities corresponding to its label predictions. To combat conﬁrmation bias, the noisy results of the prop agation mechanism (Algorithm 2) do not affect our loss during the pseudolabel assignment. A single CCP itera tion consists of many epochs while minimizing our Soft Supervised Contrastive (SSC) loss (denoted LSSC) which is a generalization of Supervised Contrastive (SupCon) loss (Khosla et al., 2020) and its selfsupervised counterpart, SimCLR loss (Chen et al., 2020a;b). Only after the itera tion has concluded do we update our soft pseudolabels by averaging all predictions across epochs. Averaged predic tions capture network oscillations and inconsistencies across batches. Unlabeled samples inﬂuence LSSCand Algorithm 2 only via their averaged pseudolabels. This process, CCP’s outermost iteration, is modeled after an algorithm devel oped to correct instancedependent label noise called SEAL (Chen et al., 2021). We ﬁnd incorrect pseudolabels correct themselves across iterations in a similar fashion (Appendix C). D) Forced propagation: Pseudolabels are often forced to carry a certain total weight, usually through the use of a softmax function (Chen et al., 2022). This eliminates a prop agation mechanism’s ability to abstain from prediction even if the similarity to all classes is weak. Adjacency matrix row normalization in graphbased label propagation produces the same effect (Zhu & Ghahramani, 2002; Kamnitsas et al., 2018). In Algorithm 2, class similarities are all computed independently. Credibility adjustments do not enforce the pseudolabel to sum to any speciﬁc value. An unlabeled sample that has nearzero similarity to all classes will have a credibility vector of nearzero values. Through the use of weighted arithmetic means in LSSCand Algorithm 2, as well as a weighted classiﬁcation loss, these samples will have minimal inﬂuence on the framework. This is especially relevant in the presence of unlabeled data that belongs to no class (called openset SSL) which can cripple existing approaches (Oliver et al., 2018). The ability to abstain from prediction enables CCP to safely leverage all pseudolabels instead of simply discarding a percentage of the weakest pseudolabels as is common practice (Sohn et al., 2020; Li et al., 2021b; Zheng et al., 2022). E) Classiﬁcation sensitivity to label errors: Deep learn ing classiﬁers with conventional crossentropy loss are sen sitive to label errors (Song et al., 2020). Classiﬁers often directly inﬂuence pseudolabel generation while simultane ously learning from noisy pseudolabels (Yang et al., 2021; Lee et al., 2013; Zheng et al., 2022) which can cause an error cascade. Recent work has explored why contrastive representations boost robustness to label noise (Ghosh &Lan, 2021). In our approach, only softly supervised con trastive representations are used for pseudolabel generation. Further, subsampling pseudolabels between iterations and after the conclusion of CCP eliminates many erroneous or weakly predicted samples. We propose an unsupervised, principled subsampling approach in Sec. 3.3. F) Propagation sensitivity to similarity metrics: Pseudolabel accuracy is often at the mercy of your similarity metric. Similar to (Berthelot et al., 2019b), our approach averages predicted credibility vectors across transformed views of data for increased error resistance. Unlike other work, if two different predictions occur for two transformed views of a sample, even when each has high conﬁdence, the average credibilityadjusted pseudolabel will consist of all nearzero values. Unlike (Berthelot et al., 2019b), no sharpening takes place on the average vector to prevent forcing a pseudolabel with weak evidence. The evaluation of SSL algorithms often only explores the fewlabel learning scenario i.e. balanced reduction of the number of given labels. Our work expands upon this with three other common realworld SSL scenarios. These are openset (some unlabeled data belongs to no class), noisy label (some of the given labels are incorrect), and class distribution misalignment (labeled and unlabeled data have different class frequency distributions). Our contributions are as follows. To the best of our knowledge, CCP is the ﬁrst SSL algorithm to 1) Make use of credibility vectors as we deﬁne them to properly represent uncertainty. 2) Imbue a pseudolabeling strategy with an approach to overcome instancedependent label noise. 3) Introduce a generalizable, unsupervised strategy to choose a dynamic pseudolabel subsampling rate. 4) Demonstrate a reliable performance boost over a supervised baseline in four realworld SSL data scenarios with a single solution. 2. Related Work "
280,Dropout can Simulate Exponential Number of Models for Sample Selection Techniques.txt,"Following Coteaching, generally in the literature, two models are used in
sample selection based approaches for training with noisy labels. Meanwhile, it
is also well known that Dropout when present in a network trains an ensemble of
sub-networks. We show how to leverage this property of Dropout to train an
exponential number of shared models, by training a single model with Dropout.
We show how we can modify existing two model-based sample selection
methodologies to use an exponential number of shared models. Not only is it
more convenient to use a single model with Dropout, but this approach also
combines the natural benefits of Dropout with that of training an exponential
number of models, leading to improved results.","Noisy labels are ubiquitous in practice. For example, noise may appear due to disagreement in crowdsourc ing based annotation, Su et al. (2012), or annotations carried out by computer programs on web crawled images, Hu et al. (2017); Ratner et al. (2017). Conse quently, it is necessary to research techniques that are robust to noisy labeling. Multiple techniques have been developed to tackle this issue. Among them, sample selection is the one that we will focus on in this paper. Sample selection can be regarded as a derivative of curriculum learning, Bengio et al. (2009). In sample selection techniques a curriculum is deﬁned/learnt to select a subset of the data in each iteration of the training. MentorNet, Jiang et al. (2018), used a single network to select a minibatch in each iteration. Self paced MentorNet only considered samples with a loss lower than a certain threshold for training. Coteach ing, Han et al. (2018), upgraded the MentorNet byutilizing two Networks, where the minibatch used for training one network was decided by the loss obtained on the samples using the second network. Further, au thors of Coteachingplus , Yu et al. (2019), argued that thereshouldbedisagreementbetweenthetwonetworks, which can be beneﬁcial for the learning. Multiple tech niqueshavebeenproposedsincethentoperformsample selection based training. However, one general trend remains among these techniques, two networks are trained for sample selection Li et al. (2020b); Sachdeva et al. (2021); Feng et al. (2021); Wei et al. (2020a). We argue that instead of two networks, an exponential number of shared models can be utilized for sample selection. This can be achieved by utilizing Dropout, Srivastava et al. (2014), in a single network. Dropout when present in a network trains an ensemble of sub networks, thus, it can simulate an exponential number of shared models. We can utilize this property of Dropout to transform existing twomodel based ap proaches to utilize an exponential number of shared models by training just a single model with Dropout. This approach can combine the natural beneﬁts of Dropout, Srivastava et al. (2014); Gal and Ghahramani (2016) with the beneﬁts of training an exponential num ber of models for performing sample selection, resulting in improved performance for the existing approaches when transformed to use Dropout. Thus, our main contributions can be listed as follows •We propose to replace the two modelbased sample selection algorithms found in the literature with algorithms using an exponential number of shared models. •We show how an exponential number of models can be utilized for sample selection using Dropouts in a single network. •We provide empirical results by transforming ex isting approaches utilizing two models to use an exponential number of shared models with the help of Dropout. Our results suggest that sucharXiv:2202.13203v1  [cs.LG]  26 Feb 2022Dropout can Simulate Exponential Number of Models for Sample Selection Techniques transformations lead to better results. 2 Related Works "
248,DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling.txt,"Learning from noisy labels is a challenge that arises in many real-world
applications where training data can contain incorrect or corrupted labels.
When fine-tuning language models with noisy labels, models can easily overfit
the label noise, leading to decreased performance. Most existing methods for
learning from noisy labels use static input features for denoising, but these
methods are limited by the information they can provide on true label
distributions and can result in biased or incorrect predictions. In this work,
we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic
patterns in the embedding space during the fine-tuning process of language
models to improve noisy label predictions. DyGen uses the variational
auto-encoding framework to infer the posterior distributions of true labels
from noisy labels and training dynamics. Additionally, a co-regularization
mechanism is used to minimize the impact of potentially noisy labels and
priors. DyGen demonstrates an average accuracy improvement of 3.10% on two
synthetic noise datasets and 1.48% on three real-world noise datasets compared
to the previous state-of-the-art. Extensive experiments and analyses show the
effectiveness of each component in DyGen. Our code is available for
reproducibility on GitHub.","In many applications, collecting clean labeled data can be much more costly compared to obtaining noisy labeled data. Noisy labels can be cheaply obtained in large quantities from sources such as crowdsourcing [ 39,46], web annotations [ 8,28], labeling rules [ 11, 60], and search engines [ 51,58]. Using largescale noisy labeled data holds the potential of training powerful deep learning models with reduced data curation costs. Particularly, finetuning pretrained language models (PLMs) with noisy labels have gained interest for a wide range of text analysis tasks [ 1,41,65]. However, the over parameterized PLMs, due to their large size, are prone to overfitting the label noise, leading to decreased performance [ 3,9,63]. This has become a critical challenge that hinders PLMs from delivering satisfactory results when trained with noisy supervision. The problem of learning from noisy supervision has been widely studied in the machine learning community. Existing approachesarXiv:2305.19395v2  [cs.CL]  13 Jun 2023KDD ’23, August 6–10, 2023, Long Beach, CA, USA Yuchen Zhuang, Yue Yu, Lingkai Kong, Xiang Chen, & Chao Zhang to this issue can be broadly classified into three categories. 1) Data Cleaning methods [ 2,6,25,34,49,52,55,65] detect noisy samples using specific criteria such as Area Under Margin [ 37] and Data Cartography [ 41] and remove, reweigh, or correct these samples for subsequent model training. 2) Regularization methods design regularized loss functions [ 14,29,31,44,48,64] or train multiple models to regularize each other [ 15,16,42,45,55,65], with the goal of improving robustness under label noise. 3) Noise Transition Estimation methods [ 7,36,50,53,54,63] estimate the transition matrix𝑝(˜𝑦|𝑦,x)that maps clean labels 𝑦to noisy labels ˜𝑦, condi tioned on input features x. Noisy Prediction Calibration [ 5] is a recent approach that models the transition from noisy predictions to the true labels 𝑝(𝑦|ˆ𝑦,x). Each of these categories has its own advantages and drawbacks, and their performance depends on the specific nature of the noise and the input features being used. A major challenge in existing methods for learning from noisy supervision is their dependence on either the original input features or the embeddings learned with noisy labels. Both scenarios pose limitations when finetuning PLMs with noisy labels. First, the original input features xfrom PLMs have limited expressivity and therefore cannot effectively distinguish between noisy and clean labels [ 24]. This can hurt the efficiency of data cleaning methods and the models that learn the noisetotruth transitions based on x. Furthermore, the input features may hold some information about the true labels 𝑦, however, they only encompass a limited understanding of the relationship between the true labels 𝑦and the noisy labels ˜𝑦. This limitation leads to a reduced capability for generalization to all types of noise. Second, finetuning PLMs with noisy labels can also hinder the effectiveness of denoising, as label noise can compromise the quality of the learned embeddings. Overfitting to the label noise can cause the model to memorize incorrect labels and mistakenly consider some noisy samples as clean ones, even with metrics in regularization methods during finetuning. This also impedes noise transition estimation methods from accurately modeling the generation of noise. Consequently, many existing studies are grounded in strong assumptions or are hindered by imprecise noise estimation, resulting in inconsistent performance across varying types of label noise [40]. In this work, we have discovered that noisy and clean samples exhibit distinct behaviors in the embedding space during PLM fine tuning with noisy labels. During the early stages of finetuning, we found that the noisy samples tend to be closer to the cluster associ ated with the true label 𝑦. However, as training progresses, these noisy samples are gradually drawn towards the cluster associated with the assigned noisy label ˜𝑦. Therefore, the noisy samples tend to have relatively larger distances to their assigned label clusters due to this training dynamics pattern. Such dynamic patterns can be quantified by the Euclidean distance between each sample and its assigned cluster center at each training epoch. In Figure 1, we visualize the computed distance in the embedding space with the mean (yaxis) and standard deviation (xaxis) over epochs. This plot clearly illustrates that noisy samples tend to have larger means and standard deviations as they move from the true label cluster to the noisy label cluster during training. We thus propose a dynamicsenhanced generative model Dy Gen for denoised finetuning of PLMs. Our model is based on the observation that noisy and clean samples have different dynamicsin the embedding space during the finetuning process. To take advantage of this dynamic pattern, our model treats the true labels as latent variables and infers them from the dynamic patterns and the noisy labels. Our model differs from previous generative denois ing models [ 50,53,54] in its use of features and modeling of how the features and noisy labels are generated. Unlike these previous models, which generate both the noisy label ˆ𝑦and the input feature xconditioned on the true label 𝑦(𝑝(x,˜𝑦|𝑦)), our model leverages dynamic training patterns 𝑤and treats the true label 𝑦as the latent encoding of ˆ𝑦. This makes it easier to learn, as it only requires gen erating the noisy label ˆ𝑦, and allows for inference of the posterior 𝑝(𝑦|ˆ𝑦,w)using the variational autoencoding framework. Further more, we can use the discriminative power of the dynamic patterns to induce the prior distribution 𝑝(𝑦|w)of our generative model. To improve robustness in inferring the true label, we also employ a coregularization loss that encourages multiple branches of our generative model to reach a consensus for the posterior 𝑝(𝑦|ˆ𝑦,w). We have conducted thorough experiments on two datasets with various synthetic noise types and three datasets from the WRENCH benchmark [ 60] with realworld weak label noise. Our method Dy Gen consistently surpasses the stateoftheart baselines, with an average improvement of 2.13%across various noise types and ratios on both synthetic and realworld datasets. Furthermore, DyGen demonstrates remarkable robustness even under extreme label noise ratios, as high as 50%. Additionally, DyGen enhances model calibra tion by generating predicted probabilities that are more accurately aligned with the true label distribution due to its dynamicsbased probabilistic denoising approach. Our contributions are as follows: •We have discovered that dynamic training patterns in the hidden embedding space during PLM finetuning can effectively distin guish between clean and noisy samples. Utilizing this insight, we have devised a denoised finetuning approach for PLMs. To our knowledge, this is the first time that dynamic training patterns are used to achieve robust finetuning of PLMs with noisy labels. •We design a generative model that models the reconstruction of the noisy label ˆ𝑦from the latent true label 𝑦and the training dynamics w. We induce a prior distribution for the latent variable 𝑦based on the dynamics wand present a training procedure based on variational autoencoding. •To enhance robustness, we employ multiple branches that co regularize each other to reach consensus for the posterior 𝑝(𝑦|ˆ𝑦,w). •We have conducted a comprehensive analysis of the noisy learning problems in text data, covering both synthetic and realworld noise scenarios. Our proposed method, DyGen , consistently outperforms other approaches across different types and levels of noise. 2 RELATED WORK "
289,Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation.txt,"Unsupervised domain adaptation (UDA) requires source domain samples with
clean ground truth labels during training. Accurately labeling a large number
of source domain samples is time-consuming and laborious. An alternative is to
utilize samples with noisy labels for training. However, training with noisy
labels can greatly reduce the performance of UDA. In this paper, we address the
problem that learning UDA models only with access to noisy labels and propose a
novel method called robust local preserving and global aligning network
(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One
is learning a classifier by a robust informative-theoretic-based loss function.
The other is constructing two adjacency weight matrices and two negative weight
matrices by the proposed local preserving module to preserve the local topology
structures of input data. We conduct theoretical analysis on the robustness of
the proposed RLPGA and prove that the robust informative-theoretic-based loss
and the local preserving module are beneficial to reduce the empirical risk of
the target domain. A series of empirical studies show the effectiveness of our
proposed RLPGA.","UNSUPERVISED domain adaptation emphasizes [1], [2], [3] the problem of learning a classiﬁer that can be transferred across two domains. In general, the samples in the source domain are labeled, while the samples in the target domain are unlabeled. The main challenge in this research area is to reduce the difference between the probability distributions of two domains [4], [5], [6], [7]. To this end, the strategy based on discrepancy minimization has attracted much attention. Among them, adversarial learning methods have achieved remarkable performance improvements [8]. The training set of unsupervised adversarial domain adaptation models consists of two parts including the la beled source domain samples and unlabeled target domain samples. However, it is usually very expensive and tedious to accurately label large source domain training samples. An alternative way is to collect labels of samples from some crowdsourcing platforms in which the cost is cheaper and aW. Qiang and J. Li are with the University of Chinese Academy of Sciences, Beijing, China. They are also with the Science & Technology on Integrated Information System Laboratory, Institute of Software Chi nese Academy of Sciences, Beijing, China. Email: a01114115@163.com, Jiangmeng2019@iscas.ac.cn aC. Zheng is with the Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China. Email: changwen@iscas.ac.cn aB. Su is with the Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artiﬁcial Intelligence, Renmin Uni versity of China, Beijing, 100872, China. Email: subingats@gmail.com aH. Xiong is with the Hong Kong University of Science and Technology (Guangzhou). Email: xionghui@ust.hk. aThey have contributed equally to this work (Corresponding author: Bing Su). a©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promo tional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.easier but the obtained labels are always contaminated by noise. As a result, the performance of adversarial domain adaptation models learning from noisy labels will be de creased. One reason is that adversarial domain adaptation models usually learn the classiﬁer by minimizing the cross entropy loss. The crossentropy loss can be regarded as the distance between the outputs of the classiﬁer and the labels, so it is sensitive to label noises. That is to say, when a careless annotator tends to label positive class to negative class, then the distancebased loss would force adversarial domain adaptation to learn a classiﬁer who is more likely to output negative class than to output true label. However, to make the domain adaptive models robust to label noises, it is not enough to only employ robust classiﬁcation loss to train the models. The main reason is that robust loss can only reduce the impact of noisy labels, but can not completely eliminate it. Actually, for the source domain, learning with noisy labels can reduce the feature discriminability of samples in the latent space. This can lead to that a sample belonging to the one class is easy to be misclassiﬁed into another class. From a geometrically intuitive point of view, if a sample belonging to class i is incorrectly labeled as class j, then the gradient back propagation operation will force the sample to go from a place surrounded by many samples of the same type to a place surrounded by many different class samples. There fore, besides learning based on a robust classiﬁcation loss, designing an unsupervised method to maintain the local structure of the data distribution is also very important. To tackle these issues, we propose a novel method for adversarial domain adaptation named Robust Local Preserv ing and Global Aligning Network (RLPGA). RLPGA consists of three parts including a robust loss function for learning a classiﬁer, a local preserving module, and a global aligning module. Firstly, RLPGA projects samples of both domains into a latent space. Then, a robust informativetheoreticarXiv:2203.04156v1  [cs.CV]  8 Mar 2022SUBMITTED TO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 2 based loss function is minimized to learn the classiﬁer. The global aligning module minimizes the Wasserstein distance between source domain distribution and target domain dis tribution. The local preserving module constructs two ad jacency weight matrices and two negative weight matrices to encode the local topological relationship among samples and propose an objective function based on the graphs for local preserving. The major contributions of this paper are threefold: aA robust informativetheoreticbased loss function is proposed to measure the performance of the classiﬁer for adversarial domain adaptation. aTo reduce the effect of label noises from the per spective of the learned feature representation, we propose a new objective function for preserving lo cal neighbor topology based on constructing two adjacency weight matrices and two negative weight matrices. We jointly minimize the Wasserstein dis tance between two domain distributions and the new objective. In this way, the margins between different classes are enlarged and hence the learned features are more discriminative and robust. aWe provide theoretical analysis on the robustness of RLPGA, and prove that the robust loss and the enhanced feature discriminability are beneﬁcial to reduce the empirical risk in the target domain. 2 R ELATED WORKS "
406,A generic ensemble based deep convolutional neural network for semi-supervised medical image segmentation.txt,"Deep learning based image segmentation has achieved the state-of-the-art
performance in many medical applications such as lesion quantification, organ
detection, etc. However, most of the methods rely on supervised learning, which
require a large set of high-quality labeled data. Data annotation is generally
an extremely time-consuming process. To address this problem, we propose a
generic semi-supervised learning framework for image segmentation based on a
deep convolutional neural network (DCNN). An encoder-decoder based DCNN is
initially trained using a few annotated training samples. This initially
trained model is then copied into sub-models and improved iteratively using
random subsets of unlabeled data with pseudo labels generated from models
trained in the previous iteration. The number of sub-models is gradually
decreased to one in the final iteration. We evaluate the proposed method on a
public grand-challenge dataset for skin lesion segmentation. Our method is able
to significantly improve beyond fully supervised model learning by
incorporating unlabeled data.","Image semantic segmentation is a basic and crucial step for many biomedical image analysis tasks (e.g. tumour quan tiﬁcation). Nowadays, many encoderdecoder based deep convolutional neural networks (DCNNs) such as UNet [1] achieve stateoftheart performance for image segmentation using fullysupervised learning. However, data annotation is extremely time consuming especially for medical imaging where highly skilled expertise is required. Several methods have been proposed to address this chal lenge. Data augmentation is commonly used as an effective solution. A few studies show that geometric transformations and intensity shifts to increase the number of annotated data can achieve better performance than only using the original 1https://github.com/ruizhel/semisegmentationlabeled data [2]. Moreover, semisupervised learning meth ods that use both labeled and unlabeled data for model train ing have also been intensively studied. In 2015, Rasmus et al. [3] adapted the Ladder Network for simultaneously min imizing the sum of supervised and unsupervised cost func tions by backpropagation for image classiﬁcation. Baur et al. [4] proposed a semisupervised learning method based on fully convolutional networks (FCNs) and random feature embedding for image segmentation. More recently, Lahiri et al. [5] used the discriminator in generative adversarial net works (GANs) to both segment images and discriminate be tween real and generated fake images. Labeled images help to improve the segmentation accuracy, and unlabeled images are used to increase the discrimination power. However, this method only extracts the global information of images to im prove the ability to discriminate true and false samples but does not pay attention to the extraction of segmentation in formation. More closely related to our proposed method is [6]. They developed a networkbased semisupervised learn ing framework using selflearning techniques. A fully super vised model is ﬁrstly trained on labeled data, then pseudo la bels are generated for unlabeled data using this model and reﬁned by a fullyconnected conditional random ﬁeld (CRF). Subsequently, both labeled data and pseudo labeled data are used to reﬁne the initial model. This process is repeated un til convergence. However, the quality of the pseudo labels is highly dependent on the initial model and has a high impact on the ﬁnal performance. Similar to [6], in this paper, we also train an initial model using labeled data and subsequently reﬁne the model using unlabeled data with pseudo labels. Different from [6], we use an ensemble technique to reduce the negative inﬂuence of poor quality pseudo labels. Ensemble learning is a widely used technique in machine learning [7]. It trains multiple weak classiﬁers and then combines these weak decisions to generate a ﬁnal solution. Several classical classiﬁers (e.g. random forests [8]) are based on this idea to achieve state oftheart performance prior to the arrival of deep learning methods. Due to heavy computational load, very few studies have integrated ensemble learning with deep learning mod els. In 2017, Dolz et al [9] proposed a suggestive annotation model for infant brain MRI segmentation in a fully supervisedarXiv:2004.07995v1  [cs.CV]  16 Apr 2020Fig. 1 . Overview of the proposed framework. manner. It achieves the stateoftheart performance by com bining 10 CNNs. To the best of our knowledge, no previous studies have reported the use of ensemble learning and DCNN to achieve semisupervised learning. In this paper, we propose a DCNN based semisupervised learning framework that aims to learn a generic model that only uses a few annotated data with some unlabeled data for model training. We evaluate our framework based on a public skin lesion segmentation dataset, showing it outperforms both fully supervised learning method using only labeled data and Bai’s method [6] by a large margin. 2. METHODOLOGY "
217,Person Re-identification in the Wild.txt,"We present a novel large-scale dataset and comprehensive baselines for
end-to-end pedestrian detection and person recognition in raw video frames. Our
baselines address three issues: the performance of various combinations of
detectors and recognizers, mechanisms for pedestrian detection to help improve
overall re-identification accuracy and assessing the effectiveness of different
detectors for re-identification. We make three distinct contributions. First, a
new dataset, PRW, is introduced to evaluate Person Re-identification in the
Wild, using videos acquired through six synchronized cameras. It contains 932
identities and 11,816 frames in which pedestrians are annotated with their
bounding box positions and identities. Extensive benchmarking results are
presented on this dataset. Second, we show that pedestrian detection aids
re-identification through two simple yet effective improvements: a
discriminatively trained ID-discriminative Embedding (IDE) in the person
subspace using convolutional neural network (CNN) features and a Confidence
Weighted Similarity (CWS) metric that incorporates detection scores into
similarity measurement. Third, we derive insights in evaluating detector
performance for the particular scenario of accurate person re-identification.","Automated entry and retail systems at theme parks, pas senger ﬂow monitoring at airports, behavior analysis for automated driving and surveillance are a few applications where detection and recognition of persons across a cam era network can provide critical insights. Yet, these two problems have generally been studied in isolation within computer vision. Person reidentiﬁcation (reID) aims to ﬁnd occurrences of a query person ID in a video sequence, 1L. Zheng, H. Zhang and S. Sun contribute equally. This work was partially sup ported by the Google Faculty Award and the Data to Decisions Cooperative Research Centre. This work was supported in part to Dr. Qi Tian by ARO grant W911NF 1510290 and Faculty Research Gift Awards by NEC Laboratories of America and Blippar. This work was supported in part by National Science Foundation of China (NSFC) 61429201. Project page: http://www.liangzheng.com.cn Raw	video	framesGalleryDetection	result Cam	2,	3,…Cam	1 …(a)	Pedestrian	Detection	(b)	Person	Reidentification	Figure 1: Pipeline of an endtoend person reID system. It consists of two modules: pedestrian detection and person recognition (to differentiate from the overall reID). This pa per not only benchmarks both components, but also provides novel insights in their interactions. where stateoftheart datasets and methods start from pre deﬁned bounding boxes, either handdrawn [22, 25, 37] or automatically detected [21, 45]. On the other hand, sev eral pedestrian detectors achieve remarkable performance on benchmark datasets [12, 30], but little analysis is available on how they can be used for person reID. In this paper, we propose a dataset and baselines for practi cal person reID in the wild, which moves beyond sequential application of detection and recognition. In particular, we study three aspects of the problem that have not been con sidered in prior works. First, we analyze the effect of the combination of various detection and recognition methods on person reID accuracy. Second, we study whether detec tion can help improve reID accuracy and outline methods to do so. Third, we study choices for detectors that allow for maximal gains in reID accuracy. Current datasets lack annotations for such combined eval uation of person detection and reID. Pedestrian detection datasets, such as Caltech [10] or Inria [6], typically do not have ID annotations, especially from multiple cameras. On the other hand, person reID datasets, such as VIPeR [16] or CUHK03 [21], usually provide just cropped bounding boxes without the complete video frames, especially at a large scale. As a consequence, a largescale dataset that eval uates both detection and overall reID is needed. To address this, Section 3 presents a novel largescale dataset calledarXiv:1604.02531v2  [cs.CV]  6 Apr 2017PRW that consists of 932identities, with bounding boxes across 11;816frames. The dataset comes with annotations and extensive baselines to evaluate the impacts of detection and recognition methods on person reID accuracy. In Section 4, we leverage the volume of the PRW dataset to train stateoftheart detectors such as RCNN [15], with various convolutional neural network (CNN) architectures such as AlexNet [19], VGGNet [31] and ResidualNet [17]. Several wellknown descriptors and distance metrics are also considered for person reID. However, our joint setup al lows two further improvements in Section 4.2. First, we propose a cascaded ﬁnetuning strategy to make full use of the detection data provided by PRW, which results in im proved CNN embeddings. Two CNN variants, are derived w.r.t the ﬁne tuning strategies. Novel insights can be learned from the new ﬁnetuning method. Second, we propose a Conﬁdence Weighted Similarity (CWS) metric that incor porates detection scores. Assigning lower weights to false positive detections prevents a drop in reID accuracy due to the increase in gallery size with the use of detectors. Given a dataset like PRW that allows simultaneous eval uation of detection and reID, it is natural to consider whether any complementarity exists between the two tasks. For a particular reID method, it is intuitive that a bet ter detector should yield better accuracy. But we argue that the criteria for determining a detector as better are applicationdependent. Previous works in pedestrian de tection [10, 28, 43] usually use Average Precision or Log Average Miss Rate under IoU >0:5for evaluation. How ever, through extensive benchmarking on the proposed PRW dataset, we ﬁnd in Section 5 that IoU >0:7is a more effec tive rule in indicating detector inﬂuences on reID accuracy. In other words, the localization ability of detectors plays a critical role in reID. Figure 1 presents the pipeline of the endtoend reID system discussed in this paper. Starting from raw video frames, a gallery is created by pedestrian detectors. Given a query personofinterest, gallery bounding boxes are ranked according to their similarity with the query. To summarize, our main contributions are: A novel largescale dataset, Person Reidentiﬁcation in the Wild (PRW), for simultaneous analysis of person detection and reID. Comprehensive benchmarking of stateoftheart detec tion and recognition methods on the PRW dataset. Novel insights into how detection aids reID, along with an effective ﬁnetuning strategy and similarity measure to illustrate how they might be utilized. Novel insights into the evaluation of pedestrian detec tors for the speciﬁc application of person reID. Figure 2: Annotation interface. All appearing pedestrians are annotated with a bounding box and ID. ID ranges from 1 to 932, and 2 stands for ambiguous persons. 2. Related Work "
534,Ensemble Knowledge Distillation for Learning Improved and Efficient Networks.txt,"Ensemble models comprising of deep Convolutional Neural Networks (CNN) have
shown significant improvements in model generalization but at the cost of large
computation and memory requirements. In this paper, we present a framework for
learning compact CNN models with improved classification performance and model
generalization. For this, we propose a CNN architecture of a compact student
model with parallel branches which are trained using ground truth labels and
information from high capacity teacher networks in an ensemble learning
fashion. Our framework provides two main benefits: i) Distilling knowledge from
different teachers into the student network promotes heterogeneity in feature
learning at different branches of the student network and enables the network
to learn diverse solutions to the target problem. ii) Coupling the branches of
the student network through ensembling encourages collaboration and improves
the quality of the final predictions by reducing variance in the network
outputs. Experiments on the well established CIFAR-10 and CIFAR-100 datasets
show that our Ensemble Knowledge Distillation (EKD) improves classification
accuracy and model generalization especially in situations with limited
training data. Experiments also show that our EKD based compact networks
outperform in terms of mean accuracy on the test datasets compared to
state-of-the-art knowledge distillation based methods.","Ensemble methods have shown considerable improvements in model generalization and produced state of the art results in several ma chine learning competitions (e.g., Kaggle) [4]. These ensemble meth ods typically contain multiple deep Convolutional Neural Networks (CNN) as subnetworks which are pretrained on largescale datasets to extract discriminative features from the input data. The size of an ensemble is not constrained by training because the subnetworks can be trained independently, and their outputs can be computed in parallel. However, in many applications limited training data is not sufﬁcient to effectively train deep CNN models compared to small or compact networks. For instance in healthcare applications, the amount of available data is constrained by the number of patients. Therefore, improving generalization capability of compact network without requiring largescale annotated datasets is of utmost impor tance. Furthermore, today’s high performing deep CNN based en semble models have GigaFLOPS compute and GigaBytes storage requirements [12], making them prohibitive in resource constrained systems (e.g., mobile or edgedevices) which have stringent require ments on memory, latency and computational power. 1IBM Research Australia, email: umarasif@au1.ibm.com 2IBM Research Australia, email: jbtang@au1.ibm.com 3IBM Research Australia, email: sharrer@au1.ibm.comTo overcome these challenges, model compression techniques such as parameter pruning [24] is a common way to reduce model size with tradeoffs between accuracy and efﬁciency. Other tech niques include hand crafting efﬁcient CNN architectures such as SqueezeNets [13], MobileNets [11], and ShufﬂeNets [26]. Recently, neural network search showed an effective way to generate efﬁcient CNN architectures [21, 3] by extensively tuning parameters such as network width, depth, ﬁlter types and sizes. These models showed better efﬁciency than handcrafted networks but, at the cost of ex tremely large tuning cost. Another stream of work in building efﬁ cient networks for resource constrained scenarios is through Knowl edge distillation [10]. It enables small low memory footprint net works to mimic the behavior of large complex networks by training small networks using the predictions of large networks as soft labels in addition to the ground truth hard labels. In this paper we also explore Knowledge Distillation (KD) based strategies to improve model generalization and classiﬁcation perfor mance for applications with memory and compute restrictions. For this, we present a CNN architecture with parallel branches which dis till high level features from different teacher networks during train ing and maintains low computational overhead during inference. Our architecture provides two main beneﬁts: i)It combines a student network with different teacher networks and distills diverse feature representations into the student network during training. This pro motes heterogeneity in feature learning and enables the student net work to mimic diverse highlevel feature spaces produced by the teacher networks. ii)It combines the distilled information through parallelbranches in an ensembling manner. This reduces variance in the branchlevel outputs and improves the quality of the ﬁnal predic tions of the student network. In summary, the main contributions of this paper are as follows: 1. We present an Ensemble Knowledge Distillation (EKD) frame work which improves classiﬁcation performance and model gen eralization of small and compact networks by distilling knowledge from multiple teacher networks into a compact student network using an ensemble architecture. 2. We present a novel training objective function to distill ensemble knowledge into a single student network. Our objective function optimizes the parameters of the student network with a goal of learning mappings between input data and ground truth labels, and a goal of minimizing the difference between high level features of the teacher networks and the student network. 3. We perform ablation study of our framework on CIFAR10 and CIFAR100 datasets in terms of different CNN architectures, vary ing ensemble sizes, and limited training data scenarios. Experi ments show that by encouraging heterogeneity in feature learn ing through the proposed ensemble distillation, our EKDbasedarXiv:1909.08097v3  [cs.CV]  2 Apr 2020compact networks produce superior accuracy compared to the net works without using knowledge distillation. 2 Related Work "
500,CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise.txt,"In this paper, we study the problem of learning image classification models
with label noise. Existing approaches depending on human supervision are
generally not scalable as manually identifying correct or incorrect labels is
time-consuming, whereas approaches not relying on human supervision are
scalable but less effective. To reduce the amount of human supervision for
label noise cleaning, we introduce CleanNet, a joint neural embedding network,
which only requires a fraction of the classes being manually verified to
provide the knowledge of label noise that can be transferred to other classes.
We further integrate CleanNet and conventional convolutional neural network
classifier into one framework for image classification learning. We demonstrate
the effectiveness of the proposed algorithm on both of the label noise
detection task and the image classification on noisy data task on several
large-scale datasets. Experimental results show that CleanNet can reduce label
noise detection error rate on held-out classes where no human supervision
available by 41.5% compared to current weakly supervised methods. It also
achieves 47% of the performance gain of verifying all images with only 3.2%
images verified on an image classification task. Source code and dataset will
be available at kuanghuei.github.io/CleanNetProject.","One of the key factors that drive recent advances in large scale image recognition is massive collections of labeled images like ImageNet [5] and COCO [15]. However, it is normally expensive and timeconsuming to collect large scale manually labeled datasets. In practice, for fast de velopment of new image recognition tasks, a widely used surrogate is to automatically collect noisy labeled data from Internet [6, 11, 25]. Yet many studies have shown that label noise can affect accuracy of the induced classiﬁers signiﬁ cantly [7, 19, 22, 27], making it desirable to develop algo rithms for learning in presence of label noise. Work performed while working at Microsoft.Learning with label noise can be categorized by type of supervision: methods that rely on human supervision and methods that do not. For instance, some of the largescale training data were constructed using classiﬁers trained on manually veriﬁed seed images to remove label noise (e.g. LSUN [37] and Places [38]). Some studies for learning con volutional neural networks (CNNs) with noise also rely on manual labeling to estimate label confusion [20, 35]. The methods using human supervision exhibit a disadvantage in scalability as they require labeling effort for every class. For classiﬁcation tasks with millions of classes [4, 8], it is in feasible to have even one manual annotation per class. In contrast, methods without human supervision (e.g. model predictionsbased ﬁltering [7] and unsupervised outliers re moval [17, 24, 34]) are scalable but often less effective and more heuristic. Going with any of the existing approaches, either all the classes or none need to be manually veriﬁed. It is difﬁcult to have both scalability and effectiveness. In this work, we strive to reconcile this gap. We ob serve that one of the key ideas for learning from noisy data is ﬁnding “class prototypes” to effectively represent classes. Methods learn from manually veriﬁed seed images like [37] and methods assume majority correctness like [1] belong to this category. Inspired by this observation, we develop an attention mechanism that learns how to select representa tive seed images in a reference image set collected for each class with supervised information, and transfer the learned knowledge to other classes without explicit human super vision through transfer learning. This effectively addresses the scalability problem of the methods that rely on human supervision. Thus, we introduce “label cleaning network” (Clean Net), a novel neural architecture designed for this setting. First, we develop a reference set encoder with the atten tion mechanism to encode a set of reference images of a class to an embedding vector that represents that class. Sec ond, in parallel to reference set embedding, we also build a query embedding vector for each individual image and impose a matching constraint in training to require a query embedding to be similar to its class embedding if the query is relevant to its class. In other words, the model can tell 1arXiv:1711.07131v2  [cs.CV]  25 Mar 2018!"" Query #$%&'%,) $*+,*'%,'%,.'%,/01""Reference Set Encoder 0&"" Query Encoder 0*"" Reference Imagess3,4∈6%cos$%&,$*Cosine Similarity Loss,%,)& ,*01""Feature Extraction,%,&,%,.&,%,9&Reference Features,%,:&∈;%&<Figure 1. CleanNet architecture for learning a class embedding vectors cand a query embedding vector qwith a similarity matching constraint. There exists one class embedding for each of theLclasses. Details of component g()are depicted in Fig. 2. whether an image is mislabeled by comparing its query em bedding with its class embedding. Since class embeddings generated from different reference sets represents different classes where we wish the model to adapt to, CleanNet can generalize to classes without explicit human supervision. Fig. 1 illustrates the endtoend differentiable model. As the ﬁrst step of this work, we demonstrate that Clean Net is an effective tool for label noise detection. Simple thresholding based on the similarity between the reference set and the query image lead to good results compared with existing methods. Label noise detection not only is useful for training image classiﬁers with noisy data, but also has important values in applications like image search result ﬁl tering and linking images to knowledge graph entities. CleanNet predicts the relevance of an image to its noisy class label. Therefore, we propose to use CleanNet to assign weights to image samples according to the imagetolabel relevance to guide training of the image classiﬁer. On the other hand, as a better classiﬁer provides more discrimina tive convolutional image features for learning CleanNet, we refresh the CleanNet using the newly trained classiﬁer. We introduce a uniﬁed learning scheme to train the CleanNet and image classiﬁer jointly. To summarize, our contributions include a novel neural architecture CleanNet that is designed to make label noise detection and learning from noisy data with human super vision scalable through transfer learning. We also propose a uniﬁed scheme for training CleanNet and the image clas siﬁer with noisy data. We carried out comprehensive experimentation to evaluate our method for label noise detec tion and image classiﬁcation on three large datasets with realworld label noise: Clothing1M [35], WebVision [13], and Food101N. Food101N contains 310K images we col lected from Internet with the Food101 taxonomy [2], and we added “veriﬁcation label” that veriﬁes whether a noisy class label is correct for an image1. Experimental results show that CleanNet can reduce label noise detection er ror rate on heldout classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images veriﬁed on an image classiﬁcation task. 2. Related Work "
310,Image recognition from raw labels collected without annotators.txt,"Image classification problems are typically addressed by first collecting
examples with candidate labels, second cleaning the candidate labels manually,
and third training a deep neural network on the clean examples. The manual
labeling step is often the most expensive one as it requires workers to label
millions of images. In this paper we propose to work without any explicitly
labeled data by i) directly training the deep neural network on the noisy
candidate labels, and ii) early stopping the training to avoid overfitting.
With this procedure we exploit an intriguing property of standard
overparameterized convolutional neural networks trained with (stochastic)
gradient descent: Clean labels are fitted faster than noisy ones. We consider
two classification problems, a subset of ImageNet and CIFAR-10. For both, we
construct large candidate datasets without any explicit human annotations, that
only contain 10%-50% correctly labeled examples per class. We show that
training on the candidate examples and regularizing through early stopping
gives higher test performance for both problems than when training on the
original, clean data. This is possible because the candidate datasets contain a
huge number of clean examples, and, as we show in this paper, the noise
generated through the label collection process is not nearly as adversarial for
learning as the noise generated by randomly flipping labels.","Much of the recent success in building machine learning systems for image classication can be attributed to training deep neural networks on large, humanly annotated datasets, such as Ima geNet [Den+09] or CIFAR10 [TFF08]. However, assembling such datasets is timeconsuming and expensive: Both ImageNet and CIFAR10 were constructed by rst searching the web for candidate images and second labeling the candidate images by workers to obtain clean labels. The rst step is relatively inexpensive and already yields labeled examples, but the accuracy of those automatically collected, henceforth called candidate labels is low: For example, depending on each class, only about 10%50% of the candidate labels we automatically collected from Flickr (as described later) are correct, and only about half of the candidate labels obtained in the process of constructing the CIFAR10 dataset are correct [TFF08]. The second step in constructing such a dataset is expen sive because it requires either employing expert labelers or asking workers through a crowdsourcing platform for several annotations per image, and aggregating them to a clean label. 1arXiv:1910.09055v3  [cs.LG]  25 Feb 20201.1 Contributions In this paper we propose to train directly on the noisy candidate examples, eectively skipping the expensive human labeling step. To make this work, we exploit an intriguing property of large, overparameterized neural networks: If trained with stochastic gradient descent or variants thereof, neural networks t clean labels signicantly faster than noisy ones. This fact is well known, see for example the experiments by Zhang et al. [Zha+17, Fig. 1a]. What is notwell known is that this eect is suciently strong to enable training on candidate labels only. Our idea is that, if neural networks t clean labels faster than noise, then training them on a set containing clean and wrong labels and early stopping the training resembles training on the clean labels only. Our main nding is that early stopping the training on candidate examples can enable better image classication performance than when trained on clean labels, provided the total number of clean labels in the candidate training set is suciently large. We show this on CIFAR10 and ImageNet, two of the most widelyused image classication benchmarks [Ham17], with candidate training sets that we constructed without any human labeling step. This result questions the expensive practice of building clean humanly labeled training sets, and suggests that it can be better to collect larger, noisier datasets instead. Better ImageNet performance by learning from candidates: We demonstrate that we can achieve stateoftheart ImageNet classication performance on a subset of the original classes, without learning from the original clean labels. Specically, we chose 100 classes for which there are suciently many Flickr search results and for which there are no signicant semantic overlaps between the keywords and/or the search results. For those classes, we constructed a new candidate training set by collecting images from Flickr by keyword search, using the keywords of the ImageNet classes (from the WordNet hierarchy [Mil95]). We choose Flickr to collect candidate images because it enables image search unaltered by learning algorithms, unlike what a search engine like Google yields. Our results show that the performance of ResNet50 [He+16], a standard ImageNet benchmark network, improves signicantly by training and early stopping on the noisy candidate examples, compared to training on the original, high quality ImageNet training set. Specically, for the 100 class problem ResNet50 trained on the original training set achieves a top1 classication error of 15% whereas on our candidate training set, with early stopping, ResNet50 achieves a better error rate of 10.56%. Better CIFAR10 performance by learning from candidates: On CIFAR10, we achieve higher classication performance by training on candidate labels than any standard model achieves when trained on the original training set. The goal of the CIFAR10 classication problem is to classify 32x32 color images in 10 classes. The clean training set consists of 5000 images per class and was obtained by labeling the images from the Tiny Images dataset with expert workers. We constructed a new noisy training set consisting of candidate examples only, by picking the images from the Tiny Images dataset with the labels of the CIFAR10 classes, followed by cleaning to prevent any trivial (same images) or nontrivial (similar images) overlaps with the test set. Only about half of the examples in this candidate dataset are correctly labeled. We trained the best performing networks for CIFAR10, (ResNet [He+16], ShakeShake [Gas17], VGG [SZ15], DenseNet [Hua+17], PyramidNet [HKK17] and PNASNet [Liu+18]), and each model 2achieved signicantly higher performance with early stopping than when trained on the original, clean examples. In numbers, the best performing model trained on the CIFAR10 training set has 7% classi cation error on the CIFAR10.1 test set [Rec+19], while the best performing model trained on our candidate training set achieves 5.85%, with early stopping training of the PyramidNet110 model, lower than any standard model achieves when trained on the original training set. Early stopping is critical: Early stopping is critical to achieve the best performance. By keeping track of the performance of a large clean subset of the noisy training set, we show that the clean labels are tted signicantly faster than the false labels, and a good point to stop is when most of the clean labels are tted well, because at this point most of the false labels have not been tted yet. Limitations of learning from candidate labels: Whether our approach works depends on whether there is signicant overlap of the candidate labels; we demonstrate this by showing that when training on ImageNet classes that have a signicant semantic overlap, without labeling, yields obviously to high prediction errors for such classes. Real label noise is far from adversarial: Finally, we study the dierence between \real"" noise in the data obtained through the data collection process (from sources such as search engines and Flickr) and articially generated noise through randomly  ipping the labels of a clean dataset, which is often used in the literature as a proxy for the former. We nd that \real"" noise is more structured and therefore is easier to t and less harmful to the classication performance in contrast to articially generated noise, which is harder to t and more harmful. These ndings are consistent with those from the recent paper [JHY19], which quanties dierences of synthetic and real noise. 1.2 Related work "
606,Deep Visual Re-Identification with Confidence.txt,"Transportation systems often rely on understanding the flow of vehicles or
pedestrian. From traffic monitoring at the city scale, to commuters in train
terminals, recent progress in sensing technology make it possible to use
cameras to better understand the demand, i.e., better track moving agents
(e.g., vehicles and pedestrians). Whether the cameras are mounted on drones,
vehicles, or fixed in the built environments, they inevitably remain scatter.
We need to develop the technology to re-identify the same agents across images
captured from non-overlapping field-of-views, referred to as the visual
re-identification task. State-of-the-art methods learn a neural network based
representation trained with the cross-entropy loss function. We argue that such
loss function is not suited for the visual re-identification task hence propose
to model confidence in the representation learning framework. We show the
impact of our confidence-based learning framework with three methods: label
smoothing, confidence penalty, and deep variational information bottleneck.
They all show a boost in performance validating our claim. Our contribution is
generic to any agent of interest, i.e., vehicles or pedestrians, and outperform
highly specialized state-of-the-art methods across 5 datasets. The source code
and models are shared towards an open science mission.","An important goal of transportation research is to improve and provide efﬁcient public transportation systems that can accommodate many agents, whether it be vehicles or pedestrians, every day. This is especially important nowadays with the huge trafﬁc congestion costing billions of dollars [ 1]. As a result, research efforts have been directed towards management and control of vehicle and pedestrian ﬂows. Important prerequisites for such transportation network analysis are origindestination (OD) matrices, which allow researchers to understand a population’s trip demand. With the recent developments in new methods, such as datadriven methods [ 2], OD estimation have achievedarXiv:1906.04692v2  [cs.CV]  10 Sep 2020DestinationOrigin Destination 42% 15%OriginFigure 1: In this work, we present a new visual reidentiﬁcation method, i.e., whether agents (vehicles or pedestrians) captured in different images in nonoverlapping views belong to the same agent. Agents with the orange bounding box belong to the same agent. Note different agents can be visually very similar. good performance in various tasks of trafﬁc management. However, it still faces the issue of how representative the chosen samples are of the population. One way to deal with this problem is to collect more data from the population, which is expensive and timeconsuming when using traditional methods such as surveys or interviews. Another way is to make use of smartphones to collect such data [ 3]. Visual reidentiﬁcation is a faster and cheaper way to collect these data. Visual reidentiﬁcation is the task of associating images of the same agent taken from different cameras or from the same camera in different occasions. This is represented in Fig. 1. This task is nowadays possible due to the complex network of cameras already places in and around cities. Moreover, recent works have been pushing towards the use of drone technology to collect massive amounts of data in order to study the trafﬁc phenomena, such as the recently released pNEUMA dataset [ 4]. All these collected data can be used as inputs to a visual reidentiﬁcation model to associate different agents together and obtain their paths. The task of reidentiﬁcation (reid) has long been a task of extracting features/representations from two observations and measuring how similar these features are. Since different variations affect these features, many works have introduced different methods to improve their extraction. Initially, 2these features were handcrafted and include spatiotemporal information such as color, width and height, and salient edge histograms. Some work have also tried to use different input modalities such as depth [ 5,6,7], infrared [ 8], LiDAR [ 9], or Inductive Loop Detectors for vehicles [ 10]. These features, however, fail drastically when dealing with unexpected scenarios. To remedy this problem and with the advent of machine learning, researchers are now beneﬁting from the strength of deep learning to be able to extract more general and more discriminative features allowing them to reach high performance. Since then, an arms race of methods was built on top of this by making use of different objectspeciﬁc characteristics ( e.g., human semantic segmentation, pose) and by learning features through the supervision of a crossentropy loss. A main pitfall of learning with crossentropy supervision is the fact that it separates the different inputs solely based on the labels without taking into consideration the actual similarity between the inputs. None of the recent methods have tackled the problem where, even though two very similar agents are distinct, their similarity score should encode information about how similar they appear while also distinguishing them. The network usually tries its best to ﬁnd a boundary between the different classes even for inputs that are very similar. This leads the network to ﬁnd unreasonable explanations for the differences in labels and thus would negatively affect its generalizability. Since reid also deals with the problem of having a small set of images per class, it would aggravate this issue. Controlling the network’s conﬁdence in its predictions would alleviate this problem. To the best of your knowledge, this is the ﬁrst paper to apply this concept in the ﬁeld of reidentiﬁcation. In this paper, we propose to model conﬁdence when learning representations appropriate for reid. By inducing doubt while training a network, we are able to tackle the inherent problem discussed previously when crossentropy is used in a distance metric and representation learning problem such as person or vehicle reidentiﬁcation. Inspired by previous works that use uncertainty to regularize the network, we study three alternatives that aim at reducing the conﬁdence of the network and show a gain of 67 % in mAP across 5 different datasets. Although these methods have shown only a small improvement in other image classiﬁcation tasks [ 11,12,13], they drastically improve the performance of reid models due to its innate problem (Section 3). By combining our methods with advanced ranking methods, we outperform stateoftheart models without modeling additional characteristics speciﬁc to the object in question. The software is opensource and is made available online1. 2 Related Work "
167,Fused Deep Neural Network based Transfer Learning in Occluded Face Classification and Person re-Identification.txt,"Recent period of pandemic has brought person identification even with
occluded face image a great importance with increased number of mask usage.
This paper aims to recognize the occlusion of one of four types in face images.
Various transfer learning methods were tested, and the results show that
MobileNet V2 with Gated Recurrent Unit(GRU) performs better than any other
Transfer Learning methods, with a perfect accuracy of 99% in classification of
images as with or without occlusion and if with occlusion, then the type of
occlusion. In parallel, identifying the Region of interest from the device
captured image is done. This extracted Region of interest is utilised in face
identification. Such a face identification process is done using the ResNet
model with its Caffe implementation. To reduce the execution time, after the
face occlusion type was recognized the person was searched to confirm their
face image in the registered database. The face label of the person obtained
from both simultaneous processes was verified for their matching score. If the
matching score was above 90, the recognized label of the person was logged into
a file with their name, type of mask, date, and time of recognition.
MobileNetV2 is a lightweight framework which can also be used in embedded or
IoT devices to perform real time detection and identification in suspicious
areas of investigations using CCTV footages. When MobileNetV2 was combined with
GRU, a reliable accuracy was obtained. The data provided in the paper belong to
two categories, being either collected from Google Images for occlusion
classification, face recognition, and facial landmarks, or collected in
fieldwork. The motive behind this research is to identify and log person
details which could serve surveillance activities in society-based
e-governance.","Face masks have been made mandatory during those days of pandemic. This has hindered the  identification of individuals in surveillance systems. Research has taken its path in  identifying a person by their individual face features even upon wearing a medica l face mask.  Although the main focus of attention has been only on medical masks, there are other types  of face occlusion, which also gain equal importance in the identification of a person. The  research work has been carried out taking into consideration other types of occluded faces,  initially being limited to four types to launch the face recognition model. The occlusions 2   include medical masks, objects, scarves, hands. Occluded face recognition has become an  outstanding problem in the domain of image pro cessing and computer vision to favor e  governance systems. Face detection has more impact in face recognition where very high  precision is preferred in terms of surveillance. In spite of a drastic development in the domain  of computer vision and machine le arning, a lot of issues related to occluded face detection are  still to be addressed. This has become a high interest area for computer science, where the  focus is not only on static images but also on video frames. Very high accuracy in image  classificati on and object detection has already been achieved [1,2]. However, the detection of  face occlusions is an extremely challenging task for the existing models of face detection  [7,8,9,10]. Even though many methods have been proposed and many are in use for fa ce  detection and occlusion detection, challenges still exist when it comes to video surveillance.  Problems occurring in existing systems are poor datasets and sometimes the existence of  noise due to masks/occlusions. These issues have been studied using li mited datasets  [11,12,13], and the current challenge is to develop an efficient face occlusion detection model  working for a vast dataset.   The model for face area detection proposed in this paper is done using OpenCV Deep Neural  Network (DNN) [3], TensorFl ow [4], Keras [5], and MobileNetV2 architecture [6], which is  used as an image classifier. The proposed model can be integrated into surveillance systems  for better person recognition, even under occlusion.   The main contributions of the paper include :  i. Realtime face occlusion detection through OpenCV DNN. Even faces under  different orientations and occlusions can be detected and the proposed model  outperforms the previous ones.   ii. Person identification with occluded faces.   iii. The fusion of RNN and DNN models to improve the accuracy of existing models.  A state oftheart technique is proposed, namely MobileNetV2 [6] with GRU [14]   component, to provide a precise classification.     The rest of the paper is structured as follows: Section 2 discusses the rela ted work, detailing  the recent technologies for face recognition and person identification; Section 3 discusses the  proposed approach through a fusion between Deep neural network (MobileNetV2) and  Recurrent Neural Network (GRU) accompanied with the ResNet backbone based Caffe  implementation on face detection; Section 4 presents the results and their discussion,  followed by a conclusion and future suggestions in Section 5.      2. Related Work   "
395,ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks.txt,"To train robust deep neural networks (DNNs), we systematically study several
target modification approaches, which include output regularisation, self and
non-self label correction (LC). Two key issues are discovered: (1) Self LC is
the most appealing as it exploits its own knowledge and requires no extra
models. However, how to automatically decide the trust degree of a learner as
training goes is not well answered in the literature? (2) Some methods penalise
while the others reward low-entropy predictions, prompting us to ask which one
is better?
  To resolve the first issue, taking two well-accepted propositions--deep
neural networks learn meaningful patterns before fitting noise [3] and minimum
entropy regularisation principle [10]--we propose a novel end-to-end method
named ProSelfLC, which is designed according to learning time and entropy.
Specifically, given a data point, we progressively increase trust in its
predicted label distribution versus its annotated one if a model has been
trained for enough time and the prediction is of low entropy (high confidence).
For the second issue, according to ProSelfLC, we empirically prove that it is
better to redefine a meaningful low-entropy status and optimise the learner
toward it. This serves as a defence of entropy minimisation.
  We demonstrate the effectiveness of ProSelfLC through extensive experiments
in both clean and noisy settings. The source code is available at
https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021.
  Keywords: entropy minimisation, maximum entropy, confidence penalty, self
knowledge distillation, label correction, label noise, semi-supervised
learning, output regularisation","There exist many target (label) modiﬁcation approaches. They can be roughly divided into two groups: (1) Output regularisation (OR), which is proposed to penalise over conﬁdent predictions for regularising deep neural networks. It includes label smoothing (LS) [42, 29] and conﬁdence penalty (CP) [33]; (2) Label correction (LC). On the one *Prof. David A. Clifton was supported by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC).hand, LC regularises neural networks by adding the simi larity structure information over training classes into one hot label distributions so that the learning targets become structured and soft . On the other hand, it can correct the semantic classes of noisy label distributions. LC can be further divided into two subgroups: Nonself LC and Self LC. The former requires extra learners, while the latter re lies on the model itself. A typical approach of Nonself LC is knowledge distillation (KD), which exploits the pre dictions of other model(s), usually termed teacher(s) [17]. Self LC methods include PseudoLabel [23], bootstrapping (Bootsoft and Boothard) [35], Joint Optimisation (Joint soft and Jointhard) [43], and TfKD self[55]. According to an overview in Figure 1 (detailed derivation is in Section 3 and Table 1), in label modiﬁcation, the output target of a data point is deﬁned by combining a onehot label distribu tion and its corresponding prediction or a predeﬁned label distribution . Firstly, we present the drawbacks of existing approaches: (1) OR methods naively penalise conﬁdent outputs without leveraging easily accessible knowledge from other learners or itself (Figure 1a); (2) Nonself LC relies on accurate aux iliary models to generate predictions (Figure 1b). (3) Self LC is the most appealing because it exploits its own knowl edge and requires no extra learners. However, there is a core question that is not well answered: In Self LC, how much should we trust a learner to leverage its knowledge? As shown in Figure 1b, in Self LC, for a data point, we have two labels: a predeﬁned onehot qand a predicted structured p. Its learning target is (1"
514,Noisy Concurrent Training for Efficient Learning under Label Noise.txt,"Deep neural networks (DNNs) fail to learn effectively under label noise and
have been shown to memorize random labels which affect their generalization
performance. We consider learning in isolation, using one-hot encoded labels as
the sole source of supervision, and a lack of regularization to discourage
memorization as the major shortcomings of the standard training procedure.
Thus, we propose Noisy Concurrent Training (NCT) which leverages collaborative
learning to use the consensus between two models as an additional source of
supervision. Furthermore, inspired by trial-to-trial variability in the brain,
we propose a counter-intuitive regularization technique, target variability,
which entails randomly changing the labels of a percentage of training samples
in each batch as a deterrent to memorization and over-generalization in DNNs.
Target variability is applied independently to each model to keep them diverged
and avoid the confirmation bias. As DNNs tend to prioritize learning simple
patterns first before memorizing the noisy labels, we employ a dynamic learning
scheme whereby as the training progresses, the two models increasingly rely
more on their consensus. NCT also progressively increases the target
variability to avoid memorization in later stages. We demonstrate the
effectiveness of our approach on both synthetic and real-world noisy benchmark
datasets.","Much of the recent advances in deep learning can be attributed to supervised learning algorithms which require huge amounts of annotated data [6, 20]. However, manu ally annotating the data is laborious and usually expensive task [25] which can be prone to error when not veriﬁed by multiple annotators. Furthermore, to utilize the widespread opensource data, various techniques were proposed for au tomatically annotating the data using user tags and key words [22, 34] and scaling up crowdsourced datasets [24]. Equal contribution.Accepted as a conference paper at WACV 2021. 0 50 100 150 200012LossCE Clean Noisy 0 50 100 150 2001.52.02.5NCT 0 50 100 150 200 Epoch0255075100Accuracy (%) 0 50 100 150 200 Epoch0255075100Figure 1. Average crossentropy loss and accuracy on CIFAR10 with 50% symmetric label noise for the training samples with clean and noisy labels across the training epoch. Left: As training progresses standard model with crossentropy loss (CE) memorizes the noisy labels. Right: Our proposed method, noisy concurrent training (NCT) effectively pre vents the models from memorizing the noisy labels even though no dis tinction is made between them during training. While these approaches allow the creation of large datasets for training, they lead to noisy annotations. A number of studies have shown that label noise has an adverse effect on the performance of the models [8, 30, 38]. It is therefore pertinent to adapt the training procedure to leverage these datasets. Deep neural networks (DNNs) have been shown to easily ﬁt random labels [2] which makes it challenging to train the models efﬁciently. The majority of the existing methods for training under label noise can be broadly categorized into two approaches: i) correcting the labels by estimating the noise transition matrix [9, 26], ii) identifying the noisy la bels to either ﬁlter out [10, 37] or downweight those sam ples [13, 23]. However, the former approach depends on accurately estimating the noise transition matrix which is difﬁcult especially for a high number of classes, and the latter approach requires an efﬁcient method for identifying noisy labels and/or an estimate of the percentage of noisy instances. Amongst these, there has been more focus on separating the noisy and clean instances where a common criterion is to consider lowloss instances as a proxy for clean labels [1, 10]. However, harder instances can be per ceived as noisy and hence the model can be biased towards 1arXiv:2009.08325v1  [cs.CV]  17 Sep 2020easy instances. Both approaches consider the annotations quality as the primary reason for the decrease in model’s performance and hence the proposed solutions rely on accu rately relabeling, ﬁltering out or downweighting instances with incorrect labels. Here we provide an alternative viewpoint on the issue of learning with noisy labels and attempt to improve the ro bustness of the underlying training framework. We focus on the insufﬁciency of the standard training method. The crossentropy loss maximizes a bound on the mutual infor mation between onehot encoded labels and the learned rep resentation. The model receives no information about the similarity of a data point among the classes and hence when the provided label is incorrect, it has no source of useful information about the instance or extra supervision to mit igate the adverse effect of the noisy label. There is also a lack of regularization to discourage the model from memo rizing the training labels. To overcome these issues, we propose noisy concurrent training (NCT) which introduces variability in supervision signal in a collaborative learning framework and takes ad vantage of building consensus among two different models. Each model, in addition to a supervised learning loss, is trained with a mimicry loss that aligns the posterior distri butions of the two models for building consensus on the sec ondary class probabilities as well as the primary class pre diction. To discourage memorization, we derive inspiration from neuroscience where the role of noise in the nervous system has been extensively studied. Based on trialtotrial response variation in the brain [28] and the constructive role noise plays in forcing the biological neural networks to be more robust and explore more states [7], we propose to use a counterintuitive regularization technique we refer to as target variability as a deterrent to memorization and over generalization in DNNs. Speciﬁcally, target variability entails randomly changing the labels of a percentage of training samples in a batch, independently for each model. In addition to discourag ing memorization, this keeps the two models sufﬁciently di verged and therefore retains the beneﬁts of mutual learning, i.e. ﬁltering different types of errors and avoiding conﬁr mation bias in selftraining. Furthermore, since DNNs tend to learn simple patterns ﬁrst and memorize the noisy labels in the later epochs [2], NCT employs a dynamic learning scheme whereby as training progresses, the contribution of the supervised learning loss diminishes and the models fo cus more on building consensus. NCT also progressively increases the target variability to counter the higher ten dency of DNNs to memorize the noisy labels at the later stages. We show the efﬁcacy of our proposed approach on noisy versions of CIFAR10, CIFAR100 [14], and Tiny ImageNet [17] as well as two realworld noisy datasets Clothing1M [35] and WebVisionv1 [19]. Empirical resultsshow the versatility and effectiveness of NCT under differ ent noise types and noise levels. In addition to improving the performance of the model on noisy datasets, NCT also improves the performance on clean datasets which demon strates its utility as a generalpurpose robust learning frame work. 2. Related Work "
450,Apparent Age Estimation Using Ensemble of Deep Learning Models.txt,"In this paper, we address the problem of apparent age estimation. Different
from estimating the real age of individuals, in which each face image has a
single age label, in this problem, face images have multiple age labels,
corresponding to the ages perceived by the annotators, when they look at these
images. This provides an intriguing computer vision problem, since in generic
image or object classification tasks, it is typical to have a single ground
truth label per class. To account for multiple labels per image, instead of
using average age of the annotated face image as the class label, we have
grouped the face images that are within a specified age range. Using these age
groups and their age-shifted groupings, we have trained an ensemble of deep
learning models. Before feeding an input face image to a deep learning model,
five facial landmark points are detected and used for 2-D alignment. We have
employed and fine tuned convolutional neural networks (CNNs) that are based on
VGG-16 [24] architecture and pretrained on the IMDB-WIKI dataset [22]. The
outputs of these deep learning models are then combined to produce the final
estimation. Proposed method achieves 0.3668 error in the final ChaLearn LAP
2016 challenge test set [5].","Age estimation from face images is an interesting com puter vision problem. It has various applications, ranging from customer relations to biometrics and entertainment. Although, there are various studies on real age estimation, [7, 6, 26] to name a few, problem of apparent age estimation from face images is a recently introduced topic, which has received signiﬁcant attention [4, 20, 14, 22, 15]. Besides the common difﬁculties in facial image process ing and analysis, such as pose and illumination, the main challenges associated with age estimation have been the im Both authors contributed equally to this work. Figure 1. Sample images from the ChaLearn LAP 2016 apparent age estimation challenge dataset. Numbers below the images rep resent their average apparent age and standard deviations. The higher the standard deviation is, the harder it gets to predict appar ent age of a person. Especially, in old age group, standard devia tions are generally large, which makes it difﬁcult to perform accu rate age prediction from the images of old people. For instance, in LAP 2016 training set, there are 1095 images with standard devia tions less than three, and only 31 of them belong to someone older than 40. pact of ethnicity and subjective factors. The task of apparent age estimation alleviates the challenges posed by subjective factors. Since every person ages differently, real age may not be easy to deduce from face images. However, apparent age estimation is based on annotaters’ perception of subarXiv:1606.02909v1  [cs.CV]  9 Jun 2016jects’ ages, who are displayed in the images. Therefore, the judgments for the age labes are expected to be based on visual appearance cues rather than personal characteristics. On the other hand, these perceived ages are subjective and they depend on the annotators, leading to result in multiple age labels for the same face image. This is a very intrigu ing problem, considering that for object classiﬁcation prob lems, we normally have a single label per image. Sample images from the ChaLearn LAP 2016 dataset can be seen in Figure 1. In this study, we have proposed a novel approach for ap parent age estimation, which addresses the problem of im precise, uncertain multiple labels by grouping the face im ages that are within a speciﬁed age range, and by training an ensemble of deep learning models using these age groups and their ageshifted groupings. Convolutional Neural Networks (CNNs) have shown sig niﬁcant performance improvement in several computer vi sion problems, such as image classiﬁcation [23], object de tection [23], image segmentation [30], and face recognition [19]. Moreover, all the best performing systems proposed in ChaLearn LAP 2015 [4] were based on CNNs [22, 15, 31]. Due to these reasons, we have opted for CNNs for the pro posed system and have conducted a thorough study to efﬁ ciently transfer already existing models for the problem at hand. In addition, ensemble models are known to increase performance further. To beneﬁt from this, we have gener ated different age groupings and train multiple CNN mod els. Finally, we have combined the outputs of these CNN models to produce the ﬁnal estimation result. The contributions of this study can be summarized as follows: (i) We proposed an apparent age estimation sys tem that takes into account the imprecise, uncertain multi ple labels available for the task. Instead of using avarage age labels as class labels, we have grouped the face im ages that are within a speciﬁed age range. An ensemble of CNNs have been trained using these age groups and their ageshifted groupings. (ii) We have conducted an exten sive assessment about transferability of existing CNN mod els for apparent age estimation. (iii) We have analyzed the apparent age estimation problem in detail and pointed the challenges associated to it. The rest of the paper is organized as follows: In Sec tion 2, a brief overview of related work is provided. In Sec tion 3, the problem is stated and the challenges associated to it are pointed. The proposed method is explained in detail in Section 4. Experimental results are presented and discussed in Section 5. Finally, in Section 6, the paper is concluded with a brief summary and discussion. 2. Related Work "
12,Deep metric learning for multi-labelled radiographs.txt,"Many radiological studies can reveal the presence of several co-existing
abnormalities, each one represented by a distinct visual pattern. In this
article we address the problem of learning a distance metric for plain
radiographs that captures a notion of ""radiological similarity"": two chest
radiographs are considered to be similar if they share similar abnormalities.
Deep convolutional neural networks (DCNs) are used to learn a low-dimensional
embedding for the radiographs that is equipped with the desired metric. Two
loss functions are proposed to deal with multi-labelled images and potentially
noisy labels. We report on a large-scale study involving over 745,000 chest
radiographs whose labels were automatically extracted from free-text
radiological reports through a natural language processing system. Using 4,500
validated exams, we demonstrate that the methodology performs satisfactorily on
clustering and image retrieval tasks. Remarkably, the learned metric separates
normal exams from those having radiological abnormalities.","Chest radiographs are performed to diagnose and monitor a wide range of conditions aecting lungs, heart, bones, and Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). SAC 2018April 9–13, 2018, Pau, France Copyright held by the owner/author(s). ACM 9781450351911/18/04. DOI: https://doi.org/10.1145/3167132.3167379soft tissues. Despite being commonly performed, their read ing is challenging and interpretation discrepancies can occur. There is a need to develop machine learning algorithms that can assist the reporting radiologist. In this work we address the problem of learning a distance metric for chest radio graphs using a very large repository of historical exams that have already been reported. An ideal metric should be able to cluster together radiographs presenting similar radiologi cal abnormalities and place them far away from exams with normal radiological appearance. Learning a suitable met ric would enable a variety of applications, from automated retrieval of radiologically similar exams, for teaching and training, to their automated prioritization based on visual patterns. The problem we discuss here is challenging for several rea sons. First, the number of potential abnormalities that can be observed in a chest radiograph can be quite large. Visual patterns detected in radiographs are important cues used by the clinicians when making a diagnosis. Often, during the reporting time, the clinician will describe the visual pattern using descriptors (e.g. \enlarged heart"") or stating the exact medical pathology associated with the visual pattern (e.g. \consolidation in the right lower lobe""). A metric learning algorithm should be able to deal with any such labels and their potential overlaps. Second, the labels may not always be accurate or comprehensive due to the fact that not all the abnormalities are always reported in an image, e.g. due to omissions or when deemed unimportant by the radiologist. When these labels are automatically obtained from freetext reports, as we do in this work, mislabelling errors may also occur. Third, certain abnormalities are less frequently ob served than others, and may not even exist in the training dataset. To support this study, we have prepared a large repository consisting of over 745 ;000 chest radiograph examinations extracted from the PACS (Picture Archiving and Commu nication System) of a large teaching hospital in London. To our knowledge, this is the largest chest radiograph reposi tory to ever be deployed in a machine learning study. Due to the large sample size, manual annotation of all the exams is unfeasible. All the historical freetext reports have been parsed using a Natural Language Processing (NLP) system, which has identied and classied any mention of radiologi cal abnormalities. As a result of this process, each lm has been automatically assigned to one or multiple labels. Our contributions are the following. First, we discuss the probarXiv:1712.07682v1  [stat.ML]  11 Dec 2017A1  The lungs and pleural spaces are  clear. No pneumothorax. The heart is  not enlarged.  Wrong report refers to another xray!  A2  Large leftsided pleural effusion  with almost complete collapse of left  lower lobe. Rightsided thoracostomy  tube.  B1  The heart size is at the upper limits  of normal, the lungs are clear.  B2  The heart is enlarged.  No active  lung lesion.  Figure 1: Examples of pairs of images that are placed close to each other in the learned embedding space shown in Fig. 3. A1 was incorrectly reported, but a second reading shows the presence of pleural eusion and a medical device, which justies its proximity to A2. B1 was labelled as \normal"", but a second reading reveals some degree of cardiomegaly and, as such, the scan is placed close to B2. An extract from the original reports can be found under each image. Fig. 3 contains the legend for the labels. lem of deep metric learning with multilabelled images and propose two versions of a loss function specically designed to deal with overlapping and potentially noisy labels. At the core of the architecture, a DCN is used to learn compact im age representations capturing the visual patterns described by the labels. Second, we report on a largescale evaluation of the proposed methodology using a manually curated sub set of over 4 ;500 exams. Each historical radiological report was reviewed by two independent clinicians who extracted all the labels associated to the lms. We report on compar ative results for two tasks, clustering and image retrieval, and provide evidence that the learned metric can be used to cluster radiographs with a normal appearance as well as clusters of abnormal exams with cooccurring abnormalities. 2. RELATED WORK "
99,Capturing cross-session neural population variability through self-supervised identification of consistent neuron ensembles.txt,"Decoding stimuli or behaviour from recorded neural activity is a common
approach to interrogate brain function in research, and an essential part of
brain-computer and brain-machine interfaces. Reliable decoding even from small
neural populations is possible because high dimensional neural population
activity typically occupies low dimensional manifolds that are discoverable
with suitable latent variable models. Over time however, drifts in activity of
individual neurons and instabilities in neural recording devices can be
substantial, making stable decoding over days and weeks impractical. While this
drift cannot be predicted on an individual neuron level, population level
variations over consecutive recording sessions such as differing sets of
neurons and varying permutations of consistent neurons in recorded data may be
learnable when the underlying manifold is stable over time. Classification of
consistent versus unfamiliar neurons across sessions and accounting for
deviations in the order of consistent recording neurons in recording datasets
over sessions of recordings may then maintain decoding performance. In this
work we show that self-supervised training of a deep neural network can be used
to compensate for this inter-session variability. As a result, a sequential
autoencoding model can maintain state-of-the-art behaviour decoding performance
for completely unseen recording sessions several days into the future. Our
approach only requires a single recording session for training the model, and
is a step towards reliable, recalibration-free brain computer interfaces.","Neural decoders require stable neurons in a recorded population in order to accurately predict behaviour such as movement or to allow decoding of stimuli. However, over time instabilities in the recording equipment and drift in neural activity lead to instabilities that prevent reusing a decoder trained on one day for a session recorded on another day [Huber et al., 2012, Ziv et al., 2013, Driscoll et al., 2017]. At the same time, neural population activity is highly structured and often conﬁned to lowdimensional manifolds [Cunningham and Byron, 2014] that can be recovered using latent variable modelling approaches [Hurwitz et al., 2021]. Importantly, recent work showed that movementrelated latent neural dynamics in population activity from the primate motor cortex is stable and could be recovered over intervals as long as two years [Gallego et al., 2020]. This suggests that despite the variability at the level of single neurons, in each session a subset of neuronsarXiv:2205.09829v2  [qbio.NC]  5 Jan 2023will remain informative about behaviour. A stable crosssession decoder therefore has to be able to identify these neurons and utilise them for decoding. Therefore, here we focus on identifying known recording neurons in unseen sessions. In particular, we hypothesised that a latent encoding of neural activity can be augmented by information about which neurons were seen during training, and at which position in the input. We show that this is sufﬁcient to decode behaviour (in our case different cued arm movements by a monkey with simultaneous motor cortex recordings) with high accuracy across unseen sessions. We achieve this with a selfsupervised approach through training a recurrent neural network (RNN) to predict original neuron positions following data perturbation in a manner mirroring session to session variability. In essence, the closer our perturbations mimic real intersession variability (as shown in Figure 1), the higher our behaviour prediction performance on an unseen session. These perturbations include adding spikes to existing neurons from randomly generated neurons, removing spikes from existing neurons, shifting the entire neuron population by a constant amount, slightly shifting neurons in time, replacing neurons with randomly generated neurons and eliminating neurons entirely. Original Neurons Lost Neurons Replaced Probe Array Shift Neurons Move Neurons Added Figure 1: Intersession ensemble variability possible when recording from neural populations. Neu rons from the original recording session can be lost to the recording array, new neurons can become visible, neurons can move between electrodes, original neurons can be replaced by unseen neurons and the entire probe array can shift, causing a systematic change in neuron position. In addition, spike sorting can induce variability as the signal to noise ratio of individual neurons changes between sessions. The perturbations we apply to each trial of recordings is in response to each of these sources of variability. We model each unseen test trial as an instance of a perturbed seen train trial and subsequently, our sequential autoencoder model attempts to map each unseen trial to a known trial. This neuron locator RNN is trained to predict original neuron position within a single recording session from many perturbed variations of trials of this training session. Once trained to predict original neuron positions, a separate network, which in this case is a sequential autoencoder based on Latent Factor Analysis via Dynamical Systems (LFADS) [Pandarinath et al., 2017], is trained to predict original unperturbed neural recording trials from perturbed variations of trials from the same session. The encoder of this sequential autoencoder receives as additional input the embedding of the neuron locator RNN activations, conditioning the encoder to produce latent variables which are informative enough to accurately reconstruct the original recording. The encoder produces latent variables which are separated by behaviour (arm movement direction) in a selfsupervised manner, from which behaviour can be predicted without the model being explicitly trained on behaviour. Importantly, the joint neuron locator RNN and LFADS encoder ensemble can predict behaviourally relevant latent variables for unseen recording sessions that yield high decoding accuracy. Currently, there are no existing approaches to accurately predict behaviour from an unseen recording session when training on just one single session. We not only show this is possible with our method, but that our approach is robust to intersession variability for up to 8 days when a sufﬁcient number of neurons are persistent across sessions. 2 Related Work "
424,SENT: Sentence-level Distant Relation Extraction via Negative Training.txt,"Distant supervision for relation extraction provides uniform bag labels for
each sentence inside the bag, while accurate sentence labels are important for
downstream applications that need the exact relation type. Directly using bag
labels for sentence-level training will introduce much noise, thus severely
degrading performance. In this work, we propose the use of negative training
(NT), in which a model is trained using complementary labels regarding that
``the instance does not belong to these complementary labels"". Since the
probability of selecting a true label as a complementary label is low, NT
provides less noisy information. Furthermore, the model trained with NT is able
to separate the noisy data from the training data. Based on NT, we propose a
sentence-level framework, SENT, for distant relation extraction. SENT not only
filters the noisy data to construct a cleaner dataset, but also performs a
re-labeling process to transform the noisy data into useful training data, thus
further benefiting the model's performance. Experimental results show the
significant improvement of the proposed method over previous methods on
sentence-level evaluation and de-noise effect.","Relation extraction (RE), which aims to extract the relation between entity pairs from unstructured text, is a fundamental task in natural language processing. The extracted relation facts can beneﬁt various downstream applications, e.g., knowledge graph completion (Bordes et al., 2013; Wang et al., 2014), information extraction (Wu and Weld, 2010) and question answering (Yao and Van Durme, 2014; Fader et al., 2014). A signiﬁcant challenge for relation extraction is the lack of largescale labeled data. Thus, distant  Corresponding authors. ? ? Obama is the 44thpresident of the United States .①Place_of_birth ②Employee_of ①Place_of_birth ②Employee_of Lived_in (unincluded label)The sentence bag of <Obama, United States>Which   label  ? Obama was back to the United States yesterday.ObamaUnited StatesObama was born in the  United States .Bag labels (We need) Sentence labelsFigure 1: Two types of noise exist in baglevel labels: 1) Multilabel noise: the exact label (“place ofbirth” or “employee of”) for each sentence is unclear; 2) Wronglabel noise: the third sentence inside the bag actually expresses “live in” which is not included in the bag labels. supervision (Mintz et al., 2009) is proposed to gather training data through automatic alignment between a database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multi instance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classiﬁed into two categories: 1) the soft denoise methods that leverage soft weights to differentiate the inﬂuence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard denoise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these baglevel approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentence level relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentencelevel relation ex traction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng et al. (2018)) have made efforts on sentencelevel (or instancelevel)arXiv:2106.11566v1  [cs.CL]  22 Jun 2021distant RE, empirically verifying the deﬁciency of baglevel methods on sentencelevel evaluation. However, the instance selection approaches of these methods depend on rewards(Feng et al., 2018) or frequent patterns(Jia et al., 2019) determined by baglevel labels, which contain much noise. For one thing, one bag might be assigned to multiple bag labels, leading to difﬁculties in onetoone mapping between sentences and labels. As shown in Fig.1, we have no access to the exact relation between “place ofbirth” and “employee of” for the sentence “Obama was born in the United States.”. For another, the sentences inside a bag might not express the bag relations. In Fig.1, the sentence “Obama was back to the United States yesterday” actually express the relation “live in”, which is not included in the bag labels. In this work, we propose the use of negative training (NT) (Kim et al., 2019) for distant RE. Different from positive training (PT), NT trains a model by selecting the complementary labels of the given label, regarding that “the input sentence does not belong to this complementary label”. Since the probability of selecting a true label as a complementary label is low, NT decreases the risk of providing noisy information and prevents the model from overﬁtting the noisy data. Moreover, the model trained with NT is able to separate the noisy data from the training data (a histogram in Fig.3 shows the separated data distribution during NT). Based on NT, we propose SENT, a sentence level framework for distant RE. During SENT training, the noisy instances are not only ﬁltered with a noiseﬁltering strategy, but also transformed into useful training data with a relabeling method. We further design an iterative training algorithm to take full advantage of these datareﬁning processes, which signiﬁcantly boost performance. Our codes are publicly available at Github1. To summarize the contribution of this work: •We propose the use of negative training for sentencelevel distant RE, which greatly protects the model from noisy information. •We present a sentencelevel framework, SENT, which includes a noiseﬁltering and a relabeling strategy for reﬁning distant data. •The proposed method achieves signiﬁcant improvement over previous methods in terms of both RE performance and denoise effect. 1https://github.com/rtmaww/SENT2 Related Work "
506,Bag of Tricks for Developing Diabetic Retinopathy Analysis Framework to Overcome Data Scarcity.txt,"Recently, diabetic retinopathy (DR) screening utilizing ultra-wide optical
coherence tomography angiography (UW-OCTA) has been used in clinical practices
to detect signs of early DR. However, developing a deep learning-based DR
analysis system using UW-OCTA images is not trivial due to the difficulty of
data collection and the absence of public datasets. By realistic constraints, a
model trained on small datasets may obtain sub-par performance. Therefore, to
help ophthalmologists be less confused about models' incorrect decisions, the
models should be robust even in data scarcity settings. To address the above
practical challenging, we present a comprehensive empirical study for DR
analysis tasks, including lesion segmentation, image quality assessment, and DR
grading. For each task, we introduce a robust training scheme by leveraging
ensemble learning, data augmentation, and semi-supervised learning.
Furthermore, we propose reliable pseudo labeling that excludes uncertain
pseudo-labels based on the model's confidence scores to reduce the negative
effect of noisy pseudo-labels. By exploiting the proposed approaches, we
achieved 1st place in the Diabetic Retinopathy Analysis Challenge.","Diabetic retinopathy (DR) is an eye disease that can result in vision loss and blindness in people with diabetes, but early DR might cause no symptoms or only mild vision problems [7]. Therefore, early detection and management of DR play a crucial role in improving the clinical outcome of eye condition. Color fundus photography,  uorescein angiography (FA), and optical coherence tomog raphy angiography (OCTA) have been used in diabetic eye screening to acquire valuable information for DR diagnosis and treatment planning. Recently, in the screening, ultrawide OCTA (UWOCTA) images have been widely used lever aging their advantages such as more detailed visualization of vessel structures, ?Correspondence to Jaeyoung KimarXiv:2210.09558v1  [eess.IV]  18 Oct 20222 Gitaek Kwon et al. and ability to capture a much wider view of the retinal compared to previous standard approaches [34]. With the advancements of deep learning (DL), applying DLbased methods for medical image analysis has become an active research area in the ophthalmol ogy elds [13, 23, 27, 28]. Notably, the availability to large amounts of annotated fundus photography has been one of the key elements driving the quick growth and success of developing automated DR analysis tools. Sun et al. [29] develop the automatic DR diagnostic models using color fundus images, and Zhou et al. [36] propose a collaborative learning approach to improve the accuracy of DR grading and lesion segmentation by semisupervised learning on the color fundus photography. Although previous studies investigate the eectiveness of applying DL to DR grading and lesion detection tasks based on color fundus images, DR analysis tool leveraging UWOCTA are still underconsideration. One of the rea sons lies in the fact that annotating highquality UWOCTA images is inherently dicult because the annotation of medical images requires manual labeling by experts. Consequently, when we consider about the practical restrictions, it is one of the most crucial things to develop a robust model even in the lack of data. To address the above realworld setting, we introduce the bag of tricks for DR analysis tasks using the Diabetic Retinopathy Analysis Challenge (DRAC22) dataset, which consists of three tasks (i.e., lesion segmentation, image quality assessment, and DR grading) [25]. To alleviate the negative eect introduced by the lack of labeled data, we investigate the eectiveness of data augmentations, ensembles of deep neural networks, and semisupervised learning. Furthermore, we propose reliable pseudo labeling (RPL) that selects reliable pseudolabels based on a trained classier's condence scores, and then the classier is re trained with labeled and trustworthy pseudolabeled data. In our study, we nd that Deep Ensembles [11], testtime data augmentation (TTA), and RPL have powerful eects for DR analysis tasks. Our solutions are combinations of the above techniques and achieved 1st place in all tasks for DRAC22. 2 Related Work "
112,Domain-Adversarial Training of Neural Networks.txt,"We introduce a new representation learning approach for domain adaptation, in
which data at training and test time come from similar but different
distributions. Our approach is directly inspired by the theory on domain
adaptation suggesting that, for effective domain transfer to be achieved,
predictions must be made based on features that cannot discriminate between the
training (source) and test (target) domains. The approach implements this idea
in the context of neural network architectures that are trained on labeled data
from the source domain and unlabeled data from the target domain (no labeled
target-domain data is necessary). As the training progresses, the approach
promotes the emergence of features that are (i) discriminative for the main
learning task on the source domain and (ii) indiscriminate with respect to the
shift between the domains. We show that this adaptation behaviour can be
achieved in almost any feed-forward model by augmenting it with few standard
layers and a new gradient reversal layer. The resulting augmented architecture
can be trained using standard backpropagation and stochastic gradient descent,
and can thus be implemented with little effort using any of the deep learning
packages. We demonstrate the success of our approach for two distinct
classification problems (document sentiment analysis and image classification),
where state-of-the-art domain adaptation performance on standard benchmarks is
achieved. We also validate the approach for descriptor learning task in the
context of person re-identification application.","The cost of generating labeled data for a new machine learning task is often an obstacle for applying machine learning methods. In particular, this is a limiting factor for the fur ther progress of deep neural network architectures, that have already brought impressive advances to the stateoftheart across a wide variety of machinelearning tasks and appli cations. For problems lacking labeled data, it may be still possible to obtain training sets that are big enough for training largescale deep models, but that suer from the shift in data distribution from the actual data encountered at \test time"". One important example is training an image classier on synthetic or semisynthetic images, which may come in abundance and be fully labeled, but which inevitably have a distribution that is dierent from real images (Liebelt and Schmid, 2010; Stark et al., 2010; V azquez et al., 2014; Sun and Saenko, 2014). Another example is in the context of sentiment analysis in written reviews, where one might have labeled data for reviews of one type of product ( e.g., movies), while having the need to classify reviews of other products ( e.g., books). Learning a discriminative classier or other predictor in the presence of a shift be tween training and test distributions is known as domain adaptation (DA). The proposed approaches build mappings between the source (trainingtime) and the target (testtime) domains, so that the classier learned for the source domain can also be applied to the target domain, when composed with the learned mapping between domains. The appeal of the domain adaptation approaches is the ability to learn a mapping between domains in the situation when the target domain data are either fully unlabeled ( unsupervised domain annotation ) or have few labeled samples ( semisupervised domain adaptation ). Below, we focus on the harder unsupervised case, although the proposed approach ( domainadversarial learning ) can be generalized to the semisupervised case rather straightforwardly. Unlike many previous papers on domain adaptation that worked with xed feature representations, we focus on combining domain adaptation and deep feature learning within one training process. Our goal is to embed domain adaptation into the process of learning representation, so that the nal classication decisions are made based on features that are both discriminative and invariant to the change of domains, i.e., have the same or very similar distributions in the source and the target domains. In this way, the obtained feedforward network can be applicable to the target domain without being hindered by the shift between the two domains. Our approach is motivated by the theory on domain adaptation (BenDavid et al., 2006, 2010), that suggests that a good representation for crossdomain transfer is one for which an algorithm cannot learn to identify the domain of origin of the input observation. We thus focus on learning features that combine (i) discriminativeness and (ii) domain invariance. This is achieved by jointly optimizing the underlying features as well as two discriminative classiers operating on these features: (i) the label predictor that predicts class labels and is used both during training and at test time and (ii) the domain classier that discriminates between the source and the target domains during training. While the parameters of the classiers are optimized in order to minimize their error on the training set, the parameters of the underlying deep feature mapping are optimized in order to minimize the loss of the label classier and to maximize the loss of the domain classier. The latter 2DomainAdversarial Neural Networks update thus works adversarially to the domain classier, and it encourages domaininvariant features to emerge in the course of the optimization. Crucially, we show that all three training processes can be embedded into an appro priately composed deep feedforward network, called domainadversarial neural network (DANN) (illustrated by Figure 1, page 12) that uses standard layers and loss functions, and can be trained using standard backpropagation algorithms based on stochastic gradi ent descent or its modications ( e.g., SGD with momentum). The approach is generic as a DANN version can be created for almost any existing feedforward architecture that is trainable by backpropagation. In practice, the only nonstandard component of the pro posed architecture is a rather trivial gradient reversal layer that leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during the backpropagation. We provide an experimental evaluation of the proposed domainadversarial learning idea over a range of deep architectures and applications. We rst consider the simplest DANN architecture where the three parts (label predictor, domain classier and feature extractor) are linear, and demonstrate the success of domainadversarial learning for such architecture. The evaluation is performed for synthetic data as well as for the sentiment analysis problem in natural language processing, where DANN improves the stateoftheart marginalized Stacked Autoencoders (mSDA) of Chen et al. (2012) on the common Amazon reviews benchmark. We further evaluate the approach extensively for an image classication task, and present results on traditional deep learning image data sets|such as MNIST (LeCun et al., 1998) and SVHN (Netzer et al., 2011)|as well as on Office benchmarks (Saenko et al., 2010), where domainadversarial learning allows obtaining a deep architecture that considerably improves over previous stateoftheart accuracy. Finally, we evaluate domainadversarial descriptor learning in the context of person reidentication application (Gong et al., 2014), where the task is to obtain good pedes trian image descriptors that are suitable for retrieval and verication. We apply domain adversarial learning, as we consider a descriptor predictor trained with a Siameselike loss instead of the label predictor trained with a classication loss. In a series of experiments, we demonstrate that domainadversarial learning can improve crossdataset reidentication considerably. 2. Related work "
283,Deep Learning Approach to Diabetic Retinopathy Detection.txt,"Diabetic retinopathy is one of the most threatening complications of diabetes
that leads to permanent blindness if left untreated. One of the essential
challenges is early detection, which is very important for treatment success.
Unfortunately, the exact identification of the diabetic retinopathy stage is
notoriously tricky and requires expert human interpretation of fundus images.
Simplification of the detection step is crucial and can help millions of
people. Convolutional neural networks (CNN) have been successfully applied in
many adjacent subjects, and for diagnosis of diabetic retinopathy itself.
However, the high cost of big labeled datasets, as well as inconsistency
between different doctors, impede the performance of these methods. In this
paper, we propose an automatic deep-learning-based method for stage detection
of diabetic retinopathy by single photography of the human fundus.
Additionally, we propose the multistage approach to transfer learning, which
makes use of similar datasets with different labeling. The presented method can
be used as a screening method for early detection of diabetic retinopathy with
sensitivity and specificity of 0.99 and is ranked 54 of 2943 competing methods
(quadratic weighted kappa score of 0.925466) on APTOS 2019 Blindness Detection
Dataset (13000 images).","Diabetic retinopathy (DR) is one of the most threat ening complications of diabetes in which damage oc curs to the retina and causes blindness. It damages the blood vessels within the retinal tissue, causing them to leak ﬂuid and distort vision. Along with diseases leading to blindness, such as cataracts and glaucoma, DR is one of the most frequent ailments, according to the US, UK, and Singapore statistics (NCHS, 2019; NCBI, 2018; SNEC, 2019). DR progresses with four stages: Mild nonproliferative retinopathy , the earliest stage, where only microaneurysms can occur; Moderate nonproliferative retinopathy , a stage which can be described by losing the blood ves sels’ ability of blood transportation due to their distortion and swelling with the progress of the a https://orcid.org/0000000226787556 b https://orcid.org/0000000199959454 c https://orcid.org/0000000164994575disease; Severe nonproliferative retinopathy results in de prived blood supply to the retina due to the in creased blockage of more blood vessels, hence signaling the retina for the growing of fresh blood vessels; Proliferative diabetic retinopathy is the advanced stage, where the growth features secreted by the retina activate proliferation of the new blood ves sels, growing along inside covering of retina in some vitreous gel, ﬁlling the eye. Each stage has its characteristics and particular properties, so doctors possibly could not take some of them into account, and thus make an incorrect di agnosis. So this leads to the idea of creation of an automatic solution for DR detection. At least 56% of new cases of this disease could be reduced with proper and timely treatment and mon itoring of the eyes (Rohan T, 1989). However, the initial stage of this ailment has no warning signs, and it becomes a real challenge to detect it on the early start. Moreover, welltrained clinicians sometimesarXiv:2003.02261v1  [cs.LG]  3 Mar 2020could not manually examine and evaluate the stage from diagnostic images of a patient’s fundus (accord ing to Google’s research (Krause et al., 2017), see Figure 1). At the same time, doctors will most of ten agree when lesions are apparent. Furthermore, existing ways of diagnosing are quite inefﬁcient due to their duration time, and the number of ophthalmol ogists included in patient’s problem solution. Such sources of disagreement cause wrong diagnoses and unstable groundtruth for automatic solutions, which were provided to help in the research stage. Figure 1: Google showed that ophtalmologists’ diagnoses differ for same fundus image. Best viewed in color. Thus, algorithms for DR detection began to ap pear. The ﬁrst algorithms were based on different classical algorithms from computer vision and set ting thresholds (Michael D. Abrmoff and Quellec, 2010; Christopher E.Hann, 2009; Nathan Silberman and Subramanian, 2010). Nevertheless, in the past few years, deep learning approaches have proved their superiority over other algorithms in tasks of classiﬁ cation and object detection (Harry Pratt, 2016). In particular, convolutional neural networks (CNN) have been successfully applied in many adjacent subjects and for diagnosis of diabetic retinopathy itself (Shao hua Wan, 2018; Harry Pratt, 2016). In 2019, APTOS (Asia Paciﬁc Tele Ophthalmology Society) and competition ML platform Kaggle challenged ML and DL researchers to develop a ﬁveclass DR automatic diagnosing solu tion (APTOS 2019 Blindness Detection Dataset). In this paper, we propose the transfer learning approach and an automatic method for detection of the stage of diabetic retinopathy by single photography of the human fundus. This approach is able to learn useful features even from a noisy and small dataset and could be used as a DR stages screening method in automatic solutions. Also, this method was ranked 54 of 2943 different methods on APTOS 2019 Blindness Detection Competition and achieved the quadratic weighted kappa score of 0.92546.2 RELATED WORK "
132,Becoming More Robust to Label Noise with Classifier Diversity.txt,"It is widely known in the machine learning community that class noise can be
(and often is) detrimental to inducing a model of the data. Many current
approaches use a single, often biased, measurement to determine if an instance
is noisy. A biased measure may work well on certain data sets, but it can also
be less effective on a broader set of data sets. In this paper, we present
noise identification using classifier diversity (NICD) -- a method for deriving
a less biased noise measurement and integrating it into the learning process.
To lessen the bias of the noise measure, NICD selects a diverse set of
classifiers (based on their predictions of novel instances) to determine which
instances are noisy. We examine NICD as a technique for filtering, instance
weighting, and selecting the base classifiers of a voting ensemble. We compare
NICD with several other noise handling techniques that do not consider
classifier diversity on a set of 54 data sets and 5 learning algorithms. NICD
significantly increases the classification accuracy over the other considered
approaches and is effective across a broad set of data sets and learning
algorithms.","The goal of supervised machine learning is to induce an accurate gen eralizing function from a set of labeled training instances. However, most re alworld data sets are noisy. Generally, two types of noise are considered: attribute noise and label noise. Previous work has found that, in general, labe l noise is more harmful than attribute noise [2, 3]. The consequences of labe l noise, as summarized by Fr´ enay and Verleysen [4], include 1) a deterioration o f classi ﬁcation performance, 2) increased learning requirements and mod el complex ity, and 3) a distortion of observed frequencies. Knowing which inst ances are noisy and/or detrimental is nontrivial as, in most cases, all that is known about a task is contained in the set of training instances. As discussed in the related work section, prior work has examined ha n dling label noise using a variety of approaches that are generally spe ciﬁc to, or inspired by, an individual learning algorithm or information theo retic measure. One commonly used approach removes the instances tha t are mis classiﬁed by a learning algorithm [5, 6]. Although such an approach is bia sed towards the learning algorithm that is used, it has generally been sho wn to work well on the examined data sets and learning algorithms especially with the addition of artiﬁcial noise. However, it has also been shown that the ef ﬁcacy of a speciﬁc noise handling technique for a given learning algorit hm is dependent upon the data set characteristics and, in some cases, using a noise handling technique reducesthe classiﬁcation accuracy – especially without the addition of artiﬁcial noise [1, 7]. As ensembles often perform bet ter than any one of its constituent base classiﬁers in classiﬁcation [8], other prior work has used ensemble techniques to improve handling class noise [9, 10, 1 1]. For an ensemble to be more accurate than any individual classiﬁer of the ensem ble, the base classiﬁers need to be accurate (better than random ) and diverse [12]. Using an ensemble technique for identifying noise implicitly lessens t he dependence on a single hypothesis. However, none of the previous work has explicitly focused on selecting diverse base classiﬁers when using an e nsemble approach. Inspired by the ﬁnding that noise handling is notalways eﬃcacious and using the principles of why ensembles increase classiﬁcation accurac y (specif ically diversity), we propose noise identiﬁcation using classiﬁer diversity (NICD). NICD ﬁrst explicitly selects a set of diverse learning algorith ms where diversity is determined by the predictions of the classiﬁers. T he di versity lessens the dependence of a noise measure on a speciﬁc hyp othesis. 2Without diversity, the same hypothesis could be overrepresente d if two hy potheses always classify the same way. We examine using the set of d iverse learning algorithms to 1) ﬁlter the instances, 2) weight the instance s, and 3) as the base classiﬁers for a voting ensemble. Onaset of54datasetsand5learningalgorithms, wecompareNICDw ith 8 ﬁltering techniques, 2 weighting techniques, and a voting ensemble com posed of diﬀerent base classiﬁers – all of which do not explicitly take c lassiﬁer diversity into account. We ﬁnd that using classiﬁer diversity signiﬁca ntly im proves the accuracy for ﬁltering, weighting, and voting ensembles across a broadset of data sets, learning algorithms and noise levels – demonstrat ing a robustness to label noise. The term broadrefers to the characteristic that the noise handling method was notdeveloped speciﬁcally for a given set of data sets and learning algorithms. Overall, using NICD in a votin g ensemble to select a diverse set of base classiﬁers achieves signiﬁca ntly higher classiﬁcation accuracy then using a standard noise handling techniq ue. The remainder of the paper is organized as follows. Section 2 reviews prior work in handling label noise. Section 3 then presents our metho dology for selecting diverse learning algorithms. Our experimental method ology is presented in Section 4. The results are provided in Section 5. Sectio n 6 concludes the paper. 2 Related Work "
569,Lightweight Adaptive Mixture of Neural and N-gram Language Models.txt,"It is often the case that the best performing language model is an ensemble
of a neural language model with n-grams. In this work, we propose a method to
improve how these two models are combined. By using a small network which
predicts the mixture weight between the two models, we adapt their relative
importance at each time step. Because the gating network is small, it trains
quickly on small amounts of held out data, and does not add overhead at scoring
time. Our experiments carried out on the One Billion Word benchmark show a
significant improvement over the state of the art ensemble without retraining
of the basic modules.","The goal of statistical language modeling is to es timate the probability of sentences or sequences of words (Bahl et al., 1990). By the chain rule of probability theory, this is equivalent to esti mation of the conditional probability of a word given all preceding words. This problem is key to natural language processing, with applications not only in typeahead systems, but also ma chine translation (Brown et al., 1993) and au tomatic speech recognition (Bahl et al., 1990). While earlier work on statistical language model ing focused on ngram language models (Kneser and Ney, 1995; Chen and Goodman, 1999), re cent advances are based on variants of neural lan guage models (Bengio et al., 2003; Mikolov et al., 2010; Dauphin et al., 2016), which have yielded state of the art performance on several large scale benchmarks (Jozefowicz et al., 2016). Neural ap proaches require less memory than ngrams and they generalize better, but with a substantial in crease in computational complexity both at train ing and test time. Despite the superior perfor mance of neural models, even better results in 1 2 3 4 5 Target word frequency bucket0.500.751.001.251.50Ratio of perplexities PPL(lstm)/PPL(ensemble) PPL(lstm)/PPL(ngram)Figure 1: As the frequency of the wordtopredict de creases (from left to right), the relative performance of neural models gets better compared to ngrams (orange curve). Yet, ensembling the two models (with a ﬁxed scalar weight) is more effective on rarer words (blue curve). Bins were built by sorting words by frequency and by dividing them into buckets with equal probabil ity mass. terms of perplexity can be achieved by ensembling neural models with ngrams (Mikolov et al., 2011; Chelba et al., 2013; Jozefowicz et al., 2016). How ever, Fig. 1, which shows results using a single constant scalar to weigh the output distribution of a neural model and an ngram model, suggests that the relative contributions of the two models are not simple. For example, the neural model generalizes better than the ngram on rarer words, yet on rarer words the ensemble yields the largest gains. In this work we will study more sophisticated methods for combining the results of ngram and neural language models than a ﬁxed scalar weight. We will propose a simple gating network which takes as input a handful of features based on fre quency statistics to produce as output an input de pendent weight to be used in the ensemble, effec tively turning the ensemble into an adaptive mix ture of experts model. We show that given already trained neural and ngram language models, the gating network can be trained quickly on a handarXiv:1804.07705v2  [cs.CL]  26 Oct 2018ful of examples. The gating network consistently yields better results than the ensemble which uses a ﬁxed weight in the mixture, while adding a neg ligible computational cost. We evaluated our pro posed approach on the One Billion Word bench mark (Chelba et al., 2013), the biggest publicly available benchmark for language modeling, and on the Wall Street Journal corpus, demonstrating seizable gains on both datasets. 2 Related work "
154,Neighborhood-Regularized Self-Training for Learning with Few Labels.txt,"Training deep neural networks (DNNs) with limited supervision has been a
popular research topic as it can significantly alleviate the annotation burden.
Self-training has been successfully applied in semi-supervised learning tasks,
but one drawback of self-training is that it is vulnerable to the label noise
from incorrect pseudo labels. Inspired by the fact that samples with similar
labels tend to share similar representations, we develop a neighborhood-based
sample selection approach to tackle the issue of noisy pseudo labels. We
further stabilize self-training via aggregating the predictions from different
rounds during sample selection. Experiments on eight tasks show that our
proposed method outperforms the strongest self-training baseline with 1.83% and
2.51% performance gain for text and graph datasets on average. Our further
analysis demonstrates that our proposed data selection strategy reduces the
noise of pseudo labels by 36.8% and saves 57.3% of the time when compared with
the best baseline. Our code and appendices will be uploaded to
https://github.com/ritaranx/NeST.","In the era of deep learning, neural network models have achieved promising performance in most supervised learn ing settings, especially when combined with selfsupervised learning techniques (Chen et al. 2020; Devlin et al. 2019; Hu et al. 2020; Zhu et al. 2022). However, they still require a sufﬁcient amount of labels to achieve satisfactory perfor mances on many downstream tasks. For example, in the text domain, curating NLP datasets often require domain experts to read thousands of documents and carefully label them with domain knowledge. Similarly, in the graph domain, molecules are examples naturally represented as graphs, and characterizing their properties relies on density functional theory (DFT) (Cohen, MoriS ´anchez, and Yang 2012) which often takes several hours. Such a dependency on labeled data is one of the barriers to deploy deep neural networks (DNNs) in realworld applications. To better adapt the DNNs to target tasks with limited la bels, one of the most popular approaches is semisupervised learning (SSL), which jointly leverages unlabeled data and *Corresponding author. Copyright © 2023, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.labeled data to improve the model’s generalization power on the target task (Yang et al. 2021; Wang et al. 2022). Although generative models (Gururangan et al. 2019) and consistency based regularization (Tarvainen and Valpola 2017; Miy ato et al. 2018; Xie et al. 2020a) methods have been pro posed for semisupervised learning, they either suffer from the issue of limited representation power (Tsai, Lin, and Fu 2022) or require additional resources to generate high quality augmented samples (e.g., for text classiﬁcation, Xie et al. (2020a) generate augmented text via backtranslation, which rely on a Machine Translation model trained with massive labeled sentence pairs). Consequently, they cannot be readily applied to lowresource scenarios. Selftraining is a proper tool to deal with the deﬁciency of labeled data via gradually enlarging the training set with pseudolabeled data (Rosenberg, Hebert, and Schneiderman 2005). Speciﬁcally, it can be interpreted as a teacherstudent framework : the teacher model generates pseudo labels for the unlabeled data, and the student model updates its pa rameters by minimizing the discrepancy between its predic tions and the pseudo labels (Xie et al. 2020b; Mukherjee and Awadallah 2020). Though conceptually simple, selftraining has achieved superior performance for various tasks with limited labels, such as image classiﬁcation (Sohn et al. 2020; Rizve et al. 2021), natural language understanding (Du et al. 2020), sequence labeling (Liang et al. 2020), and graph learning (Hao et al. 2020). Selftraining has also been suc cessfully extended to other settings including weak super vision (Zhang et al. 2021b) and zeroshot learning (Li, Savarese, and Hoi 2022). However, one major challenge of selftraining is that it suffers from conﬁrmation bias (Arazo et al. 2020) — when the teacher model memorizes some biases and generates incorrect pseudo labels, the student model will be rein forced to train with these wrong biases. As a result, the biases may amplify over iterations and deteriorates the ﬁ nal performance. To suppress the noisy pseudo labels in selftraining, Xu et al. (2021); Zhang et al. (2021a); Sohn et al. (2020); Kim et al. (2022b) leverage model predic tive conﬁdence with a thresholding function, Mukherjee and Awadallah (2020); Tsai, Lin, and Fu (2022) propose to leverage model uncertainty to select samples with low un certainty, and Wang et al. (2021) use metalearning to con duct instance reweighting for sequence labeling. AlthougharXiv:2301.03726v2  [cs.LG]  15 Feb 2023these approaches attempt to reduce the label noise, they se lect the data for selftraining based on the model prediction only. However, the predictions of the deep neural network can be overconﬁdent and biased (Guo et al. 2017; Kong et al. 2020; Kan, Cui, and Yang 2021), and directly using such predictions without any intervention to ﬁlter pseudo la bels cannot effectively resolve the label noise issue. Another problem from selftraining is training instability , as it se lects pseudolabeled data only based on the prediction of the current round. Due to the stochasticity involved in training neural networks (e.g., random initialization, training order), the prediction can be less stable (Yu et al. 2022b), especially for the noisy pseudolabeled data (Xia et al. 2022). Conse quently, the noise in the previous rounds may propagate to later rounds, which deteriorate the ﬁnal performance. Motivated by the above, we propose NeST , a simple yet powerful approach guided by the data representations, to boost the performance of selftraining for fewshot learn ing. Inspired by recent works indicating that the represen tations from deep neural networks can be discriminative and less affected by noisy labels (Li et al. 2021), we harness the features learned from the neural models to select the most reliable samples in selftraining. In addition, several works have indicated that samples within the same category tend to share similar representations, such as categoryguided text mining (Meng et al. 2020) and motifdriven graph learn ing (Zhang et al. 2020). Similarly, we hypothesize that a sample’s pseudo label is more likely to be correct only if its prediction is similar to the neighbor labeled instances in the embedding space. To fulﬁll the denoising purpose, NeST creates the neighborhood for each unlabeled data by ﬁnd ing the top knearest labeled samples, then calculates the divergence between its current prediction and the label of its neighbors to rank the unlabeled data. As a result, only the in stances with the lowest divergence will be selected for self training, which mitigates the issue of label noise. Moreover, to robustly select the training samples for selftraining, we aggregate the predictions on different iterations to promote samples that have lower uncertainty over multiple rounds for selftraining. We remark that NeST is an efﬁcient substitution for exist ing selftraining approaches and can be combined with vari ous neural architectures. The contributions of this paper are: • We propose NeST to improve the robustness of self training for learning with few labels only. • We design two additional techniques, namely neighborhoodregularized sample selection to re duce label noise, and prediction aggregation to alleviate the training instability issue. • Experiments on 4 text datasets and 4 graph datasets with different volumes of labeled data verify that NeST im proves the performance by 1.83% and 2.51% respec tively and saves the running time by 57.3%. 2 Related Work "
370,Star algorithm for NN ensembling.txt,"Neural network ensembling is a common and robust way to increase model
efficiency. In this paper, we propose a new neural network ensemble algorithm
based on Audibert's empirical star algorithm. We provide optimal theoretical
minimax bound on the excess squared risk. Additionally, we empirically study
this algorithm on regression and classification tasks and compare it to most
popular ensembling methods.","Deep learning has been successfully applied to many types of problems and has reached the state oftheart performance. Deep learning models have shown good results in regression analysis and time series forecasting [Qiu+14], computer vision [He+16], as well as in natural language processing [OMK20] and other areas. In many complex problems, such as the Imagenet competition [Den+09], the best results are achieved by ensembles of neural networks, that is, it is often useful to combine the predictions of multiple neural networks to create a new one. The easiest way to ensemble multiple neural networks is to average their predictions [Dru+94]. As shown in work [Kaw16], the number of local minima grows exponentially with the number of parameters. And since modern neural network training methods are based on stochastic optimization, two identical architectures optimized with different initializations will probably converge to different solutions. Such a technique for obtaining neural networks with subsequent construction of an ensemble by majority voting or averaging is used, for example, in article [Car+04]. In addition to the fact that the class of deep neural networks has a huge number of local minima, it is also nonconvex. It was shown in work [LM09] that for the procedure of minimizing the empirical risk in a nonconvex class of functions, the order of convergence is not optimal. In fact, most modern neural network training methods do just that: they minimize the mean value of some error function on the training set. J.Y . Audibert proposed the star procedure method, which has optimal rate of convergence of excess squared risk [Aud07]. Motivated by this observation and the huge success of ensembles of neural networks, we propose a modiﬁcation of the star procedure that will combine the advantages of both methods. In short, the procedure we propose can be described as follows: we run dindependent learning processes of neural networks, obtaining empirical risk minimizers bg1; :::;bgd, freeze their weights, then we initialize a new model and connect all d+ 1models with a layer of convex coefﬁcients, after that we start the process of optimizing all nonfrozen parameters. This whole procedure can be viewed as a search for an empirical minimizer in all possible ddimensional simplices spanned by dminimizers and a class of neural networks. As is known, the minimization of the empirical risk with respect to the convex hull is not optimal in the same way as with respect to the original class of functions. Our method, however, minimizes over some set intermediate between the original class of functions and its convex hull, allowing us to combine the advantages of model ensembling and the star procedure. One can look at this procedure as a new way to train one large neural network with a block architecture, as well as a new way of aggregating models. In this work, we carry out a theoretical analysis of the Preprint. Under review.arXiv:2206.00255v1  [cs.LG]  1 Jun 2022behavior of the proposed algorithm for solving the regression problem with a class of sparse neural networks, and also check the operation of the algorithm in numerical experiments on classiﬁcation and regression problems. In addition to this, we take into account that it is impossible to achieve a global minimum in the class of neural networks, and we consider the situation of imprecise minimization. Themain results of our work can be formulated as follows: • A multidimensional modiﬁcation of the star procedure is proposed. •We prove that the resulting estimate satisﬁes the exact oracle inequality. It follows from this estimate that the order of convergence of the algorithm (in terms of sample size n) for a ﬁxed neural network architecture is optimal. Our results improve over the imprecise oracle inequality in [Sch20]. •We give an upper bound on the generalization error for the case of approximate empirical risk minimizers, which implies the stability of our algorithm against minimization errors. •Based on our algorithm, we propose a new method for training block architecture neural networks, which is quite universal in terms of procedures. We also propose a new way to solve the aggregation problem. •We illustrate the efﬁciency of our approach with numerical experiments on realworld datasets. The rest of this paper is organized as follows. In Section 2, we make an overview of neural network ensembling methods and brieﬂy discuss the advantages of the star algorithm. In Section 3, following the SchmidtHieber notation [Sch20], we deﬁne a class of sparse fully connected neural networks and formulate a number of statements from which it follows that the algorithm we proposed has a fast rate of convergence. All proofs are attached in additional materials. In Section 4, we discuss the implementation of our algorithm, point out a number of possible problems, and suggest several modiﬁcations to ﬁx them. It also describes the conditions for conducting numerical experiments and presents some of their results. At the end, we offer two possible views on our procedure: a new way to train block neural networks and a fairly ﬂexible model aggregation procedure. 2 Related work "
458,Virtual CNN Branching: Efficient Feature Ensemble for Person Re-Identification.txt,"In this paper we introduce an ensemble method for convolutional neural
network (CNN), called ""virtual branching,"" which can be implemented with nearly
no additional parameters and computation on top of standard CNNs. We propose
our method in the context of person re-identification (re-ID). Our CNN model
consists of shared bottom layers, followed by ""virtual"" branches, where neurons
from a block of regular convolutional and fully-connected layers are
partitioned into multiple sets. Each virtual branch is trained with different
data to specialize in different aspects, e.g., a specific body region or pose
orientation. In this way, robust ensemble representations are obtained against
human body misalignment, deformations, or variations in viewing angles, at
nearly no any additional cost. The proposed method achieves competitive
performance on multiple person re-ID benchmark datasets, including Market-1501,
CUHK03, and DukeMTMC-reID.","In person reidentication (reID), the main goal is to nd in a gallery set a matching image with the same identity as a query set. This is a challenging prob lem due to the potentially large dierence between the query and the matching image, such dierence resulting from intrinsic changes, e.g., body position, to extrinsic, e.g., viewing angles. Although a complete person reID system also involves person detection and person tracking, we use the term person reID primarily to refer to the task of person retrieval, which remains an open and heavily studied topic among the computer vision community [1]. In terms of realworld applications, automated person reID has great potential, especially within the elds of security, enabling ecient identication of human subjects from images or video data without human intervention. While motivated by this important problem, the overall framework here proposed is expected to have numerous applications, as it will be clear after fully describing it. We propose a CNN ensemble architecture in Fig. 1 for person reID, to im prove robustness to human body misalignments, deformations, or orientation ?qiang.qiu@duke.eduarXiv:1803.05872v1  [cs.CV]  15 Mar 2018Fig. 1. Proposed model architecture. We adopt the DenseNet architecture with four dense blocks. We share parameters for the rst three blocks, then partition neurons in the last block into virtual branches (purple neurons represent neurons that are selected for the rst branch). With this architecture, no additional parameters are needed. Each branch is learned using dierent data to specialize in dierent aspects (Section 3.2), e.g., according to human landmark information (shown) or pose orientation information. Triplet loss is used as the baseline loss function (Section 3.4). For the example training scheme as shown, we also use the keypoint loss, a.k.a the localizationinducing loss component (Section 3.2). variance (Fig. 2). By training several models, each specialized for a dierent aspect, e.g., a specic body region or pose orientation, we construct a nal ensemble whose predictions are more robust against body misalignment, pose deformations or variations in camera angles. Conventional approaches to ensem ble deep learning are limited by the computational expense of training multiple deep models and longer inference times during testing. Thus, to make our pro posed system usable, an ecient method for implementing ensemble networks is needed that minimizes the number of additional parameters and that can be easily parallelized with limited computational resources, e.g., low GPU memory. To enable our deep model to exhibit ensemble behavior without adding extra parameters, we introduce the method of \virtual branching."" Since lowerlevel features are often similar between deep convolutional neural networks, we con struct a pseudoensemble network by inducing variation only in the top layersof the model, while sharing the lowlevel features in the bottom layers. Whereas in conventional ensemble learning, each member of the ensemble is often treated as a standalone model with information only being shared during the nal pre diction stage, virtual branching assigns a certain subset of the neurons in the original CNN model to each branch partition. Although no physical connections between neurons are broken, we omit activations of neurons not assigned to the same branch (Fig. 1). Compared to Dropout [2], which is used primarily as a regularization technique, our proposed method xes the neurons within each branch, and trains each set of neurons using dierent training data, allowing neurons to develop more specialized features. Fig. 2. Example challenges in person reidentication: body misalignment, pose defor mation, missing parts, dierent viewing angles, and occlusion, etc. Images taken from Market1501 dataset. We illustrate the capability of the proposed ensemble architecture with two example training schemes, tailored to the person reID task. In the rst scheme, we extract keypoint locations of select human landmarks, e.g., shoulders, hips, and ankles, to generate heatmaps, which serve as labels to localize each branch to a specic body region, to improve robustness to body misalignments. In the second scheme, we train each branch using subsets of the original dataset com prised of images of people with dierent pose orientations to improve robustness to dierent viewing angles. Compared to other person reID methods, e.g., [3], [4], and [5], our method does not use a region proposal network and can easily be implemented on existing deep CNN models at little additional computation cost. Our method achieves stateoftheart performance comparable to current methods on the Market1501, CUHK03, and DukeMTMC person reID datasets. The main contributions of this paper can be summarized as follows: {We propose an ensemble deep learning method, called \virtual branching,"" that allows a network to exhibit ensemble behavior without additional pa rameters. Thus, our method does not incur signicantly increased computa tion during both training and testing.{We introduce a localizationinducing loss function component that results in improved alignment of deep features around specic body regions for improved person reID performance. {We demonstrate that the proposed method can also produce deep features that are more robust to dierent pose orientations, i.e., variations in viewing angles. 2 Related Work "
129,On Robust Learning from Noisy Labels: A Permutation Layer Approach.txt,"The existence of label noise imposes significant challenges (e.g., poor
generalization) on the training process of deep neural networks (DNN). As a
remedy, this paper introduces a permutation layer learning approach termed
PermLL to dynamically calibrate the training process of the DNN subject to
instance-dependent and instance-independent label noise. The proposed method
augments the architecture of a conventional DNN by an instance-dependent
permutation layer. This layer is essentially a convex combination of
permutation matrices that is dynamically calibrated for each sample. The
primary objective of the permutation layer is to correct the loss of noisy
samples mitigating the effect of label noise. We provide two variants of PermLL
in this paper: one applies the permutation layer to the model's prediction,
while the other applies it directly to the given noisy label. In addition, we
provide a theoretical comparison between the two variants and show that
previous methods can be seen as one of the variants. Finally, we validate
PermLL experimentally and show that it achieves state-of-the-art performance on
both real and synthetic datasets.","Deep Neural Networks (DNNs) have achieved outstand ing performance on many vision problems, including image classiﬁcation (Krizhevsky, Sutskever, and Hinton 2012), ob ject detection (Redmon et al. 2016), semantic segmentation (Long, Shelhamer, and Darrell 2015), and scene labeling (Farabet et al. 2012). The success in these challenges heav ily relies on the availability of huge and correctly labeled datasets, which are very expensive and timedemanding to collect. To overcome this challenge, a number of crowd sourcing platforms such as Amazon’s Mechanical Turk and nonexpert sources such as Internet Web Images, where la bels are inferred by surrounding text or keywords, have been developed over the past years (Xiao et al. 2015a). Although these methods reduce the labeling cost, the labels derived from these techniques are unreliable due to the high noise rate caused by human annotators or extraction algorithms (Paolacci, Chandler, and Ipeirotis 2010; Scott, Blanchard, and Handy 2013). In addition, more challenging tasks usu ally require domain expert annotators and thus are highly prone to mislabeling (Fr ´enay and Verleysen 2013; Lloyd *These authors contributed equally.et al. 2004), such as breast tumor classiﬁcation (Lee et al. 2018). Training on such noisy datasets negatively impacts the performance of DDNs (i.e., poor generalization) due to the memorization effect (Maennel et al. 2020). Interestingly, the authors in (Zhang et al. 2021) illustrated that DNNs can eas ily ﬁt randomly labeled training data. This property is es pecially problematic when training in the presence of label noise since typical DNNs tend to memorize the noisy in stances leading to a subpar classiﬁer. To overcome this impediment, we propose a learnable permutation layer that is applied to one of the training loss arguments (i.e., model prediction or label). Each training sample has an independent permutation layer associated with a learnable parameter . The purpose of the permu tation layer is to correct the loss of noisy samples through permuting predictions or labels during training, allowing the model to learn safely from them. During inference, the per mutation layer is discarded. Our contributions can be sum marized as follows: 1. We propose a permutation layer learning framework PermLL that can effectively learn from datasets contain ing noisy labels. 2. We theoretically analyze two variants of PermLL. The ﬁrst approach applies the permutation layer to the pre dictions, while the second applies the permutation to the labels. We show that the approach of learning the labels directly, proposed in Joint Optimization (Tanaka et al. 2018), can be seen as a special case of PermLL. 3. We provide a theoretical analysis of the two proposed methods, showing that applying the permutation layer to the predictions has better theoretical properties. 4. We empirically demonstrate the effectiveness of PermLL on synthetic and real noise, achieving stateoftheart performance on CIFAR10, CIFAR100, and Cloth ing1M. 2 Related Work "
210,FINE Samples for Learning with Noisy Labels.txt,"Modern deep neural networks (DNNs) become frail when the datasets contain
noisy (incorrect) class labels. Robust techniques in the presence of noisy
labels can be categorized into two folds: developing noise-robust functions or
using noise-cleansing methods by detecting the noisy data. Recently,
noise-cleansing methods have been considered as the most competitive
noisy-label learning algorithms. Despite their success, their noisy label
detectors are often based on heuristics more than a theory, requiring a robust
classifier to predict the noisy data with loss values. In this paper, we
propose a novel detector for filtering label noise. Unlike most existing
methods, we focus on each data's latent representation dynamics and measure the
alignment between the latent distribution and each representation using the
eigendecomposition of the data gram matrix. Our framework, coined as filtering
noisy instances via their eigenvectors (FINE), provides a robust detector with
derivative-free simple methods having theoretical guarantees. Under our
framework, we propose three applications of the FINE: sample-selection
approach, semi-supervised learning approach, and collaboration with
noise-robust loss functions. Experimental results show that the proposed
methods consistently outperform corresponding baselines for all three
applications on various benchmark datasets.","Deep neural networks (DNNs) have achieved remarkable success in numerous tasks as the amount of accessible data has dramatically increased [21, 15]. On the other hand, accumulated datasets are typically labeled by a human, a laborintensive job or through web crawling [48] so that they may be easily corrupted ( label noise ) in realworld situations. Recent studies have shown that deep neu ral networks have the capacity to memorize essentially any labeling of the data [49]. Even a small amount of such noisy data can hinder the generalization of DNNs owing to their strong memorization of noisy labels [49, 29]. Hence, it becomes crucial to train DNNs that are robust to corrupted labels. As label noise problems may appear anywhere, such robustness increases reliability in many appli cations such as the ecommerce market [9], medical ﬁelds [45], ondevice AI [46], and autonomous driving systems [11]. To improve the robustness against noisy data, the methods for learning with noisy labels (LNL) have been evolving in two main directions [18]: (1) designing noiserobust objective functions or regular Equal contribution 1Code available at https://github.com/Kthyeon/FINE_official 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.arXiv:2102.11628v3  [cs.LG]  27 Oct 2021FEATURE EXTRACTOR  FEATURE VECTOR CLEAN NOISYLABEL : DOG DETECTION STRATEGY CLEAN NOISYROBUST TRAINOur Contribution : Noise Cleansing  LINEAR CLASSIFIER(a) NoiseCleansingbased Approach CLEAN NOISE𝒖𝑪𝑳𝑬𝑨𝑵 EIGEN DECOMPOSITION GAUSSIAN MIXTURE MODELFIT  (b) FINE Figure 1: Illustration of (a) basic concept of this work and (b) proposed detection framework, FINE. Noisecleansing learning generally separates clean data from the original dataset by using prediction outputs. We propose a novel derivativefree detector based on an unsupervised clustering algorithm on the highorder topological space. FINE measures the alignment of prelogits (i.e., penultimate layer representation vectors) toward the classrepresentative vector that is extracted through the eigen decomposition of the gram matrix of data representations. izations and (2) detecting and cleansing the noisy data. In general, the former noiserobust direction uses explicit regularization techniques [6, 52, 50] or robust loss functions [38, 13, 40, 51], but their performance is far from stateoftheart [49, 26] on datasets with severe noise rates. Recently, re searchers have designed noisecleansing algorithms focused on segregating the clean data (i.e., sam ples with uncorrupted labels) from the corrupted data [19, 14, 47, 18, 32, 42]. One of the popular criteria for the segregation process is the loss value between the prediction of the noisy classiﬁer and its noisy label, where it is generally assumed that the noisy data have a large loss [19, 14, 47, 18] or the magnitude of the gradient during training [51, 40]. However, these methods may still be bi ased by the corrupted linear classiﬁer towards label noise because their criterion (e.g., loss values or weight gradient) uses the posterior information of such a linear classiﬁer [24]. Maennel et al. [31] analytically showed that the principal components of the weights of a neural network align with the randomly labeled data; this phenomenon can yield more negative effects on the classiﬁer as the number of randomly labeled classes increases. Recently, Wu et al. [42] used an inherent geo metric structure induced by nearest neighbors (NN) in latent space and ﬁltered out isolated data in such topology, and its quality was sensitive to its hyperparameters regarding NN clustering in the presence of severe noise rates. To mitigate such issues for label noise detectors, we provide a novel yet simple detector frame work, ﬁltering n oisy labels via their e igenvectors (FINE) with theoretical guarantees to provide a highquality splitting of clean and corrupted examples (without the need to estimate noise rates). Instead of using the neural network’s linear classiﬁer, FINE utilizes the principal components of latent representations made by eigen decomposition which is one of the most widely used unsuper vised learning algorithms and separates clean data and noisy data by these components (Figure 1a). To motivate our approach, as Figure 1b shows, we ﬁnd that the clean data (blue points) are mainly aligned on the principal component (black dotted line), whereas the noisy data (orange points) are not; thus, the dataset is well clustered with the alignment of representations toward the principal component by ﬁtting them into Gaussian mixture models (GMM). We apply our framework to var ious LNL methods: the sample selection approach, a semisupervised learning (SSL) approach, and collaboration with noiserobust loss functions. The key contributions of this work are summarized as follows: • We propose a novel framework, termed FINE ( ﬁltering n oisy labels via their e igenvectors ), for detecting clean instances from noisy datasets. FINE makes robust decision boundary for the highorder topological information of data in latent space by using eigen decomposition of their gram matrix. • We provide provable evidence that FINE allows a meaningful decision boundary made by eigenvectors in latent space. We support our theoretical analysis with various experimental results regarding the characteristics of the principal components extracted by our FINE detector. • We develop a simple sampleselection method by replacing the existing detector method with FINE. We empirically validate that a sampleselection learning with FINE provides consistently superior detection quality and higher test accuracy than other existing alterna tive methods such as the Coteaching family [14, 47], TopoFilter [42], and CRUST [32]. 2• We experimentally show that our detection framework can be applied in various ways to existing LNL methods and validate that ours consistently improves the generalization in the presence of noisy data: sampleselection approach [14, 47], SSL approach [25], and collaboration with noiserobust loss functions [51, 40, 29]. Organization. The remainder of this paper is organized as follows. In Section 2, we discuss the recent literature on LNL solutions and meaningful detectors. In Section 3, we address our motivation for creating a noisy label detector with theoretical insights and provide our main method, ﬁltering the noisy labels via their eigenvectors (FINE). In Section 4, we present the experimental results. Finally, Section 5 concludes the paper. 2 Related Works "
453,Automatic Semantic Segmentation of the Lumbar Spine: Clinical Applicability in a Multi-parametric and Multi-centre Study on Magnetic Resonance Images.txt,"One of the major difficulties in medical image segmentation is the high
variability of these images, which is caused by their origin (multi-centre),
the acquisition protocols (multi-parametric), as well as the variability of
human anatomy, the severity of the illness, the effect of age and gender, among
others. The problem addressed in this work is the automatic semantic
segmentation of lumbar spine Magnetic Resonance images using convolutional
neural networks. The purpose is to assign a class label to each pixel of an
image. Classes were defined by radiologists and correspond to different
structural elements like vertebrae, intervertebral discs, nerves, blood
vessels, and other tissues. The proposed network topologies are variants of the
U-Net architecture. Several complementary blocks were used to define the
variants: Three types of convolutional blocks, spatial attention models, deep
supervision and multilevel feature extractor. This document describes the
topologies and analyses the results of the neural network designs that obtained
the most accurate segmentations. Several of the proposed designs outperform the
standard U-Net used as baseline, especially when used in ensembles where the
output of multiple neural networks is combined according to different
strategies.","Magnetic Resonance (MR) images are obtained by means of a technique based on magnetic ﬁelds where frequencies in the range of radio waves (8–130 MHz) are used. This technique ob tains medical images with the maximum level of detail so far. In recent years, MR images became essential to obtain qual ity images from any part of the human body thanks to the fact that MR images provide either functional and morphological information of both anatomy and pathological processes; with a spatial resolution and constrast much higher than the obtained by means of other techniques for medical image acquisition. Concerning lumbar pathologies, MR imaging is the preferred type of images between radiologists and physicians specialized in the lumbar spine and the spine in general. Thanks to MR im ages they can ﬁnd disorders in nerve structures, vertebrae, in tervertebral discs, muscles and ligaments with much more pre cision than ever (Roudsari and Jarvik, 2010). Manual inspection and analysis carried out by human experts (typically radiologists) is the most common methodology to ex tract information from MR images. Visual inspection is carried out slide by slide in order to determine the location, size and ∗Corresponding authors: jhonjsaenzg@gmail.com (J.J. S ´aenzGamboa), delaiglesia mar@gva.es (M. IglesiaVay ´a)pattern of multiple clinical ﬁndings in the lumbar structures, that can be either normal or pathological. Manual inspection of slides has a strong dependency on the experience of each ex pert, so that the variability due to di ﬀerent criteria of experts is a challenge that cannot be ignored (Carrino et al., 2009; Berg et al., 2012). Radiologists, even those with a great experience, need a lot of time to perform the visual inspection of images, so this is a very slow task as well as tedious and repetitive. In fact, the excess of information to be processed visually causes fatigue and loss of attention, which leads radiologists to not per ceive some obvious nuances because of the “temporary blind ness due to workload excess” (Konstantinou et al., 2012). Current progress of Artiﬁcial Intelligence (AI) and its appli cation to medical imaging is providing new and more sophisti cated algorithms based on Machine Learning (ML) techniques. These new algorithms are complementary to the existing ones in some cases, but in general they perform much better because most of the existing ones are knowledge based (do not learn from data). The new algorithms are much more robust to de tect the lumbar structures (i.e., vertebrae, intervertebral discs, nerves, blood vessels, muscles and other tissues) and repre sent a signiﬁcant reduction in the workload of radiologists and traumatologists (Coulon et al., 2002; Van Uitert et al., 2005; De Leener et al., 2014, 2015).arXiv:2111.08712v3  [eess.IV]  8 Nov 2022In the context of AI, automatic semantic segmentation is cur rently the most widely used technique (Litjens et al., 2017). This technique classiﬁes each individual pixel from an image into one of several classes or categories; each class or category corresponds to a type of objects from real world to be detected. In recent years, Convolutional Neural Networks (CNNs) are considered the best ML technique to address semantic segmen tation tasks. However, CNNs require a very large amount of manually annotated images to properly estimate the values of the millions of weights corresponding to all the layers of any CNN topology designed by a Deep Learning (DL) expert. Ro bustness and precision of any classiﬁer based on CNNs strongly depend on the number of samples available to train the weights of the CNN. So, the challenge in all the projects addressing the task of semantic segmentation is the availability of large enough datasets of medical images. In order to have a minimum of samples to train models, a manual segmentation procedure was designed in this work, where both MR image types T1w and T2w were used to manually adjust the boundaries between structural elements and tissues. Subsection 3.1.2 provides more detail about both MR image types. The main objective of this study is to use a limited dataset of MR images to reach an accurate and e ﬃcient segmentation of the structures and tissues from the lumbar region by means of using individually optimized CNNs or ensembles of several CNNs; all the used topologies were based on the original UNet architecture, i.e., they are variants from the UNet. This paper is organised as follows: Section 2 reviews the state of the art and references other works related to the au tomatic semantic segmentation of medical images. Section 3 provides details about the used resources, where Subsection 3.1 describes the dataset used in this work, and Subsection 3.2 pro vides details of the hardware infrastructure and software toolk its. Section 4 describes the block types used in this work to design CNN topologies as variants from the original UNet ar chitecture. Section 5 describes the experiments carried out in this work. Sections 6 and 7 present and discuss the results re spectively. Finally, Section 8 concludes by taking into account the deﬁned objectives and draws possible future works. 2. Related work "
545,Ensembles of Convolutional Neural Networks models for pediatric pneumonia diagnosis.txt,"Pneumonia is a lung infection that causes 15% of childhood mortality, over
800,000 children under five every year, all over the world. This pathology is
mainly caused by viruses or bacteria. X-rays imaging analysis is one of the
most used methods for pneumonia diagnosis. These clinical images can be
analyzed using machine learning methods such as convolutional neural networks
(CNN), which learn to extract critical features for the classification.
However, the usability of these systems is limited in medicine due to the lack
of interpretability, because of these models cannot be used to generate an
understandable explanation (from a human-based perspective), about how they
have reached those results. Another problem that difficults the impact of this
technology is the limited amount of labeled data in many medicine domains. The
main contributions of this work are two fold: the first one is the design of a
new explainable artificial intelligence (XAI) technique based on combining the
individual heatmaps obtained from each model in the ensemble. This allows to
overcome the explainability and interpretability problems of the CNN ""black
boxes"", highlighting those areas of the image which are more relevant to
generate the classification. The second one is the development of new ensemble
deep learning models to classify chest X-rays that allow highly competitive
results using small datasets for training. We tested our ensemble model using a
small dataset of pediatric X-rays (950 samples) with low quality and anatomical
variability (which represents one of the biggest challenges). We also tested
other strategies such as single CNNs trained from scratch and transfer learning
using CheXNet. Our results show that our ensemble model outperforms these
strategies obtaining highly competitive results. Finally, we confirmed the
robustness of our approach using another pneumonia diagnosis dataset [1].","Pneumonia is the most common infectious disease in humans and the leading cause of childhood morbidity refers to having a disease or a symptom of disease, or to the amount of disease within a population) and mortality in the world [ 2] that causes inﬂammation of the alveoli [ 3]. It especially affects children under 2 years old and elderly above 65. Globally, 15% of childhood mortality is caused by this disease, over 808,694 children in 2017 [ 4]. This mortality still remains around 800,000 children in 2018 under ﬁve every year, or 2,200 children every day, resulting, as UNICEF shows in [ 5], the dramatic number of one child dead every 39 seconds, which includes over 153,000 newborns. Pneumonia is mainly caused by viruses or bacteria. Most frequent associated viruses are respiratory syncytial virus (RSV), inﬂuenza virus and human parainﬂuenza virus (HPIV) [ 6]. Most frequent bacteria are Streptococcus pneumoniae ,Haemophilus inﬂuenzae , Streptococcus pyogenes andStaphylococcus aureus , and Mycoplasma pneumoniae [6]. The global prevalence of viral pneumonia origin in children is 1462%, being higher in children under two years old and decreasing with age [7, 8]. Children with bacterial pneumonia should receive antibiotics as soon as possible, whereas children with viral pneumonia usually only need supportive care. However, antivirals may have a relevant role in the treatment of viral infections [9]. When in doubt, and given that bacterial pneumonia is more serious, the symptoms are usually resolved by using empirical antibiotic treatment, which in most cases is unnecessary since a virus is the most frequent cause of communityacquired pneumonia (CAP) [10]. Pneumonia diagnosis is fundamentally clinical, without reaching an etiological diagnosis most of the time. Due to the high economic cost, and the time it takes to obtain results (days, or even weeks), pediatricians often diagnose based on: laboratory tests, xrays, and the examination of the patient. Xrays are one of the most important to diagnose CAP [ 11]. Furthermore, it is clear that a high proportion of all pneumonia cases are in fact vialbacterial coinfections, complicating decisions regarding antibiotic administration [ 12]. Therefore, pediatricians have to decide empirically whether the child needs antibiotics, and choose the best treatment with their limited available tools. As a result, most children receive antibiotics. These results in what is considered overtreatment with antibiotics, leading to the need to narrow the indications by an appropriate discriminative diagnosis [ 13]. Proxies for typical bacterial pneumonia have been proposed, but the consensus is that no single biomarker alone is enough for diagnosing bacterial pneumonia [ 14,15]. The old paradigm that bacterial pneumonia is associated with a speciﬁc radiographic pattern different from the pattern of viral pneumonia is now often criticized, although the radiological pictures of alveolar pneumonia (also termed lobar pneumonia, or pneumonia with consilidation ) appear to be bacterial in most of the cases [ 16]. The interpretation of the chest Xray radiography (CXR) is usually performed following the standards of the “WHO Vaccine Trial Investigators Radiology Working Group” [ 17]. These standards establish two possible interpretations: “consolidation” (including consolidation and/or pleural effusion as per WHO standards) and “other inﬁltrates”. However, the interobserver agreement for these two categories is low [18]. Convolutional Neural Networks (CNNs) are wellknown Deep Learning architectures. Recently great advances have undergone helping to solve several visualrelated tasks [ 19]. This kind of neural networks is inspired by the biological neurons of the visual cortex [ 20], which allows them to solve problems such as image classiﬁcation [ 21] and object recognition [ 22], among other image and video processing and recognition tasks. CNNs are also successful in other problems such as speech recognition [ 23], malware detection [ 24,25], natural language processing [ 26,27], among many others. These systems process the information in two main steps: feature extractor , where relevant features are detected; and classiﬁcation , where these features obtained from the previous step are analyzed and different probabilities will be assigned to the detected structures to carry out the classiﬁcation. In areas such as medicine, where the diagnosis is often based on the analysis of clinical images (e.g. xrays), CNNs and Deep Learning methods have proven both their usefulness and effectiveness in the detection and classiﬁcation of multiple diseases [ 20,28,29]. The classiﬁcation performance in computer vision problems, such as those mentioned above, can be improved through ensembles of models [ 30,31]. An ensemble is a kind of method based on the combination of multiple learning algorithms, the 2APREPRINT  JUNE 7, 2021 ensemble is designed to improve the performance of the particular elements that builds up the new algorithm. This technique combine the individual predictions to produce a consensus prediction. Its advantages over individual models are the performance, because the combination of multiples models can improve their individual power and the ﬁnal model can approximate better the optimal solution [ 32]; and robustness, because the ensemble models reduce the variance of prediction errors made but the contributing models by adding bias, avoiding overﬁtting of the ﬁnal model [33]. For this reason this technique can be critical in small datasets like ours, where the information is particular limited in both size and quality. However, CNNs, like many other Deep Learning and machine learning methods, are considered as "" blackbox "" algorithms, where both the input and output can be easily analysed and interpreted by the users, but where the inference process carried out by the algorithm is opaque hinders endusers’ conﬁdence in the results obtained, and therefore makes decisionmaking negatively affected. It makes this essential process (”how” and ”why” the algorithm has obtained this outcome) uninterpretable for the human being [ 34]. This may limit its application in ﬁelds such as medicine, where the practitioners need to know how the algorithm has inferred the output for each speciﬁc patient [ 35] (e.g. why the algorithm is assigning a 90 % probability for alveolar pneumonia?). This limitation can be overcome using automatic explanatory systems, called explainable AI (XAI) [ 36], which allows us to visualize which areas of the image (features) have been used to obtain the outcome generated by the algorithm, or at least alleviating, the aforementioned problem. These XAIbased systems will generate new images highlighting the areas of highest interest that the system uses to obtain the result (e.g. in our case to predict a particular kind of disease) [37]. The combination of Deep Learning models with medical knowledge allows the development of new clinical decision support systems (CDSS). These automatic systems can help in medical diagnosis reducing some typical clinical problems such as subjectivity in the interpretation of medical tests or human errors (fatigue, distraction, etc.) [38]. This combination of machine learning methods with humanbased knowledge can improve the performance of the diagnosis process, as it was stated in [ 39]. In that project, leaded by Dr. Andrew Beck, it was demonstrated that the combination of pathologists and Deep Learning models provide a signiﬁcant reduction of the error rate for breast cancer diagnosis. In the initial results pathologists obtained a 3.5% of error during classiﬁcation of the pathology, whereas the Deep Learning algorithm obtained a slightly better result of 2.9% error. However, when both humans and AI model were combined this error decreased to an impressive 0.5 % (so, the 99.5% of cases were correctly classiﬁed) [39]. The main contribution of this work can be brieﬂy summarized as the design and development of a novel Machine Learning system based on ensembles for pneumonia diagnosis in childhood. Our system estimates the probability that the Xray has a consolidation or other inﬁltrates, which would be a helpful tool for the unclear cases in case of disagreement among professionals, or in case of work overload. The result generated from the system should be userfriendly (from a health professional perspective), to achieve that, the system creates a graphical visualization using an explainable AI technique named heatmap, which highlights the areas of the image which are more relevant for the diagnosis according to the AI system [40]. An interesting point of this work is to understand how the neural network infers the pneumonia type (alveolar versus non alveolar) from the chest Xray. This could help to expedite the treatment of patients who require medication. On the other hand, this could also help to avoid giving antibiotics to patients who do not need them. This is crucial since an incorrect use of antibiotics (that is, using them in patients who do not have a bacterial infection), or an excessive use of broadspectrum antibiotics, can cause antibiotic resistance . This can become a global problem making it more difﬁcult to treat patients, the only solution being the development of new, more powerful antibiotics [ 13]. Therefore, in order to reduce the overuse of antibiotics in viral pneumonia, a correct diagnosis of bacterial pneumonia is crucial. This article has been structured as follows: Section 2 provides a short description of some relevant works in the area of AIbased detection of lung diseases; Section 3 describes the methodology followed to design and train our CNN models; Section 4 shows the experimental results; Section 5 presents the conclusions and ﬁnally, future work, with some future lines of work. 2 Related work "
611,Semi-supervised Feature Learning For Improving Writer Identification.txt,"Data augmentation is usually used by supervised learning approaches for
offline writer identification, but such approaches require extra training data
and potentially lead to overfitting errors. In this study, a semi-supervised
feature learning pipeline was proposed to improve the performance of writer
identification by training with extra unlabeled data and the original labeled
data simultaneously. Specifically, we proposed a weighted label smoothing
regularization (WLSR) method for data augmentation, which assigned the weighted
uniform label distribution to the extra unlabeled data. The WLSR method could
regularize the convolutional neural network (CNN) baseline to allow more
discriminative features to be learned to represent the properties of different
writing styles. The experimental results on well-known benchmark datasets
(ICDAR2013 and CVL) showed that our proposed semi-supervised feature learning
approach could significantly improve the baseline measurement and perform
competitively with existing writer identification approaches. Our findings
provide new insights into offline write identification.","Handwritten texts, speech, ngerprints, and faces are often applied in physiological biometric identiers. Especially, handwritten text plays an im portant role for forensics and security in proving someone's authenticity. Re search into writer identication has received renewed interest in recent years, such as historical document analysis for the massdigitization processes of historical documents [24, 29, 49] through machine learning; unfortunately, this process requires considerable time and detection costs. Therefore, many researchers have proposed stateoftheart pattern recognition approaches to automatically recognize writing styles [1, 7, 30, 38, 50]. Writer identication aims to search and recognize texts written by the same writer in a query database. Writer identication has been investigated on dierent handwritten scripts, such as English [3, 40], Chinese [18, 19, 48], Arabic [1], Indic [30], Persian [20] and Latin scripts [9]. This task gener ally presents substantial challenges because it requires the documents to be sorted according to high similarity (e.g., the distance of feature vectors). Writer identication can be classied as online writer identication and of  ine writer identication according to the handwritten document acquisition method. The latter approach can be further categorized into allographbased and textualbased methods. Texturalbased methods compute global statis tics directly from handwritten documents (pages) [2, 14, 16, 32, 33]. For example, the angles of stroke directions, the width of the ink trace, and the histograms of local binary patterns (LBP) and local ternary patterns (LTP) have been used for writer identication purposes. Allographbased methods rely on local descriptors computed from small patches (allographs), and then a global document descriptor is statistically calculated using the local descrip tors of one document [7, 8, 18]. These two methods can be further combined to form a discriminative global feature [3, 17, 48]. The semisupervised fea ture learning pipeline proposed in this work is based on allographs for oine writer identication. Although writer identication has achieved excellent performance on some benchmark datasets, there are considerable challenges in realworld applica tions. First, the use of dierent pens, the physical condition of the writer, the presence of distractions (such as multitasking and noise), and the changes 2in writing style with age are key factors resulting in the unsatisfactory per formance of writer identication. Second, the writers of the training set are dierent than those of the test set, and every writer only contributes a few handwritten text images in the typically used benchmark datasets. Third, the number of handwritten documents in benchmark datasets is highly in sucient for convolutional neural network (CNN) model training; therefore, training a reliable CNN model using limited data is a challenge. Moreover, almost all published methods are based on supervised learning, which cannot achieve landmark results due to the limited amount of labeled data present in the benchmarks. Some researchers utilize dierent data augmentation meth ods to address these problems. However, these data augmentation methods that are used in writer identication easily lead to model overtting and re quire a considerable amount of extra data. To overcome the aforementioned challenges and then tightly integrate with writer identication in practice, we propose a novel insight for writer identication. CNNs are a wellknown deep learning architecture inspired by the natu ral visual perception mechanism of living creatures. CNNs have been widely used and have achieved exciting performance in the elds of image classi cation, object recognition and object detection and tracking [15, 25, 41, 43] due to their powerful ability to learn deep features. The recent progress in writer identication is mainly attributed to advancements in CNNs based on supervised [6, 7, 8, 11, 17, 45, 49] and unsupervised feature learning [9]. The features extracted from CNNs perform better as discriminative characteris tics compared to handcrafted features. For example, Xing and Qiao et al. [49] designed a multistream CNN structure for writer identication and achieved a high identication accuracy on the IAM [31] and HWDB [28] datasets using a small amount of handwritten documents. In [8], Christlein proposed using activation features from CNNs as local descriptors for writer identication and improved the identication performance on the ICDAR2013 dataset. R. Eldan et al. [10] showed that a deeper network would learn a more discrim inative representation but will need more resources to train. Therefore, we recommend that a tradeo and a deep residual neural network with 50 layers (ResNet50) could be applied in our work. In contrast to the supervised learning approaches, semisupervised learn ing signicantly surpasses supervised learning when annotated data are lim ited in the training set, e.g., weakly labeled or unlabeled data [21, 47, 52]. In particular, semisupervised learning saves the time and budget needed for annotating data when the volume of clean labeled data is limited. Some 3recent studies investigated a semisupervised learning pipeline by combining unsupervised learning with supervised learning [39, 46] to assign an original label or a new label to unlabeled data [26, 34, 36]. Motivated by the previ ous studies, we attempt to use a modied semisupervised learning method by assigning a weighted uniform label distribution to extra unlabeled data (extra data) according to the original labeled data (real data). We believe that the proposed approach has the potential to regularize the baseline for improving identication performance. Therefore, we proposed a semisupervised method that leverages a deep CNN and the weighted label smoothing regularization (WLSR) to form a powerful model that learns discriminative representations for oine writer identication in our work. Specically, we rst preprocess the original labeled data and the extra unlabeled data. Then, these original labeled data and extra unlabeled data are fed into a deep residual neural network (ResNet) [15] simultaneously. Furthermore, the WLSR method regularizes the learn ing process by integrating the unlabeled data, which can reduce the risk of overtting and direct the model to learn more eective and discriminative features. Finally, the local features of every test handwritten document are extracted and encoded as a global feature vector for identication. To summarize, this study makes the following contributions: A. This study is a pioneering work that uses a semisupervised feature learning pipeline to integrate extra unlabeled images and original labeled images into the ResNet model for writer identication. B. The WLSR method of semisupervised learning is used to regularize the identication model with unlabeled data. We thoroughly evaluate its availability on public datasets. C. Our results show that the proposed semisupervised learning model had a consistent improvement over the deep residual neural network baseline and achieved better performance than existing approaches on benchmark datasets. The remainder of this paper is organized as follows. Sec. 2 provides an overview of the related works in the eld of writer identication. The process of the semisupervised learning pipeline is presented in Sec. 3. The perfor mance and evaluation are given in Sec. 4. Sec. 5 presents the discussion. Sec. 6 provides a summary and the outlook for future research. 42. Related Work "
315,Contrastive Learning and Self-Training for Unsupervised Domain Adaptation in Semantic Segmentation.txt,"Deep convolutional neural networks have considerably improved
state-of-the-art results for semantic segmentation. Nevertheless, even modern
architectures lack the ability to generalize well to a test dataset that
originates from a different domain. To avoid the costly annotation of training
data for unseen domains, unsupervised domain adaptation (UDA) attempts to
provide efficient knowledge transfer from a labeled source domain to an
unlabeled target domain. Previous work has mainly focused on minimizing the
discrepancy between the two domains by using adversarial training or
self-training. While adversarial training may fail to align the correct
semantic categories as it minimizes the discrepancy between the global
distributions, self-training raises the question of how to provide reliable
pseudo-labels. To align the correct semantic categories across domains, we
propose a contrastive learning approach that adapts category-wise centroids
across domains. Furthermore, we extend our method with self-training, where we
use a memory-efficient temporal ensemble to generate consistent and reliable
pseudo-labels. Although both contrastive learning and self-training (CLST)
through temporal ensembling enable knowledge transfer between two domains, it
is their combination that leads to a symbiotic structure. We validate our
approach on two domain adaptation benchmarks: GTA5 $\rightarrow$ Cityscapes and
SYNTHIA $\rightarrow$ Cityscapes. Our method achieves better or comparable
results than the state-of-the-art. We will make the code publicly available.","The goal in semantic image segmentation is to assign the correct class label to each pixel. This makes it suitable for complex imagebased scene analysis that is required in ap plications like automated driving. However, in order to gen erate a labeled training dataset, pixellevel annotation must ﬁrst be performed by humans. Since detailed manual labeling can take 90 minutes per image [8], it is associated with high costs. A potential workaround would be to generate the images and corresponding segmentation maps synthetically using computer game environments like Grand Theft Auto V (GTA5) [29]. However, even current segmentation mod els [2, 34, 1] do not generalize well to data from a different domain. In fact, their segmentation performance decreases drastically when there is a discrepancy between the training and test distribution as in a synthetictoreal scenario. The research ﬁeld of unsupervised domain adaptation (UDA) studies how to transfer knowledge from a labeled source domain to an unlabeled target domain. The aim is to achieve the best possible results in the target domain, whereas the performance in the source domain is not con sidered. Current methods for UDA address the problem by minimizing the distribution discrepancy between the do mains while doing a supervised training on source data. The distribution alignment can take place in the pixel space [15, 23, 36, 44], feature space [16, 17, 41, 45], output space [38, 40, 39, 41, 24], or even in several spaces in parallel. While adversarial training (AT) [10] is commonly used to minimize the distribution discrepancy between domains, it can fail to align the correct semantic categories. This is because adversarial training minimizes the mismatch be tween global distributions rather than classspeciﬁc ones, which can negatively affect the results [7]. This is also true for discrepancy measures such as the maximum mean dis crepancy [31], which can be minimized without aligning the correct class distributions across domains [19]. To address the problem of misaligned classes across domains, we rely on contrastive learning (CL) [12]. The basic idea of CL is to encourage positive data pairs to be similar and negative data pairs to be apart. To perform domain adaptation and match the features of the correct semantic categories, posi tive pairs consist of features from the same class but differ ent domains while negative pairs are from different classes and possibly from different domains [19, 27]. Due to the lack of labels in the target domain, the class of each target feature must be determined based on the predictions of the model. However, since different classarXiv:2105.02001v1  [cs.CV]  5 May 2021prior probabilities can bias the ﬁnal segmentation layer to wards the source domain, we extend our approach with selftraining (ST), i.e. using target predictions as pseudo labels. We start from the observation that target pixels are predicted with high uncertainty [40]. Moreover, these un certain predictions may also vary between different classes during training and are therefore usually not considered for selftraining. By using a memoryefﬁcient temporal ensem ble that combines the predictions of a single network over time [21], we obtain the predictive tendency of the model. This allows us to create robust pseudolabels even for the uncertain predictions that have high information content. The temporal ensemble has the additional advantage that pseudolabels are updated directly during training, which reduces the computational complexity compared to a sepa rate stagewise recalculation [45, 26, 22, 49, 44]. We summarize our contributions as follows: First, we ex tend contrastive learning and selftraining using a memory efﬁcient temporal ensemble to UDA for semantic segmen tation. Second, we empirically show that both approaches are able to transfer knowledge between domains. Further more, we show that combining our contrastive learning and selftraining (CLST) approach leads to a symbiotic setup that yields competitive and superior stateoftheart results for GTA5!Cityscapes and SYNTHIA !Cityscapes, re spectively. 2. Related works "
325,Model Composition: Can Multiple Neural Networks Be Combined into a Single Network Using Only Unlabeled Data?.txt,"The diversity of deep learning applications, datasets, and neural network
architectures necessitates a careful selection of the architecture and data
that match best to a target application. As an attempt to mitigate this
dilemma, this paper investigates the idea of combining multiple trained neural
networks using unlabeled data. In addition, combining multiple models into one
can speed up the inference, result in stronger, more capable models, and allows
us to select efficient device-friendly target network architectures. To this
end, the proposed method makes use of generation, filtering, and aggregation of
reliable pseudo-labels collected from unlabeled data. Our method supports using
an arbitrary number of input models with arbitrary architectures and
categories. Extensive performance evaluations demonstrated that our method is
very effective. For example, for the task of object detection and without using
any ground-truth labels, an EfficientDet-D0 trained on Pascal-VOC and an
EfficientDet-D1 trained on COCO, can be combined to a RetinaNet-ResNet50 model,
with a similar mAP as the supervised training. If fine-tuned in a
semi-supervised setting, the combined model achieves +18.6%, +12.6%, and +8.1%
mAP improvements over supervised training with 1%, 5%, and 10% of labels.","Deep learning has enabled achieving outstanding results on a wide range of applications in computer vision and image processing [5, 27]. However, the diversity of datasets and neural network architectures necessitates a careful selection of model architecture and training data that match best to the target application. Often times, for a same task, many models are available. These models might be trained on different datasets, or might come in different capacities, architectures, or even bit precisions. Motivation: A natural question that arises in this case, is whether we can combine the neural networks so that one combined network can perform the same task as several input networks. Fig. 1 shows an example, where two input object detection models to detect ‘per son’ and ‘vehicle’ are combined in one model. The beneﬁts of combining models include: a) possible latency improvements due to running one inference as opposed to many, b) in case input models cover partially overlapping or nonoverlapping classes/categories, one can build a stronger model with the union of the classes/categories through model composition © 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2110.10369v1  [cs.LG]  20 Oct 20212 BANITALEBIDEHKORDI, KANG, AND ZHANG: MODEL COMPOSITION Figure 1: An example of the proposed model composi tion approach for object detec tion. Two input models for detecting ‘person’ and ‘vehicle’ categories are combined to cre ate one model that does both. (i.e. merging models’ skills as in Fig. 1), and c) for applications involving model deploy ment, e.g. for cloud services providers, it can reduce the deployment frequency/load. Challenges: Creating a combined model from several input models is a challenging task. First, depending on the target task, the output model may need to have a speciﬁc architecture, and not necessarily one that is dictated by the input models. The input models themselves also might have different architectures. Second, in case input models are provided by users of a cloud system, or by different model creators/clients, the individual model owners would likely prefer not to share their training data, labels, not even weights or code. A privacy pre serving model composition approach should rely on only a minimum amount of information from the model creators. Third, input models may have only partially overlapping or disjoint class categories. This imposes a major challenge when combining the individual models. Existing methods: The existing solutions are mostly based on techniques such as knowl edge distillation [6, 16, 34] or ensembling [35], which may be useful when classes/categories are identical and labeled data are available, but not for the case of arbitrary classes/categories with only unlabeled data. More details regarding the existing approaches are provided in sec tion 2. In summary, to the best of our knowledge, the existing methods do not fully address the three challenges mentioned above. Our contributions: In this paper, we propose a simple yet effective method to address the model composition of neural networks. Our method supports combination of an arbitrary number of networks with arbitrary architectures. To train a combined model, we leverage the abundance of unlabeled data and having labels or original training data of the input models is not a requirement. However, if any labeled data are available, the algorithm uses them to further boost the performance of the output model. Furthermore, we put no restrictions on the type and number of object categories of the input models. We demonstrate the effectiveness of our method through an extensive set of experiments for the task of object detection. 2 Related works "
472,Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification.txt,"Online multi-object tracking is a fundamental problem in time-critical video
analysis applications. A major challenge in the popular tracking-by-detection
framework is how to associate unreliable detection results with existing
tracks. In this paper, we propose to handle unreliable detection by collecting
candidates from outputs of both detection and tracking. The intuition behind
generating redundant candidates is that detection and tracks can complement
each other in different scenarios. Detection results of high confidence prevent
tracking drifts in the long term, and predictions of tracks can handle noisy
detection caused by occlusion. In order to apply optimal selection from a
considerable amount of candidates in real-time, we present a novel scoring
function based on a fully convolutional neural network, that shares most
computations on the entire image. Moreover, we adopt a deeply learned
appearance representation, which is trained on large-scale person
re-identification datasets, to improve the identification ability of our
tracker. Extensive experiments show that our tracker achieves real-time and
state-of-the-art performance on a widely used people tracking benchmark.","Tracking multiple objects in a complex scene is a challenging problem in many video analysis and multimedia applications, such as visual surveillance, sport analysis, and autonomous driving. The objective of multiobject tracking is to estimate trajectories of objects in a speciﬁc category. Here we tackle the problem of people tracking by taking advantage of person reidentiﬁcation. Multiobject tracking beneﬁts a lot from advances in ob ject detection in the past decade. The popular trackingby detection methods apply the detector on each frame, and as sociate detection across frames to generate object trajectories. Both intracategory occlusion and unreliable detection are tremendous challenges in such a tracking framework [1, 2]. Intracategory occlusion and similar appearances of objects 0.70.80.91.0 619 622 625 628Score Frame Detection Track Candidates SelectedFig. 1 : Candidate selection based on uniﬁed scores. Candi dates from detection and tracks are visualized as blue solid rectangles and red dotted rectangles, respectively. Detection and tracks can complement each other for data association. can result in ambiguities in data association. Multiple cues, including motion, shape and object appearances, are fused to mitigate this problem [3, 4]. On the other hand, detec tion results are not always reliable. Pose variation and occlu sion in crowded scenes often cause detection failures such as false positives, missing detection, and nonaccurate bound ing. Some studies proposed to handle unreliable detection in a batch mode [2, 5, 6]. These methods address detection noises by introducing information from future frames. Detec tion results in whole video frames or a temporal window are employed and linked to trajectories by solving a global op timization problem. Tracking in a batch mode is noncausal and not suitable for timecritical applications. In contrast to these works, we focus on the online multiple people tracking problem, using only the current and past frames. In order to handle unreliable detection in an online mode, our tracking framework optimally selects candidates from outputs of both detection and tracks in each frame (as shownarXiv:1809.04427v1  [cs.CV]  12 Sep 2018in Figure 1). In most of the existing trackingbydetection methods, when talking about data association, candidates to be associated with existing tracks are only made up of de tection results. Yan et al. [4] proposed to treat the tracker and object detector as two independent identities, and keep results of them as candidates. They selected candidates based on handcrafted features, e.g., color histogram, optical ﬂow, and motion features. The intuition behind generating redun dant candidates is that detection and tracks can complement each other in different scenarios. On the one hand, reliable predictions from the tracker can be used for shortterm asso ciation in case of missing detection or nonaccurate bounding. On the other hand, conﬁdent detection results are essential to prevent tracks drifting to backgrounds in the long term. How to score outputs of both detection and tracks in an uniﬁed way is still an open question. Recently, deep neural networks, especially convolutional neural networks (CNN), have made great progress in the ﬁeld of computer vision and multimedia. In this paper, we take full advantage of deep neural networks to tackle unreliable detec tion and intracategory occlusion. Our contribution is three fold. First, we handle unreliable detection in online tracking by combining both detection and tracking results as candi dates, and selecting optimal candidates based on deep neu ral networks. Second, we present a hierarchical data associ ation strategy, which utilizes spatial information and deeply learned person reidentiﬁcation (ReID) features. Third, we demonstrate realtime and stateoftheart performance of our tracker on a widely used people tracking benchmark. 2. RELATED WORK "
48,Attention Disturbance and Dual-Path Constraint Network for Occluded Person Re-Identification.txt,"Occluded person re-identification (Re-ID) aims to address the potential
occlusion problem when matching occluded or holistic pedestrians from different
camera views. Many methods use the background as artificial occlusion and rely
on attention networks to exclude noisy interference. However, the significant
discrepancy between simple background occlusion and realistic occlusion can
negatively impact the generalization of the network.To address this issue, we
propose a novel transformer-based Attention Disturbance and Dual-Path
Constraint Network (ADP) to enhance the generalization of attention networks.
Firstly, to imitate real-world obstacles, we introduce an Attention Disturbance
Mask (ADM) module that generates an offensive noise, which can distract
attention like a realistic occluder, as a more complex form of
occlusion.Secondly, to fully exploit these complex occluded images, we develop
a Dual-Path Constraint Module (DPC) that can obtain preferable supervision
information from holistic images through dual-path interaction. With our
proposed method, the network can effectively circumvent a wide variety of
occlusions using the basic ViT baseline. Comprehensive experimental evaluations
conducted on person re-ID benchmarks demonstrate the superiority of ADP over
state-of-the-art methods.","Person reidentiﬁcation (ReID) refers to the process of matching pedestrian images captured by nonoverlapping cameras. This technique has gained popularity in recent years as surveillance systems have become more advanced and widespread. With the rapid development of deep learn *Corresponding Author. Significant discrepancy Functional SimilarTrain on Vanilla occlusionTest on Real occlusionTrain on ADM occlusionTest on Vanilla occlusionTest on Real occlusion (a) Baseline (b) ADP Figure 1. Visualization of attention to baseline and proposed ADP. (a) The baseline trained with the assistance of background occlu sion failed to avoid the realistic occlusion in the testing set. (b) The ADP trained by the proposed realitysimilar occlusion ADM performs well on both artiﬁcial and real occlusion. ing technology [8, 33, 5], ReID has also achieved re markable performance [20, 42, 45, 6, 2, 38] by meriting from its powerful feature extraction capabilities. How ever, most existing methods assume that the pedestrians in retrieved images are unobstructed, ignoring the possi ble occlusion problems that can occur in realworld sce narios. Consequently, these methods signiﬁcantly degrade when dealing with occluded images. While recent endeav ors have facilitated person ReID under occlusion condi tions [39, 36, 31, 18, 28, 16], two main problems associated with occlusions still need to be addressed. Firstly, the pres ence of obstacles will vanish some parts of the human body, missing and misaligned extracted features. Traditional Re ID methods cannot perform valid retrievals when some dis criminative parts are obscured. Secondly, occlusions intro duce noise into extracted features, polluting the ﬁnal featurearXiv:2303.10976v1  [cs.CV]  20 Mar 2023representation of each image. When dealing with these pol luted features, different identities may have high similarities due to the same obstacle, resulting in incorrect matches. To address the aforementioned problems, some meth ods [34, 7, 23, 24] use additional trained networks, such as human parsing and keypoint estimation, to align differ ent human parts. With the aid of these extra networks, the occluded parts can be repaired by disseminating informa tion from the visible parts. However, these approaches are severely limited due to the domain gap between the pre trained network and the ReID dataset. Recently, with the exploration of attention mechanisms for various vision tasks, it has also been adopted for oc cluded person ReID to eliminate the interference of noisy information [44, 29, 12]. During the process of attention learning, many data augmentation strategies [1, 36, 51] gen erate artiﬁcial occlusion, which directs the attention to per son and forces it to avoid occluded regions. Currently, the most widely used artiﬁcial occlusion methods are ran dom erasing [48] or using the background as occlusion [1]. Nevertheless, pretrained attention networks are inherently more likely to focus on the semantically rich foreground than the background. Therefore, network will inevitably tend to ignore the occlusion constituted by the background, which will result in a lack of generalization. To illustrate this point, we utilized background for artiﬁcial occlusion based on the ViT baseline in TransReID [12] and visual ize the attention for both the training and testing sets in Fig.1(a). The results demonstrate that while the baseline can avoid artiﬁcial occlusions well in the training set, atten tion is still disturbed in the face of actual occlusions in the testing set due to the signiﬁcant discrepancy between artiﬁ cial occlusion and actual occlusion. In this paper, we propose a solution to the challenges mentioned above by introducing an Attention Disturbance Mask (ADM) module that simulates realworld occlusions with greater ﬁdelity. The primary way in which occlusions disrupt models is by impeding attention. However, obtain ing enough occluded data to enable the model to avoid such disruptions is difﬁcult. To surmount this problem, we utilize an attackoriented methodology that produces noise masks with the capacity to simulate the interference effects of ac tual obstructions at the feature level. This enables us to con struct occlusions that mirror the effects of those encountered in realworld scenarios. As illustrated in Figure 1(b), the proposed Attention Disturbance Module (ADM) performs a similar role to realworld occlusions by introducing dis ruptions to the neural network’s attention. This ﬁnding di rectly veriﬁes the capability of our designed ADM in faith fully emulating occlusions at the feature level. By training the network on such occlusions that closely resemble those encountered in realworld scenarios, we can effectively en hance its robustness against occlusions during testing.However, handling complex occlusions directly can pose optimization challenges for the network. To address this is sue, we propose the DualPath Constraint Module (DPC) to handle both holistic and occluded images simultaneously, thus using holistic features as an extra supervisor to guide attention more towards the target pedestrian. Notably, the network parameters in the proposed DPC are shared by both paths, while the individual classiﬁers learn information about holistic and occluded images separately. The classi ﬁer of the holistic path can be considered a prototype for passing holistic information to the occluded path as an extra supervisor, and global metric learning is also used to inter act with the information of each path and align the holistic and occluded features. The main contributions of our method can be summa rized as below: • We ﬁrst introduce a novel attackbased augmenta tion strategy called the Attention Disturbance Mask (ADM), which simulates real occlusion at the feature level and effectively diverts attention away from actual occlusions during testing. • We propose a DualPath Constraint module (DPC) that utilizes dualpath interactions to encourage the net work to learn a more generalized attention mechanism. DPC is compatible with existing occlusionbased data augmentation methods and can provide signiﬁcant per formance improvements. • The two proposed methods are both used to assist in the training of the baseline, and can be discarded in the inference stage, making them easy to be compatible with many existing methods, indicating the efﬁciency and wide applicability of our method. • Trained with our proposed ADP, the transformer base line can achieve new stateoftheart performance on multiple benchmark datasets e.g.,74:5%on Rank1 on OccludedDuke dataset. 2. Related Work "
332,Adapting Convolutional Neural Networks for Geographical Domain Shift.txt,"We present the winning solution for the Inclusive Images Competition
organized as part of the Conference on Neural Information Processing Systems
(NeurIPS 2018) Competition Track. The competition was organized to study ways
to cope with domain shift in image processing, specifically geographical shift:
the training and two test sets in the competition had different geographical
distributions. Our solution has proven to be relatively straightforward and
simple: it is an ensemble of several CNNs where only the last layer is
fine-tuned with the help of a small labeled set of tuning labels made available
by the organizers. We believe that while domain shift remains a formidable
problem, our approach opens up new possibilities for alleviating this problem
in practice, where small labeled datasets from the target domain are usually
either available or can be obtained and labeled cheaply.","Domain shift is a problem often arising in machine learning, when a model i s trained on a dataset that might be sufﬁciently large and diverse, but later the model is supposed to be applied to datasets with a different data distribution . One important example of this problem is the geographical domain shift in image processing, when, e.g., the same semantic category of objects can look quite different o n photos taken in different geographical locations (see Fig. 1). Domain shift also often results from dataset bias : e.g., a dataset of human faces heavily shifted towards Cauca sian faces would suffer from this problem when applied in, e.g., Asia. Modern techniques in domain adaptation (see references in S ection 2) usually op erate in conditions where the target domain is completely di fferent from the source domain in some aspects; e.g., the source domain are syntheti c images generated arti ﬁcially and the target domain includes the corresponding re al images. Geographical domain shift is a more subtle problem: in an image classiﬁcat ion problem with geo graphical shift, some classes will not change at all from the source to target domain, while others might change radically. In this work, we present the winning solution for the Inclusi ve Images Competi tion [ 1] organized as part of the Conference on Neural Information P rocessing Systems 1(NeurIPS 2018) Competition Track. Based on the work [ 25], the challenge was in tended to develop solutions for the geodiversity problem, w ith a training set skewed towards North America and Western Europe and test sets drawn from completely dif ferent geographic distributions that were not revealed to t he competitors. One interesting property of our solution is that it is relati vely straightforward and simple. We did not use any state of the art models for domain ad aptation, and our ﬁnal solution is an ensemble of several CNNs where only the last la yer is ﬁnetuned with the help of a small labeled set of tuning labels (Stage 1 set) t hat was made available by the organizers. It turned out that this set had a geographica l distribution similar enough to the hidden Stage 2 evaluation set, and the very small set of tuning labels (only 1000 examples) proved to sufﬁce, with proper techniques such as d ata augmentation and ensembling, to adapt the base models to a new domain. The paper is organized as follows. In Section 2, we survey some related work on domain shift and domain adaptation. Section 3introduces the problem of the Inclusive Images Challenge and describes the dataset and evaluation m etrics. Section 4presents our solution in detail, Section 5shows experimental results for both singlemodel so lutions and ensembles, and Section 6concludes the paper. 2 Related Work "
124,SOMA: Solving Optical Marker-Based MoCap Automatically.txt,"Marker-based optical motion capture (mocap) is the ""gold standard"" method for
acquiring accurate 3D human motion in computer vision, medicine, and graphics.
The raw output of these systems are noisy and incomplete 3D points or short
tracklets of points. To be useful, one must associate these points with
corresponding markers on the captured subject; i.e. ""labelling"". Given these
labels, one can then ""solve"" for the 3D skeleton or body surface mesh.
Commercial auto-labeling tools require a specific calibration procedure at
capture time, which is not possible for archival data. Here we train a novel
neural network called SOMA, which takes raw mocap point clouds with varying
numbers of points, labels them at scale without any calibration data,
independent of the capture technology, and requiring only minimal human
intervention. Our key insight is that, while labeling point clouds is highly
ambiguous, the 3D body provides strong constraints on the solution that can be
exploited by a learning-based method. To enable learning, we generate massive
training sets of simulated noisy and ground truth mocap markers animated by 3D
bodies from AMASS. SOMA exploits an architecture with stacked self-attention
elements to learn the spatial structure of the 3D body and an optimal transport
layer to constrain the assignment (labeling) problem while rejecting outliers.
We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is
more accurate and robust than existing state of the art research methods and
can be applied where commercial systems cannot. We automatically label over 8
hours of archival mocap data across 4 different datasets captured using various
technologies and output SMPL-X body models. The model and data is released for
research purposes at https://soma.is.tue.mpg.de/.","Markerbased optical motion capture (mocap) systems record 2D infrared images of light reﬂected or emitted by a set of markers placed at key locations on the surface of a subject’s body. Subsequently, the mocap systems recover the precise position of the markers as a sequence of sparse and unordered points or short tracklets. Powered by years of commercial development, these systems offer high tem poral and spatial accuracy. Richly varied mocap data from such systems is widely used to train machine learning meth ods in action recognition, motion synthesis, human motion modeling, pose estimation, etc. Despite this, the largest ex isting mocap dataset, AMASS [31], has about 45 hours of mocap, much smaller than video datasets used in the ﬁeld. Mocap data is limited since capturing and processing it is expensive. Despite its value, there are large amounts of archival mocap in the world that have never been labeled;arXiv:2110.04431v1  [cs.CV]  9 Oct 2021this is the “dark matter” of mocap. The problem is that, to solve for the 3D body, the raw mocap point cloud (MPC) must be “labeled”; that is, the points must be assigned to physical “marker” locations on the subject’s body. This is challenging because the MPC is noisy and sparse and the labeling problem is ambiguous. Existing commercial tools, e.g. [22, 30], offer partial automation, however none provide a complete solution to automatically handle vari ations in marker layout, i.e. number of markers used and their rough placement on the body, variation in subject body shape and gender, and variation across capture technolo gies namely active vs passive markers or brands of system. These challenges typically preclude costeffective labeling of archival data, and add to the cost of new captures by re quiring manual cleanup. Automating the mocap labeling problem has been ex amined by the research community [15, 17, 20]. Existing methods focus on ﬁxing the mistakes in already labeled markers through denoising [9, 20]. Recent work formulates the problem in a matching framework, directly predicting the label assignment matrix for a ﬁxed number of markers in a restricted setup [15]. In short, the existing methods are limited to a constrained range of motions [15], a single body shape [9, 17, 20], a certain capture scenario, a spe cial marker layout, or require a subjectspeciﬁc calibration sequence [15, 22, 30]. Other methods require highquality real mocap marker data for training, effectively prohibiting their scalability to novel scenarios [9, 15]. To address these shortcomings we take a datadriven ap proach and train a neural network endtoend with self attention components and an optimal transport layer to pre dict a perframe constrained inexact matching between mo cap points and labels. Having enough “real” data for train ing is not feasible, therefore we opt for synthetic data. Given a marker layout, we generate synthetic mocap point clouds with realistic noise, and then train a layoutspeciﬁc network that can cope with realistic variations across a whole mocap dataset. While previous works have exploited synthetic data [17, 20], they are limited in terms of body shapes, motions, marker layouts, and noise sources. Even with a large synthetic corpus of MPC, labeling a cloud of sparse 3D points, containing outliers and missing data, is a highly ambiguous task. The key to the solution lies in the fact that the points are structured, as is their variation with articulated pose. Speciﬁcally, they are constrained by the shape and motion of the human body. Given sufﬁcient training data, our attentional framework learns to exploit lo cal context at different scales. Furthermore, if there were no noise, the mapping between labels and points would be onetoone. We formulate these concepts as a uniﬁed train ing objective enabling endtoend model training. Specif ically, our formulation exploits a transformer architecture to capture local and global contextual information usingselfattention (Sec. 4.1). By generating synthetic mocap data with varying body shapes and poses, SOMA implic itly learns the kinematic constraints of the underlying de formable human body (Sec. 4.4). A onetoone match be tween 3D points and markers, subject to missing and spuri ous data, is achieved by a special normalization technique (Sec. 4.2). To provide a common output framework, consis tent with [31], we use MoSh [29, 31] as a postprocessing step to ﬁt SMPLX [38] to the labeled points; this also helps deal with missing data caused by occlusion or dropped markers. The SOMA system is outlined in Fig. 3. To generate training data, SOMA requires a rough marker layout that can be obtained by a single labeled frame, which requires minimal effort. Afterward, virtual markers are automatically placed on a SMPLX body and animated by motions from AMASS [31]. In addition to common mocap noise models like occlusions [15, 17, 20], and ghost points [17, 20], we introduce novel terms to vary maker placement on the body surface and we copy noise from real marker data in AMASS (Sec. 4.4). We train SOMA once for each mocap dataset and apart from the one layout frame, we do not require any labeled real data. After training, given a noisy MPC frame as input, SOMA predicts a distribution over labels of each point, including a null la bel for ghost points. We evaluate SOMA on several challenging datasets and ﬁnd that we outperform the current state of the art numer ically while being much more general. Additionally, we capture new MPC data using a Vicon mocap system and compare handlabeled groundtruth to Sh ¯ogun and SOMA output. SOMA performs similarly compared with the com mercial system. Finally, we apply the method on archival mocap datasets: Mixamo [11], DanceDB [5], and a previ ously unreleased portion of the CMU mocap dataset [12]. In summary, our main contributions are: (1) a novel neu ral network architecture exploiting selfattention to process sparse deformable point cloud data; (2) a system that con sumes mocap point clouds directly and outputs a distribu tion over marker labels; (3) a novel synthetic mocap gener ation pipeline that generalizes to real mocap datasets; (4) a robust solution that works with archival data, different mo cap technologies, poor data quality, and varying subjects and motions; (5) 220 minutes of processed mocap data in SMPLX format, trained models, and code are released for research purposes. 2. Related Work "
508,Emotional Expression Detection in Spoken Language Employing Machine Learning Algorithms.txt,"There are a variety of features of the human voice that can be classified as
pitch, timbre, loudness, and vocal tone. It is observed in numerous incidents
that human expresses their feelings using different vocal qualities when they
are speaking. The primary objective of this research is to recognize different
emotions of human beings such as anger, sadness, fear, neutrality, disgust,
pleasant surprise, and happiness by using several MATLAB functions namely,
spectral descriptors, periodicity, and harmonicity. To accomplish the work, we
analyze the CREMA-D (Crowd-sourced Emotional Multimodal Actors Data) & TESS
(Toronto Emotional Speech Set) datasets of human speech. The audio file
contains data that have various characteristics (e.g., noisy, speedy, slow)
thereby the efficiency of the ML (Machine Learning) models increases
significantly. The EMD (Empirical Mode Decomposition) is utilized for the
process of signal decomposition. Then, the features are extracted through the
use of several techniques such as the MFCC, GTCC, spectral centroid, roll-off
point, entropy, spread, flux, harmonic ratio, energy, skewness, flatness, and
audio delta. The data is trained using some renowned ML models namely, Support
Vector Machine, Neural Network, Ensemble, and KNN. The algorithms show an
accuracy of 67.7%, 63.3%, 61.6%, and 59.0% respectively for the test data and
77.7%, 76.1%, 99.1%, and 61.2% for the training data. We have conducted
experiments using Matlab and the result shows that our model is very prominent
and flexible than existing similar works.","The human voice is very complex and can show a wide range of emotions. Emotion in speech gives  information about how people act or feel. There are many functions of the human vocal system that make it  possible for humans to speak. These include tone, pitch, energy, entropy , and m any other aspects of the  speech. The increasing need for human machine interactions indicates that more tasks to be done to improve  the results of these interactions, like giving computer and machine interfaces the ability to understand how a  person feels when they speak. Emotions play a big part in how people talk to each other. People and machines  should be able to work together more effectively if computers have built in skills for figuring out how people  feel [2], [5]. Today, a lot of resources  and time  is being spent on improving artificial intelligence and smart  machines to make our life easier and more comfortable. According to the findings of several pieces of  literature, human feelings regulate the decision making process to some extent [1] [4]. If the machine can  figure out how people are feeling when they speak, it will be able to respond and communicate properl y.   Recognizing people's feelings based on what they say is still a challenge. The authors in [3] propose CNNs  (Convolutional Neural Networ ks), RNNs (Recurrent Neural Networks), and time distributed CNNs based  network s in their study, but they do not use any usual hand crafted features that are typically used to identify  emotional speech. To solve the SER (Speech Emotional Recognition) proble m, they  integrate LSTM (Long  Short Term Memory) network layers into a deep hierarchical CNN feature extraction architecture. They have  got almost 87% and 78% accuracy.   2. RELATED WORKS   "
389,Sequential Bayesian Neural Subnetwork Ensembles.txt,"Deep neural network ensembles that appeal to model diversity have been used
successfully to improve predictive performance and model robustness in several
applications. Whereas, it has recently been shown that sparse subnetworks of
dense models can match the performance of their dense counterparts and increase
their robustness while effectively decreasing the model complexity. However,
most ensembling techniques require multiple parallel and costly evaluations and
have been proposed primarily with deterministic models, whereas sparsity
induction has been mostly done through ad-hoc pruning. We propose sequential
ensembling of dynamic Bayesian neural subnetworks that systematically reduce
model complexity through sparsity-inducing priors and generate diverse
ensembles in a single forward pass of the model. The ensembling strategy
consists of an exploration phase that finds high-performing regions of the
parameter space and multiple exploitation phases that effectively exploit the
compactness of the sparse model to quickly converge to different minima in the
energy landscape corresponding to high-performing subnetworks yielding diverse
ensembles. We empirically demonstrate that our proposed approach surpasses the
baselines of the dense frequentist and Bayesian ensemble models in prediction
accuracy, uncertainty estimation, and out-of-distribution (OoD) robustness on
CIFAR10, CIFAR100 datasets, and their out-of-distribution variants: CIFAR10-C,
CIFAR100-C induced by corruptions. Furthermore, we found that our approach
produced the most diverse ensembles compared to the approaches with a single
forward pass and even compared to the approaches with multiple forward passes
in some cases.","Deep learning has been the engine that powers stateoftheart performance in a wide array of machine learning tasks [ 1]. However, deep learning models still suffer from many fundamental issues from the perspective of statistical modeling, which are crucial for ﬁelds such as autonomous driving, healthcare, and science [ 2]. One of the major challenges is their ability to reliably model uncer tainty while capturing complex data dependencies and being computationally tractable. Probabilistic machine learning and, especially, the Bayesian framework provides an exciting avenue to address these challenges. In addition to superior uncertainty quantiﬁcation, Bayesian models have also been shown to have improved robustness to noise and adversarial perturbations [ 3] due to probabilistic prediction capabilities. Bayesian neural networks (BNNs) have pushed the envelope of probabilistic machine learning through the combination of deep neural network architecture and Bayesian infer ence. However, due to the enormous number of parameters, BNNs adopt approximate inference techniques such as variational inference with a fully factorized approximating family [ 4]. Although Preprint. Under review.arXiv:2206.00794v1  [stat.ML]  1 Jun 2022this approximation is crucial for computational tractability, they could lead to underutilization of BNN’s true potential [5]. Recently, ensemble of neural networks [ 6] has been proposed to account for the parameter/model uncertainty, which has been shown to be analogous to the Bayesian model averaging and sampling from the parameter posteriors in the Bayesian context to estimate the posterior predictive distribution [7]. In this spirit, the diversity of the ensemble has been shown to be a key to improving the predictions, uncertainty, and robustness of the model. To this end, diverse ensembles can mitigate some of the shortcomings introduced by approximate Bayesian inference techniques without compromising computational tractability. Several different diversityinducing techniques have been explored in the literature. The approaches range from using a speciﬁc learning rate schedule [ 8], to introducing kernalized repulsion terms among the ensembles in the loss function at train time [ 9], mixture of approximate posteriors to capture multiple posterior modes [ 10], appealing to sparsity (albeit adhoc) as a mechanism for diversity [ 11,12] and ﬁnally appealing to diversity in model architectures through neural architecture and hyperparameter searches [13, 14]. However, most approaches prescribe parallel ensembles, with each individual model part of an ensemble starting with a different initialization, which can be expensive in terms of computation as each of the ensembles has to train longer to reach the highperforming neighborhood of the parameter space. Although the aspect of ensemble diversity has taken center stage, the cost of training these ensembles has not received much attention. However, given that the size of models is only growing as we advance in deep learning, it is crucial to reduce the training cost of multiple individual models forming an ensemble in addition to increasing their diversity. To this end, sequential ensembling techniques offer an elegant solution to reduce the cost of obtaining multiple ensembles, whose origin can be traced all the way back to [ 15,16], wherein ensembles are created by combining epochs in the learning trajectory. [ 17,18] use intermediate stages of model training to obtain the ensembles. [ 19] used boosting to generate ensembles. In contrast, recent works by [8,20,12] force the model to visit multiple local minima by cyclic learning rate annealing and collect ensembles only when the model reaches a local minimum. Notably, the aforementioned sequential ensembling techniques in the literature have been proposed in the context of deterministic machine learning models. Extending the sequential ensembling technique to Bayesian neural networks is attractive because we can potentially get highperforming ensembles without the need to train from scratch, analogous to sampling with a Markov chain Monte Carlo sampler that extracts samples from the posterior distribution. Furthermore, sequential ensembling is complementary to the parallel ensembling strategy, where, if the models and computational resources permit, each parallel ensemble can generate multiple sequential ensembles, leading to an overall increase in the total number of diverse models in an ensemble. A new frontier to improve the computational tractability and robustness of neural networks is spar sity [ 21]. Famously, the lottery ticket hypothesis [ 22] established the existence of sparse subnetworks that can match the performance of the dense model. Studies also showed that such subnetworks tend to be inherently diverse as they correspond to different neural connectivity [ 11,12]. However, most sparsityinducing techniques proposed have been in the context of deterministic networks and use adhoc and posthoc pruning to achieve sparsity [ 23,24]. Moreover, the weight pruning methods have been shown to provide inefﬁcient computational gains owing to the unstructured sparse subnetworks [25]. In Bayesian learning, the prior distribution provides a systematic approach to incorporate inductive bias and expert knowledge directly into the modeling framework [ 26]. First, the automatic datadriven sparsity learning in Bayesian neural networks is achieved using sparsityinducing priors [27]. Second, the use of group sparsity priors [ 28–30] provides structural sparsity in Bayesian neural networks leading to signiﬁcant computational gains. We leverage the automated structural sparsity learning using spikeandslab priors similar to [ 30] in our approach to sequentially generate multiple Bayesian neural subnetworks with varying sparse connectivities which when combined yields highly diverse ensemble. To this end, we propose Sequential Bayesian Neural Subnetwork Ensembles ( SeBayS ) with the following major contributions: •We propose a sequential ensembling strategy for Bayesian neural networks (BNNs) which learns multiple subnetworks in a single forwardpass. The approach involves a single exploration phase with a large (constant) learning rate to ﬁnd highperforming sparse network connectivity yielding structurally compact network. This is followed by multiple exploitation 2phases with sequential perturbation of variational mean parameters using corresponding variational standard deviations together with piecewiseconstant cyclic learning rates. •We combine the strengths of the automated sparsityinducing spikeandslab prior that allows dynamic pruning during training, which produces structurally sparse BNNs, and the proposed sequential ensembling strategy to efﬁciently generate diverse and sparse Bayesian neural networks, which we refer to as Bayesian neural subnetworks . 2 Related Work "
544,Autoselection of the Ensemble of Convolutional Neural Networks with Second-Order Cone Programming.txt,"Ensemble techniques are frequently encountered in machine learning and
engineering problems since the method combines different models and produces an
optimal predictive solution. The ensemble concept can be adapted to deep
learning models to provide robustness and reliability. Due to the growth of the
models in deep learning, using ensemble pruning is highly important to deal
with computational complexity. Hence, this study proposes a mathematical model
which prunes the ensemble of Convolutional Neural Networks (CNN) consisting of
different depths and layers that maximizes accuracy and diversity
simultaneously with a sparse second order conic optimization model. The
proposed model is tested on CIFAR-10, CIFAR-100 and MNIST data sets which gives
promising results while reducing the complexity of models, significantly.","Machine learning has made great progress in the last few years due to ad vances in the use of Deep Neural Networks (DNN), but many of the proposed neural architectures come with high computational and memory requirements Blalock et al. (2020). These demands increase the cost of training and deploy ing these architectures, and constrain the spectrum of devices that they can be used on. Although deep learning has been very promising in many areas in terms of technology in recent years, it has some problems that need improvement. Even though deep learning can distinguish changes and subtle dierences in data with interconnected neural networks, it makes it very dicult to dene hyperparameters and determine their values before training the data. For this reason, dierent pruning methods have been proposed in the literature to reduce the parameters of convolutional networks Han et al. (2016); Hanson & Pratt (1988); LeCun & Cortes (2010); Str om (1997). The common problem of deep learning and pruning algorithms that have been studied in recent years is the decision of the pruning percentage with the heuristic approach at the pruning stage, making the success percentage of the deep learning algorithm dependent on the pruning parameter. On the other hand, the optimization models proposed with zeronorm penalty to ensure sparsity ignore the diversity of the layers as they only take into account the percentage of success. The combination of the layers that are close to each other does not increase the percentage of accuracy. As the primal example of DNNs, Convolutional Neural Networks (CNNs) are feedforward architectures originally proposed to perform image processing tasks, Li et al. (2021) but they oered such high versatility and capacity that allowed them to be used in many other tasks including time series prediction, signal identication and natural language processing. Inspired by a biological visual perceptron that displays local receptive elds Goodfellow et al. (2016), a CNN uses learnable kernels to extract the relevant features at each processing 2level. This provides the advantage of local processing of information and weight sharing. Additionally, the use of the pooling mechanism allows the data to be downsampled and the features to be invariant representations of previous features. Recently, a great deal of attention has been paid to obtain and train com pact CNNs with the eects of pruning Wen et al. (2016); Zhou et al. (2016). Generally, pruning techniques dier from each other in the choice of structure, evaluation, scheduling, and netuning Blalock et al. (2020). However, tuning these choices according to its costs and requirements, the network should be pruned. This is a systematic reduction in neural network parameters while aiming to maintain a performance prole comparable to the original architec ture Blalock et al. (2020). Pruning of parameters in neural networks has been used to reduce the complexity of the models and prevent overtting. Dier ent methods on pruning in deep neural networks are proposed in the literature such as; Biased Weight Decay Hanson & Pratt (1988), Optimal Brain Damage LeCun et al. (1989) and Optimal Brain Surgeon Hassibi & Stork (1992). These aim to identify redundant parameters represented by connections within archi tectures and perform pruning by removing such redundancies Srinivas & Babu (2015); Han et al. (2016). In another study, a new layerbased pruning technique was proposed for deep neural networks where the parameters of each layer are pruned independently according to the secondorder derivatives of a layerbased error function with respect to the relevant parameters Dong et al. (2017). Poor estimation performance is observed after pruning is limited to a linear combi nation of reconstructed errors occurring in each layer Dong et al. (2017). Some pruning techniques, such as unstructured pruning, work on individual parame ters that induce sparsity while the memory footprint of the architecture reduces although there is not any speed advantage. Other techniques work on groups of parameters, where a neuron lter layer is considered as a separate target for the algorithm, thus provides the ability to take advantage of existing computational hardware Li et al. (2016). Evaluation of pruned parameters can be based on relevance to training 3metrics, absolute value, activation, and contribution of gradients Blalock et al. (2020). The architecture can be considered and evaluated as a whole Lee et al. (2019); Frankle & Carbin (2019), while parameters can be scored locally with reference to a small group of closely related groups of networks Han et al. (2016). Iterative pruning is performed by considering a subset of the archi tecture Han et al. (2016) and the speed of the pruning algorithm varies in a single step process Liu et al. (2019). Changing the pruning rate according to a certain rule have also been tried and implemented in the literature Gale et al. (2019). After applying the pruning algorithm, some techniques continued to use the same training weights; while the reduced architecture obtained in the studies was retrained to a certain state Frankle & Carbin (2019) or initialized and restarted from the initial state Liu et al. (2019). Deep Ensembling techniques increase reliability of DNNs as multiple diverse models are combined into a stronger model Fort et al. (2019). It was shown that members of the deep ensemble models provide dierent predictions on the same input increasing diversity Garipov et al. (2018) and providing more calibrated probabilities Lakshminarayanan et al. (2017). In one of the recent studies, ensemble technique is chosen to increase the complexity by training end toend two EcientNetb0 models with bagging. Adaptive ensemble technique is used by netuning within a trainable combination layer which outperforms dierent studies for widely known datasets such as CIFAR10 and CIFAR100 Bruno et al. (2022). Since pretrained models boosts eciency while simplifying the hyperparameter tuning, increasing the performance on these datasets are achieved with the help of transfer learning and pretraining Kolesnikov et al. (2020); Sun et al. (2017). Researchers accelerated CNNs but get lower perfor mance metrics on CIFAR100. One of these techniques is FATNET algorithm where high resolution kernels are used for classication while reducing the num ber of trainable parameters based on Fourier transform Ibadulla et al. (2022). Ensemble pruning methods are used to reduce the computational complexity of ensemble models, and remove the duplicate models existing in the ensemble. Finding a small subset of classiers that perform equivalent to a boosted ensem 4ble is an NPhard problem Tamon & Xiang (2000). Search based methods can be used to select an ensemble as an alternative. These methods conduct a search in the ensemble space and evaluate performance of various subsets of classiers Sagi & Rokach (2018). Deep learning can actually be thought as multilayer ar ticial neural networks, which is an example of ensemble learning. The overall percentage of success for ensemble learning is proportional to the percentage of average accuracy of the ensemble and the diversity of each learner within the ensemble. However, the percentage of accuracy and diversity have tradeos among themselves. In other words, an increase in the percentage of accuracy within the community causes a decrease in diversity, which means redundant merge of similar layers. It has been shown that a pruning problem can be re framed to a quadratic integer problem for classication to seek a subset that optimizes accuracydiversity tradeo using a semidenite programming tech nique Zhang et al. (2006). In the vein of that research, multiple optimization models have been proposed to utilize the quadratic nature of pruning problems. Sparse problem minimization and loworder approximation techniques have recently been used in numerous elds such as computer vision, machine learn ing, telecommunications, and more Quach et al. (2017). In such problems, in cluding zeronorm corresponds to sparsity which makes the objective function nonconvex in the optimization problem. For this reason, dierent relaxation techniques have been proposed in the optimization literature for nonconvex ob jective functions. For class of a nonconvex quadratic constrained problem, SDP relaxation techniques have been developed based on the dierence of two con vex functions (Dierence of Convex { DC) decomposition strategy Zheng et al. (2011); OzogurAkyuz et al. (2020). However, since SDP algorithms for high dimensional problems are slow and occupy a lot of memory, SDP algorithms have been relaxed into quadratic conic problems. SecondOrder Cone programming (SOCP) is a problemsolving class that lies between linear (LP) or quadratic programming (QP) and semidenitive programming (SDP). Like LP and SDP, SOCPs can be solved eciently with primal dual interiorpoint methods Potra & Wright (2000). Also, various en 5gineering problems can be formulated as quadratic conic problems Lobo et al. (1998). In the literature, new convex relaxations are suggested for nonconvex quadratically constrained quadratic programming (QCQP) problems related to this issue. Since basic semidenite programming relaxation is often too loose for general QCQP, recent studies have focused on enhancing convex relaxations using valid linear or SOC inequalities. Valid secondorder cone constraints were created for the nonconvex QCQP and the duality dierence was reduced by us ing these valid constraints Jiang & Li (2019). In addition, a branch and bound algorithm has been developed in the literature to solve continuous optimiza tion problems in which a nonconvex objective function is minimized under nonconvex inequality constraints that satisfy certain solvability assumptions Beck & Pan (2017). In this study, we propose a novel optimization model to prune the ensemble of CNNs with a SecondOrder Cone Programming (SOCP) model. The proposed optimization model selects the best subset of models in the ensemble of CNNs by optimizing the accuracydiversity tradeo while reducing the number of models and computational complexity. In the following chapters; denition of the loss function and relaxations for SOCP model will be introduced in Section 2, developing the ensemble of CNNs with hardware, software specications, datasets and results will be given in Section 3 and lastly, the conclusions and future work will be mentioned in Section 4, respectively. Contribution of The Study The contributions of the proposed study are listed as follows; •One of the unique values that this study will add to the literature is the fact that the parameter called the pruning percentage, which will meet the needs of the deep learning literature, will be directly obtained by the proposed secondorder conical optimization model. •Another important original value will be a solution to the ignorance of the diversity criterion in the literature for pruning deep learning networks 6only on the basis of accuracy performance. The objective function in the proposed optimization model simultaneously optimizes the accuracy and diversity criteria by pruning the ensemble of CNNs. •Since SOCP models gives more successful and faster results than the other sparse optimization models. Hence, the number of models in the ensemble of CNNs will be reduced signicantly by using the proposed SOCP model. 2. Methods and Materials "
141,BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification.txt,"Deep learning methods have shown outstanding classification accuracy in
medical imaging problems, which is largely attributed to the availability of
large-scale datasets manually annotated with clean labels. However, given the
high cost of such manual annotation, new medical imaging classification
problems may need to rely on machine-generated noisy labels extracted from
radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been
modelled from datasets with noisy labels, but their training procedure is in
general not robust to noisy-label samples, leading to sub-optimal models.
Furthermore, CXR datasets are mostly multi-label, so current noisy-label
learning methods designed for multi-class problems cannot be easily adapted. In
this paper, we propose a new method designed for the noisy multi-label CXR
learning, which detects and smoothly re-labels samples from the dataset, which
is then used to train common multi-label classifiers. The proposed method
optimises a bag of multi-label descriptors (BoMD) to promote their similarity
with the semantic descriptors produced by BERT models from the multi-label
image annotation. Our experiments on diverse noisy multi-label training sets
and clean testing sets show that our model has state-of-the-art accuracy and
robustness in many CXR multi-label classification benchmarks.","The promising results produced by deep neural networks (DNN) in medical image analysis (MIA) problems [30] is attributed to the availability of largescale datasets with ac curately annotated images. Given the high cost of acquir ing such datasets, the ﬁeld is considering more affordable automatic annotation processes by Natural Language Pro cessing (NLP) approaches that extract multiple labels (each label representing a disease) from radiology reports [20,54]. *First two authors contributed equally to this work. Descriptor Subgraph Sample Graph  Multilabel  (BoMD) Smoothly RelabelMIDFeature extractorRobust Loss  Multiclass  (Noisyrobust)Feature extractorClassifier  Multiclass  (Noisycleaning) Pseudo Labelling Sample Graph  Multiclass  (Graph based) Feature extractorFeature extractor  Multiclass  (Transition matrix) ClassifierTrainsition MatrixClassifierFigure 1: Comparison of multiclass LNL methods [1, 21, 29, 33] and our noisy multilabel approach, BoMD, where the feature extractor returns a single descriptor vper im age,Dis the noisy training set, Cis the clean set, and ~Dis the relabelled training set. BoMD has two compo nents: 1) learning of a bag of multilabel image descriptors (MID)fv1;v2;v3gto represent the image, and 2) smooth relabelling of images driven by a graph structure based on the ﬁnegrained relationships between MID descriptors. However, mistakes made by NLP combined with uncer tain radiological ﬁndings can introduce label noise [42,43], as can be found in NLPannotated Chest Xray (CXR) datasets [20, 54] whose noisy multilabels can mislead su pervised training processes. Nevertheless, even without ad dressing this noisy multilabel problem, current CXR multi label classiﬁers [3,15,22,37,48,61] show promising results. Although these methods show encouraging multilabel clas siﬁcation accuracy, there is still potential for improvement that can be realised by properly addressing the noisy multi label learning problem present in CXR datasets [20, 54].arXiv:2203.01937v3  [eess.IV]  12 May 2023Current learning with noisy label (LNL) approaches fo cus on leveraging the hidden cleanlabel information to as sist the training of DNNs (see Fig. 1). This can be achieved with techniques that clean the label noise [26,29], robustify loss functions [21, 31, 33], estimate label transition matri ces [1,12,59,62], smooth training labels [36,56,63], and use graphs to explore the latent structure of data. [21, 57, 58]. These methods have been designed for noisy multiclass problems and do not easily extend to noisy multilabel learning, which is challenging given the potential multi ple label mistakes for each training sample. To the best of our knowledge, the stateoftheart (SOTA) approach that handles noisy multilabel learning is NVUM [31], which is based on an extension of early learning regularisation (ELR) [33]. NVUM shows promising results, but it is chal lenged by the different early convergence patterns of mul tiple labels, which can lead to poor performance for partic ular label noise conditions, as shown in our experiments. Additionally, NVUM is only evaluated on realworld CXR datasets [20, 54] without any systematic assessment of ro bustness to varying label noise conditions, preventing a more complete understanding of its functionality. In this paper, we propose a new solution speciﬁcally designed for the noisy multilabel problem by answering the following question: can the detection and correction of noisy multilabelled samples be facilitated by leverag ing the semantic information of training labels? available from language models [19, 28, 46]? This question is moti vated by the successful exploration of language models in computer vision [3, 7, 16, 64], with methods that leverage semantic information to inﬂuence the training of visual de scriptors; an idea that has not been explored in noisy multi label classiﬁcation. To answer this question, we introduce the 2stage Bag of Multilabel Descriptors (BoMD) method (see Fig. 1) to smoothly relabel noisy multilabel image datasets that can then be used for training common multi label classiﬁers. The ﬁrst stage trains a feature extractor to produce a bag of multilabel image descriptors by promot ing their similarity with the semantic embeddings from lan guage models. For the second stage, we introduce a novel graph structure, where each image is represented by a sub graph built from the multilabel image descriptors, learned in the ﬁrst stage, to smoothly relabel the noisy multilabel images. Compared with graphs built directly from a sin gle descriptor per image [21], our graph structure with the multilabel image descriptors has the potential to capture more ﬁnegrained image relationships, which is crucial to deal with multilabel annotation. We also propose a new benchmark to systematically assess noisy multilabel meth ods. In summary, our contributions are: 1. A novel 2stage learning method to smoothly relabel noisy multilabel datasets of CXR images that can then be used for training a common multilabel classiﬁer;2. A new bag of multilabel image descriptors learning method that leverages the semantic information avail able from language models to represent multilabel im ages and to detect noisy samples; 3. A new graph structure to smoothly relabel noisy multilabel images, with each image being represented by a subgraph of the learned multilabel image descrip tors that can capture ﬁnegrained image relationships; 4. The ﬁrst systematic evaluation of noisy multilabel methods that combine the PadChest [6] and Chest X ray 14 [54] datasets. We show the effectiveness of our BoMD on a bench mark that consists of training with two noisy multilabel CXR datasets and testing on three clean multilabel CXR datasets [31]. Results show that our approach has more accurate classiﬁcations than previous multilabel classiﬁers developed for CXR datasets and noisylabel classiﬁers. Re sults on our proposed benchmark show that BoMD is gen erally more accurate and robust than competing methods under our systematic evaluation. 2. Related Works "
262,Exploiting Temporal Coherence for Self-Supervised One-shot Video Re-identification.txt,"While supervised techniques in re-identification are extremely effective, the
need for large amounts of annotations makes them impractical for large camera
networks. One-shot re-identification, which uses a singular labeled tracklet
for each identity along with a pool of unlabeled tracklets, is a potential
candidate towards reducing this labeling effort. Current one-shot
re-identification methods function by modeling the inter-relationships amongst
the labeled and the unlabeled data, but fail to fully exploit such
relationships that exist within the pool of unlabeled data itself. In this
paper, we propose a new framework named Temporal Consistency Progressive
Learning, which uses temporal coherence as a novel self-supervised auxiliary
task in the one-shot learning paradigm to capture such relationships amongst
the unlabeled tracklets. Optimizing two new losses, which enforce consistency
on a local and global scale, our framework can learn learn richer and more
discriminative representations. Extensive experiments on two challenging video
re-identification datasets - MARS and DukeMTMC-VideoReID - demonstrate that our
proposed method is able to estimate the true labels of the unlabeled data more
accurately by up to $8\%$, and obtain significantly better re-identification
performance compared to the existing state-of-the-art techniques.","Person reidentication (reID) aims to solve the challenging problem of matching identities across nonoverlapping views in a multicamera system. The surge of deep neural networks in computer vision [ 17,30] has been re ected in person reID as well, with impressive performance over a wide variety of datasets [ 33,5]. However, this improved performance has predominantly been achieved through supervised learning , facilitated by the availability of large amounts of annotated data. However, acquiring identity labels for a large set of unlabeled tracklets is an extremely timeconsuming and cumbersome task. Consequently, methods which can ameliorate this annotation problem and work with limited supervision, such asunsupervised learning orsemisupervised learning techniques, are of primary importance in the context of person reID.arXiv:2007.11064v1  [cs.CV]  21 Jul 20202 D. S. Raychaudhuri and A. K. RoyChowdhury Du"
594,PENCIL: Deep Learning with Noisy Labels.txt,"Deep learning has achieved excellent performance in various computer vision
tasks, but requires a lot of training examples with clean labels. It is easy to
collect a dataset with noisy labels, but such noise makes networks overfit
seriously and accuracies drop dramatically. To address this problem, we propose
an end-to-end framework called PENCIL, which can update both network parameters
and label estimations as label distributions. PENCIL is independent of the
backbone network structure and does not need an auxiliary clean dataset or
prior information about noise, thus it is more general and robust than existing
methods and is easy to apply. PENCIL can even be used repeatedly to obtain
better performance. PENCIL outperforms previous state-of-the-art methods by
large margins on both synthetic and real-world datasets with different noise
types and noise rates. And PENCIL is also effective in multi-label
classification tasks through adding a simple attention structure on backbone
networks. Experiments show that PENCIL is robust on clean datasets, too.","DEEPlearning has shown very impressive performance on various vision problems, e.g., classiﬁcation, detec tion and semantic segmentation. Although there are many factors for the success of deep learning, one of the most important is the availability of largescale datasets with clean annotations like ImageNet [1]. However, collecting a large scale dataset with clean labels is expensive and timeconsuming. On one hand, expert knowledge is necessary for some datasets such as the ﬁnegrained CUB200 [2], which demands knowledge from ornithologists. On the other hand, we can easily collect a large scale dataset with noisy annotations from various websites [3], [4], [5]. These noisy annotations can be obtained by extracting labels from the surrounding texts or using the searching keywords [6]. For a huge dataset like JFT300M (which contains 300 million images), it is impossible to manually label it and inevitably about 20% noisy labels exist in this dataset [7]. Hence, being able to deal with noisy labels is essential. The label noise problem has been studied for a long time [8], [9]. Along with the recent successes of various deep learning methods, noise handling in deep learning has gained momentum, too [6], [10], [11]. However, existing methods often have prerequisites that may not be practical in many applications, e.g., an auxiliary set with clean labels [6] or prior information about the noise [12]. Some methods are very complex [13], which hurts their deployment capability. Overﬁtting to noise is another serious difﬁculty. For a DNN with enough capacity, it can memorize the random labels [14]. Thus, some noise handling methods may ﬁnally still overﬁt and their performance decline seriously, i.e., they are not robust. Their accuracies on the clean test set reach a peak This research was partially supported by the National Natural Science Foundation of China (61772256, 61422203). K. Yi, G.H. Wang and J. Wu are with the National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Email:fyik,wangguohua,wujx g@lamda.nju.edu.cn. J. Wu is the corresponding author. Manuscript receivedin the middle of the training process, but will degrade afterwards and the accuracies after the ﬁnal training epoch are poor [12], [15]. We attack the label noise problem from two aspects. First, we model the label for an image as a distribution among all possible labels [16] instead of a ﬁxed categorical value. This probabilistic modeling lends us the ﬂexibility to handle noisecontaminated and noisefree labels in a uniﬁed manner. Second, inspired by [17], we maintain and update the label distributions in both network parameter learning (in which label distributions act as labels) and label learning (in which label distributions are updated to correct noise). Unlike [17] which updates labels simply by using the running average of network predictions, we correct noise and update our label distributions in a principled endtoend manner. The proposed framework is called PENCIL, meaning probabilistic endtoend noise correction in labels . The PENCIL framework only uses the noisy labels to initialize our label distributions, then iteratively correct the noisy labels by updating the label distributions, and the network loss function is computed using the label distributions rather than the noisy labels. Our contributions are as follows. We propose an endtoend framework PENCIL for noisy label handling. PENCIL is independent of the backbone network structure and does not need an auxiliary clean dataset or prior information about noise, thus it is easy to apply. PENCIL utilizes back propagation to probabilistically update and correct image labels in addition to updating the network parameters. To the best of our knowledge, PENCIL is the ﬁrst method in this line. We propose a variant of the DLDL method [16], which is essential for correcting noise contained in our label distributions. PENCIL achieves stateoftheart accuracy on datasets with both synthetic and real world noisy labels (e.g., CIFAR10, CIFAR100 and Clothing1M). We also propose an attention structure and extend the PENCIL framework to handle multi label tasks without orwith label noise.arXiv:2202.08436v1  [cs.CV]  17 Feb 20222 Unlike DLDL, we use inverse KLdivergence in our method. And we show that inverse KLdivergence is indeed more suitable for noise correction than the original KLdivergence. PENCIL is robust. It is not only robust in learning with noisy labels, but also robust enough to apply in datasets with zero or small amount of potential label noise (e.g., CUB200) to improve accuracy. A preliminary version of the PENCIL framework has appeared as a conference publication [18]. 2 R ELATED WORKS "
253,Estimating Instance-dependent Bayes-label Transition Matrix using a Deep Neural Network.txt,"In label-noise learning, estimating the transition matrix is a hot topic as
the matrix plays an important role in building statistically consistent
classifiers. Traditionally, the transition from clean labels to noisy labels
(i.e., clean-label transition matrix (CLTM)) has been widely exploited to learn
a clean label classifier by employing the noisy data. Motivated by that
classifiers mostly output Bayes optimal labels for prediction, in this paper,
we study to directly model the transition from Bayes optimal labels to noisy
labels (i.e., Bayes-label transition matrix (BLTM)) and learn a classifier to
predict Bayes optimal labels. Note that given only noisy data, it is ill-posed
to estimate either the CLTM or the BLTM. But favorably, Bayes optimal labels
have less uncertainty compared with the clean labels, i.e., the class
posteriors of Bayes optimal labels are one-hot vectors while those of clean
labels are not. This enables two advantages to estimate the BLTM, i.e., (a) a
set of examples with theoretically guaranteed Bayes optimal labels can be
collected out of noisy data; (b) the feasible solution space is much smaller.
By exploiting the advantages, we estimate the BLTM parametrically by employing
a deep neural network, leading to better generalization and superior
classification performance.","The study of classiﬁcation in the presence of noisy labels has been of interest for three decades (Angluin & Laird, 1988), but becomes more and more important in weakly 1University of Technology Sydney2Xidian University3Hong Kong Baptist University4Computer Science and Engineering, UC Santa Cruz5RIKEN Center for Advanced Intelligence Project 6TML Lab, Sydney AI Centre, The University of Sydney. Corre spondence to: Tongliang Liu <tongliang.liu@sydney.edu.au >. Proceedings of the 39thInternational Conference on Machine Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy right 2022 by the author(s).supervised learning (Thekumparampil et al., 2018; Li et al., 2020b; Guo et al., 2018; Xiao et al., 2015; Zhang et al., 2017a; Yang et al., 2021b;a). The main reason behind this is that datasets are becoming bigger and bigger. To improve annotation efﬁciency, these largescale datasets are often collected from crowdsourcing platforms (Yan et al., 2014), online queries (Blum et al., 2003), and image engines (Li et al., 2017), which suffer from unavoidable label noise (Yao et al., 2020a). Recent researches show that the label noise signiﬁcantly degenerates the performance of deep neural networks, since deep models easily memorize the noisy labels (Zhang et al., 2017a; Yao et al., 2020a). Generally, the algorithms for combating noisy labels can be categorized into statistically inconsistent algorithms and statistically consistent algorithms . The statistically inconsis tent algorithms are heuristic, such as selecting possible clean examples to train the classiﬁer (Han et al., 2020; Yao et al., 2020a; Yu et al., 2019; Han et al., 2018b; Malach & Shalev Shwartz, 2017; Ren et al., 2018; Jiang et al., 2018), re weighting examples to reduce the effect of noisy labels (Ren et al., 2018), correcting labels (Ma et al., 2018; Kremer et al., 2018; Tanaka et al., 2018; Wang et al., 2022), or adding regularization (Han et al., 2018a; Guo et al., 2018; Veit et al., 2017; Vahdat, 2017; Li et al., 2017; 2020b; Wu et al., 2020). These approaches empirically work well, but there is no theoretical guarantee that the learned classiﬁers can converge to the optimal ones learned from clean data. To address this limitation, algorithms in the second category aim to design classiﬁerconsistent algorithms (Yu et al., 2017; Zhang & Sabuncu, 2018; Kremer et al., 2018; Liu & Tao, 2016; Northcutt et al., 2017; Scott, 2015; Natarajan et al., 2013; Goldberger & BenReuven, 2017; Patrini et al., 2017; Thekumparampil et al., 2018; Yu et al., 2018; Liu & Guo, 2020; Xu et al., 2019; Xia et al., 2020b), where classi ﬁers learned on noisy data will asymptotically converge to the optimal classiﬁers deﬁned on the clean domain. Thelabel transition matrix T(x)plays an important role in building statistically consistent algorithms. Traditionally, the transition matrix T(x)is deﬁned to relate clean distribu tion and noisy distribution, where T(x) =P(~YjY;X = x)andXdenotes the random variable of instances/features, ~Yas the variable for the noisy label, and Yas the variablearXiv:2105.13001v3  [cs.LG]  14 Jul 2022Estimating Instancedependent Bayeslabel Transition Matrix using a Deep Neural Network .05.85.05.02.03clean class posterior.25.55.05.05.10noisy class posterior01000Bayes class posterior Figure 1. The noisy class posterior is learned from noisy data. Bayes optimal label can be inferred from the noisy class posterior if the noisy rate is controlled. Also, the Bayes optimal label is less uncertain since the Bayes class posterior is onehot vector. for the clean label. The above matrix is denoted as the cleanlabel transition matrix , which is widely used to learn aclean label classiﬁer by employing the noisy data. The learned clean label classiﬁer is expected to predict a prob ability distribution over a set of predeﬁned classes given an input, i.e. clean class posterior probability P(YjX). The clean class posterior probability is the distribution from which clean labels are sampled. However, Bayes optimal labelsY,i.e., the class labels that maximize the clean class posteriors YjX:= arg max YP(YjX), are mostly used as the predicted labels and for computing clas siﬁcation accuracy. Motivated by this, in this paper, we propose to directly model the transition matrix T(x)that relates Bayes optimal distribution andnoisy distribution , i.e.,T(x) =P(~YjY;X=x), whereYdenotes the variable for Bayes optimal label . The Bayes optimal la bel classiﬁer can be learned by exploiting the Bayeslabel transition matrix directly. Studying the transition between Bayes optimal distribution and noisy distribution is considered advantageous to that of studying the transition between clean distribution and noisy distribution. The main reason is due to that the class posteriors ofBayes optimal labels areonehot vectors while those of clean labels are not. Two advantages can be in troduced by this to better estimate the instancedependent transition matrix: (a) A set of examples with theoretically guaranteed Bayes optimal labels can be collected out of noisy data . The intrinsic reason that Bayes optimal labels can be inferred from the noisy data while clean labels can not is that the Bayes optimal labels are deterministic while clean labels are stochastic ; the Bayes optimal labels are the labels that maximize theclean class posteriors while clean labels are sampled from the clean class posteriors . In the presence of label noise, the labels that maximize thenoisy class posteriors could be identical to those that maximize theclean class posteriors (Bayes optimal labels) under mild conditions; e.g., see, Cheng et al. (2020). Therefore some instances’ Bayes optimal labels can be inferred from their noisy class posteriors while their clean labels are impossible to infer since the clean class posteriors are unobservable, as shown in Figure 1. (b) The feasible solution space of the Bayeslabel transition matrix is much smaller than that of the cleanlabel transition matrix. This is because that Bayes optimal labels have less uncertainty compared with the clean labels. The transition matrix deﬁned by Bayesoptimal labels and the noisy labels therefore has less hy pothesis complexity (Liu et al., 2017), and can be estimated more efﬁciently with the same amount of training data. These two advantages naturally motivate us to collect a set of examples and exploit their Bayes optimal labels to ap proximate the Bayeslabel transition matrix T(x). Due to the high complexity of the instancedependent matrix T(x), we simplify its estimation by parameterizing it us ing a deep neural network. The collected examples, inferred Bayes optimal labels, and their noisy labels are served as data points to optimize the deep neural network to approxi mate theT(x). Compared with the previous method (Xia et al., 2020a), which made assumptions and leveraged hand crafted priors to approximate the instancedependent transi tion matrices, we train a deep neural network to estimate the instancedependent label transition matrix with a reduced feasible solution space, which achieves lower approxima tion error, better generalization, and superior classiﬁcation performance. Before delving into details, we summarize our main contri butions as below: •In instancedependent labelnoise learning, compared with the cleanlabel transition matrix, this paper pro poses to study the transition probabilities between Bayes optimal labels and noisy labels, i.e.,Bayeslabel transition matrix , which is easier to be parametrically learned because of the certainty and accessibility of the Bayes optimal labels. •This paper proposes to leverage a deep neural network to capture the noisy patterns and generate the transi tion matrix for each input instance; it is the ﬁrstone that estimates the instancedependent label transition matrix in a parametric way. •The effectiveness of the proposed method is veriﬁed on three synthetic noisy datasets and a largescale real world noisy dataset, signiﬁcant performance improve ments on both synthetic and realworld noisy datasets and all experiment settings are achieved. 2. Related Work "
69,A Semi-Supervised Two-Stage Approach to Learning from Noisy Labels.txt,"The recent success of deep neural networks is powered in part by large-scale
well-labeled training data. However, it is a daunting task to laboriously
annotate an ImageNet-like dateset. On the contrary, it is fairly convenient,
fast, and cheap to collect training images from the Web along with their noisy
labels. This signifies the need of alternative approaches to training deep
neural networks using such noisy labels. Existing methods tackling this problem
either try to identify and correct the wrong labels or reweigh the data terms
in the loss function according to the inferred noisy rates. Both strategies
inevitably incur errors for some of the data points. In this paper, we contend
that it is actually better to ignore the labels of some of the data points than
to keep them if the labels are incorrect, especially when the noisy rate is
high. After all, the wrong labels could mislead a neural network to a bad local
optimum. We suggest a two-stage framework for the learning from noisy labels.
In the first stage, we identify a small portion of images from the noisy
training set of which the labels are correct with a high probability. The noisy
labels of the other images are ignored. In the second stage, we train a deep
neural network in a semi-supervised manner. This framework effectively takes
advantage of the whole training set and yet only a portion of its labels that
are most likely correct. Experiments on three datasets verify the effectiveness
of our approach especially when the noisy rate is high.","With the recent development of deep neural networks, we have witnessed great advancements in visual recogni tion tasks such as image classiﬁcation [43, 51, 44, 46], object detection [7, 55, 45, 13], and semantic segmenta tion [29, 5, 15, 8]. Take the famed object recognition challenge ILSVRC [43] for instance, the InceptionResNet v2 [50] achieves a remarkable top5 accuracy of 95.3% in 2017. The success of the deep neural networks is powered in part by largescale welllabeled training data. However, noisyset modelfortestingrefinednoisysetcorrect/reweightfullysupervisedlearningnoisyset modelfortestingidentifysemisupervisedlearningunlabeledsetlabeledsetOurapproachPreviousapproachesFigure 1. Comparison between our framework and the one of ex isting methods for learning from noisy labels. Instead of trying to correct or reweigh all the labels of the noisy data, we ignore the ambiguous ones and train a deep neural network in a semi supervised fashion. it is actually a very daunting task to laboriously annotate an ImageNetlike training set. On the contrary, it is fairly convenient, fast, and cheap to collect training images from the Web along with their noisy labels. This signiﬁes the need of alternative approaches to train deep neural networks using such noisy labels. Indeed, the very ﬁrst source of training data is often from the Web when we face a new visual recognition task. Therefore, the methods that effectively learn from the noisy labels can sig niﬁcantly reduce the human labeling efforts, even to zero effort in some scenarios. There has been a rich line of recent works that aim to ad dress the problem of learning from noisy labels. We catego rize them to two groups: one directly learns from the noisy labels and the other relies on an extra set of clean data. For the former, a label cleansing module is often applied in or der to identify the correctly labeled data [4, 11, 3]. Alterna tively, one may model the noise to reweigh the data terms in the loss functions [28, 37, 35]. The performance of these algorithms heavily depend on the precision of the labelarXiv:1802.02679v3  [cs.CV]  21 Mar 2018cleansing or the estimated noisy rates. They perform well when the noise rates can be safely managed [30, 47, 35, 37], but could suffer from the ambiguity between mislabeled ex amples and “hard cases” — the data points whose labels are correct but hard to be captured by the neural networks’ classiﬁcation boundary. For the second group of methods, an extra set of clean data is used to guide the learning agent through the noisy data. Li et al. [27] enforce the network trained from the noisy data to imitate the behavior of another network learned from the clean set. Vahdat [56] constructs an undi rected graphical model to represent the relationship between the clean and noisy data. Veit et al. [59] also use a secondary network to clean the labels of the noisy data such that the main network receives more accurate supervision than from the original training set. An absence of the clean set would prevent these methods from being applied to some situa tions. Besides, like the approaches in the ﬁrst group, they still aim to correct the labels of the noisy set and could make mistakes in this procedure. Despite their promising results, both groups of the exist ing methods come with a common caveat — they attempt to correct the noisy labels or reweigh the terms of all the data points no matter how difﬁcult it is to do so. This inevitably incurs errors for some of the data points. In this paper, we contend that it is better to completely ignore the labels of some of the data points than to keep their wrong labels. Af ter all, the wrong labels could mislead the training proce dure of the network. We suggest a twostage framework for the learning from noisy labels. In the ﬁrst stage, we iden tify a small portion of data points from the noisy training set of which the labels are correct with a high probability. The noisy labels of the other data points are then removed. In the second stage, we train a deep neural network in a semisupervised manner. This framework effectively takes advantage of the whole training set and yet only a portion of its labels which is most likely correct. Figure 1 contrasts our framework to the one taken by most existing methods. It is worth noting that the ﬁrst stage of our framework can be implemented in a variety of ways. Many existing methods for learning from the noisy labels are actually ap plicable. This paper presents our preliminary study by a simple and efﬁcient selfreﬁning method for the ﬁrst stage and leaves the exploration of more sophisticated approaches to the future work. In particular, we rank all the data points within each class and then keep the labels of the top few. The ranking is performed by the multiway classiﬁcation neural network learned from the original training set when there is no clean set available, and by a binary classiﬁer of each class which is trained to differentiate the data of clean and noisy labels when the clean set is given. In the second stage, we apply the temporal ensembling [23] to train a deep neural network in the semisupervised manner. To the bestof our knowledge, this is the ﬁrst time that the temporal en sembling is tested on a large set of natural images — around 1M images of which the resolutions are about 256x256. It is also worth noting that when there exists a small clean set in addition to the noisy training set, the semisupervised learning approach [25] has been considered a baseline in the experiments of [60]. In a sharp contrast to the observa tions of [38], we show that, under our twostage framework, semisupervised learning methods can actually give rise to stateoftheart results for the task of learning from noisy labels. The rest of this paper is organized as follows. Sec tion 2 discusses several related areas to our method. In Section 3, our twostage approach is described in details. Section 4 shows the experimental results of our approach in two diminutive datasets and a largescale real noisy dataset. Finally, we conclude the paper in Section 5. 2. Related work "
519,Weakly Supervised Pseudo-Label assisted Learning for ALS Point Cloud Semantic Segmentation.txt,"Competitive point cloud semantic segmentation results usually rely on a large
amount of labeled data. However, data annotation is a time-consuming and
labor-intensive task, particularly for three-dimensional point cloud data.
Thus, obtaining accurate results with limited ground truth as training data is
considerably important. As a simple and effective method, pseudo labels can use
information from unlabeled data for training neural networks. In this study, we
propose a pseudo-label-assisted point cloud segmentation method with very few
sparsely sampled labels that are normally randomly selected for each class. An
adaptive thresholding strategy was proposed to generate a pseudo-label based on
the prediction probability. Pseudo-label learning is an iterative process, and
pseudo labels were updated solely on ground-truth weak labels as the model
converged to improve the training efficiency. Experiments using the ISPRS 3D
sematic labeling benchmark dataset indicated that our proposed method achieved
an equally competitive result compared to that using a full supervision scheme
with only up to 2$\unicode{x2030}$ of labeled points from the original training
set, with an overall accuracy of 83.7% and an average F1 score of 70.2%.","The recent development of convolutional neural networks (CNNs) has been followed by the progress of point cloud se mantic segmentation tasks, and there are constantly new meth ods that achieve stateoftheart results. Most of these meth ods focus on designing new network structures or convolution kernels based on the characteristics of point cloud data (Hu et al., 2020; Thomas et al., 2019; Huang et al., 2020; Steinsiek et al., 2017), whereas few focus on data labeling. In experi ments, competitive results rely on a large amount of data an notations, which continuously require labeling operators with expert knowledge. The labeling of point cloud data is particu larly difﬁcult, and the operator usually needs to conﬁrm the cat egory of a point from multiple perspectives. Although labeling work is difﬁcult and timeconsuming, obtaining raw point cloud data has become easier. With the advancement of light detection and ranging sensor technology and the diversiﬁcation of data acquisition platforms, obtaining massive unlabeled point cloud data is no longer a problem. Thus, extracting information that helps the task of semantic segmentation from unlabeled data is an essential issue. By using a small amount of annotated data to achieve classiﬁcation results comparable to full supervision, the workload of data annotation would be considerably reduced and the efﬁciency in related applications would be improved. Semi and weakly supervised learning are commonly used to address situations in which the number of labels is scarce. There are some comprehensive reviews on these two types of supervised learning methods (Chapelle et al., 2010; Zhou, 2017; Zhu, 2005). Choosing a suitable weak label annotation strategy is key to balancing the labeling workload and experimental results. In image processing, weak labels are represented as a few labeled images (Dong and Xing, 2018), a few labeled Corresponding author. (a)  (b) Figure 1. Comparison of (a) fullylabeled points and (b) weak label. (b) Situation in which only 100 points per class ( 2h) are labeled. The size of labeled points in (b) is enlarged for better visualization. pixels (Bearman et al., 2016), and a few bounding boxes or cat egories that appear on the images (Kolesnikov and Lampert, 2016; Papandreou et al., 2015). Until now, there have been very few works that used weakly supervised methods to pro cess point cloud data. Wei et al. (2020) applied a point class activation map to classify point clouds using only cloudlevel labels. However, we believe that it is not practical for point cloud data, particularly in outdoor scenes that cover a wide region, because the complete data must be divided into many small blocks and categories contained in each block must be speciﬁed. In comparison, assigning labels to a few points is a more direct approach. Polewski et al. (2015) used an active learning method to detect standing dead trees from airborne laser scanning (ALS) point cloud data combined with infrared images. Lin et al. (2020) proposed an active and incremental learning strategy for ALS semantic segmentation, and manual annotation was iteratively added for training. Nonetheless, the setting of a weak label is used to annotate all points of tiles, andarXiv:2105.01919v1  [cs.CV]  5 May 2021manual intervention is required during training. Through the oretical analysis and experimental results, Xu and Lee (2020) found that when the number of labeled points remains constant, the spatially sparse annotations achieve better results than those samples gathering in certain object instances, and this weak label setting was adopted in this work. An illustration of the sparse weaklabel situation is shown in Fig. 1. A weakly super vised semantic point cloud segmentation framework was pro posed by Xu and Lee (2020), and an approximate result of fully supervised learning was obtained using 10 %labels. However, the characteristic of this weak label can be understood as being spatially continuous at a lower resolution, and the workload of labeling is still large. Guinard and Landrieu (2017) utilized the point cloud segmentation method to improve the classiﬁcation accuracy with very few labels, but the result largely depends on the segmentation accuracy and focuses on classifying the point cloud of the area where the initial weak label is located, which is transductive learning. Pseudolabels, assigning annotations to unlabeled data based on the predictions of the current model, can enable the use of un labeled data in parameter updates. Through the pseudolabel method, the classiﬁcation model can calculate the loss func tion and backpropagation from the dataset with more labels, thereby improving the accuracy. Iscen et al. (2019) and Lee (2013) applied pseudolabels in image classiﬁcation, and Zou et al. (2021) designed pseudolabeling and data augmentation to improve the performance of image semantic segmentation. Ber thelot et al. (2019) mixed several proven semisupervision strategies and proposed a holistic framework. Only a few stud ies have utilized pseudolabels in point cloud processing. Yao et al. (2020) introduced a pseudolabeling method into point cloud semantic segmentation. However, similar to Guinard and Landrieu (2017), the framework is also transductive learning , and the performance of the model has not been veriﬁed on un trained test data. To cooperate with pseudolabels, it is essential to propose an effective semantic segmentation network structure. Owing to the irregular distribution of point clouds, the network structure is not as uniform as that of twodimensional (2D) CNNs. Early processing methods project point clouds onto images and dir ectly use mature 2D CNNs to train the model (Boulch et al., 2018). Another branch of methods voxelizes point clouds and proposes threedimensional (3D) CNNs to process voxel data (Tchapmi et al., 2017). The disadvantage of these two meth ods is that as point clouds need to be converted into regularized data, geometric information may be lost. PointNet and Point Net++ (Qi et al., 2017a; Qi et al., 2017b) are pioneers in the development of shared multilayer perceptrons (MLPs) to dir ectly analyze point clouds. However, the spatial distribution relationship of the point cloud requires welldesigned MLPs, which complicates network structures. Compared to pointwise MLP networks, graph convolution networks construct a graph through relative spatial positions between points for feature ex traction and fusion (Wang et al., 2019). Point convolution net works are similar to graphbased methods, in which point ker nels are designed to learn local geometric information (Thomas et al., 2019). In this study, we explored how to obtain reliable semantic seg mentation results with very few labels. A pseudolabelassisted point cloud semantic segmentation framework with extremely few annotations is proposed. We used KPConv (Thomas et al., 2019), a point convolution network, as our encoder network. Our weak label was deﬁned as sparsely labeled points randomlydistributed in space. An initial model was trained using the selected weak labels. Pseudolabels were then generated by the trained model. Considering that the model obtained from weak label training is underﬁtting to the entire data space, an adaptive threshold is proposed to balance the number and accur acy of pseudolabels. The training procedure that combines the groundtruth labels, referred to weak labels, and pseudolabels was iteratively performed. To accelerate the training progress and reduce the inﬂuence of pseudo labels containing errors on the model, we updated the pseudo labels when the model con verged on the groundtruth labels. Experiments on the ALS dataset showed the effectiveness of the method. 2. METHODOLOGY "
232,Learning to segment from misaligned and partial labels.txt,"To extract information at scale, researchers increasingly apply semantic
segmentation techniques to remotely-sensed imagery. While fully-supervised
learning enables accurate pixel-wise segmentation, compiling the exhaustive
datasets required is often prohibitively expensive. As a result, many non-urban
settings lack the ground-truth needed for accurate segmentation. Existing open
source infrastructure data for these regions can be inexact and non-exhaustive.
Open source infrastructure annotations like OpenStreetMaps (OSM) are
representative of this issue: while OSM labels provide global insights to road
and building footprints, noisy and partial annotations limit the performance of
segmentation algorithms that learn from them. In this paper, we present a novel
and generalizable two-stage framework that enables improved pixel-wise image
segmentation given misaligned and missing annotations. First, we introduce the
Alignment Correction Network to rectify incorrectly registered open source
labels. Next, we demonstrate a segmentation model -- the Pointer Segmentation
Network -- that uses corrected labels to predict infrastructure footprints
despite missing annotations. We test sequential performance on the AIRS
dataset, achieving a mean intersection-over-union score of 0.79; more
importantly, model performance remains stable as we decrease the fraction of
annotations present. We demonstrate the transferability of our method to lower
quality data, by applying the Alignment Correction Network to OSM labels to
correct building footprints; we also demonstrate the accuracy of the Pointer
Segmentation Network in predicting cropland boundaries in California from
medium resolution data. Overall, our methodology is robust for multiple
applications with varied amounts of training data present, thus offering a
method to extract reliable information from noisy, partial data.","Processing remotelysensed imagery is a promising approach to evaluate ground conditions at scale for little cost. Algorithms that in take satellite imagery have accurately measured crop type [ 34],[21], cropped area [ 11], building coverage [ 41] [40], urbanization [ 1], and road networks [ 6] [42]. However, successful implementation of image segmentation algorithms for remote sensing applications depends on large amounts of data and highquality annotations. Wealthy, urbanized settings can more readily apply segmentation Figure 1: Types of label noise present in open source data. Building footprints are the class of interest. algorithms, due to either the presence of or the ability to collect significant amounts of carefully annotated data. In contrast, more rural regions often lack the means to exhaustively collect ground truth data. Some open source datasets exist for such settings, and by successfully coupling these annotations with remotely sensed imagery, researchers can gain insights into the status of infrastruc ture and development where wellcurated sources of these data do not exist. [20] [2]. Although these global open source ground truth datasets – e.g. OpenStreetMaps (OSM) – offer large amounts of labels for use at no cost, the annotations within suffer from multiple types of noise [28] [4]:missing or omitted annotations , defined as objects being present in the image and not existing in the label [ 28];misaligned annotations occur when annotations are translated and/or rotated from its true position [ 38]; and incorrect annotations – annotations that do not directly correspond to the object of interest in the image. Figure 1 presents examples of these three types of label noise. Noisy datasets present a training challenge when using tradi tional segmentation algorithms, as the model cannot learn to as sociate image features and target labels when the relationship is obscured by noise. To address the issues of misaligned and omitted annotations, and in order to extract information from imperfect data, we present a simple and generalizable method for pixelwise image segmentation. First, we address annotation misalignment by proposing an Alignment Correction Network (ACN). With a small number of images and human verified ground truth annotations, the ACN learns to correct misaligned labels. Next, the corrected open source annotations are used to train the Pointer Segmenta tion Network (PSN), a model which takes in a point location and identifies the object containing that point. Learning associations from a representative point is a widely acknowledged method of object detection: [ 5] notes that an intuitive way for humans to refer to an object is through the action of pointing. By ‘ pointingout ’ thearXiv:2005.13180v1  [cs.CV]  27 May 2020object instance of interest, our network ignores other instances that may not have corresponding annotations, therefore prevent ing performance degradation caused by annotationless instances within the image. As a result, our sequential approach presents a method for handling misaligned data as well as varying levels of label completeness without explicitly changing the loss func tion to compensate for noise. While our approach cannot replace large amounts of carefully annotated outlines, it can complement existing open source datasets and algorithms, reduce the cost of obtaining large amounts of full annotations, and allow researchers to extract information from imperfect datasets. This paper’s key contributions are as follows: •We introduce the Alignment Correction Network (ACN), a means to verify and correct misaligned annotations using a small amount of human verified ground truth labeled data. •We propose the Pointer Segmentation Network (PSN), a model that can reliably predict polygon boundaries on remotely sensed imagery despite omitted training annotations and without requiring any bespoke loss functions. •We demonstrate the applicability of our methodology to three different segmentation problems: building footprint detection with a highlyaccurate dataset, building footprint detection with noisier training data, and cropland boundary prediction. Taken as a whole, our approach enables resource constrained actors to use large amounts of misaligned and partial labels – coupled with a very small amount of human verified ground truth annotations – to train image segmentation algorithms for a variety of tasks. The rest of the paper is organized as follows: In Related Work , we discuss related literature; in Methods , we describe our novel methodological contributions; in Results , we present results for the ACN and the PSN for all segmentation tasks; and in Conclusion , we restate our most salient findings. 2 RELATED WORK "
342,A Baseline for Multi-Label Image Classification Using An Ensemble of Deep Convolutional Neural Networks.txt,"Recent studies on multi-label image classification have focused on designing
more complex architectures of deep neural networks such as the use of attention
mechanisms and region proposal networks. Although performance gains have been
reported, the backbone deep models of the proposed approaches and the
evaluation metrics employed in different works vary, making it difficult to
compare each fairly. Moreover, due to the lack of properly investigated
baselines, the advantage introduced by the proposed techniques are often
ambiguous. To address these issues, we make a thorough investigation of the
mainstream deep convolutional neural network architectures for multi-label
image classification and present a strong baseline. With the use of proper data
augmentation techniques and model ensembles, the basic deep architectures can
achieve better performance than many existing more complex ones on three
benchmark datasets, providing great insight for the future studies on
multi-label image classification.","Multilabel image classiﬁcation has been a hot topic in com puter vision community. Its extensive applications include but are not limited to image retrieval, automatic image anno tation, web image search and image tagging [1, 2, 3, 4, 5]. The abundant labelled data (e.g. ImageNet [6]) and ad vanced computational hardware have promoted the devel opment of deep convolutional neural network (CNN) based methods on singlelabel image classiﬁcation [7, 8]. Recently, such successful models have been extended to multilabel classiﬁcation tasks with promising performance reported by [9, 10, 11, 12, 13, 14, 15], proving that CNN models are ca pable of handling this challenging and more general problem. However, due to the varying backbones [15, 16] employed in the deep models, the achieved performance cannot be di rectly compared with each other. In addition, the lack of thoroughly investigated baselines of these deep CNN models hinders an explicit evaluation of the beneﬁt brought by ad vanced frameworks specially designed for multilabel image classiﬁcation.To address the aforementioned issues, we present a thor ough investigation on different baseline deep CNN models for multilabel image classiﬁcation. We focus on two state oftheart deep CNN architectures (i.e., VGG16 [17] and ResNet101[8]) as they have been widely employed in multi label image classiﬁcation [12, 15]. We evaluate the models by taking advantage of varying data augmentation techniques and model ensemble, surprisingly achieving comparable or superior performance on three benchmark datasets than the stateoftheart results achieved by more complex models. The contributions of this work are summarized as follows: We investigate the impacts of varying image sizes and data augmentation techniques including “mixup” which has not been employed in multilabel image classiﬁcation. We use score level fusion to investigate the complemen tarity of different models and point out possible direc tions for future model design. We present a strong baseline for multilabel image clas siﬁcation with performance comparable with stateof theart on three benchmark datasets. 2. RELATED WORK "
597,Generalized Negative Correlation Learning for Deep Ensembling.txt,"Ensemble algorithms offer state of the art performance in many machine
learning applications. A common explanation for their excellent performance is
due to the bias-variance decomposition of the mean squared error which shows
that the algorithm's error can be decomposed into its bias and variance. Both
quantities are often opposed to each other and ensembles offer an effective way
to manage them as they reduce the variance through a diverse set of base
learners while keeping the bias low at the same time. Even though there have
been numerous works on decomposing other loss functions, the exact mathematical
connection is rarely exploited explicitly for ensembling, but merely used as a
guiding principle. In this paper, we formulate a generalized bias-variance
decomposition for arbitrary twice differentiable loss functions and study it in
the context of Deep Learning. We use this decomposition to derive a Generalized
Negative Correlation Learning (GNCL) algorithm which offers explicit control
over the ensemble's diversity and smoothly interpolates between the two
extremes of independent training and the joint training of the ensemble. We
show how GNCL encapsulates many previous works and discuss under which
circumstances training of an ensemble of Neural Networks might fail and what
ensembling method should be favored depending on the choice of the individual
networks. We make our code publicly available under
https://github.com/sbuschjaeger/gncl","Ensemble algorithms offer state of the art performance in many Machine Learning applications and often outperform single classiﬁers by a large margin. One of the main theoret 1Artiﬁcial Intelligence Group, TU Dortmund Univer sity, Germany. Correspondence to: Sebastian Buschj ¨ager <sebastian.buschjaeger@tudortmund.de >.ical driving forces behind the understanding of ensembles is the biasvariance decomposition. The biasvariance decom position decomposes the algorithm’s error into two additive parts – its bias and its variance. Hence, a good algorithm should try to minimize both at the same time which often leads to a difﬁcult balancing act between the two quanti ties. Ensemble algorithms are wellknown to reduce the variance if a diverse set of base models is trained while also keeping the bias low making them such an effective class of algorithms. The biasvariance decomposition has been mathematically proven for the meansquared error, which sparked a plethora of different ensembling algorithms for different loss functions exploiting the general notion of ‘di versity’ in ensemble construction (Webb, 2000; Geurts et al., 2006; Brown et al., 2005; Melville & Mooney, 2005; Lee et al., 2015; Zhou & Feng, 2017; Dvornik et al., 2019). In terestingly, even though there have been numerous works on decomposing other loss functions such as 0−1loss or exponential families (e.g. loglikelihood loss), none of these theoretical insights have directly inspired new learning al gorithms. The general notion of diversity in an ensemble is still one of the main driving forces in designing new ensem bling algorithms, while the exact mathematical connection is rarely exploited explicitly. We argue, that diversity can be hurtful sometimes and must be controlled with respect to the base learners. To do so, we formulate a generalized biasvariance decomposition and study it in the context of Deep Learning. From this decompo sition, we derive two different algorithmic extremes: Either, we train the entire ensemble jointly in an endtoend fashion or we train each model completely independent from each other. We present a generalization of Negative Correlation Learning (GNCL) that smoothly interpolates between these two extremes and thus can capitalize on the entire spectrum of methods inbetween. We show, how GNCL generalizes many existing ensembling techniques in a single framework and use it to explore under which circumstances training of an ensemble might fail and what ensembling methods should be favored depending on the choice of the individual networks. Our contributions are: •A Generalized BiasVariance Decomposition: We present the ﬁrst biasvariance decomposition for arbi trary twice differentiable loss functions.arXiv:2011.02952v2  [cs.LG]  9 Dec 2020Generalized Negative Correlation Learning for Deep Ensembling •Generalized Negative Correlation Learning: From this decomposition we derive Generalized Negative Correlation Learning (GNCL) and show how it gener alizes existing NCLlike algorithms into a single frame work. •Experimental evaluation: We compare our ap proach against stateoftheart ensemble algorithms for Deep Learning methods. We show how GNCL smoothly interpolates between different ensembling techniques offering the overall best performance. Our code is available under https://github.com/ sbuschjaeger/gncl . •Explanation of results: Our theoretical results ac curately explain when certain ensembling methods should be favored over others: For small capacity Neu ral Networks, EndtoEnd learning should be favored, whereas, for larger capacity models, ensembling should shift towards independent training of the individual models. The paper is organized as follows: The next section surveys related work and focuses on the biasvariance decomposi tion as well as ensembling methods in the realm of Deep Learning. Section 3 then derives the biasvariance decompo sition, whereas section 4 formalizes it into the Generalized Negative Correlation Learning algorithm. In section 5 we experimentally evaluate our method and section 6 concludes the paper. 2. Related Work "
79,Dual GNNs: Graph Neural Network Learning with Limited Supervision.txt,"Graph Neural Networks (GNNs) require a relatively large number of labeled
nodes and a reliable/uncorrupted graph connectivity structure in order to
obtain good performance on the semi-supervised node classification task. The
performance of GNNs can degrade significantly as the number of labeled nodes
decreases or the graph connectivity structure is corrupted by adversarial
attacks or due to noises in data measurement /collection. Therefore, it is
important to develop GNN models that are able to achieve good performance when
there is limited supervision knowledge -- a few labeled nodes and noisy graph
structures. In this paper, we propose a novel Dual GNN learning framework to
address this challenge task. The proposed framework has two GNN based node
prediction modules. The primary module uses the input graph structure to induce
regular node embeddings and predictions with a regular GNN baseline, while the
auxiliary module constructs a new graph structure through fine-grained spectral
clusterings and learns new node embeddings and predictions. By integrating the
two modules in a dual GNN learning framework, we perform joint learning in an
end-to-end fashion. This general framework can be applied on many GNN baseline
models. The experimental results validate that the proposed dual GNN framework
can greatly outperform the GNN baseline methods when the labeled nodes are
scarce and the graph connectivity structure is noisy.","Graph Neural Networks (GNN) have been successfully employed to solve multiple tasks such as node classiﬁcation, graph completion, and edge prediction across a variety of application domains including computational chemistry Shi et al. [2020], proteinprotein interactions Zitnik and Leskovec [2017] and knowledgebase completion Schlichtkrull et al. [2018]. In particular, many GNN advancements have addressed the typical node classiﬁcation task in a semisupervised learning setting where only a subset of nodes in the graph are labeled, including the well known Graph Convolutional Networks (GCNs) Kipf and Welling [2017], Graph Attention Networks (GATs) Veli ˇckovi ´c et al. [2018], Topology Adaptive Graph Convolutional Networks (TAGs) Du et al. [2018] and Dynamic Neighborhood Aggregation networks (DNAs) Fey [2019]. These GNN models have achieved great results on the benchmark GNN learning datasets. However, they typically require a large number of labeled nodes as well as reliable/uncorrupted graph structures in order to obtain good performance. Their performance can degrade signiﬁcantly as the number of labeled nodes becomes scarce Li et al. [2018], Lin et al. [2020] (as shown in Figure 1a) or when the graph structures are noisy or corrupted Wang et al. [2020], Chen et al. [2020] such as many edges are deleted (as shown in Figure 1b). The performance drop of GNNs with limited labeled data can be explained by the inability of GNNs to propagate the label information from the labeled nodes to the rest of the graph. That is, while it is known that deep GNN architectures can cause oversmoothing problems Li et al. [2018], a relatively shallow GNN architecture can fail to propagate messages across the whole grapharXiv:2106.15755v1  [cs.LG]  29 Jun 2021●●●●● ●●●●● ●●●●● ●●●●● 455055606570 05101520 # Labeled Nodes / ClassAccuracy(a) Few Labeled Nodes ● ● ● ● ●● ● ● ● ●● ● ● ●●● ● ● ● ● 5055606570 0.20.40.60.81.0  % of Deleted EdgesAccuracy (b) Noisy Graph Structure Figure 1: Performance degradation of GNN models on CiteSeer: (a) Few Labeled Nodes; (b) Noisy Graph Structures. and cause the classiﬁer to overﬁt the small neighborhoods of the labeled nodes. When there are very small numbers of labeled nodes, this will induce serious overﬁtting problems and degrade the classiﬁcation performance Li et al. [2018], Sun et al. [2020], Lin et al. [2020]. In addition, GNNs learn discriminative node embeddings for effective node classiﬁcation by propagating messages across the edges of the graph. Hence noisy or corrupted graph structures can greatly impair the message passing process and degrade the ultimate node classiﬁcation performance Li et al. [2018], Wang et al. [2020]. On the other hand, the dependence of GNN models on a large number of labeled nodes and reliable graph structures can seriously limit their applications, as in many application domains it is very expensive or difﬁcult to obtain a large number of labeled examples. It is also very difﬁcult to guarantee reliable/uncorrupted graph structures given the numerous sources of noises that could damage the graph connectivity structures, ranging from adversarial attacks on the graph structures to data collection or measurement noises Wang et al. [2020], Chen et al. [2020]. Therefore, it is important to develop GNN models that can learn efﬁciently with few labeled nodes and are robust to corrupted/unreliable graph structures. In this work, we propose a novel dual GNN learning framework that is resilient to low label rates and corrupted/noisy graph structures with deleted edges. The proposed framework is made up of two node prediction modules. The ﬁrst module can be treated as a standard primary GNN model which takes the original graph data as input. It suffers from the aforementioned message propagation drawbacks when labeled nodes are scarce and graph structures are noisy. To address this problem, the second GNN module employs a ﬁnegrained spectral clustering method based on the node embedding results of the ﬁrst module to construct a new adjacency matrix and hence a new graph structure, aiming to enable effective information propagation across the graph, and facilitate the subsequent node embedding and node classiﬁcation learning. The two modules coordinate with each other within the integrated dual learning framework to perform endtoend training with a joint objective function. This general dual learning framework can be applied on many standard GNN baseline models. We conduct experiments with four baseline graph neural network learning models: GCNs Kipf and Welling [2017], GATs Veli ˇckovi ´c et al. [2018], TAGs Du et al. [2018], and DNAs Fey [2019]. The experimental results show that the proposed framework can signiﬁcantly improve the baseline models when the labeled nodes are very scarce and graph structure is noisy/corrupted across multiple benchmark datasets. 2 Related Works "
610,Should Ensemble Members Be Calibrated?.txt,"Underlying the use of statistical approaches for a wide range of applications
is the assumption that the probabilities obtained from a statistical model are
representative of the ""true"" probability that event, or outcome, will occur.
Unfortunately, for modern deep neural networks this is not the case, they are
often observed to be poorly calibrated. Additionally, these deep learning
approaches make use of large numbers of model parameters, motivating the use of
Bayesian, or ensemble approximation, approaches to handle issues with parameter
estimation. This paper explores the application of calibration schemes to deep
ensembles from both a theoretical perspective and empirically on a standard
image classification task, CIFAR-100. The underlying theoretical requirements
for calibration, and associated calibration criteria, are first described. It
is shown that well calibrated ensemble members will not necessarily yield a
well calibrated ensemble prediction, and if the ensemble prediction is well
calibrated its performance cannot exceed that of the average performance of the
calibrated ensemble members. On CIFAR-100 the impact of calibration for
ensemble prediction, and associated calibration is evaluated. Additionally the
situation where multiple different topologies are combined together is
discussed.","Deep learning approaches achieve stateoftheart performance in a wide range of applications, including image classiﬁcation. However, these networks tend to be overconﬁdent in their predictions, they often exhibit poor calibration. A system is well calibrated, if when the system makes a prediction with probability of 0.6 then 60% of the time that prediction is correct. Calibration is very important in deploying system, especially in risksensitive tasks, such as medicine (Jiang et al., 2012), autodriving (Bojarski et al., 2016), and economics (Gneiting et al., 2007). It was shown by NiculescuMizil & Caruana (2005) that shallow neural networks are well calibrated. However, Guo et al. (2017) found that more complex neural network model with deep structures do not exhibit the same behaviour. This work motivated recent research into calibration for general deep learning systems. Previous research has mainly examined calibration based on samples from the true data distributionfx(i);y(i)gN i=1p(x;!);y(i)2f!1;:::;!Kg(Zadrozny & Elkan, 2002; Vaicenavicius et al., 2019). This analysis relies on the limiting behaviour as N!+1to deﬁne a well calibrated system P(y= ^yjP(^yjx;) =p) =p() lim N!+1X i2Sp j(y(i);^y(i)) jSp jj=p (1) whereSp j=fijP(^y(i)=jjx(i);) =p;i= 1;:::;Ngand^y(i)the model prediction for x(i). (s;t) = 1 ifs=t, otherwise 0. However, Eq. (1) doesn’t explicitly reﬂect the relation between P(y= ^yjP(^yjx;) =p)and the underlying data distribution p(x;y). In this work we examine Preprint. Under review.arXiv:2101.05397v1  [cs.LG]  13 Jan 2021this explicit relationship and use it to deﬁne a range of calibration evaluation criteria, including the standard samplebased criteria. One issue with deeplearning approaches is the large number of model parameters associated with the networks. Deep ensembles (Lakshminarayanan et al., 2017) is a simple, effective, approach for handling this problem. It has been found to improve performance, as well as allowing measures of uncertainty. In recent literature there has been “contradictory” empirical observations about the relationship between the calibration of the members of the ensemble and the calibration of the ﬁnal ensemble prediction (Rahaman & Thiery, 2020; Wen et al., 2020). In this paper, we examine the underlying theory and empirical results relating to calibration with ensemble methods. We found, both theoretically and empirically, that ensembling multiple calibrated models decreases the conﬁdence of ﬁnal prediction, resulting in an illcalibrated ensemble prediction. To address this, strategies to calibrate the ﬁnal ensemble prediction, rather than individual members, are required. Additionally we empiricaly examine the situation where the ensemble is comprised of models with different topologies, and resulting complexity/performance, requiring nonuniform ensemble averaging. In this study, we focus on posthoc calibration of ensemble, based on temperature annealing. Guo et al. (2017) conducted a thorough comparison of various existing posthoc calibration methods and found that temperature scaling was a simple, fast, and often highly effective approach to calibration. However, standard temperature scaling acts globally for all regions of the input samples, i.e. all logits are scaled towards one single direction, either increasing or decreasing the distribution entropy. To address this constraint, that may hurt some legitimately conﬁdent predictions, we investigate the effect of regionspeciﬁc temperatures. Empirical results demonstrate the effectiveness of this approach, with minimal increase in the number of calibration parameters. 2 Related Work "
473,Improving group robustness under noisy labels using predictive uncertainty.txt,"The standard empirical risk minimization (ERM) can underperform on certain
minority groups (i.e., waterbirds in lands or landbirds in water) due to the
spurious correlation between the input and its label. Several studies have
improved the worst-group accuracy by focusing on the high-loss samples. The
hypothesis behind this is that such high-loss samples are
\textit{spurious-cue-free} (SCF) samples. However, these approaches can be
problematic since the high-loss samples may also be samples with noisy labels
in the real-world scenarios. To resolve this issue, we utilize the predictive
uncertainty of a model to improve the worst-group accuracy under noisy labels.
To motivate this, we theoretically show that the high-uncertainty samples are
the SCF samples in the binary classification problem. This theoretical result
implies that the predictive uncertainty is an adequate indicator to identify
SCF samples in a noisy label setting. Motivated from this, we propose a novel
ENtropy based Debiasing (END) framework that prevents models from learning the
spurious cues while being robust to the noisy labels. In the END framework, we
first train the \textit{identification model} to obtain the SCF samples from a
training set using its predictive uncertainty. Then, another model is trained
on the dataset augmented with an oversampled SCF set. The experimental results
show that our END framework outperforms other strong baselines on several
real-world benchmarks that consider both the noisy labels and the
spurious-cues.","The standard Empirical Risk Minimization (ERM) has shown a high error on speciﬁc groups of data although it achieves the low test error on the indistribution datasets. One of the reasons accounting for such degradation is the presence of spuriouscues . The spurious cue refers to the feature which is highly correlated with labels on certain training groups—thus, easy to learn—but not correlated with other groups in the test set (Nagarajan et al., 2020; Wiles et al., 2022). This spuriouscue is problematic especially occurs when the model cannot classify the minority samples although the model can correctly classify the majority of the training samples using the spurious cue. In practice, deep neural networks tend to ﬁt easytolearn simple statistical correlations like the spuriouscues (Geirhos et al., 2020). This problem arises in the realworld scenarios due to various factors such as an observation bias and environmental factors (Beery et al., 2018; Wiles et al., 2022). For in stance, an object detection model can predict an identical object differently simply because of the differences in the background(Ribeiro et al., 2016; Dixon et al., 2018; Xiao et al., 2020). In nutshell, there is a low accuracy problem caused by the spuriouscues being present in a certain group of data. In that sense, importance weighting (IW) is one of the classical techniques to resolve this problem. Recently, several deep learning methods related to IW (Sagawa et al., 2019; 2020; Liu et al., 2021; Nam et al., 2020) have shown a remarkable empirical success. The main idea of those IWrelated methods is to train a model with using data oversampled with hard (highloss) samples. The assumption behind such approaches is that the highloss samples are free from the spurious cues because these shortcut features generally reside mostly in the lowloss samples Geirhos et al. (2020). These authors contributed equally 1arXiv:2212.07026v1  [cs.LG]  14 Dec 2022Preprint For instance, JustTrainTwice (JTT) trains a model using an oversampled training set containing the error set generated by the identiﬁcation model . On the other hand, noisy labels are another factor of performance degradation in the realworld scenario. Noisy labels commonly occur in massivescale human annotation data, biology and chem istry data with inevitable observation noise (Lloyd et al., 2004; Ladbury & Arold, 2012; Zhang et al., 2016). In practice, the proportions of incorrectly labeled samples in the realworld humanannotated image datasets can be up to 40% (Wei et al., 2021). Moreover, the presence of noisy labels can lead to the failure of the highlossbased IW approaches, since a large value of the loss indicates not only that the sample may belong to a minority group but also that the label may be noisy (Ghosh et al., 2017). In practice, we observed that even a relatively small noise ratio (10%) can impair the high lossbased methods on the benchmarks with spuriouscues, such as Waterbirds and CelebA. This is because the high lossbased approaches tend to focus on the noisy samples without focusing on the minority group with spurious cues. Our observation motivates the principal goal of this paper: how can we better select only spurious cuefree (SCF) samples while excluding the noisy samples? As an answer to this question, we pro pose the predictive uncertaintybased sampling as an oversampling criterion, which outperforms the errorsetbased sampling. The predictive uncertainty has been used to discover the minority or unseen samples (Liang et al., 2017; Van Amersfoort et al., 2020). We utilize such uncertainty to detect the SCF samples. In practice, we train the identiﬁcation model via the noiserobust loss and the Bayesian neural network framework to obtain reliable uncertainty for the minority group samples. By doing so, the proposed identiﬁcation model is capable of properly identifying the SCF sample while preventing the noisy labels from being focused on. After training the identiﬁcation model, similar to JTT, the debiased model is trained with the SCF set oversampled dataset. Our novel framework, ENtropybased Debiasing (END), shows an impressive worstgroup accuracy on several benchmarks with various degrees of symmetric label noise. Furthermore, as a theoretical motivation, we demonstrate that the predictive uncertainty (entropy) is a proper indicator for iden tifying the SCF set regardless of the existence of the noisy labels in the simple binary classiﬁcation problem setting. To summarize, our key contributions are three folds: 1. We propose a novel predictive uncertaintybased oversampling method that effectively selects the SCF samples while minimizing the selection of noisy samples. 2. We rigorously prove that the predictive uncertainty is an appropriate indicator for identifying a SCF set in the presence of the noisy labels, which well supports the proposed method. 3. We propose additional model considerations for realworld applications in both classiﬁcation and regression tasks. The overall framework shows superior worstgroup accuracy compared to recent strong baselines in various benchmarks. 2 R ELATED WORKS "
567,Distilling the Knowledge of Romanian BERTs Using Multiple Teachers.txt,"Running large-scale pre-trained language models in computationally
constrained environments remains a challenging problem yet to be addressed,
while transfer learning from these models has become prevalent in Natural
Language Processing tasks. Several solutions, including knowledge distillation,
network quantization, or network pruning have been previously proposed;
however, these approaches focus mostly on the English language, thus widening
the gap when considering low-resource languages. In this work, we introduce
three light and fast versions of distilled BERT models for the Romanian
language: Distil-BERT-base-ro, Distil-RoBERT-base, and
DistilMulti-BERT-base-ro. The first two models resulted from the individual
distillation of knowledge from two base versions of Romanian BERTs available in
literature, while the last one was obtained by distilling their ensemble. To
our knowledge, this is the first attempt to create publicly available Romanian
distilled BERT models, which were thoroughly evaluated on five tasks:
part-of-speech tagging, named entity recognition, sentiment analysis, semantic
textual similarity, and dialect identification. Our experimental results argue
that the three distilled models offer performance comparable to their teachers,
while being twice as fast on a GPU and ~35% smaller. In addition, we further
test the similarity between the predictions of our students versus their
teachers by measuring their label and probability loyalty, together with
regression loyalty - a new metric introduced in this work.","Knowledge transfer from Transformerbased language models (Vaswani et al., 2017) trained on large amounts of data achieves stateoftheart results on most Natural Language Processing (NLP) tasks (Devlin et al., 2019; Liu et al., 2019; He et al., 2020). However, the best performing models usually have billions (Brown et al., 2020) or even trillions (Fedus et al., 2021) of parame ters, making them impractical in certain realworld sit uations. Moreover, both training and using these lan guage models usually comes at a high environmental cost (Strubell et al., 2019). Several attempts were made to reduce the size of mod els by distilling their knowledge (Hinton et al., 2015) accumulated during the pretraining phase (Sanh et al., 2019), after ﬁnetuning the model on a speciﬁc task (Turc et al., 2019), or both pretraining and ﬁnetuning (Jiao et al., 2020). Other methods consider shrinking the size of the models by either quantizing their weights to integer values (Shen et al., 2020), or pruning parts of the neural network (Brix et al., 2020). In addition, efﬁcient attention mechanisms were developed to over come the quadratic bottleneck in the sequence length of multihead attention (Zaheer et al., 2020; Choromanski et al., 2020). However, the vast majority of these efforts focused on developing English models, and little attention was paid on increasing the efﬁciency of pretrained mod *Work done during an interhsip at the Research Institute for Artiﬁcial Intelligence, Romanian Academy.els on other languages, with few singular cases of such compressed models like BERTino (Muffo and Bertino, 2020) for Italian, MBERTA for Arabic (Alyafeai and Ahmad, 2021), or GermDistilBERT1for German. As a response to this issue, we focus on Romanian, a lan guage on which BERT has recently attracted a surge of attention from the local community and has shown promising results in various areas like dialect identi ﬁcation (Zaharia et al., 2021; Popa and S ,tef˘anescu, 2020; Zaharia et al., 2020), document classiﬁcation (Avram et al., 2021) or satire detection in news (Ro goz et al., 2021). Thus, our work introduces three com pressed BERT versions for the Romanian language that were obtained through a distillation process: •DistilBERTbasero2was obtained by distilling the knowledge of BERTbasero (Dumitrescu et al., 2020) using its original training corpus and to kenizer; •DistilRoBERTbase3was created from RoBERTbase (Masala et al., 2020) in simi lar conditions (i.e., using both original training corpus and tokenizer); 1https://huggingface.co/ distilbertbasegermancased 2https://huggingface.co/racai/ distilbertbaseromaniancased 3https://huggingface.co/racai/ distilbertbaseromanianuncasedarXiv:2112.12650v3  [cs.CL]  13 Apr 2022•DistilMultiBERTbasero4considered the dis tillation of the knowledge from an ensemble con sisting of BERTbasero and RoBERTbase, while relying on the combined corpus and coupled with the tokenizer of the former model. Our three compressed models were further evaluated on ﬁve Romanian datasets and the results showed that they maintained most of the performance of the original models, while being approximately twice as fast when run on a GPU. In addition, we also measure the label, probability and regression loyalties between each of the three distilled models and their teachers, as quantifying the performance on speciﬁc tasks does not show how similar the predictions between a teacher and a student really are. The models, together with the distillation and evaluation scripts were opensourced to improve the reproducibility of this work5. The rest of the paper is structured as follows. The next section presents a series of solutions related to the knowledge distillation of pretrained language models. The third section outlines our approach of distilling the knowledge of Romanian BERTs, whereas the fourth section presents the evaluation setup and their perfor mance on various Romanian tasks. The ﬁfth section evaluates the prediction loyalty between each distilled version and its teacher, while the sixth section evalu ates their inference speed. The ﬁnal section concludes our work and outlines potential future work. 2. Related Work "
1,Multi-Objective Interpolation Training for Robustness to Label Noise.txt,"Deep neural networks trained with standard cross-entropy loss memorize noisy
labels, which degrades their performance. Most research to mitigate this
memorization proposes new robust classification loss functions. Conversely, we
propose a Multi-Objective Interpolation Training (MOIT) approach that jointly
exploits contrastive learning and classification to mutually help each other
and boost performance against label noise. We show that standard supervised
contrastive learning degrades in the presence of label noise and propose an
interpolation training strategy to mitigate this behavior. We further propose a
novel label noise detection method that exploits the robust feature
representations learned via contrastive learning to estimate per-sample
soft-labels whose disagreements with the original labels accurately identify
noisy samples. This detection allows treating noisy samples as unlabeled and
training a classifier in a semi-supervised manner to prevent noise memorization
and improve representation learning. We further propose MOIT+, a refinement of
MOIT by fine-tuning on detected clean samples. Hyperparameter and ablation
studies verify the key components of our method. Experiments on synthetic and
real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves
state-of-the-art results. Code is available at https://git.io/JI40X.","Building a new dataset usually involves manually la beling every sample for the particular task at hand. This process is cumbersome and limits the creation of large datasets, which are usually necessary for training deep neu ral networks (DNNs) in order to achieve the required per formance. Conversely, automatic data annotation based on web search and user tags [ 29,22] leverages the use of larger data collections at the expense of introducing some incorrect labels. This label noise degrades DNN perfor mance [ 3,52] and this poses an interesting challenge that has recently gained a lot of interest in the research commu nity [45, 41, 23, 50, 12, 1, 28, 55, 13, 31].In image classiﬁcation problems, label noise usually in volves different noise distributions [ 22,55]. Indistribution noise types consist of samples with incorrect labels, but whose image content belongs to the dataset classes. When indistribution noise is synthetically introduced, it usually follows either an asymmetric or symmetric random distribu tion. The former involves label ﬂips to classes with some semantic meaning, e.g., a cat is ﬂipped to a tiger, while the latter does not. Furthermore, web label noise types are usually dominated by outofdistribution samples where the image content does not belong to the dataset classes. Recent studies show that all label noise types impact DNN perfor mance, although performance degrades less with web noise [22, 34]. Robustness to label noise is usually pursued by identify ing noisy samples to: reduce their contribution in the loss [23,11], correct their label [ 1,28], or abstain their classiﬁca tion [ 42]. Other methods exploit interpolation training [ 53], regularizing label noise information in DNN weights [ 13], or small sets of correctly labeled data [ 18,55]. However, most previous methods rely exclusively on classiﬁcation losses and little effort has being directed towards incorporating sim ilarity learning frameworks [ 32], i.e. directly learning image representations rather than a class mapping [45]. Similarity learning frameworks are very popular in com puter vision for a variety of applications including face recog nition [ 44], ﬁnegrained retrieval [ 37], or visual search [ 35]. These methods learn representations for samples of the same class (positive samples) that lie closer in the feature space than those of samples from different classes (negative sam ples). Many traditional methods are based on sampling pairs or triplets to measure similarities [ 7,19]. However, super vised and unsupervised contrastive learning approaches that consider a high number of negatives have recently received signiﬁcant attention due to their success in unsupervised learning [ 5,14,27]. In the context of label noise, there are some attempts at training with simple similarity learning losses [ 45], but there are, to the best of our knowledge, no works exploring more recent contrastive learning losses [ 24]. This paper proposes MultiObjective Interpolation Train ing (MOIT), a framework to robustly learn in the presencearXiv:2012.04462v2  [cs.CV]  18 Mar 2021of label noise by jointly exploiting synergies between con trastive and semisupervised learning. The former intro duces a regularization of the contrastive loss in [ 24] to learn noiserobust representations that are key for accurately de tecting noisy samples and, ultimately, for semisupervised learning. The latter performs robust image classiﬁcation and boosts performance. Our MOIT+ reﬁnement further demonstrates that ﬁnetuning on the detected clean data can boost performance. MOIT/MOIT+ achieves stateofthe art results across a variety of datasets (CIFAR10/100 [ 26], miniImageNet [ 22], and miniWebVision [ 29]) with both synthetic and realworld web label noise. Our main contri butions are as follows: 1.A multiobjective interpolation training (MOIT) frame work where supervised contrastive learning and semi supervised learning help each other to robustly learn in the presence of both synthetic and web label noise under a single hyperparameter conﬁguration. 2.An interpolated contrastive learning (ICL) loss that imposes linear relations both on the input and the con trastive loss to mitigate the performance degradation observed for the supervised contrastive learning loss in [24] when training with label noise. 3.A novel label noise detection strategy that exploits the noiserobust feature representations provided by ICL to enable semisupervised learning. This detection strat egy performs a knearest neighbor search to infer per sample label distributions whose agreements with the original labels identify correctly labeled samples. 4.A ﬁnetuning strategy over detected clean data (MOIT+) that further boosts performance based on sim ple noise robust losses from the literature. 2. Related work "
187,SpectraNet: Learned Recognition of Artificial Satellites From High Contrast Spectroscopic Imagery.txt,"Effective space traffic management requires positive identification of
artificial satellites. Current methods for extracting object identification
from observed data require spatially resolved imagery which limits
identification to objects in low earth orbits. Most artificial satellites,
however, operate in geostationary orbits at distances which prohibit ground
based observatories from resolving spatial information. This paper demonstrates
an object identification solution leveraging modified residual convolutional
neural networks to map distance-invariant spectroscopic data to object
identity. We report classification accuracies exceeding 80% for a simulated
64-class satellite problem--even in the case of satellites undergoing constant,
random re-orientation. An astronomical observing campaign driven by these
results returned accuracies of 72% for a nine-class problem with an average of
100 examples per class, performing as expected from simulation. We demonstrate
the application of variational Bayesian inference by dropout, stochastic weight
averaging (SWA), and SWA-focused deep ensembling to measure classification
uncertainties--critical components in space traffic management where routine
decisions risk expensive space assets and carry geopolitical consequences.","The ideal data type for identifying resident space ob jects (RSOs; artificial satellites) is resolved imagery. An alysts can easily interpret the information content of im ages and recent work has demonstrated that deep neural networks provide efficiency enhancements to signal extrac tion [15, 16, 22]. In addition, data collection is passive – RSOs always reflect incident sunlight, and ground based telescopes collect without interfering with the target. Unfortunately, smaller and more distant RSOs require *jonathan.gazak.1.ctr@us.af.mil †justin.fletcher.14.ctr@us.af.mil Figure 1: Pristine renders of three satellites, Hubble, DI RECTV , and Almaz [21]. Satellite spectral energy distri butions are complex and vary strongly with orientation and illumination angle. increasingly large telescopes to resolve, placing the major ity of RSOs beyond the limits of current imaging technol ogy. For positive identification of objects beyond low earth orbit (LEO; altitude <1000 km), new approaches are as necessary as they are elusive. One promising technique, spectroscopy, is both passive and distanceinvariant, having been used to study the most distant objects in the universe for well over 50 years [14]. In the field of space domain awareness, spectroscopy has lagged behind its potential for three reasons. First, the in accuracy of material reflection models (bidirectional re flection functions, or BRDFs) does not allow attribution of spectroscopic features to satellite materials and geometries. Second, spectroscopic data is not easy to interpret (see Fig ure 2)−out of reach of human analysts even after extensive expert data reduction. Third, underlying truth data on mate rial and geometry are difficult to obtain as spectra are highly variable based on orientation (Figure 1). These hurdles pre clude transition of spectroscopic solutions beyond labora tory settings. In this work we demonstrate learned spectroscopic pos itive identification by modeling a high yield, low cost sen sor and training convolutional neural networks (CNNs) on simulated output. By eliminating reliance on both physics based priors and exquisite data reduction, our technique (SpectraNet) identifies RSOs with accuracy exceeding 80% under the most difficult condition of random axis orienta 4012arXiv:2201.03614v1  [cs.LG]  10 Jan 2022Figure 2: Top panel : A simulated raw FPA observation of 18 Scorpii, a star which is a close analog of our Sun [3], such that this spectrum and FPA frame are typical of resident space objects reflecting solar radiation. These raw frames are used to train models in this paper. Bottom : The 1D reduced spectrum of 18 Scorpii after a raw FPA frame is fully calibrated [2]. tion and for large numbers of satellite classes. We verify simulated results by collecting on sky spec tral observations of RSOs in geostationary (GEO; altitude >35000 km) orbits. SpectraNet learns to classify objects in our on sky dataset with accuracy of ∼72%, in agreement with simulated results given the limited ( ∼100 examples per class) dataset. In this paper we contribute: • A novel method capable of identifying spatially un resolved artificial satellites −a critical technology for space domain awareness −by allowing deep Bayesian residual networks to learn spectroscopic features from raw scientific imagery. These models can produce well calibrated softmax probabilities, enabling practical ap plications of SpectraNet at low sample counts. • Both real and synthetic datasets representing the spec troscopic satellite identification application domain and baseline analysis of classifier performance on those datasets as a function of observation count (dataset size).1 • A framework for designing spectroscopic positive identification systems by experimenting across number of classes, observation quality (signal to noise), and number of examples needed to achieve suitable classi fication performance. In §2 we discuss topics related to this work, followed by our specific problem formulation in §3. We describe the datasets used for training in §4, and the experiments con ducted in §5, before summarizing results in §6. 2. Related Works "
396,Enhancing Diversity in Teacher-Student Networks via Asymmetric branches for Unsupervised Person Re-identification.txt,"The objective of unsupervised person re-identification (Re-ID) is to learn
discriminative features without labor-intensive identity annotations.
State-of-the-art unsupervised Re-ID methods assign pseudo labels to unlabeled
images in the target domain and learn from these noisy pseudo labels. Recently
introduced Mean Teacher Model is a promising way to mitigate the label noise.
However, during the training, self-ensembled teacher-student networks quickly
converge to a consensus which leads to a local minimum. We explore the
possibility of using an asymmetric structure inside neural network to address
this problem. First, asymmetric branches are proposed to extract features in
different manners, which enhances the feature diversity in appearance
signatures. Then, our proposed cross-branch supervision allows one branch to
get supervision from the other branch, which transfers distinct knowledge and
enhances the weight diversity between teacher and student networks. Extensive
experiments show that our proposed method can significantly surpass the
performance of previous work on both unsupervised domain adaptation and fully
unsupervised Re-ID tasks.","Person reidentiﬁcation (ReID) targets at retrieving a person of interest across nonoverlapping cameras. Since there are domain gaps resulting from illumination condi tion, camera property and viewpoint variation, a ReID model trained on a source domain usually shows a huge per formance drop on other domains. Unsupervised domain adaptation (UDA) targets at shift ing the model trained from a source domain with identity annotation to a target domain via learning from unlabeled target images. In the real world, unlabeled images in a target domain can be easily recorded, which is almost laborfree. It is intuitive to use these images to adapt a pretrained Re ID model to the desired domain. Fully unsupervised ReID 1Code at https://github.com/chenhao2345/ABMT .further minimises the supervision by removing pretraining on the labelled source domain. Stateoftheart UDA Person ReID methods [8, 27] and unsupervised methods [17] assign pseudo labels to unla beled target images. The generated pseudo labels are gener ally very noisy. The noise is mainly from several inevitable factors, such as the strong domain gaps and the imperfection of clustering. In this way, an unsupervised ReID problem is naturally transferred into Generating pseudo labels and Learning from noisy labels problems. To generate pseudo labels, the most intuitive way is to use a clustering algorithm, which gives a good starting point for clustering based UDA ReID [29, 6]. Recently, Ge et al. [8] propose to add a Mean Teacher [23] model as online soft pseudo label generator, which effectively reduces the error ampliﬁcation during the training with noisy labels. In this paper, we also use both clusteringbased hard labels and teacherbased soft labels in our baseline. To handle noisy labels, one of the most popular ap proaches is to train paired networks so that each network helps to correct its peer, e.g., twostudent networks in Co teaching [9] and twoteachertwostudent networks in MMT [8]. However, these paired models with identical struc ture are prone to converge to each other and get stuck in a local minimum. There are several attempts to alleviate this problem, such as Coteaching+ [28], ACT [27] and MMT [8]. These attempts of keeping divergence between paired models are mainly based on either different train ing sample selection [28, 27] or different initialization and data augmentation[8]. In this paper, we propose a strong alternative by designing asymmetric neural network struc ture in the Mean Teacher Model. We use two independent branches with different depth and global pooling methods as last layers of a neural network. Features extracted from both branches are concatenated as the appearance signa ture, which enhances the feature diversity in the appearance signature and allows to get better clusteringbased hard la bels. Each branch gets supervision from its peer branch of different structure, which enhances the divergence between paired teacherstudent networks. Our proposed decoupling method does not rely on different source domain initializaarXiv:2011.13776v1  [cs.CV]  27 Nov 2020tions, which makes it more effective in the fully unsuper vised scenario where the source domain is not available. In summary, our contributions are: 1. We propose to enhance the feature diversity inside person ReID appearance signatures by splitting last layers of a backbone network into two asymmetric branches, which increases the quality of clustering based hard labels. 2. We propose a novel decoupling method where asym metric branches get crossbranch supervision, which avoids weights in paired teacherstudent networks con verging to each other and increases the quality of teacherbased soft labels. 3. Extensive experiments and ablation study are con ducted to validate the effectiveness of each proposed component and the whole framework. 2. Related Work "
168,An Instance Transfer based Approach Using Enhanced Recurrent Neural Network for Domain Named Entity Recognition.txt,"Recently, neural networks have shown promising results for named entity
recognition (NER), which needs a number of labeled data to for model training.
When meeting a new domain (target domain) for NER, there is no or a few labeled
data, which makes domain NER much more difficult. As NER has been researched
for a long time, some similar domain already has well labelled data (source
domain). Therefore, in this paper, we focus on domain NER by studying how to
utilize the labelled data from such similar source domain for the new target
domain. We design a kernel function based instance transfer strategy by getting
similar labelled sentences from a source domain. Moreover, we propose an
enhanced recurrent neural network (ERNN) by adding an additional layer that
combines the source domain labelled data into traditional RNN structure.
Comprehensive experiments are conducted on two datasets. The comparison results
among HMM, CRF and RNN show that RNN performs bette than others. When there is
no labelled data in domain target, compared to directly using the source domain
labelled data without selecting transferred instances, our enhanced RNN
approach gets improvement from 0.8052 to 0.9328 in terms of F1 measure.","In recent years, Web data and knowledge man agement attracts the interests from industry and research ﬁelds. There are various promising appli cations, such as intelligent recommendation, ma chine Question & Answer, knowledge graph and so on. Named entity recognition is a fundamental and very important step in the automatic informa tion extraction. A recognition method with high quality can directly improve the followup pro cessing results of Web data management products.NER research shows great successes in various domains and becomes a hot topic ( Sun et al. , 2016 ;Eiselt and Figueroa ,2013 ), such as so cial media ( Vavliakis et al. ,2013 ;Yao and Sun , 2016 ), language texts ( Karaa and Slimani ,2017 ), biomedicine ( Song et al. ,2016 ;Amith et al. , 2017 ), and so on. As we know, texts of different domains may vary from features, writing styles and structures. Domain NER meets a challenge that annotating data for new domains is labor intensive. For domain NER, a new domain (target domain) has no or a few labelled data. However, it is nat ural to think that if some similar domain (source domain) with enough lablled data already exists, it is possible to borrow some from this similar do main. Domain adaptation targets at transferring the source domain knowledge to the target do main ( Liu et al. ,2016 )) , which is a effective way to solve the problem of labelling large amount of data on new corpus or domains. In other words, if a NER modle is trained well for some ﬁxed source domain, it is interesting to study how to deploy them across one or more different target domains. In this paper, we focus on domain NER for the politics text domain in Chinese high schools to support an automatic question and answer system (Q&A) that will take the national college entrance examination (NCEE) in the future. However there is no public politics text corpus used by Chinese high school for NER task. Moreover, there is no labeled data. We notice that People’s Daily corpus is free for public download and similar to politics text. Therefore, we propose an instance transfer based approach for domain NER with enhanced Recurrent Neural Network (RNN). First, we de sign an instance transfer strategy to extract similar sentences from the People’s Daily corpus (source domain). Here, instances mean labelled data and politics text is our target domain. Then, recurrent neural network (RNN) model can be trained based on the transferred instances. Moreover, we improve the traditional RNN model by enhancing its activation function and structure. Finally, an instance transfer enhanced RNN (ERNN) model is proposed to do NER for politics text target do main, which is trained based on the transferred instances from similar source domain (People’s daily corpus). Compared with the traditional RNN model, experimental results show that our ERNN with the instance transfer strategy can get im provement from 80.52% to 93.28% in terms of F1 measure. In addition, we consider other situation where a small number of labelled data is available to fur ther investigate the performance of our proposed approach. Since the politics text as target domain data is needed to obtain, we collect texts from high school books and relevant websites and manually label a very small set of them, making labeled data much less than the unlabeled one. Experimental results show that labeling data for target domain is quite useful, reaching at 92.13 in terms of F1 measure. However, our instance transfer based en hanced RNN can further improve the F1 value to 93.81. Finally, we adopt the cotraining approach using our proposed ERNN model and CRF by tak ing advantage of large unannotated target domain data, which get the F1 value of 94.02. The rest of the paper is organized as follows. In Section 2, we introduce related work on NER (especially on domain NER), recurrent neural net work and transfer learning. Section 3elaborates our approach including the instance transfer strat egy, our proposed ERNN. Experiments and results are given in Section 4, then we make the conclu sion and future work in Section i 5. 2 Related Work "
279,Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels.txt,"Performing controlled experiments on noisy data is essential in understanding
deep learning across noise levels. Due to the lack of suitable datasets,
previous research has only examined deep learning on controlled synthetic label
noise, and real-world label noise has never been studied in a controlled
setting. This paper makes three contributions. First, we establish the first
benchmark of controlled real-world label noise from the web. This new benchmark
enables us to study the web label noise in a controlled setting for the first
time. The second contribution is a simple but effective method to overcome both
synthetic and real noisy labels. We show that our method achieves the best
result on our dataset as well as on two public benchmarks (CIFAR and
WebVision). Third, we conduct the largest study by far into understanding deep
neural networks trained on noisy labels across different noise levels, noise
types, network architectures, and training settings. The data and code are
released at the following link: http://www.lujiang.info/cnlw.html","Performing experiments on controlled noise is essential in understanding Deep Neural Networks (DNNs) trained on noisy labeled data. Previous work performs controlled experiments by injecting a series of synthetic label noises into a wellannotated dataset such that the dataset’s noise level can vary, in a controlled manner, to reﬂect different magnitudes of label corruption in real applications. Through studying controlled synthetic label noise, researchers have discovered theories and methodologies that have greatly fostered the development of this ﬁeld. 1Google Research, Mountain View, United States2Google Cloud AI, Sunnyvale, United States3Cornell University, Ithaca, United States. Correspondence to: Lu Jiang <lu jiang@google.com >. Proceedings of the 37thInternational Conference on Machine Learning , Online, PMLR 119, 2020. Copyright 2020 by the au thor(s).However, due to the lack of suitable datasets, previous work has only examined DNNs on controlled synthetic label noise, and realworld label noise has never been studied in a con trolled setting. This leads to two major issues. First, as synthetic noise is generated from an artiﬁcial distribution, a tiny change in the distribution may lead to inconsistent or even contradictory ﬁndings. For example, contrary to the common understanding that DNNs trained on synthetic noisy labels generalize poorly (Zhang et al., 2017), Rolnick et al. (2017) showed that DNNs can be robust to massive label noise when the noise distribution is made slightly different. Due to the lack of datasets, these ﬁndings, unfor tunately, have not yet been veriﬁed beyond synthetic noise in a controlled setting. Second, the vast majority of previous studies prefer to verify robust learning methods on a spec trum of noise levels because the goal of these methods is to overcome a wide range of noise levels. However, current evaluations are limited because they are conducted only on synthetic label noise. Although there do exist datasets of real label noise e.g.WebVision (Li et al., 2017a), Clothing 1M (Xiao et al., 2015), etc, they are not suitable for con trolled evaluation in which a method must be systematically veriﬁed on multiple different noise levels, because the train ing images in these datasets are not manually labeled and hence their data noise level is ﬁxed and unknown. In this paper, we study a realistic type of label noise in a con trolled setting called web labels. “Weblylabeled” images are commonly used in the literature (Bootkrajang & Kab ´an, 2012; Li et al., 2017a; Krause et al., 2016; Chen & Gupta, 2015), in which both images and labels are crawled from the web and the noisy labels are automatically determined by matching the images’ surrounding text to a class name during web crawling or equivalently by querying the search index afterward. Unlike synthetic labels, web labels follow a realistic label noise distribution but have not been studied in a controlled setting. We make three contributions in this paper. First, we establish the ﬁrst benchmark of controlled web label noise, where each training example is carefully annotated to indicate whether the label is correct or not. Speciﬁcally, we auto matically collect images by querying Google Image Search using a set of class names, have each image annotated by 35 workers, and create training sets of ten controlled noise levels. As the primary goal of our annotation is to identifyarXiv:1911.09781v3  [cs.LG]  27 Aug 2020Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels images with incorrect labels, to obtain a sufﬁcient number of these images we have to collect a total of about 800,000 annotations over 212,588 images. The new benchmark en ables us to go beyond synthetic label noise and study web label noise in a controlled setting. For convenience, we will refer it as web label noise (or red noise ) to distinguish it from synthetic label noise (or blue noise )1. Second, this paper introduces a simple yet highly effective method to overcome both synthetic and realworld noisy labels. It is based on a new idea of minimizing the empirical vicinal risk using curriculum learning. We show that it con sistently outperforms baseline methods on our datasets and achieves stateoftheart performance on two public bench marks of synthetic and realworld noisy labels. Notably, on the challenging benchmark WebVision 1.0 (Li et al., 2017a) that consists of 2.2 million images of realworld noisy labels, it yields a signiﬁcant improvement of 3%in the top1 accu racy, achieving the bestpublished result under the standard training setting. Finally, we conduct the largest study by far into understand ing DNNs trained on noisy labels across a variety of noise types (blue and red), noise levels, training settings, and net work architectures. Our study conﬁrms the existing ﬁndings of Zhang et al. (2017) and Arpit et al. (2017) on synthetic labels, and brings forward new ﬁndings that may challenge our preconceptions about DNNs trained on noisy labels. See the ﬁndings in Section 5.2. It is worth noting that these ﬁnd ings along with benchmark results are a result of conducting thousands of experiments using tremendous computation power (hundreds of thousands of V100 GPU hours). We hope our (i) benchmark, (ii) new method, and (iii) ﬁndings will facilitate future deep learning research on noisy labeled data. We will release our data and code. 2. Related Work "
562,Label-free timing analysis of modularized nuclear detectors with physics-constrained deep learning.txt,"Pulse timing is an important topic in nuclear instrumentation, with
far-reaching applications from high energy physics to radiation imaging. While
high-speed analog-to-digital converters become more and more developed and
accessible, their potential uses and merits in nuclear detector signal
processing are still uncertain, partially due to associated timing algorithms
which are not fully understood and utilized. In this paper, we propose a novel
method based on deep learning for timing analysis of modularized nuclear
detectors without explicit needs of labelling event data. By taking advantage
of the inner time correlation of individual detectors, a label-free loss
function with a specially designed regularizer is formed to supervise the
training of neural networks towards a meaningful and accurate mapping function.
We mathematically demonstrate the existence of the optimal function desired by
the method, and give a systematic algorithm for training and calibration of the
model. The proposed method is validated on two experimental datasets. In the
toy experiment, the neural network model achieves the single-channel time
resolution of 8.8 ps and exhibits robustness against concept drift in the
dataset. In the electromagnetic calorimeter experiment, several neural network
models (FC, CNN and LSTM) are tested to show their conformance to the
underlying physical constraint and to judge their performance against
traditional methods. In total, the proposed method works well in either ideal
or noisy experimental condition and recovers the time information from waveform
samples successfully and precisely.","Pulse timing is an important research topic in nuclear detector signal processing, with applications ranging from high energy physics to radiation imaging. Accurate time measurements are meaningful for precisely determining the vertex of interactions as well as the dynamics of incident particles. In the past decade, highspeed analogto digital converters (ADC) were designed for frontend electronics of nuclear detectors [1] and incorporated into their electronic data ow. Traditionally, some xed algorithms can be used to extract time information from a time series of pulse samples, such as leading edge discrimination or constant fraction discrimination [2]. However, when they work in noisy or changing conditions, the performance of these xed algorithms drops signicantly [3]. On the other hand, machine learning techniques, especially neural networks (NN) and deep learning, open another door for possible solution of time extraction from waveform samples. Recent literature has demonstrated that NNs can approximate the Cram er Rao lower bound in a broad range of working conditions [3]. It is estimated that NNs, the key components of intelligent frontend processing, will be widely used in future nuclear detector systems [4, 5, 6, 7], empowered by the evergrowing development of hardware acceleration for NN inference [8, 9]. However, one prerequisite for NNs to achieve superior performance is a justied reference (groundtruth label) in training. While labelled data are easily available in computer vision and many other machine learning tasks, they are not so for nuclear detector experiments. To provide accurate time references for realworld nuclear detectors, additional timing equipments or specic experimental schemes are needed, which will reversely shrink the signicance of using machine learningbased methods. Therefore, it is worthwhile to exploit the builtin structure of nuclear detectors for potential timing correlations and make use of them in the training process. The idea of training NNs without explicit labels was originally invented to locate or detect particular objects in images in the domain of computer vision [10]. Researches in multiple disciplines combined the idea into the formulation of loss functions to include a physicsconstrained term for better accuracy and consistency with the underlying physical laws [11, 12, 13, 14, 15, 16]. For example, in [11] physical constraints were converted into the matrix multiplication form and worked as soft or hard constraints to assist the training process with the standard loss function; in [12] two domainspecic loss terms were used in conjunction with the original loss of NNs to improve prediction performance. Some recent studies went even further to eliminate the canonical loss term and totally rely on physical relations. For example, in [17] the authors proposed an unsupervised learning scheme only requiring the loss from optical laws and performed a physicsbased pretraining with the loss function; in [18] a weaklysupervised learning framework was devised without the need of groundtruth labels for indoor position estimation. Finally, the same principle was extended to solve partial dierential equations which worked as mathematical constraints together with boundary conditions,Labelfree timing for modularized nuclear detectors 3 while NNs were used to represent the solution of equations regarding input variables [19, 20, 21]. In this work, we focus on the aspect of timing analysis and propose a novel method building on the intrinsic modularization of common nuclear detectors. The motivation of the work is to take full advantage of the universal approximation abilities of NNs while oering an available way to avoid using labelled data and to ease optimization of the model. Compared to some former works which also used nuclear detector signals to estimate timeof ight (such as [22, 23]), the major innovations and contributions of the paper include: We propose a practical methodology to use NNs for pulse timing within the conventional nuclear detector data ow without explicit needs of labelled data. A loss function from physical constraints in combination with a regularizer automatically guides the NN model to nd the optimal solution. We invent an algorithmic framework to make the single NN generate the desired time estimates based on an arbitrary time origin by posttraining calibration . We conduct experiments with practical modularized nuclear detectors, validate our method on two experimental datasets, and show its feasibility and accuracy when applying to nuclear detector signals. 2. Methodology "
