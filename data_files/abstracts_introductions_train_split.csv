Unnamed: 0,titles,abstract,introduction
380,Boosting Active Learning for Speech Recognition with Noisy Pseudo-labeled Samples.txt,"The cost of annotating transcriptions for large speech corpora becomes a
bottleneck to maximally enjoy the potential capacity of deep neural
network-based automatic speech recognition models. In this paper, we present a
new training pipeline boosting the conventional active learning approach
targeting label-efficient learning to resolve the mentioned problem. Existing
active learning methods only focus on selecting a set of informative samples
under a labeling budget. One step further, we suggest that the training
efficiency can be further improved by utilizing the unlabeled samples,
exceeding the labeling budget, by introducing sophisticatedly configured
unsupervised loss complementing supervised loss effectively. We propose new
unsupervised loss based on consistency regularization, and we configure
appropriate augmentation techniques for utterances to adopt consistency
regularization in the automatic speech recognition task. From the qualitative
and quantitative experiments on the real-world dataset and under real-usage
scenarios, we show that the proposed training pipeline can boost the efficacy
of active learning approaches, thus successfully reducing a sustainable amount
of human labeling cost.","EndtoEnd Automatic Speech Recognition (E2EASR) mod els [1, 2, 3, 4] have achieved impressive improvements in Large V ocabulary Automatic Speech Recognition (LV ASR). However, although they achieve stateoftheart performance [5, 6], the methods require more number of samples, decreasing the economical efÔ¨Åciency considering the high labeling cost of the speech data. The cost to annotate labels might be more troublesome in ASR because the cost to transcribe utterances *Authors contributed equally to this research. The authors are sorted by alphabetical order. This work is done while Heesu Kim did internship at Clova AI Research, NA VER Corp.is more expensive due to its errorprone property compared to simple classiÔ¨Åcation problems such as object class for image classiÔ¨Åcation. The reason E2EASR models require enormous data stems from the fact that they are trained in endtoend without strong inductive bias such as explicit acoustic and language models while having lots of model parameters [5]. Therefore, maximizing the training efÔ¨Åciency in labeling cost is necessary for the stateoftheart E2EASR model. Active Learning (AL) approach, has been studied to re duce the labeling cost by selecting samples most effective for a model training from many unlabeled candidates.The selected samples are annotated by human experts, so Human Labeled Samples (HLS) become the important anchors in training the model. However, the number of HLS is restricted due to the labeling budget, so we usually cannot get sufÔ¨Åcient amount of the labeled data for model capacity. Furthermore, even the deÔ¨Ånitions of effectiveness are different among AL studies, the consensus is that the effective samples for training are in most case unfamiliar and uncertain ones to the current model. Therefore, even HLS complements the model to han dle unfamiliar samples, it cannot fully exploit the potential of E2EASR models because of constrained labeling budget and bais existing in the selected HLS. To mitigate such problems in AL without additional label ing cost, we propose to utilize the unlabeled samples which are not selected for HLS. Inspired by SemiSupervised Learn ing (SSL) , we use the unlabeled samples, relatively familiar and conÔ¨Ådent in view of the current model contrary to HLS, by generating their pseudolabels ( PseudoLabeled Samples (PLS) ) and appending the samples to the training dataset. Unfortunately, simply introducing PLS would not lead to the improvement of model performance mainly because of the two reasons. One is that if PLS are selected conserva tively, PLS are too familiar to model, so they do not incur any effective variation in model parameters after training. the other is that if PLS are selected speculatively, they are likely to have noisy pseudolabels, consequently hurting model per formance. Therefore, in this paper, we propose a training pipeline toarXiv:2006.11021v2  [eess.AS]  5 Nov 2020‚Ä¶certainAL PLSHLSSSLFig. 1 : An overview of the training pipeline proposed in this paper. overcome the limitations of both AL and SSL.To this end, we introduce Consistency Regularization (CR) [7, 8, 9] technique which regularizes the sideeffect of noisy pseudolabels by forcing a model to predict consistently on both of genuine and distorted versions of a sample. The experimental results suggest that our new training pipeline can fully utilize PLS in training. The overview of the training pipeline proposed in this paper is depicted in Figure 1. CR is mostly applied in computer vision tasks [7, 8, 9] and CR has not been actively considered in ASR task because of its incompatibility to distortions (i.e., augmentations), which has been reported to be effective for vision tasks. However, we show that appropriate augmentations, which do not corrupt essential linguistic information, can enable CR on utterances for ASR. By introducing loss for CR to train objective with the carefully conÔ¨Ågured augmentations, our training pipeline restores the degradation of performance caused by restricted labeling budget in AL without any additional labeling cost. Consequently, it achieves a signiÔ¨Åcant reduction of the label ing cost, as well as minimizing the performance degradation. To validate the proposed training pipeline, we conduct ex tensive experiments on the realworld samples acquired from services deployed to endusers, which provides voice search and voice control for IoTs. The amount of collected samples is about 500 hours of utterances recorded from various de vices and users. Comparing with conventional AL on such a dataset, our proposed training pipeline boosts the perfor mance of the model by 12.76% and 4.02% in Characterlevel Error Rate (CER) when the labeling budget is 1/3 and 1/10 of unlabeled samples, respectively. In summary, our contributions to achieving such an objec tive can be summarized as threefold: 1) this work adopts con sistency regularization on samples with noisy pseudolabels in E2EASR model training to boost the effect of active learn ing for labelefÔ¨Åciency. 2) we conÔ¨Ågure the feasible augmen tations for utterances to adapt consistency regularization for ASR, and 3) we verify and analyze the efÔ¨Åcacy of the pro posed training pipeline including consistency regularization with extensive realworld data and realistic environments.2. RELATED WORKS "
397,Semi-supervised learning of deep metrics for stereo reconstruction.txt,"Deep-learning metrics have recently demonstrated extremely good performance
to match image patches for stereo reconstruction. However, training such
metrics requires large amount of labeled stereo images, which can be difficult
or costly to collect for certain applications. The main contribution of our
work is a new semi-supervised method for learning deep metrics from unlabeled
stereo images, given coarse information about the scenes and the optical
system. Our method alternatively optimizes the metric with a standard
stochastic gradient descent, and applies stereo constraints to regularize its
prediction. Experiments on reference data-sets show that, for a given network
architecture, training with this new method without ground-truth produces a
metric with performance as good as state-of-the-art baselines trained with the
said ground-truth. This work has three practical implications. Firstly, it
helps to overcome limitations of training sets, in particular noisy ground
truth. Secondly it allows to use much more training data during learning.
Thirdly, it allows to tune deep metric for a particular stereo system, even if
ground truth is not available.","The stereo reconstruction problem consists in estimat ing a depth map from two images taken from differ ent viewpoints. The problem has many practical appli cations in robotics [ 34], remote sensing [ 43], and 3D graphics[ 47]. It has been heavily investigated for several decades [ 40], and recent developments focused on designing highorder, regionbased and objectspeciÔ¨Åc priors [60,10,55,17,24,29,52,51], and improving ‚àóstepan.tulyakov@epÔ¨Ç.chefÔ¨Åciency of large scale stereo [ 36,25,16,7]. Perhaps the most signiÔ¨Åcant recent breakthrough was to use deep metrics [ 12,58]. It led to considerable gains in processing speed and reconstruction accuracy (see Tables4,5, and6). Our work improvesupon this line ofresearch. 2 Related work "
199,End-to-end Recurrent Denoising Autoencoder Embeddings for Speaker Identification.txt,"Speech 'in-the-wild' is a handicap for speaker recognition systems due to the
variability induced by real-life conditions, such as environmental noise and
the emotional state of the speaker. Taking advantage of the principles of
representation learning, we aim to design a recurrent denoising autoencoder
that extracts robust speaker embeddings from noisy spectrograms to perform
speaker identification. The end-to-end proposed architecture uses a feedback
loop to encode information regarding the speaker into low-dimensional
representations extracted by a spectrogram denoising autoencoder. We employ
data augmentation techniques by additively corrupting clean speech with
real-life environmental noise in a database containing real stressed speech.
Our study presents that the joint optimization of both the denoiser and speaker
identification modules outperforms independent optimization of both components
under stress and noise distortions as well as hand-crafted features.","Speech in real life is commonly noisy and under unconstrained conditions that are dicult to predict and aggravate their recognition and understanding. E. RituertoGonz alez Group of Multimedia Processing, Department of Signal Theory and Communications, University Carlos III of Madrid, Av. Universidad 30, Legan es, 28911 Madrid, Spain Email: erituert@ing.uc3m.es C. Pel aezMoreno Group of Multimedia Processing, Department of Signal Theory and Communications, University Carlos III of Madrid, Av. Universidad 30, Legan es, 28911 Madrid, Spain Email: carmen@tsc.uc3m.esarXiv:2003.07688v5  [eess.AS]  16 May 20222 Esther RituertoGonz alez, Carmen Pel aezMoreno Speaker Recognition (SR) systems need high performance under these `real world' conditions. This is extremely dicult to achieve due to both extrinsic and intrinsic variations and is commonly referred to as Speaker Recognition inthewild . Extrinsic variations encompass background chatter and music, environmental noise, reverberation, channel and microphone eects, etc. On the other hand, intrinsic variations are the inherent factors to the speakers themselves present in speech, such as age, accent, emotion, intonation or speaking rate Stoll (2011). Automatic speech recognition (ASR) systems aim to extract the linguistic information from speech in spite of the intrinsic and extrinsic variations Villalba et al. (2020). However, speaker recognition (SR) takes advantage of the intrinsic or idiosyncratic variations to nd out the uniqueness of each speaker. Besides intraspeaker variability (emotion, health, age), the speaker identity results from a complex combination of physiological and cultural aspects. Still, the role of emotional speech has not been deeply explored in SR. Although it could be considered an idiosyncratic trait, it poses a challenge due to the distortions it produces on the speech signal. It in uences the speech spectrum signicantly, having a considerable impact on the features extracted from it and deteriorating the performance of SR systems. At the same time, extrinsic variations have been a long standing challenge aecting the basis of all speech technologies. Deep Neural Networks have given rise to substantial improvements due to their ability to deal with realworld, noisy datasets without the need for handcrafted features specically designed for robustness. One of the most important ingredients to the success of such methods, however, is the availability of large and diverse training datasets. 1.1BINDI, a cyberphysical system to combat genderbased violence Our goal in this paper is to include a robust Speaker Identication module in BINDI, a cyberphysical system designed to combat genderbased violence within the project EMPATIA1. Adopting a multimodal approach RituertoGonz alez et al. (2020), BINDI employs intelligent signal processing to autonomously detect a violent situation by means of biosignals collected by smart sensors {including a microphone{ integrated in wearable edge devices interconnected through the smartphone of the user. One of the edge devices is currently conceptualized as a smart bracelet that includes three dierent physiological sensors: Blood Volume Pulse (BVP), Galvanic Skin Response (GSR) and Skin Temperature (SKT) Miranda et al. (2017); Miranda Calero et al. (2018). The second is designed as a smart pendant equipped with a microelectromechanical microphone and able to acquire audio and speech from the environment and the user RituertoGonz alez et al. (2019). This body area network factor form 1http://portal.uc3m.es/portal/page/portal/inst_estudios_genero/proyectos/ UC3M4SafetyTitle Suppressed Due to Excessive Length 3 allows to gather data from the desired sources of information to feed the dierent intelligent decision engines capable of alerting of a violent situation. The duration of the batteries of Bindi has been identied as one of the key features to make it practical for genderbased violence victims (GVV). To this end, a lightweight aective computing system based on the biosignals captured by the smart bracelet acts as a rst trigger. These biosignals are periodically captured and processed within this edge device that provides a rapid inference schema. Note that the response provided by this rst physiological layer is designed as a lowcost and lowconsumption trigger. Due to the importance of avoiding missing any potentially violent situation, this trigger is oriented towards detecting any possible cues of fear or panicrelated emotions and therefore to achieve a high recall rate. However, due to its constrained computational capacity and resources, its precision is not as good as it could be under ideal conditions. Then, once an alarm is triggered by the bracelet, the pendant begins to acquire audio, which is compressed and sent via Bluetooth Low Energy (BLE) to the smartphone or smartwatch. The purpose of this acquisition is twofold: rst,this audio is encrypted and sent to a secure server to be used as an evidence in a potential trial Campos Gavi~ no and Larrabeiti L opez (2020) and second, it is used to disambiguate and contextualise the previous biosignals and therefore to improve the overall detection rate. Since the processing of the audio signal takes place in the smartphone, it allows for more energyconsuming methods. In particular, we aim at processing the speech contained in the audio to identify and track the user's voice RituertoGonz alez et al. (2018) and to detect fear and panic. Therefore, the speech information complements that carried by the physiological biosignals. In this paper, our scope is limited to the processing of the audio modality and in particular to the Speaker Identication (SI) task. In the threatening situations we intend to detect, identifying the user and achieving high SI rates is crucial, and it is most likely that the speaker is under an intense negative emotional state, such as panic, fear, anxiety, or its more moderate relative, stress. 1.2Contribution In this paper, we address the combined problem of the lack of environmental noise robustness of SR systems and the eects of negative emotional speech on their performance. Our contribution capitalizes on using robust speaker discriminator oriented embeddings extracted from a Recurrent Denoising Autoencoder combined with a Shallow Neural Network {a feedforward neural network{ acting as a backend classier for the task of Speaker Identication, as detailed in Figure 1. This endtoend architecture is designed to work under adverse conditions, both from the point of view of distorted speech due to stressing situations, and environmental noise.4 Esther RituertoGonz alez, Carmen Pel aezMoreno We choose speech recorded under spontaneous stress conditions due to its reallife nature. Induced, simulated or acted emotions {especially negative ones{ are known to be perceived and automatically detected much more strongly than real emotions. This suggests that actors are prone to overacting, which casts doubt on the reliability of these samples Wilting et al. (2006), being a big drawback for devices working in real life conditions such as BINDI. Moreover, we augment our database with synthetic noisy signals by additively contaminating the dataset with environmental noise to emulate speech recorded in real conditions. We discuss a recurrent denoising autoencoder architecture based on Gated Recurrent Units (GRU), where the recurrent architecture targets modelling the temporal context of speech utterances. The encoder network extracts frame level embeddings from the speech spectrograms and is jointly optimized with a feed forward network whose output layer calculates speaker class posteriors. With the help of the denoising module that attempts to remove environmental noise information, and the SNN that targets recognizing the speaker, all information that is not directly employed for speakers' identication is dismissed from the embeddings. In particular, the loss function associated with this last dense network is also fed into the denoising autoencoder to guide its eorts towards the SR task, as will be further described in section 3. Finally, we put forward that these speaker discrimination oriented embeddings are more robust to noise and stress variability than those optimized separately by comparing the eects of automatically extracted embeddings by this twostage connected architecture against the two modules separately, handcrafted features previously demonstrated to be suited for this problem and a frequency recurrent alternative obtained by transposing the inputs to the GRU autoencoder. Moreover, we achieve these results with a computationally lightweight multitask endtoend architecture that extracts emotion and noiserobust speaker discriminant embeddings, in spite of the few datasets available for this purpose. 2 Related Work "
359,Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators.txt,"Multi-label learning is an emerging extension of the multi-class
classification where an image contains multiple labels. Not only acquiring a
clean and fully labeled dataset in multi-label learning is extremely expensive,
but also many of the actual labels are corrupted or missing due to the
automated or non-expert annotation techniques. Noisy label data decrease the
prediction performance drastically. In this paper, we propose a novel Gold
Asymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that
operates robust against noisy labels. GALC-SLR estimates the noise confusion
matrix using single-label samples, then constructs an asymmetric loss
correction via estimated confusion matrix to avoid overfitting to the noisy
labels. Empirical results show that our method outperforms the state-of-the-art
original asymmetric loss multi-label classifier under all corruption levels,
showing mean average precision improvement up to 28.67% on a real world dataset
of MS-COCO, yielding a better generalization of the unseen data and increased
prediction performance.","Realworld images naturally contain multiple objects corresponding to diverse labels, which elevates deep learning models for multilabel learning. Multilabel classiÔ¨Åcation is an extension of multiclass classiÔ¨Åcation where the input is not assigned only a single label, but multiple ones. It is extremely time consuming and expensive to collect high quality labels for singlelabel images. Even longstanding and highly curated datasets, e.g. CIFAR [Krizhevsky et al., 2009], contain wrong labels [Chen et al., 2019a]. Multilabel learning, where each image has multiple possible label combinations, exacerbates this problem [Zhao and Gomes, 2021]. For instance, [Veit et al., 2017] shows that the Open Images dataset [Krasin et al., 2016], which is widely used for multilabel and multiclass image classiÔ¨Åcation, contains 26.6% false positives among the training label set. In singlelabel classiÔ¨Åcation, label noise has been widely studied [Algan and Ulusoy, 2021]. Due to the memorization effect Deep Neural Networks (DNNs) can overÔ¨Åt to noise degrading signiÔ¨Åcantly their performance [Zhang et al., 2017]. Several techniques have been proposed to counter the effect of wrong labels [Hendrycks et al., 2018, Han et al., 2018a, Patrini et al., 2017, Yao et al., 2019]. However, multilabel classiÔ¨Åcation is a more complex problem. As depicted in Fig. 1(a), each image comes with multiple labels including some wrong and some clean ones. This has a negative impact on the performance of DNNs. According to [Song et al., 2020], existing noise resilient methods for singlelabel are not able to learn the correlation among multiple labels. Even so little attention has been given to evaluating multilabel classiÔ¨Åers with noisy labels [Zhao and Gomes, 2021].arXiv:2108.02032v1  [cs.CV]  4 Aug 2021MultiLabel Gold Asymmetric Loss Correction with SingleLabel Regulators ImageReferenceLabelsSettreebicyclecanalperson treecarcanalcat MultiLabel with Wrong Labels  (a) Example of image with wrong labels 0.00.10.20.30.40.50.6 Noise fraction3040506070mAP74.91 69.55 64.71 58.89 52.18 45.24 36.81 (b) Impact of increasing wrong label ratios Figure 1: Wrong labels and their impact in multilabel classiÔ¨Åcation. To Ô¨Åll the gap in noiseresilient multilabel classiÔ¨Åers, we propose Gold Asymmetric Loss Correction with SingleLabel Regulators ( GALCSLR ).GALCSLR assumes that a small subset of the training samples can be trusted. We use this additional information to accurately estimate the noise corruption matrix. Due to class imbalance and label correlations, learning the noise in realworld multilabel datasets is more complicated than in realworld singlelabel datasets. Hence, we introduce a novel method that uses single label regulators to rebalance the predictions towards a targeted label. This leads to accurate noise estimations used to correct the wrong labels during training making the model robust to label noise even in the more challenging multilabel setting. In comparison to the stateoftheart Asymmetric Loss (ASL) multilabel classiÔ¨Åer [Baruch et al., 2020] GALCSLR is signiÔ¨Åcantly more accurate under label noise. ASL balances the probabilities of different samples by treating positive and negative samples differently, i.e. asymmetrically. In empirical evaluation on the MSCOCO dataset [Lin et al., 2014] GALCSLR outperforms ASL under all tested noise ratios from 0% to 60%. GALCSLR improves the mean Average Precision (mAP) over ASL on average by 13.81% and up to 28.67%. The contributions of this paper are summarized as follows: ‚Ä¢We design a noise estimation technique that uses trusted multilabel and singlelabel data in order to calculate the corruption matrix. ‚Ä¢Using our noise estimation, we design a robust multilabel classiÔ¨Åer, GALCSLR , based on a loss correction approach. ‚Ä¢We compare GALCSLR against a stateoftheart classiÔ¨Åer under noisy labels and study the behaviour of GALCSLR in target ablation study experiments. 1.1 Motivation example Our motivation stems from the detrimental effects label noise in training data can have on the model performance. We demonstrate this using the stateoftheart ASL [Baruch et al., 2020] method to train a TResNetM [Ridnik et al., 2021a] network on the MSCOCO dataset [Lin et al., 2014]. ASL applied on TResNet ranks top on the leader board for MLC on MSCOCO1. We inject symmetric label noise (details in Section 4.1) at various corruption levels, from 0% to 60%, and report the mean average precision to asses the impact of wrong labels. mAP is considered by many recent works [Lanchantin et al., 2020, Chen et al., 2019b] as important metric for performance evaluation in multilabel classiÔ¨Åcation, since it takes into consideration both falsenegative and falsepositive rates [Baruch et al., 2020]. Fig. 1(b) shows the results: each additional 10% noisy labels leads to a 5%8% reduction in mAP score. Since it is hard and costly to avoid label noise [Zhao and Gomes, 2021], it is vital to develop robust classiÔ¨Åers that can avoid overÔ¨Åtting to the label noise in the training data. 2 Related Work "
152,ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification.txt,"We present ENHANCE, an open dataset with multiple annotations to complement
the existing ISIC and PH2 skin lesion classification datasets. This dataset
contains annotations of visual ABC (asymmetry, border, colour) features from
non-expert annotation sources: undergraduate students, crowd workers from
Amazon MTurk and classic image processing algorithms. In this paper we first
analyse the correlations between the annotations and the diagnostic label of
the lesion, as well as study the agreement between different annotation
sources. Overall we find weak correlations of non-expert annotations with the
diagnostic label, and low agreement between different annotation sources. We
then study multi-task learning (MTL) with the annotations as additional labels,
and show that non-expert annotations can improve (ensembles of)
state-of-the-art convolutional neural networks via MTL. We hope that our
dataset can be used in further research into multiple annotations and/or MTL.
All data and models are available on Github:
https://github.com/raumannsr/ENHANCE.","Machine learning oÔ¨Äers many opportunities, but medical imag ing datasets, for example for skin lesion diagnosis, are limited, and overÔ¨Åtting can o ccur. To illustrate, Winkler and colleagues found that superimposed scale bars (Winkler et al., 2021) or skin markings (Winkler et al., 2019) in dermoscopic images may impair the d iagnostic performance of the convolutional neural network (CNN) when unintentionally o verÔ¨Åtting these artifacts during model training. ¬©2021 Raumanns, Schouten, Joosten, Pluim and Cheplygina. Li cense: CCBY 4.0. https://www.melbajournal.org/papers/2021020.html .Raumanns et al. A promising approach to generalize better in small sample si ze settings is multitask learning (MTL), wherethe model has to learn diÔ¨Äerent tasks si multaneously. This approach showedimprovedperformanceinvariousmedicalapplicatio ns, forexample, forbreastlesions (Shi et al., 2019; Liu et al., 2018). However, when moving fro m singletask to multitask models, we need additional annotations. Applying MTL is cha llenging because datasets typically do not have such additional annotations. Further more, building a medical image dataset from scratch with expert annotations is timeconsu ming and costly. We present a dataset of additional annotations for skin lesi on diagnosis based on non expert annotations on three dermatoscopic criteria: asymm etry, border and color (socalled ABC criteria). In dermatology, the use of the ABCDE (asymmet ry, border, color, diame ter, and evolution or elevation) rule is widespread. Howeve r, scoring the diameter (D) and evolution or elevation (E) are more complex tasks and theref ore less suitable for nonexpert annotation. The term nonexpert is deÔ¨Åned here as annotations provided by three diÔ¨Äer ent annotation sources: undergraduate students, crowd wor kers from Amazon MTurk and automated annotations through classic image processing al gorithms. We study the quality of nonexpert annotations from diÔ¨Äerent viewpoints. Firstly, we determine the discriminative power of ABC features for diag nosis. We show to what extent the ABC annotations correlate to the diagnosis, and w e study how we can use ABC annotations to improve the performance of a CNN. Secondl y, the inter agreement level for A, B and C feature between the diÔ¨Äerent annotation so urces. The study ex tends our research on the topic (Raumanns et al., 2020) by usi ng automated annotations as well as comparing the performance on three open source CNN architectures, in partic ular: VGG16 (Simonyan and Zisserman, 2015), Inception v3 ( Szegedy et al., 2016) and ResNet50 (He et al., 2016) encoders. Further, we investigat e whether MTL is also beneÔ¨Å cial for automated annotations and show that the performanc e beneÔ¨Åts from using multiple annotations in MTL. Besides addressing the lack of expert annotations using non expert ones, we make the dataset with collected ABC annotations and code open, elimi nating obstacles for future research. More speciÔ¨Åcally, the investigation addresses t he following research questions: 1.What is the correlation between the ABC annotations and the diagnostic label? 2.How can we use the ABC annotations to improve the performance of a CNN? 3.What is the inter agreement level for A, B and C feature between the diÔ¨Äerent annotation sources? 4.How can CNN performance beneÔ¨Åt from using multiple annotations? Our results give valuable insights into the quality of none xpert annotations. Using the collected nonexpert annotations in three diÔ¨Äerent CNNs , we show that these are of added value for the performance of the models. This suggests that the use of nonexpert annotations might be promising for application in similar d omains. 2ENHANCE (ENriching Health data by ANnotations of Crowd and E xperts) 2. Related Work "
352,Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions.txt,"This paper presents a neural relation extraction method to deal with the
noisy training data generated by distant supervision. Previous studies mainly
focus on sentence-level de-noising by designing neural networks with intra-bag
attentions. In this paper, both intra-bag and inter-bag attentions are
considered in order to deal with the noise at sentence-level and bag-level
respectively. First, relation-aware bag representations are calculated by
weighting sentence embeddings using intra-bag attentions. Here, each possible
relation is utilized as the query for attention calculation instead of only
using the target relation in conventional methods. Furthermore, the
representation of a group of bags in the training set which share the same
relation label is calculated by weighting bag representations using a
similarity-based inter-bag attention module. Finally, a bag group is utilized
as a training sample when building our relation extractor. Experimental results
on the New York Times dataset demonstrate the effectiveness of our proposed
intra-bag and inter-bag attention modules. Our method also achieves better
relation extraction accuracy than state-of-the-art methods on this dataset.","Relation Extraction is a fundamental task in nat ural language processing (NLP), which aims to extract semantic relations between entities. For example, sentence ‚Äú[ Barack Obama ]e1was born in [Hawaii ]e2‚Äù expresses the relation BornIn be tween entity pair Barack Obama andHawaii . Conventional relation extraction methods, such as (Zelenko et al., 2002; Culotta and Sorensen, 2004; Mooney and Bunescu, 2006), adopted su pervised training and suffered from the lack of 1The code is available at https://github.com/ZhixiuYe/ IntraBagandInterBagAttentions .bag sentence correct? B1S1. Barack Obama was born in the United States .Yes S2.Barack Obama was the 44th pres ident of the United StatesNo B2S3. Kyle Busch , aLas Vegas res ident who ran second to Johnson last year, Ô¨Ånished third, followed by Kasey Kahne, Jeff Gordon and mark martin .No S4. Hendrick drivers Ô¨Ånished in three of the top four spots at Las Vegas , in cluding Kyle Busch in second and ...No Table 1: Examples of sentences with relation place ofbirth annotated by distant supervision, where ‚ÄúYes‚Äù and ‚ÄúNo‚Äù indicate whether or not each sentence actually expresses this relation. largescale manually labeled data. To address this issue, the distant supervision method (Mintz et al., 2009) was proposed, which generated the data for training relation extraction models automatically. The distant supervision assumption says that if two entities participate in a relation, allsentences that mention these two entities express that rela tion. It is inevitable that there exists noise in the data labeled by distant supervision. For example, the precision of aligning the relations in Freebase to the New York Times corpus was only about 70% (Riedel et al., 2010). Thus, the relation extraction method proposed in (Riedel et al., 2010) argued that the distant su pervision assumption was too strong and relaxed it toexpressedatleastonce assumption. This as sumption says that if two entities participate in a relation, at least one sentence that mentions these two entities might express that relation. An ex ample is shown by sentences S1 and S2 in Table 1. This relation extraction method Ô¨Årst divided the training data given by distant supervision into bags where each bag was a set of sentences con taining the same entity pair. Then, bag representa tions were derived by weighting sentences withinarXiv:1904.00143v1  [cs.CL]  30 Mar 2019each bag. It was expected that the weights of the sentences with incorrect labels were reduced and the bag representations were calculated mainly us ing the sentences with correct labels. Finally, bags were utilized as the samples for training relation extraction models instead of sentences. In recent years, many relation extraction meth ods using neural networks with attention mecha nism (Lin et al., 2016; Ji et al., 2017; Jat et al., 2018) have been proposed to alleviate the inÔ¨Çu ence of noisy training data under the expressedat leastonce assumption. However, these methods still have two deÔ¨Åciencies. First, only the target re lation of each bag is used to calculate the attention weights for deriving bag representations from sen tence embeddings at training stage. Here we argue that the bag representations should be calculated in a relationaware way. For example, the bag B1 in Table 1 contains two sentences S1 and S2. When this bag is classiÔ¨Åed to relation BornIn , the sen tence S1 should have higher weight than S2, but when classiÔ¨Åed to relation PresidentOf , the weight of S2 should be higher. Second, the expressed atleastonce assumption ignores the noisy bag problem which means that all sentences in one bag are incorrectly labeled. An example is shown by bag B2 in Table 1. In order to deal with these two deÔ¨Åciencies of previous methods, this paper proposes a neu ral network with multilevel attentions for dis tant supervision relation extraction. At the instance/sentencelevel, i.e., intrabag level, all possible relations are employed as queries to cal culate the relationaware bag representations in stead of only using the target relation of each bag. To address the noisy bag problem, a bag group is adopted as a training sample instead of a single bag. Here, a bag group is composed of bags in the training set which share the same relation label. The representation of a bag group is calculated by weighting bag representations using a similarity based interbag attention module. The contributions of this paper are threefold. First, an improved intrabag attention mechanism is proposed to derive relationaware bag represen tations for relation extraction. Second, an inter bag attention module is introduced to deal with the noisy bag problem which is ignored by the expressedatleastonce assumption. Third, our methods achieve better extraction accuracy than stateoftheart models on the widely used NewYork Times (NYT) dataset (Riedel et al., 2010). 2 Related Work "
86,A new semi-supervised self-training method for lung cancer prediction.txt,"Background and Objective: Early detection of lung cancer is crucial as it has
high mortality rate with patients commonly present with the disease at stage 3
and above. There are only relatively few methods that simultaneously detect and
classify nodules from computed tomography (CT) scans. Furthermore, very few
studies have used semi-supervised learning for lung cancer prediction. This
study presents a complete end-to-end scheme to detect and classify lung nodules
using the state-of-the-art Self-training with Noisy Student method on a
comprehensive CT lung screening dataset of around 4,000 CT scans.
  Methods: We used three datasets, namely LUNA16, LIDC and NLST, for this
study. We first utilise a three-dimensional deep convolutional neural network
model to detect lung nodules in the detection stage. The classification model
known as Maxout Local-Global Network uses non-local networks to detect global
features including shape features, residual blocks to detect local features
including nodule texture, and a Maxout layer to detect nodule variations. We
trained the first Self-training with Noisy Student model to predict lung cancer
on the unlabelled NLST datasets. Then, we performed Mixup regularization to
enhance our scheme and provide robustness to erroneous labels.
  Results and Conclusions: Our new Mixup Maxout Local-Global network achieves
an AUC of 0.87 on 2,005 completely independent testing scans from the NLST
dataset. Our new scheme significantly outperformed the next highest performing
method at the 5% significance level using DeLong's test (p = 0.0001). This
study presents a new complete end-to-end scheme to predict lung cancer using
Self-training with Noisy Student combined with Mixup regularization. On a
completely independent dataset of 2,005 scans, we achieved state-of-the-art
performance even with more images as compared to other methods.","The prevalence of lung cancer is increasing yearly and with the highest mortality rate  among other types of cancer [1]. It is estimated that 228,820  new lung cancer cases will be  diagnosed in the year 2020 in the United States  [2]. With the advance of medical a nd surgical  treatment shifting towards precision medicine, early detection of the disease will significantly  improve patients‚Äô survival rate.    To date, most researchers have successfully developed methods to detect or classify lung  nodules independently. However, there are only relatively few methods that can simultaneously  detect and classify nodules from computed tomography (CT) scans [3]. To address  this  shortcoming, we present  in this study  an end toend scheme  to detect and classif y lung nodules  in order to predict lung cancer, which is evaluated on a comprehensive CT lung screeni ng dataset  of around 4,000 CT scans.   A general  difficulty in the medical imaging field is the unavailability of sufficient  labelled /annotated data to validate new deep learning methods  [4]. Relatively small annotated  datasets including LUNA16 and LIDC IDRI  [5,6]  are typically used for lung nodule detection  or classification. The LUNA16 and LIDC IDRI datasets contain detailed annotated locations of  each lung nodule within the CT scans. The National Lung Screening Trial (NLST) lung  screening dataset [7], released by the National Cancer Institute (NCI), is a vast resource of lung  screening CT scans; however, a downside of the NLST dataset is that the ground truth data only  provides a final diagnosis of cancer (i.e., 1) or cancer free (i.e., 0) for each pa tient and does not  provide the labelled/annotated locations of the nodules within the CT scans.   To overcome this deficiency in the NLST dataset, we use Self training with Noisy  Student Training [8] to leverage on the unlabelled data/nodules  to the b est of our knowledge  this has not been done before. Self training has produced state oftheart results on image  classification in ImageNet [8]. Using the self training method, a trained teacher model will  generate pseudo  labels on unlabelled images. T hen, a student model is trained on both labelled  and pseudo labelled images, and noise is simultaneously added to improve the student‚Äôs learning  capacity. In our self training framework, we use our state oftheart Local Global network [9] to  classify unlabelled nodules in the NLST dataset and train a new student model with noise on the  pseudo labelled data. Similar to the results obtained in the original self training paper [8], we  obtained significant improvement in our lung nod ule classification scheme in terms of the area  under the receiver operating characteristic curve (AUC) and we obtained state oftheart results  for lung canc er predic tion on the NLST dataset.   Our main contributions are as follows:  1. We develop a new automati c end toend scheme that simultaneously detects and  classifies  lung nodules to predict  lung cancer from CT scans.   2. We implement the first Selftraining with Noisy Student model for lung cancer prediction  on the unlabelled NLST dataset and show that the results generalize to big independent  datasets.   3. We enhance our state oftheart lung nodule classification scheme, namely the Local  Global network , by implementing the Mixup technique to improve final model  performance  and including  a Maxout Layer to classify  large variations of lung nodules .  4. We train and  develop our new scheme on around 4,000 CT scans and achieve state of theart performance  even with more  images  as compared to other studies .     2. Related work  "
457,Memory-based control with recurrent neural networks.txt,"Partially observed control problems are a challenging aspect of reinforcement
learning. We extend two related, model-free algorithms for continuous control
-- deterministic policy gradient and stochastic value gradient -- to solve
partially observed domains using recurrent neural networks trained with
backpropagation through time.
  We demonstrate that this approach, coupled with long-short term memory is
able to solve a variety of physical control problems exhibiting an assortment
of memory requirements. These include the short-term integration of information
from noisy sensors and the identification of system parameters, as well as
long-term memory problems that require preserving information over many time
steps. We also demonstrate success on a combined exploration and memory problem
in the form of a simplified version of the well-known Morris water maze task.
Finally, we show that our approach can deal with high-dimensional observations
by learning directly from pixels.
  We find that recurrent deterministic and stochastic policies are able to
learn similarly good solutions to these tasks, including the water maze where
the agent must learn effective search strategies.","The use of neural networks for solving continuous control problems has a long tradition. Several recent papers successfully apply modelfree, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12]. However, all of this work assumes fully observed state. Many real world control problems are partially observed. Partial observability can arise from dif ferent sources including the need to remember information that is only temporarily available such as a way sign in a navigation task, sensor limitations or noise, unobserved variations of the plant under control (system identiÔ¨Åcation), or statealiasing due to function approximation. Partial ob servability also arises naturally in many tasks that involve control from vision: a static image of a dynamic scene provides no information about velocities, occlusions occur as a consequence of the threedimensional nature of the world, and most vision sensors are bandwidthlimited and only have a restricted Ô¨Åeldofview. Resolution of partial observability is nontrivial. Existing methods can roughly be divided into two broad classes: On the one hand there are approaches that explicitly maintain a belief state that corresponds to the distribution over the world state given the observations so far. This approach has two major disadvantages: The Ô¨Årst is the need for a model, and the second is the computational cost that is typically associated with the update of the belief state [8, 23]. 1arXiv:1512.04455v1  [cs.LG]  14 Dec 2015On the other hand there are model free approaches that learn to form memories based on interactions with the world. This is challenging since it is a priori unknown which features of the observations will be relevant later, and associations may have to be formed over many steps. For this reason, most model free approaches tend to assume the fullyobserved case. In practice, partial observability is often solved by handcrafting a solution such as providing multipleframes at each timestep to allow velocity estimation [16, 14]. In this work we investigate a natural extension of two recent, closely related policy gradient algo rithms for learning continuousaction policies to handle partially observed problems. We primarily consider the Deterministic Policy Gradient algorithm (DPG) [24], which is an offpolicy policy gradient algorithm that has recently produced promising results on a broad range of difÔ¨Åcult, high dimensional continuous control problems, including direct control from pixels [14]. DPG is an actorcritic algorithm that uses a learned approximation of the actionvalue (Q) function to obtain approximate actionvalue gradients. These are then used to update a deterministic policy via the chainrule. We also consider DPG‚Äôs stochastic counterpart, SVG(0) ([6]; SVG stands for ‚ÄúStochastic Value Gradients‚Äù) which similarly updates the policy via backpropagation of actionvalue gradients from an actionvalue critic but learns a stochastic policy. We modify both algorithms to use recurrent networks trained with backpropagation through time. We demonstrate that the resulting algorithms, Recurrent DPG (RDPG) and Recurrent SVG(0) (RSVG(0)), can be applied to a number of partially observed physical control problems with di verse memory requirements. These problems include: shortterm integration of sensor information to estimate the system state (pendulum and cartpole swingup tasks without velocity information); system identiÔ¨Åcation (cart pole swingup with variable and unknown polelength); longterm mem ory (a robot arm that needs to reach out and grab a payload to move it to the position the arm started from); as well as a simpliÔ¨Åed version of the water maze task which requires the agent to learn an exploration strategy to Ô¨Ånd a hidden platform and then remember the platform‚Äôs position in order to return to it subsequently. We also demonstrate successful control directly from pixels. Our results suggest that actorcritic algorithms that rely on bootstrapping for estimating the value function can be a viable option for learning control policies in partially observed domains. We further Ô¨Ånd that, at least in the setup considered here, there is little performance difference between stochastic and deterministic policies, despite the former being typically presumed to be preferable in partially observed domains. 2 Background We model our environment as discretetime, partiallyobserved Markov Decision process (POMDP). A POMDP is described a set of environment states Sand a set of actions A, an initial state distribu tionp0(s0), a transition function p(st+1jst;at)and reward function r(st;at). This underlying MDP is partially observed when the agent is unable to observe the state stdirectly and instead receives observations from the set Owhich are conditioned on the underlying state p(otjst). The agent only indirectly observes the underlying state of the MDP through the observations. An optimal agent may, in principle, require access to the entire history ht= (o1;a1;o2;a2;:::at"
366,A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model.txt,"In this work, we explore Parameter-Efficient-Learning (PEL) techniques to
repurpose a General-Purpose-Speech (GSM) model for Arabic dialect
identification (ADI). Specifically, we investigate different setups to
incorporate trainable features into a multi-layer encoder-decoder GSM
formulation under frozen pre-trained settings. Our architecture includes
residual adapter and model reprogramming (input-prompting). We design a
token-level label mapping to condition the GSM for Arabic Dialect
Identification (ADI). This is challenging due to the high variation in
vocabulary and pronunciation among the numerous regional dialects. We achieve
new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We
further reduce the training budgets with the PEL method, which performs within
1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable
parameters. Our study demonstrates how to identify Arabic dialects using a
small dataset and limited computation with open source code and pre-trained
models.","Dialect identiÔ¨Åcation [1, 2] (DI) amounts to identifying similar dialects belonging to the same language family. It is a speciÔ¨Åc case of language identiÔ¨Åcation [3] (LID) task. However, DI is more challenging than LID owing to the fact that dialects share similar acoustic and linguistic characteristics compared to dif ferent languages. Very minute differences in pronunciation [4] and accent are used as cues to identify dialects. Moreover, DI does not share the advantage of publicly available speech recog nition models pretrained on large speech data corpora for net work initialization. Despite these challenges, DI remains rela tively unexplored compared to LID. In this study, we leverage upon a recent openaccess and generalpurpose speech recognition architecture, Whisper [5], pretrained on a large speech corpus from OpenAI, to address DI in resourceconstrained and datalimited conditions. We use ParameterEfÔ¨Åcient Learning [6, 7] (PEL) to adapt a large pre trained model by training small additive modules embedded into the frozen pretrained model. By doing so, we require less training time and computing resources to Ô¨Ånetune the model for DI. Figure 1 is a schematic of the proposed parameter efÔ¨Åcient learning framework. We choose to perform DI in Arabic owing to its substantial regional variations and the widespread use of Arabic as an ofÔ¨Åcial language in over 22 countries [8]. Notably, signiÔ¨Åcant differences exist between the standard written form, referred to as Modern Standard Arabic, and the local colloquial dialects spoken in each region. Interestingly, not all dialects are Figure 1: Overview of proposed parameterefÔ¨Åcient learn ing framework for Arabic dialect identiÔ¨Åcation building upon parameterefÔ¨Åcient learning [7] and label mapping [10]. mutually intelligible. In this study, we present several contributions: (1) Firstly, we introduce the novel use of ParameterEfÔ¨ÅcientLearning (PEL) for this task, marking the Ô¨Årst application of this ap proach to Arabic dialect identiÔ¨Åcation [8]. (2) We investigate different designs to incorporate trainable features into a multi layer encoderdecoder frozen model. (3) We achieve new state oftheart accuracy on the ofÔ¨Åcial testing and development sets of ADI17 [9] dataset using only 30.95% of the training data. (4) Lastly, we demonstrate that our PEL method achieves equiv alent performance to full Ô¨Ånetuning using only 2.5% of (extra) network parameters. 2. Related work "
374,SyReNets: Symbolic Residual Neural Networks.txt,"Despite successful seminal works on passive systems in the literature,
learning free-form physical laws for controlled dynamical systems given
experimental data is still an open problem. For decades, symbolic mathematical
equations and system identification were the golden standards. Unfortunately, a
set of assumptions about the properties of the underlying system is required,
which makes the model very rigid and unable to adapt to unforeseen changes in
the physical system. Neural networks, on the other hand, are known universal
function approximators but are prone to over-fit, limited accuracy, and bias
problems, which makes them alone unreliable candidates for such tasks. In this
paper, we propose SyReNets, an approach that leverages neural networks for
learning symbolic relations to accurately describe dynamic physical systems
from data. It explores a sequence of symbolic layers that build, in a residual
manner, mathematical relations that describes a given desired output from input
variables. We apply it to learn the symbolic equation that describes the
Lagrangian of a given physical system. We do this by only observing random
samples of position, velocity, and acceleration as input and torque as output.
Therefore, using the Lagrangian as a latent representation from which we derive
torque using the Euler-Lagrange equations. The approach is evaluated using a
simulated controlled double pendulum and compared with neural networks, genetic
programming, and traditional system identification. The results demonstrate
that, compared to neural networks and genetic programming, SyReNets converges
to representations that are more accurate and precise throughout the state
space. Despite having slower convergence than traditional system
identification, similar to neural networks, the approach remains flexible
enough to adapt to an unforeseen change in the physical system structure.","Neural networks have successfully been applied in a complex range of hard to solve problems, e.g. convolutional neural networks [ 1], generative adversarial networks [ 2] and transformers [ 3], respectively changed their subÔ¨Åelds of research. Yet, there exist areas that are still barely inÔ¨Çuenced by neural architectures. In this paper, we are particularly interested in modeling dynamics of physical systems. The importance of having a reliable dynamic model is clear for feedforward motion generation, for motion planning, and when a swift dynamic response is required. It also allows for more accurate simulations of the underlying system, which in turn permits longer horizon of motion predictions. Additionally, the controller would be able to work with smaller gains to achieve a given state, which leads to less stiff actuators that are safer to interact with [4, 5]. For many decades, the standard approach for modeling the dynamics of a given physical system is done by measuring or estimating the actuation positions, velocities, accelerations and Ô¨Ånding relations Preprint. Under review.arXiv:2105.14396v1  [cs.LG]  30 May 2021between the commanded inputs (torque for example). Those relations would then be formulated by a set of mathematical equations that describes the motion of the system [ 6]. As an example, Equation 1 represents the inverse dynamics of a serialchain rigidbody robotic system: =M(q)q+C(q;_q)_q+G(q) (1) It describes the relation between torque ( ) and position, velocity, and acceleration ( q;_q;q) at jointlevel.M,CandGare preformulated matrix functions that mathematically describe inertial forces, centrifugal and Coriolis forces, and gravitational forces, respectively, in terms of mass, center of mass, and inertial matrix of each rigidbody. Traditional system identiÔ¨Åcation methods aim to approximate them and, therefore, is still used due to its mathematical stability in the whole observable state space of the agent. The disadvantage is that the equations have Ô¨Åxed terms and even if some of the numerical parameters can be changed over time it limits the physical system to a given set of assumptions (e.g. shape and actuation type), making it complex to optimize for systems made out of hard to model materials (e.g. rubber) or that might be subject to unforeseen changes in structure for different applications. Neural networks are generally known to be universal function approximators [ 7] but they are prone to overÔ¨Åt to the training data and have biased decisions when trained with an imbalanced or incomplete dataset [ 8‚Äì10]. Therefore, they, alone, are not reliable enough to be a surrogate model. However, for most dynamical systems belonging to our speciÔ¨Åc use case, there exists an analytical exact mathematical solution that can accurately describe the dynamics. For those, a different class of algorithms can be applied. For a lack of better term, we denote them as universal exact function estimators since instead of approximating a function they seek to estimate the exact underlying one that solves the problem. The most well known representative of this class is genetic programming [11], which is an evolutionary strategy for evolving symbolic tree representations. Each tree encodes a symbolic function. There, several generations of candidate symbolic solutions are tested and combined until a convergence criterion is reached. Despite suffering the same problems as neural networks, since those methods are mostly not dependent on gradient, if an underlying function exists it can eventually be found due to the random nature of the evolutionary process, given enough time and reinitializations. However, the search is highly inefÔ¨Åcient since the process is fundamentally dependent on randomness, leading to repetitive and ""unnecessary"" evaluations of candidate solutions. Therefore, those approaches tend to not scale well with the increase of dimensionality of the problem [12]. The truth is that exact symbolic estimation is inherently an NPhard problem, which explains why dynamic model learning is still an open problem to solve. We argue that a hybrid approach is a reasonable way to obtain the best of both worlds, namely the expressiveness of neural network architectures and the stability potential of symbolic representations. This would allow a general symbolic learning architecture to be applicable to any physical system that can be expressed analytically, potentially mitigating the disadvantages of ""approximation"" algorithms and the search inefÔ¨Åciency of the exact estimation methods. In this paper, we present Symbolic Residual Neural Networks ( SyReNets ), a neural network archi tecture capable of learning symbolic mathematical relations from streams of data. SyReNets can potentially be applied to estimate a vast number of functions, describing dynamics of many different physical systems. Traditionally, most methods try to learn the inverse dynamics (as denoted by Equation 1), since it conveys the relation between position, velocity and acceleration to applied torques [ 13]. However, each actuator adds one function to the number of equations to learn. Aiming at learning a Ô¨Åx and minimal number of equations we chose to learn the Lagrangian representation of the system: L=T"
537,Multi-attention Networks for Temporal Localization of Video-level Labels.txt,"Temporal localization remains an important challenge in video understanding.
In this work, we present our solution to the 3rd YouTube-8M Video Understanding
Challenge organized by Google Research. Participants were required to build a
segment-level classifier using a large-scale training data set with noisy
video-level labels and a relatively small-scale validation data set with
accurate segment-level labels. We formulated the problem as a multiple instance
multi-label learning and developed an attention-based mechanism to selectively
emphasize the important frames by attention weights. The model performance is
further improved by constructing multiple sets of attention networks. We
further fine-tuned the model using the segment-level data set. Our final model
consists of an ensemble of attention/multi-attention networks, deep bag of
frames models, recurrent neural networks and convolutional neural networks. It
ranked 13th on the private leader board and stands out for its efficient usage
of resources.","With the fast development of digital recording devices and online video sharing platforms, the number of videos available is increasing exponentially, making video under standing a challenging problem in computer vision. As a Ô¨Årst step towards video understanding, a signiÔ¨Åcant amount of work has been dedicated to video classiÔ¨Åcation. How ever, the video understanding problem goes beyond a stan dard classiÔ¨Åcation problem. Temporally localizing the pres ences of objects/actions can help us to identify relevant mo ments in a video and thus better understand its content. This work was presented at the 3rd Workshop on YouTube8M Large Scale Video Understanding, at the International Conference on Computer Vision (ICCV 2019) in Seoul, Korea.Moreover, a video can contain a number of topics that are not always characterized by every time segment within the video. Hence, a better temporal localization algorithm can enable applications such as improved video search (search within a video), highlights extraction, etc. To accelerate the research of temporal localization, Google AI recently released the YouTube8M Segment Dataset. In this work, we focus on a segmentlevel classiÔ¨Åcation task presented in the third YouTube8M Challenge using this segmentlevel dataset. Previous YouTube8M challenges focused on developing models for videolevel predictions. Standard deep neural networks like convolutional neural networks (CNNs) [9, 21] and recurrent neural networks (RNNs) [12, 16] have been used for videolevel classiÔ¨Åcation and both have achieved stateoftheart results. Pooling via clustering schemes, such as Vector of Locally aggregated Description (VLAD) [7, 15] and deep bagofframes (DBoF) [1, 18], has also been widely used among the top competitors. However, these framelevel classiÔ¨Åers are designed to classify video level labels and cannot necessarily perform segmentlevel predictions well. Different temporal action localization net works have also been proposed to solve the temporal lo calization problem. One popular structure is a twostage, proposalplusclassiÔ¨Åcation framework [3]. But to utilize large videolabeled training sets, such a model cannot be directly applied. To better leverage the large training dataset which only has noisy videolevel labels together with a comparatively smaller segmentlevel validation dataset, we propose to uti lize a multiinstance learning (MIL) model [13, 5] to simul taneously temporally localize and classify the target seg ments. The core idea is to use multiple attention weights to emphasize critical frames from different highlevel top ics in the video. The proposed model performed better than both standard neural networks and pooling via clustering methods using our training procedure. Before discussing 1arXiv:1911.06866v1  [cs.CV]  15 Nov 2019the models for the task in this challenge, we will give a brief overview of the dataset and the unique challenges it poses. 1.1. YouTube8M Segment Dataset The YouTube8M Segment Dataset is an extension of the previous YouTube8M Dataset [1, 11] which in cludes humanveriÔ¨Åed segmentlevel labels. The previous YouTube8M dataset contains 6 million highquality video samples from YouTube, which were split into 3 partitions: training, validation and test sets, following approximately 70%,20% and10% split. The video samples were encoded as a hidden representation produced by Inceptionv3 [20] pretrained on the ImageNet dataset [6] for both audio and video features taken at a rate of 1 Hz. This dataset only contains videolevel annotations with 3862 class labels and an average of 3 labels per video. In the YouTube8M Segment Dataset, multiple 5second segments are sampled based on classiÔ¨Åer scores to encour age both positive and negative within a video. Then each segment is labeled by human raters from a subset of original vocabulary, excluding entities that are not temporally local izable. In total, 237K segments covering 1000 categories are labeled. These video segment labels provide a valuable resource for temporal localization. In the 3rd YouTube8M video understanding challenge, we are required to predict segmentlevel labels in the test set. Submissions are evalu ated using the Mean Average Precision @ Ks(MAP@Ks), whereKs= 100;000. For each entity, the MAP@ Ksscore is calculated as MAP @Ks=1 CCX c=1KsP k=1P(k)rel(k) Nc(1) whereCis the number of Classes, P(k)is the precision at cutoffk,Ksis the number of segments predicted per class, rel(k)is an indicator function equaling 1if the item at rank kis a relevant (correct) class, or zero otherwise, and Ncis the number of positivelylabeled segments for each class. The paper is organized as follows. In section 2, we re view the general model architectures we used, along with some related work. Details of our models are introduced in section 3. Section 4 presents the evaluation of all models, and section 5 concludes the paper. 2. Related work "
269,Boundary Regularized Building Footprint Extraction From Satellite Images Using Deep Neural Network.txt,"In recent years, an ever-increasing number of remote satellites are orbiting
the Earth which streams vast amount of visual data to support a wide range of
civil, public and military applications. One of the key information obtained
from satellite imagery is to produce and update spatial maps of built
environment due to its wide coverage with high resolution data. However,
reconstructing spatial maps from satellite imagery is not a trivial vision task
as it requires reconstructing a scene or object with high-level representation
such as primitives. For the last decade, significant advancement in object
detection and representation using visual data has been achieved, but the
primitive-based object representation still remains as a challenging vision
task. Thus, a high-quality spatial map is mainly produced through complex
labour-intensive processes. In this paper, we propose a novel deep neural
network, which enables to jointly detect building instance and regularize noisy
building boundary shapes from a single satellite imagery. The proposed deep
learning method consists of a two-stage object detection network to produce
region of interest (RoI) features and a building boundary extraction network
using graph models to learn geometric information of the polygon shapes.
Extensive experiments show that our model can accomplish multi-tasks of object
localization, recognition, semantic labelling and geometric shape extraction
simultaneously. In terms of building extraction accuracy, computation
efficiency and boundary regularization performance, our model outperforms the
state-of-the-art baseline models.","Acquiring information about the structure on the surface of th e  earth without making physical conta ct is generally achieved by  the remote sensing techniques (Campbell, Wynne, 2011) .  Applications like digital mapping, land use analysis, disaster  monitoring and climate modelling largely use satellite images.  The satellite images have been important in the  creation of the  digital maps for the Geographic Information System (GIS) and  the building footprint information is playing an instrumental role  in urban planning, smart city construction and many others. In  addition, the building footprints with regulariz ed boundaries are  able to produce polygons of vector representation, which hold  stronger transferability over multiple GIS platforms therefore  having an expensive domain of applications. For example, the  regularized building polygons can produce more accur ate 3D  building models. Nonetheless, as satellite images are readily  available and accessible, so there is always a demand for better  quality of the building footprints. This demand has not yet been  properly fulfilled due to numerous challenges. Firstly, t he  building footprints on the GIS maps need the manual or semi  automatic procedure to reach the high precision, which is quite  timeconsuming and labour intensive. Secondly, the enormous  diversity of the outlooks of the building roofs creates barriers for  large scale building footprint extraction. Also, the geometric  potential of the satellite images has not been exploited. Due to  the pixel wised and grid based representation of the images, it‚Äôs  fairly demanding to learn the geometric information of polygon   shapes. In recent years, deep learning has brought a revolution in  Artificial Intelligence (AI).     Deep learning is largely been used in the fields of computer  vision, speech recognition, natural language processing, etc and   it can give some exceptional results as compared to many  traditional techniques in the remote sensing dom ain as well.    Motivated  by the challenges of regularized building footprint  extraction and to exploit the potential of deep learning, in this  study, we will present our model which is based on deep neural  networks. The model utilizes the spatial, semantic  and geometric  information to perform automatic building footprint extraction  and handle the problem of building boundary regularization.      Our general framework illustrated in Fig 1, takes advantage of  the typical supervised learning mechanism and can be considered  as the instance segmentation model combined with geometric  learning. Our framework can simultaneously recognize and  localize multiple objects, assign semantic labels at pixel level  and predict polygons of geometric shapes.             Figure 1. Framew ork of the Study       2. RELATED WORK   "
436,Exploiting Class Similarity for Machine Learning with Confidence Labels and Projective Loss Functions.txt,"Class labels used for machine learning are relatable to each other, with
certain class labels being more similar to each other than others (e.g. images
of cats and dogs are more similar to each other than those of cats and cars).
Such similarity among classes is often the cause of poor model performance due
to the models confusing between them. Current labeling techniques fail to
explicitly capture such similarity information. In this paper, we instead
exploit the similarity between classes by capturing the similarity information
with our novel confidence labels. Confidence labels are probabilistic labels
denoting the likelihood of similarity, or confusability, between the classes.
Often even after models are trained to differentiate between classes in the
feature space, the similar classes' latent space still remains clustered. We
view this type of clustering as valuable information and exploit it with our
novel projective loss functions. Our projective loss functions are designed to
work with confidence labels with an ability to relax the loss penalty for
errors that confuse similar classes. We use our approach to train neural
networks with noisy labels, as we believe noisy labels are partly a result of
confusability arising from class similarity. We show improved performance
compared to the use of standard loss functions. We conduct a detailed analysis
using the CIFAR-10 dataset and show our proposed methods' applicability to
larger datasets, such as ImageNet and Food-101N.","Objects are relatable to each other. A cat bears more resemblance to a dog than to a car. Such distinctions that come easily to us humans help us understand even unseen objects, but such similarities can also be the cause of con fusion. Can you distinguish rabbits from hares? Current AI systems are not immune to confusability arising from class similarities. [6] and [1] have shown that the most confusedCrossentropy Loss LogProjection Loss0% noise ratio  80% noise ratio Figure 1: The tSNE plots of trained models on CIFAR10 using Crossentropy and LogProjection loss, with 0% and 80% asym metric noise ratio and trusted set M= 1k. Although crossentropy attempts to learn feature embeddings that differentiate classes, we observe that similar classes still end up relatively closer. This suggests that AI models have a natural tendency to learn fea tures that cluster according to class similarities . A model trained instead with our proposed LogProjection loss exploits rather than Ô¨Åght this natural clustering , irrespective of the noise. classes on the ImageNet dataset follow the WordNet hierar chy, implying that class similarity is a contributing factor to poor model performance. Machine learning systems are usually not provided any supervision on the interclass similarity, as the typical la bels used for machine learning do not explicitly capture such class similarities. This leaves the models to implicitly 1arXiv:2103.13607v1  [cs.CV]  25 Mar 2021Crossentropy Loss LogProjection LossAccuracy (%) Accuracy (%) Epochs Epochs Figure 2: Image classiÔ¨Åcation results on CIFAR10 with 80% asymmetric noise ratio and trusted set M= 1k. Labels: noisy train evaluation. The model trained using conÔ¨Ådence labels and LogProjection loss autocorrects, whereas the crossentropy loss leads to model overÔ¨Åtting on the noisy training data. discern the class similarity information from the training data to distinguish between classes, leading to poor perfor mance. Several works [23, 15], have looked at this problem by treating it as a regularization issue. [23] presented soft labels, a label regularization strategy wherein a Ô¨Åxed value is assigned to all the nontarget classes, but using soft labels has been shown to actually lead to loss of class similarity information [18]. We introduce conÔ¨Ådence labels that explicitly model the interclass similarity relations and thus provide models the supervision to learn such relations. ConÔ¨Ådence labels are probabilistic labels denoting the likelihood of similarity (or confusability) among the classes. We deÔ¨Åne them as a vec tor of (real or pseudo) probabilities of each possible la bel, which is analogous to a neuralnetwork classiÔ¨Åer‚Äôs Ô¨Å nal softmax activations. In the present work, we obtain conÔ¨Ådence labels on a perclass basis, either though heuris tic measures or by using pretrained models. (Future work will explore applying unique conÔ¨Ådence labels per image or even per pixel.) ConÔ¨Ådence labels are an easy way of introducing apriori interclass similarity information into the neural network training, which, when coupled with our novel projective loss functions, encourages both preserving and learning the naturally occurring class distributions. Most of the typical machine learning objective functions try to learn features that semiequally separate all the classes into different clusters in the latent feature space, ideally pulling apart all the clusters to be orthogonal to each other. Implicit in prior approaches is the assumption that classes are not only distinguishable, but that with the right features, each class is equally distinguishable from every other class. However, these systems still end up having tight clusters for similar classes, which is suggestive of a naturallyoccurring distribution arising from interclass similarity [18], leading us to suspect that any attempt at ‚Äúovercoming‚Äù these naturally occurring distributions may be futile. We further sus pect that modifying the label space to be orthogonal to each other may be incorrect, especially when working on open set classes [2]. A model that is trained on a closed set of classes might not appropriately Ô¨Åt an unseen class into its label space when operating under the assumption that every class is distinct. To preserve the naturally occurring class similarity dis tribution, we introduce projective loss functions tailored to work with conÔ¨Ådence labels with an ability to relax the loss penalty for similar classes. Our projective loss functions preserve and reinforce the naturally occurring cluster distri butions, such that similar classes stay closer while still sep arating dissimilar classes. In contrast, typical loss functions (e.g. crossentropy) impartially force all class clusters to move apart (pushing toward orthogonality), without regard to interclass similarity. Figure1, we observe that similar classes ( e.g. cat and dog) end up being relatively closer ir respective of the label distribution trained upon. Similarly, contrastive learning approaches [10] handle interclass similarity by trying to bring the feature embed dings of sameclass instances closer while moving away from feature embeddings of different classes. This approach is predicated on the assumption of interclass dissimilarity, thus ignoring the existence of interclass similarity. We be lieve our approach is the Ô¨Årst to explicitly exploit the inter class similarity. We test our approach to modeling the label space in the challenging setting of noisy labels (mislabelled data). We show that the mere usage of our projective loss combined with our conÔ¨Ådence labels, without yet incorporating many of the other standard trainingenhancement techniques, can achieve comparable performance to the stateoftheart sys tems when dealing with high label noise stemming form in ter class similarity (i.e. asymmetric or semantic noise [13]). Contributions: Our primary novel ideas are that (A) classsimilarity information should be exploited during training, that (B) similar classes need not, and should not, be well separated in latent feature space, and that (C) noisy la bels often occur as a result of class similarity, which would lead naturally occurring label noise to be asymmetrically clustered. Our main technical contributions are (1) Introducing our novel ConÔ¨Ådence Labels as a new way of labelling data based on probabilistic labels that denote the likelihood of similarity. (2) Methods for the creation and generation of conÔ¨Ådence labels that instil class similarity prior to model training. (3) A novel family of Projective loss functions tailored to handle conÔ¨Ådence labels, which reinforce natu rally occurring class similarity distributions/relations. (4) Training mechanisms for working with conÔ¨Ådence labels and projective loss functions. Our simple, but effective, strategy explicitly exploitsclass similarity information for training more robustly with noisy labels. It achieves signiÔ¨Åcant robustness at high asym metric noise levels on CIFAR10 [11] comparable to current stateoftheart methods, while being a straightforward and simple strategy and not increasing training time as com pared to current methods. We also show improvements on largescale datasets, ImageNet [7] and Food101N [12], without yet incorporating many of the ‚Äústandard‚Äù training enhancement techniques such as AutoAugment [5] and Co sine learning rate decay with warm restarts [16]. 2. Related Work "
21,Person Re-identification with Deep Similarity-Guided Graph Neural Network.txt,"The person re-identification task requires to robustly estimate visual
similarities between person images. However, existing person re-identification
models mostly estimate the similarities of different image pairs of probe and
gallery images independently while ignores the relationship information between
different probe-gallery pairs. As a result, the similarity estimation of some
hard samples might not be accurate. In this paper, we propose a novel deep
learning framework, named Similarity-Guided Graph Neural Network (SGGNN) to
overcome such limitations. Given a probe image and several gallery images,
SGGNN creates a graph to represent the pairwise relationships between
probe-gallery pairs (nodes) and utilizes such relationships to update the
probe-gallery relation features in an end-to-end manner. Accurate similarity
estimation can be achieved by using such updated probe-gallery relation
features for prediction. The input features for nodes on the graph are the
relation features of different probe-gallery image pairs. The probe-gallery
relation feature updating is then performed by the messages passing in SGGNN,
which takes other nodes' information into account for similarity estimation.
Different from conventional GNN approaches, SGGNN learns the edge weights with
rich labels of gallery instance pairs directly, which provides relation fusion
more precise information. The effectiveness of our proposed method is validated
on three public person re-identification datasets.","Person reidentication is a challenging problem, which aims at nding the per son images of interest in a set of images across dierent cameras. It plays a signicant role in the intelligent surveillance systems. ?Hongsheng Li is the corresponding author.arXiv:1807.09975v1  [cs.CV]  26 Jul 20182 Y. Shen, H. Li, S. Yi, D. Chen and X. Wang ProbeProbeGallery 1ProbeGallery 2ProbeGallery 3ProbeGallery 4Gallery 1Gallery 2Gallery 4Gallery 3 SiameseCNN		""#		""$		""%		""&Similarity Estimator		'#		'$		'%		'& (a) Conventional Approach. ProbeGallery 1ProbeGallery 1Gallery 2Gallery 4Gallery 3 SiameseCNN		""#	""$		""%		""&Similarity Estimator		'#		'$		'%		'&ProbeGallery 2ProbeGallery 3ProbeGallery 4Graph edge (b) Our proposed SGGNN. Fig. 1. Illustration of our Proposed SGGNN method and conventional person re identication approach. (a) The pipeline of conventional person reidentication ap proach, the pairwise relationships between dierent probegallery pairs are ignored. The similarity score of each probegallery pair di(i= 1;2;3;4) is estimated individ ually. (b) Our proposed SGGNN approach, pairwise relationships between dierent probegallery pairs are involved with deeply learned message passing on a graph for more accurate similarity estimation. To enhance the reidentication performance, most existing approaches at tempt to learn discriminative features or design various metric distances for better measuring the similarities between person image pairs. In recent years, witness the success of deep learning based approaches for various tasks of com puter vision [25,17,51,62,59,12,39,63,67,31,20], a large number of deep learning methods were proposed for person reidentication [37,81,64,40]. Most of these deep learning based approaches utilized Convolutional Neural Network (CNN) to learn robust and discriminative features. In the mean time, metric learning methods were also proposed [4,3,72] to generate relatively small feature distances between images of same identity and large feature distances between those of dierent identities. However, most of these approaches only consider the pairwise similarity while ignore the internal similarities among the images of the whole set. For instance, when we attempt to estimate the similarity score between a probe image and a gallery image, most feature learning and metric learning approaches only con sider the pairwise relationship between this single probegallery image pair in both training and testing stages. Other relations among dierent pairs of images are ignored. As a result, some hard positive or hard negative pairs are dicult to obtain proper similarity scores since only limited relationship information among samples is utilized for similarity estimation.Person ReID with Deep SimilarityGuided Graph Neural Network 3 To overcome such limitation, we need to discover the valuable internal simi larities among the image set, especially for the similarities among the gallery set. One possible solution is utilizing manifold learning [2,42], which considers the similarities of each pair of images in the set. It maps images into a manifold with more smooth local geometry. Beyond the manifold learning methods, reranking approaches [78,16,70] were also utilized for rening the ranking result by inte grating similarities between topranked gallery images. However, both manifold learning and reranking approaches have two major limitations: (1) most mani fold learning and reranking approaches are unsupervised, which could not fully exploit the provided training data label into the learning process. (2) These two kinds of approaches could not benet feature learning since they are not involved in training process. Recently, Graph Neural Network (GNN) [6,18,23,45] draws increasing at tention due to its ability of generalizing neural networks for data with graph structures. The GNN propagates messages on a graph structure. After mes sage traversal on the graph, node's nal representations are obtained from its own as well as other node's information, and are then utilized for node classi cation. GNN has achieved huge success in many research elds, such as text classication [13], image classication [6,46], and human action recognition [66]. Compared with manifold learning and reranking, GNN incorporates graph com putation into the neural networks learning, which makes the training endtoend and benets learning the feature representation. In this paper, we propose a novel deep learning framework for person re identication, named SimilarityGuided Graph Neural Network (SGGNN). SG GNN incorporates graph computation in both training and testing stages of deep networks for obtaining robust similarity estimations and discriminative feature representations. Given a minibatch consisting of several probe images and gallery images, SGGNN will rst learn initial visual features for each image (e.g., global average pooled features from ResNet50 [17].) with the pairwise re lation supervisions. After that, each pair of probegallery images will be treated as a node on the graph, which is responsible for generating similarity score of this pair. To fully utilize pairwise relations between other pairs (nodes) of im ages, deeply learned messages are propagated among nodes to update and rene the pairwise relation features associated with each node. Unlike most previous GNNs' designs, in SGGNN, the weights for feature fusion are determined by sim ilarity scores by gallery image pairs, which are directly supervised by training labels. With these similarity guided feature fusion weights, SGGNN will fully exploit the valuable label information to generate discriminative person image features and obtain robust similarity estimations for probegallery image pairs. The main contribution of this paper is twofold. (1) We propose a novel Similarity Guided Graph Neural Network (SGGNN) for person reidentication, which could be trained endtoend. Unlike most existing methods, which uti lize intergalleryimage relations between samples in the postprocessing stage, SGGNN incorporates the intergalleryimage relations in the training stage to enhance feature learning process. As a result, more discriminative and accurate4 Y. Shen, H. Li, S. Yi, D. Chen and X. Wang person image feature representations could be learned. (2) Dierent from most Graph Neural Network (GNN) approaches, SGGNN exploits the training label supervision for learning more accurate feature fusion weights for updating the nodes' features. This similarity guided manner ensures the feature fusion weights to be more precise and conduct more reasonable feature fusion. The eective ness of our proposed method is veried by extensive experiments on three large person reidentication datasets. 2 Related Work "
255,Learning from Noisy Labels with Decoupled Meta Label Purifier.txt,"Training deep neural networks(DNN) with noisy labels is challenging since DNN
can easily memorize inaccurate labels, leading to poor generalization ability.
Recently, the meta-learning based label correction strategy is widely adopted
to tackle this problem via identifying and correcting potential noisy labels
with the help of a small set of clean validation data. Although training with
purified labels can effectively improve performance, solving the meta-learning
problem inevitably involves a nested loop of bi-level optimization between
model weights and hyper-parameters (i.e., label distribution). As compromise,
previous methods resort to a coupled learning process with alternating update.
In this paper, we empirically find such simultaneous optimization over both
model weights and label distribution can not achieve an optimal routine,
consequently limiting the representation ability of backbone and accuracy of
corrected labels. From this observation, a novel multi-stage label purifier
named DMLP is proposed. DMLP decouples the label correction process into
label-free representation learning and a simple meta label purifier. In this
way, DMLP can focus on extracting discriminative feature and label correction
in two distinctive stages. DMLP is a plug-and-play label purifier, the purified
labels can be directly reused in naive end-to-end network retraining or other
robust learning methods, where state-of-the-art results are obtained on several
synthetic and real-world noisy datasets, especially under high noise levels.","Deep learning has achieved signiÔ¨Åcant progress on vari ous recognition tasks. The key to its success is the availabil Yuanpeng Tu, Boshen Zhang, Yuxi Li contribute equally to this work. Fully  supervisionùë•ùë°Update weights:ùë§ùë°‚Ä≤ ùêøùë£ùëéùëôSelf supervision ùë§ùë°‚àóCoupled  optimization loop Decouplingùêøùë£ùëéùëô (a) Traditional (b) DMLPùë¶ùë° ùë•ùë° ùë¶ùë°ùë•ùë£ùë¶ùë£ ùë•ùë£ùë¶ùë£Updated label :ùë¶ùë°‚Ä≤ ùë¶ùë°‚àóùë¶ùë°‚àó Fixed weightsUpdate labe lFigure 1. (a) Traditional coupled alternating update to solve meta label puriÔ¨Åcation problem, and (b) the proposed DMLP method that decouples the label puriÔ¨Åcation process into representation learning and a simple nonnested meta label puriÔ¨Åer. ity of largescale datasets with reliable annotations. Collect ing such datasets, however, is timeconsuming and expensive. Easy ways to obtain labeled data, such as web crawling [31], inevitably yield samples with noisy labels, which is not appropriate to be directly utilized to train DNN since these complex models are vulnerable to memorize noisy labels [2]. Towards this problem, numerous Learning with Noisy Label (LNL) approaches were proposed. Classical LNL methods focus on identifying the noisy samples and reduc ing their effect on parameter updates by abandoning [12] or assigning smaller importance. However, when it comes to extremely noisy and complex scenarios, such scheme struggles since there is no sufÔ¨Åcient clean data to train a dis criminative classiÔ¨Åer. Therefore, label correction approaches are proposed to augment clean training samples by revising noisy labels to underlying correct ones. Among them, meta learning based approaches [9, 16, 25] achieve stateoftheart performance via resorting to a small clean validation setarXiv:2302.06810v3  [cs.CV]  17 Feb 2023and taking noisy labels as hyperparameters, which provides sound guidance toward underlying label distribution of clean samples. However, such meta puriÔ¨Åcation inevitably in volves a nested bilevel optimization problem on both model weight and hyperparameters (shown as Fig. 1 (a)), which is computationally infeasible. As a compromise, the alternat ing update between model weights and hyperparameters is adopted to optimize the objective [9, 16, 25], resulting in a coupled solution for both representation learning and label puriÔ¨Åcation. Empirical observation. Intuitively, alternate optimiza tion over a large search space (model weight and hyper parameters) may lead to suboptimal solutions. To investi gate how such approximation affects results in robust learn ing, we conduct empirical analysis on CIFAR10 [14] with recent label puriÔ¨Åcation methods MLC [40] and MSLC [9], which consist of a deep model and a meta label correction network, and make observation as Fig. 2. Coupled optimization hinders quality of corrected la bels. We Ô¨Årst compare the Coupled meta corrector MLC with its extremely Decoupled variant where the model weights are Ô¨Årst optimized for 70epochs with noisy labels and get Ô¨Åxed, then labels are puriÔ¨Åed with the guidance of validation set. We adopt the accuracy of corrected label to measure the performance of puriÔ¨Åcation. From Fig. 2 (a), we can clearly observe that compared with Decoupled counterpart, joint optimization yields inferior correction per formance, and these miscorrection will reversely affect the representation learning in coupled optimization. Coupled optimization hinders representation ability. We investigate the representation quality by evaluating the linear prob accuracy [6] of extracted feature in Fig. 2 (b). We Ô¨Ånd the representation quality of Coupled training is much worse at the beginning, which leads to slow and un stable representation learning in the later stage. To further investigate the effect on representation learning, we also resort to a well pretrained backbone with selfsupervised learning [5] as initialization, recent research [39] shows pre trained representation is substantially helpful for LNL frame work. However, we Ô¨Ånd this conclusion does not strictly hold for coupled meta label correctors. As shown in Fig. 2 (c), by comparing the classiÔ¨Åcation accuracy from classi Ô¨Åer of MLC/MSLC, we observe the pretrained model only brings marginal improvement if model weights is still cou pled with hyperparameters. In contrast, when the weight of backbone is Ô¨Åxed and decoupled from the label puriÔ¨Åcation and classiÔ¨Åer, the improvement becomes more signiÔ¨Åcant. Decoupled Meta PuriÔ¨Åcation. From the observation above, we Ô¨Ånd the decoupling between model weights and hyperparameters of meta correctors is essential to label ac curacy and Ô¨Ånal results. Therefore, in this paper, we aim at detaching the meta label puriÔ¨Åcation from representation learning and designing a simple meta label puriÔ¨Åer whichis more friendly to optimization of label distribution prob lem than existing complex meta networks [9, 40]. Hence we propose a general multistage label correction strategy, named Decoupled Meta Label PuriÔ¨Åer (DMLP). The core of DMLP is a metalearning based label puriÔ¨Åer, however, to avoid solving the bilevel optimization with a coupled solution, DMLP decouples this process into selfsupervised representation learning and a linear metalearner to Ô¨Åt un derlying correct label distribution (illustrated as Fig. 1 (b)), thus simpliÔ¨Åes the label puriÔ¨Åcation stage as a singlelevel optimization problem. The simple metalearner is carefully designed with two mutually reinforcing correcting processes, named intrinsic primary correction (IPC) and extrinsic aux iliary correction (EAC) respectively. IPC plays the role of purifying labels in a global sense at a steady pace, while EAC targets at accelerating the puriÔ¨Åcation process via looking ahead (i.e., training with) the updated labels from IPC. The two processes can enhance the ability of each other and form a positive loop of label correction. Our DMLP framework is Ô¨Çexible for application, the puriÔ¨Åed labels can either be directly applied for naive endtoend network retraining, or exploited to boost the performance of existing LNL frame works. Extensive experiments conducted on mainstream benchmarks, including synthetic (noisy versions of CIFAR) and realworld (Clothing1M) datasets, demonstrate the supe riority of DMLP. In a nutshell, the key contributions of this paper include: We analyze the necessity of decoupled optimization for label correction in robust learning, based on which we propose DMLP, a Ô¨Çexible and novel multistage label puriÔ¨Åer that solves bilevel metalearning problem with a decoupled manner, which consists of representation learning and non nested meta label puriÔ¨Åcation; In DMLP, a novel nonnested meta label puriÔ¨Åer equipped with two correctors, IPC and EAC is proposed. IPC is a global and steady corrector, while EAC accelerates the correction process via training with the updated labels from IPC. The two processes form a positive training loop to learn more accurate label distribution; Deep models trained with puriÔ¨Åed labels from DMLP achieve stateoftheart results on several synthetic and real world noisy datasets across various types and levels of label noise, especially under high noise levels. Extensive ablation studies are provided to verify the effectiveness. 2. Related Works "
51,Combining Deep Learning and String Kernels for the Localization of Swiss German Tweets.txt,"In this work, we introduce the methods proposed by the UnibucKernel team in
solving the Social Media Variety Geolocation task featured in the 2020 VarDial
Evaluation Campaign. We address only the second subtask, which targets a data
set composed of nearly 30 thousand Swiss German Jodels. The dialect
identification task is about accurately predicting the latitude and longitude
of test samples. We frame the task as a double regression problem, employing a
variety of machine learning approaches to predict both latitude and longitude.
From simple models for regression, such as Support Vector Regression, to deep
neural networks, such as Long Short-Term Memory networks and character-level
convolutional neural networks, and, finally, to ensemble models based on
meta-learners, such as XGBoost, our interest is focused on approaching the
problem from a few different perspectives, in an attempt to minimize the
prediction error. With the same goal in mind, we also considered many types of
features, from high-level features, such as BERT embeddings, to low-level
features, such as characters n-grams, which are known to provide good results
in dialect identification. Our empirical results indicate that the handcrafted
model based on string kernels outperforms the deep learning approaches.
Nevertheless, our best performance is given by the ensemble model that combines
both handcrafted and deep learning models.","The organizers of the 2020 VarDial Evaluation Campaign (G Àòaman et al., 2020) proposed a shared task targeted towards the geolocation of short texts, e.g. tweets, namely the Social Media Variety Geoloca tion (SMG) task. Typically formulated as a double regression problem, the task is about predicting the location, expressed in latitude and longitude, from where the text received as input was posted on a cer tain social media platform. Twitter and Jodel are the platforms used for data collection, divided by the language area in three subtasks, namely: ‚Ä¢ Standard German Jodels (DEAT)  formed of conversations initiated in Germany and Austria in regional dialectal forms (Hovy and Purschke, 2018). ‚Ä¢ Swiss German Jodels (CH)  based on a smaller number of Jodel conversations from Switzerland (Hovy and Purschke, 2018). ‚Ä¢ BCMS Tweets  from the area of Bosnia and Herzegovina, Croatia, Montenegro and Serbia where the macrolanguage used is BCMS, with both similarities and a fair share of variation among the component languages (Ljube Àási¬¥c et al., 2016). In this paper, we focus only on the second subtask, SMGCH, proposing a variety of handcrafted and deep learning models, as well as an ensemble model that combines all our previous models through metalearning. Our Ô¨Årst model is a Support Vector Regression (SVR) classiÔ¨Åer (Chang and Lin, 2002) based on string kernels, which are known to perform well in other dialect identiÔ¨Åcation tasks (Butnaru and Ionescu, 2018b; Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). Our second model is a characterlevel convolutional neural network (CNN) (Zhang et al., 2015), which is also known to providearXiv:2010.03614v1  [cs.CL]  7 Oct 2020good results in dialect identiÔ¨Åcation (Butnaru and Ionescu, 2019; Tudoreanu, 2019). Due to the high pop ularity and the outstanding results of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) in solving mainstream NLP tasks, we decided to try out a Long ShortTerm Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) based on German BERT embeddings as our third model. Lastly, we combine our three models into an ensemble that employs Extreme Gradient Boosting (XGBoost) (Chen and Guestrin, 2016) as metalearner. We conducted experiments on the development set provided by the organizers, in order to decide which models to choose for our three submissions for the SMGCH subtask. Our results indicate that the ensemble model attains the best results. Perhaps surprisingly, our shallow approach based on string kernels outperforms both deep learning models. Our observations are consistent across the development and the test sets provided by the organizers. The rest of this paper is organized as follows. We present related work on dialect identiÔ¨Åcation and geolocation of short texts in Section 2. Our approaches are described in more detail in Section 3. We present the experiments and empirical results in Section 4. Finally, our conclusions are drawn in Sec tion 5. 2 Related Work "
337,The DKU-DukeECE System for the Self-Supervision Speaker Verification Task of the 2021 VoxCeleb Speaker Recognition Challenge.txt,"This report describes the submission of the DKU-DukeECE team to the
self-supervision speaker verification task of the 2021 VoxCeleb Speaker
Recognition Challenge (VoxSRC). Our method employs an iterative labeling
framework to learn self-supervised speaker representation based on a deep
neural network (DNN). The framework starts with training a self-supervision
speaker embedding network by maximizing agreement between different segments
within an utterance via a contrastive loss. Taking advantage of DNN's ability
to learn from data with label noise, we propose to cluster the speaker
embedding obtained from the previous speaker network and use the subsequent
class assignments as pseudo labels to train a new DNN. Moreover, we iteratively
train the speaker network with pseudo labels generated from the previous step
to bootstrap the discriminative power of a DNN. Also, visual modal data is
incorporated in this self-labeling framework. The visual pseudo label and the
audio pseudo label are fused with a cluster ensemble algorithm to generate a
robust supervisory signal for representation learning. Our submission achieves
an equal error rate (EER) of 5.58% and 5.59% on the challenge development and
test set, respectively.","This report describes the submission of the DKUDukeECE team to the selfsupervision speaker veriÔ¨Åcation task of the 2021 V oxCeleb Speaker Recognition Challenge (V oxSRC). In our previous work on selfsupervised speaker represen tation learning [1], we proposed a twostage iterative labeling framework. In the Ô¨Årst stage, contrastive selfsupervised learn ing is used to pretraining the speaker embedding network. This allows the network to learn a meaningful feature representa tion for the Ô¨Årst round of clustering instead of random initial ization. In the second stage, a clustering algorithm iteratively generates pseudo labels of the training data with the learned representation, and the network is trained with these labels in a supervised manner. The clustering algorithm can discover the intrinsic structure of the representation of the unlabeled data, providing meaningful supervisory signals comparing to con trastive learning which draws negative samples uniformly from the training data without label information. The idea behind the proposed framework is to take advantage of the DNN‚Äôs ability to learn from data with label noise and bootstrap its discrimina tive power. In this work, we extend this iterative labeling framework to multimodal audiovisual data, considering that complemen tary information from different modalities can help the cluster ing algorithm generate more meaningful supervisory signals.SpeciÔ¨Åcally, we train a visual representation network to en code face information using the pseudo labels generated by au dio data. With the resulted visual representations, clustering is performed to generate pseudo labels for visual data. Then, we employ a cluster ensemble algorithm to fuse pseudolabels gen erated by different modalities. This fused pseudolabel is then used to train speaker and face representation networks. With the clustering ensemble algorithm, information in one modal ity can Ô¨Çow to the other modality, providing more robust and faulttolerant supervisory signals. 2. Methods "
459,Prediction of speech intelligibility with DNN-based performance measures.txt,"This paper presents a speech intelligibility model based on automatic speech
recognition (ASR), combining phoneme probabilities from deep neural networks
(DNN) and a performance measure that estimates the word error rate from these
probabilities. This model does not require the clean speech reference nor the
word labels during testing as the ASR decoding step, which finds the most
likely sequence of words given phoneme posterior probabilities, is omitted. The
model is evaluated via the root-mean-squared error between the predicted and
observed speech reception thresholds from eight normal-hearing listeners. The
recognition task consists of identifying noisy words from a German matrix
sentence test. The speech material was mixed with eight noise maskers covering
different modulation types, from speech-shaped stationary noise to a
single-talker masker. The prediction performance is compared to five
established models and an ASR-model using word labels. Two combinations of
features and networks were tested. Both include temporal information either at
the feature level (amplitude modulation filterbanks and a feed-forward network)
or captured by the architecture (mel-spectrograms and a time-delay deep neural
network, TDNN). The TDNN model is on par with the DNN while reducing the number
of parameters by a factor of 37; this optimization allows parallel streams on
dedicated hearing aid hardware as a forward-pass can be computed within the
10ms of each frame. The proposed model performs almost as well as the
label-based model and produces more accurate predictions than the baseline
models.","The intelligibility of speech is crucial for our social interaction, and it is an essential measure for a diagnosis of hearing decits through speech audiometry and the optimization of speech enhancement algorithms in hearing aids or cochlear implants. Accurate models that predict speech intelligibility (SI) in the presence of dierent masking noises are desirable since they can quantify the outcome of such optimization and, therefore, reduce the requirement of SI measurements that are usually timeconsuming and costly. Several models for SI prediction have been proposed to include the signal processing strategies of the auditory system. A classic model is the speech intelligibility index (SII), which is based on the weighted signaltonoise ratio in separate frequency bands that are combined to estimate SI for longer speech segments (e.g., an utterance) (ANSI, 1997). The extended SII (ESII) (Rhebergen and Versfeld, 2005) uses the same frequency weighting and covers a shorttime analysis of SNRs in each channel, and can take into account the eect of temporal modulations to enable listeninginthedips. The shorttime objective intelligibility (STOI, Taal et al., 2011) quanties the degradation of speech signals by comparing shorttime segments of the original clean speech signal with its degraded counterpart, calculating the correlation of these segments in frequency bands. Ewert and Dau (2000) proposed to include explicit temporal modulation ltering for separate frequency channels, an approach later rened by Jrgensen et al. (2013) by using temporal segments with a variable length that depend on the modulation lter frequency, which resulted in the multi resolution speech envelope power spectrum model (mrsEPSM). Schubotz et al. (2016) compared these models in a study to determine how well they can predict the speech reception threshold (SRT), which is the signaltonoise ratio (SNR) at which 50% of words presented in an acousticallycontrolled environment are correctly recognized. In this com parison, mrsEPSM produced the best average predictions across all maskers 2that varied from stationary speechshaped noise to a single background talker in the timefrequency domain. Additionally, this benchmark provides evidence about the in uence of amplitude modulation, shortterm energy, and informational masking in speech intelligibility. Another model that uses auditory periphery modeling to predict the SI is the hearing aid speech perception index (HASPI, Kates and Arehart, 2014). It calculates the envelope and the temporal ne structure of the noisy speech signal and the clean speech reference. The coherence between the test and reference signals and the correlation of their envelopes results in the HASPI. This index can predict the SI of normalhearing listeners and hearingimpaired listeners by including hearing loss into the auditory model; however, the clean speech material is required for the prediction. An alternative modeling approach combines signal extraction based on auditory principles with pattern matching algorithms borrowed from automatic speech recognition (ASR). Barker and Cooke (2006) introduced a glimpsing model in which the abovethreshold timefrequency patches (glimpses) were used as features for a backend that combines a Gaussian mixture model (GMM) with a hidden Markov model (HMM) to produce a transcript from the input glimpses, which was compared to listener responses. Recent work by Tang et al. (2016) introduced the binaural distortion weighted glimpse proportion metric, building upon the previous glimpsing models of monaural signals ( betterear glimpsing ) and incorporated binaural masking level dierences. J urgens and Brand (2009) combined the output of a model of the internal representation of auditory perception (PEMO, Dau et al., 1997b) with the dynamic time warping algorithm, which produced accurate predictions of phoneme recognition in normalhearing listeners. A GMMHMM approach dubbed Framework for Acoustic Discrimination Experiments (FADE) was proposed by Sch adler et al. (2015). This model produces accurate SRT estimates by retraining a GMMHMM system at dierent SNRs and by selecting the model that produces the lowest SRT when using the same training and test sentences. All previously mentioned models either require separate clean and degraded speech (as in the case of STOI and HASPI) or separate speech and noise signals, either for calculating the frequencydependent SNR as for the SII, by identifying abovenoise speech glimpses as in the model by Barker and Cooke (2006), or by using identical underlying speech utterances for training and testing (FADE, Sch adler et al., 2015). 3The main focus of ASR is to nd the most probable uttered word sequence given the acoustic input. This problem can be expressed probabilistically as nding the candidate word sequence Wwith the highest probability given a set of features Xfrom the observed audio signal. ^W= argmax wP(WjX) (1) Conventionally, the posterior probability is factorized as a product of an acoustic likelihood P(XjW) and a prior model of word sequences P(W) (language model ). The acoustic likelihood can be obtained by multiplying the conditional distributions of acoustic features ( acoustic model ) with those of a phonetic sequence given a word sequence ( lexicon or pronunciation model ). In simpler terms, the acoustic model predicts the probability of the speech sound given a word sequence, and the language model computes the likelihood of that word sequence. Motivated by the success of deep learning in ASR (Hinton et al., 2012), Spille et al. (2018) proposed an ASR model that combines a deep neural network (DNN) trained to estimate isolated phoneme probabilities given the acoustic observation with an HMM to consider transition probabilities between phonemes. The predictive power of this model exceeded the four baseline models mentioned above on the dataset collected by Schubotz et al. (2016). The root meansquare error (RMSE) between measurement and prediction was 1.8 dB on average when using multicondition training and modulation features, which was considerably better than the baseline models RMSEs between 5.6 and 12.5 dB. The training and test sets applied to the model are speakerindependent, marking a step towards referencefree SI models, which could serve in assisted hearing. A use case for such a model is the constant monitoring of speech intelligibility in the current acoustic scene and identifying the optimal speech enhancement algorithm for that scene. However, the previous model uses identical noise signals for training and testing (the eect of disjunct training/testing noises in this model has not yet been explored), and it requires the correct labels of the words in the utterance used as model input. These labels are compared to the transcript produced by the ASR system from which the recognition accuracy is calculated. For online applications of speech intelligibility models, e.g., for realtime comparison and selection of hearing aid and other signal processing algorithms, both the 4identical noises and the label requirement are essential limitations. Further, it is unclear if the computational demands of the model are compatible with mobile listening devices. This paper introduces an SI prediction model that requires neither the speech reference nor the actual labels of the tested utterances. The model follows the DNNbased approach introduced by Spille et al. (2018). The resulting model is not blind concerning the noise signals, for it follows the training/testing procedure suggested by Spille et al. (2018), using identical noise signals. The primary enhancement is that we provide a method for estimating the word error rate (WER) directly from the phoneme posterior probabilities emitted by the DNN instead of computing the WER between the reference transcript and the one decoded by the HMM. The WER estimation algorithm is referred to as mean temporal distance (MTD) and was rst proposed for estimating error rates of automatic recognizers by analyzing the mean distance of phoneme probability vectors obtained from a neural network over a time interval (Hermansky et al., 2013). Intuitively, the higher the MTD value, the more representative the dierence between phonemes contained in the measured time window. In preliminary work, we studied whether MTD was applicable for estimating the WER in SI models, focusing on only four types of masker and one network architecture with comparably high computational demands (Castro Martinez et al., 2020). When considering applications for modelsintheloop in hearing aids, the computational complexity becomes quite relevant. In a previous study, we were able to show that fully connected feedforward DNNs can be used in combination with a hearing aid coprocessor (Castro Martinez et al., 2019). However, the approach is currently limited to run single models in constrained mobile systems due to the computational complexity of these network architectures. This restriction prevents a simultaneous evaluation of two or more processing strategies (e.g., signal enhancement algorithms in the hearing aid) to select the currently best enhancement. We also investigate whether the computational complexity can be reduced by replacing the fullyconnected network with a timedelay deep neural network (TDNN), recently introduced for ecient ASR (Peddinti et al., 2015). This network architecture models temporal context by introducing input dependencies from previous layers ( delays ); because the input is a concatenated sequence of time frames, these delays can be seen as the input at dierent points in time. TDNNs are particularly interesting when modeling speech intelligibility 5as they cover a broader temporal context than traditional fully connected DNNs. Our previous model included temporal context on feature level using amplitude modulation lters (Moritz et al., 2015) and at the input layer by concatenating a given number of frames before and after the current one, thus requiring additional computational resources. Given the number of concatenated features and the input layer size, the overall number of parameters in the network exploded. We explore whether TDNNs can exploit temporal dependencies in the acoustic model more eciently by taking just some specic context frames instead of all of them. Previous research has extensively compared TDNNs with other architec tures for acoustic modeling, including DNNs (Peddinti et al., 2015), CNN embeddings (Rownicka et al., 2018), LSTM and similar architectures that implicitly capture temporal dependencies (Huang et al., 2019), and several other architectures using the same framework as our studies (Georgescu et al., 2019). Moreover, feature engineering combined with DNNs has been the subject of our previous studies (Castro Martinez et al., 2014; Castro Martinez and Sch adler, 2016; Castro Martinez et al., 2019). Phoneme posteriorgram Estimated WER + Matrix sentencesMaskingnoisesNormalhearing listenersTrue SRTEstimatedSRT S1S2S3Hidden Markov model TranscriptWord labelWERModulationfeaturesFully connectedDNNSpectralfeaturesTime delayneural networkMean temporal distance AEFGHBD AGHSpeech CorporaC Figure 1: Building blocks of the modeling approach: Speech intelligibility in noisy sentences is compared to the estimated SRT. To obtain this estimate, a DNN is trained as part of a standard ASR system and subsequently used to measure the degradation of phoneme representations in noise using a performance measure. From this, the WER of the ASR system is estimated, resulting in the predicted SRT. Blue letters link to the corresponding section of the paper. In summary, the main goals of this paper are to explore ASRbased models of speech intelligibility without the need for transcripts or speech reference in a wide range of maskers and to contrast traditional DNNs with more ecient TDNNs for computational eciency. 62. Methods "
205,NRGNN: Learning a Label Noise-Resistant Graph Neural Network on Sparsely and Noisily Labeled Graphs.txt,"Graph Neural Networks (GNNs) have achieved promising results for
semi-supervised learning tasks on graphs such as node classification. Despite
the great success of GNNs, many real-world graphs are often sparsely and
noisily labeled, which could significantly degrade the performance of GNNs, as
the noisy information could propagate to unlabeled nodes via graph structure.
Thus, it is important to develop a label noise-resistant GNN for
semi-supervised node classification. Though extensive studies have been
conducted to learn neural networks with noisy labels, they mostly focus on
independent and identically distributed data and assume a large number of noisy
labels are available, which are not directly applicable for GNNs. Thus, we
investigate a novel problem of learning a robust GNN with noisy and limited
labels. To alleviate the negative effects of label noise, we propose to link
the unlabeled nodes with labeled nodes of high feature similarity to bring more
clean label information. Furthermore, accurate pseudo labels could be obtained
by this strategy to provide more supervision and further reduce the effects of
label noise. Our theoretical and empirical analysis verify the effectiveness of
these two strategies under mild conditions. Extensive experiments on real-world
datasets demonstrate the effectiveness of the proposed method in learning a
robust GNN with noisy and limited labels.","Graph structured data is very pervasive in realworld, such as social networks [ 9], financial transaction networks [ 37] and traffic net works [ 40]. Graph Neural Networks (GNNs) have shown great abil ity in modeling graph structured data and are attracting increasing attention [ 1,9,15,38]. Generally, GNNs adopt the messagepassing process to update node representations by aggregating the informa tion from their neighbors [ 15,36]. One of the most important and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ‚Äô21, August 14‚Äì18, 2021, Virtual Event, Singapore ¬©2021 Association for Computing Machinery. ACM ISBN 9781450383325/21/08. . . $15.00 https://doi.org/10.1145/3447548.3467364popular tasks that benefits from this messagepassing mechanism is node classification in a semisupervised manner. With this mech anism, labeled nodes can propagate their information to unlabeled nodes [9, 35], thus resulting in superior performance of GNNs. Despite the great performance of GNNs for semisupervised node classification, the majority of existing methods assume the training labels are clean; while for many realworld graphs and applications, the collected labels could be noisy and limited. For instance, for the geolocation prediction in social networks, only a small portion of users will fill in the geolocation; and the provided locations can be noisy because users randomly fill in wrong locations to protect their privacy or users have moved to new locations but forget to update them in social networks [ 21]. Similarly, for bot detection in social media, the labeling process can be tedious, costly, and errorprone, which can end up with limited noisily labeled nodes [17]. The graph with noisy and limited labels could significantly de grade the performance of GNNs for semisupervised node classi fication. First, recent work has shown that neural networks will overfit to the noisy labels and results in poor generalization perfor mance [ 31,42]. As a generalization of neural networks for graphs, GNNs are also likely to have poor performance trained on noisy labels. Second , for graphs, the noisy information can propagate through the network topology. Falsely labeled nodes will nega tively affect their unlabeled neighbors. Since the graph is sparsely labeled, neighbors of falsely labeled nodes are unlikely to accept the information from nodes with true labels to correct the repre sentations. In addition, many unlabeled nodes will only be able to aggregate information from unlabeled nodes when the labels are limited. Thus, the performance of GNNs trained on noisily and sparsely labeled graph would be poor. Though extensive approaches have been proposed for learning with noisy labels such as loss correction [ 7,31] and sample selec tion [ 10,13,19,23,41], they are not directly applicable for learning GNNs with limited noisy labels. First, generally, these methods as sume a large amount of noisy labels are available for learning noise distribution or for sampling correct labels. They are challenged by the small label size. Second , the majority of existing work for noisy labels [ 10,19,23,31] focus on independent and identically distributed (i.i.d) data such as images, which cannot handle the information propagation of noisy labels on graphs. The work on learning a robust GNN with noisy and limited labels is rather lim ited [ 8,43]. Therefore, it is important to develop a robust GNN that could deal with noisy and limited labels. Since the labeled nodes can propagate its information to the un labeled nodes, it is promising to correct the predictions of unlabeled nodes affected by falsely labeled nodes by linking them with nodes of clean labels. However, in practice, we do not know which labels are clean. Alternatively, for an unlabeled node ùë£ùëñ, we propose toarXiv:2106.04714v1  [cs.LG]  8 Jun 2021linkùë£ùëñwith labeled nodes of high feature similarity with ùë£ùëñto make it robust to label noise and facilitate the message passing of GNNs. The basic idea is if two nodes have high feature similarity, they are more likely to have the same label. Thus, if the probability that labeled nodes having correct labels is higher than that of having incorrect labels, by connecting ùë£ùëñwith more labeled nodes of high feature similarity with ùë£ùëñ, we can potentially bring more correct label information to ùë£ùëñ. Our theoretical and empirical analysis in Sec 3.4 verify the effectiveness of linking unlabeled nodes with noisily labeled nodes under mild conditions. In addition, with this strategy, we can first train a classifier to obtain accurate pseudo labels to ease the problem of learning with noisy and limited labels. By extending the label set with pseudo labels, more supervision could be utilized to make predictions for unlabeled nodes. Link ing unlabeled nodes with similar nodes of accurate pseudo labels could further reduce the issue of label noise, which is verified in Sec 3.5. Though promising, there are no existing work exploring these strategies for learning GNNs with noisy and limited labels. Therefore, in this paper, we investigate a novel problem of learn ingNoise Resistant GNNs on sparsely and noisily labeled graphs. In essence, we are faced with two challenges: (i) How to effectively link unlabeled nodes with labeled nodes to alleviate the effects of label noise and benefit the prediction? (ii) Given the graph with noisy and limited labels, how can we obtain accurate pseudo labels? To solve these challenges, we proposed a novel framework named noiseresistant GNN (NRGNN)1. NRGNN adopts a GNNbased edge predictor to predict edges to benefit the classification on graphs with noisy and limited labels. Since the existing edges in the graph generally link nodes in similar attributes [ 24], these edges could provide supervision to train a good edge predictor. The graph den sified by linking unlabeled nodes with similar noisily labeled nodes is utilized to obtain accurate pseudo labels, which extends the label set to provide more supervision for node classification. NRGNN also adopts the edge predictor to link unlabeled with similar ex tended labeled nodes to further reduce the effects of label noise. In summary, our main contributions are: ‚Ä¢We investigate a novel problem of learning noiseresistant GNNs on graphs with noisy and limited labels; ‚Ä¢We propose a new framework which can generate accurate pseudo labels and assign highquality edges between unlabeled nodes and (pseudo) labeled nodes to alleviate label noise issue; ‚Ä¢Theoretical and empirical analysis are conducted to verify the effectiveness of the proposed strategies against label noise; ‚Ä¢Extensive experiments on realworld datasets demonstrate the effectiveness of the proposed NRGNN in node classification on graphs with noisy and limited labels. 2 RELATED WORK "
362,Decoupled Multi-task Learning with Cyclical Self-Regulation for Face Parsing.txt,"This paper probes intrinsic factors behind typical failure cases (e.g.
spatial inconsistency and boundary confusion) produced by the existing
state-of-the-art method in face parsing. To tackle these problems, we propose a
novel Decoupled Multi-task Learning with Cyclical Self-Regulation (DML-CSR) for
face parsing. Specifically, DML-CSR designs a multi-task model which comprises
face parsing, binary edge, and category edge detection. These tasks only share
low-level encoder weights without high-level interactions between each other,
enabling to decouple auxiliary modules from the whole network at the inference
stage. To address spatial inconsistency, we develop a dynamic dual graph
convolutional network to capture global contextual information without using
any extra pooling operation. To handle boundary confusion in both single and
multiple face scenarios, we exploit binary and category edge detection to
jointly obtain generic geometric structure and fine-grained semantic clues of
human faces. Besides, to prevent noisy labels from degrading model
generalization during training, cyclical self-regulation is proposed to
self-ensemble several model instances to get a new model and the resulting
model then is used to self-distill subsequent models, through alternating
iterations. Experiments show that our method achieves the new state-of-the-art
performance on the Helen, CelebAMask-HQ, and Lapa datasets. The source code is
available at
https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr.","Face parsing, as a finegrained semantic segmentation task, intends to assign a pixelwise label for each facial component, e.g., eyes, nose, and mouth. The detailed anal ysis of semantic facial parts is essential in many highlevel applications, such as face swapping [28], face editing [15], This work is done when Qingping Zheng is an intern at Huawei. Image Image GT GT EAGRNet Image Ours GT Figure 1. The first three rows show typical failure cases of spatial inconsistency and boundary confusion when applying EARGNet [36] to face parsing. The last row displays noisy labels on the training datasets. and facial makeup [29]. Benefit from the learning capac ity of deep Convolutional Neural Networks (CNNs) and the labor effort put in pixellevel annotations [15, 21, 35], methods based on Fully Convolutional Networks (FCNs) [7, 10, 18‚Äì20, 23, 36, 47, 48] have achieved a promising per formance on the fully supervised face parsing. Neverthe less, the local characteristic of the convolutional kernel pre vents FCNs from capturing global contextual information [25], which is crucial for semantically parsing facial com ponents in an image. To address this issue, most of the regionbased face pars ing methods [10,20,47] integrate CNN features into variant CRFs to learn global information. However, these methods do not consider the correlation among various objects. To this end, Te et al. [36] proposes the EAGRNet method to model a regionlevel graph representation over a face image by propagating information across all vertices on the graph. Even though EAGRNet enables reasoning over nonlocalregions to get global dependencies between distant facial components and achieves stateoftheart performance, it still faces the problems of spatial inconsistency and bound ary confusion. In EAGRNet, PSP module [45] adopts an av erage pooling layer [22] to capture the global context prior, leading to an inconsistent spatial topology. Moreover, EA GRNet integrates additional clues of binary edges into con text embedding to improve the parsing results. However, it is hard for EAGRNet to handle boundaries between highly irregular facial parts ( e.g. hair and cloth in Figure 1) and dis tinguish clear boundaries between different face instances in the crowded scenarios (multifaces in Figure 1). Besides, learning a reliable model for face parsing re quires accurate pixellevel annotations. Nonetheless, there inevitably exist careless manual labeling errors on the train ing dataset as shown in the last row of Figure 1. Te et al. [36] employ the traditional fully supervised learning scheme to train EAGRNet, failing to locate label noise be cause all pixels in the ground truth are processed equally. Notably, overlooking such incomplete annotations restricts the model generalization and prevents the performance from increasing to a higher level. In this paper, we propose an endtoend face parsing method, which is based on Decoupled Multitask Learn ing with Cyclical SelfRegulation (DMLCSR). Specifi cally, given an input of facial image, the ResNet101 [8] pretrained on ImageNet is taken as the backbone to extract features from different levels. Afterwards, our multitask model consists of three tasks, namely face parsing, binary edge detection, and category edge detection. These tasks share lowlevel weights from the backbone but do not have highlevel interactions. Therefore, our multitask learning approach can detach additional edge detection tasks from face parsing at the inference stage. To tackle spatial incon sistency raised by the pooling operation, we develop a Dy namic Dual Graph Convolutional Network (DDGCN) in the face parsing branch to capture longrange contextual infor mation. The proposed DDGCN contains no extra pooling operation and it can dynamically fuse the global context ex tracted from GCNs in both spatial and feature spaces. To solve the boundary confusion in both singleface and multi face scenarios, the proposed categoryaware edge detection module exploits more semantic information than the binary edge detection module used in EARGNet [36]. To address the problem caused by noisy labels in train ing datasets, we introduce a cyclically learning scheduler inspired by selftraining [3, 16, 34, 41, 42, 42, 49] to achieve advanced cyclical selfregulation. The proposed CSR con tains a selfensemble strategy that can aggregate a set of his torical models to obtain a new reliable model and another selfdistillation method that exploits the soft labels gener ated by the aggregated model to guide the successive model learning. Finally, the proposed CSR iteration alternates between these two procedures, correcting the noisy labels dur ing training and promoting the model generalization. The proposed CSR can significantly promote the reliability of the model and labels in a cyclical training scheduler with out introducing extra computation costs. To summarize, our main contributions are as follows: ‚Ä¢ We propose a decoupled multitask network includ ing face parsing, binary edge detection, and category edge detection. The face parsing branch introduces a DDGCN without any extra pooling operation to solve the problem of spatial inconsistency, and an additional category edge detection branch is designed to handle the boundary confusion. ‚Ä¢ We introduce a cyclical selfregulation mechanism during training. The iteration alternates between one selfensemble procedure, boosting model generaliza tion progressively, and another selfdistillation pro cessing, regulating noisy labels. ‚Ä¢ Our method establishes new stateoftheart perfor mance on the Helen [35] (93.8% overall F1 score), LaPa [21] (92.4% mean F1) and CelebAMaskHQ [15] (86.1% mean F1) datasets. Compared to EARGNet [36], our method utilizes fewer computation resources as the edge prediction modules can be decoupled from the whole network, decreasing the inference time from 89ms to 31ms but achieving much better performance. 2. Related Work "
176,Noise-Resilient Ensemble Learning using Evidence Accumulation Clustering.txt,"Ensemble Learning methods combine multiple algorithms performing the same
task to build a group with superior quality. These systems are well adapted to
the distributed setup, where each peer or machine of the network hosts one
algorithm and communicate its results to its peers. Ensemble learning methods
are naturally resilient to the absence of several peers thanks to the ensemble
redundancy. However, the network can be corrupted, altering the prediction
accuracy of a peer, which has a deleterious effect on the ensemble quality. In
this paper, we propose a noise-resilient ensemble classification method, which
helps to improve accuracy and correct random errors. The approach is inspired
by Evidence Accumulation Clustering , adapted to classification ensembles. We
compared it to the naive voter model over four multi-class datasets. Our model
showed a greater resilience, allowing us to recover prediction under a very
high noise level. In addition as the method is based on the evidence
accumulation clustering, our method is highly flexible as it can combines
classifiers with different label definitions.","Ensemble Learning [1] methods combine several algorithms performing the same task to obtain a betterquality group. Ensemble learning methods play on diverse group aspects: the number of algorithms [ 2,3], their weighting based on their contribution [4, 5, 6], and their selection based on their diversity [7]. Ensemble learning methods are well adapted to the distributed setup, where several machines host each a single algorithm and send their results to a central node aggregating the results [ 8,9,10]. They can be adapted to decentralized peertopeer networks [ 9], where a dynamic group collaborate to improve its accuracy by electing a leader or by aggregating the group‚Äôs results. Distributed systems are prone to network failures, where communications are broken between some nodes, or corrupted with noise [ 11]. In addition, nodes can change of behavior if controled by malicious entities. Ensemble methods are resilient to the absence of one or more weak learners thanks to group redundancy [ 7,12]. However, the corruption of a learner‚Äôs predictions is equivalent to a negative change of accuracy, which has a deleterious effect on the group quality. Thus, there are two ways to deal with corrupted computers: detecting inaccurate peers to avoid data pollution or resilience to error. The detection can be done using network monitoring methods, or exploiting trust to weigh peers based on their past contributions. However, this approach is not adapted to a dynamic environment such as a peertopeer network where a peer lifetime is very short and may change temporary behavior. In contrast, being resilient to error is more suitable as all inputs are accepted but more challenging to design as it requires smart correction algorithms.arXiv:2110.09212v1  [cs.LG]  18 Oct 2021NoiseResilient EL using EAC In this article, we propose a noiseresilient ensemble classiÔ¨Åcation method, correcting errors while improving accuracy. The method uses the Evidence Accumulation Clustering approach to rectify class boundaries and correct corrupted labels by performing a local weighted vote. The approach was tested under several noise condition over four datasets and tolerated high noise levels without accuracy degradation. This paper is structured as follows. The Ô¨Årst section presents the related works regarding ensemble learning methods and resilience to error. The second section details the proposed ensemble classiÔ¨Åcation method. The datasets and the classiÔ¨Åers‚Äô setup are detailed in the experimental section, followed by the results. Finally, the paper ends with a discussion and a conclusion. 2 Related Works "
226,Ensemble Manifold Segmentation for Model Distillation and Semi-supervised Learning.txt,"Manifold theory has been the central concept of many learning methods.
However, learning modern CNNs with manifold structures has not raised due
attention, mainly because of the inconvenience of imposing manifold structures
onto the architecture of the CNNs. In this paper we present ManifoldNet, a
novel method to encourage learning of manifold-aware representations. Our
approach segments the input manifold into a set of fragments. By assigning the
corresponding segmentation id as a pseudo label to every sample, we convert the
problem of preserving the local manifold structure into a point-wise
classification task. Due to its unsupervised nature, the segmentation tends to
be noisy. We mitigate this by introducing ensemble manifold segmentation (EMS).
EMS accounts for the manifold structure by dividing the training data into an
ensemble of classification training sets that contain samples of local
proximity. CNNs are trained on these ensembles under a multi-task learning
framework to conform to the manifold. ManifoldNet can be trained with only the
pseudo labels or together with task-specific labels. We evaluate ManifoldNet on
two different tasks: network imitation (distillation) and semi-supervised
learning. Our experiments show that the manifold structures are effectively
utilized for both unsupervised and semi-supervised learning.","The frontiers of computer vision have been reshaped profoundly in the last few years due to the ever wider deployment of convolutional neural networks (CNNs). A cornucopia of CNNs have been developed and now deÔ¨Åne the state of the art for many vision tasks. On the other hand, manifold learning has been a central concept for many learning algorithms over decades, with applications to tasks as diverse as dimensionality reduction [32, 41], hashing [34], feature encoding [43], clustering [13, 37], semisupervised learning [2], model imitation [9], and visualization [26]. Thus, it stands to reason to devise methods to utilize manifold structures more effectively in training CNNs. c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:1804.02201v1  [cs.CV]  6 Apr 20182 D. DAI, W. LI, T. KROEGER, L. VAN GOOL: ENSEMBLE MANIFOLD SEGMENTATION Figure 1: ManifoldNet Pipeline. The bottom panel shows ensemble manifold segmentation (EMS) for the ensemble of pseudolabels. The top panel features the architecture of the network, which consists of two network streams shared parameters. The left stream is guided by a taskspeciÔ¨Åc loss function ( e.g. classiÔ¨Åcation loss for semisupervised classiÔ¨Åcation and L2 loss for regression) and is trained with samples whose target values are available. The right stream is guided by an ensemble of pseudo classiÔ¨Åcation tasks and trained with the unlabeled samples used for the clustering. The main challenge to train a CNN with manifold structures is to incorporate the latter‚Äôs structure onto the former‚Äôs architecture. Manifold structures are often expressed by a graph or an afÔ¨Ånity matrix of all data samples. This is inconvenient to use with many CNNs, as these are tailored for classiÔ¨Åcation tasks with crisp class labels. Systems do exist for learning CNNs on graphs [4, 10, 28], which accommodate afÔ¨Ånity graphs directly as training data. However, the scalability of this strand of method is limited, since afÔ¨Ånity graphs can potentially be large. In order to seamlessly couple manifold structures with the architecture of modern CNNs, we propose to segment data manifold. It is segmented so that samples that are close to each other fall into the same group or ‚Äòpseudoclass‚Äô. The corresponding pseudolabels are fed to the CNN to train it for classiÔ¨Åcation: grouping similar samples and separating dissimilar ones. This is in line with the aim of manifold learning. Yet, the labels obtained from the segmentation can be noisy due to its unsupervised nature. To mitigate this, we propose ensemble manifold segmentation (EMS) to create an ensemble of segmentations that are accurate individually and mutually diverse. EMS leads to an ensemble of pseudo classiÔ¨Åcation tasks, which results in an ensemble task architecture featuring an ensemble of loss functions. Figure 1 shows the architecture of our method. It consists of two copies of the same network, with shared weight parameters. The right stream is trained with unlabeled data and their pseudolabels as generated fromD. DAI, W. LI, T. KROEGER, L. VAN GOOL: ENSEMBLE MANIFOLD SEGMENTATION 3 ensemble clustering; the stream on the left is trained under ‚Äòreal‚Äô supervision, with training samples whose real target labels are available. The method is dubbed ManifoldNet. Mani foldNet can be trained with only the right stream or with the two streams jointly, depending on the nature of the tasks. For instance, for unsupervised learning tasks such as dimension ality reduction, hashing and unsupervised network imitation (distillation), one can use only the right stream. For tasks such as semisupervised classiÔ¨Åcation, the two streams are trained with different training sets, one labeled, one unlabeled. This Ô¨Çexibility greatly increases the applicability of the method. ManifoldNet translates manifold structures to crisp labels, which gives representational and training advantages with modern CNNs. Apart from being intuitive and easy to im plement, ManifoldNet has additional beneÔ¨Åts. Compared to manifold learning methods [9, 32, 41], it comes naturally with an outofsample ability: the trained CNNs can be used for many tasks, in the same way as standard CNNs can, e.g. as a feature extractor; Compared to deep embedding [42, 46, 48, 49], ManifoldNet is better scalable as it can be trained in a pointwise manner. ManifoldNet is a very general framework and can be easily applied to many different tasks. As Figure 1 shows, much of the work needed to apply it to a new task lies in adopting a taskspeciÔ¨Åc network architecture. The method is orthogonal to the meth ods [11, 12, 45] for selflearning feature representations. ManifoldNet is evaluated on two different tasks: network imitation and semisupervised classiÔ¨Åcation. Experiments show that it effectively utilizes manifold structures for both unsupervised and semisupervised learn ing. 2 Related Work "
333,Compensation Learning in Semantic Segmentation.txt,"Label noise and ambiguities between similar classes are challenging problems
in developing new models and annotating new data for semantic segmentation. In
this paper, we propose Compensation Learning in Semantic Segmentation, a
framework to identify and compensate ambiguities as well as label noise. More
specifically, we add a ground truth depending and globally learned bias to the
classification logits and introduce a novel uncertainty branch for neural
networks to induce the compensation bias only to relevant regions. Our method
is employed into state-of-the-art segmentation frameworks and several
experiments demonstrate that our proposed compensation learns inter-class
relations that allow global identification of challenging ambiguities as well
as the exact localization of subsequent label noise. Additionally, it enlarges
robustness against label noise during training and allows target-oriented
manipulation during inference. We evaluate the proposed method on %the widely
used datasets Cityscapes, KITTI-STEP, ADE20k, and COCO-stuff10k.","Semantic segmentation is a wellknown and challeng ing task in computer vision [6, 34, 50]. Thanks to the large investment of time and resources, the research community published a large number of elaborately curated datasets to train and evaluate methods for semantic segmentation [16, 37, 53, 60, 79, 92]. Nevertheless, the industry needs an in creasing amount of accurately annotated data and spends billion dollars to curate them [17]. Unfortunately, the an notation task stays challenging for humans even with ad vanced semiautomated annotation frameworks [1, 10, 72], because ambiguous image elements often can be assigned to multiple classes. Thus, annotated data is often noisy, with the consequence that the optimization of stochastic meth ods like neural networks is corrupted and the evaluation is distorted. Even the ground truth of widely used research benchmarks, which form the basis of this and many other papers, are subject to noise, as lamented by [42]. Semi automated annotation without incorporating label noise is therefore a serious problem in semantic segmentation. 1arXiv:2304.13428v1  [cs.CV]  26 Apr 2023While tackling the impact of noisy labels is a well known research topic [7, 19, 31, 63], avoiding noisy labels during labeling is shallow investigated. Because modern semi automated annotation frameworks estimate an initial guess with a pretrained segmentation framework [1,9,10,72], an obvious way to improve the annotation framework is to im prove the segmentation framework. To remove the residual error in the estimate, the human curator is still asked to in spect and correct the entire image. To reduce this effort, un certainty estimation can help to guide the curator to Ô¨Ånd the most likely error regions. Current approaches like Bayesian Neural Networks [52,73], that estimate and incorporate un certainty in semantic segmentation aim to make the training more robust against label noise, but mainly detect bound aries between neighboring segments [4, 7, 73]. Instead of using uncertainty estimation to make training more robust against noise, we aim to utilize robust training methods and uncertainty estimation to avoid new noise dur ing data annotation. Therefore, we present a novel method transferring compensation learning to semantic segmenta tion to compensate noise and ambiguities with endtoend trainable compensation weights. Compensation learning, which adds ground truth depending bias to model predic tions, has been introduced by Yao et al. [85] for image clas siÔ¨Åcation. It allows the lowering of the inÔ¨Çuence of simi lar classes in order to reduce the impact of ambiguities and noise. We induce symmetry to make compensation learning stable during training and introduce an adaptive uncertainty branch that estimates the local importance of compensation. Experiments on the widely used segmentation datasets Cityscapes [16], KITTISTEP [79], ADE20k [92], and COCOstuff10k [37] show that our method learns inter pretable interclass compensations and is able to estimate prediction uncertainties. We present how compensation identiÔ¨Åes challenging class pairs and the uncertainty lo calizes prediction errors very accurately. Besides the in terpretable guidance for data annotation, our method in creases the robustness of training semantic segmentation methods with noisy labels and additionally introduces a use ful method to improve the segmentation accuracy of certain classes. Moreover, we analyze and visualize interclass am biguities for the datasets. In summary , our work contributes a novel framework1 to improve semiautomated annotation that ‚Ä¢ learns humaninterpretable compensation weights of global interclass ambiguities. ‚Ä¢ introduces a novel uncertainty branch to adapt the compensation locally. The branch provides local guid ance to image regions with high risk of errors. ‚Ä¢ improves robustness against noise during training. 1Code available at https://github.com/tntLUH/compensation learning‚Ä¢ allows applicationoriented manipulation of segmenta tion accuracy during inference. 2. Related Work "
264,Dual-Correction Adaptation Network for Noisy Knowledge Transfer.txt,"Previous unsupervised domain adaptation (UDA) methods aim to promote target
learning via a single-directional knowledge transfer from label-rich source
domain to unlabeled target domain, while its reverse adaption from target to
source has not jointly been considered yet so far. In fact, in some real
teaching practice, a teacher helps students learn while also gets promotion
from students to some extent, which inspires us to explore a dual-directional
knowledge transfer between domains, and thus propose a Dual-Correction
Adaptation Network (DualCAN) in this paper. However, due to the asymmetrical
label knowledge across domains, transfer from unlabeled target to labeled
source poses a more difficult challenge than the common source-to-target
counterpart. First, the target pseudo-labels predicted by source commonly
involve noises due to model bias, hence in the reverse adaptation, they may
hurt the source performance and bring a negative target-to-source transfer.
Secondly, source domain usually contains innate noises, which will inevitably
aggravate the target noises, leading to noise amplification across domains. To
this end, we further introduce a Noise Identification and Correction (NIC)
module to correct and recycle noises in both domains. To our best knowledge,
this is the first naive attempt of dual-directional adaptation for noisy UDA,
and naturally applicable to noise-free UDA. A theory justification is given to
state the rationality of our intuition. Empirical results confirm the
effectiveness of DualCAN with remarkable performance gains over
state-of-the-arts, particularly for extreme noisy tasks (e.g., ~+ 15% on Pw->Pr
and Pr->Rw of Office-Home).","DEEP neural network has achieved remarkable success in many applications, such as image classiÔ¨Åcation and semantic segmentation. However, it relies on large scared and highquality annotated data, which is usually difÔ¨Åcult to collect. Unsupervised domain adaptation (UDA) [1], which aims to adopt a fullylabeled source domain to help the learning of unlabeled target domain, has attracted much attention in recent years. Most UDA methods trans fer knowledge from source to target by learning domain invariant representation across domains, mainly including discrepancybased [2], [3] and adversarialbased methods [4], [5], [6]. Discrepancybased methods explicitly reduce the distribution discrepancy between domains by minimizing some distance metric, such as Maximum Mean Discrep ancy (MMD) [3], Correlation Alignment (CORAL) [7] and Wasserstein distance [8]. Adversarialbased methods align feature distributions across domains by adversarial training between feature generator and domain discriminator [5], or between different classiÔ¨Åers [2]. In real UDA tasks, the source domain usually involves noises, further giving rise to noisy UDA [9], [10]. For ex ample, source data collected from crowdsourced platforms Yunyun Wang and Weiwen Zheng are with the Jiangsu Key Labo ratory of Big Data Security & Intelligent Processing, Computer Sci ence and Engineering, Nanjing University of Posts & Telecommuni cations, Nanjing 210046, China. Email: wangyunyun@njupt.edu.cn, 1020041209@njupt.edu.cn. Songcan Chen is with the MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Computer Science & Technology/AI, Nanjing University of Aeronautics & Astronautics, Nanjing 210023, China. E mail: s.chen@nuaa.edu.cn. Corresponding Author: Songcan Chen (s.chen@nuaa.edu.cn). Manuscript received April 19, 2005; revised August 26, 2015.or internet medias will inevitably be corrupted by noises over both features and labels. The feature noise corrupts original features and may increase the difÔ¨Åculty of do main alignment, while label noise worsens the expected risk of classiÔ¨Åcation, thus incurs misclassiÔ¨Åcation of target instances. It makes previous UDA methods easy to fail in noisy environments. Recently, some researches [9], [10], [11], [12] have been dedicated to noisy UDA learning, which can be mainly divided into two categories. One kind of meth ods, such as Transferable Curriculum Learning (TCL) [9] and Robust Domain Adaptation (RDA) [10], adopts small loss criterion to separate source instances into clean and noisy parts, then transfer source knowledge to target with only clean instances detected. The other kind, including Noisy Universal Domain Adaptation (Noisy UniDA) [11] and Noise Resistible MutualTraining (NRMT) [12], uses colearning strategy with multiple classiÔ¨Åers to reduce the impact of label noise in adaptation. Those previous UDA methods all adopt a single  directional knowledge transfer from labeled source to un labeled target for helping the target learning. However, in some real teaching practice, a teacher helps students learn, while also gets promotion from students to some extent. Inspired by such a philosophy, the reverse adaptation from the target should intuitively be able to boost the source learning as well, especially for weak noisy sources. To our best knowledge, however, it has not been jointly considered in UDA so far. In this paper, we attempt to explore a dual directional knowledge transfer between domains, and pro pose a DualCorrection Adaptation Network (DualCAN) to achieve mutual promotion and cooperation across domains. However, it is worth noting that there is asymmetrical label knowledge between domains, since target domainarXiv:2207.04423v1  [cs.CV]  10 Jul 2022JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 contains much less label information than the source, thus the transfer from target to source poses a more difÔ¨Åcult challenge than the sourcetotarget counterpart. First, the target pseudolabels predicted by source commonly involve noises due to transfer and model bias, hence in the reverse adaptation, they may hurt the source performance, and consequently bring a negative targettosource transfer. Sec ondly, the source domain usually contains innate noises in real tasks. It will inevitably aggravate the target noises, and incurs noise ampliÔ¨Åcation across domains. To address those issues, we further introduce a crucial Noise IdentiÔ¨Åcation and Correction (NIC) module in DualCAN to correct the noises in both domains. After that, those corrected instances are further recycled in learning rather than simply dis carded, in order for a full knowledge utilization, especially in high noisy environment. In implementing DualCAN, knowledge transfer iterates back and forth between sourcetotarget (ST) task and target tosource (TS) task. In ST, source knowledge is adapted to generate target pseudolabels, and the pseudolabels are fur ther corrected by NIC with selfsupervised knowledge. In TS, the target knowledge is transferred reversely to correct source noise and further boost source learning. With such a dualdirectional knowledge transfer between domains, noises in both domains are corrected collaboratively, and performances in both domains are promoted mutually. It is analogous to the philosophy that teaching beneÔ¨Åts both the teacher and students alike. Quite naturally, dualdirectional transfer can be adopted for both noisy and noisefree UDA tasks, while its learning concept is also applicable for some other learning tasks, for example, using downstream tasks to reversely help feature learning in selfsupervised learn ing. The main contributions of this paper are summarized as follows: A DualCorrection Adaptation Network (DualCAN) is proposed for noisy UDA learning. To our best knowledge, this is the Ô¨Årst work of dualdirectional adaptation to mutually promote learning and correct noises in both domains. The noisy instances are corrected and recycled by a Noise IdentiÔ¨Åcation and Correction (NIC) module, in order to prevent noise ampliÔ¨Åcation aross domains, and achieve a full knowledge utilization, especially in high noisy environment. A theory justiÔ¨Åcation is given to state the rationality of DualCAN. Moreover, empirical comparisons are conducted in realworld tasks under different noisy settings, in order to conÔ¨Årm the effectiveness of pro posal. The rest of the paper is organized as follows, section 2 introduces the related works, section 3 gives the preliminar ies, the proposed DualCAN is described in details in section 4, and the comparison results are given in section 5. Finally, section 6 is the conclusion. 2 R ELATED WORK "
88,Grounded Recurrent Neural Networks.txt,"In this work, we present the Grounded Recurrent Neural Network (GRNN), a
recurrent neural network architecture for multi-label prediction which
explicitly ties labels to specific dimensions of the recurrent hidden state (we
call this process ""grounding""). The approach is particularly well-suited for
extracting large numbers of concepts from text. We apply the new model to
address an important problem in healthcare of understanding what medical
concepts are discussed in clinical text. Using a publicly available dataset
derived from Intensive Care Units, we learn to label a patient's diagnoses and
procedures from their discharge summary. Our evaluation shows a clear advantage
to using our proposed architecture over a variety of strong baselines.","The ability of recurrent neural networks to model sequential data and capture longterm dependencies makes them powerful tools for natural language processing. These models maintain a state at each time step, representing the relevant history and taskspeciÔ¨Åc beliefs. Based on the current value of this recurrent state and a new input, the state is updated at each time step. Recurrent models have become a popular choice for a variety of natural language processing tasks such as language modeling [Mikolov et al., 2010], text classiÔ¨Åcation [Graves, 2012], or machine translation [Cho et al., 2014a]. The success of this paradigm has been driven in great part by a number of structural innovations since the original version of Elman [1990]. Recurrent cells such as the Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] or Gated Recurrent Units (GRU) [Cho et al., 2014b], for example, alleviate the problem of vanishing gradients [Bengio et al., 1994]. Attention mechanisms [Bahdanau et al., 2014] and Memory Networks [Sukhbaatar et al., 2015] have also signiÔ¨Åcantly increased the expressiveness of recurrent architectures, revealing their potential to tackle more complex tasks such as question answering [Rajpurkar et al., 2016]. One notable property of these models, however, is that they often require signiÔ¨Åcant amounts of training data to perform at their best [Bajgar et al., 2016], which can limit their application domain. In this work, we focus on developing recurrent models for the task of extracting medical concepts from Intensive Care Unit discharge summaries. This is a multiclass, multilabel text classiÔ¨Åcation task with a target vocabulary of several thousand concepts. Given the difÔ¨Åculty to obtain very large medical datasets, there is a need to come up with new, more dataefÔ¨Åcient architectures. To this end, we introduce the Grounded Recurrent Neural Network (GRNN). At a high level, we ground the model‚Äôs hidden state by introducing dimensions whose sole purpose is to track the model‚Äôs belief in the presence of speciÔ¨Åc labels for the current example. We Ô¨Ånd that this addition aids optimization, and outperforms standard recurrent models for text classiÔ¨Åcation. Although each new label adds to the hidden state size, we impose a semi diagonal constraint on the recurrent transition matrices, so that the size of our model grows linearly with the number of labels. We show that this not only lets the model scale with the number of labels, but also helps with optimization. Furthermore, our approach leads to increased interpretability, which is especially appreciated in medical applications. Indeed, even though we do not provide our model with the location of phrases of interest for a labelarXiv:1705.08557v1  [stat.ML]  23 May 2017at training time, we can track changes in the grounded dimensions tied to a speciÔ¨Åc concept as a document is read, indicating evidence in text for or against its presence when the model‚Äôs belief changes drastically. We evaluate our model on the publicly available MIMIC datasets [Saeed et al., 2011, Johnson et al., 2016] to predict ICD9 (International ClassiÔ¨Åcation of Diseases [Organization et al., 1978]) codes given a patient‚Äôs discharge summary text [Perotte et al., 2014, Lita et al., 2008]. These codes are usually determined by humans perusing health records and selecting relevant codes from very long lists. Due to the high number of ICD9 codes, there is signiÔ¨Åcant human error, arising from the cognitive load of such a task [BirmanDeych et al., 2005, Hsia et al., 1988] and differences in human judgment [Pestian et al., 2007]. The effort needed, the errors in the coding process and the inconsistency of labeling can be mitigated through automatically detecting concepts in text or offering suggestions as smarter autocomplete, which motivates our contribution. We also show our model‚Äôs performance on a tag prediction dataset built from StackOverÔ¨Çow data. Section 2 presents relevant previous work, Section 3 describes the model, and we present experimental results in Section 4. Section 5 concludes and outlines possible future research directions. 2 Related Work "
560,CNN: Single-label to Multi-label.txt,"Convolutional Neural Network (CNN) has demonstrated promising performance in
single-label image classification tasks. However, how CNN best copes with
multi-label images still remains an open problem, mainly due to the complex
underlying object layouts and insufficient multi-label training images. In this
work, we propose a flexible deep CNN infrastructure, called
Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment
hypotheses are taken as the inputs, then a shared CNN is connected with each
hypothesis, and finally the CNN output results from different hypotheses are
aggregated with max pooling to produce the ultimate multi-label predictions.
Some unique characteristics of this flexible deep CNN infrastructure include:
1) no ground truth bounding box information is required for training; 2) the
whole HCP infrastructure is robust to possibly noisy and/or redundant
hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may
be well pre-trained with a large-scale single-label image dataset, e.g.
ImageNet; and 5) it may naturally output multi-label prediction results.
Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasets
well demonstrate the superiority of the proposed HCP infrastructure over other
state-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3%
after the fusion with our complementary result in [47] based on hand-crafted
features on the VOC2012 dataset, which significantly outperforms the
state-of-the-arts with a large margin of more than 7%.","SINGLE label image classiÔ¨Åcation, which aims to assign a label from a predeÔ¨Åned set to an image, has been extensively studied during the past few years [14], [18], [10]. For image representation and classiÔ¨Åcation, conventional approaches utilize care fully designed handcrafted features, e.g., SIFT [32], along with the bagofwords coding scheme, followed by the feature pooling [25], [44], [37] and classic classiÔ¨Åers, such as Support Vector Machine (SVM) [4] and random forests [2]. Recently, in contrast to the handcrafted features, learnt image features with deep network structures have shown their great potential in various vision recognition tasks [26], [21], [24], [36]. Among these architectures, one of the great est breakthroughs in image classiÔ¨Åcation is the deep convolutional neural network (CNN) [24], which has achieved the stateoftheart performance (with 10% gain over the previous methods based on hand crafted features) in the largescale singlelabel object recognition task, i.e., ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [10] with more than one million images from 1,000 object categories. Multilabel image classiÔ¨Åcation is however a more general and practical problem, since the majority of Yunchao Wei is with Department of Electrical and Computer Engineering, National University of Singapore, and also with the Institute of Informa tion Science, Beijing Jiaotong University, email: wychao1987@gmail.com. Yao Zhao is with the Institute of Information Science, Beijing Jiaotong University, Beijing 100044, China. Bingbing Ni is with the Advanced Digital Sciences Center, Singapore. Wei Xia, Junshi Huang, Jian Dong and Shuicheng Yan are with De partment of Electrical and Computer Engineering, National University of Singapore.realworld images are with more than one objects of different categories. Many methods [37], [6], [12] have been proposed to address this more challenging problem. The success of CNN on singlelabel image classiÔ¨Åcation also sheds some light on the multi label image classiÔ¨Åcation problem. However, the CNN model cannot be trivially extended to cope with the multilabel image classiÔ¨Åcation problem in an inter pretable manner, mainly due to the following reasons. Firstly, the implicit assumption that foreground ob jects are roughly aligned, which is usually true for singlelabel images, does not always hold for multi label images. Such alignment facilitates the design of the convolution and pooling infrastructure of CNN for singlelabel image classiÔ¨Åcation. However, for a typical multilabel image, different categories of ob jects are located at various positions with different scales and poses. For example, as shown in Figure 1, for singlelabel images, the foreground objects are roughly aligned, while for multilabel images, even with the same label, i.e., horse and person , the spa tial arrangements of the horse and person instances vary largely among different images. Secondly, the interaction between different objects in multilabel images, like partial visibility and occlusion, also poses a great challenge. Therefore, directly applying the original CNN structure for multilabel image classiÔ¨Å cation is not feasible. Thirdly, due to the tremendous parameters to be learned for CNN, a large number of training images are required for the model training. Furthermore, from singlelabel to multilabel (with n category labels) image classiÔ¨Åcation, the label space has been expanded from nto2n, thus more trainingarXiv:1406.5726v3  [cs.CV]  9 Jul 2014JOURNAL OF L ATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014 2 Single label images from ImageNet   Multi label images from Pascal VOC  horse&   person   dog&   person   Fig. 1. Some examples from ImageNet [10] and Pascal VOC 2007 [13]. The foreground objects in singlelabel images are usually roughly aligned. However, the as sumption of object alighment is not valid for multilabel images. Also note the partial visibility and occlusion between objects in the multilabel images. data is required to cover the whole label space. For singlelabel images, it is practically easy to collect and annotate the images. However, the burden of collection and annotation for a large scale multilabel image dataset is generally extremely high. To address these issues and take full advantage of CNN for multilabel image classiÔ¨Åcation, in this paper, we propose a Ô¨Çexible deep CNN structure, called HypothesesCNNPooling (HCP). HCP takes an arbitrary number of object segment hypotheses as the inputs, which may be generated by the sateofthe art objectiveness detection techniques, e.g., binarized normed gradients (BING) [8], and then a shared CNN is connected with each hypothesis. Finally the CNN output results from different hypotheses are aggre gated by max pooling to give the ultimate multi label predictions. Particularly, the proposed HCP in frastructure possesses the following characteristics: No groundtruth bounding box information is required for training on the multilabel image dataset. Different from previous works [12], [5], [15], [35], which employ groundtruth bounding box information for training, the proposed HCP requires no bounding box annotation. Since bounding box annotation is much more costly than labelling, the annotation burden is signiÔ¨Åcantly reduced. Therefore, the proposed HCP has a better generalization ability when transferred to new multilabel image datasets. The proposed HCP infrastructure is robust to the noisy and/or redundant hypotheses. To suppress the possibly noisy hypotheses, a crosshypothesismaxpooling operation is carried out to fuse the outputs from the shared CNN into an integrative prediction. With max pooling, the high predictive scores from those hypotheses containing objects are reserved and the noisy ones are ignored. Therefore, as long as one hypothesis contains the object of interest, the noise can be suppressed after the crosshypothesis pooling. Redundant hypotheses can also be well addressed by max pooling. No explicit hypothesis label is required for training. The stateoftheart CNN models [15], [35] utilize the hypothesis label for training. They Ô¨Årst compute the IntersectionoverUnion (IoU) overlap between hypotheses and groundtruth bounding boxes, and then assign the hypothesis with the label of the groundtruth bounding box if their overlap is above a threshold. In contrast, the proposed HCP takes an arbitrary number of hypotheses as the inputs without any explicit hypothesis labels. The shared CNN can be well pretrained with a largescale singlelabel image dataset. To address the problem of insufÔ¨Åcient multilabel training images, based on the HypothesesCNNPooling architecture, the shared CNN can be Ô¨Årst well pretrained on some largescale singlelabel dataset, e.g., ImageNet, and then Ô¨Ånetuned on the target multilabel dataset. The HCP outputs are intrinsically multilabel prediction results. HCP produces a normalized probability distribution over the labels after the softmax layer, and the the predicted probability values are intrinsically the Ô¨Ånal classiÔ¨Åcation conÔ¨Ådence values for the corresponding categories. Extensive experiments on two challenging multi label image datasets, Pascal VOC 2007 and VOC 2012, well demonstrate the superiority of the proposed HCP infrastructure over other stateofthearts. The rest of the paper is organized as follows. We brieÔ¨Çy review the related work of multilabel classiÔ¨Åcation in Section 2. Section 3 presents the details of the HCP for image classiÔ¨Åcation. Finally the experimental results and conclusions are provided in Section 4 and Section 5, respectively. 2 R ELATED WORK "
91,MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection.txt,"This paper describes our system for SemEval-2021 Task 5 on Toxic Spans
Detection. We developed ensemble models using BERT-based neural architectures
and post-processing to combine tokens into spans. We evaluated several
pre-trained language models using various ensemble techniques for toxic span
identification and achieved sizable improvements over our baseline fine-tuned
BERT models. Finally, our system obtained a F1-score of 67.55% on test data.","Toxic speech has become a rising issue for social media communities. Abusive content is very di verse and therefore offensive language and toxic speech detection is not a trivial issue. Besides, so cial media moderation of lengthy comments and posts is often a timeconsuming process. In this regard, the task of detecting toxic spans in social media texts deserves close attention. This work is based on the participation of our team, named MIPTNSUUTMN, in Se mEval 2021 Task 5, ‚ÄúToxic Spans Detection‚Äù (Pavlopoulos et al. ,2021 ). Organizers of the shared task provided participants with the trial, train, and test sets of English social media comments annotated at the span level indicating the presence or absence of text toxicity. We formulated the task as a token classiÔ¨Åcation problem and investigated several BERTbased models using twostep knowledge transfer. We found that preliminary Ô¨Ånetuning of the model on data that is close to the target domain im proves the quality of the token classiÔ¨Åcation. The source code of our models is available at https://github.com/morozowdmitry/semeval21 . The paper is organized as follows. A brief re view of related work is given in Section 2. The def inition of the task has been summarized in Section3. The proposed methods and experimental set tings have been elaborated in Section 4. Section 5 contains the results and error analysis respectively. Section 6 is a conclusion. 2 Related Work "
220,DNS: Determinantal Point Process Based Neural Network Sampler for Ensemble Reinforcement Learning.txt,"Application of ensemble of neural networks is becoming an imminent tool for
advancing the state-of-the-art in deep reinforcement learning algorithms.
However, training these large numbers of neural networks in the ensemble has an
exceedingly high computation cost which may become a hindrance in training
large-scale systems. In this paper, we propose DNS: a Determinantal Point
Process based Neural Network Sampler that specifically uses k-dpp to sample a
subset of neural networks for backpropagation at every training step thus
significantly reducing the training time and computation cost. We integrated
DNS in REDQ for continuous control tasks and evaluated on MuJoCo environments.
Our experiments show that DNS augmented REDQ outperforms baseline REDQ in terms
of average cumulative reward and achieves this using less than 50% computation
when measured in FLOPS.","In the past decade, reinforcement learning (RL) algorithms powered by highcapacity function approximators such as deep neural networks have been used to master complex sequential decision problems such as Atari games (Mnih et al., 2015), boards games like Chess, Go and Shogi (Silver et al., 2016; 2017; 2018) and robotic manipulation (Liu et al., 2021). Despite having impressive results, deep reinforce ment learning (DRL) algorithms suffer from whole host of problems such as sample inefÔ¨Åciency ( ≈Åukasz Kaiser et al., 2020), overestimation bias (van Hasselt, 2010; Hado van Hasselt et al., 2016; Lan et al., 2020; Anschel et al., 2017; Fujimoto et al., 2018) and imbalance between exploration and exploitation (Lee et al., 2020; Osband et al., 2016). *Equal contribution1Intel Labs2Intel Corporation. Corre spondence to: Hassam Sheikh <hassam.sheikh@intel.com, has samsheikh1@gmail.com >. Proceedings of the 39thInternational Conference on Machine Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy right 2022 by the author(s).Considering the success of ensembles in supervised learn ing, use of ensemble of neural networks is becoming popular in deep reinforcement learning (DRL) and are being used to address the aforementioned issues. For example, in (Lan et al., 2020; Anschel et al., 2017; Fujimoto et al., 2018) have used ensemble to address the overestimation bias problem. In (Chen et al., 2021) proposed REDQ that uses ensemble with high updatetodate ratio to address the sample inef Ô¨Åciency problem. Similarly (Lee et al., 2020) have used ensemble for efÔ¨Åcient exploration. Despite ensembles providing elegant theoretical and practi cal solutions, they introduce new practical problems such as high computation cost and long training times. The high computation cost problem is specially evident in actorcritic settings where DRL algorithms use a high number of critic networks. One such example is the REDQ algorithm that uses ten critic networks and updates all of them in every training step which leads to high computation cost as well as high wallclock time. To address this issue, we present DNS: a Determinantal Point Process based Neural Network Sampler that speciÔ¨Å cally useskDPP (Kulesza & Taskar, 2011) to sample a sub set of neural networks for backpropagation at every training step. DNS uses Centered Kernel Alignment (CKA) (Korn blith et al., 2019) values to form the similarity matrix which are then used by the kDPP to sample the subset on neural networks for backpropagation. The motivation for sampling a subset of networks came from a hypothesis which we show in Section 4.1 that the Qvalues from the critics converge prematurely during training thus eliminating the need of training all the critics at every training step. Additionally, we show that in the event that the CKA ma trix is not positive semideÔ¨Ånite, the closest positive semi deÔ¨Ånite matrix is just a diagonal perturbation of the CKA matrix and its resulting kernel matrix is still Hermitian posi tive semideÔ¨Ånite. We applied DNS on REDQ and performed experiments on MuJoCo environments (Todorov et al., 2012) and show that a simple sampling technique can signiÔ¨Åcantly reduce the training time and computation while maintaining similar performance as training all the networks in the ensemble.arXiv:2201.13357v3  [cs.LG]  17 May 2022DNS: D eterminantal Point Process Based N eural Network S ampler for Ensemble Reinforcement Learning To summarize, our contributions are following: 1.We empirically show that neural network based value function approximators collapse prematurely during training in ensemble reinforcement learning. 2.To address this issue, we propose a Determinantal Point Process based Neural Network Sampler that samples a subset of valuefunction approximators for backpropa gation at every training step. 3.We apply DNS on REDQ, that uses an ensemble of tencritic networks. Our experiments have shown that DNS sampling achieves similar or better results than REDQ in 50% computation when measured in FLOPS. 4.We also provide a theoretical analysis and proof that shows thatkDPP sampling of actionvalue functions leads to lower actionvalue minimization variance than random sampling kactionvalues. Additionally, we show how sufÔ¨Åciency conditions for kDPP sampling can easily be met for the Deep RL use case. 2. Related Work "
221,Uncertainty Based Detection and Relabeling of Noisy Image Labels.txt,"Deep neural networks (DNNs) are powerful tools in computer vision tasks.
However, in many realistic scenarios label noise is prevalent in the training
images, and overfitting to these noisy labels can significantly harm the
generalization performance of DNNs. We propose a novel technique to identify
data with noisy labels based on the different distributions of the predictive
uncertainties from a DNN over the clean and noisy data. Additionally, the
behavior of the uncertainty over the course of training helps to identify the
network weights which best can be used to relabel the noisy labels. Data with
noisy labels can therefore be cleaned in an iterative process. Our proposed
method can be easily implemented, and shows promising performance on the task
of noisy label detection on CIFAR-10 and CIFAR-100.","In the last decade Deep neural networks (DNNs) have proven their predictive power in many supervised learning tasks with complex data patterns. Especially when the train ing set is large, representative, and correctly labeled, DNNs are the current stateoftheart on several learning tasks. Un fortunately, the latter assumption does not hold in many re alistic cases (e.g. medical imaging, crowdsourced label ing), and DNNs have been shown to overÔ¨Åt on noisy labels, leading to poor generalization performance. For example, [33] shows that DNNs can easily Ô¨Åt randomly assigned la bels on the training set, which leads to poor test perfor mance. Therefore, it is important to detect and correct for noisy labels in the training set. We propose an iterative label noise Ô¨Åltering process, based on the predictive uncertainty of the training images. Ensembles [17] and MC dropout [8] are used to obtain un certainty estimates for each image. We show that the un certainties of the noisy images and the uncertainties of the clean images follow two different distributions, enabling the Bosch Center for ArtiÔ¨Åcial Intelligence. First name.last name@de.bosch.comdetection of potentially noisy labels. After the detection of the noisy labels, the detected image could be taken out of the training set, its weight on the loss could be decreased, or it could be relabeled through an oracle or any appropriate relabeling approach. 2. Related Work "
307,Influential Rank: A New Perspective of Post-training for Robust Model against Noisy Labels.txt,"Deep neural network can easily overfit to even noisy labels due to its high
capacity, which degrades the generalization performance of a model. To overcome
this issue, we propose a new approach for learning from noisy labels (LNL) via
post-training, which can significantly improve the generalization performance
of any pre-trained model on noisy label data. To this end, we rather exploit
the overfitting property of a trained model to identify mislabeled samples.
Specifically, our post-training approach gradually removes samples with high
influence on the decision boundary and refines the decision boundary to improve
generalization performance. Our post-training approach creates great synergies
when combined with the existing LNL methods. Experimental results on various
real-world and synthetic benchmark datasets demonstrate the validity of our
approach in diverse realistic scenarios.","The current deep learning has made a huge breakthrough because of ‚Äòdata‚Äô. Thus, many researchers in both academia and industry endeavor to obtain considerable data. How ever, realworld data inevitably contain some proportion of incorrectly labeled data, owing to perceptual ambiguity, or errors from human or machine annotations. These noisy la bels negatively affect the generalization performance of a trained model since a deep neural network (DNN) can eas ily overÔ¨Åt to even noisy labels due to its high capacity [52]. Therefore, learning from noisy labels (LNL) has received much attention in recent years [13, 49, 38, 57, 28, 5, 16] due to the increasing need to handle noisy labels in practice. To handle noisylabel problem, prior literature aims to distinguish between clean and mislabeled data, and use this information to train a robust classiÔ¨Åer during training. To this end, prior works mainly rely on the assumption that the clean labels are more likely to have smaller losses before the model is overÔ¨Åtted [2]. However, due to the high capacity of deep neural networks (DNNs), DNNs can Ô¨Åt even noisy labels [52]; thus it is challenging to correctly detect mis labeled data during training. Hence, various methods have been proposed to use more robust models before overÔ¨Åtting, CEVolminNet CoteachingELR+DivideMix UNICON6065707580859095AccuracyOriginal w/ RoG w/ OursFigure 1. Test accuracy improvement over various methods on CIFAR10N (Worst). As a posttraining method, our pro posed InÔ¨Çuential Rank can improve various pretrained models by large margin, compared to the postprocessing baseline method, RoG [23]. The used CIFAR10N (Worst) is a humanannotated realworld noisy dataset with about 40% noise rate [46]. such as leveraging the model with early stopping [39, 26], or using multiple networks with cotraining for sample se lection [11, 51, 24]. Here, we introduce a different perspective against the mainstream research. We propose a new posttraining LNL approach, which can synergize with the model trained using prior robust methods, further enhancing the generalization capability of the model. Given a pretrained model, the pro posed posttraining scheme reÔ¨Ånes the model by exploiting the ‚ÄòoverÔ¨Åtting property‚Äô of mislabeled samples. ‚ÄòOverÔ¨Åt ting property‚Äô of mislabeled samples is derived from two following intuitions. (1) Mislabeled samples are more likely to distort the decision boundary than clean samples. Thus removing the mislabeled samples is likely to sway the de cision boundary signiÔ¨Åcantly. (2) The overÔ¨Åtted model pre dicts poorly on unseen data, and the mislabeled sample is usually the main culprit for the model to classify new data with incorrect labels. The details on these intuitions are dis cussed in Section 3.1. These intuitions on overÔ¨Åtting motivate us to propose a novel method named InÔ¨Çuential Rank , which leverages the samples‚Äô inÔ¨Çuence on the decision boundary and on un seen samples to enhance robustness. To this end, we pro pose overÔ¨Åtting score on model (OSM) and overÔ¨Åtting score on data (OSD). OSM measures the inÔ¨Çuence of a trainingarXiv:2106.07217v4  [cs.CV]  19 Apr 2023sample on changes in model parameters, and OSD measures the inconsistency of the sample‚Äôs inÔ¨Çuence on the classi Ô¨Åcation prediction for a small number of clean validation data. Based on OSM and OSD, InÔ¨Çuential Rank updates the trained model by removing high inÔ¨Çuential samples and mitigating their negative inÔ¨Çuence on the classiÔ¨Åer. Since the posttraining provides a new information ( i.e., sample‚Äôs inÔ¨Çuence) to any pretrained models, InÔ¨Çuen tial Rank can effectively improve robustness of existing LNL methods. Through extensive experiments on multiple benchmark data sets, we demonstrate the validity of our method, and show that InÔ¨Çuential Rank can improve the performance of the model consistently whether or not it is pretrained with LNL methods, as shown in Figure 1. Fur thermore, we show that InÔ¨Çuential Rank is useful in two different applications other than LNL. The proposed over Ô¨Åtting scores can be effective for (1) data cleansing that Ô¨Ål ters out erroneous examples in realworld video data and (2)regularization that boosts the classiÔ¨Åcation performance on clean data. Our key contributions can be summarized as follows: ‚Ä¢Posttraining : InÔ¨Çuential Rank is a novel posttraining approach for LNL, which leverages the overÔ¨Åtting scores of training examples on the decision boundary. ‚Ä¢Practicability : InÔ¨Çuential Rank is applicable to any pre trained models and works synergistically with other ex isting LNL methods. ‚Ä¢Extensibility : InÔ¨Çuential Rank can be easily extended to cleansing video dataset and a regularization for reducing overÔ¨Åtting arising from clean but inÔ¨Çuential samples. 2. Related Work "
316,ProbAct: A Probabilistic Activation Function for Deep Neural Networks.txt,"Activation functions play an important role in training artificial neural
networks. The majority of currently used activation functions are deterministic
in nature, with their fixed input-output relationship. In this work, we propose
a novel probabilistic activation function, called ProbAct. ProbAct is
decomposed into a mean and variance and the output value is sampled from the
formed distribution, making ProbAct a stochastic activation function. The
values of mean and variances can be fixed using known functions or trained for
each element. In the trainable ProbAct, the mean and the variance of the
activation distribution is trained within the back-propagation framework
alongside other parameters. We show that the stochastic perturbation induced
through ProbAct acts as a viable generalization technique for feature
augmentation. In our experiments, we compare ProbAct with well-known activation
functions on classification tasks on different modalities: Images(CIFAR-10,
CIFAR-100, and STL-10) and Text (Large Movie Review). We show that ProbAct
increases the classification accuracy by +2-3% compared to ReLU or other
conventional activation functions on both original datasets and when datasets
are reduced to 50% and 25% of the original size. Finally, we show that ProbAct
learns an ensemble of models by itself that can be used to estimate the
uncertainties associated with the prediction and provides robustness to noisy
inputs.","Activation functions add nonlinearity to neural networks making them learn complex functional mappings from data [ 1]. Different activation functions with different characteristics have been proposed. Sigmoid [ 2] and hyperbolic tangent (Tanh) were the popular ones during the early usage of neural networks [ 3] mainly due to their monotonicity, continuity, and bounded properties. In recent times, the RectiÔ¨Åed Linear Unit (ReLU) [ 4] has become an extremely popular activation function for neural networks. Several variants of ReLU have been proposed, e.g., Leaky ReLU [ 5], Parametric ReLU (PReLU) [6], and Exponential Linear Unit (ELU) [7]. However, all of these are deterministic activation functions with Ô¨Åxed inputoutput relationships. In this work, we propose a new activation function, called ProbAct , which is not only trainable but also 1 k_shridhar16@cs.unikl.de |{joonho.lee, hayashi, brian, seokjun.kang, uchida} @human.ait.kyushuu.ac.jp pmehta9@ur.rochester.edu{sheraz.ahmed, andreas.dengel}@dfki.de Preprint. Under review.arXiv:1905.10761v2  [cs.LG]  16 Jun 2020(a) ReLU  (b) ProbAct  (c) Effect of stochastic perturbation Figure 1: Comparison of (a) ReLU and (b) the proposed activation function. (c) is the effect of stochastic perturbation by ProbAct at feature spaces in a neural network. stochastic in nature. The idea of ProbAct is inspired by the stochastic behavior of biological neurons. Noise in neuronal spikes can arise due to uncertain biomechanical effects [ 8]. We try to emulate a similar behavior in the information Ô¨Çow to the neurons by injecting stochastic sampling from a Gaussian distribution to the activations. Consequently, even for the same input value x, the output value from ProbAct varies stochastically ‚Äî a capability conventional activation functions do not offer. The induced perturbations prove to be effective in avoiding overÔ¨Åtting during training, thus yielding better generalizations. Since the operation is a resemblance to feature augmentation, we call it augmentationbyactivation . Furthermore, we show that ProbAct improves the classiÔ¨Åcation accuracy by 23% compared to ReLU or other conventional activation functions on established image datasets and 12% on text datasets. The main contributions of our work are as follows: We introduce a novel activation function, called ProbAct, whose output undergoes stochastic perturbation. We propose a novel method of governing the stochastic perturbation with parameters trained through backpropagation. We show that ProbAct improves the performance on various visual and textual classiÔ¨Åcation tasks while generalizing well on reduced datasets. We also show that the improvement by ProbAct is realized by the augmentationbyactivation, which acts as a stochastic regularizer to prevent overÔ¨Åtting of the network and acts as a feature augmentation method. Finally, we demonstrate that ProbAct learns an ensemble of models by itself, allowing the estimation of predictive uncertainties and robustness to noisy data. 2 Related Work "
542,Learning with Noisy Labels for Robust Point Cloud Segmentation.txt,"Point cloud segmentation is a fundamental task in 3D. Despite recent progress
on point cloud segmentation with the power of deep networks, current deep
learning methods based on the clean label assumptions may fail with noisy
labels. Yet, object class labels are often mislabeled in real-world point cloud
datasets. In this work, we take the lead in solving this issue by proposing a
novel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing
noise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with
the spatially variant noise rate problem specific to point clouds.
Specifically, we propose a novel point-wise confidence selection to obtain
reliable labels based on the historical predictions of each point. A novel
cluster-wise label correction is proposed with a voting strategy to generate
the best possible label taking the neighbor point correlations into
consideration. We conduct extensive experiments to demonstrate the
effectiveness of PNAL on both synthetic and real-world noisy datasets. In
particular, even with $60\%$ symmetric noisy labels, our proposed method
produces much better results than its baseline counterpart without PNAL and is
comparable to the ideal upper bound trained on a completely clean dataset.
Moreover, we fully re-labeled the validation set of a popular but noisy
real-world scene dataset ScanNetV2 to make it clean, for rigorous experiment
and future research. Our code and data are available at
\url{https://shuquanye.com/PNAL_website/}.","In recent years, the development of deep neural networks (DNNs) has led to great success in 3D point cloud segmen tation [11, 36, 32]. Thanks to the powerful learning capac ity, once highquality annotations are given, DNNsbased point segmentation methods can achieve remarkable perfor mance. However, such high learning capacity is a double edged sword, i.e., it can also overÔ¨Åt label noise and incur *Jing Liao is the corresponding author.performance degradation if there are incorrect annotations. In fact, compared to annotating 2D images, clean 3D data labels are more difÔ¨Åcult to obtain. It is mainly be cause: 1) the point number to annotate is often very mas sive, e.g., million scale in annotating a typical indoor scene in ScanNetV2 [6]; 2) the annotating process is inherently more complex and requires more expertise for the annota tors, e.g., constantly changing the view, position and scale to understand the underlying 3D structure. As a result, even the commonly used 3D scene dataset ScanNetV2 [6], which is already a version after reÔ¨Åning the label from the Scan Net, has a large portion of label noise, as shown in Figure 1. Based on the above considerations, there is an urgent need to study how to learn with noisy labels for robust point cloud segmentation. However, to the best of our knowl edge, most research works about learning with noisy la bels focus on image classiÔ¨Åcation, and no previous study exists for point cloud segmentation. More importantly, such works designed for image recognition cannot be di rectly applied to point cloud segmentation. For example, among the most popular methods, sample selection meth ods [12, 33, 24, 26, 17] often assume that the noise rate of all samples is a known constant value. However, the noise rates are often unknown and variable. Robust loss function methods [35, 30] cannot achieve consistent noise robustness to large noise rates. Whereas label correction methods [23, 26, 1] are designed to correct for imagelevel label noise, the point cloud segmentation task requires to correct pointlevel noises. Considering that the point labels within each instance are strongly correlated, directly apply ing these methods to each point independently without con sidering the local correlation is suboptimal. In this paper, we present a novel point noiseadaptive learning (PNAL) framework, which is the Ô¨Årst attempt to empower the point cloud segmentation model with re sistance to annotation noise. SpeciÔ¨Åcally, to cope with unknown, possibly heavy, and varying noise rates, we designed a pointlevel conÔ¨Ådence selection mechanism, which obtains reliable labels based on the historical predic tions of each point without requiring a known noise rate. Next, in order to fully utilize the local correlation amongarXiv:2107.14230v2  [cs.CV]  5 Aug 2021chairbed floorcabinet tabletable desk bookshelf deskInput Scenes RealWorld Noisy GT Labels Predictions of PNAL Figure 1. Illustration of the instancelevel label noise concept in point cloud segmentation. From left to right are the input (noisy instances highlighted red boxes), the manual annotation given by the realworld dataset ScanNetV2, and the prediction of PNAL (more in line with the real category). It is noticeable that this popular dataset suffers from label noise, such as mislabeling the Ô¨Çoor as a chair, even that it is already a relabeled version of ScanNet. Our PNAL framework is trained on this noisy dataset but still achieves correct predictions. labels, we propose a label correction process performed at cluster level . This is done by the proposed voting strat egythat tries to merge reliable labels from relevant points to provide the best possible label for each point cluster, with acomputationally efÔ¨Åcient implementation. To demonstrate the effectiveness of our PNAL, we com pare the proposed framework with various possible base lines based on different network backbones on the syn thetic noisy label dataset from stanford largescale 3d in door spaces (S3DIS) [2], which shows the great advantage of PNAL on both performance and efÔ¨Åciency. On the real world noisy dataset ScannetV2 [6], we notice that both its training and validation set suffers from the noisy label is sue. Therefore, we not only conducted experiments on the original training and validation set. Also, for a more rig orous evaluation, we reÔ¨Åne the validation set by manually correcting the noisy labels and evaluate PNAL on this clean set. These results indicate that PNAL is also robust to real world noise. To further explore PNAL, a complete ablation study, training process analysis, and robustness test were also performed. To summarize, our contributions are fourfold. ‚Ä¢ To the best of our knowledge, this is the Ô¨Årst work in vestigating noisy labels on point cloud data, which has a wide and urgent need for 3D applications where the volume of data is growing drastically. ‚Ä¢ A novel noiserate blind PNAL framework is proposed to handle spatially variant noise rates in point cloud. It consists of pointlevel conÔ¨Ådence selection, cluster level label correction with voting mechanism, and can be easily applied to different network architectures. ‚Ä¢ Extensive experiments are conducted to show the clear improvements by PNAL, on both synthetic and realworld noisy label datasets. ‚Ä¢ We relabeled the validation set of ScanNetV2 by cor recting noises and will make it public to facilitate both point cloud segmentation and noise label learning. 2. Related Work "
237,RetiNet: Automatic AMD identification in OCT volumetric data.txt,"Optical Coherence Tomography (OCT) provides a unique ability to image the eye
retina in 3D at micrometer resolution and gives ophthalmologist the ability to
visualize retinal diseases such as Age-Related Macular Degeneration (AMD).
While visual inspection of OCT volumes remains the main method for AMD
identification, doing so is time consuming as each cross-section within the
volume must be inspected individually by the clinician. In much the same way,
acquiring ground truth information for each cross-section is expensive and time
consuming. This fact heavily limits the ability to acquire large amounts of
ground truth, which subsequently impacts the performance of learning-based
methods geared at automatic pathology identification. To avoid this burden, we
propose a novel strategy for automatic analysis of OCT volumes where only
volume labels are needed. That is, we train a classifier in a semi-supervised
manner to conduct this task. Our approach uses a novel Convolutional Neural
Network (CNN) architecture, that only needs volume-level labels to be trained
to automatically asses whether an OCT volume is healthy or contains AMD. Our
architecture involves first learning a cross-section pathology classifier using
pseudo-labels that could be corrupted and then leverage these towards a more
accurate volume-level classification. We then show that our approach provides
excellent performances on a publicly available dataset and outperforms a number
of existing automatic techniques.","By and large, Optical Coherence Tomography (OCT) has reshaped the Ô¨Åeld of ophthalmology ever since its inception in the early 90s [1]. At its core, OCT uses infraredlight interferometry to image through tissue in order to characterize anatomical structures beyond their surface. Given its simplic ity, affordability and safety, it is no surprise that its use has gained widespread popularity for both disease diagnosis and treatment. Similarly, its use has gained traction in other medical Ô¨Åelds such as for histopathology and skin cancer analysis [2]. Indeed, with an ability to image the posterior part of the eye in 3D ( e.g. the retina) at micrometer resolution, OCT imaging now allows for visualization of most retinal layers [3, 4] and more impor tantly, numerous pathological markers, such as intraretinal Ô¨Çuid, drusens or cysts [5, 6]. As illus trated in Fig. 1, such markers can be observed in OCT crosssectional images, or Bscans and have S. Apostolopoulos and R. Sznitman are with the ARTORG Center, University of Bern, Switzerland. E mail: Ô¨Årstname.lastname@artorg.unibe.ch yC. Ciller is with the Radiology Department, CIBM, Lausanne University and University Hospital, Lau sanne and with the Ophthalmic Technology Group, ARTORG Center Univ. of Bern, Switzerland zS. De Zanet is with the Ecole Polytechnique Federale de Lausanne, Switzerland. xS. Wolf is with the Bern University Hospital, Inselspital, Switzerland. 1arXiv:1610.03628v1  [cs.CV]  12 Oct 2016Figure 1: An example of a Bscan crosssection of a patient with AMD in the foveal pit area. Visible are the multiple retinal layers, including the Retinal Pigment Epithelium (RPE) and Bruch¬¥s Membrane (BM). The latter is perturbed with drusen , which manifest as bumps disrupting this continuous layer. been linked to a number of eye conditions, including AgeRelated Macular Degeneration (AMD) and Diabetic Retinopathy (DR) which currently affect over 8.7% of the world population and 159 million people worldwide, respectively [5, 7, 8]. Moreover, these pathologies are the major cause of blindness in developed countries [9]. Alarmingly, the number of people with either of these dis eases is projected to skyrocket, with AMD affecting an estimated 196 million people by 2020 and 288 million people by 2040 [8]. Genetic factors, race, smoking habits and the ever growing world population are responsible for this pathology growth [10]. While OCT has gained signiÔ¨Åcant importance in recent years for AMD and DR screening [11, 12], the process to do so remains time consuming however. In effect, 3D OCT volumes, also referred to as Cscans , are comprised of 50100 crosssectional Bscans. Traditionally, inspection of each Bscan is necessary in order to properly ruleout most retinal diseases. This process is particularly tedious not only due to its timeconsuming nature, but also due to the multiple crosssections that need to be inspected simultaneously to identify elusive and scarce traces of earlystage ocular diseases. In this context, automated algorithms for pathology identiÔ¨Åcation in OCT volumes would be of great beneÔ¨Åt for clinicians and ophthalmologists, as access to OCT devices becomes common and nationwide screening programs commence [13]. Recently, research has given way to a variety of image processing methods for OCT imaging. Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27]. More speciÔ¨Åc to pathology identiÔ¨Åcation, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of rel evant pathological markers [28, 29, 30, 31, 32] or classiÔ¨Åcation of 2D Bscans or 3D C scans [33, 34, 32, 35, 36]. While effective to some extent, most of these works have leveraged Bscan level groundtruth information in order to learn classiÔ¨Åcation functions. These more detailed labels are unfortunately often not available and as such, limit the usability of these solutions. To this end, we present a new strategy towards automatic pathology identiÔ¨Åcation in OCT Cscans using only volume level annotations. To do this, we introduce a novel Convolution Neural Network (CNN) architecture, named RetiNet , that directly estimates the state of a Cscan solely using the image data and without needing additional information. At its core, our approach uses (1) a task speciÔ¨Åc volume preprocessing strategy where we Ô¨Çatten and normalize the data in an OCTspeciÔ¨Åc manner, (2) we then train a 2D Bscan CNN using pseudolabels that could be corrupted in order to prelearn Ô¨Ålters that respond to relevant image features and (3) reuse the learned features in a Cscan level CNN that takes a mosaic of Bscans as input and classiÔ¨Åes the entire Cscan at once. Using a publicly available OCT dataset [5], we show that our approach is highly effective at separating AMD 2from control subjects and outperforms existing stateoftheart methods for image classiÔ¨Åcation. In addition, we not only show that RetiNet outperforms excellent recent networks from the computer vision literature trained from scratch, but also surpasses the performance of stateoftheart pre trained networks with adapted Ô¨Ålters. Last, we show how our approach provides high performances in terms accuracy, learning pathologyspeciÔ¨Åc Ô¨Ålters capable to identifying pathological markers effectively. The remainder of this article is organized as follows: The following section discusses the relevant related work. Sec. 3 then describes in detail our approach and the RetiNet architecture. Following this, we describe our experimental section and the evaluation of several baseline strategies in Sec. 4. We then conclude with Ô¨Ånal remarks in Sec. 5. 2 Related Work "
115,Local Multi-Label Explanations for Random Forest.txt,"Multi-label classification is a challenging task, particularly in domains
where the number of labels to be predicted is large. Deep neural networks are
often effective at multi-label classification of images and textual data. When
dealing with tabular data, however, conventional machine learning algorithms,
such as tree ensembles, appear to outperform competition. Random forest, being
a popular ensemble algorithm, has found use in a wide range of real-world
problems. Such problems include fraud detection in the financial domain, crime
hotspot detection in the legal sector, and in the biomedical field, disease
probability prediction when patient records are accessible. Since they have an
impact on people's lives, these domains usually require decision-making systems
to be explainable. Random Forest falls short on this property, especially when
a large number of tree predictors are used. This issue was addressed in a
recent research named LionForests, regarding single label classification and
regression. In this work, we adapt this technique to multi-label classification
problems, by employing three different strategies regarding the labels that the
explanation covers. Finally, we provide a set of qualitative and quantitative
experiments to assess the efficacy of this approach.","Multilabel classiÔ¨Åcation is a popular machine learning task, concerned with assigning multiple different labels to a single sample [ 1]. There are plenty of applications employing multilabel classiÔ¨Åcation, such as semantic indexing [ 2] and object detection [ 3]. Multilabel classiÔ¨Åcation has also proven useful in the predictive maintenance [ 4] and Ô¨Ånancial services [ 5] sectors, where tabular data are mainly used. When this sort of data is available, ensemble methods are typically outperforming other families of methods [ 6,7]. Ensembles, however, are intrinsically not explainable. This is an important weakness, as explainability is useful for the vast majority of ML applications, and a necessity when they impact human lives or incur economic costs [8, 9]. This paper focuses on the explainability of random forest (RF) [10] models in the context of multilabel classiÔ¨Åcation. There exists a lot of work on the explainability of RF for regression and single label classiÔ¨Åcation tasks [ 11,12,13, 14]. However, adapting these methods to multilabel tasks, where RF models Ô¨Ånd frequent use [ 15,16,17], is not straightforward. There are also techniques that have been speciÔ¨Åcally designed for multilabel tasks [ 18,19]. These are, however, independent of the explained model‚Äôs architecture, and therefore cannot exploit the speciÔ¨Åc properties of RF models to their beneÔ¨Åt.arXiv:2207.01994v1  [cs.LG]  5 Jul 2022APREPRINT  JULY 6, 2022 To address the lack of RFspeciÔ¨Åc explainability techniques for multilabel classiÔ¨Åcation in the literature, we propose an extension of LionForests (LF) [ 14] towards explaining multilabel classiÔ¨Åcation decisions. We introduce three different strategies concerning the scope of the provided explanation (single label, predicted labelset, label subsets). We compare these strategies against similar stateoftheart techniques, through a set of quantitative and qualitative experiments. The results highlight the conciseness of the explanations of the proposed approach. The rest of this paper is organized as follows. Section 2 discusses relevant research, while Section 3 introduces important concepts of the LF method. Section 4 presents the three novel strategies for explaining multilabel RFs. The experimental procedure, along with the data sets used, and the results, are mentioned in Section 5. Finally, we conclude and propose future steps for this research in Section 6. 2 Related work "
297,A Novel Self-Supervised Re-labeling Approach for Training with Noisy Labels.txt,"The major driving force behind the immense success of deep learning models is
the availability of large datasets along with their clean labels.
Unfortunately, this is very difficult to obtain, which has motivated research
on the training of deep models in the presence of label noise and ways to avoid
over-fitting on the noisy labels. In this work, we build upon the seminal work
in this area, Co-teaching and propose a simple, yet efficient approach termed
mCT-S2R (modified co-teaching with self-supervision and re-labeling) for this
task. First, to deal with significant amount of noise in the labels, we propose
to use self-supervision to generate robust features without using any labels.
Next, using a parallel network architecture, an estimate of the clean labeled
portion of the data is obtained. Finally, using this data, a portion of the
estimated noisy labeled portion is re-labeled, before resuming the network
training with the augmented data. Extensive experiments on three standard
datasets show the effectiveness of the proposed framework.","The success of deep learning models like Alexnet [15], VGG [32], ResNet [12], etc. for image classiÔ¨Åcation tasks can be attributed to the availability of large, annotated datasets like ImageNet [16]. But, obtaining clean annotations of large datasets is very expensive, and thus recent research has focused on learning from weakly supervised data, where the labels are often noisy , since they have been acquired from web searches [18] or crowdsourcing [34]. The presence of noisy labels can severely degrade the performance of the learned classiÔ¨Åers, since deep neural networks can even overÔ¨Åt on the noisy labels with sufÔ¨Åcient training, due to their memorization capability [37] [3]. Recently, several approaches have been proposed to handle label noise in the training data [37] [11] [35] [13] [20] [10] [28]. One direction to address this problem is to estimate the noise transition matrix itself by utilizing an additional softmax layer [10] for modeling the channel noise. An alternative twostep approach along the same lines is proposed in [28]. However, it is observed that estimation of the noise transition matrix is often hard and * equal contribution. c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:1910.05700v3  [cs.CV]  1 Jan 20202DEVRAJ: SELFSUPERVISED RELABELING METHOD FOR HANDLING NOISY LABELS computationally expensive, especially when a large number of classes are present in the data [11]. A more recent and elegant direction to handle label noise is to utilise the concept of small loss instances [13] [30] [11]. Here, the algorithms estimate which samples are correctly labeled, and uses them to train the network subsequently. MentorNet [13] uses a pretrained network (trained in a selfpaced manner with a curriculum loss) to select clean labeled data to train the Ô¨Ånal classiÔ¨Åcation model. Coteaching [11] trains two networks in parallel and updates the weights of the networks using only the small loss samples. In addition, the gradients of the two networks are switched during the parameter update, which leads to better performance. It is observed [11] that when the training continues for a long time, the two networks generally converge to the same state and start performing similarly, leading to the accumulation of errors. Here, we propose a novel framework based on the Coteaching approach [11], which also uses the concept of smallloss instances along with selfsupervision and relabeling, to signiÔ¨Åcantly improve the training of deep networks with very noisy training data. The proposed approach has four main steps. First, to deal with signiÔ¨Åcant amount of label noise, we utilize selfsupervision as a pretraining step, so that the network can learn robust features without the use of any labels. Second, we use a parallel network architecture similar to the one in [11] to get an estimate of the small loss samples. In the third step, utilizing a portion of the small loss samples, the class means for all the categories are computed, which are then used to relabel the large loss samples. Finally, the network training is resumed by taking all the small loss samples along with a portion of the relabeled large loss samples based on a conÔ¨Ådence measure. The proposed framework is termed as mCTS2R (modiÔ¨Åed coteaching with selfsupervision and relabeling). The main contributions of this work are as follows: We develop a novel approach by utilizing selfsupervision and relabeling of the large loss data using the small loss samples for training deep networks under signiÔ¨Åcant label noise. Our framework uses two parallel networks only to determine the small loss instances. Unlike [11] [20] which requires two networks for the entire training, the Ô¨Ånal training of the proposed framework after relabeling proceeds using a single network. This makes our model computationally much lighter. We propose using a selfsupervised training paradigm like [9] to learn about the data distribution without using the labels, which helps to avoid overÔ¨Åtting on the noisy labels. Extensive experiments on three benchmark datasets show the effectiveness of the pro posed framework. 2 Related Work "
76,Learning Scene Flow in 3D Point Clouds with Noisy Pseudo Labels.txt,"We propose a novel scene flow method that captures 3D motions from point
clouds without relying on ground-truth scene flow annotations. Due to the
irregularity and sparsity of point clouds, it is expensive and time-consuming
to acquire ground-truth scene flow annotations. Some state-of-the-art
approaches train scene flow networks in a self-supervised learning manner via
approximating pseudo scene flow labels from point clouds. However, these
methods fail to achieve the performance level of fully supervised methods, due
to the limitations of point cloud such as sparsity and lacking color
information. To provide an alternative, we propose a novel approach that
utilizes monocular RGB images and point clouds to generate pseudo scene flow
labels for training scene flow networks. Our pseudo label generation module
infers pseudo scene labels for point clouds by jointly leveraging rich
appearance information in monocular images and geometric information of point
clouds. To further reduce the negative effect of noisy pseudo labels on the
training, we propose a noisy-label-aware training scheme by exploiting the
geometric relations of points. Experiment results show that our method not only
outperforms state-of-the-art self-supervised approaches, but also outperforms
some supervised approaches that use accurate ground-truth flows.","Scene Ô¨Çow estimation is to capture 3D motions of dy namic scenes, which is important for many applications such as robotics and autonomous driving. Recently, directly estimating scene Ô¨Çow from point clouds has received in creasing attention. Nevertheless, it is challenging to esti mate scene Ô¨Çow from point clouds, due to the sparsity and nonuniform density of point clouds. Typical approaches [11, 35, 47, 50] estimate scene Ô¨Çow from point clouds by training neural networks in a fully su pervised manner, which relies on groundtruth scene Ô¨Çow annotation. However, it is expensive and timeconsuming to acquire groundtruth scene Ô¨Çows for realworld point clouds, since such annotations usually need to annotate 3D Figure 1. Illustration of our main idea . We leverage multi modality data ( i.e. monocular RGB images Iand point clouds P) to generate pseudo scene Ô¨Çow labels for point clouds, such that the reliance on groundtruth scene Ô¨Çow is circumvented. With the pseudo labels, our method trains a scene Ô¨Çow network to estimate scene Ô¨Çow from point clouds. motions for every point of a point cloud. To alleviate this is sue, researchers resort to training the scene Ô¨Çow network on labeled synthetic data. However, these methods are limited in the effectiveness and generalization ability in realworld applications due to the domain gap between the synthetic data and realworld data. Alternatively, some selfsupervised methods [21, 24, 31, 52] train the scene Ô¨Çow network by constructing pseudo scene Ô¨Çow labels from point clouds. For example, Mittal et al. [31] approximated pseudo scene Ô¨Çow labels based on the coordinate differences of 3D points, where the closest points to the next point cloud are treated as pseudo cor respondence. These methods circumvent the reliance on groundtruth scene Ô¨Çows. However, they fail to achieve competitive performance compared with fully supervised approaches. To achieve competitive performance without the need for groundtruth scene Ô¨Çow, we seek to generate highquality pseudo scene Ô¨Çow labels for training scene Ô¨Çow networks. However, it is nontrivial to establish highquality pseudo Ô¨Çow labels from the point cloud itself since a raw point cloud consists of only sparse point coordinates. Can we jointly leverage multimodality data ( i.e. monocular RGB images and point clouds) to generate pseudo scene Ô¨Çow la bels for point clouds ? (see Fig. 1). Different from pointarXiv:2203.12655v1  [cs.CV]  23 Mar 2022clouds, monocular images contain rich information such as object appearance and detailed texture information. Such rich information provides discriminative cues for estimating 2D motions, which can be used to facilitate pseudo scene Ô¨Çow label generation. The training dataset only needs to additionally provide RGB images captured by an afford able monocular camera, which is more accessible com pared with expensive scene Ô¨Çow annotation. Different from these methods [21, 24, 31, 52], our method leverages multi modality data for generating pseudo scene Ô¨Çow labels. However, it is challenging to generate pseudo scene Ô¨Çow labels. We can not directly infer 3D motion from the 2D motion only relying on monocular images, despite the rich information of monocular images. To address this issue, we propose a multimodalitybased pseudo scene Ô¨Çow genera tion module, through decomposing the 3D motion of a point into one 2D motion in the XY direction and another 1D motion in the Z direction. We thereby can leverage monoc ular images to capture the 2D motions of points in the im age plane and use point clouds to lift estimated 2D motions to pseudo scene Ô¨Çow labels ( i.e. 3D motions). With the generated pseudo labels, our method can train scene Ô¨Çow networks on point clouds without groundtruth scene Ô¨Çow. Another challenge is that our pseudo scene Ô¨Çow labels are inevitably noisy, compared with groundtruth ones, due to imperfect 2D motion estimation results. The noisy la bels would negatively affect the training of the scene Ô¨Çow network, leading to the degradation of estimation accuracy. To address this issue, we propose a noisylabelaware learn ing scheme for scene Ô¨Çow estimation. Our scheme exploits the geometric information of point clouds to detect inaccu rate labels in a soft manner. With the conÔ¨Ådence scores of pseudo labels, we construct a training loss that less relies on pseudo labels with lower conÔ¨Ådences when training scene Ô¨Çow networks. The main contributions of our work are summarized as follows: (1)We propose a novel method that trains the scene Ô¨Çow network without relying on groundtruth scene Ô¨Çow. (2)We show how to leverage multimodality data, i.e. point clouds and monocular images, for generating pseudo scene Ô¨Çow labels. (3)Our noisylabelaware learning scheme effectively trains the scene Ô¨Çow network by reducing the negative effect of the inherent noise of pseudo labels dur ing training. (4)Experiment results show that our method not only outperforms stateoftheart selfsupervised ap proaches but also outperforms some supervised approaches that use accurate groundtruth annotations, even though our pseudo labels are noisy and inferior. (5)The pseudo scene Ô¨Çow labels allow us to train the scene Ô¨Çow network on large scale realworld LiDAR data without groundtruth scene Ô¨Çow, reducing the domain gap.2. Related Work "
317,Membership Inference Attack for Beluga Whales Discrimination.txt,"To efficiently monitor the growth and evolution of a particular wildlife
population, one of the main fundamental challenges to address in animal ecology
is the re-identification of individuals that have been previously encountered
but also the discrimination between known and unknown individuals (the
so-called ""open-set problem""), which is the first step to realize before
re-identification. In particular, in this work, we are interested in the
discrimination within digital photos of beluga whales, which are known to be
among the most challenging marine species to discriminate due to their lack of
distinctive features. To tackle this problem, we propose a novel approach based
on the use of Membership Inference Attacks (MIAs), which are normally used to
assess the privacy risks associated with releasing a particular machine
learning model. More precisely, we demonstrate that the problem of
discriminating between known and unknown individuals can be solved efficiently
using state-of-the-art approaches for MIAs. Extensive experiments on three
benchmark datasets related to whales, two different neural network
architectures, and three MIA clearly demonstrate the performance of the
approach. In addition, we have also designed a novel MIA strategy that we
coined as ensemble MIA, which combines the outputs of different MIAs to
increase the attack accuracy while diminishing the false positive rate.
Overall, one of our main objectives is also to show that the research on
privacy attacks can also be leveraged ""for good"" by helping to address
practical challenges encountered in animal ecology.","In animal ecology, the ability to reidentify (reID) an indi vidual animal across multiple encounters allows for addressing a broad range of questions such as ecosystem function, community, and population dynamics as well as behavioral ecology [2, 27]. In many cases, especially for aquatic species such as marine mammals, reID requires extensive training and practical experience for a human to acquire sucient expertise to be able to accurately recognize a particular indi vidual. To partially circumvent this issue, biologists usually rely on approaches such as tagging and photoidentication (photoID) [59, 27]. While accurate, the tagging approach is intrusive to animals and is often expensive and laborious. In contrast, the photoID approach uses visual identication from camera images ( e.g., handheld camera, camera trap, or drones), which is noninvasive for animals and has a lower cost. Nonetheless, there are some practical and methodolog ical challenges associated with its use. First, even among experienced researchers, there is a nonnegligible chance of human error and bias when reviewing photos [17]. Second, it is also timeconsuming and expensive in terms of human involvement to manually lter through thousands of images. To overcome these limitations, one possible strategy is to rely on computer vision techniques to standardize and au tomatize the animal reID process [54]. To realize this, for decades, \feature engineering"", which can be dened as the process of selecting or transforming raw data into informa tive features, has been the most commonly used technique. Basically, it means that most of the algorithms for animal reID are designed and implemented to focus exclusively on predetermined traits, such as patterns of spots or stripes, to discriminate among individuals. However, feature engineer ing requires programming experience, sucient familiarity with the species considered to identify relevant features. In addition, this approach lacks in generality as once a feature detection algorithm has been designed for one species, it is unlikely to be useful for others [21]. More recently, the last decade has witnessed the emer gence of deep learning systems that make use of large data volumes to automatically learn discriminative features [34]. In particular, Convolutional Neural Networks (CNNs) have achieved stateoftheart results in a variety of uses cases based on the assumption of a closed world ( i.e., a xed number of classes/identities), However, CNNs are known to lack robustness when deployed in realworld classica tion/recognition applications, in which incomplete knowl 1arXiv:2302.14769v1  [cs.CV]  28 Feb 2023edge of the world during training result in unknown classes being submitted to the model during testing. This corre sponds for instance to the situation in which when used in the wild, the model will have to recognize individuals that it has not seen during training. In marine ecology, one of the main challenges related to animal reID, such as wild whales, is the encounter of large populations in which there is frequently the addition of new individual appearing due to birth or migration, therefore creating an \openset"" setting [52] wherein the identity model must deal with \classes"" ( i.e., individuals) unseen during training. Thus, a desirable feature for an animal reID ap proach is to have the ability to identify not only animals that belong to the catalog but also recognize new individuals (i.e., previously unknown animals). To address this issue, we investigate the use of Membership Inference Attacks (MIA), which is a form of privacy leakage in which the objective of the adversary is to decide whether a given data sample was in a machine learning model's training dataset [55, 62, 50, 39, 32, 12]. Knowing that a specic data sample was used to train a particular model may lead to potential privacy breaches if for instance this membership reveals a sensitive characteristic ( e.g., being part of the cohort of patients hav ing a particular disease or being a member of a vulnera ble group). The gist of our approach is we could leverage on a MIA to discriminate whether a new beluga whale was present or not in the training set. Then, this information can be used in the reID pipeline to take the decision to clas sify known individuals or to add a new entry in the catalog for an unknown individual. To summarize, in this paper our main contribution is the proposition of a novel approach for whales discrimina tion through images (photoID), which relies on the use of MIAs. In particular, one of our objective is to show that by drawing on the signicant body of work on MIAs, it is possible to eciently address the \openset"" vs \closed set"" problem. To demonstrate this, extensive experiments have been conducted with three stateoftheart MIAs that leverage dierent information produced by the model ( i.e., prediction condence, predicted and ground truth label, or both of them) as well as dierent attack strategies (neu ral networkbased, metricbased and querybased attacks). More precisely, we have performed a comprehensive mea surement of the success of MIAs to address the openset problem over two model architectures (ResNet50 [19] and DenseNet121 [23]), three benchmark image datasets related to whales species (GREMM [37], Humpback [15, 9] and NOAA [40]) along with three stateoftheart MIAs, namely Yeom et al. [62], Salem et al. [50] and LabelOnly [14], thus building a total of 36 attack scenarios. In addition, previ ous works [55, 50, 14] assume the leak information is more likely for machine learning models on the in uence of over tting, we ensure this assumption by evaluating overtted and nonovertted models while monitoring the false pos itive rate as recommended in [8, 48] for the reliability of the results. Finally, we introduced a novel attack design for whale discrimination, which we coined as ensemble MIAs, which combines the outputs of dierent MIAs to increase the attack accuracy while decreasing the false positive rate. The outline of the paper is as follows. First in Section 2, we review the relevant background on automated photo iden tication systems as well as on membership inference at tack. Then in Section 3, we describe the St. Lawrencebeluga whale reid pipeline from side pictures, the training of the attack model as well as the dierent MIA strategies that we propose to implement the discrimination between known and unknown belugas. Afterwards in Section 4, we present the experimental setting used to evaluate our ap proach, which includes the datasets, the experimental con guration as well as the target and attack models. Finally in Section 5, we report on the performance of the approach under dierent scenarios, before discussing how the attack can generalize to dierent settings as well as the factors in  uencing its success and its robustness before concluding in Section 6. 2 Related Work "
367,How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?.txt,"Incorrectly labeled examples, or label noise, is common in real-world
computer vision datasets. While the impact of label noise on learning in deep
neural networks has been studied in prior work, these studies have exclusively
focused on homogeneous label noise, i.e., the degree of label noise is the same
across all categories. However, in the real-world, label noise is often
heterogeneous, with some categories being affected to a greater extent than
others. Here, we address this gap in the literature. We hypothesized that
heterogeneous label noise would only affect the classes that had label noise
unless there was transfer from those classes to the classes without label
noise. To test this hypothesis, we designed a series of computer vision studies
using MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous
label noise during the training of multi-class, multi-task, and multi-label
systems. Our results provide evidence in support of our hypothesis: label noise
only affects the class affected by it unless there is transfer.","Supervised deep learning models have been successful in various tasks such as large scale image classiÔ¨Åcation, object detection, semantic segmentation, and many more [16,24,26]. One of the signiÔ¨Åcant contributions behind the success of supervised deep learning is the availability of welllabeled large datasets. However, such welllabeled datasets are only available for a handful of problems [8,19]. Often tools like Amazon Mechanical Turk [6] and Computer Vision Annotation Tool (CV AT) [3] are used to la bel them. The problem with these tools is that they are expensive and require signiÔ¨Åcant time and human effort to label. To circumvent that, many datasets in the real world are either incompletely labeled or extracted from sources that inherently contain label noise [9]. Label noise is detrimental to the training of any deep learning model as it directly impacts the model‚Äôs learning ability [37]. Vision tasks learned with noisy labels don‚Äôt generalize well, resulting in poor test performance[33]. It is essential to thoroughly study the impact of noisy labels to understand how they are associated with poor per formance. This knowledge can be used to improve the current methods that learn witharXiv:2106.15475v3  [cs.CV]  26 Sep 20212 B. Khanal et al. Fig. 1: An example of how classdependent heterogeneous label noise is introduced by corrupting the labels of CIFAR10 and MNIST Dataset. We investigated the impact of noisy labels (red) on the model‚Äôs performance on clean labels (green). noisy labels [18]. However, to the best of our knowledge, most of the works studied up to now have mainly focused on examining the performance of the deep learning model under the inÔ¨Çuence of homogeneous noisy labels imposed by corrupting all the true labels with the same degree [4,27]. We know that the noise may not always be homogeneous and can depend on various heterogeneous sources [35]. Some of the labels might be affected to a greater extent than others because of which the label noise is heterogeneous in nature. The previous studies have not thoroughly investigated the heterogeneous case in supervised vision tasks. Therefore, some open questions still exist. For example: what is the impact of heterogeneous noisy labels of certain classes on the performance of a class with clean labels (as shown in Fig 1) when they are trained together in a naive classiÔ¨Åcation setting? We want to examine to what extent the noisefree class is affected by classdependent noisy labels. We further extended the question to study the impact in other classiÔ¨Åcation settings, such as multitask and multilabel learning. Multitask learning[1] is an approach where a single network is trained to perform two or more tasks. While training a multitask network, the tasks could positively or negatively interfere resulting in positive or negative transfer respectively. Positive trans fer improves the performance of another task, while negative transfer impacts its perfor mance. We hypothesized that if there is a positive transfer between two or more tasks, then training with noisy tasks should impact the performance of clean tasks. The trans ferred beneÔ¨Åt obtained by training tasks together should drop with an increase in label noise in helping tasks. In this work, we veriÔ¨Åed our hypothesis with experiments. Finally, we also investigated the impact of labeldependent noisy labels in multi label learning, in which noise is present only in a certain group of labels, and examined the impact on the group with clean labels. In summary, the key contributions of our work are:How Does Heterogeneous Label Noise Impact Generalization in Neural Nets? 3 Using the popular vision datasets: MNIST, CIFAR10, CIFAR100, and MSCOCO dataset, we assessed the impact of classdependent, taskdependent, and labeldependent heterogeneous noisy labels on multiclass classiÔ¨Åcation, multitask learning, and multi label learning settings, respectively with an attempt to the Ô¨Åll gap that previous studies didn‚Äôt cover. By investigating taskdependent heterogeneous noisy labels, we showed that if there is a positive transfer from one task to another, inducing label noise in helping task should also impact the performance of other tasks that have clean labels. The drop in task transfer beneÔ¨Åt is proportional to the number of noisy labels in helping tasks, i.e., the higher the noisy labels, the higher the transfer drop. 2 Related Works "
483,Towards Federated Learning against Noisy Labels via Local Self-Regularization.txt,"Federated learning (FL) aims to learn joint knowledge from a large scale of
decentralized devices with labeled data in a privacy-preserving manner.
However, since high-quality labeled data require expensive human intelligence
and efforts, data with incorrect labels (called noisy labels) are ubiquitous in
reality, which inevitably cause performance degradation. Although a lot of
methods are proposed to directly deal with noisy labels, these methods either
require excessive computation overhead or violate the privacy protection
principle of FL. To this end, we focus on this issue in FL with the purpose of
alleviating performance degradation yielded by noisy labels meanwhile
guaranteeing data privacy. Specifically, we propose a Local Self-Regularization
method, which effectively regularizes the local training process via implicitly
hindering the model from memorizing noisy labels and explicitly narrowing the
model output discrepancy between original and augmented instances using self
distillation. Experimental results demonstrate that our proposed method can
achieve notable resistance against noisy labels in various noise levels on
three benchmark datasets. In addition, we integrate our method with existing
state-of-the-arts and achieve superior performance on the real-world dataset
Clothing1M. The code is available at https://github.com/Sprinter1999/FedLSR.","The pervasiveness of large end devices (e.g. personal mobile phones and IoT devices), has contributed to the drastically increasing scale of data generating from distributed clients among the network. Federated learning (FL) is a new distributed learning paradigm that enables a global model to be trained collaboratively by multiple clients while keeping the private data decentralized on devices. As shown in Figure 1, conventional FL [ 42] process mainly con sists of two periods: 1) the selected clients perform local training process on private labeled data after obtaining the distributed global model, and then upload the updated local trained models to the server; 2) the server synchronizes and aggregates these local trained models to obtain the updated global model for next round. This above process continues until the global model converges or within limited communication rounds. Many existing works towards FL have achieved wide success in dealing with four main challenges when deploying a practical FL system, including statistical heterogeneity, systems heterogeneity, communication efficiency, and privacy concerns as referred in [ 35]. But most existing works are based on an important assumption that the raw data owned by clients are perfectly labeled. In practice, it is hard to guarantee the label correctness of collected training data, since highquality annotations requires expensive human intelligence and efforts. Moreover, data are very likely to contain incorrect labels (a.k.a noisy labels) in FL, since labels are usually produced independently by clients with various labelgenerating methods, such as filtering images‚Äô surrounding context [ 34] or exploiting machinegenerated labels [28]. Data with noisy labels inevitably cause the model performance degradation. In detail, Arpit et al. propose the memorization ef fect of deep network [ 1] which indicates that data with correct labels fit before data with incorrect labels, thus the model perfor mance first rises up and then gradually drops during the train ing process. Although learning on data with noisy labels [ 45] has been widely studied in the data centralized setting, most exist ing works cannot be straightforwardly applied to FL, due to un bearable computation burden and exorbitant communication over head for resourceconstrained devices. For example, [ 23,33,47] perform computationheavy procedures for noise cleaning [ 34] , which brings nonnegligible synchronization cost during conduct ing the serverside model aggregation, and thus negatively affects global model convergence. While other methods cannot satisfy some unique characteristics of FL. For example, [ 38] requires di rect access to all data samples in the early learning process, but the server only selects a small fraction of clients to conduct local training on these clients‚Äô private data in FL.arXiv:2208.12807v1  [cs.LG]  25 Aug 2022CIKM ‚Äô22, October 17‚Äì21, 2022, Atlanta, GA, USA. Xuefeng Jiang, Sheng Sun, Yuwei Wang, and Min Liu Cloud Server global modelglobal model local datasetlocal datasetglobal model local datasetlocal trained model local  trained  model local trained model Figure 1: An illustration of the common workflow of federated learning. The global model is updated by aggregating local trained models of a set of selected clients. Note that the local datasets can contain data with noisy labels. To reduce the negative effect caused by noisy labels in FL, most existing researches [ 7,54,65] utilize an auxiliary dataset with per fect labels to identify the noise level of clients or conduct the sample level selection for training. However, since it is hard and impractical to obtain a predefined, fixed and perfectlylabeled auxiliary dataset, these methods fail to tackle the issue of training on data with noisy labels in reality. Fortunately, Yang et al. [ 66] firstly propose an auxiliarydatasetfree FL method, which collects local classwise feature centroids from clients to form global classwise feature cen troids as global supervision to effectually deal with the noisy labels. However, this method can bring about underlying privacy leakage risks, since these centroids transimitted between clients and the server can be inversely utilized to reveal some sensitive informa tion about the private data. Therefore, it is necessary to obtain extra reliable supervision for dealing with noisy labels without compromising data privacy. Data augmentation is a lowcost and widelyutilized technique to effectively obtain extra supervision information, and has been ap plied in many machine learning problems such as supervised learn ing [ 27,64], semisupervised learning [ 4,30,63], selfsupervised contrastive learning [ 2,6,17] and many other domains. Neverthe less, few works have explored the utilization of data augmentation technique in the scope of learning on data with noisy labels [47]. In this work, we focus on the local training process of FL, and uti lize data augmentation technique to obtain extra supervision, so as to promote performance against noisy labels without violating the privacy principle. Considering the presence of data with noisy la bels, our intuition is threefold: 1) The prediction for the augmented sample should be close to the prediction for the original sample. 2) Model predictions can be more close to the ground truth than the corresponding noisy labels. 3) The proposed approach should be privacypreserving to be applied in the practical FL system. Following this intuition, we propose a method named Local Self Regularization to tackle the issue of training on data with noisy labels in FL, which tries to mitigate the performance degradation caused by noisy labels in the premise of protecting data privacy. Specifically, we implicitly regularize local training by enhancing model discrimination confidence to prevent it from memorizing noisy labels, and further utilize self knowledge distillation tech nique to explicitly regularize the model output discrepancy between original and augmented instances. We provide some insights to deal with noisy labels in FL through the following contributions:‚Ä¢We empirically show that in the presence of noisy labels, the memorization effect of deep network proposed in data centralized setting still exists in FL, which brings degradation to the global model performance. ‚Ä¢We propose an auxiliarydatasetfree and privacypreserving FL method named Local SelfRegularization, which implicitly regularizes the model from memorizing noisy labels and explicitly regularizes the model output discrepancy between original and augmented instances. ‚Ä¢We present the effectiveness of our method through exten sive experiments on three benchmark datasets compared to the stateofthearts in various noise levels. In addition, our method has potential to incorporate with other existing methods to further improve performance, and this insight is also verified on the realworld Clothing1M dataset. 2 RELATED WORKS "
287,Active Learning for Noisy Data Streams Using Weak and Strong Labelers.txt,"Labeling data correctly is an expensive and challenging task in machine
learning, especially for on-line data streams. Deep learning models especially
require a large number of clean labeled data that is very difficult to acquire
in real-world problems. Choosing useful data samples to label while minimizing
the cost of labeling is crucial to maintain efficiency in the training process.
When confronted with multiple labelers with different expertise and respective
labeling costs, deciding which labeler to choose is nontrivial. In this paper,
we consider a novel weak and strong labeler problem inspired by humans natural
ability for labeling, in the presence of data streams with noisy labels and
constrained by a limited budget. We propose an on-line active learning
algorithm that consists of four steps: filtering, adding diversity, informative
sample selection, and labeler selection. We aim to filter out the suspicious
noisy samples and spend the budget on the diverse informative data using strong
and weak labelers in a cost-effective manner. We derive a decision function
that measures the information gain by combining the informativeness of
individual samples and model confidence. We evaluate our proposed algorithm on
the well-known image classification datasets CIFAR10 and CIFAR100 with up to
60% noise. Experiments show that by intelligently deciding which labeler to
query, our algorithm maintains the same accuracy compared to the case of having
only one of the labelers available while spending less of the budget.","Obtaining a labeled dataset for training machine learning models is a time consuming and expensive task. The common practise of curating data continuously collects both data and the metadata that serves as labels Hendrycks et al. [2018] from the public domain. This immediately reudcesthe cost of acquiring a high volume of labeled data for deep learning models but introduces the challenge of handling data streams with noisy labels Han et al. [2018], Wang et al. [2018], i.e., data that is annotated with wrong class labels. Human experts Sheng et al. [2008] are sought after to correct labels to enhance the robustness of learning models against label noise. Cognitive science Yan et al. [2014] has shown that humans are better in answering binary questions, such as True/False questions, and are less skilled in multiplechoice questions, e.g., identifying one out of 100 classes in CIFAR100 benchmark Krizhevsky et al. [2009]. It is more expensive to use strong labelers who are skilled in directly pointing out true class labels than weak labelers who can only (dis)agree with the provided labels. To cost effectively correct the noisy labels by experts Sheng et al. [2008], Chang et al. [2017], it is imperative to assign the correct tasks to the labelers according to their skills and the Preprint. Under review.arXiv:2010.14149v1  [cs.LG]  27 Oct 2020difÔ¨Åculty levels of querying tasks. This becomes particularly challenging in streaming data scenarios due to future uncertainty, e.g., how to allocate the correction effort across the learning horizon. Online learning from streaming data corresponds to today‚Äôs data common practise to train machine learning models with a small set of data that is periodically curated from the public domains. Machine learning models thus need to be learned online from the stream data. Moreover, due to privacy or regulation constraints gdp, or storage limits, data turns are available for the limited duration. Moreover, in such scenarios, combating label noise adds to the challenge. The prior art tries to enhance the robustness of the deep model training against noisy labels in the offline scenario by (i) Ô¨Åltering out noisy data through model disagreement Han et al. [2018], Yu et al. [2019], (ii) correcting noisy labels through the estimated noisy corruption matrix Patrini et al. [2017], or (iii) modifying the loss functions Ma et al. [2018], Wang et al. [2019]. Among the related studies, high quality labels from human experts are used to correct labels through the estimate noise corruption matrix, e.g., the trust data set in Distillation Li et al. [2017] and GLC Hendrycks et al. [2018]. The trust data sheds light on how a small percentage of high quality labels can prevent deep models from accuracy degradation due to label noise. However, the focus on the offline datasets renders existing approaches insufÔ¨Åcient for handling noisy data streams only a subset of which can be learnt at a time. Indeed, online learning from noisy labels can lead to a much more severe degradation of accuracy than the typical offline case. Moreover, such a subset of trust data is randomly selected and the cost of acquiring additional data label yet to be modeled. In this paper we address the challenge of training a deep classiÔ¨Åer from noisy labeled data streams that are collected over time and that can only be learnt for a limited time. We propose an active learning framework, DuoLab, which aims to learn a robust classiÔ¨Åer by costeffectively assigning the labelcorrection task to either a strong or a weak labeler within a labeling budget. Extensively querying the strong labeler can easily lead to budget exhaustion, whereas the weak labeler might require multiple queries to achieve the cleansing goal. DuoLab consist of four steps: Ô¨Åltering out the suspicious data, choosing diverse samples, informativeness ranking, and selecting labelers. While training the classiÔ¨Åer, DuoLab leverages the output of its classiÔ¨Åer to Ô¨Ålter suspicious data samples. As for label cleansing, we propose a labeler selection function Q that combines the overall model conÔ¨Ådence and the informativeness of individual samples. Our contributions are the following. First we design a cost and skill aware active learning framework for noisy data streams. Secondly, by leveraging the diversity of the labelers we are able to greatly enhance the robustness of deep models even in challenging online learning scenarios. The proposed Q function can effectively assess the model conÔ¨Ådence and informativeness of data samples and assign the suitable labelers accordingly. 2 Related Work "
493,Automatic Neural Network Hyperparameter Optimization for Extrapolation: Lessons Learned from Visible and Near-Infrared Spectroscopy of Mango Fruit.txt,"Neural networks are configured by choosing an architecture and hyperparameter
values; doing so often involves expert intuition and hand-tuning to find a
configuration that extrapolates well without overfitting. This paper considers
automatic methods for configuring a neural network that extrapolates in time
for the domain of visible and near-infrared (VNIR) spectroscopy. In particular,
we study the effect of (a) selecting samples for validating configurations and
(b) using ensembles.
  Most of the time, models are built of the past to predict the future. To
encourage the neural network model to extrapolate, we consider validating model
configurations on samples that are shifted in time similar to the test set. We
experiment with three validation set choices: (1) a random sample of 1/3 of
non-test data (the technique used in previous work), (2) using the latest 1/3
(sorted by time), and (3) using a semantically meaningful subset of the data.
Hyperparameter optimization relies on the validation set to estimate test-set
error, but neural network variance obfuscates the true error value. Ensemble
averaging - computing the average across many neural networks - can reduce the
variance of prediction errors.
  To test these methods, we do a comprehensive study of a held-out 2018 harvest
season of mango fruit given VNIR spectra from 3 prior years. We find that
ensembling improves the state-of-the-art model's variance and accuracy.
Furthermore, hyperparameter optimization experiments - with and without
ensemble averaging and with each validation set choice - show that when
ensembling is combined with using the latest 1/3 of samples as the validation
set, a neural network configuration is found automatically that is on par with
the state-of-the-art.","This paper considers how to automatically con gure neural network hyperparameters such that it extrapolates in time for visible and nearinfrared (VNIR) spectroscopy. Hyperparameter optimiza tion (HPO) is a signicant undertaking. Neural networks are congured by choosing an architec ture (such as number of layers) and hyperparame ter values (such as learning rate), all of which may be optimized at once during HPO. Even when us ing stateoftheart Bayesian optimization software, Corresponding author Email addresses: mcdirks@cs.ubc.ca (Matthew Dirks), poole@cs.ubc.ca (David Poole)HPO still involves many decisions and intuitions (some of which are explained in a recent tutorial [1]). This paper is about further streamlining the process of hyperparameter optimization in order to do so automatically, without overtting, and in a manner that mimics an expertlytuned model. A dataset is partitioned into test and nontest samples. Given the nontest samples, the goal is to build a predictor that works the best on the test set. The test set is only used to evaluate nal models. If a neural network is trained on all nontest data it will overt. To avoid overtting, the nontest data is partitioned into calibration and validation sets (in machine learning literature, these are often called training and development sets). The calibration set Preprint submitted to Chemometrics and Intelligent Laboratory Systems October 5, 2022arXiv:2210.01124v1  [eess.IV]  3 Oct 2022is used to train the model and the validation set is used as a proxy of the test set. Neural network hyperparameters are chosen to minimize prediction error on the validation set (in this case, it's sometimes called a tuning set). HPO may overt the validation set and the best method to combat this is an open area of research [2]. To avoid overtting, a combination of expert intuition and handtuning is often used. One approach, re cently studied [3] for chemometrics, is to nd stable optima where the RMSE on the validation set (with respect to the hyperparameters) is wide (doesn't change much with slight perturbations) rather than narrow [2]. We take a complementary approach: encourage the model to extrapolate by the choice of validation samples used in hyperparameter opti mization. Extrapolating in time is often dicult because the future is dierent from the past. The dataset of mangoes by Anderson et al [4, 5] is a good ex ample: Using spectra from 3 years, the goal is to predict dry matter (DM) content in the next year. Thus, we want a neural network conguration that doesn't overt the past but extrapolates well to the future. In previous work, the validation set is 1/3 of nontest data, sampled randomly. We test two alternatives to avoid overtting in HPO and encour age the neural network model to extrapolate: First, we use the latest 1/3 of samples (sorted by time). Second, we use a semantically meaningful subset [6]; specically, the latest harvest season (2017). Due to the stochastic nature of training algo rithms, neural networks have dierent weights and dierent errors each time they're trained. We re port the distributions of RMSE scores for the pur pose of fairly evaluating each method. The variance of errors is also problematic for HPO because the prediction error on the validation set is an estimate of how well the model will perform on the test set and in deployment; a poor estimate leads to a sub optimal neural network conguration. We investigate using ensembles to reduce the variance of validationset error during hyperparam eter optimization. Ensembles (of many kinds) have been shown to improve accuracy, reduce variance, and improve robustness to domain shift [7, 8, 9]. Specically, we obtain an ensemble by reinitializing a neural network randomly and retraining it a number of times [7, 10]; this model reduces the por tion of the variance that is due to random initial ization. To test these methods, we do a comprehensivestudy of a heldout 2018 harvest season of mango fruit given VNIR spectra from 3 prior years [4]. We conduct hyperparameter optimization for each choice of validation set and compare HPO with and without ensemble averaging. The results in this study sheds light on reproducible and automated practices for conguring and training neural net works for spectroscopy; these results can inform practitioners what steps to take in building their own models to make predictions for future samples. 2. Methodologies "
536,Overhead MNIST: A Benchmark Satellite Dataset.txt,"The research presents an overhead view of 10 important objects and follows
the general formatting requirements of the most popular machine learning task:
digit recognition with MNIST. This dataset offers a public benchmark extracted
from over a million human-labelled and curated examples. The work outlines the
key multi-class object identification task while matching with prior work in
handwriting, cancer detection, and retail datasets. A prototype deep learning
approach with transfer learning and convolutional neural networks (MobileNetV2)
correctly identifies the ten overhead classes with an average accuracy of
96.7%. This model exceeds the peak human performance of 93.9%. For upgrading
satellite imagery and object recognition, this new dataset benefits diverse
endeavors such as disaster relief, land use management, and other traditional
remote sensing tasks. The work extends satellite benchmarks with new
capabilities to identify efficient and compact algorithms that might work
on-board small satellites, a practical task for future multi-sensor
constellations. The dataset is available on Kaggle and Github.","The most popular  starting test  for both new and established machine learning  algorithms  relies on handwritten digit  [1] or letter [2]  recognition . If a method  does not work with the M odified National Institute of Standards and Technology  dataset (MNIST), it most likely will not work on more challenging tasks. As  illustrated in Figure 1, t he core task corresponds to a multi class image challenge,  one which  proves common and useful in other fields [3] outside of algorithms to  interpret handwriting. Research ers over the last two decades [4] have spawned  more than 48,000 MNIST related publications , with a quarter of those appearing  in 2020. The reverse of this  universality stems from the relative ease which   modern algorithms have solve d the problem (>99% accuracy after a few  iterations)  [56].  Critics of the digit dataset {09} note that it contains near  duplicates  and lacks diversity in example data such that  modifying a single pixel  (among 784 pixels in a 28x28 image) can flip some algorithms to misidentify the expected digit  [79].  Alternative practical extensions of the digit recognition task now include alphabetic h andwriting in multiple  languages ( e.g. English  [2], Chinese  [10], Russian  [11], Kan nada [12], American Sign Language  [13]) and  related everyday object  recognition , the most popular of which includes 10 categories of  skin cancer [14]  and clothing  [15] in thumbnail grayscale images  (HAM 1000  [14] and Fashion MNIST  [15]).   The present work provides another challenging object recognition task : labelling objects from  overhead  satellite imagery  (Figure 2) .  To take advantage of the vast machine learning literature on digit recognition,  we mirror the format of the original MNIST closely [1] and thus , like Fashion MNIST  [15], we aim to  provide the research community with a nother  dropin replacement for bench marking  [16]. The grayscale  (28x28 pixel) imagery provides  a challenging object recognition task  [17]. As viewed from above, objects   such as planes, ships, and stadiums  offer no obvious preferred orientation, so rotating or image shearing  may not augment d ataset diversity  [18]. Overhead object classifiers also suffer from scale variations that   Figure 1. Examples of  MNIST families for digits,  letters, signing, and objects  can range  more than  two orders of magnitude between a small car to a stadium  [1921]. Compared to  alternative terrestrial (color) datasets (like CIFAR 10 thumbnail [22]), a classifier of satellite imagery may  span different camera resolutions, orientations, and daynight contrast levels  in different seasons and  shadows. The research offers  a novel benchmark  [23] for recognizing objects in thumbnail satellite images,  a reformatting strategy to connect  with the vast MNIST algorithm literature and finally, an initial solution  to the classification problem using transfer learning and MobileNet V2 [24]. The original contributions of  the present work include: 1) generalizing the standard MNIST format  [1,16]  and dataset design to handle  challenging satellite object detection (called Overhead MNIST); 2) exploring the unique aspects of  overhead object recognition such as diverse object scale  lengths  and rotational invariance  [17,21 ]; 3)  classifying 10 classes of objects with multiple algorithms, including some efficient enough to run on board  satellites for automated tasking and cueing. Absent the 70,00 0 overhead thumbnails  of the original digit  recognition [1] , we examine the requirements for solving the problem as a function of sample size.     Figure 2.  Ten Classes Labelled for Overhead MNIST.  The class selection includes dynamic (car, plane, helicopter, ship) and static  (parking lot, runway mark, harbor) targets along with infrastructure related objects (storage tanks, stadium, oil gas fields)   2. METHODS   "
180,Recognizing Abnormal Heart Sounds Using Deep Learning.txt,"The work presented here applies deep learning to the task of automated
cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We
describe an automated heart sound classification algorithm that combines the
use of time-frequency heat map representations with a deep convolutional neural
network (CNN). Given the cost-sensitive nature of misclassification, our CNN
architecture is trained using a modified loss function that directly optimizes
the trade-off between sensitivity and specificity. We evaluated our algorithm
at the 2016 PhysioNet Computing in Cardiology challenge where the objective was
to accurately classify normal and abnormal heart sounds from single, short,
potentially noisy recordings. Our entry to the challenge achieved a final
specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved
the greatest specificity score out of all challenge entries and, using just a
single CNN, our algorithm differed in overall score by only 0.02 compared to
the top place finisher, which used an ensemble approach.","Advances in deep learning [LeCun et al. , 2015 ]are be ing made at a rapid pace, in part due to challenges such as ILSVRC ‚Äì the ImageNet LargeScale Visual Recognition Challenge [Russakovsky et al. , 2015 ]. Successive improve ments in deep neural network architectures have resulted in computer vision systems that are better able to recognize and classify objects in images [Linet al. , 2013; Szegedy et al. , 2015 ]and winning ILSVRC entries [Szegedy et al. , 2014; Heet al. , 2015 ]. While a large focus of deep learning has been on automated analysis of image and text data, advances are also increasingly being seen in areas that require process ing other input modalities. One such area is the medical do main where inputs into a deep learning system could be phys iologic time series data. An increasing number of large scale challenges in the medical domain, such as [Kaggle, 2014 ]and [Kaggle, 2015 ]have also resulted in improvements to deep learning architectures [Liang and Hu, 2015 ].PhysioNet [Goldberger et al. , 2000 ]has held a Comput ing in Cardiology Challenge since 2000 that requires partic ipants to automatically analyze physiologic time series data. The 2016 challenge [Clifford et al. , 2016 ]asked participants to perform automated analysis of phonocardiogram (PCG) waveforms, i.e. heart sound data collected using digital stethoscopes. The objective of the challenge was to accu rately classify normal and abnormal heart sounds. Record ings were collected from both healthy individuals, as well as those with heart disease, including heart valve disease and coronary artery disease. A PCG plot showing the recording of the (normal) sounds made by the heart is given in Figure 1. Figure 1: A phonocardiogram showing the recording of nor mal heart sounds, together with corresponding electrocardio gram tracing. S1is the Ô¨Årst heart sound and marks the begin ning of systole. Source [Springer et al., 2016]. Heart disease remains the leading cause of death globally, resulting in more people dying every year due to cardiovas cular disease compared to any other cause of death [World Health Organization, 2017 ]. Successful automated PCG anal ysis can serve as a useful diagnostic tool to help determine whether an individual should be referred on for expert di agnosis, particularly in areas where access to clinicians and medical care is limited. In this work, we present an algorithm that accepts PCG waveforms as input and uses a deep convolutional neural net work architecture to classify inputs as either normal or abnorarXiv:1707.04642v2  [cs.SD]  19 Oct 2017mal using the following steps: 1. Segmentation of time series A logistic regression hidden semiMarkov model is used to segment incoming heart sound instances into shorter segments beginning at the start of each heartbeat, i.e. the S1heart sound. 2. Transformation of segments into heat maps Using Melfrequency cepstral coefÔ¨Åcients, one dimensional time series input segments are converted into two dimensional spectrograms (heat maps) that capture the timefrequency distribution of signal energy. 3. ClassiÔ¨Åcation of heat maps using a deep neural network A convolutional neural network is trained to perform automatic feature extraction and distinguish between normal and abnormal heat maps. The contributions of this work are as follows: 1. We introduce a deep convolutional neural network ar chitecture designed to automatically analyze physiologic time series data for the purposes of identifying abnor malities in heart sounds. 2. Given the costsensitive nature of misclassiÔ¨Åcation, we describe a novel loss function used to train the above network that directly optimizes the sensitivity and speci Ô¨Åcity tradeoff. 3. We present results from the 2016 PhysioNet Computing in Cardiology Challenge where we evaluated our algo rithm and achieved a Top 10 place Ô¨Ånish out of 48 teams who submitted a total of 348 entries. The remainder of this paper is organized as follows. In Section 2, we discuss related works, including historical ap proaches to automated heart sound analysis and deep learning approaches that process physiologic time series input data. Section 3 introduces our approach and details each step of the algorithm. Section 4 further describes the modiÔ¨Åed cost sensitive loss function used to tradeoff the sensitivity and speciÔ¨Åcity of the network‚Äôs predictions, followed by Section 5, which details the network training decisions and param eters. Section 6 presents results from the 2016 PhysioNet Computing in Cardiology Challenge and in Section 7 we pro vide a Ô¨Ånal discussion and end with conclusions in Section 8. 2 Related Work "
372,Anomaly Detection in Retinal Images using Multi-Scale Deep Feature Sparse Coding.txt,"Convolutional Neural Network models have successfully detected retinal
illness from optical coherence tomography (OCT) and fundus images. These CNN
models frequently rely on vast amounts of labeled data for training, difficult
to obtain, especially for rare diseases. Furthermore, a deep learning system
trained on a data set with only one or a few diseases cannot detect other
diseases, limiting the system's practical use in disease identification. We
have introduced an unsupervised approach for detecting anomalies in retinal
images to overcome this issue. We have proposed a simple, memory efficient,
easy to train method which followed a multi-step training technique that
incorporated autoencoder training and Multi-Scale Deep Feature Sparse Coding
(MDFSC), an extended version of normal sparse coding, to accommodate diverse
types of retinal datasets. We achieve relative AUC score improvement of 7.8\%,
6.7\% and 12.1\% over state-of-the-art SPADE on Eye-Q, IDRiD and OCTID datasets
respectively.","Anomaly detection is a widely discussed topic in the Ma chine learning community. Ocular illnesses such as diabetic retinopathy (DR), agerelated macular degeneration (AMD), Macular Hole, Central Serous Retinopathy, and glaucoma im pact more than 270 million people globally. Anomaly detec tion in retinal data is a signiÔ¨Åcant problem that is useful in identifying any abnormalities in the patients [1]. Supervised classiÔ¨Åcation algorithms [2, 3, 4] are often used to classify normal and anomalous data. However, training supervised classiÔ¨Åers require a good amount of annotated data which is often hard to obtain in the Ô¨Åeld of retinal imaging. Even when annotated data is available, the classiÔ¨Åers might suffer from the class imbalance as the prevalence of normal samples is frequently higher than that of abnormal samples. Also, some lesions are uncommon, and the presence of speciÔ¨Åc lesions is unknown until the diagnosis is made. Moreover, the labels collected from different clinical experts may be different, andhence the annotation process can produce noisy or biased la bels [5]. In this context, anomaly detection in an unsupervised manner can be helpful. In our proposed method, we initially train an autoencoder to reconstruct image patches from normal images to learn domainspeciÔ¨Åc Ô¨Ånegrained features. This step will help learn more relevant micro and macrolevel features unique to normal samples in local and global contexts. Once the au toencoder is trained, we utilize multiscale features extracted from the encoder‚Äôs multiple layers from various depths as an input vector for sparse coding. With multiscale deep fea tures, it will capture more global context from the image to detect anomalies with different scales or sizes. Our method has outperformed stateoftheart unsupervised algorithms on three different retinal datasets. Our main contributions in this paper are It is simple, mem ory efÔ¨Åcient, and easy to train, unlike other methods, We have proposed a multistep training strategy that combines autoen coder training and MultiScale Deep Feature Sparse Coding (MDFSC) for anomaly detection to adopt a different type of datasets, We extended MultiScale Deep Feature Sparse Cod ing (MDFSC) from sparse coding for various types of retinal datasets. 2. RELATED WORK "
528,Nonlinear Local Metric Learning for Person Re-identification.txt,"Person re-identification aims at matching pedestrians observed from
non-overlapping camera views. Feature descriptor and metric learning are two
significant problems in person re-identification. A discriminative metric
learning method should be capable of exploiting complex nonlinear
transformations due to the large variations in feature space. In this paper, we
propose a nonlinear local metric learning (NLML) method to improve the
state-of-the-art performance of person re-identification on public datasets.
Motivated by the fact that local metric learning has been introduced to handle
the data which varies locally and deep neural network has presented outstanding
capability in exploiting the nonlinearity of samples, we utilize the merits of
both local metric learning and deep neural network to learn multiple sets of
nonlinear transformations. By enforcing a margin between the distances of
positive pedestrian image pairs and distances of negative pairs in the
transformed feature subspace, discriminative information can be effectively
exploited in the developed neural networks. Our experiments show that the
proposed NLML method achieves the state-of-the-art results on the widely used
VIPeR, GRID, and CUHK 01 datasets.","Person reidentiÔ¨Åcation aims to recognize people who have been observed from different disjoint cameras, which has become an effective tool for people association and be havior analysis in video surveillance [14, 42]. Due to the complex variations in illumination, pose, viewpoint, occu lusion and image resolution across camera views, person reidentiÔ¨Åcation still remains a challenging problem in com puter vision. Previously proposed approaches which improve the per son reidentiÔ¨Åcation performance [4, 25, 27, 37, 46, 49, 50] can be mainly categorized into two classes: 1) developing robust descriptors to handle the variations in pedestrian im ages; 2) designing discriminative distance metrics to mea sure the similarity of pedestrian images. For the Ô¨Årst cate Apairofpersonimages(ùíôùüè,ùíôùüê) center1centerKglobalùúî((ùíôùüè,ùíôùüê)ùúî)(ùíôùüè,ùíôùüê)ùúî*=ùõΩùúî(ùëë((ùíôùüè,ùíôùüê)ùúî)ùëë)(ùíôùüè,ùíôùüê)ùúî*ùëë*(ùíôùüè,ùíôùüê)DistanceMetric:ùê∑ùíôùüè,ùíôùüê=	  ¬†‚àëùúî1ùíôùüè,ùíôùüêùëë1(ùíôùüè,ùíôùüê))12*Figure 1: Basic idea of the proposed NLML method. The whole network consists of one global neural network and a set of local neural networks which correspond to differ ent local clusters. For a given pair of person images x1 andx2, we compute the nonnegative weight wk(x1;x2) forkth local region and pass it through Klocal and one global deep neural networks to get the representation at out put layerdk(x1;x2). The Ô¨Ånal distance metric D(x1;x2)is deÔ¨Åned as a linear combination of K+ 1matrices. gory, different cues (color, shape, texture) from pedestrian images are employed for feature representation. Repre sentative descriptors in person reidentiÔ¨Åcation include lo cal binary patterns (LBP) [46], ensemble of local feature (ELF) [16], midlevel Ô¨Ålter [50] and local maximal oc currence (LOMO) [27]. For the second category, a dis tance metric is learned from labeled training samples, un der which the interclass and intraclass variations of pedes trian images are increased and decreased, respectively. Typ 1arXiv:1511.05169v1  [cs.CV]  16 Nov 2015ical metric learning algorithms include large margin nearest neighbor (LMNN) [44], information theoretic metric learn ing (ITML) [9], and pairwise constrained component anal ysis (PCCA) [36]. While metric learning methods achieved good perfor mance in many person reidentiÔ¨Åcation systems [3, 7, 9, 20, 21, 26, 37, 38, 44, 46], most of them learn a Maha lanobis distance metric to transform samples into a new feature space, which are not capable enough of exploiting the nonlinear relationship of pedestrian samples distributed in a nonlinear feature space due to large intraclass vari ations. Moreover, a single distance metric usually suffers limitations while handling data which varies locally. To address this, we propose a nonlinear local metric learning (NLML) method for person reidentiÔ¨Åcation. Figure 1 il lustrates the basic idea of the proposed NLML method. Un like most existing metric learning methods, NLML devel ops one global feedforward neutral network and a set of local feedforward neutral networks to jointly learn multi ple sets of nonlinear transformations. The learning proce dure is formulated as a large margin optimization problem and the gradient descent algorithm is employed to estimate the networks. Experimental results on the VIPeR, GRID and CUHK 01 datasets demonstrate the efÔ¨Åcacy of the pro posed NLML method. 2. Related Work "
480,ECOVNet: An Ensemble of Deep Convolutional Neural Networks Based on EfficientNet to Detect COVID-19 From Chest X-rays.txt,"This paper proposed an ensemble of deep convolutional neural networks (CNN)
based on EfficientNet, named ECOVNet, to detect COVID-19 using a large chest
X-ray data set. At first, the open-access large chest X-ray collection is
augmented, and then ImageNet pre-trained weights for EfficientNet is
transferred with some customized fine-tuning top layers that are trained,
followed by an ensemble of model snapshots to classify chest X-rays
corresponding to COVID-19, normal, and pneumonia. The predictions of the model
snapshots, which are created during a single training, are combined through two
ensemble strategies, i.e., hard ensemble and soft ensemble to ameliorate
classification performance and generalization in the related task of
classifying chest X-rays.","Coronavirus disease 2019 (COVID19) is a contagious disease that was caused by the Severe Acute Respiratory Syndrome Coronavirus 2 (SARSCoV2). The disease was Ô¨Årst detected in Wuhan City, Hubei Province, China in December 2019, and was related to contact with a seafood wholesale market and quickly spread to all parts of the world [1]. The World Health Organization (WHO) promulgated the outbreak of the COVID19 pandemic on March 11, 2020. As of September 20, 2020, this perilous virus has not only overwhelmed the world, but also affected millions of lives. So far, there have been 30;675;675conÔ¨Årmed COVID19 cases and 954;417conÔ¨Årmed deaths [2]. To limit the spread of this infection, all infected countries strive to cover many strategies such as encourage people to maintain social distancing as well as lead hygienic life, enhance the infection screening system through multifunctional testing, seek mass vaccination to reduce the pandemic ahead of time, etc. The reverse transcriptasepolymerase chain reaction (RTPCR) is a modular diagnosis method, however, it has certain limitations, such as the accurate detection of suspect patients causes delay since the testing procedures inevitably preserve the strict necessity of conditions at the clinical laboratory [3] and falsenegative results may lead to greater impact in the prevention and control of the disease [4]. To make up for the shortcomings of RTPCR testing, researchers around the world are seeking to promote a fast and reliable diagnostic method to detect COVID19 infection. The WHO and Wuhan University Zhongnan Hospital respectively issued quick guides [5, 6], suggesting that in addition to detecting clinical symptoms, chest imaging can also be used to evaluate the disease to diagnose and treat COVID19. In [7], the authors have contributed a proliÔ¨Åc guideline for medical practitioners to use chest radiography and computed tomography (CT) to screen and assess the disease progression of COVID19 cases. Although CT scans have higher sensitivity, it also has some drawbacks, such as high cost and the need for high doses of radiation during screening, which exposes pregnant women and children to greater radiation risks [8]. On the other hand, diagnosis based on chest Xray appears to be a propitious solution for COVID19 detection and treatment. In [9], Ng et al. remarked that COVID19 infection pulmonary manifestation is immensely delineated by chest Xray images. Moreover, in the case of an artiÔ¨Åcial intelligence (AI)based disease recognition system, medical practitioners have already emphasized chest Xrays to explore potential symptoms of COVID19 infection, such as opaque patterns in the lungs [10]. The purpose of this study is to ameliorate the accuracy of COVID19 detection system from chest Xray images. In this context, we contemplate a CNNbased architecture since it is illustrious for its topnotch recognition performance in image classiÔ¨Åcation or detection. For medical image analysis, higher detection accuracy along with crucial Ô¨Åndings is a top aspiration, and in current years, CNN based architectures are comprehensively featured the critical Ô¨Åndings related to medical imaging that‚Äôs why we constructed the proposed architecture with CNN. In order to achieve the deÔ¨Åned purpose, this paper presents a novel CNN based architecture called ECOVNet, exploiting the cuttingedge EfÔ¨ÅcientNet [11] family of CNN models together with ensemble strategies. The pipeline of the proposed architecture commences with the data augmentation approach, then optimizes and Ô¨Ånetunes the pretrained EfÔ¨ÅcientNet models, creating respective model‚Äôs snapshots. After that, generated model snapshots are integrated into an ensemble, i.e., soft voting and hard voting, to make predictions. The motivation for using EfÔ¨ÅcientNets is that they are known for their high accuracy, while being smaller and faster than the best existing CNN architectures. Moreover, an ensemble technique has proven to be effective in predicting since it produces a lower error rate compared with the prediction of a single model. Owing to the limited number of COVID19 images currently available, diagnosing COVID19 infection is more challenging, thereby investing with a visual explainable approach is applied for further analysis. In this regard, we use a Gradientbased Class Activation Mapping algorithm, i.e., GradCAM [12], providing explanations of the predictions and identifying relevant features associated with COVID19 infection. The key contributions of this paper are as follows: ‚Ä¢We propose a novel CNN based architecture that includes frontend pretrained EfÔ¨ÅcientNets for feature extraction and model snapshots to detect COVID19 from chest Xrays. ‚Ä¢Taking into account the following assumption, the decisions of multiple radiologists are considered in the Ô¨Ånal prediction, so we propose an ensemble in the proposed architecture to make predictions, thus making a credible and fair evaluation of the system. ‚Ä¢We visualize a class activation map through GradCAM to explain the prediction as well as to identify the critical regions in the chest Xray. ‚Ä¢Finally, we appraise our architecture with stateoftheart architectures through empirical observations to highlight the effectiveness of the proposed architecture in detecting COVID19. The remainder of the paper is arranged as follows: Section 2 discusses related work. Section 3 explains the details of the data set and proposed network architecture, as well as its adjustments to the detection of COVID19 infection. The results of our experimental evaluation is presented in Section 4. Finally, Section 5 concludes paper and highlights the future work. 2APREPRINT  OCTOBER 19, 2020 2 Related Works "
72,Locally Adaptive Label Smoothing for Predictive Churn.txt,"Training modern neural networks is an inherently noisy process that can lead
to high \emph{prediction churn} -- disagreements between re-trainings of the
same model due to factors such as randomization in the parameter initialization
and mini-batches -- even when the trained models all attain similar accuracies.
Such prediction churn can be very undesirable in practice. In this paper, we
present several baselines for reducing churn and show that training on soft
labels obtained by adaptively smoothing each example's label based on the
example's neighboring labels often outperforms the baselines on churn while
improving accuracy on a variety of benchmark classification tasks and model
architectures.","Deep neural networks (DNNs) have proved to be im mensely successful at solving complex classiÔ¨Åcation tasks across a range of problems. Much of the effort has been spent towards improving their predictive performance (i.e. accuracy), while comparatively little has been done to wards improving the stability of training these models (Zheng et al., 2016). Modern DNN training is inherently noisy due to factors such as the random initialization of network parameters (Glorot & Bengio, 2010), the mini batch ordering (Loshchilov & Hutter, 2015), the effects of various data augmentation (Shorten & Khoshgoftaar, 2019) or preprocessing tricks (Santurkar et al., 2018), and the nondeterminism arising from the hardware (Turner & Nowotny, 2015), all of which are exacerbated by the non convexity of the loss surface (Scardapane & Wang, 2017). This results in local optima corresponding to models that have very different predictions on the same data points. This may seem counterintuitive, but even when the differ ent runs all produce very high accuracies for the classiÔ¨Åca tion task, their predictions can still differ quite drastically 1Google Research, Mountain View, USA. Correspondence to: Dara Bahri <dbahri@google.com >. Proceedings of the 38thInternational Conference on Machine Learning , PMLR 139, 2021. Copyright 2021 by the author(s).as we will show later in the experiments. Thus, even an optimized training procedure can lead to high prediction churn , which refers to the proportion of samplelevel dis agreements between classiÔ¨Åers caused by different runs of the same training procedure1. In practice, reducing such predictive churn can be critical. For example, in a production system, models are often con tinuously improved on by being trained or retrained with new data or better model architectures and training pro cedures. In such scenarios, a candidate model for release must be compared to the current model serving in produc tion. Oftentimes, this decision is conditioned on more than just overall ofÔ¨Çine test accuracy‚Äì in fact, the ofÔ¨Çine metrics are often not completely aligned with the actual goal, es pecially if these models are used as part of a larger system (e.g. maximizing ofÔ¨Çine clickthrough rate vs. maximiz ing revenue or user satisfaction) (Deng et al., 2013; Beel et al., 2013; Dmitriev & Wu, 2016). As a result, these com parisons require extensive and costly live experiments, re quiring human evaluation in situations where the candidate and the production model disagree (i.e. in many situations, the true labels are not available without a manual labeler) (Theocharous et al., 2015; Deng, 2015; Deng & Shi, 2016). In these cases, it can be highly desirable to lower predictive churn. Despite the practical relevance of lowering churn, there has been surprisingly little work done in this area, which we highlight in the related work section. In this work, we fo cus on predictive churn reduction under retraining the same model architecture on an identical train and test set. Our main contributions are as follows: ‚Ä¢ We provide one of the Ô¨Årst comprehensive analyses of baselines to lower prediction churn, showing that pop ular approaches designed for other goals are effective baselines for churn reduction, even compared to meth ods designed for this goal. ‚Ä¢ We improve label smoothing, a global smoothing method popular for calibrating model conÔ¨Ådence, by utilizing the local information leveraged by the k 1Concretely, given two classiÔ¨Åers applied to the same test sam ples, the prediction churn between them is the fraction of test sam ples with different predicted labels.arXiv:2102.05140v2  [cs.LG]  11 Jun 2021Locally Adaptive Label Smoothing for Predictive Churn NN labels thus introducing a locally adaptive label smoothing which we show to often outperform the baselines on a wide range of benchmark datasets and model architectures. ‚Ä¢ We show new theoretical results for the kNN labels suggesting the usefulness of the kNN label. We show under mild nonparametric assumptions that for a wide range ofk, thekNN labels uniformly approximates the optimal soft label and when kis tuned optimally, achieves the minimax optimal rate. We also show that whenkis linear inn, the distribution implied by thekNN label approximates the original distribution smoothed with an adaptive kernel. 2. Related Works "
249,In-Place Scene Labelling and Understanding with Implicit Scene Representation.txt,"Semantic labelling is highly correlated with geometry and radiance
reconstruction, as scene entities with similar shape and appearance are more
likely to come from similar classes. Recent implicit neural reconstruction
techniques are appealing as they do not require prior training data, but the
same fully self-supervised approach is not possible for semantics because
labels are human-defined properties.
  We extend neural radiance fields (NeRF) to jointly encode semantics with
appearance and geometry, so that complete and accurate 2D semantic labels can
be achieved using a small amount of in-place annotations specific to the scene.
The intrinsic multi-view consistency and smoothness of NeRF benefit semantics
by enabling sparse labels to efficiently propagate. We show the benefit of this
approach when labels are either sparse or very noisy in room-scale scenes. We
demonstrate its advantageous properties in various interesting applications
such as an efficient scene labelling tool, novel semantic view synthesis, label
denoising, super-resolution, label interpolation and multi-view semantic label
fusion in visual semantic mapping systems.","Enabling intelligent agents, such as indoor mobile robots, to plan contextsensitive actions in their environ ment requires both a geometric and semantic understanding of the scene. Machine learning methods have proven to be valuable in both geometric and semantic prediction tasks, but the performance of these methods suffers when the dis tribution of the training data does not match the scenes ob served at testtime. Though the issue can be mitigated by gathering costly annotated data or semisupervised learn ing, it is not always feasible in openset scenarios with var ious known and unknown classes. For this reason, it is ad vantageous to have methods that can selfsupervise. In par ticular, there has been recent success in using scenespeciÔ¨Åc methods (e.g. NeRF [16]) that represent the shape and ra diance of a single scene with a neural network trained from scratch using only images and associated camera poses. Semantic scene understanding means attaching class la Fusionvia LearningLabel DenoisingSuperResolutionLabelPropagationLabel Synthesis LabelInterpolationFigure 1: Neural radiance Ô¨Åelds (NeRF) jointly encoding appearance and geometry contain strong priors for segmen tation and clustering. We build upon this to create a scene speciÔ¨Åc 3D semantic representation, SemanticNeRF, and show that it can be efÔ¨Åciently learned with inplace super vision to perform various potential applications. bels to a geometric model. The tasks of estimating the geometry of a scene and predicting its semantic labels are strongly related, as parts of a scene that have similar shape are more likely to belong to the same semantic category than those which differ greatly. This has been shown in work on multitask learning [9, 33], where networks that simultane ously predict both shape and semantics perform better than when the tasks are tackled separately. Unlike scene geometry, however, semantic classes are a humandeÔ¨Åned concept and it is not possible to semanti cally label a novel scene in a purely selfsupervised man ner. The best that could be achieved would be to cluster selfsimilar structures of a scene into categories; but some labelling would always be needed to associate these clusters with humandeÔ¨Åned semantic classes. In this paper, we show how to design a scenespeciÔ¨Åc network for joint geometric and semantic prediction and train it on images from a single scene with only weak se mantic supervision (and no geometric supervision). Be cause our single network must generate both geometry andarXiv:2103.15875v2  [cs.CV]  21 Aug 2021semantics, the correlation between these tasks means that semantics prediction can beneÔ¨Åt from the smoothness, co herence and selfsimilarity learned by selfsupervision for geometry. In addition, multiview consistency is inherent to the training process and enables the network to produce accurate semantic labels of the scene, including for views that are substantially different from any in the input set. Our system takes as input a set of RGB images with as sociated known camera poses. We also supply some partial or noisy semantic labels for the images, such as ground truth labels for a small fraction of the images, or noisy or coarse label maps for a higher number of images. We train our net work to jointly produce implicit 3D representations of both the geometry and semantics for the whole scene. We evaluate our system both quantitatively and qualita tively on scenes from the Replica dataset [28], and quali tatively on realworld scenes from the ScanNet dataset [3]. Generating dense semantic labels for a whole scene from partial or noisy input labels is important for practical appli cations, like when a robot encounters a new scene and either only a small amount of insitu labelling is feasible, or only an imperfect singleview network is available. 2. Related Work "
404,Learning to Align Multi-Camera Domains using Part-Aware Clustering for Unsupervised Video Person Re-Identification.txt,"Most video person re-identification (re-ID) methods are mainly based on
supervised learning, which requires cross-camera ID labeling. Since the cost of
labeling increases dramatically as the number of cameras increases, it is
difficult to apply the re-identification algorithm to a large camera network.
In this paper, we address the scalability issue by presenting deep
representation learning without ID information across multiple cameras.
Technically, we train neural networks to generate both ID-discriminative and
camera-invariant features. To achieve the ID discrimination ability of the
embedding features, we maximize feature distances between different person IDs
within a camera by using a metric learning approach. At the same time,
considering each camera as a different domain, we apply adversarial learning
across multiple camera domains for generating camera-invariant features. We
also propose a part-aware adaptation module, which effectively performs
multi-camera domain invariant feature learning in different spatial regions. We
carry out comprehensive experiments on three public re-ID datasets (i.e.,
PRID-2011, iLIDS-VID, and MARS). Our method outperforms state-of-the-art
methods by a large margin of about 20\% in terms of rank-1 accuracy on the
large-scale MARS dataset.","Person reidentiÔ¨Åcation (reID) [55, 27, 46, 5, 44, 35, 51, 39, 4] aims to match IDs of a personofinterest across multiple distinct camera views. These days videobased re ID [9, 43, 24, 56, 49, 12, 45] has been extensively stud ied in video surveillance systems for public safety. Among the reID methods, supervised learning approaches lead to substantial performance improvement. However, anno tating person IDs across multiple cameras entails signiÔ¨Å cantly high labor costs. This timeconsuming labeling work makes reidentiÔ¨Åcation systems hard to be applied in real world situations since the cost increases dramatically as the Figure 1: We map the features of people extracted from Imagenet pretrained neural networks to a 2D space using tSNE [30]. The numbers in different colors denote cam era domains. It shows that a large domain gap induces the cluster within a camera, which implies that controlling the camera domain gap is important to improve the ID discrim ination ability of reidentiÔ¨Åcation systems. number of cameras increases. Therefore, a line of work [33, 41, 6, 22, 47, 48, 46] focuses on unsupervised reID approaches that propose to learn IDdiscriminative feature representations without crosscamera person ID labeling. The major obstacle of unsupervised person re identiÔ¨Åcation is camera domain discrepancy generated by the difference of viewpoint and background as shown in Fig. 1. Recent studies focus on training a model from an additional labeled source dataset and transfer the knowl 1arXiv:1909.13248v4  [cs.CV]  13 May 2020edge to unlabeled target camera domains [5, 33, 42] or iteratively updating a training set using reliable top Ksam ples across multiplecamera domains [27, 47, 46, 23, 48]. However, performance degradation occurs while transfer ring knowledge between different domains. Also, adopting topKsampling is hard to select reliable samples when the camera domain gap is large. Therefore, even though the aforementioned two groups show promising performance in small camera systems [43, 13], the nature of imperfect translation and domain locality degrades the performance of large camera systems [53, 54]. The main motivation of this paper is to construct a feature space that considers the relationship of all cam eras without information transfer between camera domains. To this end, we propose a simple yet effective approach, named PartsAware camera Domain Alignment Learning (PADAL). With a carefully designed architecture, we Ô¨Årst maximize the distance between different ID features at each camera domain by using a metric learning approach. While maintaining a feature discrimination ability, we obtain the domaininvariant features by adopting the concept of do main adversarial learning [16, 37]. Note that our work dif fers from conventional domain adaptation methods in the point that every camera domain has a different number of IDs. This is because not all people move through all cam eras in the system. For example, considering a fourcamera system in a real application, it is possible that a person moves through cameras 1, 2, and 4 but does not appear in the camera 3. To further improve the effect of multicamera domain alignment, we propose a Partaware Adaptation Module (PAM). Most images used in person reidentiÔ¨Åcation are obtained by the detection algorithm [7] or hand labeling. Therefore, it is natural that each body part is located in a similar area in the reID images. For example, head ,torso , andlegsusually appear on the top, middle, and bottom part of images, respectively. We leverage this prior informa tion to reduce camera domain discrepancy. Technically, we make the clusters of the similar spatial features from the embedding feature maps by using the unsupervised cluster ing algorithm (i.e., Kmeans). According to the locations of clustered spatial features, we assign a specialized part domain discriminator for each cluster. We Ô¨Ånd that aligning part features more effectively generates camerainvariant features than aligning global features. To sum up, our main contributions can be summarized as follows: 1) We propose a novel learning scheme for un supervised video person reidentiÔ¨Åcation, named PADAL, which aims to construct the embedding space that repre sents camerainvariant and IDdiscriminative features. 2) To enhance the effectiveness of PADAL, we present a Part aware Adaptation Module (PAM) to effectively minimize the discrepancy from multiple camera domains by aligning partlevel distributions. 3) We conduct extensive exper iments on three public videobased person reID datasets (PRID2011 [13], iLIDSVID [43], and MARS [53]). Our experimental results show that PADAL is more effective on a large camera setting than other methods, which is an advantage to a realworld application. The proposed method improves video reID performance on the large scale MARS dataset up to 20% in rank1 accuracy. 2. Related Work "
426,Deep Bayesian Active Semi-Supervised Learning.txt,"In many applications the process of generating label information is expensive
and time consuming. We present a new method that combines active and
semi-supervised deep learning to achieve high generalization performance from a
deep convolutional neural network with as few known labels as possible. In a
setting where a small amount of labeled data as well as a large amount of
unlabeled data is available, our method first learns the labeled data set. This
initialization is followed by an expectation maximization algorithm, where
further training reduces classification entropy on the unlabeled data by
targeting a low entropy fit which is consistent with the labeled data. In
addition the algorithm asks at a specified frequency an oracle for labels of
data with entropy above a certain entropy quantile. Using this active learning
component we obtain an agile labeling process that achieves high accuracy, but
requires only a small amount of known labels. For the MNIST dataset we report
an error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These
results are obtained without employing any special network architecture or data
augmentation.","In recent years deep learning has shown great po tential in solving classication and regression tasks of increasing complexity and diculty. For aca demic purposes, several labeled data sets with as sociated tasks are available to support and facilitate research on machine learning. Though in many practical applications (e.g. in industry, medicine and microbiology) where raw data is available in abundance, labeled information is not readily avail able and the process of generating labels can be time consuming and expensive. Therefore, the de velopment of methods that provide strong predic tive models from as few labels as possible is a eld of high interest. The elds of active learning and semisupervised learning address this issue and provide two ap Bergische Universiat at Wuppertal, Fac ulty of Mathematics and Natural Sciences, frottmann,kkahlg@math.uniwuppertal.de , hanno.gottschalk@uniwuppertal.deproaches to obtain strong predictive models using only few labels, see Gal et al. 2016; Hu et al. 2017; Kingma et al. 2014a; Lee 2013; Pitelis et al. 2014; Rasmus et al. 2015; Rifai et al. 2011; Weston et al. 2012. They both assume a situation where the complete set of data is large, but labels are known only for a small fraction of it. The eld of semisupervised learning has a long history. Already in Suddarth et al. 1990 unlabeled data had been injected into the training of neural networks in order to improve generalization perfor mance. Most approaches rely on the Expectation Maximization (cf. Dempster et al. 1977, EM) tech nique which is a clustering algorithm. In the semi supervised context, EM is used to assign unlabeled data to a nite number of clusters which are ini tially dened by the small set of labeled samples. That is, an initial model is trained and then, using the resulting model, labels are assigned to unla beled data, which in turn are used to further train the model. The pseudolabel approach, introduced in Lee 2013, is such an EM technique. It uses the la bels predicted by the neural network itself and can be viewed as well as an auxiliary loss in the training phase which is inserted to reduce classication en tropy on the unlabeled data. It is well known that the EM strategy works well in presence of low den sity class separation. Thus it is unclear if and how this approach is able to adequately classify sam ples with high classication uncertainty. In case a quantitative measure of classication uncertainty can be dened, unlabeled data should only be used for training if their uncertainty is small. However, some samples typically retain high uncertainty in the semisupervised training cycle. This is where active learning comes into play, in that it is most valuable to acquire ground truth labels for samples with high classication uncertainty and add those to training. In this way active and semisupervised learning complement each other naturally. Both learning approaches benet from good un certainty quantication mechanisms. With the ad vent of MonteCarlo (MC) dropout Gal et al. 2016, we have an instrument at hand that makes it fea sible to construct sensitive metrics to monitor clas sication uncertainty. Bayesian inference has been used in an active deep learning approach introduced 1arXiv:1803.01216v1  [cs.LG]  3 Mar 2018in Gal et al. 2017. In recent years, there were also eorts on design ing specialized network architectures that incorpo rate components like denoising autoencoders, see Rasmus et al. 2015. Also deep generative models were used for semisupervised learning, see Kingma et al. 2014b. Combining the active learning and the semi supervised learning track, a method for synthetic aperture radar image recognition has been pub lished in Gao et al. 2017. In this paper, we present a deep Bayesian Active SemiSupervised learning (deepBASS) approach that is based on an EM deep learning approach for classication tasks paired with an active learn ing component and approximate Bayesian uncer tainty. We rst train a Convolutional Neural Net work (CNN) on a small sample of labeled training data. Afterwards we employ the EM technique, i.e., we iteratively predict classes and assign these as pseudolabels to the unlabeled data set. Then, we train one epoch on the pseudolabeled data and the groundtruthlabeled data. While doing so, we make sure that the prediction accuracy on the ground truth remains high. During this process, the algorithm asks an oracle for additional label information where the neural network shows in creased classication uncertainty, e.g., high classi cation entropy. For all predictions and uncertainty estimations we incorporate MC dropout inference. The remainder of this work is structured as fol lows: In section 2 we classify our method with re spect to existing approaches in the literature. Then we introduce our method in detail in section 3 in cluding all necessary notations. Using a simple toy example in section 4 we motivate the combination of active and semisupervised learning. Using the MNIST dataset, we compare two settings in sec tion 5 where on one hand all unlabeled data is present in training from the beginning and where on the other hand unlabeled data is added only in crementally. Both settings are combined with two dierent label acquisition policies. Concluding the experiments we compare our method with other semisupervised and active learning approaches. 2 Related Work "
118,Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision.txt,"Deep learning has emerged as a versatile tool for a wide range of NLP tasks,
due to its superior capacity in representation learning. But its applicability
is limited by the reliance on annotated examples, which are difficult to
produce at scale. Indirect supervision has emerged as a promising direction to
address this bottleneck, either by introducing labeling functions to
automatically generate noisy examples from unlabeled text, or by imposing
constraints over interdependent label decisions. A plethora of methods have
been proposed, each with respective strengths and limitations. Probabilistic
logic offers a unifying language to represent indirect supervision, but
end-to-end modeling with probabilistic logic is often infeasible due to
intractable inference and learning. In this paper, we propose deep
probabilistic logic (DPL) as a general framework for indirect supervision, by
composing probabilistic logic with deep learning. DPL models label decisions as
latent variables, represents prior knowledge on their relations using weighted
first-order logical formulas, and alternates between learning a deep neural
network for the end task and refining uncertain formula weights for indirect
supervision, using variational EM. This framework subsumes prior indirect
supervision methods as special cases, and enables novel combination via
infusion of rich domain and linguistic knowledge. Experiments on biomedical
machine reading demonstrate the promise of this approach.","Deep learning has proven successful in a wide range of NLP tasks (Bahdanau et al., 2014; Bengio et al., 2003; Clark and Manning, 2016; Hermann et al., 2015; Sutskever et al., 2014). The versatility stems from its capacity of learning a compact rep resentation of complex input patterns (Goodfellow This work was conducted at Microsoft Research. Deep LearningProbabilistic LogicKnowledge Virtual Evidence Latent VariableIndirect SupervisionFigure 1 : Deep Probabilistic Logic: A general framework for combining indirect supervision strategies by composing probabilistic logic with deep learning. Learning amounts to maximizing conditional likelihood of virtual evidence given in put by summing up latent label decisions. et al., 2016). However, success of deep learning is bounded by its reliance on labeled examples, which are expensive and timeconsuming to produce. In direct supervision has emerged as a promising di rection for breaching the annotation bottleneck. A powerful paradigm is joint inference (Chang et al., 2007; Poon and Domingos, 2008; Druck et al., 2008; Ganchev et al., 2010), which leverages linguistic and domain knowledge to impose con straints over interdependent label decisions. More recently, another powerful paradigm, often loosely called weak supervision , has gained in popularity. The key idea is to introduce labeling functions to automatically generate (noisy) training examples from unlabeled text. Distant supervision is a promi nent example that used existing knowledge bases for this purpose (Craven and Kumlien, 1999; Mintz et al., 2009). Data programming went further by soliciting labeling functions from domain experts (Ratner et al., 2016; Bach et al., 2017). Indirectsupervision methods have achieved re markable successes in a number of NLP tasks, but they also exhibit serious limitations. Distant su pervision often produces incorrect labels, whereas labeling functions from data programming vary inarXiv:1808.08485v1  [cs.CL]  26 Aug 2018The deletion mutation onexon 19ofEGFR gene was present in16 patients, while theL858Epoint mutation onexon 21wasnoted in10. Allpatients were treated with gefitinib andshowed apartial response . TREAT( Gefitinib, EGFR, L858E )Figure 2 : Example of crosssentence relation extrac tion for precision cancer treatment. quality and coverage, and may contradict with each other on individual instances. Joint inference incurs greater modeling complexity and often requires specialized learning and inference procedures. Since these methods draw on diverse and often orthogonal sources of indirect supervision, com bining them may help address their limitations and amplify their strengths. Probabilistic logic offers an expressive language for such an integration, and is well suited for resolving noisy and contradictory in formation (Richardson and Domingos, 2006). Un fortunately, probabilistic logic generally incurs in tractable learning and inference, often rendering endtoend modeling infeasible. In this paper, we propose deep probabilistic logic (DPL) as a unifying framework for indirect supervision (Figure 1). SpeciÔ¨Åcally, we made four contributions. First, we introduce a modular design to compose probabilistic logic with deep learning, with a supervision module that represents indirect supervision using probabilistic logic, and a predic tion module that performs the end task using a deep neural network. Label decisions are modeled as latent variables and serve as the interface between the two modules. Second, we show that all popular forms of indi rect supervision can be represented in DPL by gen eralizing virtual evidence (Subramanya and Bilmes, 2007; Pearl, 2014). Consequently, these diverse methods can be easily combined within a single framework for mutual ampliÔ¨Åcation. Third, we show that our problem formulation yields a welldeÔ¨Åned learning objective (maximiz ing conditional likelihood of virtual evidence). We proposed a modular learning approach by decom posing the optimization over the supervision and prediction modules, using variational EM, which enables us to apply stateoftheart methods for probabilistic logic and deep learning. Finally, we applied DPL to biomedical machine reading (Quirk and Poon, 2017; Peng et al., 2017). Biomedicine offers a particularly attractive appli cation domain for exploring indirect supervision. Biomedical literature grows by over one millioneach year1, making it imperative to develop ma chine reading methods for automating knowledge curation (Figure 2). While crowd sourcing is hardly applicable, there are rich domain knowledge and structured resources to exploit for indirect supervi sion. Using crosssentence relation extraction and entity linking as case studies, we show that distant supervision, data programming, and joint inference can be seamlessly combined in DPL to substan tially improve machine reading accuracy, without requiring any manually labeled examples.2 2 Related Work "
474,Beat by Beat: Classifying Cardiac Arrhythmias with Recurrent Neural Networks.txt,"With tens of thousands of electrocardiogram (ECG) records processed by mobile
cardiac event recorders every day, heart rhythm classification algorithms are
an important tool for the continuous monitoring of patients at risk. We utilise
an annotated dataset of 12,186 single-lead ECG recordings to build a diverse
ensemble of recurrent neural networks (RNNs) that is able to distinguish
between normal sinus rhythms, atrial fibrillation, other types of arrhythmia
and signals that are too noisy to interpret. In order to ease learning over the
temporal dimension, we introduce a novel task formulation that harnesses the
natural segmentation of ECG signals into heartbeats to drastically reduce the
number of time steps per sequence. Additionally, we extend our RNNs with an
attention mechanism that enables us to reason about which heartbeats our RNNs
focus on to make their decisions. Through the use of attention, our model
maintains a high degree of interpretability, while also achieving
state-of-the-art classification performance with an average F1 score of 0.79 on
an unseen test set (n=3,658).","Cardiac arrhythmias are a heterogenous group of con ditions that is characterised by heart rhythms that do not follow a normal sinus pattern. One of the most com mon arrhythmias is atrial Ô¨Åbrillation (AF) with an age dependant population prevalence of 2.33.4%[1]. Due to the increased mortality associated with arrhythmias, re  ceiving a timely diagnosis is of paramount importance for patients [1, 2]. To diagnose cardiac arrhythmias, medical professionals typically consider a patient‚Äôs electrocard io gram (ECG) as one of the primary factors [2]. In the past, clinicians recorded these ECGs mainly using multilead clinical monitors or Holter devices. However, the recent advent of mobile cardiac event recorders has given patients the ability to remotely record short ECGs using devices with a single lead.We propose a machinelearning approach based on re current neural networks (RNNs) to differentiate between various types of heart rhythms in this more challenging set ting with just a single lead and short ECG record lengths. To ease learning of dependencies over the temporal dimen sion, we introduce a novel task formulation that harnesses the natural beatwise segmentation of ECG signals. In ad dition to utilising several heartbeat features that have be en shown to be highly discriminative in previous works, we also use stacked denoising autoencoders (SDAE) [3] to capture differences in morphological structure. Further more, we extend our RNNs with a soft attention mech anism [4‚Äì7] that enables us to reason about which ECG segments the RNNs prioritise for their decision making. 2. Methodology "
259,Towards Unsupervised Deep Graph Structure Learning.txt,"In recent years, graph neural networks (GNNs) have emerged as a successful
tool in a variety of graph-related applications. However, the performance of
GNNs can be deteriorated when noisy connections occur in the original graph
structures; besides, the dependence on explicit structures prevents GNNs from
being applied to general unstructured scenarios. To address these issues,
recently emerged deep graph structure learning (GSL) methods propose to jointly
optimize the graph structure along with GNN under the supervision of a node
classification task. Nonetheless, these methods focus on a supervised learning
scenario, which leads to several problems, i.e., the reliance on labels, the
bias of edge distribution, and the limitation on application tasks. In this
paper, we propose a more practical GSL paradigm, unsupervised graph structure
learning, where the learned graph topology is optimized by data itself without
any external guidance (i.e., labels). To solve the unsupervised GSL problem, we
propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME
for abbreviation) with the aid of self-supervised contrastive learning.
Specifically, we generate a learning target from the original data as an
""anchor graph"", and use a contrastive loss to maximize the agreement between
the anchor graph and the learned graph. To provide persistent guidance, we
design a novel bootstrapping mechanism that upgrades the anchor graph with
learned structures during model learning. We also design a series of graph
learners and post-processing schemes to model the structures to learn.
Extensive experiments on eight benchmark datasets demonstrate the significant
effectiveness of our proposed SUBLIME and high quality of the optimized graphs.","Recent years have witnessed the prosperous development of graph based applications in numerous domains, such as chemistry, bioin formatics and cybersecurity. As a powerful deep learning tool to model graphstructured data, graph neural networks (GNNs) have drawn increasing attention and achieved stateoftheart perfor mance in various graph analytical tasks, including node classifica tion [ 22,40], link prediction [ 21,32], and node clustering [ 42,55]. GNNs usually follow a messagepassing scheme, where node repre sentations are learned by aggregating information from the neigh bors on an observed topology (i.e., the original graph structure). Most GNNs rely on a fundamental assumption that the original structure is credible enough to be viewed as groundtruth informa tion for model training. Such assumption, unfortunately, is usually violated in realworld scenarios, since graph structures are usu ally extracted from complex interaction systems which inevitably contain uncertain, redundant, wrong and missing connections [ 45]. Such noisy information in original topology can seriously damage the performance of GNNs. Besides, the reliance on explicit struc tures hinders GNNs‚Äô broad applicability. If GNNs are capable of uncovering the implicit relations between samples, e.g., two images containing the same object, they can be applied to more general domains like vision and language. To tackle the aforementioned problems, deep graph structure learning (GSL) is a promising solution that constructs and improves the graph topology with GNNs [ 7,12,20,58]. Concretely, these methods parameterize the adjacency matrix with a probabilistic model [ 12,45], full parameterization [ 20] or metric learning model [7,11,53], and jointly optimize the parameters of the adjacency matrix and GNNs by solving a downstream task (i.e., node classifi cation) [ 58]. However, existing methods learn graph structures in a supervised scenario, which brings the following issues: (1) The reliance on label information. In supervised GSL methods, human annotated labels play an important role in providing supervision signal for structure improvement. Such reliance on labels limits the application of supervised GSL on more general cases where annotation is unavailable. (2) The bias of learned edge distribution. Node classification usually follows a semisupervised setting, where only a small fraction of nodes (e.g., 140/2708 in Cora dataset) are under the supervision of labels. As a result, the connections among these nodes and their neighbors would receive more guidance inarXiv:2201.06367v1  [cs.LG]  17 Jan 2022WWW ‚Äô22, April 25‚Äì29, 2022, Lyon, France. Liu et al. Node Labels DataLearnedGraph ‚Ä¶‚Ä¶‚Ä¶GNNbased Model‚Ä¶‚Ä¶‚Ä¶ImproveSuperviseInput DataInputLearnedGraphImproveGNNbased ModelBenefitBenefitNode ClassificationNode ClusteringLinkPrediction‚Ä¶Downstream TasksNode ClassificationTask for Supervision (a) Supervised GSL paradigm. Node Labels DataLearnedGraph ‚Ä¶‚Ä¶‚Ä¶GNNbased Model‚Ä¶‚Ä¶‚Ä¶ImproveSuperviseInput DataInputLearnedGraphImproveGNNbased ModelBenefitBenefitNode ClassificationNode ClusteringLinkPrediction‚Ä¶Downstream TasksNode ClassificationTask for Supervision (b) Our proposed unsupervised GSL paradigm. Figure 1: Concept maps of (a) the existing supervised GSL paradigm and (b) our proposed unsupervised GSL paradigm. structure learning, while the relations between nodes far away from them are rarely discovered by GSL [ 11]. Such imbalance leads to the bias of edge distribution, affecting the quality of the learned structures. (3) The limitation on downstream tasks. In existing meth ods, the structure is specifically learned for node classification, so it may contain more taskspecific information rather than general knowledge. Consequently, the refined topology may not benefit other downstream tasks like link prediction or node clustering, indicating the poor generalization ability of the learned structures. To address these issues, in this paper, we investigate a novel un supervised learning paradigm for GSL, namely unsupervised graph structure learning . As compared in Fig. 1, in our learning paradigm, structures are learned by data itself without any external guidance (i.e., labels), and the acquired universal, edgeunbiased topology can be freely applied to various downstream tasks. In this case, one natural question can be raised: how to provide sufficient supervi sion signal for unsupervised GSL? To answer this, we propose a novel StrUcture Bootstrapping contrastive LearnIng fra MEwork (SUBLIME for abbreviation) to learn graph structures with the aid of selfsupervised contrastive learning [ 25]. Concretely, our method constructs an ‚Äúanchor graph‚Äù from the original data to guide struc ture optimization, with a contrastive loss to maximize the mutual information (MI) between anchor graph and the learned structure. Through maximizing their consistency, informative hidden connec tions can be discovered, which well respects the node proximity conveyed by the original features and structures. Meanwhile, as we optimize the contrastive loss on the representations of every node, all potential edge candidates will receive the essential super vision, which promotes a balanced edge distribution in the inferred topology. Furthermore, we design a bootstrapping mechanism to update anchor graph with the learned edges, which provides a selfenhanced supervision signal for GSL. Besides, we carefully de sign multiple graph learners and postprocessing schemes to model graph topology for diverse data. In summary, our core contributions are threefold: ‚Ä¢Problem. We propose a novel unsupervised learning para digm for graph structure learning, which is more practical and challenging than the existing supervised counterpart. To the best of our knowledge, this is the first attempt to learn graph structures with GNNs in an unsupervised setting. ‚Ä¢Algorithm. We propose a novel unsupervised GSL method SUBLIME , which guides structure optimization by maximiz ing the agreement between the learned structure and a crafted selfenhanced learning target with contrastive learning.‚Ä¢Evaluations. We perform extensive experiments to corrobo rate the effectiveness and analyze the properties of SUBLIME via thorough comparisons with stateoftheart methods on eight benchmark datasets. 2 RELATED WORK "
193,Progressive Ensemble Networks for Zero-Shot Recognition.txt,"Despite the advancement of supervised image recognition algorithms, their
dependence on the availability of labeled data and the rapid expansion of image
categories raise the significant challenge of zero-shot learning. Zero-shot
learning (ZSL) aims to transfer knowledge from labeled classes into unlabeled
classes to reduce human labeling effort. In this paper, we propose a novel
progressive ensemble network model with multiple projected label embeddings to
address zero-shot image recognition. The ensemble network is built by learning
multiple image classification functions with a shared feature extraction
network but different label embedding representations, which enhance the
diversity of the classifiers and facilitate information transfer to unlabeled
classes. A progressive training framework is then deployed to gradually label
the most confident images in each unlabeled class with predicted pseudo-labels
and update the ensemble network with the training data augmented by the
pseudo-labels. The proposed model performs training on both labeled and
unlabeled data. It can naturally bridge the domain shift problem in visual
appearances and be extended to the generalized zero-shot learning scenario. We
conduct experiments on multiple ZSL datasets and the empirical results
demonstrate the efficacy of the proposed model.","Despite the effectiveness of deep convolutional neural networks (CNNs) on supervised image classiÔ¨Åcation prob lems, zero shot learning (ZSL) remains a challenging and fundamental problem due to the rapid expansion of image categories and the lacking in labeled training data. As a special unsupervised domain adaptation, ZSL aims to trans fer information from the source domain, a set of training classes with labeled data, to make predictions in the target domain, a set of test classes with only unlabeled data. Dif ferent from standard domain adaptation, in ZSL the labeled training classes and unlabeled test classes have no overlaps‚Äì they are entirely disjoint. Based on the visibility of the instance labels, the training classes and the test classes are usually referred to as seen andunseen classes respectively. Existing zeroshot image recognitions have centered on deploying label embeddings in a common semantic space, e.g., in terms of high level visual attributes, to bridge the domain gap between seen andunseen classes. For example, animals share some common characteristics such as ‚Äòblack‚Äô, ‚Äòyellow‚Äô, ‚Äòspots‚Äô, ‚Äòstripes‚Äô and so on. Thus each animal class, either seen or unseen, can be represented as a binary vector in the semantic attribute space, with each element denoting the appearance/absence of certain attribute. Much ZSL effort in this direction has focused on developing effective mapping models from the input visual feature space to the semantic label embedding space [ 24,10,6,19], or learning suitable compatibility functions between the two spaces [ 2,27,33], to facilitate prediction information transfer from the seen classes to the unseen classes. However, these methods iden tify visualsemantic mappings only on the labeled seen class data, which poses a fundamental domain shift problem due to the appearance variations of visual attributes across seen andunseen classes, and has negative impact on crossclass generalization (i.e., ZSL performance) [11, 18]. In this paper, we propose a novel ZSL framework with an progressive ensemble network to address the domain shift problem and improve the generalization ability of ZSL. Ex isting ZSL works rely on a single set of label embeddings to build interclass label relations for knowledge transfer, which can hardly to be suitable for all the unseen classes. In stead we construct a deep ensemble network that consists of multiple image classiÔ¨Åcation functions with a shared feature extraction convolutional neural network and different label embedding representations. Each label embedding represen tation facilitates information transfer from the seen classes to a subset of unseen classes, while enhancing the diversity of the multiple classiÔ¨Åers. By exploiting multiple classiÔ¨Åers in an ensemble manner, we expect the ensemble network can overcome the prediction noise and class bias in the original label embeddings to gain robust zeroshot predictions. More over, we exploit the unlabeled data from unseen classes in 1arXiv:1805.07473v2  [cs.LG]  6 Apr 2019a progressive ensemble framework to overcome the domain shift problem. In each iteration, we select the most conÔ¨Å dently predicted unlabeled instances from each unseen class under the current ensemble network, and combine these se lected instances and their predicted pseudolabels with the original labeled seen class data together to reÔ¨Åne the en semble network parameters, especially its feature extraction component. By incorporating the unseen class instances into the ensemble network training and dynamically reÔ¨Åne the selected instances in each iteration, we expect the dynamic progressive training process can effectively avoid the issue of overÔ¨Åtting to the seen classes and improve the general ization ability of the ensemble network on unseen classes. With the ensemble network directly handling multiclass classiÔ¨Åcation over all classes, the proposed approach can be conveniently extended to address generalized ZSL. We con duct experiments on three standard ZSL datasets under both conventional ZSL and generalized ZSL settings. The empiri cal results demonstrate the proposed approach outperforms the stateoftheart ZSL methods. 2. Related Work "
438,Neural Architectures for Nested NER through Linearization.txt,"We propose two neural network architectures for nested named entity
recognition (NER), a setting in which named entities may overlap and also be
labeled with more than one label. We encode the nested labels using a
linearized scheme. In our first proposed approach, the nested labels are
modeled as multilabels corresponding to the Cartesian product of the nested
labels in a standard LSTM-CRF architecture. In the second one, the nested NER
is viewed as a sequence-to-sequence problem, in which the input sequence
consists of the tokens and output sequence of the labels, using hard attention
on the word whose label is being predicted. The proposed methods outperform the
nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and
Czech CNEC. We also enrich our architectures with the recently published
contextual embeddings: ELMo, BERT and Flair, reaching further improvements for
the four nested entity corpora. In addition, we report flat NER
state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003
English.","In nested named entity recognition, entities can be overlapping and labeled with more than one la bel such as in the example ‚ÄúThe Florida Supreme Court‚Äù containing two overlapping named entities ‚ÄúThe Florida Supreme Court‚Äù and‚ÄúFlorida‚Äù .1 Recent publications on nested named entity recognition involve stacked LSTMCRF NE rec ognizer ( Ju et al. ,2018 ), or a construction of a special structure that explicitly captures the nested entities, such as a constituency graph ( Finkel and Manning ,2009 ) or various modiÔ¨Åcations of a di rected hypergraph ( Lu and Roth ,2015 ;Katiyar and Cardie ,2018 ;Wang and Lu ,2018 ). 1Example from ACE2004 ( Doddington et al. ,2004 ), https://catalog.ldc.upenn.edu/LDC2005T09 .We propose two completely neural network ar chitectures for nested nested named entity recog nition which do not explicitly build or model any structure and infer the relationships between nested NEs implicitly: ‚Ä¢In the Ô¨Årst model, we concatenate the nested entity multiple labels into one multilabel, which is then predicted with a standard LSTMCRF ( Lample et al. ,2016 ) model. The advantages of this model are simplicity and effectiveness, because an already exist ing NE pipeline can be reused to model the nested entities. The obvious disadvantage is a large growth of NE classes. ‚Ä¢In the second model, the nested entities are encoded in a sequence and then the task can be viewed as a sequencetosequence (seq2seq) task, in which the input sequence are the tokens (forms) and the output se quence are the labels. The decoder predicts labels for each token, until a special label ""<eow>"" (end of word) is predicted and the decoder moves to the next token. The expressiveness of the models depends on a nonambiguous encoding of the nested entity structure. We use an enhanced BILOU scheme de scribed in Section 4.1. The proposed models surpass the current nested NER state of the art on four nested entity cor pora: ACE2004, ACE2005, GENIA and Czech CNEC. When the recently introduced contextual embeddings ‚Äì ELMo ( Peters et al. ,2018 ), BERT (Devlin et al. ,2018 ) and Flair ( Akbik et al. ,2018 ) ‚Äì are added to the architecture, we reach further improvements for the above mentioned nested en tity corpora and also exceed current state of the art for CoNLL2002 Dutch and Spanish and for CoNLL2003 English.2 Related Work "
589,L_DMI: An Information-theoretic Noise-robust Loss Function.txt,"Accurately annotating large scale dataset is notoriously expensive both in
time and in money. Although acquiring low-quality-annotated dataset can be much
cheaper, it often badly damages the performance of trained models when using
such dataset without particular treatment. Various methods have been proposed
for learning with noisy labels. However, most methods only handle limited kinds
of noise patterns, require auxiliary information or steps (e.g. , knowing or
estimating the noise transition matrix), or lack theoretical justification. In
this paper, we propose a novel information-theoretic loss function,
$\mathcal{L}_{DMI}$, for training deep neural networks robust to label noise.
The core of $\mathcal{L}_{DMI}$ is a generalized version of mutual information,
termed Determinant based Mutual Information (DMI), which is not only
information-monotone but also relatively invariant. \emph{To the best of our
knowledge, $\mathcal{L}_{DMI}$ is the first loss function that is provably
robust to instance-independent label noise, regardless of noise pattern, and it
can be applied to any existing classification neural networks straightforwardly
without any auxiliary information}. In addition to theoretical justification,
we also empirically show that using $\mathcal{L}_{DMI}$ outperforms all other
counterparts in the classification task on both image dataset and natural
language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a
variety of synthesized noise patterns and noise amounts, as well as a
real-world dataset Clothing1M. Codes are available at
https://github.com/Newbeeer/L_DMI .","Deep neural networks, together with large scale accurately annotated datasets, have achieved remark able performance in a great many classiÔ¨Åcation tasks in recent years ( e.g.,[11,18]). However, it is usually money and time consuming to Ô¨Ånd experts to annotate labels for large scale datasets. While collecting labels from crowdsourcing platforms like Amazon Mechanical Turk is a potential way to get annotations cheaper and faster, the collected labels are usually very noisy. The noisy labels hampers the performance of deep neural networks since the commonly used cross entropy loss is not noiserobust. This raises an urgent demand on designing noiserobust loss functions. Some previous works have proposed several loss functions for training deep neural networks with noisy labels. However, they either use auxiliary information[12,29](e.g., having an additional set of clean data or the noise transition matrix) or steps[20,33](e.g. estimating the noise transition matrix), or ‚àóEqual Contribution. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1909.03388v2  [cs.LG]  4 Nov 2019make assumptions on the noise[7,48]and thus can only handle limited kinds of the noise patterns (see perliminaries for deÔ¨Ånition of different noise patterns). One reason that the loss functions used in previous works are not robust to a certain noise pattern, say diagonally nondominant noise, is that they are distancebased, i.e., the loss is the distance between the classiÔ¨Åer‚Äôs outputs and the labels ( e.g. 01 loss, cross entropy loss). When datapoints are labeled by a careless annotator who tends to label the a priori popular class ( e.g. For medical images, given the prior knowledge is 10% malignant and 90% benign, a careless annotator labels ‚Äúbenign‚Äù when the underline true label is ‚Äúbenign‚Äù and labels ‚Äúbenign‚Äù with 90% probability when the underline true label is ‚Äúmalignant‚Äù.), the collected noisy labels have a diagonally nondominant noise pattern and are extremely biased to one class (‚Äúbenign‚Äù). In this situation, the distancedbased losses will prefer the ‚Äúmeaningless classiÔ¨Åer"" who always outputs the a priori popular class (‚Äúbenign‚Äù) than the classiÔ¨Åer who outputs the true labels. To address this issue, instead of using distancebased losses, we propose to employ information theoretic loss such that the classiÔ¨Åer, whose outputs have the highest mutual information with the labels, has the lowest loss. The key observation is that the ‚Äúmeaningless classiÔ¨Åer"" has no information about anything and will be naturally eliminated by the informationtheoretic loss. Moreover, the informationmonotonicity of the mutual information guarantees that adding noises to a classiÔ¨Åer‚Äôs output will make this classiÔ¨Åer less preferred by the informationtheoretic loss. However, the key observation is not sufÔ¨Åcient. In fact, we want an information measure I to satisfy I(classiÔ¨Åer 1‚Äôs output ;noisy labels )>I(classiÔ¨Åer 2‚Äôs output ;noisy labels ) ‚áîI(classiÔ¨Åer 1‚Äôs output ;clean labels )>I(classiÔ¨Åer 2‚Äôs output ;clean labels ): Unfortunately, the traditional Shannon mutual information (MI) does not satisfy the above formula, while we Ô¨Ånd that a generalized information measure, namely, DMI (Determinant based Mutual Information), satisÔ¨Åes the above formula. Like MI, DMI measures the correlation between two random variables. It is deÔ¨Åned as the determinant of the matrix that describes the joint distribution over the two variables. Intuitively, when two random variables are independent, their joint distribution matrix has low rank and zero determinant. Moreover, DMI is not only informationmonotone like MI, but also relatively invariant because of the multiplication property of the determinant. The relative invariance of DMI makes it satisfy the above formula. Based on DMI, we propose a noiserobust loss function LDMI which is simply LDMI(data;classiÔ¨Åer )‚à∂=‚àílog[DMI(classiÔ¨Åer‚Äôs output ;labels)]: As shown in theorem 4.1 later, with LDMI, the following equation holds: LDMI(noisy data ;classiÔ¨Åer )=LDMI(clean data ;classiÔ¨Åer )+noise amount ; and the noise amount is a constant given the dataset. The equation reveals that withLDMI, training with the noisy labels is theoretically equivalent with training with the clean labels in the dataset, regardless of the noise patterns, including the noise amounts . In summary, we propose a novel information theoretic noiserobust loss function LDMI based on a generalized information measure, DMI. Theoretically we show that LDMI is robust to instance independent label noise. As an additional beneÔ¨Åt, it can be easily applied to any existing classiÔ¨Åcation neural networks straightforwardly without any auxiliary information. Extensive experiments have been done on both image dataset and natural language dataset including FashionMNIST, CIFAR10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts as well as a realworld dataset Clothing1M. The results demonstrate the superior performance of LDMI. 2 Related Work "
155,Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness.txt,"In this paper, we present a strategy for training convolutional neural
networks to effectively resolve interference arising from competing hypotheses
relating to inter-categorical information throughout the network. The premise
is based on the notion of feature binding, which is defined as the process by
which activation's spread across space and layers in the network are
successfully integrated to arrive at a correct inference decision. In our work,
this is accomplished for the task of dense image labelling by blending images
based on their class labels, and then training a feature binding network, which
simultaneously segments and separates the blended images. Subsequent feature
denoising to suppress noisy activations reveals additional desirable properties
and high degrees of successful predictions. Through this process, we reveal a
general mechanism, distinct from any prior methods, for boosting the
performance of the base segmentation network while simultaneously increasing
robustness to adversarial attacks.","The advent of Deep Neural Networks (DNNs) has seen overwhelming improvement in dense image labeling tasks [2, 5, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 27, 30, 40], however, for some common benchmarks [10] the rate of improvement has slowed down. While one might assume that barriers to further improvement require changes at the architectural level, it has also been borne out that pretraining across a variety of datasets [26, 33] can improve performance exceeding improvements seen from changing the model architecture. However, there are challenging scenarios for which DNNs have difÔ¨Åculty on regardless of pretraining or architectural changes, such as highly occluded scenes, or objects appearing out of their normal context [35]. It is not clear though, for dense image labeling tasks, how to resolve these speciÔ¨Åc scenarios for more robust prediction quality on a perpixel level. A question that naturally follows from this line of reasoning is: How can the number of locally challenging cases be increased, or the problem made more difÔ¨Åcult in general? In this paper, we address this problem using a principled approach to improve performance and that also implies a more general form of robustness. As inspiration, we look to a paradigm discussed often in the realm of human vision: the binding problem [34, 37]. The crux of this problem is that given a complex decomposition of an image into features that represent different concepts, or different parts of the image, how does one proceed to successfully relate activations corresponding to common sources in the input image to label a whole from its parts, or separate objects. Motivated by the binding problem, a successful solution in the computer vision domain should rely on both determining correspondences in activations among features that represent disparate concepts, and also to associate activations tied to related features that are subject to spatial separation in the image. To address similar issues for the image classiÔ¨Åcation task, recent studies [36, 38, 39] have considered mixing two image examples with constraints on the distribution of features. However, these methods suffer from biases in the dataset used, as they have no strategy when deciding on which images to mix which is crucial for the dense labeling problem. Additionally, these strategies do not adequately separate information from different sources in the image as they only require the network to make a single (classiÔ¨Åcation) prediction during training. In our work, the means of solving the feature binding problem takes a direct form, which involves training networks on a specially designed dataset of mixed images to simultaneously address problems of dense image labeling [3, 27, 30], and blind source separation [12, 16]. Humans show a surprising level of capability in interpreting a superposition (e.g., average) of two images, both interpreting the contents of each scene and determining the membership of local patterns within a given scene. The underlying premise of this work involves producing networks capable of simultaneously performing dense image labeling for pairs of images while also separating labels according to the source images. If one selects pairs on the basis of a weighted average (see Fig. 1 (left)), this allows treatment of the corresponding dense image labeling problem in the absence of source separation by extension. This process supports several objectives: (i) it signiÔ¨Åcantly increases the number of occurrences that are locally ambiguous that need to be resolved to produce a correct categorical assignment, (ii) it forces broader spatial context to be considered in making categorical assignments, and (iii) it stands to create more powerful networks for standard dense labeling tasks and dealing with adversarial perturbations by forcing explicit requirements on how the network uses the input. The end goal of our procedure is to improve overall performance as well as increase the prediction quality on complex images (see Fig. 1 (right)), heavily occluded scenes, and also invoke robustness to challenging adversarial inputs. Our main contributions are as follows:ISLAM, KOWAL, DERPANIS, BRUCE: CATEGORY DEPENDENT MIXUP 3 ‚Ä¢ To the best of our knowledge we present the Ô¨Årst work which applies image blending to the dense labeling task. To this end, we propose a novel training pipeline which simultaneously solves the problems of dense labeling and blind source separation. ‚Ä¢ We further introduce a new categorical clustering strategy which exploits semantic knowledge of the dataset to mix input images based on their class distributions. ‚Ä¢ We show, through extensive quantitative and qualitative experiments, that our pipeline outperforms recent image blending methods [38, 39] on the PASCAL VOC 2012 dataset [10], while simultaneously improving robustness to adversarial attacks. 2 Related Work "
502,Learning Discriminative Features via Label Consistent Neural Network.txt,"Deep Convolutional Neural Networks (CNN) enforces supervised information only
at the output layer, and hidden layers are trained by back propagating the
prediction error from the output layer without explicit supervision. We propose
a supervised feature learning approach, Label Consistent Neural Network, which
enforces direct supervision in late hidden layers. We associate each neuron in
a hidden layer with a particular class label and encourage it to be activated
for input signals from the same class. More specifically, we introduce a label
consistency regularization called ""discriminative representation error"" loss
for late hidden layers and combine it with classification error loss to build
our overall objective function. This label consistency constraint alleviates
the common problem of gradient vanishing and tends to faster convergence; it
also makes the features derived from late hidden layers discriminative enough
for classification even using a simple $k$-NN classifier, since input signals
from the same class will have very similar representations. Experimental
results demonstrate that our approach achieves state-of-the-art performances on
several public benchmarks for action and object category recognition.","Convolutional neural networks (CNN) [ 20] have ex hibited impressive performances in many computer vision tasks such as image classiÔ¨Åcation [ 17], object detection [ 5] and image retrieval [ 27]. When large amounts of training data are available, CNN can automatically learn hierarchi cal feature representations, which are more discriminativ e thanprevioushandcraftedones[ 17]. Encouraged by their impressive performance in static image analysis tasks, several CNNbased approaches have been developed for action recognition in videos [ 12,15, 25,28,35,44]. Although promising results have been re ported, the advantagesof CNN approachesover traditional ones [34] are not as overwhelming for videos as in static images. Comparedtostaticimages,videoshavelargervari ationsin appearanceas well as high complexityintroduced by temporal evolution, which makes learning features for recognition from videos more challenging. On the other ‚àóIndicates equal contributions.hand, unlike largescale and diverse static image data [ 2], annotateddata foraction recognitiontasks is usually insu f Ô¨Åcient, since annotatingmassive videosis prohibitivelye x pensive. Therefore,withonlylimitedannotateddata,lear n ingdiscriminativefeaturesviadeepneuralnetworkcanlea d to severe overÔ¨Åttingand slow convergence. To tackle these issues, previous works have introduced effective practica l techniques such as ReLU [ 24] and Dropout [ 10] to im provetheperformanceofneuralnetworks,buthavenotcon sidered directly improving the discriminative capability of neurons. The features from a CNN are learned by back propagatingpredictionerrorfromtheoutputlayer[ 19],and hidden layers receive no direct guidance on class informa tion. Worse, in verydeep networks,the early hiddenlayers often suffer from vanishing gradients, which leads to slow optimization convergence and the network convergingto a poor local minimum. Therefore, the quality of the learned features of the hidden layers might be potentially dimin ished[43,6]. To tackle these problems, we propose a new supervised deep neural network, Label Consistent Neural Network , to learn discriminative features for recognition. Our ap proachprovidesexplicitsupervision, i.e.label information, to late hidden layers, by incorporating a label consistency constraint called ‚Äúdiscriminative representationerror‚Äù loss, which is combined with the classiÔ¨Åcation loss to form the overallobjectivefunction. ThebeneÔ¨Åtsofourapproachare twofold: (1)with explicitsupervisionto hiddenlayers,t he problemof vanishinggradientscan be alleviated and faster convergence is observed; (2) more discriminative late hid denlayerfeaturesleadtoincreaseddiscriminativepowero f classiÔ¨Åers at the outputlayer; interestingly,the learned dis criminative features alone can achieve good classiÔ¨Åcation performance even with a simple kNN classiÔ¨Åer. In prac tice, our new formulation can be easily incorporated into anyneuralnetworktrainedusingbackpropagation. Ourap proach is evaluated on publicly available action and object recognitiondatasets. Althoughwe onlypresentexperimen talresultsforactionandobjectrecognition,themethodca n be applied to other tasks such as image retrieval, compres sion,restorations etc.,sinceitgeneratesclassspeciÔ¨Åccom pactrepresentations. 11.1.Main Contributions ThemaincontributionsofLCNN arethreefold. ‚Ä¢Byaddingexplicitsupervisiontolatehiddenlayersvia a ‚Äúdiscriminative representation error‚Äù, LCNN learns more discriminative features resulting in better clas siÔ¨Åer training at the output layer. The representa tionsgeneratedbylatehiddenlayersarediscriminative enoughtoachievegoodperformanceusingasimple k NNclassiÔ¨Åer. ‚Ä¢Thelabelconsistencyconstraintalleviatestheproblem of vanishinggradientsand leads to faster convergence during training, especially when limited training data isavailable. ‚Ä¢Weachievestateoftheartperformanceonseveralac tion and object category recognition tasks, and the compact classspeciÔ¨Åc representations generated by LCNNcanbe directlyusedinotherapplications. 2. Related Work "
344,Multi-Label Text Classification using Attention-based Graph Neural Network.txt,"In Multi-Label Text Classification (MLTC), one sample can belong to more than
one class. It is observed that most MLTC tasks, there are dependencies or
correlations among labels. Existing methods tend to ignore the relationship
among labels. In this paper, a graph attention network-based model is proposed
to capture the attentive dependency structure among the labels. The graph
attention network uses a feature matrix and a correlation matrix to capture and
explore the crucial dependencies between the labels and generate classifiers
for the task. The generated classifiers are applied to sentence feature vectors
obtained from the text feature extraction network (BiLSTM) to enable end-to-end
training. Attention allows the system to assign different weights to neighbor
nodes per label, thus allowing it to learn the dependencies among labels
implicitly. The results of the proposed model are validated on five real-world
MLTC datasets. The proposed model achieves similar or better performance
compared to the previous state-of-the-art models.","MultiLabel Text ClassiÔ¨Åcation (MLTC) is the task of assigning one or more labels to each input sample in the corpus. This makes it both a challenging and essential task in Natural Language Processing(NLP). We have a set of labelled training data f(xi;yi)gn i=1; where xi2RDare the input features with Ddimen sion for each data instances and yi2f0;1gare the targets. The vector yihas one in the jth coordinate if the ith data point belongs to jth class. We need to learn a mapping (prediction rule) between the features and the labels, such that we can predict the class label vector yof a new data point xcorrectly. MLTC has many realworld applications, such as text categorization (Schapire and Singer, 2000), tag recommendation (Katakis et al., 2008), information retrieval (Gopal and Yang, 2010), and so on. Before deep learning, the solution to the MLTC task used to focus on traditional machine learning algorithms. In Proceedings of the 12th International Conference on Agents and ArtiÔ¨Åcial Intelligence (ICAART 2020) DOI: 10.5220/0008940304940505 ISBN: 9789897583957 Copyright¬©2020 by SCITEPRESS Science and Technol ogy Publications, Lda. All rights reservedDifferent techniques have been proposed in the literature for treating multilabel classiÔ¨Åcation prob lems. In some of them, multiple singlelabel classi Ô¨Åers are combined to emulate MLTC problems. Other techniques involve modifying singlelabel classiÔ¨Åers by changing their algorithms to allow their use in multilabel problems. The most popular traditional method for solv ing MLTC is Binary Relevance (BR) (Zhang et al., 2018). BR emulates the MLTC task into multiple independent binary classiÔ¨Åcation problems. How ever, it ignores the correlation or the dependencies among labels (Luaces et al., 2012). Binary Rele vance has stimulated research for Ô¨Ånding approaches to capture and explore the label correlations in various ways. Some methods, including Deep Neural Net work (DNN) based and probabilistic based models, have been introduced to model dependencies among labels, such as Hierarchical Text ClassiÔ¨Åcation. (Sun and Lim, 2001), (Xue et al., 2008), (Gopal et al., 2012) and (Peng et al., 2019). Recently Graphbased Neural Networks (Wu et al., 2019) e.g. Graph Convo lution Network (Kipf and Welling, 2016), Graph At tention Networks (Velickovic et al., 2018) and Graph Embeddings (Cai et al., 2017) have received consid erable research attention. This is due to the fact that many realworld problems in complex systems, sucharXiv:2003.11644v1  [cs.CL]  22 Mar 2020as recommendation systems (Ying et al., 2018), so cial networks and biological networks (Fout et al., 2017) etc, can be modelled as machine learning tasks over large networks. Graph Convolutional Network (GCN) was proposed to deal with graph structures. The GCN beneÔ¨Åts from the advantage of the Convo lutional Neural Network(CNN) architecture: it per forms predictions with high accuracy, but a relatively low computational cost by utilizing fewer parame ters compared to a fully connected multilayer per ceptron (MLP) model. It can also capture essential sentence features that determine node properties by analyzing relations between neighboring nodes. De spite the advantages as mentioned above, we suspect that the GCN is still missing an essential structural feature to capture better correlation or dependencies between nodes. One possible approach to improve the GCN per formance is to add adaptive attention weights depend ing on the feature matrix to graph convolutions. To capture the correlation between the labels bet ter, we propose a novel deep learning architecture based on graph attention networks. The proposed model with graph attention allows us to capture the dependency structure among labels for MLTC tasks. As a result, the correlation between labels can be au tomatically learned based on the feature matrix. We propose to learn interdependent sentence classiÔ¨Åers from prior label representations (e.g. word embed dings) via an attentionbased function. We name the proposed method Multilabel Text classiÔ¨Åcation us ingAttention based Graph Neural NET work (MAG NET). It uses a multihead attention mechanism to extract the correlation between labels for the MLTC task. SpeciÔ¨Åcally, these are the following contribu tions: ‚Ä¢ The drawbacks of current models for the MLTC task are analyzed. ‚Ä¢ A novel endtoend trainable deep network is pro posed for MLTC. The model employs Graph At tention Network (GAT) to Ô¨Ånd the correlation be tween labels. ‚Ä¢ It shows that the proposed method achieves sim ilar or better performance compared to previous Stateoftheart(SoTA) models across two MLTC metrics and Ô¨Åve MLTC datasets. 2 RELATED WORK "
541,Double Descent Optimization Pattern and Aliasing: Caveats of Noisy Labels.txt,"Optimization plays a key role in the training of deep neural networks.
Deciding when to stop training can have a substantial impact on the performance
of the network during inference. Under certain conditions, the generalization
error can display a double descent pattern during training: the learning curve
is non-monotonic and seemingly diverges before converging again after
additional epochs. This optimization pattern can lead to early stopping
procedures to stop training before the second convergence and consequently
select a suboptimal set of parameters for the network, with worse performance
during inference. In this work, in addition to confirming that double descent
occurs with small datasets and noisy labels as evidenced by others, we show
that noisy labels must be present both in the training and generalization sets
to observe a double descent pattern. We also show that the learning rate has an
influence on double descent, and study how different optimizers and optimizer
parameters influence the apparition of double descent. Finally, we show that
increasing the learning rate can create an aliasing effect that masks the
double descent pattern without suppressing it. We study this phenomenon through
extensive experiments on variants of CIFAR-10 and show that they translate to a
real world application: the forecast of seizure events in epileptic patients
from continuous electroencephalographic recordings.","The evolution of the generalization error during training is often closely analyzed by machine learning practitioners to make substantial decisions about hyperparameter tuning, model architecture, or collection of additional data. The estimation of the generalization error can consequently be considered to be at the core of machine learning. Knowing when to stop training neural networks is crucial to reaching optimal generalization perfor mance. Estimating the exact optimal stopping time is still subject to debate, with novel earlystopping strategies continuously being proposed in the literature [ 8,27]. Most early stopping strategies would assume a steady increase in the generalization error across epochs as a reason to stop training. On new, unresolved tasks, networks may not have access to enough input information to predict the labels correctly. In this scenario, most machine learning practitioners would also stop training after observing a divergence of the generalization error, and potentially modify their research direction. Preprint. Under review.arXiv:2106.02100v2  [cs.LG]  17 Sep 2021For example, this can occur in medical datasets when the acquired patient data could be insufÔ¨Åcient to identify the target outcome. In most cases, most early stopping algorithms and machine learning practitioners would assume a steady increase in the generalization error across epochs as a reason to stop training. However, research has shown that small and noisy datasets may trigger an epochwise double descent optimiza tion pattern [ 22]. This means that after getting worse through the epochs, the generalization error reduces again, potentially leading to an overall smaller generalization error. Stopping the training before this second descent would be suboptimal. Double descent has been qualitatively deÔ¨Åned by others [ 22]. In this article, we mathematically deÔ¨Åne epochwise double descent and conÔ¨Årm earlier Ô¨Åndings demonstrating that double descent appears when the training set is both small and has noisy labels. We also empirically show that: ‚Ä¢Both training and generalization sets must have noisy labels for a double descent pattern to appear. ‚Ä¢If only the labels of the training set are noisy, double descent does not occur. Instead, a plateau pattern may appear. ‚Ä¢Even when the training set is small and labels are noisy in both training and generalization sets, there exists a learning rate for which the double descent pattern does not appear. ‚Ä¢The double descent pattern appears when the learning rate is too small, with the exact value of the learning depending on the dataset and task. ‚Ä¢Increasing the learning rate may create an aliasing effect that hides the double descent pattern without suppressing it. 2 Related Work "
330,Audio-Visual Efficient Conformer for Robust Speech Recognition.txt,"End-to-end Automatic Speech Recognition (ASR) systems based on neural
networks have seen large improvements in recent years. The availability of
large scale hand-labeled datasets and sufficient computing resources made it
possible to train powerful deep neural networks, reaching very low Word Error
Rate (WER) on academic benchmarks. However, despite impressive performance on
clean audio samples, a drop of performance is often observed on noisy speech.
In this work, we propose to improve the noise robustness of the recently
proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based
architecture by processing both audio and visual modalities. We improve
previous lip reading methods using an Efficient Conformer back-end on top of a
ResNet-18 visual front-end and by adding intermediate CTC losses between
blocks. We condition intermediate block features on early predictions using
Inter CTC residual modules to relax the conditional independence assumption of
CTC-based models. We also replace the Efficient Conformer grouped attention by
a more efficient and simpler attention mechanism that we call patch attention.
We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip
Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and
visual modalities allows to better recognize speech in the presence of
environmental noise and significantly accelerate training, reaching lower WER
with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC)
model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on
LRS2 and LRS3 test sets. Code and pretrained models are available at
https://github.com/burchim/AVEC.","Endtoend Automatic Speech Recognition based on deep neural networks has become the standard of stateof theart approaches in recent years [25, 47, 18, 16, 17, 31, 7]. The availability of large scale handlabeled datasets and suf Ô¨Åcient computing resources made it possible to train power 40 ms rateVisual Conformer Stage 2 20 ms rate Visual Conformer Stage 1 Visual Frontend   Conv3d + ResNet18  Audio Frontend   STFT + Conv2d  Audio Conformer Stage 1Audio Conformer Stage 2Audio Conformer Stage 3AudioV isual  Fusion ModuleAudioV isual Conformer Stage Visual  BackendAudio   Backend80 ms rateCTC loss 40 ms rate80 ms rate 80 ms rateFigure 1: AudioVisual EfÔ¨Åcient Conformer architec ture. The model is trained endtoend using CTC loss and takes raw audio waveforms and lip movements from the speaker as inputs. ful deep neural networks for ASR, reaching very low WER on academic benchmarks like LibriSpeech [34]. Neural ar chitectures like Recurrent Neural Networks (RNN) [15, 19], Convolution Neural Networks (CNN) [10, 28] and Trans formers [12, 23] have successfully been trained from raw audio waveforms and melspectrograms audio features to transcribe speech to text. Recently, Gulati et al. [16] proposed a convolutionaugmented transformer architec ture (Conformer) to model both local and global dependen cies using convolution and attention to reach better speech recognition performance. Concurrently, Nozaki et al. [33]arXiv:2301.01456v1  [cs.CV]  4 Jan 2023improved CTCbased speech recognition by conditioning intermediate encoder block features on early predictions us ing intermediate CTC losses [14]. Burchi et al. [7] also pro posed an EfÔ¨Åcient Conformer architecture using grouped attention for speech recognition, lowering the amount of computation while achieving better performance. Inspired from computer vision backbones, the EfÔ¨Åcient Conformer encoder is composed of multiple stages where each stage comprises a number of Conformer blocks to progressively downsample and project the audio sequence to wider fea ture dimensions. Yet, even if these audioonly approaches are breaking the stateoftheart, one major pitfall for using them in the realworld is the rapid deterioration of performance in the presence of ambient noise. In parallel to that, Audio Visual Speech Recognition (A VSR) has recently attracted a lot of research attention due to its ability to use image process ing techniques to aid speech recognition systems. Preced ing works have shown that including the visual modality of lip movements could improve the robustness of ASR sys tems with respect to noise while reaching better recognition performance [41, 42, 36, 1, 45, 29]. Xu et al. [45] pro posed a twostage approach to Ô¨Årst separate the target voice from background noise using the speakers lip movements and then transcribe the Ô¨Åltered audio signal with the help of lip movements. Petridis et al. [36] uses a hybrid architec ture, training an LSTMbased sequencetosequence (S2S) model with an auxiliary CTC loss using an early fusion strategy to reach better performance. Ma et al. [29] uses Conformer backend networks with ResNet18 [20] front end networks to improve recognition performance. Other works focus on Visual Speech Recognition (VSR), only using lip movements to transcribe spoken language into text [4, 9, 48, 3, 49, 37, 30]. An important line of research is the use of crossmodal distillation. Afouras et al.[3] and Zhao et al. [49] proposed to improve the lip read ing performance by distilling from an ASR model trained on a largescale audioonly corpus while Ma et al. [30] uses predictionbased auxiliary tasks. Prajwal et al. [37] also proposed to use subwords units instead of characters to transcribe sequences, greatly reducing running time and memory requirements. Also providing a language prior, re ducing the language modelling burden of the model. In this work we focus on the design of a noise robust speech recognition architecture processing both audio and visual modalities. We use the recently proposed CTC based EfÔ¨Åcient Conformer architecture [7] and show that including the visual modality of lip movements can suc cessfully improve noise robustness while signiÔ¨Åcantly ac celerating training. Our AudioVisual EfÔ¨Åcient Conformer (A VEC) reaches lower WER using 4 times less training steps than its audioonly counterpart. Moreover, we are the Ô¨Årst work to apply intermediate CTC losses betweenblocks [27, 33] to improve visual speech recognition perfor mance. We show that conditioning intermediate features on early predictions using Inter CTC residual modules allows to close the gap in WER between autoregressive and non autoregressive A VSR systems based on S2S. This also helps to counter a common failure case which is that audiovisual models tend to ignore the visual modality. In this way, we force prefusion layers to learn spatiotemporal features. Fi nally, we replace the EfÔ¨Åcient Conformer grouped attention by a more efÔ¨Åcient and simpler attention mechanism that we call patch attention. Patch attention reaches similar per formance to grouped attention while having a lower com plexity. The contributions of this work are as follows: ‚Ä¢ We improve the noise robustness of the recently pro posed EfÔ¨Åcient Conformer architecture by processing both audio and visual modalities. ‚Ä¢ We condition intermediate Conformer block features on early predictions using Inter CTC residual modules to relax the conditional independence assumption of CTC models. This allows us to close the gap in WER between autoregressive and nonautoregressive meth ods based on S2S. ‚Ä¢ We propose to replace the EfÔ¨Åcient Conformer grouped attention by a more efÔ¨Åcient and simpler at tention mechanism that we call patch attention. Patch attention reaches similar performance to grouped at tention with a lower complexity. ‚Ä¢ We experiment on publicly available LRS2 and LRS3 datasets and reach stateoftheart results using audio and visual modalities. 2. Method "
70,Learning to Learn from Noisy Labeled Data.txt,"Despite the success of deep neural networks (DNNs) in image classification
tasks, the human-level performance relies on massive training data with
high-quality manual annotations, which are expensive and time-consuming to
collect. There exist many inexpensive data sources on the web, but they tend to
contain inaccurate labels. Training on noisy labeled datasets causes
performance degradation because DNNs can easily overfit to the label noise. To
overcome this problem, we propose a noise-tolerant training algorithm, where a
meta-learning update is performed prior to conventional gradient update. The
proposed meta-learning method simulates actual training by generating synthetic
noisy labels, and train the model such that after one gradient update using
each set of synthetic noisy labels, the model does not overfit to the specific
noise. We conduct extensive experiments on the noisy CIFAR-10 dataset and the
Clothing1M dataset. The results demonstrate the advantageous performance of the
proposed method compared to several state-of-the-art baselines.","One of the key reasons why deep neural networks (DNNs) have been so successful in image classiÔ¨Åcation is the collections of massive labeled datasets such as COCO [14] and ImageNet [20]. However, it is timeconsuming and expensive to collect such highquality manual annota tions. A single image often requires agreement from mul tiple annotators to reduce label error. On the other hand, there exist other less expensive sources to collect labeled data, such as search engines, social media websites, or re ducing the number of annotators per image. However, those lowcost approaches introduce lowquality annotations with label noise . Many studies have shown that label noise can signiÔ¨Åcantly affect the accuracy of the learned classi Ô¨Åers [2, 23, 32]. In this work, we address the following problem: how to effectively train on noisy labeled datasets? Some methods learn with label noise by relying on hu man supervision to verify seed images [11, 29] or estimatelabel confusion [16, 31]. However, those methods exhibit a disadvantage in scalability for large datasets. On the other hand, methods without human supervision ( e.g. label cor rection [18, 24] and noise correction layers [5, 23]) are scalable but less effective and more heuristic. In this work we propose a metalearning based noisetolerant (MLNT) training to learn from noisy labeled data without human su pervision or access to any clean labels. Rather than design ing a speciÔ¨Åc model, we propose a modelagnostic training algorithm, which is applicable to any model that is trained with gradientbased learning rule. The prominent issue in training DNNs on noisy labeled data is that DNNs often overÔ¨Åt to the noise, which leads to performance degradation. Our method addresses this issue by optimizing for a model‚Äôs parameters that are less prone to overÔ¨Åtting and more robust against label noise. SpeciÔ¨Å cally, for each minibatch, we propose a metaobjective to train the model, such that after the model goes through con ventional gradient update, it does not overÔ¨Åt to the label noise. The proposed metaobjective encourages the model to produce consistent predictions after it is trained on a vari ety of synthetic noisy labels. The key idea of our method is: a noisetolerant model should be able to consistently learn the underlying knowledge from data despite different label noise . The main contribution of this work are as follows. We propose a noisetolerant training algorithm, where a metaobjective is optimized before conventional training. Our method can be theoretically applied to any model trained with gradientbased rule. We aim to optimize for a model that does not overÔ¨Åt to a wide spectrum of artiÔ¨Åcially generated label noise. We formulate our metaobjective as: train the model such that after it learns from various synthetic noisy labels using gradient update, the updated models give consistent predictions with a teacher model. We adapt a selfensembling method to construct the teacher model, which gives more reliable predictions unaf fected by the synthetic noise. We perform experiments on two datasets with syn thetic and realworld label noise, and demonstrate the 1arXiv:1812.05214v2  [cs.LG]  12 Apr 2019advantageous performance of the proposed method in image classiÔ¨Åcation tasks compared to stateoftheart methods. In addition, we conduct extensive ablation study to examine different components of the proposed method. Our code is publicly available1. 2. Related Work "
487,Human-like Clustering with Deep Convolutional Neural Networks.txt,"Classification and clustering have been studied separately in machine
learning and computer vision. Inspired by the recent success of deep learning
models in solving various vision problems (e.g., object recognition, semantic
segmentation) and the fact that humans serve as the gold standard in assessing
clustering algorithms, here, we advocate for a unified treatment of the two
problems and suggest that hierarchical frameworks that progressively build
complex patterns on top of the simpler ones (e.g., convolutional neural
networks) offer a promising solution. We do not dwell much on the learning
mechanisms in these frameworks as they are still a matter of debate, with
respect to biological constraints. Instead, we emphasize on the
compositionality of the real world structures and objects. In particular, we
show that CNNs, trained end to end using back propagation with noisy labels,
are able to cluster data points belonging to several overlapping shapes, and do
so much better than the state of the art algorithms. The main takeaway lesson
from our study is that mechanisms of human vision, particularly the hierarchal
organization of the visual ventral stream should be taken into account in
clustering algorithms (e.g., for learning representations in an unsupervised
manner or with minimum supervision) to reach human level clustering
performance. This, by no means, suggests that other methods do not hold merits.
For example, methods relying on pairwise affinities (e.g., spectral clustering)
have been very successful in many scenarios but still fail in some cases (e.g.,
overlapping clusters).","Clustering, a.k.a unsupervised classiÔ¨Åcation or nonparametric density estimation, is central to many datadriven domains and has been studied heavily in the past. The task in clustering is to group a given collection of unlabeled patterns into meaningful clusters such that objects within a cluster are more similar to each other than they are to objects in other clusters. Clustering provides a summary representation of data at a coarse level and is used widely in many disciplines (e.g., computer ver sion, bioinformatics, text processing) for exploratory data analysis (a.k.a pattern mining) as well as representation learning (e.g., bag of words). Despite the introduction of thousands of clustering al gorithms in the past Aggarwal & Reddy (2013), some challenges still remain. For instance, existing algorithms fall short in dealing with different cluster shapes, high dimensions, automatically deter mining the number of clusters or other parameters, large amounts of data, choosing the appropriate similarity measure, incorporating domain knowledge, and cluster evaluation. Further, no clustering algorithm can consistently win over other algorithms, handle all test cases, and perform at the level of humans. Deep neural networks have become a dominant approach to solve various tasks across many Ô¨Åelds. They have been proven successful in several domains including computer vision Krizhevsky et al. (2012), natural language processing Collobert et al. (2011), and speech recognition Dahl et al. (2012) for tasks such as scene and object classiÔ¨Åcation Krizhevsky et al. (2012), pixellevel labeling for image segmentation Long et al. (2015); Zheng et al. (2015), modeling attention Borji & Itti (2013); Borji et al. (2013), image generation Goodfellow et al. (2014), robot arm control Levine et al. (2015), Authors contributed equally. 1arXiv:1706.05048v2  [cs.LG]  11 Dec 2017speech recognition Graves & Jaitly (2014), playing Atari games Mnih et al. (2015) and beating the Go champion. Deep Convolutional Neural Networks (CNNs) LeCun et al. (1998) have been particularly successful over vision problems. One reason is that nearby pixels in natural scenes are highly correlated. Further natural objects are compositional. These facts allow applying the same Ô¨Ålters across spatial locations (and hence share weights), and build complex Ô¨Ålters from simpler ones to detect high level patterns (e.g., object parts, objects). We advocate that these properties are highly appealing when dealing with clustering problems. For instance, the classic two half moons example can be solved by applying a Ô¨Ålter that is selective to each half moon. Or, when two clusters with different shapes overlap, the problem can be solved by having Ô¨Ålters responding to each shape. Solving these cases is very challenging by just looking at local regions around points and being blind to the highlevel patterns. Incorporating domain knowledge, while working in some cases, does not give a general solution for solving all clustering problems. The human visual system easily solves these 2D problems because it is a general system with a rich set of learned or evolved Ô¨Ålters. We believe that deep CNNs, although imperfect models of the human vision as they lack feedback and lateral connections carry a huge promise for solving clustering tasks. Further, as we will argue, they offer a uniÔ¨Åed solution to both classiÔ¨Åcation and clustering tasks. The current demarcation between classiÔ¨Åcation and clustering becomes murky when we notice that researchers often refer to human judgments in evaluating the outcomes of clustering algorithms. Indeed, humans learn quite a lot about the visual world during their life time. Moreover, the structure of the visual system has been Ô¨Ånetuned through the evolution. Thus, certainly, there is a learning component involved which has been often neglected in formulating clustering algorithms. While this is sensible from an application point of view (e.g., pattern mining), not only it limits the pursuit for stronger algorithms but also narrows our understanding of human vision. Learning techniques have been utilized for clustering in the past (e.g., Bach & Jordan (2004); Pin heiro et al. (2016)), for example for tuning parameters (e.g., Bach & Jordan (2004)). Deep networks have also been exploited for clustering (e.g., Hsu & Kira (2015); Hershey et al. (2016); Wang et al. (2016)). However, to our knowledge, while CNNs have been already adopted for image segmen tation, so far they have not been exploited for generic clustering. Our goal is to investigate such possibility. To this end, instead of borrowing from clustering to do image segmentation, we follow the opposite direction and propose a deep learning based approach to clustering. Our method builds on the fully convolutional network literature, in particular, recent work on edge detection and semantic segmentation which utilize multiscale local and nonlocal cues Ronneberger et al. (2015). Thanks to a high volume of labeled data, high capacity of deep networks, powerful optimization algorithms, and high computational power, deep models win on these tasks. We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects (e.g., Yamins et al. (2014); DiCarlo & Cox (2007); LeCun et al. (1998); Krizhevsky et al. (2012); Borji & Itti (2014). Our study enriches our understanding of the concept of clustering and its relation to classiÔ¨Åcation. 2 R ELATED WORK "
324,Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation.txt,"Semantic segmentation with limited annotations, such as weakly supervised
semantic segmentation (WSSS) and semi-supervised semantic segmentation (SSSS),
is a challenging task that has attracted much attention recently. Most leading
WSSS methods employ a sophisticated multi-stage training strategy to estimate
pseudo-labels as precise as possible, but they suffer from high model
complexity. In contrast, there exists another research line that trains a
single network with image-level labels in one training cycle. However, such a
single-stage strategy often performs poorly because of the compounding effect
caused by inaccurate pseudo-label estimation. To address this issue, this paper
presents a Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and
SSSS. The SLRNet uses cross-view self-supervision, that is, it simultaneously
predicts several complementary attentive LR representations from different
views of an image to learn precise pseudo-labels. Specifically, we reformulate
the LR representation learning as a collective matrix factorization problem and
optimize it jointly with the network learning in an end-to-end manner. The
resulting LR representation deprecates noisy information while capturing stable
semantics across different views, making it robust to the input variations,
thereby reducing overfitting to self-supervision errors. The SLRNet can provide
a unified single-stage framework for various label-efficient semantic
segmentation settings: 1) WSSS with image-level labeled data, 2) SSSS with a
few pixel-level labeled data, and 3) SSSS with a few pixel-level labeled data
and many image-level labeled data. Extensive experiments on the Pascal VOC
2012, COCO, and L2ID datasets demonstrate that our SLRNet outperforms both
state-of-the-art WSSS and SSSS methods with a variety of different settings,
proving its good generalizability and efficacy.","Semantic segmentation is a fundamental computer vi sion task that aims to assign a label to each pixel, promoting the development of many downstream tasks, such as scene parsing, autonomous driving, and medi cal image analysis (Chen et al., 2018; Zhou et al., 2019; Havaei et al., 2017). Recently, deep learning based se mantic segmentation models (Long et al., 2015; Chen et al., 2018), trained with largescale data labeled at pixel level, have achieved impressive progress. However, such supervised approaches require intensive manual annotations that are timeconsuming and expensive, which have inspired many investigations about learn ing with lowcost annotations, such as semisupervisedarXiv:2203.10278v1  [cs.CV]  19 Mar 20222 Junwen Panet al. I II IPùëì!(I) PP""P#MVMCCVLRùëì$%&ùëì!!(I)ùëì!""(I)ùëì!(I"")ùëì!(I#)ùëì$%&ùëì'$&ùëì'$&(I) 1StageNet(II) CrossPseudoSeg (III) PseudoSeg(IV) SLRNetùë°(ùë°)ùë°""ùë°# Fig. 1: Overview of pseudo supervision architectures: (I) Single pseudo supervision for WSSS (Araslanov and Roth, 2020), (II) Cross pseudo supervision for SSSS (Chen et al., 2021), (III) PseudoSeg supervision for SSSS (Zou et al., 2021), and (IV) The SLRNet with MVMC and CVLR can mitigate the compounding ef fect of pseudo supervision error. ` !' means the forward operation, ` 99K' means pseudo supervision, `/' on ` !' means stopgradient and pseudolabel generation. tde notes the transformation operator, sandware short for \strong"" and \weak"" respectively. semantic segmentation (SSSS) with limited amounts of labeled data, weakly supervised semantic segmenta tion (WSSS) with bounding boxes (Dai et al., 2015), scribbles (Lin et al., 2016), points (Bearman et al., 2016), and imagelevel labels (Kolesnikov and Lampert, 2016). Nevertheless, there is a considerable gulf between weakly supervised and semisupervised approaches. Most popular imagelevel WSSS methods (Ahn et al., 2019; Dong et al., 2020; Sun et al., 2020) resort to multiple training and renement stages to obtain more accurate pseudolabels while avoiding error accumulation. These methods often start from a weakly supervised localization, such as a class activation map (CAM) (Zhou et al., 2016), which highlights the most discriminative regions in an image. In this approach, diverse enhanced CAMgenerating networks (Lee et al., 2019; Wang et al., 2020b; Sun et al., 2020) and CAMrenement procedures (Ahn and Kwak, 2018; Ahn et al., 2019; Shimoda and Yanai, 2019) have been designed to expand the highlighted area to the entire object or eliminate the wrongly highlighted area. Although these multistage methods can produce more accurate pseudolabels, they suer from the need of a large number of hyperparameters and complex training procedures. Singlestage WSSS methods (Zheng et al., 2015; Papandreou et al., 2015) have received less attention because their segmentation is less accurate than that of multistage methods. Re cently, Araslanov and Roth (2020) proposed a simple singlestage WSSS model that generates pixellevelpseudolabels online as selfsupervision (Fig. 1 (I)). However, its accuracy is still not comparable with that of multistage approaches. In contrast, the simple online pseudosupervision scheme has made promising progress in SSSS (Fig. 1(II) (Chen et al., 2021) and (III) (Zou et al., 2021)). We argue that the cause of the inferior performance of the online pseudo supervised WSSS is the com pounding eect of errors caused by online inaccurate pseudo supervision. Like multistage renements, on line pseudolabel supervision should gradually improve the semantic delity and completeness during the training process. However, this also increases the risk that errors are mimicked and accumulated with the gradient  ows being backpropagated from the top to the lower layers. Consistency learning is widely used as additional supervision to semisupervised learn ing (Ouali et al., 2020; Chen et al., 2021). However, in practice, existing consistencybased methods are not applicable to imagelevel weakly supervised settings. First, they require pixellevel supervision to avoid the collapsing solution (Chen and He, 2021). Second, the dominance of consistency harms the region expansion for WSSS. To this end, we propose the Selfsupervised Low Rank Network (SLRNet) for singlestage WSSS and SSSS. As illustrated in Fig. 1(IV), the SLRNet simultaneously predicts several segmentation masks for various augmented versions of one image, which are jointly calibrated and rened by a multiview mask cal ibration (MVMC) module to generate one pseudomask for selfsupervision. The pseudomask leverages the complementary information from various augmented views, which enforces the crossview consistency on the predictions. To further regularize the network, the SLRNet introduces the LR inductive bias implemented by a crossview lowrank (CVLR) module. The CVLR exploits the collective matrix factorization to jointly decompose the learned representations from dierent views into submatrices while recovering a clean LR signal subspace. Through the dictionary shared over dierent views, a variety of related features from various views can be rened and amplied to eliminate the ambiguities or false predictions. Thereby, the input features of the decoder deprecate noisy information, and this can eectively prevent the network from over tting to the false pseudolabels. Additionally, instead of directly randomly initializing the submatrices, a latent space regularization is designed to improve the optimization eciency. The SLRNet is an ecient and elegant framework that generalizes well to dierent labelecient seg mentation settings without additional training phases.Learning SelfSupervised LowRank Network for SingleStage Weakly and SemiSupervised Semantic Segmentation 3 For instance, to simultaneously utilize imagelevel and pixellevel labels, previous SSSS methods (Lee et al., 2019; Wei et al., 2018) have to generate and rene pseudolabels oine using WSSS model, which are bundled with pixellevel labels to train a network in the next stage. Such a multistage scheme provides a marginal improvement over dedicated SSSS algo rithms (Ouali et al., 2020; Zou et al., 2021) with unlabeled data. In contrast, the SLRNet directly introduces additional pixellevel supervision while combining it with imagelevel data without extra cost. In other words, the online pseudomask generation takes into account both imagelevel and pixellevel labels in a single training phase and is undoubtedly more accurate. To the best of our knowledge, the SLRNet is the rst attempt to bridge these tasks into a unied singlestage scheme, allowing it to maximize exploiting various annotations with a limited budget. In our experiments, we rst validate the perfor mance of SLRNet in an imagelevel WSSS setting on several datasets, including Pascal VOC 2012 (Ev eringham et al., 2010), COCO (Lin et al., 2014), and L2ID (Wei et al., 2020). Extensive experiments demonstrate that the crossview supervision and the CVLR help improve semantic delity and completeness of the generated segmentation masks. Notably, the SLRNet also establishes new stateofthearts for various labelecient semantic segmentation tasks, including 1) WSSS with imagelevel labeled data, 2) SSSS with pixellevel and imagelevel labeled data and 3) SSSS with pixellevel labeled and unlabeled data. Moreover, the SLRNet achieves the best performance at the WSSS Track of CVPR 2021 Learning from Limited and Imperfect Data (L2ID) Challenge (Wei et al., 2020), outperforming other competitors by large margins of9:35% in terms of mIoU. The main contributions of this work are summarized as follows: 1) We propose an eective crossview selfsupervision scheme, incorporating the CVLR module, to allevi ate the compounding eect of selfsupervision errors for the online pseudolabel training. 2) We present a plugandplay collective matrix factor ization method with latent space regularization for multiview LR representation learning, which can be readily embedded into any Siamese networks for endtoend training. 3) The SLRNet provides a unied framework that can be well generalized to learn a segmentation model from dierent limited annotations in various WSSS and SSSS settings. 4) The SLRNet achieves leading performance com pared to a variety of stateoftheart methods onPascal VOC 2012, COCO, and L2ID datasets for both WSSS and SSSS tasks. 2 Related Work "
538,Multi-Modal Emotion Detection with Transfer Learning.txt,"Automated emotion detection in speech is a challenging task due to the
complex interdependence between words and the manner in which they are spoken.
It is made more difficult by the available datasets; their small size and
incompatible labeling idiosyncrasies make it hard to build generalizable
emotion detection systems. To address these two challenges, we present a
multi-modal approach that first transfers learning from related tasks in speech
and text to produce robust neural embeddings and then uses these embeddings to
train a pLDA classifier that is able to adapt to previously unseen emotions and
domains. We begin by training a multilayer TDNN on the task of speaker
identification with the VoxCeleb corpora and then fine-tune it on the task of
emotion identification with the Crema-D corpus. Using this network, we extract
speech embeddings for Crema-D from each of its layers, generate and concatenate
text embeddings for the accompanying transcripts using a fine-tuned BERT model
and then train an LDA - pLDA classifier on the resulting dense representations.
We exhaustively evaluate the predictive power of every component: the TDNN
alone, speech embeddings from each of its layers alone, text embeddings alone
and every combination thereof. Our best variant, trained on only VoxCeleb and
Crema-D and evaluated on IEMOCAP, achieves an EER of 38.05%. Including a
portion of IEMOCAP during training produces a 5-fold averaged EER of 25.72%
(For comparison, 44.71% of the gold-label annotations include at least one
annotator who disagrees).","Due to the growing presence of AIpowered systems in our lives, affective computing has become an important part of humancomputer interaction. Emotion plays a role in our thoughts and actions and is an integral part of the way we communicate (Choi et al., 2018). The ability to leverage context to understand emotions communicated both verbally and nonverbally is trivial for humans but remains difÔ¨Åcult for machines (Chen et al., 2019). Emotional responses depend on both our psyche and physiology and are governed by our perception of situations, people and objects. They also depend on our mental state (mood, motivation, temperament) (Tripathi et al., 2018a). The way we exhibit and perceive emotion may also differ based on our age, gender, race, culture and accent (Latif et al., 2019). In addition to all of this, unlike targets in other classiÔ¨Åcation tasks, the emotions we experience are rarely distinct: they often coexist without clear temporal boundaries, adding considerable complexity to the task (Tzirakis et al., 2017). Despite these difÔ¨Åculties, automated emotion recognition has social and commercial applications that make it worth pursuing. In the medical domain, it has exciting potential: to identify and diagnose depression and stress in individuals (Zhu et al., 2017; Rana et al., 2019), to monitor and help people with bipolar disorder (Rana, 2016) and to assist the general public in maintaining mental health. Commercial applications include call center customer management, advertising through neuromarketing and social media engagement (Tzirakis et al., 2017; Choi et al., 2018; Chen et al., 2019). As intelligent chatbots and virtual assistants have become more widely used, emotion detection has become a vital component in the design, development and deployment of these conversational agents (Yoon et al., 2018).arXiv:2011.07065v1  [eess.AS]  13 Nov 2020Early research in emotion detection focused on binary classiÔ¨Åcation in a single modality, whether in text, speech (Chernykh and Prikhodko, 2017; Neumann and Vu, 2017), or images (Dhall et al., 2015). Textbased classiÔ¨Åers used the ngram vocabulary of sentences to predict their polarity and speech models modeled the vocal dynamics that characterize these emotions. These approaches are inherently limited: a binary granularity and cues from a single modality are far removed from the actual human process they‚Äôre meant to model. As a result, joint approaches which leverage all available modalities (e.g., both speech and text in applications like home assistants) are promising. While existing multimodal emotion corpora like IEMOCAP (Busso et al., 2008) and CremD (Cao et al., 2014) have been critical for the progress in affective computing to date, they suffer from three issues that are the focus of our work. First, these corpora tend to be small due to the high costs of annotating for emotion. This precludes the use of deep neural models with high model complexity as they require many training samples to generalize well. This also compounds the second difÔ¨Åculty inherent to many emotion datasets: while there are usually many neutral, happy and sad training examples, there are often very few examples of rarer emotions like disgust making them difÔ¨Åcult to classify. This issue is not easily solved by combining different corpora due to the third issue, their lack of mutual compatibility ‚Äì they differ in the emotions identiÔ¨Åed, the types of dialogue and number of speakers represented and the naturalness of the recordings (see Figure 1). This severely restricts the generalizability of models trained on a single corpus. Contemporary literature has dealt with these problems by dropping labels (Pappagari et al. (2020); Chen and Zhao (2020); Yoon et al. (2020)). Hard and scarce emotions like disgust are dropped from the corpus and the models are trained and evaluated on the trimmed corpus. This allows evaluating models on different corpora by using utterances exhibiting only the most common emotions. While this is a reasonable, the resulting performance is not a complete reÔ¨Çection of how these models perform once deployed to production. When emotion models are used in realworld applications, we can expect them to encounter utterances corresponding to dropped labels. For such cases, these models are likely to exhibit degraded performance by predicting one of the known, but incorrect labels. In this work, we address the problem of data sparsity by transfer learning via the pretrainthenÔ¨Ånetune paradigm. Deep complex models can be trained on large datasets for an auxiliary but related task to learn network parameters that reÔ¨Çect abstract notions related to the target task. As the expression of emotions is highly dependent on the individual, we train a multilayer TDNN (Waibel et al. (1989)) on the task of speaker identiÔ¨Åcation using the V oxCeleb corpus (Chung et al. (2018)) and then Ô¨Ånetune its Ô¨Ånal few layers on the task of emotion identiÔ¨Åcation using the CremaD corpus (Cao et al. (2014)). Using this network, we extract speech embeddings for CremaD from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a Ô¨Ånetuned BERT model (Devlin et al. (2018)) and then train an LDA  pLDA (Fisher (1936); Ioffe (2006)) model on the resulting dense representations. pLDA allows our model to more easily adapt to previously unseen classes and domains, a requirement for both evaluating against a different emotion corpus with an incompatible label set and performing well in the wild. To understand the merits of each component, we exhaustively evaluate the predictive power of every permutation: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof. Our best variant, trained on only V oxCeleb and CremaD and evaluated on IEMOCAP, achieves an Equal Error Rate (EER) of 38:05%. Including a portion of IEMOCAP during training produces a 5fold averaged EER of 25:72%. 2 Related Work "
555,GrabQC: Graph based Query Contextualization for automated ICD coding.txt,"Automated medical coding is a process of codifying clinical notes to
appropriate diagnosis and procedure codes automatically from the standard
taxonomies such as ICD (International Classification of Diseases) and CPT
(Current Procedure Terminology). The manual coding process involves the
identification of entities from the clinical notes followed by querying a
commercial or non-commercial medical codes Information Retrieval (IR) system
that follows the Centre for Medicare and Medicaid Services (CMS) guidelines. We
propose to automate this manual process by automatically constructing a query
for the IR system using the entities auto-extracted from the clinical notes. We
propose \textbf{GrabQC}, a \textbf{Gra}ph \textbf{b}ased \textbf{Q}uery
\textbf{C}ontextualization method that automatically extracts queries from the
clinical text, contextualizes the queries using a Graph Neural Network (GNN)
model and obtains the ICD Codes using an external IR system. We also propose a
method for labelling the dataset for training the model. We perform experiments
on two datasets of clinical text in three different setups to assert the
effectiveness of our approach. The experimental results show that our proposed
method is better than the compared baselines in all three settings.","Automated medical coding is a research direction of great interest to the health care industry [3,22], especially in the Revenue Cycle Management (RCM) space, as a solution to the traditional humanpowered coding limitations. Medical cod ing is a process of codifying diagnoses, conditions, symptoms, procedures, tech niques, the equipment described in a medical chart or a clinical note of a patient. The codifying process involves mapping the medical concepts in context to one or more accurate codes from the standard taxonomies such as ICD (Interna tional Classication of Diseases) and CPT (Current Procedure Terminology). The ICD taxonomy is a hierarchy of diagnostic codes maintained by the World Health Organisation. Medical coders are trained professionals who study a med ical chart and assign appropriate codes based on their interpretation. The mostarXiv:2207.06802v1  [cs.LG]  14 Jul 20222 J. Chelladurai et al. Chief Complaint: Abdominal P ain Stated Complaint: Right Flank P ain since this AM Time Seen b y Provider: XXXXXX 22:09 Source: patient and RN notes reviewed Mode of arriv al: Ambulatory Limitations: no limitations History of Present Illness HPI narr ative: Patient presents for ev aluation of an abdominal pain with occasional r adiation to the back that began this morning while patient was engaged in light activit y.  The pain is intermittent and tends to be worse with movement. The patient reports pain in the right lower quadr ant  The patient also has had roughly 3 episodes of nonbloody diarrhea per da y over the past 2 days.  P atient denies associated fev er, cold symptoms, v omiting, r ashes, chest pain or dyspnea.  P ast medical history is pertinent for anxiet y, OCD , gener al phobia and insomnia. Clinical Impression: Nonspecific abdominal pain Patient Disposition: Home, Self CareHistory ( C0019664 ) right ( C0441994  ) lower quadrant ( C1631280) pain ( C0030193 ) intermittent ( C0205267 ) worse ( C0332271 ) movement ( C0026649 ) patient ( C0030705 ) episodes ( C0332189 ) nonbloody diarrhea ( C0151594 ) day ( C0439228 ) days ( C0439228 ) Patient ( C0030705 ) medical history ( C1704706 ) anxiety ( C0003467 ) OCD ( C0009595 ) general phobia ( C0349231 ) insomnia ( C1963237 ) Query  ExtractionNonspecific abdominal painNamed Entity Recognition and  Entity Linking Extracted QueryExtracted Entities Clinical TextContextualized Graph External Knowledge Graph rightlower quadrant pain Abdominal Pain episodesworsenon bloody diarrhea movement Nonspecific abdominal pain Right lower quadrant  R10.31Contextualized Query Predicted ICD CodeContextual  Graph  GenerationRelevant  Node  Detection IR SystemContextualized Graph  with Relevant Nodes Selectedrightlower quadrant pain Abdominal Pain episodesworsenon bloody diarrhea movementGrabQC Fig. 1: The gure describes the overall pipeline of our proposed method. signicant drawbacks in manual coding are its TurnAround Time (TAT), typi cally 2448 hours, and the inability to scale to large volumes of data. Automatic medical coding addresses both problems by applying AI and Natural Language Understanding (NLU) by mimicking and automating the manual coding process. The problem of automating the assignment of ICD codes to clinical notes is challenging due to several factors, such as the lack of consistent document struc ture, variability in physicians' writing style, choice of vocabularies to represent a medical concept, nonexplicit narratives, typographic and OCR conversion errors. Several approaches to solve these problems are available in the litera ture such as methods based on Deep Learning [22,2,18,14,23], Knowledge Bases [19] and Extractive Text Summarisation [5]. The most recent works treat this problem as a multilabel classication problem and solve them using various deep learning model architectures. Although approaches based on deep learn ing greatly reduce the manual labor required in the feature engineering process, there are certain challenges in applying these to the medical coding task: {Lack of explainability {Requirement of large amounts of labelled data {Large label space (ICD9 14,000 codes, ICD10 72,000 codes) There have been several attempts to address these challenges, for e.g., using an attention mechanism [14,10,21] , transfer learning [23] and extreme classica tion [1,23]. However, much more needs to be done to develop truly satisfactory deployable systems. In this work, we focus on the question of operating with large label space. We study how a medical coder arrives at the nal ICD codes for a given clinical note or medical chart. We observe that the medical coders study entities from dierent subsections of a document, such as Chief Complaint, Procedure, Impression, Diagnosis, Findings, etc., to construct evidence for every ICD code. We also observe that the medical coders use several commercial and noncommercial Information Retrieval (IR) tools such as Optum EncoderPro , AAPC Codify and open tools such as CDC's ICDCM10 Browser Tool for assoGrabQC: Graph based Query Contextualization for automated ICD coding 3 ciating the entities to relevant ICD codes. We propose a solution for automated ICD coding by automatically constructing a contextually enhanced text query containing entities from the clinical notes along with an existing information retrieval system. Fig. 1 depicts our method, which extracts an accurate entities based query, with which the relevant ICD codes could be fetched by querying an ICD IR system. Our method provides explainability to the retrieved codes in terms of the contextual entities, usually lacking in the endtoend DL based solutions. We propose GrabQC, a Graphbased Query contextualization method, along side an existing Information Retrieval system to automatically assign ICD codes to clinical notes and medical records. The overall architecture of our proposed method, which consists of four essential modules, is shown in Fig. 1. The rst module (Section 3.1) extracts all the data elements (entities) from the clinical notes along with their respective types such as condition, body part, symptom, drug, technique, procedure, etc. The second module (Section 3.2) extracts the primary diagnosis available typically under the Chief Complaint section of a medical chart. The third module (Section 3.3) constructs a graph from the enti ties enriched by a preconstructed external Knowledge Base. The fourth module (Section 3.4) prunes the constructed graph based on relevance to the clinical note concepts. We then construct the contextualized query for the integrated IR system to fetch the relevant ICD codes. The main contributions in this work are as follows: {GrabQC , a Graphbased Query Contextualization Module to extract and generate contextually enriched queries from clinical notes to query an IR system. {A Graph Neural Network (GNN) model to lter relevant nodes in a graph to contextualize the concepts. {A distant supervised method to generate labelled dataset for training the GNN model for relevant node detection. The rest of this paper is organised as follows. In Section 2, we provide a summary of the other approaches in the literature that solve the problem of ICD coding. We describe the proposed GrabQC module in Section 3. We present our experimental setup and the results in Section 4 and 5 respectively. We nally give concluding remarks and possible future directions of our work in Section 6. 2 Related Work "
222,PLM: Partial Label Masking for Imbalanced Multi-label Classification.txt,"Neural networks trained on real-world datasets with long-tailed label
distributions are biased towards frequent classes and perform poorly on
infrequent classes. The imbalance in the ratio of positive and negative samples
for each class skews network output probabilities further from ground-truth
distributions. We propose a method, Partial Label Masking (PLM), which utilizes
this ratio during training. By stochastically masking labels during loss
computation, the method balances this ratio for each class, leading to improved
recall on minority classes and improved precision on frequent classes. The
ratio is estimated adaptively based on the network's performance by minimizing
the KL divergence between predicted and ground-truth distributions. Whereas
most existing approaches addressing data imbalance are mainly focused on
single-label classification and do not generalize well to the multi-label case,
this work proposes a general approach to solve the long-tail data imbalance
issue for multi-label classification. PLM is versatile: it can be applied to
most objective functions and it can be used alongside other strategies for
class imbalance. Our method achieves strong performance when compared to
existing methods on both multi-label (MultiMNIST and MSCOCO) and single-label
(imbalanced CIFAR-10 and CIFAR-100) image classification datasets.","The impressive performance of deep learning meth ods has led to the creation of many largescale datasets [6, 12, 27, 23]. Due to the naturally imbalanced distri bution of objects within the world, these datasets contain imbalanced numbers of samples for different classes. The class labels in these datasets form a longtailed distribution: several classes appear frequently (the head classes), while many classes contain few samples (the tail classes). This imbalance causes classiÔ¨Åers to perform poorly, especially Figure 1. The output probability ( ^yc) distributions of a ResNet 32 classiÔ¨Åer trained on artiÔ¨Åcially imbalanced CIFAR10 for pos itive (left) and negative samples (right). For frequent classes, the predicted distribution skews towards 1; for infrequent classes, it skews towards 0. ClassiÔ¨Åers trained using PLM (bottom) reduces this bias, when compared to classiÔ¨Åers trained with binary cross entropy (top). on classes which are infrequent in training. Finding a solu tion to this problem is necessary to successfully scale deep networks to larger realworld datasets which tend to have longtail data distributions. Several recent works [2, 5, 3] attempt to solve the data imbalance issues; however, most tend to have singlelabel assumptions. For example, LDAMDRW [3] performs very well in singlelabel settings, but it assumes a single class label,y, is present for a given sample, x, to compute class margins (x;y)for their proposed loss. Not only that, but the two most common methods for learning longtailed dis tributions, reweighting and resampling, were not designed for data with multiple labels. Our experiments show that reweighting based on the inverse number of samples per forms poorly on multilabel datasets; also, it is difÔ¨Åcult to resample multilabel data due to the cooccurrence of labels within individual samples. Since many realworld applica tions like image tagging [28, 10], recommendation systemsarXiv:2105.10782v1  [cs.CV]  22 May 2021[37, 31], and action detection [12, 9], often involve multi label classiÔ¨Åcation and suffer from imbalanced data, we be lieve that classimbalance methods should be developed for both singlelabel and multilabel settings. Therefore, in this work, we propose a general solution for longtailed imbal ance which works for both multilabel and singlelabel clas siÔ¨Åcation. ClassiÔ¨Åers trained on imbalanced multilabel datasets tend to overpredict frequent classes and underpredict mi nority classes. This behaviour is displayed in Figure 1. When the class is not present within the image, the prob ability output for the frequent classes is skewed towards 1; conversely, when an infrequent class is present within the sample, the classiÔ¨Åer outputs a low probability score. These output probability distributions differ greatly from the ideal distribution (i.e. the groundtruth distribution where all pos itive samples are labeled 1 and all negative samples are la beled 0). We argue, that this behaviour is caused not only by an imbalance in the number of positive samples between different classes, but also by the ratio of positive and nega tive samples for each class . As dataset imbalance increases, the ratio of positive sam ples to negative samples increases for head classes and de creases for tail classes. We Ô¨Ånd that the change in this ra tio greatly impacts the classiÔ¨Åer‚Äôs ability to generalize. If a class has large ratio of positive samples to negative sam ples, the classiÔ¨Åer overpredicts the given class, leading to an increase of false positive predictions; conversely, a small ratio leads to underprediction and an increase of false neg atives. Assuming there is an optimal ratio which can mini mize the over/underpredictions, an algorithm can estimate and leverage this ratio to improve network performance. We present Partial Label Masking (PLM): a novel ap proach for training classiÔ¨Åers on imbalanced multilabel datasets which improves network generalization by lever aging this ratio. By partially masking positive and nega tive labels for frequent and infrequent classes respectively; our method reduces the discrepancy between the classiÔ¨Åers‚Äô output probability distribution and the groundtruth distri bution (as seen in Figure 1). Our method performs this masking stochastically for each sample and it continually adapts the target ratio based on the classiÔ¨Åer‚Äôs output prob abilities. This leads to improved precision on classes with many samples and improved recall on classes with few sam ples. Moreover, our method consistently improves perfor mance on difÔ¨Åcult classes, regardless of the number of sam ples. Our contributions include: (i) we present a general solu tion for data imbalance which balances the ratio between positive and negative samples, (ii) we propose an adap tive strategy to determine the ideal ratio which minimizes the difference between predicted probability and ground truth distributions, (iii) we empirically evaluate our methodon both multilabel datasets (imbalanced MultiMNIST and MSCOCO) and singlelabel datasets (CIFAR10 and CI FAR100), and (iv) we thoroughly analyse our method‚Äôs ability to improve classiÔ¨Åers‚Äô performance on both difÔ¨Åcult and infrequent classes. 2. Method "
177,Document Domain Randomization for Deep Learning Document Layout Extraction.txt,"We present document domain randomization (DDR), the first successful transfer
of convolutional neural networks (CNNs) trained only on graphically rendered
pseudo-paper pages to real-world document segmentation. DDR renders
pseudo-document pages by modeling randomized textual and non-textual contents
of interest, with user-defined layout and font styles to support joint learning
of fine-grained classes. We demonstrate competitive results using our DDR
approach to extract nine document classes from the benchmark CS-150 and papers
published in two domains, namely annual meetings of Association for
Computational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to
conditions of style mismatch, fewer or more noisy samples that are more easily
obtained in the real world. We show that high-fidelity semantic information is
not necessary to label semantic classes but style mismatch between train and
test can lower model accuracy. Using smaller training samples had a slightly
detrimental effect. Finally, network models still achieved high test accuracy
when correct labels are diluted towards confusing labels; this behavior hold
across several classes.","Fast, lowcost production of consistent and accurate training data enables us to use deep convolutional neural networks (CNN) to downstream document understanding [13,37,42,43]. However, carefully annotated data are difÔ¨Åcult to obtain, especially for document layout tasks with large numbers of labels (timeconsuming annotation) or with Ô¨Ånegrained classes (skilled annotation). In the scholarly document genre, a variety of document formats may not be attainable at scale thus causing imbalanced samples, since authors do not always follow section and format rules [ 10,28]. Different communities (e. g., computational linguistics vs. machine learning, or computer science vs. biology) use different structural and semantic organizations of sections and subsections. ThisarXiv:2105.14931v1  [cs.CV]  20 May 20212 Ling et al. Table and Figure  caption tags  Formatted  at diÔ¨Äerent  locationsExistence of graphics  components Document Domain Randomization: Training Data Generation:  Diverse Ô¨Ågure | table | algorithm | equation style;  Randomized text and page render. Texts in  paragraphs and  section titles. Font type, font  size, Italic or not,  bold or notFigure width relative to  the text / column width  is randomized. Text length; distances to  the Ô¨Ågure and the  subsequent paragraphs. Existence of  caption Fig. 1: Illustration of our document domain randomization (DDR) approach . A deep neural network(CNN)based layout analysis using training pages of 100% ground truth bounding boxes generated solely on simulated pages: lowÔ¨Ådelity textual content and images pasted via constrained layout randomization of Ô¨Ågure/table/algorithm/equa tion, paragraph and caption length, column width and height, twocolumn spacing, font style and size, captioned or not, title height, and randomized texts. Nine classes are used in the real document layout analysis with no additional training data: abstract , algorithm ,author ,bodytext ,caption ,equation ,Ô¨Ågure ,table , and title. Here the colored texts illustrate the semantic information; all text in the training data is black. diversity forces CNN paradigms (e. g., [ 36,43]) to use millions of training samples, sometimes with signiÔ¨Åcant amounts of noise and unreliable annotation. To overcome these training data production challenges, instead of the timeconsuming manual annotating of real paper pages to curate training data, we generate pseudopages by randomizing page appearance and semantic content to be the ‚Äúsurrogate‚Äù of training data. We denote this as document domain randomization (DDR ) (Fig. 1). DDR uses simulationbased training document generation, akin to domain randomization (DR) in robotics [ 20,34,40,41] and computer vision [ 15,29]. We randomize layout and font styles and semantics through graphical depictions in our page generator. The idea is that with enough page appearance randomization, the real page would appear to the model as just another variant. Since we know the boundingbox locations while rendering the training data, we can theoretically produce any number of highly accurate ( 100% ) training sam ples following the test data styles. A key question is what styles and semantics can be randomized to let the models learn the essential features of interest on pseudopages so as to achieve comparable results for label detection in real article pages. We address this question and study the behavior of DDR under numerous attribution settings to help guide the training data preparation. Our contributions are that we:Document Domain Randomization for Deep Learning Document Layout Extraction 3 ‚Äì Create DDR‚Äîa simple, fast, and effective training page preparation method to signiÔ¨Åcantly lower the cost of training data preparation. We demonstrate that DDR achieves competitive performance on the commonly used benchmark CS 150 [ 11], ACL300 of Association for Computational Linguistics (ACL), and VIS300 of IEEE visualization (VIS) on extracting nine classes. ‚Äì Cover realworld page styles using randomization to produce training samples that infer realworld document structures. HighÔ¨Ådelity semantics is not needed for document segmentation, and diversifying the font styles to cover the test data improved localization accuracy. ‚Äì Show that limiting the number of available training samples can lower detec tion accuracy. We reduced the training samples by half each time and showed that accuracy drops at about the same rate for all classes. ‚Äì Validated that CNN models remained reasonably accurate after training on noisy class labels of composed paper pages. We measured noisy data labels at 1‚Äì10% levels to mimic the realworld condition of human annotation with partially erroneous input for assembling the document pages. We show that standard CNN models trained with noisy labels remain accurate on numerous classes such as Ô¨Ågures, abstract, and bodytext. 2 Related Work "
31,Decoupling Representation and Classifier for Noisy Label Learning.txt,"Since convolutional neural networks (ConvNets) can easily memorize noisy
labels, which are ubiquitous in visual classification tasks, it has been a
great challenge to train ConvNets against them robustly. Various solutions,
e.g., sample selection, label correction, and robustifying loss functions, have
been proposed for this challenge, and most of them stick to the end-to-end
training of the representation (feature extractor) and classifier. In this
paper, by a deep rethinking and careful re-examining on learning behaviors of
the representation and classifier, we discover that the representation is much
more fragile in the presence of noisy labels than the classifier. Thus, we are
motivated to design a new method, i.e., REED, to leverage above discoveries to
learn from noisy labels robustly. The proposed method contains three stages,
i.e., obtaining the representation by self-supervised learning without any
labels, transferring the noisy label learning problem into a semisupervised one
by the classifier directly and reliably trained with noisy labels, and joint
semi-supervised retraining of both the representation and classifier. Extensive
experiments are performed on both synthetic and real benchmark datasets.
Results demonstrate that the proposed method can beat the state-of-the-art ones
by a large margin, especially under high noise level.","Convolutional neural networks (ConvNets) [17, 33] have achieved remarkable success in many computer vision tasks, e.g., image classiÔ¨Åcation [26, 53] and object detec tion [16, 25], because of their ability to model complex patterns. To fully exploit the learning ability of ConvNets, largescale and wellannotated datasets, e.g., ImageNet [50] and COCO [38], are needed. However, noisy labels are ubiquitous and inevitable, since such large and accurate datasets are expensive and timeconsuming to acquire. Be sides, modern ConvNets can easily overÔ¨Åt and memorize these noisy labels due to overparameterization, which sub sequently leads to very poor generalization [2, 43, 68]. Thus, how to train ConvNets robustly against noisy labelsTable 1. A comparison between the proposed REED with stateof theart methods. Manner: how a method learn from noisy labels; Semi: can semisupervised learn from samples with wrong labels; HN: can deal with high noise level; NCln: no need for extra clean training data. MethodLearning Properties manner Semi HN NCln CoTeaching [21] end2end 7 7 X Fcorrection [44] end2end 7 7 X Ren et al. [49] end2end 7 7 7 PENCIL [66] end2end 7 7 X Mcorrection [1] end2end 7 7 X NLNL [32] end2end X 7 X Zhang et al. [72] end2end 7 X 7 DivideMix [35] end2end X 7 X REED (ours) decoupled X X X becomes a problem of great importance. Recently, many approaches have been proposed to ro bustly learn from noisy labels [20], and they generally fol low three directions, i.e., sample selection [21, 29, 35, 51, 60, 67], label correction [22, 54, 56, 62, 66], and robustify ing loss functions [8, 14, 32, 39, 42, 55, 71]. SpeciÔ¨Åcally, sample selection methods construct some criterion attempt ing to pick up samples with clean labels for training. These criterion, e.g., smallloss [21, 29] and disagreement [41, 67], usually rely on the memorization effect [2, 68] of deep networks. Label correction methods attempt to directly cor rect the possibly noisy labels. They achieve this purpose by, e.g., pseudo labeling technologies [34], using class proto type [22], or even treating label as learnable and latent vari ables [54, 66]. Finally, since the existing loss functions for classiÔ¨Åcation, e.g., categorical cross entropy (CE) [14] and focal loss [37], can be skewed by noisy labels [71], more robust loss functions are proposed. Examples are general ized CE loss [71] and curriculum loss [39]. They are less biased on noisy labels and can be learned together with rep 1arXiv:2011.08145v1  [cs.CV]  16 Nov 2020resentation. Indeed, due to the superior performance resulting from the endtoend training of deep networks [17], most afore mentioned methods also jointly learn the representation and classiÔ¨Åer in an endtoend manner. However, such a joint learning scheme neglects an important issue  is there any difference in the learning behaviors between the represen tation and classiÔ¨Åer with noisy labels? Intuitively, when the representation is good enough,1the decision boundary of the classiÔ¨Åer can be easy to Ô¨Ånd even there is strong noise [23, 58]. Thus, we decouple the training scheme with noisy labels into representation and classiÔ¨Åer learning, and then look inside their learning behaviors. Interestingly, we discover that noisy labels will damage the representation learning much more signiÔ¨Åcantly than classiÔ¨Åer learning, and the classiÔ¨Åer itself indeed can exhibit strong robustness w.r.t. noisy labels with a good representation. These dis coveries are not noticed previously and new to the literature of noisy label learning. Thus, instead of the classical endtoend training method, which can lead to suboptimal performance, we are motivated to decouple the representation and classiÔ¨Åer in noisy label learning. The proposed method, named REED, contains three stages and can take good care of both repre sentation and classiÔ¨Åer by leveraging the above discoveries (see Table 1). SpeciÔ¨Åcally, in the Ô¨Årst stage, inspired by the recent advances of selfsupervised representation learning technologies [10, 24], we learn the representation through a contrastive pretext task. Then, in the second stage, we uti lize the intrinsic robustness in classiÔ¨Åer learning to obtain a reliable classiÔ¨Åer with noisy labels, which helps to trans fer the noisy label learning into a semisupervised learning problem. Finally, to fully explore the information in the transferred labels, we construct a classbalanced sampler and graphstructured regularizer to jointly Ô¨Ånetune the rep resentation and classiÔ¨Åer in the third stage. Contributions of this paper are as follows: ‚Ä¢ We decouple the classical endtoend training procedures into representation learning and classiÔ¨Åer learning and systematically explore their robustness in the presence of noisy labels respectively. We discover that representation matters much more than the classiÔ¨Åer, since the represen tation is very fragile while classiÔ¨Åer can exhibit strong robustness in the presence of noisy labels. ‚Ä¢ We propose an effective and threestage learning man ner, i.e., REED, which leverages selfsupervised repre sentation learning to solve the fragility of representation learning and make full use of the robustness of the clas siÔ¨Åer. Also by assigning credibility to samples and two improvements for semisupervised learning (i.e., graph structured regularization and classbalanced sampler), we 1We offer a detailed example in Appendix ??.further improve the ability of classiÔ¨Åcation. ‚Ä¢ We perform extensive experiments on both synthetic, i.e., noisy CIFAR10 and CIFAR100 datasets, and real benchmark, i.e., Clothing1M dataset. Results demon strate that the proposed method can beat the stateofthe art ones by a large margin, especially under high noise level. Effectiveness of each stage is also elaborated in ablation studies. Notations. Letxidenote the image, yibe the clean label, andyibe the noisy version of yi. A ConvNet typically con tains two parts, one is the representation learning part, i.e., zi=h(xi;), whereziis the representation for xiandh is implemented by the feature extractor (i.e., deep stack of convolutional and pooling layers) with parameter ; another part is the classiÔ¨Åer, i.e., f=g(zi;W), whereWis the pa rameters of a multilayer perceptron (MLP). Its prediction probability (or conÔ¨Ådence) for the ith class is given by a softmax function as pi=efi=PC k=1efk, whereCis the number of classes. Crossentropy loss is used for training, i.e.,Lce(p;y) ="
58,Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data.txt,"Jointly extracting entity pairs and their relations is challenging when
working on distantly-supervised data with ambiguous or noisy labels. To
mitigate such impact, we propose uncertainty-aware bootstrap learning, which is
motivated by the intuition that the higher uncertainty of an instance, the more
likely the model confidence is inconsistent with the ground truths.
Specifically, we first explore instance-level data uncertainty to create an
initial high-confident examples. Such subset serves as filtering noisy
instances and facilitating the model to converge fast at the early stage.
During bootstrap learning, we propose self-ensembling as a regularizer to
alleviate inter-model uncertainty produced by noisy labels. We further define
probability variance of joint tagging probabilities to estimate inner-model
parametric uncertainty, which is used to select and build up new reliable
training instances for the next iteration. Experimental results on two large
datasets reveal that our approach outperforms existing strong baselines and
related methods.","Joint extraction involves extracting multiple types of entities and relations between them using a sin gle model, which is necessary in automatic knowl edge base construction (Yu et al., 2020). One way to cheaply acquire a large amount of labeled data for training joint extraction models is through dis tant supervision (DS) (Mintz et al., 2009). DS involves aligning a knowledge base (KB) with an unlabeled corpus using handcrafted rules or logic constraints. Due to the lack of human annotators, DS brings a large proportion of noisy labels, e.g., over 30% noisy instances in some cases (Mintz et al., 2009), making it impossible to learn useful features. The noise can be either false relations due to the aforementioned rulebased matching assump tion or wrong entity tags due to limited coverage over entities in opendomain KBs.Existing distantlysupervised approaches model noise relying either on heuristics such as reinforce ment learning (RL) (Nooralahzadeh et al., 2019; Hu et al., 2021) and adversarial learning (Chen et al., 2021), or patternbased methods (Jia et al., 2019; Shang et al., 2022) to select trustable in stances. Nevertheless, these methods require de signing heuristics or handcrafted patterns which may encourage a model to leverage spurious fea tures without considering the confidence or uncer tainty of its predictions. In response to these problems, we propose UnBED ‚ÄîUncertaintyaware Bootstrap learning for joint Extraction on Distantlysupervised data. UnBED assumes that 1) low data uncertainty in dicates reliable instances using a pretrained lan guage model (PLM) in the initial stage, 2) model should be aware of trustable entity and relation la bels regarding its uncertainty after training. Our bootstrap serves uncertainty as a principle to miti gate the impact of noise labels on model learning and validate input sequences to control the num ber of training examples in each step. Particularly, we quantify data uncertainty of an instance accord ing to its winning score (Hendrycks and Gimpel, 2017) and entropy (Shannon, 1948). We define averaged maximum probability that is estimated by a joint PLM over each token in a sequence to adapt previous techniques in joint extraction scheme. Instances with low data uncertainty are collected to form an initial subset, which is used to tune the joint PLM tagger and facilitate fast convergence. Then, we define parametric uncer tainty in two perspectives‚Äîintermodel and inner model uncertainty. The former is quantified by self ensembling (Wang and Wang, 2022) and serves as a regularizer to improve model robustness against noisy labels during training. The latter is captured by probability variance in MC Dropout (Gal and Ghahramani, 2016) for selecting new confident in stances for the next training iteration. Such twoarXiv:2305.03827v2  [cs.CL]  8 Jun 2023fold model uncertainties reinforce with each other to guide the model to iteratively improve its robust ness and learn from reliable knowledge. 2 Related Work "
158,Temporal Continuity Based Unsupervised Learning for Person Re-Identification.txt,"Person re-identification (re-id) aims to match the same person from images
taken across multiple cameras. Most existing person re-id methods generally
require a large amount of identity labeled data to act as discriminative
guideline for representation learning. Difficulty in manually collecting
identity labeled data leads to poor adaptability in practical scenarios. To
overcome this problem, we propose an unsupervised center-based clustering
approach capable of progressively learning and exploiting the underlying re-id
discriminative information from temporal continuity within a camera. We call
our framework Temporal Continuity based Unsupervised Learning (TCUL).
Specifically, TCUL simultaneously does center based clustering of unlabeled
(target) dataset and fine-tunes a convolutional neural network (CNN)
pre-trained on irrelevant labeled (source) dataset to enhance discriminative
capability of the CNN for the target dataset. Furthermore, it exploits
temporally continuous nature of images within-camera jointly with spatial
similarity of feature maps across-cameras to generate reliable pseudo-labels
for training a re-identification model. As the training progresses, number of
reliable samples keep on growing adaptively which in turn boosts representation
ability of the CNN. Extensive experiments on three large-scale person re-id
benchmark datasets are conducted to compare our framework with state-of-the-art
techniques, which demonstrate superiority of TCUL over existing methods.","Person reidentication (reid) is an important problem in computer vision that aims to match images of a person captured by dierent cameras with non overlapping views [31]. It is viewed as a search problem with a goal to retrieve the most relevant images to the top ranks [30]. Many recent works have at tracted extensive research eorts to address the reid problem using deep learning [29,14,8,27,15,23]. In spite of remarkable advancements achieved by deep learn ing methods, most existing techniques adopt a supervised learning paradigm to solve the reid problem [29,14]. Hence, these supervised deep models take an assumption of availability of sucient crossview identity matching pairs ofarXiv:2009.00242v1  [cs.CV]  1 Sep 20202 U. Ali et al. manually labelled training data for each camera network. This assumption limits the generalizing capability of a reid model to multiple camera networks due to lack of labelled training samples under a new environment. As a result, many previous works have addressed this scarcity of labelled training data under a new camera network by focusing on unsupervised or semi supervised learning [10,12,17,3,25]. Most of these works typically deal with rela tively small datasets; without being able to exploit the potential of deep learning methods that require largescale datasets for better performance [10,11,19,28]. More recently, there has been greater emphasis towards clustering and domain adaptation based deep unsupervised methods for person reid [6,21,16]. However, performance of unsupervised learning approaches is much weaker as compared to supervised models. This is due to nonexistence of crossview identity labelled matching pairs, which makes it dicult to learn the discriminative representa tion for recognizing a person under severe changes in visual appearances across dierent camera networks. To improve this situation, we propose to use temporal continuity of images within a camera jointly with spatial similarity of feature maps acrosscameras to generate reliable pseudolabels for training a reid model in a progressive fashion. Temporal information is readily available for images captured by a camera, for example, frame number (Frame ID) of an image in a video sequence is easily ob tainable in an unsupervised manner. The idea is that given an image in a camera, other images of the same identity would exist in the temporal vicinity of that image. However, temporal continuity is only eective within one camera; hence, it cannot be utilized to obtain crosscamera matching identity pairs. To this end, we propose a crosscamera centerbased pseudolabeling method to cluster fea tures by their similarity to centers. Centers are initialized by performing a simple clustering task on features extracted from a baseline model pretrained on irrel evant (source) dataset. An intersection between crosscamera pseudolabels and withincamera temporal vicinity results in highly reliable pseudolabels for ne tuning a reid model. Features extracted from netuned model are then used to generate a new set of more reliable labels for further netuning in a progressive manner. It is also notable here that dierent from videobased method [13,1], our method does not rely on person trajectory obtained by tracking algorithms. Therefore, it is highly applicable to practical scenarios. In summary, the contributions of this paper are as follows: {we propose centerupdate based pseudolabeling approach in order to provide crossview discriminative information for unsupervised person reid task. {we exploit temporal continuity of images within a camera to sample reliable datapoints and utilize selfpaced progressive learning to train an eective re id model. To the best of our knowledge, this is the rst work to use temporal continuity for person reid task. {extensive experiments on three largescale datasets indicate the eectiveness of our method with performance on par with stateoftheart approaches.Temporal Continuity for Person ReId 3 2 Related Work "
97,MFNet: Multi-filter Directive Network for Weakly Supervised Salient Object Detection.txt,"Weakly supervised salient object detection (WSOD) targets to train a
CNNs-based saliency network using only low-cost annotations. Existing WSOD
methods take various techniques to pursue single ""high-quality"" pseudo label
from low-cost annotations and then develop their saliency networks. Though
these methods have achieved good performance, the generated single label is
inevitably affected by adopted refinement algorithms and shows prejudiced
characteristics which further influence the saliency networks. In this work, we
introduce a new multiple-pseudo-label framework to integrate more comprehensive
and accurate saliency cues from multiple labels, avoiding the aforementioned
problem. Specifically, we propose a multi-filter directive network (MFNet)
including a saliency network as well as multiple directive filters. The
directive filter (DF) is designed to extract and filter more accurate saliency
cues from the noisy pseudo labels. The multiple accurate cues from multiple DFs
are then simultaneously propagated to the saliency network with a
multi-guidance loss. Extensive experiments on five datasets over four metrics
demonstrate that our method outperforms all the existing congeneric methods.
Moreover, it is also worth noting that our framework is flexible enough to
apply to existing methods and improve their performance.","With the emergence of convolutional neural networks (CNNs) [16], a lot of salient object detection (SOD) meth ods [31, 22, 32, 41] based on CNNs have been proposed *Equal Contributions ‚Ä†Corresponding Author Image CAM Y1 Y2 Ground truth Figure 1. Different pseudo labels synthesized by different refine ment algorithms on class activation map (CAM), in which Y1and Y2represent pseudo labels from pixelwise [4] and superpixel wise [29] refinement algorithms, respectively. and broken the records. However, these CNNsbased SOD methods heavily rely on large amounts of handlabeling data with pixellevel annotations, which are laborintensive and timeconsuming [39]. Due to the high cost of labeling pixellevel annotations, some promising works have been proposed to explore other lowcost alternatives, including scribble [38, 35] and image level category labels [36, 29, 17]. Among them, the cate gory label based methods only require category labels for training, and an overwhelming amount of labels for the existence of object categories are already given ( e.g. Ima geNet [5]). Thus, in this paper, we focus on the imagelevel category label based salient object detection (WSOD1). Previous works on WSOD proposed various techniques such as global smooth pooling [29], multisource supervi sions [36] and alternate optimization [17] to pursue sin 1For convenience, we denote WSOD as methods based on imagelevel category label in this paper.gle ‚Äùhighquality‚Äù pseudo label for training their saliency networks. Though these works have achieved good perfor mance, the generated single ‚Äùhighquality‚Äù pseudo label is usually trapped by its prejudiced characteristics due to the different adopted refinement algorithms. For example, the incomplete deficiency ( 3rdcolumn in Figure 1) and redun dant noise ( 4thcolumn in Figure 1). Instead of pursuing single ‚Äùhighquality‚Äù pseudo labels, we propose to utilize multiple pseudo labels to establish a more robust framework and avoid the negative impacts from the single prejudiced label. To begin with, we adopt two different refinement algorithms, including a pixelwise one [4] and a superpixelwise one [29], to synthesize two different pseudo labels. Both of these two algorithms uti lize abundant appearance information in RGB images to perform refinement for class activation maps (CAMs) [43]. The pixelwise one treats each individual pixel as units, takes its class activation score as clues and then infers its neighbor pixels‚Äô scores, while the superpixelwise one takes superpixels as its operation units. As a result, the synthe sized pseudo labels Y1(from pixelwise algorithm) and Y2 (from superpixelwise algorithm) describe different charac teristics. As is shown in Figure 1, Y1provides better de tailed information, but is usually trapped in incompleteness, while Y2can cover more complete objects but introduces more extra noisy information. These observations drive us to explore how to extract and integrate more comprehensive and robust saliency cues from multiple pseudo labels. The core insight of this work is to adequately excavate the comprehensive saliency cues in multiple pseudo labels and avoid the prejudice of the single label. To be specific, for multiple pseudo labels, we 1) extract abundant accurate multiple saliency cues from multiple noisy labels, and 2) perform integration and propagate the integrated multiple cues to the saliency network. Concretely, our contributions are as follows: ‚Ä¢ We introduce a new framework to utilize multiple pseudo labels for WSOD, which employs more com prehensive and robust saliency cues in multiple labels to avoid the negative impacts of a single label. ‚Ä¢ We design a multifilter directive network (denoted as MFNet), in which multiple directive filters and a multiguidance loss are proposed to extract and in tegrate multiple saliency cues from multiple pseudo labels respectively. ‚Ä¢ Extensive experiments on five benchmark datasets over four metrics demonstrate the superiority of our method as well as the multiple pseudo labels. ‚Ä¢ We also extend the proposed framework to existing method MSW [36] and the prove its effectiveness by achieving 9.1% improvements over Fœâ Œ≤metric on ECSSD dataset.2. Related Work "
114,Activation Ensembles for Deep Neural Networks.txt,"Many activation functions have been proposed in the past, but selecting an
adequate one requires trial and error. We propose a new methodology of
designing activation functions within a neural network at each layer. We call
this technique an ""activation ensemble"" because it allows the use of multiple
activation functions at each layer. This is done by introducing additional
variables, $\alpha$, at each activation layer of a network to allow for
multiple activation functions to be active at each neuron. By design,
activations with larger $\alpha$ values at a neuron is equivalent to having the
largest magnitude. Hence, those higher magnitude activations are ""chosen"" by
the network. We implement the activation ensembles on a variety of datasets
using an array of Feed Forward and Convolutional Neural Networks. By using the
activation ensemble, we achieve superior results compared to traditional
techniques. In addition, because of the flexibility of this methodology, we
more deeply explore activation functions and the features that they capture.","Most of the recent advancements in the mathematics of neural networks come from four areas: network architec ture (Jaderberg et al., 2015) (Gulcehre et al., 2016a), opti mization method (AdaDelta (Zeiler, 2012) and Batch Nor malization (Ioffe & Szegedy, 2015) ), activations func tions, and objective functions (such as Mollifying Net works (Gulcehre et al., 2016b) ). Highway Networks (Sri vastava et al., 2015) and Residual Networks (He et al., 2016) both use the approach of adding data from a previ ous layer to one further ahead for more effective learning. On the other hand, others use Memory Networks (Weston et al., 2015) to more effectively remember past data and even answer questions about short paragraphs. These new architectures move the Ô¨Åeld of neural networks and ma chine learning forward, but one of the main driving forces that brought neural networks back into popularity is the rec tiÔ¨Åer linear unit (ReLU)(Glorot et al., 2011) (Nair & Hin ton, 2010). Because of trivial calculations in both forwardand backward steps and its ability to more effectively help a network learn, ReLU‚Äôs revolutionized the way neural net works are studied. One technique that universally increases accuracy is en sembling multiple predictive models. There are books and articles that explain the advantages of using multiple mod els rather than a single large model (Zhang & Ma, 2012). When neural networks garnered more popularity in the 90‚Äôs and early 2000‚Äôs, researchers used the same technique of ensembles (Zhou et al., 2002). Additionally, other tech niques may identify as an ensemble. For example, the na ture of Dropout (Srivastava et al., 2014) trains many differ ent networks together by dropping nodes from the entirety of a network. We focus on activation functions rather than expanding the general architectures of networks. Our work can be seen as a layer or neuron ensemble that can be applied to any variety of deep neural network via activation ensembles. We do not focus on creating yet another unique activation function, but rather ensemble a number of proven activation functions in a compelling way. The end result is a novel activation function that is a combination of existing func tions. Each activation function, from ReLU to hyperbolic tangent contain advantages in learning. We propose to use the best parts of each in a dynamic way decided by vari ables conÔ¨Åguring contributions of each activation function. These variables put weights on each activation function un der consideration and are optimized via backpropagation. As data passes through a deep neural network, each layer transforms the data to better interpret and gather features. Therefore, the best possible function at the top of a network may not be optimal in the middle or bottom of a network. The advantage of our architecture is that rather than choos ing activations at speciÔ¨Åed layers or over an entire network, one can give the network the option to choose the best pos sible activation function of each neuron at each layer. Creating an ensemble of activation functions presents some challenges in how to best combine the activation functions to extract as much information from a given dataset as pos sible. In the most basic model ensembles, one can average the given probabilities of multiple models. This is only feasible because the range of values in each model are the same, which is not replicated in most activation functions.arXiv:1702.07790v1  [stat.ML]  24 Feb 2017Activation Ensembles for Deep Neural Networks The difÔ¨Åculty lies in restricting or expanding the range of these functions without losing their inherent performance. An activation ensemble consists of two important parts. The Ô¨Årst is the main parameter attached to each activation function for each neuron. This variable assigns a weight to each activation function considered, i.e., it designs a convex combination of activation functions. The second are a set of ‚Äúoffset‚Äù parameters, and, which we use to dynamically offset normalization range for each function. Training of these new parameters occurs during typical model training and is done through backpropagation. Our work contains two signiÔ¨Åcant contributions. First, we implement novel activation functions as convex combina tions of known functions with interesting properties based upon current knowledge of activation functions and learn ing. Second, we improve the learning of any network con sidered herein, including the wellestablished residual net work in the Cifar100 learning task. 2. Related Work "
74,Convolutional Neural Networks for User Identificationbased on Motion Sensors Represented as Image.txt,"In this paper, we propose a deep learning approach for smartphone user
identification based on analyzing motion signals recorded by the accelerometer
and the gyroscope, during a single tap gesture performed by the user on the
screen. We transform the discrete 3-axis signals from the motion sensors into a
gray-scale image representation which is provided as input to a convolutional
neural network (CNN) that is pre-trained for multi-class user classification.
In the pre-training stage, we benefit from different users and multiple samples
per user. After pre-training, we use our CNN as feature extractor, generating
an embedding associated to each single tap on the screen. The resulting
embeddings are used to train a Support Vector Machines (SVM) model in a
few-shot user identification setting, i.e. requiring only 20 taps on the screen
during the registration phase. We compare our identification system based on
CNN features with two baseline systems, one that employs handcrafted features
and another that employs recurrent neural network (RNN) features. All systems
are based on the same classifier, namely SVM. To pre-train the CNN and the RNN
models for multi-class user classification, we use a different set of users
than the set used for few-shot user identification, ensuring a realistic
scenario. The empirical results demonstrate that our CNN model yields a top
accuracy of 89.75% in multi-class user classification and a top accuracy of
96.72% in few-shot user identification. In conclusion, we believe that our
system is ready for practical use, having a better generalization capacity than
both baselines.","Nowadays, common mobile device authentication mechanisms such as PINs, graphical passwords and ngerprint scans oer limited security. These mechanisms are susceptible to guessing (or spoong in the case of ngerprint scans) and to side channel attacks [1] such as smudge [2], re ection [3, 4] and video capture attacks [5{7]. On top of this, a fundamental limitation of PINs, passwords, and ngerprint scans is that these mechanisms require explicit user interaction. Due to the world wide adoption of mobile devices and the advancement of technologies, mobile devices are now equipped with multiple sensors such as accelerometers, gyroscopes, magnetometers, among others. The data recorded by these sensors during the interaction of the user with the mobile device can be used as biometric data to identify the user. Indeed, onetime or continuous user identication based on the data collected by the motion sensors of a mobile device is an actively studied task [8{22], that emerged after the integration of motion sensors into commonly used mobile devices. In this paper, we propose a novel deep learning approach that can identify the user from a single tap on smart phone's touchscreen, using the discrete signals recorded by the accelerometer and the gyroscope during the tap gesture. By minimizing the user's interaction during verication and by removing the requirement to explicitly insert PINs, graphical passwords or scan ngerprints, we eliminate many of the enumerated attacks. Our approach is based on transforming the discrete 3axis signals from the accelerometer and the gyroscope into a grayscale image representation that can be provided as input for deep convolutional neural networks (CNNs) [23,24]. Our image representation is based on repeating the six onedimensional (1D) signals using a modied version of de Brujin sequences [25], such that the 3 3 convolutional lters from the rst layer of the CNN get to \see"" every possible tuple of three 1D signals in their receptive eld. After transforming the motion signals accordingly, we pretrain several CNN architectures in a multiclass user classication setting. In the pretraining stage, we can leverage the use of data from multiple users and multiple samples per user. After pretraining and selecting the bestperforming CNN, we can employ the selected CNN as a deep feature extractor that generates useful embeddings for each tap gesture. The generated embeddings can then be used to train a lightweight model, e.g. Support Vector Machines (SVM) [26], to identify the user in a fewshot learning setting. We consider a fewshot learning setting 1arXiv:1912.03760v2  [cs.LG]  23 Mar 2020with 20 samples per user in order to enable a fast registration process (20 taps on the screen are enough), similar in terms of time to the registration processes used by standard ngerprint or face authentication systems. We conduct experiments in order to compare our user identication system based on CNN features with two baselines, one that is based on handcrafted features [18] and one that is based on recurrent neural network (RNN) features [17]. All models are evaluated in a fewshot user identication context using the same classier, namely SVM. Our CNN model (as well as the baseline RNN model) is pretrained on a multiclass user classication task. The users involved in the multiclass user classication experiment are dierent from those involved in the user identication experiment, to simulate a realistic scenario. In order to conduct our experiments, we modify the HMOG data set [20] by extracting shorter signals from the original sessions and by splitting the users in half, using the rst half for the preliminary multiclass user classication experiment and the second half for the user identication experiment. Our SVM based on CNN features proves a higher generalization capacity, surpassing both baselines in the user identication experiments. Moreover, according to McNemar's statistical testing [27] performed at a condence level of 0:01, our improvements over the baselines are signicant. With an accuracy of 96.72%, our SVM based on CNN features seems to be a viable solution for practical usage. In summary, our contribution is threefold: ‚Ä¢We propose a novel grayscale image representation of the discrete signals, designed specically to be useful as input for CNNs. ‚Ä¢We propose to pretrain CNNs on a multiclass user classication task in order to obtain useful embeddings for fewshot user identication. ‚Ä¢We perform comparative experiments showing that our method based on CNN embeddings surpasses both machine learning methods based on handcrafted features and deep learning methods based on RNN embeddings. The rest of this paper is organized as follows. In Section 2, we provide an overview of user identication systems for mobile devices, focusing mainly on systems based on analyzing motion sensors. In Section 3, we present the proposed data representation, our CNN architectures and our user identication model. In Section 4, we present the data set, the evaluation metrics and the performed experiments. In Section 5, we conclude our ndings and propose some future directions of study. 2 Related Work "
595,Increasing Trustworthiness of Deep Neural Networks via Accuracy Monitoring.txt,"Inference accuracy of deep neural networks (DNNs) is a crucial performance
metric, but can vary greatly in practice subject to actual test datasets and is
typically unknown due to the lack of ground truth labels. This has raised
significant concerns with trustworthiness of DNNs, especially in
safety-critical applications. In this paper, we address trustworthiness of DNNs
by using post-hoc processing to monitor the true inference accuracy on a user's
dataset. Concretely, we propose a neural network-based accuracy monitor model,
which only takes the deployed DNN's softmax probability output as its input and
directly predicts if the DNN's prediction result is correct or not, thus
leading to an estimate of the true inference accuracy. The accuracy monitor
model can be pre-trained on a dataset relevant to the target application of
interest, and only needs to actively label a small portion (1% in our
experiments) of the user's dataset for model transfer. For estimation
robustness, we further employ an ensemble of monitor models based on the
Monte-Carlo dropout method. We evaluate our approach on different deployed DNN
models for image classification and traffic sign detection over multiple
datasets (including adversarial samples). The result shows that our accuracy
monitor model provides a close-to-true accuracy estimation and outperforms the
existing baseline methods.","Deep neural networks (DNNs) have achieved unprecedent edly high classiÔ¨Åcation accuracy and found success in nu merous applications, including image classiÔ¨Åcation, speech recognition, and nature language processing. Nonetheless, training an errorfree or 100% accurate DNN is impossible in most practical cases. Inference accuracy is a crucial met ric for quantifying the performance of DNNs. Typically, the reported inference accuracy of a DNN is measured ofÔ¨Çine on test datasets with labels, but this can signiÔ¨Åcantly differ Accepted by the AISafety workshop colocated with IJCAI PRICAI 2020.from the true accuracy on a user‚Äôs dataset because of, e.g., data distribution shift away from the training dataset or even adversarial modiÔ¨Åcation to the user‚Äôs data [Cheet al. , 2019; Kull et al. , 2019; Malinin and Gales, 2018 ]. Moreover, ob taining the true accuracy is very challenging in practice due to the lack of groundtruth labels. The unknown inference accuracy has further decreased the transparency of already hardtoexplain DNNs and raised signiÔ¨Åcant concerns with their trustworthiness, especially in safetycritical applications. Consequently, studies on increas ing trustworthiness of DNNs have been proliferating. For example, many studies have considered outofdistribution (OOD) detection and adversarial sample detection, since OOD and adversarial samples often dramatically decrease in ference accuracy of DNNs [Hendrycks and Gimpel, 2017; Cheet al. , 2019; Lee et al. , 2018; Liang et al. , 2018 ]. While these efforts can offer an increased assurance of DNNs to users to some extent, they do not provide a quantitative mea sure of actual classiÔ¨Åcation accuracy, which is a more di rect and sensible measure of the target DNN‚Äôs performance. Some other studies propose (posthoc) processing to quan tify/estimate the prediction conÔ¨Ådence of a DNN [Guo et al. , 2017; Kull et al. , 2019; Snoek et al. , 2019 ]. Nonetheless, they typically require the target DNN‚Äôs training/validation dataset to train a (sometimes complicated) new transformation model for conÔ¨Ådence calibration, and do not transfer well to new un seen datasets. The accuracy of a target DNN on a user‚Äôs op erational dataset can also be estimated via selective random sampling, but it can suffer from a high estimation variance [Liet al. , 2019 ]. Contribution. In this paper, we propose a simple yet ef fective posthoc method ‚Äî accuracy monitoring ‚Äî which increases the trustworthiness of DNN classiÔ¨Åcation results by estimating the true inference accuracy on an actual (possibly OOD/adversarial) dataset. Concretely, as shown in Fig. 1, we propose a neural networkbased accuracy monitor model, which only takes the deployed DNN‚Äôs softmax probability output as its input and directly predicts if the DNN‚Äôs predic tion result is correct or not. Thus, over a sequence of pre diction samples from a user‚Äôs dataset, our accuracy monitor can form an estimate of the target DNN‚Äôs true inference ac curacy. Furthermore, we employ an ensemble of monitoring models based on the MonteCarlo dropout method, providing a robust estimate of the target DNN‚Äôs true accuracy.arXiv:2007.01472v1  [cs.LG]  3 Jul 2020Deployed  ModelModel  ProviderAccuracy Monitor User DataSoftmax Probabilities Monitor  ModelsEstimated AccuracyTraining DatasetLabled DatasetTraining1 212Figure 1: Accuracy monitoring for a deployed/target DNN. Utilizing as little information as the target DNN‚Äôs soft max probability output for accuracy estimation provides bet ter transferability than more complicated calibration methods [Kull et al. , 2019 ]. SpeciÔ¨Åcally, we can pretrain an accu racy monitor model based on a labeled dataset relevant to the target application of interest (e.g., public datasets for im age classiÔ¨Åcation). Then, for model transfer, we can selec tively label a small amount (1% in our work) of data from the user‚Äôs test dataset with active learning via an entropy acqui sition function [Beluch et al. , 2018 ], and retrain our monitor models on the selectively labeled data using transfer learn ing. In addition, without the need of accessing the target DNN‚Äôs training/validation datasets, our accuracy monitoring method can be easily applied as a plugin module on top of the target DNN to monitor its runtime performance on a va riety of datasets. Thus, our method is not restricted to the DNN providers themselves; instead, even an end user can em ploy our method to monitor the target DNN‚Äôs accuracy perfor mance on its own, bringing further increased trustworthiness of accuracy monitoring. To evaluate the effectiveness of our accuracy monitoring method, we consider different target DNN models for image classiÔ¨Åcation (10 classes and 1000 classes) and for trafÔ¨Åc sign detection in autonomous driving, respectively. Our results show that, by only utilizing the prediction class and softmax probability output of the deployed DNN model and labeling 1% of the user‚Äôs dataset, our method can monitor the healthy of the target DNN models, providing a remarkably accurate estimation of the true classiÔ¨Åcation accuracy on a variety of user‚Äôs datasets. 2 Related Works "
384,Improving Label Ranking Ensembles using Boosting Techniques.txt,"Label ranking is a prediction task which deals with learning a mapping
between an instance and a ranking (i.e., order) of labels from a finite set,
representing their relevance to the instance. Boosting is a well-known and
reliable ensemble technique that was shown to often outperform other learning
algorithms. While boosting algorithms were developed for a multitude of machine
learning tasks, label ranking tasks were overlooked. In this paper, we propose
a boosting algorithm which was specifically designed for label ranking tasks.
Extensive evaluation of the proposed algorithm on 24 semi-synthetic and
real-world label ranking datasets shows that it significantly outperforms
existing state-of-the-art label ranking algorithms.","Label ranking is a prediction task which deals with learn ing a mapping between an instance and a ranking (i.e., or der) of labels from a Ô¨Ånite set, representing their relevance to the instance [Zhou et al. , 2014 ]. Due to its wide applica bility, label ranking has attracted a lot of focus from the ar tiÔ¨Åcial intelligence community in recent years [H¬®ullermeier et al. , 2008; Cheng et al. , 2009; Aiguzhinov et al. , 2010; Cheng et al. , 2013; Zhou et al. , 2014; Gurrieri et al. , 2014; Destercke et al. , 2015; Aledo et al. , 2017; S ¬¥aet al. , 2017; Zhou and Qiu, 2018 ]. Applications of label ranking include for example, text classiÔ¨Åcation, where a news article may belong to multiple topics, and the goal of the label ranking algorithm is to rank the topics according to their relevance to the document. In pattern recognition, objects can be ordered according to their relevance to the image [Yang et al. , 2016 ]. In metalearning, a label ranking model can provide a list of algorithms to a given problem, ranked according to their Ô¨Åt to the problem, based on the characteristics of the problem at hand [Brazdil et al. , 2003 ]. Label ranking tasks must not be confused with multilabel tasks [Tsoumakas et al. , 2009 ]nor with learning to rank tasks [Cohen et al. , 1998 ]. In label ranking tasks, the target at tribute of each instance contains a ranking of labels, repre senting their relative relevance to the instance. In contrast, inmultilabel tasks, the target attribute is a nonranked subset of relevant labels, and in learning to rank tasks, the goal is to produce a ranked list of the instances themselves. Boosting is a wellknown and reliable ensemble technique [Schapire, 2003 ]that was shown to often outperform other learning algorithms. AdaBoost [Freund and Schapire, 1997 ] is one of the most widely used classiÔ¨Åcation boosting tech niques. Variations of AdaBoost were developed for multi class tasks [Freund and Schapire, 1997 ], multilabel tasks [Schapire and Singer, 2000 ], regression tasks [Drucker, 1997; Solomatine and Shrestha, 2004 ], and learning to rank tasks [Cohen et al. , 1998; Xu and Li, 2007; Wu et al. , 2010 ]. How ever, to the best of our knowledge, no boosting algorithm was suggested for label ranking tasks. In this paper, we propose a novel boosting algorithm, Ad aBoost.LR, which was speciÔ¨Åcally designed for label rank ing tasks. An extensive evaluation of AdaBoost.LR over 24 semisynthetic and realworld datasets shows that it signiÔ¨Å cantly outperforms existing stateoftheart label ranking al gorithms. The rest of this paper is organized as follows: we Ô¨Årst dis cuss label ranking ensembles in section 2. Thereafter, we de scribe our proposed method in section 3. This is followed by an overview of our experimental setting in section 4, a de scription of our extensive evaluation in section 5, and a sum mary and suggestions for future work in section 6. 2 Related Work "
375,Identifying Label Errors in Object Detection Datasets by Loss Inspection.txt,"Labeling datasets for supervised object detection is a dull and
time-consuming task. Errors can be easily introduced during annotation and
overlooked during review, yielding inaccurate benchmarks and performance
degradation of deep neural networks trained on noisy labels. In this work, we
for the first time introduce a benchmark for label error detection methods on
object detection datasets as well as a label error detection method and a
number of baselines. We simulate four different types of randomly introduced
label errors on train and test sets of well-labeled object detection datasets.
For our label error detection method we assume a two-stage object detector to
be given and consider the sum of both stages' classification and regression
losses. The losses are computed with respect to the predictions and the noisy
labels including simulated label errors, aiming at detecting the latter. We
compare our method to three baselines: a naive one without deep learning, the
object detector's score and the entropy of the classification softmax
distribution. We outperform all baselines and demonstrate that among the
considered methods, ours is the only one that detects label errors of all four
types efficiently. Furthermore, we detect real label errors a) on commonly used
test datasets in object detection and b) on a proprietary dataset. In both
cases we achieve low false positives rates, i.e., when considering 200
proposals from our method, we detect label errors with a precision for a) of up
to 71.5% and for b) with 97%.","Nowadays, the predominant paradigm in computer vision is to learn models from data. The performance of the model largely depends on the amount of data and its quality, i.e., the diversity of input images and label accuracy (Feng et al. [2020]; Hussain and Zeadally [2018]; Jaeger et al. [2020]; Kaur et al. [2021]; Kuutti et al. [2020]). Deep neural net works (DNNs) are particularly data hungry Sun et al. [2017]. In this work, we focus on the case of object detection where multiple objects per scene belonging to a Ô¨Åxed set of classes Figure 1: Example image from the Pascal VOC 2007 test dataset with two labeled boats marked by the blue boxes and multiple unla beled boats. are annotated via bounding boxes (Everingham et al. [2010]; Riedlinger et al. [2022a]). In many industrial and scientiÔ¨Åc applications, the labeling process consists of an iterative cycle of data acquisition, la beling, quality assessment, and model training. Labeling data is costly, time consuming and error prone, e.g. due to incon sistencies caused by multiple human labelers or a change in label policy over time. Therefore, at least a partial automa tion of the label process is desirable. Research directions that aim at this goal are active learning (Settles [2009]; Brust et al.[2018]; Roy et al. [2018]; Elezi et al. [2021]; Desai et al. [2019]) and automated label error detection (Northcutt et al. [2021a]; Rottmann and Reese [2022]; Dickinson and Meurers [2003]). Active learning alternates between data labeling and model training, where the latter is utilized for selecting new images for labeling such that the model‚Äôs accuracy increases as quickly as possible. These methods often assume that the labels are obtained by an errorfree oracle, which typically does not hold in realworld applications. The extent to which noisy labels affect the model performance is studied by Wu et al.[2018], resulting in the observation that the model is able to tolerate a certain amount of noise in training data with out losing too much performance on test sets. Other methods model label uncertainty (Riedlinger et al. [2022b]; Miller et al.[2018]) or improve robustness w.r.t. noisy labels (Li et al. [2020]; Feng et al. [2021]; Zhang and Wang [2019]). Up to now, in contrast to active learning, automated de tection of label errors has received less attention. There ex ist some works on image classiÔ¨Åcation datasets (Northcutt et al. [2021b,a]; Thyagarajan et al. [2022]) as well as onearXiv:2303.06999v1  [cs.CV]  13 Mar 2023work on semantic segmentation datasets by Rottmann and Reese [2022]. Label errors may affect generalization perfor mance, which makes their detection desirable (Northcutt et al.[2021b]). Furthermore, there is business value interest in improving or accelerating the review process by partial au tomation. Here, we for the Ô¨Årst time study the task of label error de tection in object detection datasets by a) introducing a bench mark and b) developing a detection method and compare it against three canonical baselines. We introduce a benchmark by simulating label errors on the BDD100k (Yu et al. [2020]) and EMNISTDet (Riedlinger et al. [2022a]) dataset. The lat ter is a semisynthetic dataset consisting of EMNIST letters (Cohen et al. [2017]) pasted into COCO images (Lin et al. [2014]) of which we expect to possess highly accurate labels. The types of label errors that we consider are missing labels (drops ), correct localization but wrong classiÔ¨Åcation ( Ô¨Çips), correct classiÔ¨Åcation but inaccurate localization ( shifts ), and labels that actually represent background ( spawns ). We ad dress the detection of these errors by a novel method based on monitoring instancewise object detection loss. We study the effectiveness of our method in comparison to three base lines. Then, we demonstrate for commonly used object detec tion test datasets, such as BDD100k (Yu et al. [2020]), MS COCO (Lin et al. [2014]), Pascal VOC (Everingham et al. [2010]) and Kitti (Geiger et al. [2012]), and also for a pro prietary dataset on car part detection that our method detects label errors by reviewing moderate sample sizes of 200 im ages per dataset. Our contributions can be summarized as follows: ‚Ä¢ We introduce a novel method based on the instancewise loss for detecting label errors in object detection. ‚Ä¢ We introduce a benchmark for identifying four types of label errors on BDD100k and EMNISTDet. ‚Ä¢ We apply our method to detect label errors in commonlyused and proprietary object detection datasets and manually evaluate the error detection performance for moderate sample sizes. To contribute to future development of label error detection methods and potentially cleaning up object detection datasets, we provide an implementation of our benchmark, method and baselines as well as label Ô¨Åles that include simulated label errors and model checkpoints that allow to reproduce of our results, see GitHub . The remainder of this work is structured as follows: sec tion 2 contains a summary of the literature in label error de tection in image classiÔ¨Åcation and semantic segmentation as well as modeling label uncertainty and robust training in ob ject detection. Thereafter, it is summarized how the presented work relates to the literature. In section 3 we introduce the simulation of label errors for benchmarking, followed by im plemented baselines for comparison, our method, the used evaluation metrics and the detection of real label errors. Sec tion 4 Ô¨Årst introduces our experimental setup. Then, we show results on our label error benchmark in section 4.2 and after wards, in section 4.3, we detect real label errors on commonly used object detection test datasets. Finally, we close with a summary of conclusions in section 5.2 Related Work "
60,Label Contrastive Coding based Graph Neural Network for Graph Classification.txt,"Graph classification is a critical research problem in many applications from
different domains. In order to learn a graph classification model, the most
widely used supervision component is an output layer together with
classification loss (e.g.,cross-entropy loss together with softmax or margin
loss). In fact, the discriminative information among instances are more
fine-grained, which can benefit graph classification tasks. In this paper, we
propose the novel Label Contrastive Coding based Graph Neural Network (LCGNN)
to utilize label information more effectively and comprehensively. LCGNN still
uses the classification loss to ensure the discriminability of classes.
Meanwhile, LCGNN leverages the proposed Label Contrastive Loss derived from
self-supervised learning to encourage instance-level intra-class compactness
and inter-class separability. To power the contrastive learning, LCGNN
introduces a dynamic label memory bank and a momentum updated encoder. Our
extensive evaluations with eight benchmark graph datasets demonstrate that
LCGNN can outperform state-of-the-art graph classification models. Experimental
results also verify that LCGNN can achieve competitive performance with less
training data because LCGNN exploits label information comprehensively.","Applications in many domains in the real world exhibit the favorable property of graph data structure, such as social networks [15], nancial platforms [20] and bioinformatics [5]. Graph classication aims to identify the class labels of graphs in the dataset, which is an important problem for numerous applications. For instance, in biology, a protein can be represented with a graph where each amino acid residue is a node, and the spatial relationships between residues (distances, angles) are the edges of a graph. Classication of graphs representing proteins can help predict protein interfaces [5]. Recently, graph neural networks (GNNs) have achieved outstanding perfor mance on graph classication tasks [29,33]. GNNs aims to transform nodes to lowdimensional dense embeddings that preserve graph structural information ?Two authors contributed equally to this workarXiv:2101.05486v1  [cs.LG]  14 Jan 20212 Y. Ren et al. and attributes [34]. When applying GNNs to graph classication, the standard method is to generate embeddings for all nodes in the graph and then summarize all these node embeddings to a representation of the entire graph, such as using a simple summation or neural network running on the set of node embeddings [31]. For the representation of the entire graph, a supervision component is usually utilized to achieve the purpose of graph classication. A nal output layer to gether with classication loss (e.g.,crossentropy loss together with softmax or margin loss) is the most commonly used supervision component in many existing GNNs [29,28,32,6]. This supervision component focuses on the discriminability of class but ignores the instancelevel discriminative representations. A recent trend towards learning stronger representations to serve classication tasks is to reinforce the model with discriminative information as more as possible [4]. To be explicit, graph representations, which consider both intraclass compact ness and interclass separability [14], are more potent on the graph classication tasks. Inspired by the idea of recent selfsupervised learning [3] and contrastive learning [7,18], the contrastive loss [17] is able to extract extra discriminative information to improve the model's performance. The recent works [8,18,35] of using contrast loss for representation learning are mainly carried out under the setting of unsupervised learning. These contrastive learning models treat each in stance as a distinct class of its own. Meanwhile, discriminating these instances is their learning objective [7]. The series of contrastive learning have been veried eective in learning more negrained instancelevel features in the computer vision [26] domain. Thus we plan to utilize the contrastive learning on graph classication tasks to make up for the shortcomings of supervision components, that is, ignoring the discriminative information on the instancelevel. However, when applying contrastive learning, the inherent large intraclass variations may import noise to graph classication tasks [14]. Besides, existing contrastive learn ing based GNNs (e.g., GCC [18]) detach the model pretraining and netuning steps. Compared with endtoend GNNs, the learned graph representations via contrastive learning can hardly be used in the downstream application tasks directly, like graph classication. To cope with the task of graph classication, we propose the label contrastive coding based graph neural network (LCGNN), which employs Label Contrastive Loss to encourage instancelevel intraclass compactness and interclass sepa rability simultaneously. Unlike existing contrastive learning using a single posi tive instance, the label contrastive coding imports label information and treats instances with the same label as multiple positive instances. In this way, the instances with the same label can be pulled closer, while the instances with dif ferent labels will be pushed away from each other. Intraclass compactness and interclass separability are taken into consideration simultaneously. The label contrastive coding can be regarded as training an encoder for a dictionary look up task [7]. In order to build an extensive and consistent dictionary, we propose a dynamic label memory bank and a momentumupdated graph encoder inspired by the mechanism [7]. At the same time, LCGNN also uses Classication Loss3 Query GraphsKey GraphsGraph  EncoderfqfkGraph  Encoder‚Ä¶Input Graph  Minibatch‚Ä¶‚Ä¶Momentum Update‚Ä¶‚Ä¶Key Graph  Representations Query Graph  RepresentationsMemory Bank‚Ä¶y1y2y3ymym"
110,Robust Long-Tailed Learning under Label Noise.txt,"Long-tailed learning has attracted much attention recently, with the goal of
improving generalisation for tail classes. Most existing works use supervised
learning without considering the prevailing noise in the training dataset. To
move long-tailed learning towards more realistic scenarios, this work
investigates the label noise problem under long-tailed label distribution. We
first observe the negative impact of noisy labels on the performance of
existing methods, revealing the intrinsic challenges of this problem. As the
most commonly used approach to cope with noisy labels in previous literature,
we then find that the small-loss trick fails under long-tailed label
distribution. The reason is that deep neural networks cannot distinguish
correctly-labeled and mislabeled examples on tail classes. To overcome this
limitation, we establish a new prototypical noise detection method by designing
a distance-based metric that is resistant to label noise. Based on the above
findings, we propose a robust framework,~\algo, that realizes noise detection
for long-tailed learning, followed by soft pseudo-labeling via both label
smoothing and diverse label guessing. Moreover, our framework can naturally
leverage semi-supervised learning algorithms to further improve the
generalisation. Extensive experiments on benchmark and real-world datasets
demonstrate the superiority of our methods over existing baselines. In
particular, our method outperforms DivideMix by 3\% in test accuracy. Source
code will be released soon.","ClassiÔ¨Åcation problems in realworld typically exhibit a longtailed label distribution, where most classes are associated with only a few examples, e.g., visual recognition [ 1,2,3], instance segmentation [ 4], and text categorization [ 5]. Due to the paucity of training examples, generalisation for tail classes is challenging; moreover, na√Øve learning on such data is susceptible to an undesirable bias towards head classes. Recently, longtailed learning (LTL) has gained renewed interest in the context of deep neural networks [ 6,7,8,9,10,11,12,13]. Two active strands of work involve normalisation of the classiÔ¨Åer‚Äôs weights, and modiÔ¨Åcation of the underlying loss to account for different class penalties. Each of these strands is intuitive, and has been empirically shown to be effective [14]. The abovementioned LTL methods with remarkable performance are mostly trained on clean datasets with highquality human annotations. However, in realworld machine learning applications, annotating a largescale dataset is costly and timeconsuming. Some recent works resort to the large amount of web data as a source of supervision for training deep neural networks [ 15]. While the existing works have shown advantages in various applications [ 16,17], web data is naturally classimbalanced [ 18,19] and accompanied with label noise [ 20,18,21,22]. As a result, it is crucial that deep neural networks can harvest noisy and classimbalanced training data. Although the LTL and noisy label problems have been extensively studied in previous literature, it is still poorly explored when the training dataset follows a longtailed label distribution while contains label noise. We provide a simple visualization of the studied problem in Figure 1a. Without considering label noise, we show that LTL methods severely degrade their performance in experiments. To address this problem, a direct approach is to apply methods equal contributionarXiv:2108.11569v1  [cs.LG]  26 Aug 2021Robust LongTailed Learning under Label Noise A P REPRINT /uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c /uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000013/uni00000014/uni0000004e/uni00000015/uni0000004e/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000026/uni0000004f/uni00000048/uni00000044/uni00000051 /uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c /uni00000037/uni00000048/uni00000056/uni00000057 (a) Problem setup /uni00000030/uni00000044/uni00000051/uni0000005c /uni00000030/uni00000048/uni00000047/uni0000004c/uni00000058/uni00000050 /uni00000029/uni00000048/uni0000005a/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000027/uni0000004c/uni00000059/uni0000004c/uni00000047/uni00000048/uni00000030/uni0000004c/uni0000005b /uni00000035/uni00000052/uni0000002f/uni00000037/uni0000000e (b) CIFAR10 /uni00000030/uni00000044/uni00000051/uni0000005c /uni00000030/uni00000048/uni00000047/uni0000004c/uni00000058/uni00000050 /uni00000029/uni00000048/uni0000005a/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000027/uni0000004c/uni00000059/uni0000004c/uni00000047/uni00000048/uni00000030/uni0000004c/uni0000005b /uni00000035/uni00000052/uni0000002f/uni00000037/uni0000000e (c) CIFAR100 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000048/uni00000055/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018/uni00000035/uni00000048/uni00000053/uni00000055/uni00000048/uni00000056/uni00000048/uni00000051/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni0000001a/uni0000001c/uni00000011/uni00000019 /uni0000001a/uni0000001c/uni00000011/uni00000019 /uni0000001a/uni0000001c/uni00000011/uni0000001a /uni0000001a/uni0000001c/uni00000011/uni00000019 /uni0000001a/uni0000001c/uni00000011/uni00000019 /uni0000001a/uni0000001b/uni00000011/uni00000019 /uni00000019/uni0000001b/uni00000011/uni00000015 /uni00000019/uni0000001b/uni00000011/uni00000014 /uni00000019/uni0000001a/uni00000011/uni00000014 /uni00000019/uni00000019/uni00000011/uni00000015 /uni00000019/uni00000018/uni00000011/uni00000016 /uni00000019/uni00000017/uni00000011/uni00000018 /uni00000019/uni00000019/uni00000011/uni0000001b /uni00000019/uni00000019/uni00000011/uni00000019 /uni00000019/uni00000019/uni00000011/uni00000015 /uni00000019/uni00000019/uni00000011/uni00000013 /uni00000019/uni00000018/uni00000011/uni00000013 /uni00000019/uni00000016/uni00000011/uni0000001a /uni00000019/uni00000014/uni00000011/uni0000001a /uni00000019/uni00000014/uni00000011/uni00000015 /uni00000019/uni00000014/uni00000011/uni00000014 /uni00000019/uni00000013/uni00000011/uni0000001c /uni00000019/uni00000013/uni00000011/uni00000013 /uni00000018/uni0000001c/uni00000011/uni00000015 /uni00000018/uni00000019/uni00000011/uni00000019 /uni00000018/uni00000019/uni00000011/uni00000018 /uni00000018/uni00000019/uni00000011/uni00000015 /uni00000018/uni00000018/uni00000011/uni0000001c /uni00000018/uni00000018/uni00000011/uni00000018 /uni00000018/uni00000017/uni00000011/uni00000019 /uni00000017/uni00000017/uni00000011/uni0000001a /uni00000017/uni00000017/uni00000011/uni00000014 /uni00000017/uni00000017/uni00000011/uni00000013 /uni00000017/uni00000016/uni00000011/uni00000015 /uni00000017/uni00000016/uni00000011/uni00000014 /uni00000017/uni00000015/uni00000011/uni00000019 (d) Confusion matrix Figure 1: (a) Illustration of the studied problem setup. (bc) Comparison of DivideMix and ROLT+ on datasets with imbalance ratio 100and noise level 0:2. (d) We show the test accuracy of NCM classiÔ¨Åer under different noise levels by disentangling representation and classiÔ¨Åer learning. for learning with noisy labels to LTL. One of the most commonly used approaches for learning with noisy labels is DivideMix [ 18], which uses the smallloss criterion to detect label noise. However, we note that using such approach leads to unsatisfactory results in longtailed label distribution, as shown in Figure 1b and 1c. Therefore, it remains a challenge to obtain models that can cope with LTL under label noise. To achieve performance improvement, it is a natural idea to detect noisy data while accommodating class imbalance. It is known that a classiÔ¨Åer trained on longtailed data yields higher accuracy for head classes but hurts tail classes [ 9]. Thus, to detect label noise, it is not trustworthy to use predictions and training losses produced by the biased classiÔ¨Åer. Another commonly used approach for LTL is the nearest class mean (NCM) classiÔ¨Åer that computes class prototypes and performs nearest neighbour search in embedding space [ 9]. In Figure 1d, we test NCM by disentangling the representation and classiÔ¨Åer learning. We Ô¨Årst train a feature extractor, then compute class prototypes by polluting clean labels with different noise level. It depicts that the estimation of class prototypes is robust to label noise. This observation motivates us to explore the geometric information between examples and their class prototypes as a criterion for noise detection. After acquiring the prototypes, we design a classindependent noise detector by treating examples closed to their corresponding prototypes as clean, while others as noisy. Unlike learning from balanced datasets where noisy data can be removed from training [ 23], we claim that each example is signiÔ¨Åcant, especially for tail classes. To this end, we introduce a new soft pseudolabeling mechanism that uses both label smoothing and label guessing to guide the learning of networks. Thanks to the generality of our proposed noise detection method, we can also interpret noisy examples as unlabeled data and incorporate wellestablished semisupervised learning techniques to further improve the generalisation. Our main contributions are: (i) We study the problem of longtailed learning under label noise, which is less explored and is a signiÔ¨Åcant step towards realworld applications; (ii) We Ô¨Ånd that the commonly used smallloss trick fails in longtailed learning. Thus, we establish a novel prototypical noise detection method that overcomes the limitations of smallloss trick; (iii) We propose a robust framework, ROLT. It realizes noise detection that is immune to label distribution, and compensates the problem of data scarcity for tail classes. Our framework can be built on top of semisupervised learning methods without much extra overhead, leading to an improved approach ROLT+. The proposed methods achieve strong empirical performance on benchmark and realworld datasets. 2 Related Work "
200,Learning Compact Appearance Representation for Video-based Person Re-Identification.txt,"This paper presents a novel approach for video-based person re-identification
using multiple Convolutional Neural Networks (CNNs). Unlike previous work, we
intend to extract a compact yet discriminative appearance representation from
several frames rather than the whole sequence. Specifically, given a video, the
representative frames are selected based on the walking profile of consecutive
frames. A multiple CNN architecture incorporated with feature pooling is
proposed to learn and compile the features of the selected representative
frames into a compact description about the pedestrian for identification.
Experiments are conducted on benchmark datasets to demonstrate the superiority
of the proposed method over existing person re-identification approaches.","Person reidentication(reid) has been widespread concerned recently, as this issue underpins various crit ical applications such as video surveillance, pedestrian tracking and searching. Given a target person appear ing in a surveillance camera, a reid system generally aims to identify it in the other cameras through the whole cameranetwork, i.e., determining whether in stances captured by dierent cameras belong to the same person. However, due to the in uence of clut tered background, occlusions and viewpoint variations across camera views, this task is quite challenging. A reid system may have an image or a video as in put for feature extraction. Since only limited informa tion can be exploited from a single image, it is dicult to overcome the occlusion, cameraview and pose vari ation problems and to capture the varying appearance of a pedestrian performing dierent action primitives. Thus it is better to deal with the videobased reidproblem, as videos inherently contain more temporal information of the moving person than an independent image, not to mention in many practical applications the input are videos to begin with. Besides, video is a sequence of images, so spatial and temporal cues are more abundant in a video than in a image, which can facilitate extracting more features. Figure 1. Salient appearance in person reid. In spite of the rich spacetime information provided by a video sequence, more challenges come along. So far, only a few videobased methods have been pre sented [26], [15], [20]. Most of them focus on investigat ing the temporal information related to person's mo tion, such as their gait, and perhaps even the patterns of how their bodies and clothes move. Although such movement is one type of behavioral biometrics, it is unfortunate that a large number of persons share simi larity in walking manners and related behavior [29] [31]. Moreover, since gait is considered a biometric that is not aected by the appearance of a person, most ap proaches tried to exploit it by working with silhou ettes, which are dicult to extract, especially from surveillance data with cluttered background and occlu sions [15]. Besides, timeseries analysis usually requires extracting information at dierent timescales [20]. In 1arXiv:1702.06294v2  [cs.CV]  20 Sep 2019¬∑¬∑¬∑ CNN CNN CNN CNN Feature Pooling CNN CNN CNN CNN Feature Pooling YES or NO ¬∑¬∑¬∑ CNN based feature  learning Distance metric  learning Representative frame  extraction Query set Gallery setFigure 2. An overview of the proposed videobased reid framework. the person reid problem, gait information often exists in short time, thus the information provided by move ment descriptors is limited. In some cases, it is even harder to distinguish the video representations of dif ferent identities than the stillimage appearance [29]. Unlike previous work, in this paper we intend to extract a compact appearance representation from sev eral representative frames rather than the whole frames for videobased reid. Compared to the temporalbased methods, the proposed appearance model works more similarly to human visual system. Because the visual perception studies on appearance (e.g., color, texture) and motion stimuli have shown that the pattern detec tion thresholds are much lower than the motion detec tion thresholds [24] [13] [5]. Hence, human performs better at identifying the appearance of human body or belongings than the manners of how a person walks. In most cases, people can be distinguished more eas ily from appearance such as clothes and bags on their shoulders than from gait and pose which are generally similar among dierent persons [21], as shown in Fig. 2. So, videos are highly redundant and it is unneces sary to incorporate all frames for person reid. Our study shows that several typical frames with appro priate feature extraction can oer competitive or even better identication performance. More specically, given a walking sequence, we rst split it into a couple of segments corresponding to dif ferent action primitives of a walking cycle. The most representative frames are selected from a walking cy cle by exploiting the local maxima and minima of theFlow Energy Prole (FEP) signal [26]. For each frame, we propose a CNN to learn feature based on person's joint appearance information. Since dierent frames may have dierent discriminative features for recogni tion, by introducing an appearancepooling layer, the salient appearance features of multiple frames are pre served to form a discriminative feature descriptor for the whole video sequence. The central point of our al gorithm lies in the exploration of the key appearance information of a video, contrary to the conventional methods like [20] and [15], which highly rely on accu rate temporal information. 2. Related work "
184,Higher-Order Label Homogeneity and Spreading in Graphs.txt,"Do higher-order network structures aid graph semi-supervised learning? Given
a graph and a few labeled vertices, labeling the remaining vertices is a
high-impact problem with applications in several tasks, such as recommender
systems, fraud detection and protein identification. However, traditional
methods rely on edges for spreading labels, which is limited as all edges are
not equal. Vertices with stronger connections participate in higher-order
structures in graphs, which calls for methods that can leverage these
structures in the semi-supervised learning tasks.
  To this end, we propose Higher-Order Label Spreading (HOLS) to spread labels
using higher-order structures. HOLS has strong theoretical guarantees and
reduces to standard label spreading in the base case. Via extensive
experiments, we show that higher-order label spreading using triangles in
addition to edges is up to 4.7% better than label spreading using edges alone.
Compared to prior traditional and state-of-the-art methods, the proposed method
leads to statistically significant accuracy gains in all-but-one cases, while
remaining fast and scalable to large graphs.","Given an undirected unweighted graph and a few labeled ver tices, the graph transductive learning or semisupervised learning (SSL) aims to infer the labels for the remaining unlabeled vertices [1,6,17,30,36,37,39,40]. Graph SSL finds applications in a number of settings: in a social network, we can infer a particular character istic (e.g. political leaning) of a user based on the information of her friends to produce tailored recommendations; in a userproduct bipartite rating network, based on a few manually identified fraud ulent user accounts, SSL is useful to spot other fraudulent accounts [4,10,18,19]; SSL can identify protein functions from networks of their physical interaction using just a few labels [32]. Traditional graph SSL algorithms leverage a key property of real world networks: the homophily of vertices [5,21], i.e., the nearby vertices in a graph are likely to have the same label. However, these methods tend to be limited by the fact that all the neighbors of a vertex are not equal. Consider your own friendship network where you have many acquaintances, but only a few close friends. In fact, This paper is published under the Creative Commons Attribution 4.0 International (CCBY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW ‚Äô20, April 20‚Äì24, 2020, Taipei, Taiwan ¬©2020 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CCBY 4.0 License. ACM ISBN 9781450370233/20/04. https://doi.org/10.1145/3366423.3379997 ?Network structure (clique)331400AliceBCDPQRS=K2K3K4=Figure 1: Graph SSL approaches which take only edges into account incorrectly classify the unlabeled central vertex ‚ÄòAl ice‚Äô as blue. By leveraging higherorder network structures, the proposed HOLS correctly labels Alice as red. prior research has shown that vertices with a strong connection participate in several higherorder structures, such as dense sub graphs and cliques [ 12‚Äì14,27]. Thus, leveraging the higherorder structure between vertices is crucial to accurately label the vertices. Let us elaborate this using a small friendship network example, shown in Figure 1. The central vertex, Alice, participates in a closely knit community with three friends B, C, and D, all of whom know each other. In addition, she has four acquaintances P, Q, R, and S from different walks of her life. Let the vertices be labeled by their ideological beliefs‚Äîvertices B, C, and D have the same blue label; and the rest of the vertices have the red label. Even though Alice has more red connections than blue, the connection between Alice, B, C, and D is stronger as Alice participates in three 3cliques and one 4clique with them. In contrast, Alice has no 3 and 4 cliques with P, Q, R, and S. Owing to the stronger connection with the red nodes, Alice should be labeled red as well. However, traditional graph SSL techniques that rely on edges alone label Alice as blue [ 39,40]. This calls for graph SSL methods that look beyond edges to leverage the signal present in higherorder structures to label vertices. Our present work focuses on three key research questions: ‚Ä¢RQ1. How do the data reveal that higherorder network struc tures are homogeneous in labels? ‚Ä¢RQ2. How can we leverage higherorder network structures for graph SSL in a principled manner? ‚Ä¢RQ3. Do higherorder structures help improve graph SSL? Accordingly, our contributions can be summarized as follows: (i) Analysis: Through an empirical analysis of four diverse real world networks, we demonstrate the phenomenon of higherorder label homogeneity , i.e., the tendency of vertices participating in a higherorder structure (e.g. triangle) to share the same label. (ii) Algorithm: We develop HigherOrder Label Spreading ( HOLS ) to leverage higherorder structures during graph semisupervised learning. HOLS works for any userinputted higherorder structure and in the base case, is equivalent to edgebased label spreading [ 39]. (iii) Effectiveness: We show that label spreading via higherorder structures strictly outperforms label spreading via edges by up to 4.7% statistically significant margin. Notably, HOLS is competitive with recent deep learning based methods, while running 15 √ófaster.arXiv:2002.07833v1  [cs.SI]  18 Feb 2020WWW ‚Äô20, April 20‚Äì24, 2020, Taipei, Taiwan Eswaran, Kumar, and Faloutsos Table 1: Qualitative comparison of HOLS 3 (using edges and triangles) with traditional and recent graph SSL approaches Desiderata LP[40] LS[39] BP[37] Planetoid [36] GCN [17] MixHop [1] HOLS 3 Higherorder structures ? ? ? ‚úì Theoretical guarantees ‚úì ‚úì ? ‚úì Fast algorithm ‚úì ‚úì ‚úì ‚úì For reproducibility, all the code and datasets are available at https://github.com/dhivyaeswaran/hols. 2 RELATED WORK "
345,Walk in Wild: An Ensemble Approach for Hostility Detection in Hindi Posts.txt,"As the reach of the internet increases, pejorative terms started flooding
over social media platforms. This leads to the necessity of identifying hostile
content on social media platforms. Identification of hostile contents on
low-resource languages like Hindi poses different challenges due to its diverse
syntactic structure compared to English. In this paper, we develop a simple
ensemble based model on pre-trained mBERT and popular classification algorithms
like Artificial Neural Network (ANN) and XGBoost for hostility detection in
Hindi posts. We formulated this problem as binary classification (hostile and
non-hostile class) and multi-label multi-class classification problem (for more
fine-grained hostile classes). We received third overall rank in the
competition and weighted F1-scores of ~0.969 and ~0.61 on the binary and
multi-label multi-class classification tasks respectively.","During coronavirus lockdown, number of active internet users across the globe has increased rapidly. The government enforced lockdown, which pushed people to stay indoors and thus, increased the engagement with social media platforms like Facebook, Twitter, Instagram, Whatsapp, etc. This led to increased hostile posts over social media, including cyberbullying, trolling, spreading hate, death threat, etc. A major challenge for the common users in the digital space is to identify misinformation (aka fake news) in online content. In addition to that, according to a recent survey2, on Twitter, there has been a 900% increase in hate speech directed towards Chinese people and 200% increase in the trac to Hate sites and posts written against the Asian community. It is also found3that the percentage of nonEnglish tweets in India has jumped up by 50%. This inspires a necessity of research in hostility detection in posts written in low resource but widelyused languages like Hindi. 1https://competitions.codalab.org/competitions/26654 2https://bit.ly/38BBrTu 3http://bit.ly/38Eo7gYarXiv:2101.06004v1  [cs.CL]  15 Jan 20212 Chander et al. As billions of posts appear each day on social media and antisocial elements can get full anonymity while expressing hostile behavior over the internet, iden tication of authorized information should have a reliable automation system. Even though Hindi is the third most spoken language globally, it is considered a lowresource language due to the unavailability of accurate tools and suitable datasets for various tasks in Hindi. This motivates us to take the task of hostility detection of Hindi posts on Social media. We view the hostility detection as a twostage process. First, a Coarsegrained classication is done to mark a post as hostile or nonhostile. If the post is de tected as hostile, then the second stage performs a more negrained classica tion of hostile classes. We brie y dene these two terms: 1.Coarsegrained classication : It is a binary classication problem in which each post is categorised as hostile or nonhostile. 2.Finegrained classication : It is a multilabel multiclass classication of the hostile classes. Each hostile post belongs to one or more of the following categories: fake news, hate speech, oensive and defamation. In our proposed approach, we leverage the pretrained multilingual BERT (mBERT) [6]4for input post representation and further these representations are used as input for Articial Neural Network (aka ANN) and other ML learning models for binary and multilabel multiclass classication problems. The base architecture of mBERT is the same as BERT. BERT and mBERT have proven as the state of the art models across multiple NLU and NLG tasks. The rest of this paper is organized in the following way: Section 2 presents related work; Section 3 gives an indepth explanation of our model. Section 4 presents the experimental setup. Section 5 provides results and our analysis. Finally, we provide conclusions and directions for future work in Section 6. 2 Related Work "
24,Contextual Pyramid Attention Network for Building Segmentation in Aerial Imagery.txt,"Building extraction from aerial images has several applications in problems
such as urban planning, change detection, and disaster management. With the
increasing availability of data, Convolutional Neural Networks (CNNs) for
semantic segmentation of remote sensing imagery has improved significantly in
recent years. However, convolutions operate in local neighborhoods and fail to
capture non-local features that are essential in semantic understanding of
aerial images. In this work, we propose to improve building segmentation of
different sizes by capturing long-range dependencies using contextual pyramid
attention (CPA). The pathways process the input at multiple scales efficiently
and combine them in a weighted manner, similar to an ensemble model. The
proposed method obtains state-of-the-art performance on the Inria Aerial Image
Labelling Dataset with minimal computation costs. Our method improves 1.8
points over current state-of-the-art methods and 12.6 points higher than
existing baselines on the Intersection over Union (IoU) metric without any
post-processing. Code and models will be made publicly available.","The developments in the systematic collection and organiza tion of remote sensing imagery have resulted in several high resolution aerial imagery datasets. Information from aerial imagery plays a key role in urban planning, disaster aver sion, and change detection. Building detection is a crucial aspect for the aforementioned applications. Depending on the geographical region and conditions, building structures have different shapes and sizes. This challenge is particularly ad dressed by Maggiori et al. [1]. They created a dataset of la beled aerial imagery from different locations for this problem, such that a model trained from a variety of sources general izes to the task of segmentation. Semantic segmentation in aerial imagery is challenging due to variable lighting condi tions, shapes/sizes, and large intraclass variations. In this re search, we address the problem of improving the building seg mentation by utilizing attentive multiscale pathways. Each of the paths exploits nonlocal neighborhoods that account for buildings of varying sizes. This allows our network to Fig. 1 . Comparison of building segmentation on Inria Aerial Image Labeling Dataset. RGB, GT and outputs (bottom row) ResNet101FPN without (left) and with our module (right). learn longrange dependencies at various scales with minimal computation costs. In addition to attentive multiscale path ways, we also incorporate a channelwise attention module to model interdependencies across channels. In summary, our contributions are as follows. We introduce a selfattention based contextual pyramid attention (CPA) module that accounts for various build ing sizes to segment buildings in aerial images. The proposed module outperforms current stateoftheart methods by about 2% on the IoU metric and 12.6% over FCN baselines. Through experiments, we also show that our base model offers competitive performance to current state oftheart methods, while having much lower inference costs. We also provide ablation studies on the impact of our proposed module and other comparisons.arXiv:2004.07018v1  [cs.CV]  15 Apr 20202. RELATED WORK "
467,Towards Harnessing Feature Embedding for Robust Learning with Noisy Labels.txt,"The memorization effect of deep neural networks (DNNs) plays a pivotal role
in recent label noise learning methods. To exploit this effect, the model
prediction-based methods have been widely adopted, which aim to exploit the
outputs of DNNs in the early stage of learning to correct noisy labels.
However, we observe that the model will make mistakes during label prediction,
resulting in unsatisfactory performance. By contrast, the produced features in
the early stage of learning show better robustness. Inspired by this
observation, in this paper, we propose a novel feature embedding-based method
for deep learning with label noise, termed LabEl NoiseDilution (LEND). To be
specific, we first compute a similarity matrix based on current embedded
features to capture the local structure of training data. Then, the noisy
supervision signals carried by mislabeled data are overwhelmed by nearby
correctly labeled ones (\textit{i.e.}, label noise dilution), of which the
effectiveness is guaranteed by the inherent robustness of feature embedding.
Finally, the training data with diluted labels are further used to train a
robust classifier. Empirically, we conduct extensive experiments on both
synthetic and real-world noisy datasets by comparing our LEND with several
representative robust learning approaches. The results verify the effectiveness
of our LEND.","The philosophy of recent success in deep learning mainly stems from massive highquality labeled data, leading to impressive performance in countless areas, including computer vision [22, 14], natural language processing [38, 7], speech recognition [15, 33], etc. However, the data in real world applications are often associated with label noise, due to human fatigue [13], knowledge limitation [10], or measurement error [35]. These noisy labels might degrade the performance of deep neural networks (DNNs) [1, 49], which raises a great demand for label noiserobust learning algorithms. Therefore, deep learning with label noise has been intensively studied due to its wide applications [25, 27, 23]. Previous results [1] suggest that DNNs /f_irst learn from examples with correct labels, and the noisy data will be /f_itted later. It is wellknown as the memorization eÔ¨Äect of deep learning. In spired by this remarkable /f_inding, a large group of previous works propose to employ the model predictions in the early stage of training to boost the robust learning. For model predictionbased robust methods, existing works can be generally attributed into two categories, namely label correctionbased methods and sample selectionbased methods. Label correctionbased methods [24, 43, 47, 36, 29] take the model predictions of training data as additional supervision signals to correct the potential noisy labels for guiding DNNs‚Äô training. Sample selectionbased meth ods [19, 11, 20, 12, 45] select trustworthy examples with the loss value smaller than prede/f_ined thresholds during training to mitigate the negative eÔ¨Äect of noisy labels. However, in practice, we observe that the model will make mistakes during label prediction and may be unstable in the output [16], resulting in unsatisfactory performance. Fig. 1 provides us a piece of evidence, from which we can see that the model still makes errorprone predictions even if the memorization eÔ¨Äect exists, while the embedded features remain robust. To be speci/f_ic, we have the following three observations: 1) The modelpredicted labels in Fig. 1 (b) are much more reliable than the noisy ones in Fig. 1 (c); 2) Under 30 epochs of training, the model still makes errorprone predictions for some training examples, especially for the cyan, red, and lime points in Fig. 1 (b); 3) The embedded features in the early stage of learning contain strong semantic information, as the data points with the same groundtruth labels are clustered together in Fig. 1 (a). It also indicates that the embedded features induced by the memorization eÔ¨Äect are more robust than model predictions. The reason is that the classi/f_ier output following a neural net work tends to /f_it the noise, while the embedded features are less negatively aÔ¨Äected by the noise [4]. Fig. 4 further provides us an empirical validation, where the accuracy of modelpredicted la bels measures the robustness of model predictions and the accuracy of diluted labels reveals the robustness of the feature embedding. While lots of previous works focus on developing robust models based on model predictions, scarce attention has been paid to the feature embedding. The above observations may provide us a new research insight for deep robust learning with noisy labels by leveraging the intrinsic feature embedding. In this paper, we claim that the embedded features induced by the memorization eÔ¨Äect are more robust than the modelpredicted labels. To instantiate such insight, we further propose a simple yet eÔ¨Äective feature embeddingbased label noise method, termed LabElNoiseDilution (LEND). Therein, we /f_irst compute a similarity matrix based on current embedded features to 2(a) Colored with groundtruth labels       (b) Colored with modelpredicted labels (c) Colored with original noisy labelsFigure 1: The tSNE visualization of training data under 30 epochs when training under asym metric label noise (noise rate: 0.45) on CIFAR10 dataset. ResNet18 is directly used to /f_it training data and the embedded features of the last hidden layer of the neural network are visualized by the tSNE method. Sub/f_igure (a) is colored with ground truth labels, (b) is colored with model predicted labels, and (c) is colored with original noisy training labels. capture the local structure of training data. Then, with the help of the trustworthy feature em bedding, the noisy supervision signals carried by mislabeled data are overwhelmed by nearby correctly labeled ones, Finally, the corrected labels are further employed to train a robust classi /f_ier. To be speci/f_ic, for each example, its nearby examples /f_irst make voting in deciding whether its label is trustworthy, and then this message is propagated among its neighborhoods for further noise dilution. This process is conducted on the embedding space, of which the robustness has been validated before. Finally, the diluted labels are further employed to perform sample selection to train a robust classi/f_ier. Empirically, we conduct extensive experiments on both synthetic and realworld noisy datasets by comparing our method with several representative robust learning approaches, and the results verify the superiority of our LEND. The contributions of this paper can be summarized as threefold: 1). We reveal a new /f_inding that the embedded features induced by the memorization eÔ¨Äect are more robust than the labels predicted by model, which provides us a new perspective for deep robust learning. 2). We propose a novel feature embeddingbased robust learning method, named LEND, which can make full use of the robust feature embedding in the early stage of learning. 3). We evaluate our LEND on both synthetic and realworld noisy datasets, and the experi ments well demonstrate its eÔ¨Äectiveness. The remaining of this paper is organized as follows: we /f_irst review the related works of two diÔ¨Äerent categories of label noise learning methods in Sect. 2. In Sect. 3, we provide some preliminaries of deep label noise learning and detail the memorization eÔ¨Äect of deep learning from the perspective of feature embedding. Further, we propose a simple yet eÔ¨Äective label noise 3learning method in Sect. 4 and conduct extensive experiments on both synthetic and realworld noisy datasets to verify its eÔ¨Äectiveness in Sect. 5. Finally, we conclude our paper in Sect. 6. 2 Related Work "
92,Inverse Graph Identification: Can We Identify Node Labels Given Graph Labels?.txt,"Graph Identification (GI) has long been researched in graph learning and is
essential in certain applications (e.g. social community detection).
Specifically, GI requires to predict the label/score of a target graph given
its collection of node features and edge connections. While this task is
common, more complex cases arise in practice---we are supposed to do the
inverse thing by, for example, grouping similar users in a social network given
the labels of different communities. This triggers an interesting thought: can
we identify nodes given the labels of the graphs they belong to? Therefore,
this paper defines a novel problem dubbed Inverse Graph Identification (IGI),
as opposed to GI. Upon a formal discussion of the variants of IGI, we choose a
particular case study of node clustering by making use of the graph labels and
node features, with an assistance of a hierarchical graph that further
characterizes the connections between different graphs. To address this task,
we propose Gaussian Mixture Graph Convolutional Network (GMGCN), a simple yet
effective method that makes the node-level message passing process using Graph
Attention Network (GAT) under the protocol of GI and then infers the category
of each node via a Gaussian Mixture Layer (GML). The training of GMGCN is
further boosted by a proposed consensus loss to take advantage of the structure
of the hierarchical graph. Extensive experiments are conducted to test the
rationality of the formulation of IGI. We verify the superiority of the
proposed method compared to other baselines on several benchmarks we have built
up. We will release our codes along with the benchmark data to facilitate more
research attention to the IGI problem.","In many scenarios, the objects with their features are connected by their interactions as graphs. By analyzing these edge connections and node features throughout various graphs, a graph identiÔ¨Åcation (GI) problem is formed to predict the information or properties of the graphs, such as labels or scores of the target graphs. Many studies have been developed on GI, such as graph classiÔ¨Åcation [ 1,2] and Cocorresponding authors. Preprint. Under review.arXiv:2007.05970v1  [cs.LG]  12 Jul 2020graph regression [ 3]. Formally, methods for GI aggregates the information from nodes and edges to predict or summarize the information of the whole graph. However, it is much more interesting to consider an inverse problem which has never been proposed: Can we use the information of graphs to infer the information of nodes or even edges and subgraphs? In another word, given the labels of graphs, how to Ô¨Ågure out the categories of nodes, edges, or subgraphs? This problem is interesting and very common in the real world. In social media, for example, thinking of hot events that are widely discussed in the form of graphs where users involved as nodes and the cofollowing relationship among users as edges, it is interesting to think how to pick out the malicious users from the mass of users with only the labels of these event topics such as the authenticity of each topic [ 4,5]. In drug discovery, for another example, thinking of molecules as graphs in which atoms as nodes and chemical bonds as edges, we attempt to identify the roles of certain subgraphs, or equivalently functional groups, in each molecular given its chemical or physical properties [6, 7]. In a programming language, for the last example, thinking of control Ô¨Çows for programs as graphs by considering statements as nodes and control Ô¨Çows as edges, can we detect the problematic statements if we have already known which program has bug or not [8]? All these problems can be deÔ¨Åned as a general problem as Inverse Graph IdentiÔ¨Åcation (IGI) that identiÔ¨Åes the nodes in graphs based on the information of graphs. The main difÔ¨Åculty of the IGI task is that the node labels are inaccessible so that all available information for the training model only comes from the labels of graphs. Namely, it is a node identiÔ¨Åcation task where the available training labels are much coarsergrained than the node labels we want to Ô¨Åt. It seems this problem can be resolved from the present perspective of node clustering [ 9] or node identiÔ¨Åcation [ 10]. However, they are different from the problems that we attempt to address. Unlike node clustering which only conducts clustering based on node features or graph structures, IGI is better guaranteed by the given label information of graphs, which, we will demonstrate, closely inÔ¨Çuences the clustering results. It is also different from the node identiÔ¨Åcation task because IGI does not contain any node labels for training. Another similar concept to our IGI is MultipleInstance Learning (MIL) [ 11] that adopts global labels of bags to identify the labels of local instances. However, MIL assumes that each instance is i.i.d. and there are no edge connections involved. Therefore, it is interesting and important to study the IGI problem as a set of new challenges and seek new solutions for it. In this paper, we formally deÔ¨Åne IGI and address out its different problem statements as a set of new challenges. Meanwhile, we focus on a particular study of IGI: A node clustering task by making use of graph labels and node features with an assistance of a hierarchical graph [ 9] that further characterizes the relations among graphs. To address this particular task, we propose a novel model based on Gaussian mixture model (GMM) and graph convolutional network (GCN) named as Gaussian Mixture Graph Convolutional Network (GMGCN). First, the features of each node are updated through the Graph Attention Network (GAT). Then the node features are aggregated by a Gaussian mixture layer (GML) and a new attention pooling layer proposed in the paper. After obtaining the graph representations, we adopt a hierGCN to classify the graphs. SpeciÔ¨Åcally, we design a consensus loss that plays a key role in the training process. Finally, a node clustering is carried out according to the parameters of GML. The main contributions are as follows: 1. New problem: We introduce a new problem called Inverse Graph IdentiÔ¨Åcation (IGI), which tries to identify the nodes in graphs based on the labels of graphs, and we take a formal discussion of the variants of IGI to attract more research attention on this problem. 2. New solution: We propose an effective model called GMGCN based on GMM and GCN to solve a particular study of IGI problem. To the best of our knowledge, this is the Ô¨Årst work to achieve node clustering in graph structure by integrating GMM into GCN. 3. New loss: We propose a consensus loss function to boost the model training through the principle of ""same attraction, opposite repulsion"". Experiments validate that this consensus loss greatly improves the model effect. We validate the proposed GMGCN on various synthetic datasets based on different problem statements and a realworld dataset. The results demonstrate that the proposed method is suitable to solve the IGI task. 2 Related Works "
183,Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.txt,"Deep neural networks (DNNs) have achieved tremendous success in a variety of
applications across many disciplines. Yet, their superior performance comes
with the expensive cost of requiring correctly annotated large-scale datasets.
Moreover, due to DNNs' rich capacity, errors in training labels can hamper
performance. To combat this problem, mean absolute error (MAE) has recently
been proposed as a noise-robust alternative to the commonly-used categorical
cross entropy (CCE) loss. However, as we show in this paper, MAE can perform
poorly with DNNs and challenging datasets. Here, we present a theoretically
grounded set of noise-robust loss functions that can be seen as a
generalization of MAE and CCE. Proposed loss functions can be readily applied
with any existing DNN architecture and algorithm, while yielding good
performance in a wide range of noisy label scenarios. We report results from
experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and
synthetically generated noisy labels.","The resurrection of neural networks in recent years, together with the recent emergence of large scale datasets, has enabled superhuman performance on many classiÔ¨Åcation tasks [ 21,28,30]. However, supervised DNNs often require a large number of training samples to achieve a high level of performance. For instance, the ImageNet dataset [ 6] has 3.2 million handannotated images. Although crowdsourcing platforms like Amazon Mechanical Turk have made largescale annotation possible, some error during the labeling process is often inevitable, and mislabeled samples can impair the performance of models trained on these data. Indeed, the sheer capacity of DNNs to memorize massive data with completely randomly assigned labels [ 42] proves their susceptibility to overÔ¨Åtting when trained with noisy labels. Hence, an algorithm that is robust against noisy labels for DNNs is needed to resolve the potential problem. Furthermore, when examples are cheap and accurate annotations are expensive, it can be more beneÔ¨Åcial to have datasets with more but noisier labels than less but more accurate labels [18]. ClassiÔ¨Åcation with noisy labels is a widely studied topic [ 8]. Yet, relatively little attention is given to directly formulating a noiserobust loss function in the context of DNNs. Our work is motivated by Ghosh et al. [9] who theoretically showed that mean absolute error (MAE) can be robust against noisy labels under certain assumptions. However, as we demonstrate below, the robustness of MAE can concurrently cause increased difÔ¨Åculty in training, and lead to performance drop. This limitation is particularly evident when using DNNs on complicated datasets. To combat this drawback, we advocate the use of a more general class of noiserobust loss functions, which encompass both MAE and CCE. Compared to previous methods for DNNs, which often involve extra steps and algorithmic modiÔ¨Åcations, changing only the loss function requires minimal intervention to existing architectures 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr√©al, Canada.arXiv:1805.07836v4  [cs.LG]  29 Nov 2018and algorithms, and thus can be promptly applied. Furthermore, unlike most existing methods, the proposed loss functions work for both closedset and openset noisy labels [ 40]. Openset refers to the situation where samples associated with erroneous labels do not always belong to a ground truth class contained within the set of known classes in the training data. Conversely, closedset means that all labels (erroneous and correct) come from a known set of labels present in the dataset. The main contributions of this paper are twofold. First, we propose a novel generalization of CCE and present a theoretical analysis of proposed loss functions in the context of noisy labels. And second, we report a thorough empirical evaluation of the proposed loss functions using CIFAR10, CIFAR100 and FASHIONMNIST datasets, and demonstrate signiÔ¨Åcant improvement in terms of classiÔ¨Åcation accuracy over the baselines of MAE and CCE, under both closedset and openset noisy labels. The rest of the paper is organized as follows. Section 2 discusses existing approaches to the problem. Section 3 introduces our noiserobust loss functions. Section 4 presents and analyzes the experiments and result. Finally, section 5 concludes our paper. 2 Related Work "
5,Large-Scale Pre-training for Person Re-identification with Noisy Labels.txt,"This paper aims to address the problem of pre-training for person
re-identification (Re-ID) with noisy labels. To setup the pre-training task, we
apply a simple online multi-object tracking system on raw videos of an existing
unlabeled Re-ID dataset ""LUPerson"" nd build the Noisy Labeled variant called
""LUPerson-NL"". Since theses ID labels automatically derived from tracklets
inevitably contain noises, we develop a large-scale Pre-training framework
utilizing Noisy Labels (PNL), which consists of three learning modules:
supervised Re-ID learning, prototype-based contrastive learning, and
label-guided contrastive learning. In principle, joint learning of these three
modules not only clusters similar examples to one prototype, but also rectifies
noisy labels based on the prototype assignment. We demonstrate that learning
directly from raw videos is a promising alternative for pre-training, which
utilizes spatial and temporal correlations as weak supervision. This simple
pre-training task provides a scalable way to learn SOTA Re-ID representations
from scratch on ""LUPerson-NL"" without bells and whistles. For example, by
applying on the same supervised Re-ID method MGN, our pre-trained model
improves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%,
2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or
few-shot setting, the performance gain is even more significant, suggesting a
better transferability of the learned representation. Code is available at
https://github.com/DengpanFu/LUPerson-NL","A large highquality labeled dataset for person re identification (ReID) is labor intensive and costly to cre ate. Existing fully labeled datasets [25, 52, 58, 61] for per son ReID are all of limited scale and diversity compared to other vision tasks. Therefore, model pretraining be *Corresponding author. (a) Market1501 with MGN  (b) Market1501 with IDE (c) DukeMTMC with MGN  (d) DukeMTMC with IDE Figure 1. Comparing person ReID performances of three pre trained models on two methods (IDE [59] and MGN [51]). Re sults are reported on Market1501 and DukeMTC, with different scales under the smallscale setting. IN.sup. refers to the model supervised pretrained on ImageNet, LUP .unsup. is the model un supervised pretrained on LUPserson, and LUPnl.pnl. is the model pretrained on our LUPersonNL dataset using our proposed PNL. comes a crucial approach to achieve good ReID perfor mance. However, due to the lack of largescale ReID dataset, most previous methods simply use the models pre trained on the crowdlabeled ImageNet dataset, resulting in a limited improvement because of the big domain gap be tween generic images in ImageNet and personfocused im ages desired by the ReID task. To mitigate this problem, the recent work [12] has demonstrated that unsupervised pretraining on a webscale unlabeled ReID image dataset ‚ÄúLUPerson‚Äù (subsampled from massive streeview videos) surpasses that of pretraining on ImageNet. In this paper, our hypothesis is that scalable ReID pre training methods that learn directly from raw videos cangenerate better representations . To verify it, we propose the noisy labels guided person ReID pretraining , which lever ages the spatial and temporal correlations in videos as weak supervision. This supervision is nearly costfree, and can be achieved by the tracklets of a person over time derived from any multiobject tracking algorithm, such as [56]. In par ticular, we track each person in consecutive video frames, and automatically assign the tracked persons in the same tracklet to the same ReID label and vice versa. Enabled by the large amounts of raw videos in LUPerson [12], publicly available data of this form on the internet, we create a new variant named ‚ÄúLUPerson NL‚Äù with derived pseudo ReID labels from tracklets for pretraining with noisy labels. This variant totally consists of 10Mperson images from 21K scenes with noisy labels of about 430Kidentities. We demonstrate that contrastive pretraining of ReID is an effective method of learning from this weak supervision at large scale. This new Pretraining framework utilizing Noisy Labels ( PNL ) composes three learning modules: (1) a simple supervised learning module directly learns from ReID labels through classification; (2) a prototypebased contrastive learning module helps cluster instances to the prototype which is dynamically updated by moving aver aging the centroids of instance features, and progressively rectify the noisy labels based on the prototype assignment. and (3) a labelguided contrastive learning module utilizes the rectified labels subsequently as the guidance. In contrast to the vanilla momentum contrastive learning [7,12,19] that treats only features from the same instance as positive sam ples, our labelguided contrastive learning uses the rectified labels to distinguish positive and negative samples accord ingly, leading to a better performance. In principle, joint learning of these three modules make the consistency be tween the prototype assignment from instances and the high confident (rectified) labels, as possible as it can. The experiments show that our PNL model achieves re markable improvements on various person ReID bench marks. Figure 1 indicates that the performance gain from our pretrained models is consistent on different scales of training data. For example, upon the strong MGN [51] baseline, our pretrained model improves the mAP by 4.4%,4.9%on Market1501 and DukeMTMC over the Im ageNet supervised one, and 0.9%,2.2%over the unsuper vised pretraining baseline [12]. Moreover, the gains are even larger under the smallscale and fewshot settings, where the labeled ReID data are extremely limited. To the best of our knowledge, we are the first to show that large scale noisy label guided pretraining can significantly ben efit person ReID task. Our key contributions can be summarized as follows: ‚Ä¢ We propose noisy label guided pretraining for person Re ID, which incorporates supervised learning, prototype based contrastive learning, labelguided contrastive learning and noisy label rectification to a unified framework. ‚Ä¢ We construct a largescale noisy labeled person ReID dataset ‚ÄúLUPersonNL‚Äù as a new variant of ‚ÄúLUPerson‚Äù. It is by far the largest noisy labeled person ReID dataset without any human labeling effort. ‚Ä¢ Our models pretrained on LUPersonNL push the state oftheart results on various public benchmarks to a new limit without bells and whistles. 2. Related Work "
116,KNN-enhanced Deep Learning Against Noisy Labels.txt,"Supervised learning on Deep Neural Networks (DNNs) is data hungry. Optimizing
performance of DNN in the presence of noisy labels has become of paramount
importance since collecting a large dataset will usually bring in noisy labels.
Inspired by the robustness of K-Nearest Neighbors (KNN) against data noise, in
this work, we propose to apply deep KNN for label cleanup. Our approach
leverages DNNs for feature extraction and KNN for ground-truth label inference.
We iteratively train the neural network and update labels to simultaneously
proceed towards higher label recovery rate and better classification
performance. Experiment results show that under the same setting, our approach
outperforms existing label correction methods and achieves better accuracy on
multiple datasets, e.g.,76.78% on Clothing1M dataset.","Deep Neural Networks (DNNs) have achieved remarkable success in various applications including computer vision, speech recognition, and robotics. Supervised learning on DNNs is data hungry. Obtaining a large dataset with la bels at an affordable cost is usually done by crowdsourc ing [33, 36] and web query [25, 34]. Each of those would inevitably introduce a signiÔ¨Åcant amount of noisy labels. On the other hand, DNNs are prone to overÔ¨Åt noisy train ing data [36, 1], and their generalization performance is *shuyukong2020@u.northwestern.edu ‚Ä†you.li@u.northwestern.edu ‚Ä°jwang34@iit.edu ¬ßme@aminrezaei.com ¬∂haizhou@northwestern.edudowngraded as a result. To resist noisy labels in training DNNs, numerous methods have been proposed, including robust loss for mulation [22, 9, 31], curriculum learning [10] and label correction [35, 28]. In this paper, we focus on label cor rection approach which alternatively sanitizes noisy la bels and improves the model performance. Previous work leverages prediction from DNN itself to infer the ground truth labels [35, 28]. However, such prediction is likely to be poisoned by the noise in the training dataset. There fore, we are motivated to seek for a more robust label cor rection approach. In this paper, we leverage the deep KNearest Neigh bors (KNN) algorithm to facilitate learning with noises. The KNN algorithm assumes that similar things exist in close proximity. KNN is a favorable classiÔ¨Åcation ap proach when no prior knowledge on sample distribution are available, and has shown robustness against adversar ial examples [26, 20, 30]. Our approach is based on a key observation that during the learning phase, useful fea tures are learnt in the intermediate layers despite the pres ence of corrupted labels in the dataset. We propose to use those features to discover similarity among samples. Even though the Ô¨Ånal labels are different for two samples belonging to same category, their features share high sim ilarity. Overall, we present a framework that iteratively ap plies deep KNN to infer ground truth labels and retrains the neural network with the predicted labels, thus simul taneously making progress toward higher label recovery and better classiÔ¨Åcation performance. It is a generalized framework that does not require an estimation of noise transition distributions or a clean dataset for reference. We also propose two KNN label correction algorithms. 1arXiv:2012.04224v1  [cs.CV]  8 Dec 2020The Ô¨Årst one, IterKNN, uses all the labels to infer ground truth labels. The second one, SelKNN, selects a certain amount of clean examples as reference for KNN ground truth inference. The selection principle is to Ô¨Ånd sam ples with small cumulative normalized loss because those samples are more likely to have correct labels. We empir ically show that our approach achieves stateofart perfor mance. The contributions of this paper are as follows: ‚Ä¢ To our best knowledge, we are the Ô¨Årst to apply deep KNN for label correction in corrupted training dataset. The features are extracted from intermedi ate layers of neural network. To further mitigate the impact of noisy labels, we propose a loss ranking ap proach to select samples with high conÔ¨Ådence to be labelled correctly as reference for KNN prediction. ‚Ä¢ We explore the beneÔ¨Åts of iterative retraining after label correction. We show that iterative retraining can help neural network escape from overÔ¨Åtting and discover more corrupted labels, thus achieves better performance. ‚Ä¢ We conduct extensive experiments to demonstrate the robustness of deep KNN against label noise. We found out that even though the Ô¨Ånal prediction is cor rupted by the noise in training dataset, CNN can still learn robust and useful features in deep layers to fa cilitate KNN for groundtruth label inference. We also provide insights on how deep feature is better than both of the shallow features and Ô¨Ånal logits with regard to noisy label correction.. 2 Related Work "
298,Learning to Learn from Noisy Web Videos.txt,"Understanding the simultaneously very diverse and intricately fine-grained
set of possible human actions is a critical open problem in computer vision.
Manually labeling training videos is feasible for some action classes but
doesn't scale to the full long-tailed distribution of actions. A promising way
to address this is to leverage noisy data from web queries to learn new
actions, using semi-supervised or ""webly-supervised"" approaches. However, these
methods typically do not learn domain-specific knowledge, or rely on iterative
hand-tuned data labeling policies. In this work, we instead propose a
reinforcement learning-based formulation for selecting the right examples for
training a classifier from noisy web search results. Our method uses Q-learning
to learn a data labeling policy on a small labeled training dataset, and then
uses this to automatically label noisy web data for new visual concepts.
Experiments on the challenging Sports-1M action recognition benchmark as well
as on additional fine-grained and newly emerging action classes demonstrate
that our method is able to learn good labeling policies for noisy data and use
this to learn accurate visual concept classifiers.","Humans are a central part of many visual scenes, and un derstanding human actions in videos is an important prob lem in computer vision. However, a key challenge in action recognition is scaling to the long tail of actions. In many practical applications, we would like to quickly and cheaply learn classiÔ¨Åers for new target actions where annotations are scarce, e.g. Ô¨Ånegrained, rare or niche classes. Manually annotating data for every new action becomes impossible, so there is a need for methods that can automatically learn from readily available albeit noisy data sources. A promising approach is to leverage noisy data from web Riding a camel Riding a bull Riding animals Feeding animals Feeding camels Feeding cattleTRAINING: Learn to select queries from ‚ÄúYouTube‚Äù Grazing animalsTESTING: Select right queries from ‚ÄúYouTube‚Äù Our model Semi supervised Running horse ?Figure 1: Our model uses a set of annotated data to learn a policy for how to label data for new, unseen classes. This enables learning domainspeciÔ¨Åc knowledge and how to se lect diverse exemplars while avoiding semantic drift. For example, it can learn from training data that human motion cues are important for actions involving animals (e.g. ‚Äúrid ing animals‚Äù) while animal appearance is not. This knowl edge can be applied at test time to label noisy data for new classes such as ‚Äúfeeding animals‚Äù, while traditional semi supervised methods would label based on visual similarity. queries. Training models for new classes using the data re turned by web queries has been proposed as an alternative to expensive manual annotation [7, 8, 19, 28]. Methods for automated labeling of new classes include traditional semisupervised learning approaches [14, 33, 34] as well as weblysupervised approaches [7, 8, 19]. However, these methods typically rely on iterative handtuned data label ing policies. This makes it difÔ¨Åcult to dynamically manage the risk tradeoff between exemplar diversity and semantic drift. Going further, as a result these methods typically can 1arXiv:1706.02884v1  [cs.CV]  9 Jun 2017not learn domainspeciÔ¨Åc knowledge. For example, when learning an action recognition model from a set of videos returned by YouTube queries, videos prominently featuring humans are more likely to be positives while those with out are more likely to be noise; this intuition is difÔ¨Åcult to manually quantify and encode. Even more, when learning an animalrelated action such as ‚Äúfeeding animals‚Äù, videos containing the action with different animals are likely to be useful positives even though their visual appearance may be different (Fig. 1). Such diverse classconditional data selec tion policies are impossible to manually encode. This in tuition inspires our work on learning data selection policies for noisy web search results. Our key insight is that good data labeling policies can be learned from existing manually annotated datasets. In tuitively, a good policy labels noisy data in a way where a classiÔ¨Åer trained on the labels would achieve high classiÔ¨Å cation accuracy on a manually annotated heldout set. Al though data labeling is a nondifferentiable action, this can be naturally achieved in a reinforcement learning setting, where actions correspond to labeling of examples and the reward is the effect on downstream classiÔ¨Åer accuracy. Concretely, we introduce a joint formulation of a Q learning agent [29] and a class recognition model. In con trast to related weblysupervised approaches [7, 19], the data collection and classiÔ¨Åer training steps are not disjoint but rather integrated into a single uniÔ¨Åed framework. The agent selects web search examples to label as positives, which are then used to train the recognition model. A sig niÔ¨Åcant challenge is the choice of the state representation, and we introduce a novel representation based on the distri bution of classiÔ¨Åer scores output by the recognition model. At training time, the model uses a dataset of labeled train ing classes to learn a data labeling policy, and at test time the model can use this policy to label noisy web data for new unseen classes. In summary, our main contribution is a principled formu lation for learning how to label noisy web data, using a re inforcement learning framework. To enable this, we also in troduce a novel state representation in terms of the classiÔ¨Åer score distributions from a jointly trained recognition model. We demonstrate our approach Ô¨Årst in the controlled setting of MNIST, then on the largescale Sports1M video bench mark [16]. Finally, we show that our method can be used for labeling newly emerging and Ô¨Ånegrained categories where annotated data is scarce. 2. Related work "
61,Instance-Dependent Noisy Label Learning via Graphical Modelling.txt,"Noisy labels are unavoidable yet troublesome in the ecosystem of deep
learning because models can easily overfit them. There are many types of label
noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with
IDN being the only type that depends on image information. Such dependence on
image information makes IDN a critical type of label noise to study, given that
labelling mistakes are caused in large part by insufficient or ambiguous
information about the visual classes present in images. Aiming to provide an
effective technique to address IDN, we present a new graphical modelling
approach called InstanceGM, that combines discriminative and generative models.
The main contributions of InstanceGM are: i) the use of the continuous
Bernoulli distribution to train the generative model, offering significant
training advantages, and ii) the exploration of a state-of-the-art noisy-label
discriminative classifier to generate clean labels from instance-dependent
noisy-label samples. InstanceGM is competitive with current noisy-label
learning approaches, particularly in IDN benchmarks using synthetic and
real-world datasets, where our method shows better accuracy than the
competitors in most experiments.","The latest developments in deep neural networks (DNNs) have shown outstanding results in a variety of applications ranging from computer vision [31] to nat ural language processing [48] and medical image anal ysis [47]. Such success is strongly reliant on high capacity models, which in turn, require a massive amount of correctlyannotated data for training [34, 67]. Anno tating a large amount of data is, however, arduous, costly and timeconsuming, and therefore is often done via crowd sourcing [56] that generally produces lowquality annota *arpit.garg@aiml.teamtions. Although that brings down the cost and scales up the process, the tradeoff is the mislabelling of the data, result ing in a deterioration of deep models‚Äô performance [3, 35] due to the memorisation effect [2,35,44,70]. This has, there fore, motivated the research of novel learning algorithms to tackle the label noise problem where data might have been mislabelled. Early work in label noise [17] was carried out under the assumption that label noise was instanceindependent (IIN), i.e., mislabelling occurred regardless of the informa tion about the visual classes present in images. In IIN, we generally have a transition matrix that contains a pre deÔ¨Åned probability of Ô¨Çipping between pairs of labels (e.g., any image showing a cathas a high priori probability of being mislabelled as a dogand low a priori probability of being mislabelled as a car). This type of noise can also be divided into two subtypes: symmetric , where a true label is Ô¨Çipped to another label with equal probability across all classes, and asymmetric , where a true label is more likely to be mislabeled into one of some particular classes [17]. Nev ertheless, the IIN assumption is impractical for many real world datasets because we can intuitively argue that misla bellings mostly occur because of insufÔ¨Åcient or ambiguous information about the visual classes present in images. As a result, recent studies have gradually shifted their focus toward the more realistic scenario of instancedependent noise (IDN), where label noise depends on both the true class label and the image information [62]. Many methods have been introduced to handle not only IIN, but also IDN problems. Those include, but are not limited to, sample selection [12, 27, 33, 61, 72] that detects clean and noisy labels and applies semisupervised learn ing methods on the processed data, robust losses [1, 38, 46] that can work well with either clean or noisy labels, and probabilistic approaches [66] that model the data genera tion process, including how a noisy label is created. De spite some successes, most methods are often demonstrated in IIN settings with simulated symmetric and asymmetric noise. However, their performance is degraded when evalarXiv:2209.00906v1  [cs.CV]  2 Sep 2022uated on IDN problems, which include realworld and syn thetic datasets. Although there are a few studies focusing on the IDN setting [10, 26, 62, 66, 74], their relatively inac curate classiÔ¨Åcation results suggest that the algorithms can be improved further. In this paper, we propose a new method to tackle the IDN problem, called InstanceGM. Our method is designed based on a graphical model that considers the clean label Yas a latent variable and introduces another latent variable Zrepresenting the image feature to model the generation of a label noise ^Yand an image X. InstanceGM integrates generative and discriminate models, where the generative model is based on a variational autoencoder (V AE) [28], except that we replace the conventional mean squared error (MSE) when modelling the likelihood of reconstructed im ages by a continuous Bernoulli distribution [40] that facil itates the training process since it avoids tuning additional hyperparameters. For the discriminative model, to mitigate the problem of only using clean label data during the train ing process, which is a common issue present in the similar graphical model methods [66], we rely on DivideMix [33] that uses both clean and noisylabel data for training by exploring semisupervised learning via MixMatch [5]. Di videMix is shown to be a reasonably effective discrimina tive classiÔ¨Åer for our InstanceGM. In summary, the main contributions of the proposed method are: ‚Ä¢ InstanceGM follows a graphical modelling approach to generate both the image Xand its noisy label ^Y with the true label Yand image feature Zas latent variables. The modelling is associated with the contin uous Bernoulli distribution to model the generation of instanceXto facilitate the training, avoiding tuning of additional hyperparameters (see Remark 3). ‚Ä¢ For the discriminative classiÔ¨Åer of InstanceGM, we re place the commonly used coteaching, which is a dual model that relies only on training samples classiÔ¨Åed as clean, with DivideMix [33] that uses all training sam ples classiÔ¨Åed as clean and noisy. ‚Ä¢ InstanceGM shows stateoftheart results on a va riety of IDN benchmarks, including simulated and realworld datasets, such as CIFAR10 and CI FAR100 [30], Red MiniImageNet from Controlled Noisy Web Labels (CNWL) [65], ANIMAL10N [53] and CLOTHING1M [64]. 2. Related work "
276,Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution.txt,"Crowd sourcing has become a widely adopted scheme to collect ground truth
labels. However, it is a well-known problem that these labels can be very
noisy. In this paper, we demonstrate how to learn a deep convolutional neural
network (DCNN) from noisy labels, using facial expression recognition as an
example. More specifically, we have 10 taggers to label each input image, and
compare four different approaches to utilizing the multiple labels: majority
voting, multi-label learning, probabilistic label drawing, and cross-entropy
loss. We show that the traditional majority voting scheme does not perform as
well as the last two approaches that fully leverage the label distribution. An
enhanced FER+ data set with multiple labels for each face image will also be
shared with the research community.","Understanding the unspoken words from facial and body cues is a fundamental human trait, and such aptitude is vi tal in our daily communications and social interactions. In research communities such as human computer interaction (HCI), neuroscience and computer vision, scientists have conducted extensive research to understand human emo tions. Such studies would allow creating computers that can understand human emotions as well as ourselves, and lead to seamless interactions between human and computers. Among many inputs that can be used to derive emotions, facial expression is by far the most popular. One of the pi oneer works by Paul Ekman [10] identied 6 emotions that are universal across dierent cultures. Later, Ekman [11] developed the Facial Action Coding System (FACS), which became the standard scheme for facial expression research. Facial expression analysis can thus be conducted by analyz ing facial action units for each of the facial parts (eyes, nose, mouth corners, etc.), and map them into FACS codes [30]. Unfortunately, FACS coding requires professionally trained coders to annotate, and there are very few existing data sets that are available for learning FACS based facial expressions, in particular for unconstrained realworld images.With the latest advances in machine learning, it is more and more popular to recognize facial expressions directly from input images. Such appearancebased approaches have the advantage that the ground truth labels may be abun dantly obtained through crowdsourcing platforms [1]. The cost of tagging a holistic facial emotion is often on the or der of 12 US cents, which is orders of magnitude cheaper than FACS coding. On the other hand, crowdsourced la bels are usually much noisier than FACS codes annotated by specially trained coders. This can be attributed to two main reasons. First, emotions are very subjective, and it is very common that two people have diametrically dier ent opinions on the same face image. Second, the workers in crowdsourcing platforms are paid very low, and their incen tive is more on getting more work done rather than ensuring the tagging quality. Consequently, crowdsourced labels on emotions exhibit only 65 5% accuracy, as reported for the original FER data set [12]. In this paper, we adopt the latest deep convolutional neu ral networks (DCNN) architecture, and evaluate the eec tiveness of four dierent schemes to train emotion recog nition on crowdsourced labels. In order to overcome the noisy label issue, we asked 10 crowd taggers to relabel each image in the FER data set, resulting in a new data set named FER+[2]. Then, we change the cost function of the DCNN based on dierent schemes using the distribution of tags: majority voting, multilabel learning, probabilistic label drawing, and crossentropy loss. We compare the per formance of the trained classiers and found the last two schemes to be the most eective to train emotion recogni tion classiers based on noisy labels. The rest of the paper is organized as follows. Related works are discussed in Section 2 and a description of the FER+ data set is introduced in Section 3. Then, the four schemes for DCNN training are presented in Section 4 while experimental results and conclusions are given in Section 5 and 6, respectively. 2. RELATED WORK "
43,Towards Understanding Deep Learning from Noisy Labels with Small-Loss Criterion.txt,"Deep neural networks need large amounts of labeled data to achieve good
performance. In real-world applications, labels are usually collected from
non-experts such as crowdsourcing to save cost and thus are noisy. In the past
few years, deep learning methods for dealing with noisy labels have been
developed, many of which are based on the small-loss criterion. However, there
are few theoretical analyses to explain why these methods could learn well from
noisy labels. In this paper, we theoretically explain why the widely-used
small-loss criterion works. Based on the explanation, we reformalize the
vanilla small-loss criterion to better tackle noisy labels. The experimental
results verify our theoretical explanation and also demonstrate the
effectiveness of the reformalization.","Deep neural networks (DNNs) have achieved great success in many realworld applications, but rely on largescale data with accurate labels [Deng et al. , 2009 ]. Obtaining large scale accurate labels is expensive while the alternative meth ods such as crowdsourcing [Raykar et al. , 2010 ]and web queries [Jiang et al. , 2020 ]can easily provide extensive la beled data, but unavoidably incur noisy labels. The perfor mance of deep neural networks may be severely hurt if these noisy labels are blindly used [Zhang et al. , 2017a ], and thus how to learn with noisy labels has become a hot topic. In the past few years, many deep learning methods for tack ling noisy labels have been developed. Some methods try to exploit noiserobust loss functions, e.g., MAE loss [Ghosh et al., 2017 ], TruncatedLqloss[Zhang and Sabuncu, 2018 ]and the informationtheoretic loss [Xuet al. , 2019 ]. These meth ods do not consider the speciÔ¨Åc information about label noise, and thus usually have limited utility in realworld applica tions. Some methods use the transition matrix to model label noise and construct an unbiased loss term to alleviate the in Ô¨Çuence of noisy labels [Sukhbaatar et al. , 2014; Patrini et al. , 2017; Goldberger and BenReuven, 2017; Han et al. , 2018a; Hendrycks et al. , 2018 ]. However, the performance of these Corresponding author.methods is usually suboptimal due to the difÔ¨Åculty of ac curately estimating the noise transition matrix. Some other methods try to correct the noisy labels [Maet al. , 2018; Arazo et al. , 2019; Yi and Wu, 2019 ], but may suffer from the false correction. Sometimes, although correcting the noisy labels might be challenging especially for the classiÔ¨Åcation task with a large number of classes, the detection of noisy labels is relatively easy. Along this direction, the sample selection strategy with the widelyused smallloss criterion has been proposed, i.e., treating the examples with small loss as the clean data and using them in the training pro cess. Although many methods based on the smallloss crite rion have achieved prominent performance in practice [Han et al. , 2018b; Yu et al. , 2019; Shen and Sanghavi, 2019; Song et al. , 2019; Wei et al. , 2020 ], the theoretical expla nation about when and why it works is rarely studied. When there are noisy labels in the data, it is somehow overly optimistic to expect that deep neural networks could achieve good performance without any assumption on la bel noise. Thus, most of previous studies potentially make assumptions on label noise, e.g., the condition that correct labels are not overwhelmed by the false ones [Sukhbaatar et al. , 2014; Han et al. , 2018b ]. Some methods focus on the classconditional noise setting [Natarajan et al. , 2013; Patrini et al. , 2017 ],i.e., the label noise classconditionally depends only on the latent true class, but not on the fea ture. This assumption is an approximation of realworld label noise and can encode the similarity information be tween classes. Based on this, three representative types of label noise have been considered, i.e., uniform label noise [Hendrycks et al. , 2018 ], pairwise label noise [Han et al., 2018b ]and structured label noise [Patrini et al. , 2017; Zhang and Sabuncu, 2018 ]. For these types of label noise, it is usually assumed that the diagonallydominant condition holds, and many methods could achieve good performance with this condition [Rolnick et al. , 2017; Wei et al. , 2020 ]. Unfortunately, there are few theoretical analyses to explain why this diagonallydominant condition is necessary for good performance. In this work, we Ô¨Årst reveal the theoretical con dition under which learning methods could achieve good per formance with noisy labels, which exactly matches the con dition assumed in previous methods, and then theoretically explain when and why the smallloss criterion works. Based on the explanation, we reformalize the vanilla smallloss criarXiv:2106.09291v1  [cs.LG]  17 Jun 2021terion to better tackle noisy labels. The experimental results on synthetic and realworld datasets verify our theoretical re sults and demonstrate the effectiveness of the reformalization of smallloss criterion. 2 Related Work "
592,A Gradient Mapping Guided Explainable Deep Neural Network for Extracapsular Extension Identification in 3D Head and Neck Cancer Computed Tomography Images.txt,"Diagnosis and treatment management for head and neck squamous cell carcinoma
(HNSCC) is guided by routine diagnostic head and neck computed tomography (CT)
scans to identify tumor and lymph node features. Extracapsular extension (ECE)
is a strong predictor of patients' survival outcomes with HNSCC. It is
essential to detect the occurrence of ECE as it changes staging and management
for the patients. Current clinical ECE detection relies on visual
identification and pathologic confirmation conducted by radiologists. Machine
learning (ML)-based ECE diagnosis has shown high potential in the recent years.
However, manual annotation of lymph node region is a required data
preprocessing step in most of the current ML-based ECE diagnosis studies. In
addition, this manual annotation process is time-consuming, labor-intensive,
and error-prone. Therefore, in this paper, we propose a Gradient Mapping Guided
Explainable Network (GMGENet) framework to perform ECE identification
automatically without requiring annotated lymph node region information. The
gradient-weighted class activation mapping (Grad-CAM) technique is proposed to
guide the deep learning algorithm to focus on the regions that are highly
related to ECE. Informative volumes of interest (VOIs) are extracted without
labeled lymph node region information. In evaluation, the proposed method is
well-trained and tested using cross validation, achieving test accuracy and AUC
of 90.2% and 91.1%, respectively. The presence or absence of ECE has been
analyzed and correlated with gold standard histopathological findings.","Head and neck squamous cell carcinoma (HNSCC) is one of the most com mon cancers worldwide, diagnosed in more than 550,000 patients and causing over 300,000 deaths annually (Jemal et al., 2013). For HNSCC diagnosis and treatment plan selection, CT scans are collected and analyzed to identify clin ical tumor and lymph node features (Kann et al., 2018a). In spite of modern imaging techniques, there are certain radiographic features that remain di cult to detect by clinicians, especially the presence of lymph node extracapsular extension (ECE). ECE occurs when metastatic tumor cells within the lymph node break through the nodal capsule into surrounding tissues. It is crucial to identify whether ECE occurs or not for HNSCC patient treatment management. However, current detection in practice mainly relies on the visual identication, including lymph node annotation and pathologic conrmation, which can be extremely laborintense and timeconsuming. Human errors are also inevitable. Therefore, we perform ECE identication automatically using advanced 3D deep learning technique with explainable gradientbased approach, and at the same time, not requiring manual lymph node annotation. Deep neural networks (DNNs) have demonstrated a great success in many image recognition tasks. Nevertheless, applying neural networks models on high resolution CT scans is very computational extensive. The dimension of CT scans for head and neck cancer (HNC) patients is usually 512 512 with more than 2a hundred slices. To handle such high resolution input, existing models use a combination of downsampling, dividing, and/or coarsetone schemes (Hou et al., 2016; Chlebus et al., 2018; Vorontsov et al., 2018). In this research, we perform a twostep learning scheme. The rst step is volumes of interest (VOIs) selfextraction with DNNs to provide explainable insights. The second step will train a classier with the extracted explainable VOIs. In this way, accurate ECE related lymph node regions are not demanded, and segmentation eort could be saved. Even though deep learning models have achieved impressive prediction ac curacies, their inner nonlinear structure makes them highly nontransparent to be explained by human that what information in the input data makes them actually arrive at their decisions (Samek et al., 2017). Gradientweighted class activation mapping (GradCAM) is one of the methods that look into the deep learning black boxes. The gradients of the target concept is used,  owing into the nal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept (Selvaraju et al., 2017). In this paper, we propose a novel deep learning framework, Gradient Map ping Guided Explainable Network (GMGENet), to detect ECE from 3D head and neck CT scans without requiring annotated lymph node region information. The task is to classify patients' CT scans with ECE positive/negative categories and point out the causes in the 3D CT scans. Dierent from the typically ECE detection algorithms, where the annotation of lymph node regions are required, the proposed GMGENet model includes a selfextractor, which can be trained separately to extract the explainable VOIs that are related to the ECE positive class. The selfextractor is designed based on GradCAM. Then, a DenseNet classier is trained based on the VOIs sug gested by the selfextractor and tested on an independent test set. Ground truth label has been provided for validating the selfextraction performance, and model explainability analysis are designed in the experiments. The proposed network is trained by the inputs that are highly related to 3ECE information. Therefore, extracted VOIs will contain explainable features for further classication task. In summary, the contributions of this research are highlighted as follows: (1) We propose a novel deep learning framework GMGENet with twostep learn ing scheme for ECE identication in head and neck cancer. The proposed ar chitecture includes a selfextractor, which can extract the explainable VOIs that are related to the class ECE positive. The proposed GMGENet has achieved better performance than conventional networks. (2) We train and test our proposed model on a realworld collected dataset. Lymph node label is not required in the classier, which will promote the implementation as well as improve the eciency of articial intelligence assist ECE detection. (3) To further illustrate ECE identication performance, we apply a gradient based mapping approach to generate 3D ECE probability heatmaps to pro vide visualization during the training. This technique will indicate the im portant regions related to ECE and increase the model explainability. The structure of this paper is organized as following. Section 2 provides an overview of related algorithms and applications in head and neck ECE identi cation. Section 3 describes the details of our proposed explainable GMGENet model for ECE classication. Data preparation and experimental results are illustrated in Section 4. The research ndings and future work are concluded in Section 5. 2. Related Works "
327,ADMoE: Anomaly Detection with Mixture-of-Experts from Noisy Labels.txt,"Existing works on anomaly detection (AD) rely on clean labels from human
annotators that are expensive to acquire in practice. In this work, we propose
a method to leverage weak/noisy labels (e.g., risk scores generated by machine
rules for detecting malware) that are cheaper to obtain for anomaly detection.
Specifically, we propose ADMoE, the first framework for anomaly detection
algorithms to learn from noisy labels. In a nutshell, ADMoE leverages
mixture-of-experts (MoE) architecture to encourage specialized and scalable
learning from multiple noisy sources. It captures the similarities among noisy
labels by sharing most model parameters, while encouraging specialization by
building ""expert"" sub-networks. To further juice out the signals from noisy
labels, ADMoE uses them as input features to facilitate expert learning.
Extensive results on eight datasets (including a proprietary enterprise
security dataset) demonstrate the effectiveness of ADMoE, where it brings up to
34% performance improvement over not using it. Also, it outperforms a total of
13 leading baselines with equivalent network parameters and FLOPS. Notably,
ADMoE is model-agnostic to enable any neural network-based detection methods to
handle noisy labels, where we showcase its results on both multiple-layer
perceptron (MLP) and the leading AD method DeepSAD.","Anomaly detection (AD), also known as outlier detection, is a crucial learning task with many realworld applications, in cluding malware detection (Nguyen et al. 2019), antimoney laundering (Lee et al. 2020), raredisease detection (Li et al. 2018) and so on. Although there are numerous detection al gorithms (Aggarwal 2013; Pang et al. 2021; Zhao, Rossi, and Akoglu 2021; Liu et al. 2022), existing AD methods as sume the availability of (partial) labels that are clean (i.e. without noise), and cannot learn from weak/noisy labels1. Simply treating noisy labels as (pseudo) clean labels leads to biased and degraded models (Song et al. 2022). Over the years, researchers have developed algorithms for classiÔ¨Åca tion and regression tasks to learn from noisy sources (Ro drigues and Pereira 2018; Guan et al. 2018; Wei et al. 2022), which has shown great success. However, these methods are *The project is primarily done at Microsoft Research. Copyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved. 1We use the terms noisy andweak interchangeably. /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000003/uni00000054/uni00000058/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni0000004b/uni0000004c/uni0000004a/uni0000004b/uni00000048/uni00000055/uni00000012/uni00000046/uni0000004f/uni00000048/uni00000044/uni00000051/uni00000048/uni00000055/uni00000003/uni00000057/uni00000052/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000055/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000000c/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni0000004b/uni0000004c/uni0000004a/uni0000004b/uni00000048/uni00000055/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000045/uni00000048/uni00000057/uni00000057/uni00000048/uni00000055/uni0000000c /uni0000003b/uni0000002a/uni00000025/uni00000032/uni00000027 /uni00000033/uni00000055/uni00000048/uni00000031/uni00000048/uni00000057 /uni00000027/uni00000048/uni00000059/uni00000031/uni00000048/uni00000057 /uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000024/uni00000027/uni0000002f/uni0000002a/uni00000025 /uni00000030/uni0000002f/uni00000033 /uni00000024/uni00000027/uni00000030/uni00000052/uni00000028/uni0000000e/uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000024/uni00000027 /uni00000024/uni00000027/uni00000030/uni00000052/uni00000028/uni0000000e/uni00000030/uni0000002f/uni00000033(a)Comparison with leading AD methods /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000003/uni00000054/uni00000058/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni0000004b/uni0000004c/uni0000004a/uni0000004b/uni00000048/uni00000055/uni00000012/uni00000046/uni0000004f/uni00000048/uni00000044/uni00000051/uni00000048/uni00000055/uni00000003/uni00000057/uni00000052/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000055/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000000c/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni0000004b/uni0000004c/uni0000004a/uni0000004b/uni00000048/uni00000055/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000045/uni00000048/uni00000057/uni00000057/uni00000048/uni00000055/uni0000000c /uni00000036/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c /uni00000030/uni00000044/uni0000004d/uni00000052/uni00000055/uni00000039/uni00000052/uni00000057/uni00000048 /uni0000002b/uni00000028/uni00000042/uni00000024 /uni0000002b/uni00000028/uni00000042/uni00000030/uni00000026/uni00000055/uni00000052/uni0000005a/uni00000047/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000038/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000031/uni00000048/uni00000057 /uni00000024/uni00000027/uni00000030/uni00000052/uni00000028/uni00000010/uni00000030/uni0000002f/uni00000033 (b)Comp. w/ noisy learning methods Figure 1: Performance (ROCAUC) comparison on Yelp (see results on all datasets in ¬ß4.2 and 4.3), where ADMoE outperforms two groups of baselines: (a) SOTA AD meth ods; (b) leading classiÔ¨Åcation methods for learning from multiple noisy sources. ADMoE enhanced DeepSAD and MLP are denoted as ADandAM. not tailored for AD with extreme data imbalance, and exist ing AD methods cannot learn from (multiple) noisy sources. Why is it important to leverage noisy labels in AD appli cations? Taking malware detection as an example, it is im possible to get a large number of clean labels due to the data sensitivity and the cost of annotation. However, often there exists a large number of weak/noisy historical security rules designed for detecting malware from different perspectives, e.g., unauthorized network access and suspicious Ô¨Åle move ment, which have not been used in AD yet. Though not as perfect as human annotations, they are valuable as they en code prior knowledge from past detection experiences. Also, although each noisy source may be insufÔ¨Åcient for difÔ¨Åcult AD tasks, learning them jointly may build competitive mod els as they tend to complement each other. In this work, we propose ADMoE, (to our knowledge) theÔ¨Årst weaklysupervised approach for enabling anomaly detection algorithms to learn from multiple sets of noisy labels . In a nutshell, ADMoE enhances existing neural networkbased AD algorithms by Mixtureofexperts (MoE) network(s) (Jacobs et al. 1991; Shazeer et al. 2017), which has a learnable gating function to activate different sub networks (experts) based on the incoming samples andtheir noisy labels . In this way, the proposed ADMoE can jointly learn from multiple sets of noisy labels with the majority of parameters shared, while providing specialization and scalarXiv:2208.11290v2  [cs.LG]  22 Nov 2022ability via experts. Unlike existing noisy label learning ap proaches, ADMoE does not require explicit mapping from noisy labels to network parameters, providing better scala bility and Ô¨Çexibility. To encourage ADMoE to develop spe cialization based on the noisy sources, we use noisy labels as (part of the) input features with learnable embeddings to make the gating function aware of them. Key Results . Fig. 1 shows that a multiple layer perception (MLP) (Rosenblatt 1958) enhanced by ADMoE can largely outperform both (1a) leading AD algorithms as well as (1b) noisylabel learning methods for classiÔ¨Åcation. Note AD MoE is not strictly another detection algorithm, but a gen eral framework to empower any neuralbased AD methods to leverage multiple sets of weak labels. ¬ß4 shows extensive results on more datasets, and the improvement in enhancing more complex DeepSAD (Ruff et al. 2019) with ADMoE. In summary, the key contributions of this work include: ‚Ä¢Problem formulation, baselines, and datasets . We for mally deÔ¨Åne the crucial problem of using multiple sets of noisy l abels for AD (MNLAD), and release the Ô¨Årst batch of baselines and datasets for future research2. ‚Ä¢The Ô¨Årst AD framework for learning from multiple noisy sources . The proposed ADMoE is a novel method with Mixtureofexperts (MoE) architecture to achieve specialized and scalable learning for MNLAD. ‚Ä¢Modelagnostic design . ADMoE enhances any neural networkbased AD methods, and we show its effective ness on MLP and stateoftheart (SOTA) DeepSAD. ‚Ä¢Effectiveness and realworld deployment . We demon strate ADMoE‚Äôs SOTA performance on seven benchmark datasets and a proprietary enterprise security application, in comparison with two groups of leading baselines (13 in total). It brings on average 14% and up to 34% im provement over not using it, with the equivalent number of learnable parameters and FLOPs as baselines. 2 Related Work "
561,Confidence-based Reliable Learning under Dual Noises.txt,"Deep neural networks (DNNs) have achieved remarkable success in a variety of
computer vision tasks, where massive labeled images are routinely required for
model optimization. Yet, the data collected from the open world are unavoidably
polluted by noise, which may significantly undermine the efficacy of the
learned models. Various attempts have been made to reliably train DNNs under
data noise, but they separately account for either the noise existing in the
labels or that existing in the images. A naive combination of the two lines of
works would suffer from the limitations in both sides, and miss the
opportunities to handle the two kinds of noise in parallel. This work provides
a first, unified framework for reliable learning under the joint (image,
label)-noise. Technically, we develop a confidence-based sample filter to
progressively filter out noisy data without the need of pre-specifying noise
ratio. Then, we penalize the model uncertainty of the detected noisy data
instead of letting the model continue over-fitting the misleading information
in them. Experimental results on various challenging synthetic and real-world
noisy datasets verify that the proposed method can outperform competing
baselines in the aspect of classification performance.","Deep Neural Networks (DNNs) have obtained great success in a wide spectrum of computer vision applications [ 26,41,19,18], especially when a large volume of carefullyannotated lowdistortion images are available. However, the images collected from the wild in realworld tasks are unavoidably polluted by noise in the images themselves (e.g., image corruptions [ 20] and background noise [ 43]) or the associated labels [ 36], termed as image noise ( xnoise ) and label noise ( ynoise ) respectively. Previous investigations show that the DNNs naively trained under ynoise [2,52] orxnoise [11,53] suffer from detrimental overÔ¨Åtting issues, thus exhibit poor generalization performance and serious overconÔ¨Ådence. There has been a large body of attempts towards dealing with data noise, but they mainly focus on a limited setting, where noise only exists in either the label (i.e., noisy labels) [ 36,1,31,8] or the image [ 13,27,50]. It is nontrivial to extend them to exhaustively deal with dual noises (i.e., the joint (x,y)noise ). Moreover, the techniques for handling xnoise suffer from nontrivial limitations. For example, most image denoising methods work on wellpreserved image texture [ 12], thus may easily fail when facing images that are globally blurred (see Fig. 5 in Appendix); alternative image SuperResolution (SR) solutions are usually computationally expensive [ 46]. These issues raise the requirement of a uniÔ¨Åed approach for reliable learning under dual noises. The corresponding author. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2302.05098v1  [cs.CV]  10 Feb 2023Compared to deterministic DNNs, uncertaintybased deep models (e.g., Bayesian Neural Networks (BNNs) [ 3] and deep ensemble [25]) reason about the uncertainty and hence have the potential to mitigate the overÔ¨Åtting to noisy data. Empowered by this insight, we Ô¨Årst perform a systematical investigation on leveraging uncertaintybased deep models to cope with dual noises. We observe that, despite with less overÔ¨Åtting, the uncertaintybased deep models may still suffer from the bias in the noisy data and yield compromising results. To further ameliorate the pathologies induced by data noise and achieve reliable learning, we propose a novel workÔ¨Çow for the learning of uncertaintybased deep models under dual noises. Firstly, inspired by the recent success of using predictive conÔ¨Ådence to detect the outofdistribution data [ 21], we propose to detect both the noisy images and the noisy labels by the predictive conÔ¨Ådence produced by uncertaintybased deep models. Concretely, we use the predictive probability corresponding to the label (i.e., label conÔ¨Ådence) to Ô¨Ålter out the samples with ynoise , and use the maximum conÔ¨Ådence to Ô¨Ålter out the samples with xnoise . After doing so, we propose to penalize the uncertainty [ 23] of the detected noisy data to make use of the valuable information inside the images without relying on the misleading supervisory information. Given the merits of deep ensemble [25] for providing calibrated conÔ¨Ådence and uncertainty under distribution shift revealed by related works [ 37] and our studies, we opt to place our workÔ¨Çow on deep ensemble to establish a strong, scalable, and easytoimplement baseline for learning under dual noises. Of note that the developed strategies are readily applicable to other uncertaintybased deep models like BNNs. We perform extensive empirical studies to evidence the effectiveness of the proposed method. We Ô¨Årst show that the proposed method signiÔ¨Åcantly outperforms competitive baselines on CIFAR100 and TinyImageNet datasets with different levels of synthetic (x,y)noise . We then verify the superiority of the proposed method on the challenging WebVision benchmark [ 28] which contains extensive real world noise. We further provide insightful ablation studies to show the robustness of our approach to multiple hyperparameters. 2 Related Work "
163,Contextual Modulation for Relation-Level Metaphor Identification.txt,"Identifying metaphors in text is very challenging and requires comprehending
the underlying comparison. The automation of this cognitive process has gained
wide attention lately. However, the majority of existing approaches concentrate
on word-level identification by treating the task as either single-word
classification or sequential labelling without explicitly modelling the
interaction between the metaphor components. On the other hand, while existing
relation-level approaches implicitly model this interaction, they ignore the
context where the metaphor occurs. In this work, we address these limitations
by introducing a novel architecture for identifying relation-level metaphoric
expressions of certain grammatical relations based on contextual modulation. In
a methodology inspired by works in visual reasoning, our approach is based on
conditioning the neural network computation on the deep contextualised features
of the candidate expressions using feature-wise linear modulation. We
demonstrate that the proposed architecture achieves state-of-the-art results on
benchmark datasets. The proposed methodology is generic and could be applied to
other textual classification problems that benefit from contextual interaction.","Despite its fuzziness, metaphor is a fundamental feature of language that deÔ¨Ånes the relation be tween how we understand things and how we ex press them (Cameron and Low, 1999). A metaphor is a Ô¨Ågurative device containing an implied map ping between two conceptual domains. These do mains are represented by its two main components, namely the tenor (target domain) and the vehicle (source domain) (End, 1986). According to the conceptual metaphor theory (CMT) of Lakoff and Johnson (1980), which we adopt in this work, aconcept such as ‚Äúliquids‚Äù (source domain/vehicle) can be borrowed to express another such as ‚Äúemo tions‚Äù (target domain/tenor) by exploiting single or common properties. Therefore, the conceptual metaphor ‚ÄúEmotions are Liquids‚Äù can be mani fested through the use of linguistic metaphors such as‚Äúpure love‚Äù ,‚Äústir excitement‚Äù and‚Äúcontain your anger‚Äù . The interaction between the target and the source concepts of the expression is impor tant to fully comprehend its metaphoricity. Over the last couple of years, there has been an increasing interest towards metaphor process ing and its applications, either as part of natural language processing (NLP) tasks such as machine translation (Koglin and Cunha, 2019), text sim pliÔ¨Åcation (Wolska and Clausen, 2017; Clausen and Nastase, 2019) and sentiment analysis (Ren toumi et al., 2012) or in more general discourse analysis use cases such as in analysing political discourse (CharterisBlack, 2011), Ô¨Ånancial report ing (Ho and Cheng, 2016) and health communica tion (Semino et al., 2018). Metaphor processing comprises several tasks including identiÔ¨Åcation, interpretation and cross domain mappings. Metaphor identiÔ¨Åcation is the most studied among these tasks. It is concerned with detecting the metaphoric words or expressions in the input text and could be done either on the sentence, relation or word levels. The difference be tween these levels of processing is extensively stud ied in (Zayed et al., 2020). Identifying metaphors on the wordlevel could be treated as either se quence labelling by deciding the metaphoricity of each word in a sentence given the context or single word classiÔ¨Åcation by deciding the metaphoricity of a targeted word. On the other hand, relation level identiÔ¨Åcation looks at speciÔ¨Åc grammatical relations such as the dobj oramod dependencies and checks the metaphoricity of the verb or the adjective given its association with the noun. InarXiv:2010.05633v1  [cs.CL]  12 Oct 2020relationlevel identiÔ¨Åcation, both the source and target domain words (the tenor and vehicle) are classiÔ¨Åed either as a metaphoric or literal expres sion, whereas in wordlevel identiÔ¨Åcation only the source domain words (vehicle) are labelled. These levels of analysis (paradigms) are already estab lished in literature and adopted by previous re search in this area as will be explained in Sec tion 2. The majority of existing approaches, as well as the available datasets, pertaining to metaphor processing focus on the metaphorical usage of verbs and adjectives either on the word or relation levels. This is because these syntactic types ex hibit metaphoricity more frequently than others ac cording to corpusbased analysis (Cameron, 2003; Shutova and Teufel, 2010). Although the main focus of both the relation level and wordlevel metaphor identiÔ¨Åcation is dis cerning the metaphoricity of the vehicle (source do main words), the interaction between the metaphor components is less explicit in wordlevel analysis either when treating the task as sequence labelling or singleword classiÔ¨Åcation. Relationlevel analy sis could be viewed as a deeper level analysis that captures information that is not captured on the wordlevel through modelling the inÔ¨Çuence of the tenor (e.g.noun) on the vehicle (e.g. verb/adjective). There will be reasons that some downstream tasks would prefer to have such information (i.e. ex plicitly marked relations), among these tasks are metaphor interpretation and crossdomain map pings. Moreover, employing the wider context around the expression is essential to improve the identiÔ¨Åcation process. This work focuses on relationlevel metaphor identiÔ¨Åcation represented by verbnoun and adjectivenoun grammar relations. We propose a novel approach for contextbased textual classiÔ¨Å cation that utilises afÔ¨Åne transformations. In order to integrate the interaction of the metaphor compo nents in the identiÔ¨Åcation process, we utilise afÔ¨Åne transformation in a novel way to condition the neu ral network computation on the contextualised fea tures of the given expression. The idea of afÔ¨Åne transformations has been used in NLPrelated tasks such as visual questionanswering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019).Inspired by the works on visual reasoning, we use the candidate expression of certain grammat ical relations, represented by deep contextualised features, as an auxiliary input to modulate our com putational model. AfÔ¨Åne transformations can be utilised to process one source of information in the context of another. In our case, we want to inte grate: 1) the deep contextualisedfeatures of the candidate expression (represented by ELMo sen tence embeddings) with 2) the syntactic/semantic features of a given sentence. Based on this task, afÔ¨Åne transformations have a similar role to atten tion but with more parameters, which allows the model to better exploit context. Therefore, it could be regarded as a form of a more sophisticated at tention. Whereas the current ‚Äústraightforward‚Äù at tention models are overly simplistic, our model pri oritises the contextual information of the candidate to discern its metaphoricity in a given sentence. Our proposed model consists of an afÔ¨Åne trans form coefÔ¨Åcients generator that captures the mean ing of the candidate to be classiÔ¨Åed, and a neural network that encodes the full text in which the can didate needs to be classiÔ¨Åed. We demonstrate that our model signiÔ¨Åcantly outperforms the stateof theart approaches on existing relationlevel bench mark datasets. The unique characteristics of tweets and the availability of Twitter data motivated us to identify metaphors in such content. Therefore, we evaluate our proposed model on a newly introduced dataset of tweets (Zayed et al., 2019) annotated for relationlevel metaphors. 2 Related Work "
206,Noisy Heuristics NAS: A Network Morphism based Neural Architecture Search using Heuristics.txt,"Network Morphism based Neural Architecture Search (NAS) is one of the most
efficient methods, however, knowing where and when to add new neurons or remove
dis-functional ones is generally left to black-box Reinforcement Learning
models. In this paper, we present a new Network Morphism based NAS called Noisy
Heuristics NAS which uses heuristics learned from manually developing neural
network models and inspired by biological neuronal dynamics. Firstly, we add
new neurons randomly and prune away some to select only the best fitting
neurons. Secondly, we control the number of layers in the network using the
relationship of hidden units to the number of input-output connections. Our
method can increase or decrease the capacity or non-linearity of models online
which is specified with a few meta-parameters by the user. Our method
generalizes both on toy datasets and on real-world data sets such as MNIST,
CIFAR-10, and CIFAR-100. The performance is comparable to the hand-engineered
architecture ResNet-18 with the similar parameters.","Neural Architecture Search (NAS) is the process of search ing the Architecture of Neural Networks by leveraging the computation rather than doing manually. However, NAS has still not been able to come to the mainstream due to large computational costs and availability of more efÔ¨Åcient alternatives such as transfer learning (Zhuang et al., 2020) or reusing architectures. Research has been done on using Re inforcement Learning(RL) (Zoph & Le, 2016; Baker et al., We would like to acknowledge Google Cloud for computing credits.1NAAMII, Nepal2University College London, UK. Corre spondence to: Suman Sapkota <suman.sapkota@naamii.org.np>, Binod Bhattarai <b.bhattarai@ucl.ac.uk>. DyNN workshop at the 39thInternational Conference on Machine Learning , Baltimore, Maryland, USA, 2022. Copyright 2022 by the author(s).2016) and Genetic Algorithm(GA) (Desell, 2017) for gener ating architecture from a given search space, however, these methods have huge computational costs and produce large carbon footprints (Strubell et al., 2019). Gradientbased path selection methods such as DARTS (Liu et al., 2018) and PCDARTS (Xu et al., 2019) have made NAS more efÔ¨Åcient and accessible. However, it still involves training large parameter models and selecting only a subset for the Ô¨Ånal model. In a manual architecture search process, we start with a baseline model architecture. If the model has poor perfor mance, we test more and more nonlinear models and if the model overÔ¨Åts the dataset we test smaller models, throwing away older models in the process. The initial concern is whether nonlinear capacity can be increased or decreased in the same model while reusing the trained function. To our aid, Network Morphing based methods (Elsken et al., 2017; Lu et al., 2018; Dai et al., 2019; Evci et al., 2022) have been used widely to add neurons, which increase the nonlinearity and capacity of the model. However, Network Morphism based methods are generally paired up with Reinforcement Learning (RL) (Cai et al., 2018) or Bayesian Optimization (Jin et al., 2019), which decide the morphism operation to increase the network ca pacity. This type of solution makes the dynamic nature of neural network a difÔ¨Åcult to understand. To understand the dynamics and to simulate heuristics, we need easily control lable models for changing network capacity or the number of neurons or parameters. Although there are various works on using Network Mor phism for Neural Architecture Search, we Ô¨Ånd that the meth ods are partial, either only adding neurons (Jin et al., 2019; Cai et al., 2018) and layers or not pruning layers (Gordon et al., 2018) to reduce capacity. Furthermore, those methods that add and prune neurons use it on incremental (Dai et al., 2020) or continual (Zhang et al., 2020) learning settings. Our goal to search for architecture depending on the dynam ics requires additional components to change the structure (layers and neurons) of the network itself. This gap mo tivates us to create a Network Morphism based NAS that can change the number of layers and neurons dynamically during the training phase while keeping the search efÔ¨ÅcientarXiv:2207.04467v1  [cs.LG]  10 Jul 2022Noisy Heuristics Neural Architecture Search and simple for the user. We combine multiple ideas and heuristics for creating a framework of Noisy NAS to search for capacity. Ideas from pruning and dropout support our framework for noisy heuristicbased architecture search. When small neurons are pruned, they typically recover the same accu racy and loss (Molchanov et al., 2019) without recovering the function completely. Furthermore, noisy regularization methods like Dropout (Srivastava et al., 2014) and Drop Connect (Wan et al., 2013) suggest that Deep Networks can be trained to be robust to perturbations. We can infer that Neural Networks are robust to the noisy process of addition and pruning of neurons. We can use such a noisy process to try different additions and removals of neurons iteratively which can roughly change the architecture to the desired capacity. The method of adding many neurons and remov ing poorly performing ones could be used to search for the correct place to add new neurons. Furthermore, the dynamics of Biological Neural Networks (BNN) (Wan et al., 2019) suggests that there could exist Neural Networks with dynamically changing architecture in a single model. The dynamic nature of BNN is partly due to neurogenesis (Kumar et al., 2019), neuron and synaptic pruning (Fricker et al., 2018) and neuron migration. We aim to understand the internal workings of ArtiÔ¨Åcial Neural Networks(ANN) and apply dynamics from BNN to close the gap between them. We believe that Dynamic Neural Networks along with Spiking Neural Networks (Tavanaei et al., 2019) could model BNN even better. Our Contribution: Combining the growing and shrinking mechanisms, we are able to get any desired network capacity for the best Ô¨Åtting of the dataset. Such a method is depicted by a generalization curve as shown in Figure 1. We work on the same curve, but instead of trying different capacity mod els, we change the capacity of the existing models towards the best capacity. To this end, we propose a new method for Network Morphism based Neural Architecture search using heuristics. We simplify our search space using multiple heuristics to a manageable number of metaparameters. The major contributions of our work are listed below. 1.We introduce a new method to add new neurons and layers heuristically for Network Morphism based Neu ral Architecture Search. 2.We create a new type of architecture called Hierarchi cal Residual Network for the ease of changing non linearity and number of layers during Network Mor phism. 3.We combine neuron addition, pruning and Hierarchi cal Residual Network to change the capacity of the network noisily during training, which we call Noisy Heuristics NAS. 4.We show that our method is successful in getting perfor Capacity Error/Loss  Test Error  Train Error Decrease Capacity  ( Overfitting )Increase Capacity  ( Underfitting )  Optimal  Capacity Figure 1. Generalization Curve mance near handdesigned architectures like ResNet. 5.We release code for Network Morphism, Opti mizer reusing, Pruning and Noisy Heuristic NAS in the PyTorch (Paszke et al., 2019) framework. https://github.com/tsumansapkota/NoisyHeuristicsNAS 2. Methodology "
133,PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training.txt,"Road network and building footprint extraction is essential for many
applications such as updating maps, traffic regulations, city planning,
ride-hailing, disaster response \textit{etc}. Mapping road networks is
currently both expensive and labor-intensive. Recently, improvements in image
segmentation through the application of deep neural networks has shown
promising results in extracting road segments from large scale, high resolution
satellite imagery. However, significant challenges remain due to lack of enough
labeled training data needed to build models for industry grade applications.
In this paper, we propose a two-stage transfer learning technique to improve
robustness of semantic segmentation for satellite images that leverages noisy
pseudo ground truth masks obtained automatically (without human labor) from
crowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid
Pooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation
that uses focal loss, poly learning rate, and context module. We demonstrate
the strengths of our approach through evaluations done on three popular
datasets over two tasks, namely, road extraction and building foot-print
detection. Specifically, we obtain 78.19\% meanIoU on SpaceNet building
footprint dataset, 67.03\% and 77.11\% on the road topology metric on SpaceNet
and DeepGlobe road extraction dataset, respectively.","Maps are common heritage for humanity. Creating and updating maps is an important task with many potential applications in disaster recovery, geospatial search, urban planning, ridehailing industry, etc [ 9]. Today, map features such as roads, building foot prints, and points of interest are primarily created through manual techniques. We address the problem of extracting road networks and building footprints automatically from satellite imagery in this paper. Maps of roads and buildings are expensive to build and maintain. Although modern cartography involves using satellite and aerial imagery along with GPS traces, using these data to update maps involves having humans analyze the data in a time consuming process, and thus maps of rapidly growing cities (where infras tructure is constantly under construction) are still often inaccurate and incomplete outside the urban core. Current approaches to map inference via satellite images [ 10,39] involve extensive annotations of images that is both tedious and time consuming. Currently, we have two large public datasets containing satellite images of road networks with annotations: i) SpaceNet [ 39] and DeepGlobe [ 10]. SpaceNet challenge 3 road extraction [ 39] only provides 2779 training satellite images of Las Vegas, Paris, Shanghai,arXiv:2010.06932v1  [cs.CV]  14 Oct 2020   OpenStreetMap road geometry shapefile Rasterize road network Using Tilemill (GIS software) Our rasterization Using OpenCVFigure 1: Illustration of the rasterization process to con vert OpenStreetMap data into pseudo ground truth mask im age. Top row: rasterization using TileMill software. Bottom row: our rasterization using OpenCV from the correspond ing road network graph. Best viewed in color. Khartoum with image size of 1300√ó1300 with their road center line annotations. ii) DeepGlobe [ 10] has a total of 8570 images with 6226 training, 1243 validation and 1101 testing images with pixelbased annotations, where all pixels belonging to the road are labeled, instead of labeling only centerline. Although seemingly large, these datasets are still not enough to train a robust model for analyzing satellite imagery on a global scale due to challenges posed by spatial variations ‚Äì roads differ in their appearance due to regional terrain, urban vs rural divide, state of economy, e.g., developed vs. developing countries, see Figure 2) and temporal variations resulting in images captured during different seasons with varying lighting conditions, cloud cover, etc. In this paper, we develop an efficient and effective technique to finetune semantic segmentation models robustly on different variances of satellite images. The centered idea is that we leverage OpenStreetMap(OSM) data to generate noisy pseudo ground truth of road network or building footprint and utilize it in the training process. Our contributions are threefold. First, we develop a novel method to generate pseudo road network and building footprint ground truth masks from OSM data without human annotation labor. Sec ond, we propose a twostage transfer learning to utilize the pseudo ground truth masks in the first stage, and finetune the model on high quality annotation data in the second stage. In addition, we also propose some techniques and architectural modifications, i.e., PPLinkNet, to improve the overall performance of the binary se mantic segmentation system to extract road networks and building footprints from satellite images. Finally, we achieve promising im provements on DeepGlobe road network dataset[ 10], SpaceNet road and building footprint dataset [39].2 RELATED WORK "
172,Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement.txt,"Generic Image recognition is a fundamental and fairly important visual
problem in computer vision. One of the major challenges of this task lies in
the fact that single image usually has multiple objects inside while the labels
are still one-hot, another one is noisy and sometimes missing labels when
annotated by humans. In this paper, we focus on tackling these challenges
accompanying with two different image recognition problems: multi-model
ensemble and noisy data recognition with a unified framework. As is well-known,
usually the best performing deep neural models are ensembles of multiple
base-level networks, as it can mitigate the variation or noise containing in
the dataset. Unfortunately, the space required to store these many networks,
and the time required to execute them at runtime, prohibit their use in
applications where test sets are large (e.g., ImageNet). In this paper, we
present a method for compressing large, complex trained ensembles into a single
network, where the knowledge from a variety of trained deep neural networks
(DNNs) is distilled and transferred to a single DNN. In order to distill
diverse knowledge from different trained (teacher) models, we propose to use
adversarial-based learning strategy where we define a block-wise training loss
to guide and optimize the predefined student network to recover the knowledge
in teacher models, and to promote the discriminator network to distinguish
teacher vs. student features simultaneously. Extensive experiments on
CIFAR-10/100, SVHN, ImageNet and iMaterialist Challenge Dataset demonstrate the
effectiveness of our MEAL method. On ImageNet, our ResNet-50 based MEAL
achieves top-1/5 21.79%/5.99% val error, which outperforms the original model
by 2.06%/1.14%. On iMaterialist Challenge Dataset, our MEAL obtains a
remarkable improvement of top-3 1.15% (official evaluation metric) on a strong
baseline model of ResNet-101.","THEmodel ensemble approach is a collection of neural networks whose predictions are combined at test stage by weighted averaging or voting. It has been long observed that ensembles of multiple networks are generally much more robust and accurate than a single network if the training data is noisy and intractable to handle. This beneÔ¨Åt has also been exploited indirectly when training a single network through Dropout [3], Dropconnect [4], Stochastic Depth [5], Swapout [6], etc. We extend this idea by forming Zhiqiang Shen, Yutong Zheng, Chenchen Zhu and Marios Savvides are with the Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, P A 15213, USA. Email: fzhiqians, yu tongzh, chenchez, marioss g@andrew.cmu.edu. Zhankui He is with the Department of Computer Science, University of California San Diego, CA 92093, USA. Email: zhh004@eng.ucsd.edu. Wanyun Cui is with Shanghai University of Finance and Economics. E mail: cui.wanyun@sufe.edu.cn. Jiahui Yu is with the Department of Electrical and Computer Engineering, University of Illinois at UrbanaChampaign, Illinois, IL 61801, USA. E mail: jyu79@illinois.edu.ensemble predictions during training, using the outputs of different network architectures with different or identical augmented input. Our testing still operates on a single network, but the supervision labels made on different pre trained networks correspond to an ensemble prediction of a group of individual reference networks. The traditional ensemble, or called true ensemble, has some disadvantages that are often overlooked. 1) Redun dancy: The information or knowledge contained in the trained neural networks are always redundant and has over laps between with each other. Directly combining the pre dictions often requires extra computational cost but the gain is limited. 2) Ensemble is always large and slow: Ensemble requires more computing operations than an individual network, which makes it unusable for applications with limited memory, storage space, or computational power such as desktop, mobile and even embedded devices, and for applications in which realtime predictions are needed. To address the aforementioned shortcomings, in this paper we propose to use a learningbased ensemble method. Our goal is to learn an ensemble of multiple neural networksarXiv:1908.08520v1  [cs.CV]  22 Aug 20192 (a) Standard  (b) Ours Fig. 1: Visualizations of validation images from the Ima geNet dataset [1] by tSNE [7]. We randomly sample 10 classes within 1000 classes. Left is the single model result using the standard training strategy. Right is our MEAL ensemble model result. without incurring any additional testing costs , as shown in Fig. 2. We achieve this goal by leveraging the combination of diverse outputs from different neural networks as su pervisions to guide the target network training. The refer ence networks are called Teachers and the target networks are called Students . Instead of using the traditional one hot vector labels, we use the softlabels that provide more coverage for cooccurring and visually related objects and scenes. We argue that labels should be informative for the speciÔ¨Åc image. In other words, the labels should not be identical for all the given images with the same class. More speciÔ¨Åcally, as shown in Fig. 3, an image of ‚Äútobacco shop‚Äù has similar appearance to ‚Äúlibrary‚Äù should have a different label distribution than an image of ‚Äútobacco shop‚Äù but is more similar to ‚Äúgrocery store‚Äù. It can also be observed that soft labels can provide the additional intra and inter category relations of datasets. To further improve the robustness of student networks, we introduce an adversarial learning strategy to force the student to generate similar outputs as teachers. We propose two different strategies for the generative adversarial train ing: (i) joint training with a uniÔ¨Åed framework; and (ii) al ternately update gradients with separate training processes, i.e., updating the gradients in discriminator and student network iteratively. To the best of our knowledge, there are very few existing works adopting generative adversarial learning to force the student networks to have similar distri bution outputs with the teachers, so our proposed method is a pioneer of this direction for multimodel ensemble. Our experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets. For instance, our shakeshake [8] based MEAL achieves 2.54% test error on CIFAR10, which is a relative 11:2%improvement1. On ImageNet, our ResNet 50 based MEAL achieves 21.79%/5.99% val error, which outperforms the baseline by a large margin. Furthermore, we extend our method to the problem of noisy data processing. We propose an iterative reÔ¨Ånement paradigm based on our MEAL method, which can reÔ¨Åne the labels from the teacher networks progressively and provide more accurate supervisions for the student network training. We conduct experiments on iMaterialist Challenge 1. Shakeshake baseline [8] is 2.86%. 1 2 3 4 5 # of ensembles0√ó1√ó2√ó3√ó4√ó5√ó6√óFLOPs FLOPs at Inference Time Snapshot Ensemble (Huang et al. 2017) Our FLOPs at Test TimeFig. 2: Comparison of FLOPs at inference time. Huang et al. [10] employ models at different local minimum for ensembling, which enables no additional training cost, but the computational FLOPs at test time linearly increase with more ensembles. In contrast, our method use only one model during inference time throughout, so the testing cost is independent of # ensembles. Dataset and the results show that our method can vastly improve the performance of base models. To explore what our model actually learned, we visualize the embedded features from the single model and our ensembling model. The visualization is plotted by tSNE tool [7] with the last convlayer features (2048 dimensions) from ResNet50. We randomly sample 10 classes on Ima geNet, results are shown in Fig. 1, it‚Äôs obvious that our model has better feature embedding result. In summary, our contribution in this paper is three fold. An endtoend framework with adversarial learning is designed based on the teacherstudent learning paradigm for deep neural network ensembling and noisy data learning. The proposed method can achieve the goal of ensem bling multiple neural networks with no additional testing cost . The proposed method improves the stateoftheart accuracy on CIFAR10/100, SVHN, ImageNet and iMaterialist Challenge Dataset for a variety of exist ing network architectures. A preliminary version of this manuscript [9] has been published in a previous conference. In this version, we involved and compared two different gradient update strategies for adversarial learning on our proposed MEAL framework. We also provided a novel learning paradigm for how to adopt our method on handling noisy date cir cumstances. Furthermore, we included more experiments, details, analysis and an iterative reÔ¨Ånement strategy with better performance. Currently, there are few works focus ing on adopting generative adversarial learning on feature space for learning identical distributions between teacher and student networks. Thus, this work gives very good and practical guidelines for multimodel learning/ensemble and noisy data reÔ¨Ånement. 2 R ELATED WORK "
34,Combating noisy labels by agreement: A joint training method with co-regularization.txt,"Deep Learning with noisy labels is a practically challenging problem in
weakly supervised learning. The state-of-the-art approaches ""Decoupling"" and
""Co-teaching+"" claim that the ""disagreement"" strategy is crucial for
alleviating the problem of learning with noisy labels. In this paper, we start
from a different perspective and propose a robust learning paradigm called
JoCoR, which aims to reduce the diversity of two networks during training.
Specifically, we first use two networks to make predictions on the same
mini-batch data and calculate a joint loss with Co-Regularization for each
training example. Then we select small-loss examples to update the parameters
of both two networks simultaneously. Trained by the joint loss, these two
networks would be more and more similar due to the effect of Co-Regularization.
Extensive experimental results on corrupted data from benchmark datasets
including MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is
superior to many state-of-the-art approaches for learning with noisy labels.","Deep Neural Networks (DNNs) achieve remarkable suc cess on various tasks, and most of them are trained in a su pervised manner, which heavily relies on a large number of training instances with accurate labels [14]. However, col lecting largescale datasets with fully precise annotations is expensive and timeconsuming. To alleviate this problem, data annotation companies choose some alternating meth ods such as crowdsourcing [39, 43] and online queries [3] to improve labelling efÔ¨Åciency. Unfortunately, these methods usually suffer from unavoidable noisy labels, which have been proven to lead to noticeable decrease in performance of DNNs [1, 44]. As this problem has severely limited the expansion of neural network applications, a large number of algorithms Corresponding author.have been developed for learning with noisy labels, which belongs to the family of weakly supervised learning frame works [2, 5, 6, 7, 8, 9, 11]. Some of them focus on improv ing the methods to estimate the latent noisy transition ma trix [21, 24, 32]. However, it is challenging to estimate the noise transition matrix accurately. An alternative approach is training on selected or weighted samples, e.g., Men tornet [16], gradientbased reweight [30] and Coteaching [12]. Furthermore, the stateoftheart methods including Coteaching+ [41] and Decoupling [23] have shown excel lent performance in learning with noisy labels by introduc ing the ‚ÄúDisagreement"" strategy, where ‚Äúwhen to update"" depends on a disagreement between two different networks. However, there are only a part of training examples that can be selected by the ‚ÄúDisagreement"" strategy, and these exam ples cannot be guaranteed to have groundtruth labels [12]. Therefore, there arises a question to be answered: Is ‚ÄúDis agreement"" necessary for training two networks to deal with noisy labels? Motivated by Cotraining for multiview learning and semisupervised learning that aims to maximize the agree ment on multiple distinct views [4, 19, 34, 45], a straight forward method for handling noisy labels is to apply the regularization from peer networks when training each sin gle network. However, although the regularization may im prove the generalization ability of networks by encourag ing agreement between them, it still suffers from memoriza tion effects on noisy labels [44]. To address this problem, we propose a novel approach named JoCoR ( Joint Train ing with CoR egularization). SpeciÔ¨Åcally, we train two net works with a joint loss, including the conventional super vised loss and the CoRegularization loss. Furthermore, we use the joint loss to select smallloss examples, thereby en suring the error Ô¨Çow from the biased selection would not be accumulated in a single network. To show that JoCoR signiÔ¨Åcantly improves the robust ness of deep learning on noisy labels, we conduct exten sive experiments on both simulated and realworld noisy datasets, including MNIST, CIFAR10, CIFAR100 andarXiv:2003.02752v3  [cs.CV]  22 Apr 2020Clothing1M datasets. Empirical results demonstrate that the robustness of deep models trained by our proposed ap proach is superior to many stateoftheart approaches. Fur thermore, the ablation studies clearly demonstrate the effec tiveness of CoRegularization and Joint Training. 2. Related work "
30,Adversarially Optimized Mixup for Robust Classification.txt,"Mixup is a procedure for data augmentation that trains networks to make
smoothly interpolated predictions between datapoints. Adversarial training is a
strong form of data augmentation that optimizes for worst-case predictions in a
compact space around each data-point, resulting in neural networks that make
much more robust predictions. In this paper, we bring these ideas together by
adversarially probing the space between datapoints, using projected gradient
descent (PGD). The fundamental approach in this work is to leverage
backpropagation through the mixup interpolation during training to optimize for
places where the network makes unsmooth and incongruous predictions.
Additionally, we also explore several modifications and nuances, like
optimization of the mixup ratio and geometrical label assignment, and discuss
their impact on enhancing network robustness. Through these ideas, we have been
able to train networks that robustly generalize better; experiments on CIFAR-10
and CIFAR-100 demonstrate consistent improvements in accuracy against strong
adversaries, including the recent strong ensemble attack AutoAttack. Our source
code would be released for reproducibility.","The vulnerability of neural networks to adversarial at tack has been plaguing machine learning researchers ever since the discovery by Szegedy et al. [18]. In the years since, many research efforts have been geared towards mak ing neural networks robust to adversarial perturbations, but many defense strategies have failed to stand the test of time. One of the strongest baselines for adversarial ro bustness that has repeatedly stood up to rigorous scrutiny is adversarial training [6], in particular adversarial training based on the Projected Gradient Descent (PGD) attack strat egy [14]. PGD adversarial training can be mathematically represented as a constrained inner minmax optimization, whose solution gives us a minimal perturbation that max imizes classiÔ¨Åcation loss. This minmax optimization isa characteristic of many approaches to adversarial robust training, including in other related formulations of the loss, like TRADES [27]. Such networks have repeatedly withstood numerous evaluations, as shown in works like [3]. One concern is that the robust accuracy of such networks on test sets leaves much to be desired. For instance, state of the art neural net works typically have less than 50% accuracyunderattack for the CIFAR10 dataset. This low robust accuracy occurs despite the fact that the network is able to memorize the ro bust (`1bounded boxed) training distribution with nearly 100% accuracy against its own attack model, as a result of PGD adversarial training. This points to a familiar prob lem for machine learning practitioners ‚Äì overÔ¨Åtting to the training set. As recent work has pointed out [22], robust adversarial training is just as susceptible to ‚ÄúoverÔ¨Åtting‚Äù as standard neural network training, because the robust train ing distribution is an incomplete subsampling or imperfect match to the robust test distribution due to the Ô¨Ånite size of the dataset. Recent developments have also shown that adversarial vulnerability is a problem of networks severely overÔ¨Åtting to features that are imperceptibly small yet useful for clas siÔ¨Åcation [11]. With this view, adversarial training is an advanced form of worstcase data augmentation. This view has been shown in [23] to be able to usefully improve ac curacy under various corruptions including ‚Äúnatural adver sarial examples‚Äù [10] with certain domain adaptation tools. Hence, improvements in adversarial training are useful not just for security purposes, but for a wide audience who de sires robust classiÔ¨Åcation to be more understandably as well as consistently generalizeable. Our approach focuses on the poor and overÔ¨Åtting accuracyunderattack of robust training. We frame together the effective mixup augmentation [26] introduced for typi cal (nonadversarial) training, and robust PGD adversarial optimization. We use adversarial optimization to pinpoint the locations in the interpolation space between datapoints, where the classiÔ¨Åcation decisions of the neural networks are the least smooth, i.e. that adversarially maximize the 1arXiv:2103.11589v1  [cs.LG]  22 Mar 2021KL divergence between the network‚Äôs predictions and the smoothed label interpolation between datapoints. Robust adversarial learning is susceptible to overÔ¨Åtting, and we show that data augmentation insights from standard training transfer well to adversarial optimization. Our con tributions include: ‚Ä¢ We show through intuitive geometry and empirical re sults that previous works integrating mixup and ad versarial optimization were limited in their ability to probe the vicinal distribution to Ô¨Ånd worstcase points. It is important for the adversarial optimization to be able to fully Ô¨Ånd the worstcase points to learn from. ‚Ä¢ With ablation experiments we break down the opti mization components that led our results to surpass the baselines. ‚Ä¢ Our approach demonstrates signiÔ¨Åcant improvements in robust accuracyunderattack against strong, state oftheart adversaries. We evaluate against an ensem ble of stateoftheart adversaries including a strong gradientfree blackbox attack, demonstrating that our approach provides real improvements that do not in troduce or rely on any gradient obfuscation. The rest of this paper is organized as follows. In Sec tion 2, we summarize related work exploring mixup for data augmentation or adversarial robustness. We present the background of this work and the details of our approach in Section 3. Section 4 presents the experimental results of our work, which are then comprehensively discussed in Section 5. Finally, we conclude this paper and discuss future av enues of research in Section 6. 2. Related Work "
328,CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation.txt,"Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to a different unlabeled target domain. Most existing
UDA methods focus on learning domain-invariant feature representation, either
from the domain level or category level, using convolution neural networks
(CNNs)-based frameworks. One fundamental problem for the category level based
UDA is the production of pseudo labels for samples in target domain, which are
usually too noisy for accurate domain alignment, inevitably compromising the
UDA performance. With the success of Transformer in various tasks, we find that
the cross-attention in Transformer is robust to the noisy input pairs for
better feature alignment, thus in this paper Transformer is adopted for the
challenging UDA task. Specifically, to generate accurate input pairs, we design
a two-way center-aware labeling algorithm to produce pseudo labels for target
samples. Along with the pseudo labels, a weight-sharing triple-branch
transformer framework is proposed to apply self-attention and cross-attention
for source/target feature learning and source-target domain alignment,
respectively. Such design explicitly enforces the framework to learn
discriminative domain-specific and domain-invariant representations
simultaneously. The proposed method is dubbed CDTrans (cross-domain
transformer), and it provides one of the first attempts to solve UDA tasks with
a pure transformer solution. Experiments show that our proposed method achieves
the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet.
Code and models are available at https://github.com/CDTrans/CDTrans.","Deep neural network have achieved remarkable success in a wide range of application scenar ios (Wang et al., 2022; Qian et al., 2021; Yiqi Jiang, 2022; Tan et al., 2019; Chen et al., 2021b; Jiang et al., 2021; Chen et al., 2017) but it still suffers poor generalization performance to other new domain because of the domain shift problem (Csurka, 2017; Zhao et al., 2020; Zhang et al., 2020; Oza et al., 2021). To handle this issue and avoid the expensive laborious annotations, lots of research efforts (Bousmalis et al., 2017; Kuroki et al., 2019; Wilson & Cook, 2020; VS et al., 2021) are devoted on Unsupervised Domain Adaptation (UDA). The UDA task aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. In UDA, most approaches focus on aligning distributions of source and target domain and learning domain invariant feature representations. One kind of such UDA methods are based on categorylevel alignment (Kang et al., 2019; Zhang et al., 2019; Jiang et al., 2020; Li et al., 2021b), which have achieved promising results on public UDA datasets using deep convolution neural networks (CNNs). The fundamental problems in categorylevel based alignment is the production of pseudo labels for samples in target domain to generate the input sourcetarget pairs. However, the current CNNsbased methods are not robust to the generated noisy pseudo labels for accurate domain alignment (Morerio et al., 2020; Jiang et al., 2020). With the success of Transformer in natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2018) and vision tasks (Dosovitskiy et al., 2020; Han et al., 2020; He et al., 2021; Khan et al., These authors contributed equally to this work. 1arXiv:2109.06165v4  [cs.CV]  19 Mar 2022Published as a conference paper at ICLR 2022 2021), it is found that crossattention in Transformer is good at aligning different distributions, even from different modalities e.g., visiontovision (Li et al., 2021e), visiontotext (Tsai et al., 2019; Hu & Singh, 2021) and texttospeech (Li et al., 2019). And we Ô¨Ånd that it is robust to noise in pseudo labels to some extent. Hence, in this paper, we apply transformers to the UDA task to take advantage of its robustness to noise and super power for feature alignment to deal with the problems as described above in CNNs. In our experiment, we conclude that even with noise in the labeling pair, the crossattention can still work well in aligning two distributions, thanks to the attention mechanism. To obtain more accurate pseudo labels, we designed a twoway centeraware labeling algorithm for samples in the target domain. The pseudo labels are produced based on the crossdomain similarity matrix, and a centeraware matching is involved to weight the matrix and weaken noise into the tolerable range. With the help of pseudo labels, we design the crossdomain transformer (CDTrans) for UDA. It consists of three weightsharing transformer branches, of which two branches are for source and target data respectively and the third one is the feature alignment branch, whose inputs are from sourcetarget pairs. The selfattention is applied in the source/target transformer branches and cross attention is involved in the feature alignment branch to conduct domain alignment. Such design explicitly enforces the framework to learn discriminative domainspeciÔ¨Åc and domaininvariant representations simultaneously. In summary, our contributions are threefold: ‚Ä¢ We propose a weightsharing triplebranch transformer framework, namely, CDTrans, for accurate unsupervised domain adaptation, taking advantage of its robustness to noisy labeling data and great power for feature alignment. ‚Ä¢ To produce pseudo labels with high quality, a twoway centeraware labeling method is proposed, and it boosts the Ô¨Ånal performance in the context of CDTrans. ‚Ä¢ CDTrans achieves the best performance compared to stateofthearts with a large margin on VisDA2017 (Peng et al., 2017) and DomainNet (Peng et al., 2019) datasets. 2 R ELATED WORK "
195,Unsupervised Person Re-Identification with Multi-Label Learning Guided Self-Paced Clustering.txt,"Although unsupervised person re-identification (Re-ID) has drawn increasing
research attention recently, it remains challenging to learn discriminative
features without annotations across disjoint camera views. In this paper, we
address the unsupervised person Re-ID with a conceptually novel yet simple
framework, termed as Multi-label Learning guided self-paced Clustering (MLC).
MLC mainly learns discriminative features with three crucial modules, namely a
multi-scale network, a multi-label learning module, and a self-paced clustering
module. Specifically, the multi-scale network generates multi-granularity
person features in both global and local views. The multi-label learning module
leverages a memory feature bank and assigns each image with a multi-label
vector based on the similarities between the image and feature bank. After
multi-label training for several epochs, the self-paced clustering joins in
training and assigns a pseudo label for each image. The benefits of our MLC
come from three aspects: i) the multi-scale person features for better
similarity measurement, ii) the multi-label assignment based on the whole
dataset ensures that every image can be trained, and iii) the self-paced
clustering removes some noisy samples for better feature learning. Extensive
experiments on three popular large-scale Re-ID benchmarks demonstrate that our
MLC outperforms previous state-of-the-art methods and significantly improves
the performance of unsupervised person Re-ID.","Person reidentiÔ¨Åcation (ReID) aims at searching peo ple across nonoverlapping surveillance camera views de ployed at different locations by matching person images [22, 59, 35]. Due to its importance in smart cities and largescale surveillance systems, person ReID is alreadya wellestablished research problem in computer vision [13, 17, 62, 20]. Though great progress has been made in both benchmarks and approaches in recent years, person ReID remains an open challenging problem due to the dif Ô¨Åculty of learning robust and discriminative representation with large variant intraperson appearance and high inter person similarity. Over the past decades, most of the existing person Re ID works focus on feature designing and metric learn ing [25, 4, 46]. Recently, modern deep learning has been applied to the ReID community and achieved signiÔ¨Åcant progress [22, 5, 57]. Most of these works tackle ReID in a supervised learning manner, which are limited by the small scale of ReID datasets. Nevertheless, collecting unlabeled pedestrian images are much easier and cheaper, thus train ing deep networks on large scale unlabeled data becomes increasingly necessary and practical. In fact, unsupervised learning for person ReID has be come a hot topic in more recent years [6, 52, 33, 44]. There mainly exist two types of unsupervised person ReID meth ods. The Ô¨Årst one is based on unsupervised domain adaption (UDA) where the source domain is usually a labeled dataset and the target domain is an unlabeled dataset. Most of these UDA based methods use transfer learning to learn the knowledge in the labeled source ReID dataset and transfer them to target datasets [39, 32, 48, 42]. SpeciÔ¨Åcally, some works use generative adversarial networks (GAN) for trans ferring sample images from the source domain to the tar get domain while preserving the person identity as much as possible [41, 63, 65]. Some others Ô¨Årst train models on the source domain, then leverage selfsupervised learning and clustering to estimate pseudolabels on the target domain it eratively to Ô¨Ånetune the pretrained model [33, 44, 9, 19]. The main disadvantages of unsupervised domain adaption ReID are twofold. On the one hand, it still needs ex pensive labeled data and the performance is usually limited 1arXiv:2103.04580v1  [cs.CV]  8 Mar 2021Figure 1. Comparison of recent unsupervised person ReID meth ods and our multilabel learning guided selfpaced clustering (MLC) method. by the scale of the labeled source dataset. On the other hand, these methods ignore the sample relations between the source and the target datasets. The second type of unsupervised ReID methods are based on fully unsupervised learning. Their goal is to learn discriminative representations in large scale unlabeled data. Most of these methods use clustering to generate pseudo la bels. For example, Lin et al. [27] propose a bottomup clus tering (BUC) framework that trains a network with pseudo labels iteratively. The inaccurate nature of clustering al gorithm on large intraclass variations makes the pseudo labels noisy, which in consequence leads to poor perfor mance. In order to avoid wrong merging and make full use of all the images, Ding et al . [7] propose an elegant and practical densitybased clustering approach by incorporat ing the cluster validity criterion. Wang et al. [34] consider the unsupervised person ReID as a multilabel classiÔ¨Åca tion task to progressively seek true labels. They introduce a Memorybased Multilabel ClassiÔ¨Åcation Loss (MMCL) method, which iteratively predicts multilabels and updates the network with multilabel classiÔ¨Åcation loss. As illus trated in Figure 1, the densitybased clustering strategy tries to keep highpurity samples for model training but it may ignore useful hard samples. The multilabel based strategy keeps all the samples in the memory which may introduce noisy samples in training phase. In this paper, to address the above issues of cluster ing and multilabel strategies, we propose a conceptually novel yet simple framework for unsupervised person Re ID, termed as multilabel learning guided selfpaced clus tering (MLC). SpeciÔ¨Åcally, MLC learns discriminative in formation with three crucial modules, namely a multiscale network (MN), a multilabel learning (ML) module, and a selfpaced clustering (SC) module. The MN module is used to mine the multiscale person features for better similar ity measurement. Comparing to the previous methods that only extracting the global features, the MN module captures more nonsalient or infrequent local information. Local feature learning is demonstrated as an effective strategy to en hance the feature representation [10] which is complemen tary to the global feature [36, 12]. The ML module gener ates a multilabel vector for each image based on a mem ory bank. SpeciÔ¨Åcally, each sample in the memory bank is viewed as a single class, and a sample is assigned with a multihot vector where the corresponding items are acti vated if the sample is similar with those indexed samples in memory. To this end, these images with the same iden tity could get similar multilabel vectors. To avoid train ing noisy samples which may hurt the Ô¨Ånal model, the SC module is added after several training epochs of ML. The SC module mainly removes noisy samples by densitybased clustering algorithm and assigns pseudo labels for multi class training. We jointly train the whole network in an endtoend manner. We evaluate the proposed MLC framework on three largescale datasets including Market1501, DukeMTMC reID, and MSMT17 without leveraging their annotations. Experimental results show that our MLC signiÔ¨Åcantly im proves the performance of unsupervised person ReID with out any annotations and achieves performance superior or comparable to the stateoftheart methods. h 2. Related work "
416,Robust Training of Graph Neural Networks via Noise Governance.txt,"Graph Neural Networks (GNNs) have become widely-used models for
semi-supervised learning. However, the robustness of GNNs in the presence of
label noise remains a largely under-explored problem. In this paper, we
consider an important yet challenging scenario where labels on nodes of graphs
are not only noisy but also scarce. In this scenario, the performance of GNNs
is prone to degrade due to label noise propagation and insufficient learning.
To address these issues, we propose a novel RTGNN (Robust Training of Graph
Neural Networks via Noise Governance) framework that achieves better robustness
by learning to explicitly govern label noise. More specifically, we introduce
self-reinforcement and consistency regularization as supplemental supervision.
The self-reinforcement supervision is inspired by the memorization effects of
deep neural networks and aims to correct noisy labels. Further, the consistency
regularization prevents GNNs from overfitting to noisy labels via mimicry loss
in both the inter-view and intra-view perspectives. To leverage such
supervisions, we divide labels into clean and noisy types, rectify inaccurate
labels, and further generate pseudo-labels on unlabeled nodes. Supervision for
nodes with different types of labels is then chosen adaptively. This enables
sufficient learning from clean labels while limiting the impact of noisy ones.
We conduct extensive experiments to evaluate the effectiveness of our RTGNN
framework, and the results validate its consistent superior performance over
state-of-the-art methods with two types of label noises and various noise
rates.","In realworld applications, a set of objects and their relationships can often be represented naturally as a graph. The graph data structure is widely employed in various domains such as biology, transportation, and social science. In the past decade, Graph Neural Networks (GNNs) have shown promising capacity in modeling graph data [ 13,20,38]. Typically, GNNs adopt a message passing and aggregation procedure to effectively propagate information via the graph structure. This mechanism makes GNNs very suitable for semisupervised graph learning (e.g., node classification [1, 32]). While GNNs are generally effective, most of the existing ap proaches assume that labels are sufficient and clean. However, in practice, node labels can be both scarce and noisy. For example, consider a graph from social media whose node labels are con tributed by users. It is often the case that only a small fraction of users would participate in label generation, and intentionally or for other reasons, some of the labels do not reflect the truth. Another example is crowdsourcing node labels, e.g., fake news annotation and medical knowledge graph annotation. It is easy to see that the annotation process is laborintensive and expensive, and almost inevitably, label errors are introduced due to subjective judgment. Such cases will lead to graphs with scarce and noisy node labels. It has already been observed that noisy labels could pose a severe threat to the generalization performance of deep learning mod els [2,42]. Therefore, developing labelefficient and noiseresistant GNNs is an important and challenging problem. In the literature, robust deep learning in the presence of noisy labels has been explored mainly in applications with nongraph data such as images. Several strategies have been developed to combat label noises, e.g., sample selection [ 14,16,24], robust lossarXiv:2211.06614v2  [cs.LG]  26 Feb 2023WSDM ‚Äô23, February 27March 3, 2023, Singapore, Singapore. Siyi Qian, et al. functions [ 10,34,44], and loss correction [ 27,29]. However, sim ply incorporating these approaches into GNNs will be insufficient (we will empirically show this in experiments). We observe that one unique characteristic of this problem with GNNs is that the scarcity of labels causes difficulties for nodes to receive sufficient supervision from labeled neighbors. Meanwhile, nodes in graphs are directly influenced by potential noises through message passing. Failing to balance the two would either lead to massive erroneous supervision from noisy labels or end up with insufficient learning. Hence, a main challenge to solving this problem is how to effec tively leverage supervision of clean labels while limiting the impact of noisy ones . Previous studies mainly focused on leveraging supervi sion of labels, but neglected how to limit the impact of noisy labels. For example, recently, NRGNN [ 6] investigated a robust GNN with noisy and sparse labels. It proposed to link unlabeled nodes with labeled nodes and further mine accurate pseudolabels to provide more supervision. However, NRGNN mainly emphasized leverag ing supervision of labels, which mixed up clean and noisy labels in its learning process. Whereas, as explained above, explicitly gov erning noises (i.e., limiting the impact of noisy labels) is necessary in order to further boost the robustness of GNNs. To this end, we propose a novel GNN framework called RTGNN (Robust Training of Graph Neural Networks via Noise Governance) that is capable of conducting robust learning with scarce and noisy node labels. As a distinguished design of our model, we develop a finegrained noise governance strategy in semisupervised learning. Due to the fact that deep neural networks (DNNs) tend to prior itize learning simple patterns first and may overfit to noises [ 2], we first propose an effective and scalable lossbased label division method to identify potentially noisy labels. Further, besides the commonlyused label supervision, we introduce selfreinforcement and consistency regularization as supplemental supervisions. The selfreinforcement supervision is inspired by the memorization effects of DNNs and aims to correct noisy labels. Consistency regu larization includes an interview regularization based on ensembled classifiers capable of filtering different errors, and an intraview regularization to explore the local homogeneity of the graph [39]. Specifically, we propose to train a pair of peer GNNs to enforce adequate supervision as well as govern noise labels. First, we aug ment the raw graph by linking labeled and unlabeled nodes to facilitate efficient message passing, following NRGNN [ 6]. Then, in each epoch, we progressively perform the main division of clean and noisy candidate set by the smallloss criterion [ 14]. Next, a subset of confident nodes which predict a different class from their labels in the noisy candidate set is reinforced by training with their own prediction. Similarly, we extend the label set by adding those confident and consistent unlabeled nodes to the training set. Finally, we apply interview regularization to help two classifiers coopera tively mimic each other‚Äôs soft targets and intraview regularization to enable nodes to learn from their neighbors. In summary, the main contributions of our work are as follows. ‚Ä¢We investigate the robust training problem of GNNs from the noise governance perspective, which has been under explored in previous studies. ‚Ä¢We develop a novel RTGNN model which governs label noises explicitly with selfreinforcement and consistency regularization. RTGNN also enables finegrained learningon relatively clean, potentially noisy, and pseudo labels. This allows effectively leveraging supervision information while limiting the impact of label noises. ‚Ä¢We conduct extensive experiments to evaluate the effective ness of our new approach, and the results validate the consis tent superior performance of RTGNN over stateoftheart methods with two types of noises and various noise rates. 2 RELATED WORK "
38,RankNEAT: Outperforming Stochastic Gradient Search in Preference Learning Tasks.txt,"Stochastic gradient descent (SGD) is a premium optimization method for
training neural networks, especially for learning objectively defined labels
such as image objects and events. When a neural network is instead faced with
subjectively defined labels--such as human demonstrations or annotations--SGD
may struggle to explore the deceptive and noisy loss landscapes caused by the
inherent bias and subjectivity of humans. While neural networks are often
trained via preference learning algorithms in an effort to eliminate such data
noise, the de facto training methods rely on gradient descent. Motivated by the
lack of empirical studies on the impact of evolutionary search to the training
of preference learners, we introduce the RankNEAT algorithm which learns to
rank through neuroevolution of augmenting topologies. We test the hypothesis
that RankNEAT outperforms traditional gradient-based preference learning within
the affective computing domain, in particular predicting annotated player
arousal from the game footage of three dissimilar games. RankNEAT yields
superior performances compared to the gradient-based preference learner
(RankNet) in the majority of experiments since its architecture optimization
capacity acts as an efficient feature selection mechanism, thereby, eliminating
overfitting. Results suggest that RankNEAT is a viable and highly efficient
evolutionary alternative to preference learning.","Forms of gradient descent are the natural choice of optimization method for training deep neural networks to predict objectively defined labels in tasks such as image and speech recognition, fraud detection, and event prediction. Over the last few years, we have witnessed a rapidly growing interest in the use of neural networks that are able to classify subjectively defined labels. This family of learningtorank or preference learning algorithms [ 9] that train neural networks‚Äîsuch as RankNet [ 2], DeepRank [ 29] and Lamb daMART [ 3]‚Äîyield good performance by relying primarily on gra dient descent methods. Subjectively defined labels, however, includ ing human demonstrations (e.g. creative tasks, navigation traces and paths) or human annotations (e.g. of emotion or aesthetics) yield highly complex, deceptive and noisy loss landscapes for a neural network to learn. Assuming that the plasticity of neuroevo lutionary processes would be beneficial for such loss landscapes, in this paper we test the hypothesis that evolutionary search would be a better optimizer for neural network training in preference learning (PL) tasks compared to stochastic gradient descent (SGD). To test our hypothesis, this paper explores the efficacy of neu roevolutionary search in PL tasks by building on the efficient and popular RankNet [ 2] architecture and enhancing its search capacity through neuroevolution. In particular, we introduce a novel algo rithm named RankNEAT that relies on the Siamese neural network architecture of RankNet and learns to rank via NeuroEvolution of Augmenting Topologies (NEAT) [ 36]. Unlike traditional gradient based PL methods, RankNEAT resembles the process of plastic ity [7], which induces changes in both the coupling strength and the spatial organization of synapses in biological neural networks. RankNEAT learns to rank subjectively defined labels with high degrees of accuracy through its ability to optimize the synaptic pa rameters such as the network‚Äôs weights and the edge architecture simultaneously. We test RankNEAT (neuroevolution) and compare it against the vanilla RankNet (stochastic gradient decent) in the task of player affect modeling across three games, using the AGAINarXiv:2204.06901v1  [cs.NE]  14 Apr 2022GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA Pinitas et al. [23] dataset of arousalannotated gameplay videos. Player modeling [46] is an important subfield in game research since it promotes the development of reliable human computer interaction systems and consequently improves the users‚Äô experience. Our current approach feeds images of gameplay to a pretrained vision transformer, while the last fullyconnected layer of the network is then trained to pre dict ordinal values of arousal, using RankNet or RankNEAT. Results indicate that RankNEAT is superior to SGD (RankNet) in training PL models of arousal in the majority of experiments performed. Our key findings suggest that RankNEAT is a viable PL paradigm which achieves comparable or significantly higher performances to RankNet. In this first experiment, RankNEAT optimizes the edge topology of the networks‚Äô last layer, resembling an evolutionary feature selection strategy that eliminates unnecessary features from the observed input space. Additional studies should explore how RankNEAT performs in other subjectively defined tasks and hyper parameter setups, such as increasing the topological complexity. This paper is novel in many ways. First, to the best of our knowl edge, this is the first time a NEATbased preference learner is in troduced, combining a traditional learningtorank neural network architecture with neuroevolution. Second, RankNEAT is tested broadly across three dissimilar games from the same genre show casing the robustness of the method for affect modeling. Third, the proposed approach is compared thoroughly against SGD (RankNet) across different games and hyperparameters. Finally, RankNEAT is combined with vision transformers (pretrained on ImageNet) enabling us to offer generalpurpose representations for solving tasks with subjectively defined labels. 2 RELATED WORK "
486,Learning with Noisy Labels over Imbalanced Subpopulations.txt,"Learning with Noisy Labels (LNL) has attracted significant attention from the
research community. Many recent LNL methods rely on the assumption that clean
samples tend to have ""small loss"". However, this assumption always fails to
generalize to some real-world cases with imbalanced subpopulations, i.e.,
training subpopulations varying in sample size or recognition difficulty.
Therefore, recent LNL methods face the risk of misclassifying those
""informative"" samples (e.g., hard samples or samples in the tail
subpopulations) into noisy samples, leading to poor generalization performance.
  To address the above issue, we propose a novel LNL method to simultaneously
deal with noisy labels and imbalanced subpopulations. It first leverages sample
correlation to estimate samples' clean probabilities for label correction and
then utilizes corrected labels for Distributionally Robust Optimization (DRO)
to further improve the robustness. Specifically, in contrast to previous works
using classification loss as the selection criterion, we introduce a
feature-based metric that takes the sample correlation into account for
estimating samples' clean probabilities. Then, we refurbish the noisy labels
using the estimated clean probabilities and the pseudo-labels from the model's
predictions. With refurbished labels, we use DRO to train the model to be
robust to subpopulation imbalance. Extensive experiments on a wide range of
benchmarks demonstrate that our technique can consistently improve current
state-of-the-art robust learning paradigms against noisy labels, especially
when encountering imbalanced subpopulations.","Deep Neural Networks (DNNs) have achieved remark able progress in various domains, including computer vi sion [19], health care [27], natural language processing [36], etc. In practice, training datasets may contain non negligible label noise caused by human annotators‚Äô errors. *Equal contribution. ‚Ä° Work done during an internship at Tencent AI Lab. yCorresponding authors.Therefore, training against noisy labels becomes a criti cal problem in realworld DNN deployment and has at tracted signiÔ¨Åcant attention from the research communities [1, 3, 39]. In recent years, numerous works aim to develop robust learning paradigms to combat label noise [11,16,20]. Among those, estimated clean probabilities are critical for robust training. For example, Bootstrapping [30] assigns smaller weights to the loss of possible noisy samples. Co teaching [11] maintains two DNN models, wherein one model is only trained by clean samples selected by another. Many StateOfTheArt (SOTA) methods estimate the clean probabilities based on the assumption that correctly labeled samples tend to have ‚Äúsmall loss‚Äù. For example, Dividemix [20] assumes the loss of clean and noisy samples following two Gaussian distributions, while the clean distribution has a smaller mean than the noisy one. Therefore, it utilizes a twocomponent Gaussian Mixture Model (GMM) to model and separate clean and noisy samples. Felidae Canidae Figure 1. A demonstration of the learning with noisy labels over imbalanced subpopulations setup. In the CanidaeFelidae classiÔ¨Å cation problem, the head subpopulations include dog and cat. The tail subpopulations include wolf and tiger. There also exist misla beled samples, e.g., cats mislabeled as Canidae or dogs mislabeled as Felidae. The size of the images indicates the sample size of the corresponding subpopulation. Although models trained with these methods can achieve high average performance on the overall population, they may underperform drastically in some subpopulations.1 The situation could become worse when the training dataset consists of ‚Äúimbalanced‚Äù subpopulations. Here, ‚Äúimbal anced‚Äù means training subpopulations vary in sample size 1We use ‚Äúsubpopulation‚Äù and ‚Äúgroup‚Äù interchangeably. 1arXiv:2211.08722v1  [cs.LG]  16 Nov 2022or recognition difÔ¨Åculty. Taking a CanidaeFelidae binary classiÔ¨Åcation problem as an example: the target task is to classify images into two classes, namely Canidae and Feli dae, where each class consists of different subpopulations. As shown in Fig. 1, there are two subpopulations in each class (the size of the images indicates the sample size of the corresponding subpopulation). There are also some noisy samples that adversely affect the training process. In such a problem, DNNs can easily overÔ¨Åt on samples in the head subpopulation (e.g., dogs) while the tail samples (e.g., wolfs) and the noisy samples (e.g., cats mislabeled into ‚ÄúCanidae‚Äù class) tend to have large classiÔ¨Åcation loss. Therefore, previous LNL methods would face the risk of misclassifying tail subpopulations into noisy samples, ag gravating the damage of subpopulation imbalance. We fur ther perform empirical studies on the corrupted Waterbird dataset to quantitatively demonstrate our point. SpeciÔ¨Åcally, the representative LNL method (green bar), similar to the ERM baseline (dotted black line), performs much worse on tail subpopulations, as shown in Fig. 2(a). We suggest the cause is its bad noise identiÔ¨Åcation performance on tail sub populations as in Fig. 2(b). /uni0000002b/uni00000048/uni00000044/uni00000047 /uni00000037/uni00000044/uni0000004c/uni0000004f /uni00000036/uni00000058/uni00000045/uni00000053/uni00000052/uni00000053/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni00000028/uni00000035/uni00000030/uni00000003/uni00000045/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048 /uni00000036/uni00000032/uni00000037/uni00000024/uni00000003/uni0000002f/uni00000031/uni0000002f/uni00000003/uni00000050/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047 /uni00000032/uni00000058/uni00000055/uni00000056 /uni0000002b/uni00000048/uni00000044/uni00000047 /uni00000037/uni00000044/uni0000004c/uni0000004f /uni00000036/uni00000058/uni00000045/uni00000053/uni00000052/uni00000053/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000038/uni00000026/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni00000036/uni00000032/uni00000037/uni00000024/uni00000003/uni0000002f/uni00000031/uni0000002f/uni00000003/uni00000050/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047 /uni00000032/uni00000058/uni00000055/uni00000056 Figure 2. (a) ClassiÔ¨Åcation accuracy and (b) Noise identiÔ¨Åca tion AUC of Empirical Risk Minimization (ERM), SOTA LNL method, and our method under the problem of noisy labels over imbalanced subpopulations. The dataset is the corrupted Water birds dataset under 30% noise rate. We use an improved version of DivideMix as a representative LNL method, whose details are introduced in Sec. 4. This paper studies an underexplored novel problem, i.e., learning with noisy labels over imbalanced subpopulations. As shown by the above discussions and observations, using the classiÔ¨Åcation loss (e.g., crossentropy) is inadequate to discriminate between the tail and noisy samples in this prob lem. Therefore, we resort to another way to estimate the label conÔ¨Ådence (i.e., the probability of a label being clean) by considering the sample relationship in feature space. To this end, we introduce a label conÔ¨Ådence estimation crite rion named Local Label Consistency (LLC). SpeciÔ¨Åcally, the LLC of one given sample is obtained by counting the number of samples in the featurespace nearest neighbor hood whose labels are consistent with the given sample.The design idea behind the LLC metric is that samples from the same class typically come with high feature similarity. Utilizing this idea, we employ Gaussian Mixture Models (GMM) for clustering and mapping LLC scores to label conÔ¨Ådence. Next, the label refurbishment framework uses label conÔ¨Ådence as a weight to correct the given noisy label. We further integrate the refurbished labels into Distribution ally Robust Optimization (DRO) [32,44,46], i.e., construct ing the worstcase loss based on estimated label conÔ¨Ådence. Empirically, we Ô¨Ånd the proposed refurbishDRO paradigm can achieve better tail performance. Through extensive experiments on corrupted Waterbird and CelebA datasets, where subpopulation imbalance and label noise coexist, we show the proposed method consis tently improves the performance on tail subpopulations in contrast to current SOTA methods. As Fig. 2 shows, due to the introduction of LLCbased label conÔ¨Ådence estima tion and refurbishDRO paradigm, our method (blue bar) achieves better classiÔ¨Åcation performance and noise iden tiÔ¨Åcation performance than previous methods (green bar) (details are in Sec. 4). Furthermore, our method also im proves current stateoftheart robust training paradigms on standard LNL benchmarks with possible implicit subpopu lation imbalance, including CIFAR, MiniWebVision, and ANIMAL10N. Our main contributions are summarized as follows: ‚Ä¢ We introduce a novel problem, i.e., learning with noisy labels over imbalanced subpopulations, which has been less explored in the community. Moreover, through empirical studies, we demonstrate how previ ous LNL methods underperform in this new problem. ‚Ä¢ We propose a general framework for this problem. The basic idea of the framework lies in a simple yet effec tive strategy that estimates label conÔ¨Ådence based on sample relations. The label conÔ¨Ådence is then put into our proposed refurbishDRO paradigm to further im prove robustness. ‚Ä¢ To verify the effectiveness of our method, we con duct extensive experiments on various datasets, includ ing corrupted datasets with imbalanced subpopulations and noisy labels (Waterbirds and CelebA) and standard LNL benchmark datasets (CIFAR, MiniWebVision, and ANIMAL10N). We observe that the proposed method consistently outperforms previous methods. 2. Related works "
331,The Group Loss for Deep Metric Learning.txt,"Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that ""similar objects
should belong to the same group"", the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We show state-of-the-art results on clustering and image
retrieval on several datasets, and show the potential of our method when
combined with other techniques such as ensembles","Measuring object similarity is at the core of many important machine learning problems like clustering and object retrieval. For visual tasks, this means learning a distance function over images. With the rise of deep neural networks, the focus has rather shifted towards learning a feature embedding that is easily separable using a simple distance function, such as the Euclidean distance. In essence, objects of the same class (similar) should be close by in the learned manifold, while objects of a dierent class (dissimilar) should be far away. Historically, the best performing approaches get deep feature embeddings from the socalled siamese networks [4], which are typically trained using the contrastive loss [4] or the triplet loss [41,53]. A clear drawback of these losses is that they only consider pairs or triplets of data points, missing key infor mation about the relationships between all members of the minibatch. On aarXiv:1912.00385v4  [cs.CV]  20 Jul 20202 I. Elezi et al. minibatch of size n, despite that the number of pairwise relations between sam ples isO(n2), contrastive loss uses only O(n=2) pairwise relations, while triplet loss usesO(2n=3) relations. Additionally, these methods consider only the rela tions between objects of the same class (positives) and objects of other classes (negatives), without making any distinction that negatives belong to dierent classes. This leads to not taking into consideration the global structure of the embedding space, and consequently results in lower clustering and retrieval per formance. To compensate for that, researchers rely on other tricks to train neural networks for deep metric learning: intelligent sampling [25], multitask learning [59] or hardnegative mining [40]. Recently, researchers have been increasingly working towards exploiting in a principled way the global structure of the em bedding space [36,5,12,50], typically by designing ranking loss functions instead of following the classic triplet formulations. In a similar spirit, we propose Group Loss , a novel loss function for deep metric learning that considers the similarity between all samples in a minibatch. To create the minibatch, we sample from a xed number of classes, with samples coming from a class forming a group . Thus, each minibatch consists of several randomly chosen groups, and each group has a xed number of samples. An iterative, fullydierentiable label propagation algorithm is then used to build feature embeddings which are similar for samples belonging to the same group, and dissimilar otherwise. At the core of our method lies an iterative process called replicator dynamics [52,9], that renes the local information, given by the softmax layer of a neural network, with the global information of the minibatch given by the similarity between embeddings. The driving rationale is that the more similar two samples are, the more they aect each other in choosing their nal label and tend to be grouped together in the same group, while dissimilar samples do not aect each other on their choices. Neural networks optimized with the Group Loss learn to provide similar features for samples belonging to the same class, making clustering and image retrieval easier. Ourcontribution in this work is fourfold: {We propose a novel loss function to train neural networks for deep metric embedding that takes into account the local information of the samples, as well as their similarity. {We propose a dierentiable labelpropagation iterative model to embed the similarity computation within backpropagation, allowing endtoend training with our new loss function. {We perform a comprehensive robustness analysis showing the stability of our module with respect to the choice of hyperparameters. {We show stateoftheart qualitative and quantitative results in several stan dard clustering and retrieval datasets.The Group Loss for Deep Metric Learning 3 Embedding Embedding Embedding . . . Summer  Tanger  White  Pelican  Black footed  Albatross  Indigo  Bunting CNN  CNN CNN  CNN Shared   Weights  Classes  Prior Group Loss  Similarity Refinement  Procedure  C.E. Loss Shared   Weights  Shared   Weights  = Anchor  Anchor Positive Negative  CNN CNN CNN  Triplet  Loss Shared   Weights Shared   Weights Embedding  Softmax  Softmax  Softmax  Softmax 2 31 CVPR 2020  Fig. 1: A comparison between a neural model trained with the Group Loss (left) and the triplet loss (right). Given a minibatch of images belonging to dierent classes, their embeddings are computed through a convolutional neural network. Such embeddings are then used to generate a similarity matrix that is fed to the Group Loss along with prior distributions of the images on the possible classes. The green contours around some minibatch images refer to anchors . It is worth noting that, dierently from the triplet loss, the Group Loss considers multiple classes and the pairwise relations between all the samples. Numbers from 1 to 3 refer to the Group Loss steps, see Sec 3.1 for the details. 2 Related Work "
89,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning.txt,"Learning with noisy-labels has become an important research topic in computer
vision where state-of-the-art (SOTA) methods explore: 1) prediction
disagreement with co-teaching strategy that updates two models when they
disagree on the prediction of training samples; and 2) sample selection to
divide the training set into clean and noisy sets based on small training loss.
However, the quick convergence of co-teaching models to select the same clean
subsets combined with relatively fast overfitting of noisy labels may induce
the wrong selection of noisy label samples as clean, leading to an inevitable
confirmation bias that damages accuracy. In this paper, we introduce our
noisy-label learning approach, called Asymmetric Co-teaching (AsyCo), which
introduces novel prediction disagreement that produces more consistent
divergent results of the co-teaching models, and a new sample selection
approach that does not require small-loss assumption to enable a better
robustness to confirmation bias than previous methods. More specifically, the
new prediction disagreement is achieved with the use of different training
strategies, where one model is trained with multi-class learning and the other
with multi-label learning. Also, the new sample selection is based on
multi-view consensus, which uses the label views from training labels and model
predictions to divide the training set into clean and noisy for training the
multi-class model and to re-label the training samples with multiple top-ranked
labels for training the multi-label model. Extensive experiments on synthetic
and real-world noisy-label datasets show that AsyCo improves over current SOTA
methods.","Deep neural network (DNN) has achieved remarkable success in many Ô¨Åelds, including computer vision [ 15,11], natural language processing (NLP) [ 7,35] and medical im age analysis [ 17,28]. However, the methods from those Decoupling Coteaching+ JoCoR AsyCo A A AB B B!= != !=A A AB B B!= != !=A A AB B BA A AB B Bbatch/epoch 1 batch/epoch 2 batch/epoch 3Figure 1. Comparison of methods Decoupling [ 20], Co teaching+ [ 36], JoCoR [ 29], and our AsyCo. AsyCo coteaches the multiclass model A and the multilabel model B with differ ent training strategies (denoted by the different colours of A&B). The training samples for A and B, represented by the green and red arrows, are formed by our proposed multiview consensus that uses label views from the training set and model predictions to estimate the variables wandÀÜy, which selects clean/noisy sam ples for training A and iteratively relabels samples for training B, respectively. Ô¨Åelds often require massive amount of highquality anno tated data for supervised training [ 6], which is challenging and expensive to acquire. To alleviate such problem, some datasets have been annotated via crowdsourcing [ 32], from search engines [ 27], or with NLP from radiology reports [ 28]. Although these cheaper annotation processes enable the con struction of largescale datasets, they inevitably introduce noisy labels for model training, resulting in DNN model per formance degradation. Therefore, novel learning algorithms are required to robustly train DNN models when training sets containing noisy labels. Previous methods tackle noisylabel learning from differ ent perspectives. For example, some approaches focus on prediction disagreement [36,29,20], which rely on jointly training two models to update their parameters when they disagree on the predictions of the same training samples. These two models generally use the same training strategy, so even though they are trained using samples with diver 1arXiv:2301.01143v1  [cs.CV]  1 Jan 2023gent predictions, both models will quickly converge to select similar clean samples during training, which neutralises the effectiveness of prediction disagreement. Other noisylabel learning methods are based on sample selection [16,9,1] to Ô¨Ånd clean and noisylabel samples that are treated differ ently in the training process. Sampleselection approaches usually assume that samples with small training losses are associated with clean labels, which is an assumption veri Ô¨Åed only at early training stages [ 18,37]. However, such assumption is unwarranted in later training stages because DNN models can overÔ¨Åt any type of noisy label after a cer tain number of epochs, essentially reducing the training loss for all training samples. Stateoftheart (SOTA) noisylabel learning approaches [ 16] have been designed to depend on both prediction disagreement and sample selection meth ods to achieve better performance than either method alone. Nevertheless, these SOTA methods are still affected by the fast convergence of both models and label noise overÔ¨Åtting, which raises the following questions: 1) Are there more effective ways to maximise the prediction disagreement be tween both models, so they consistently produce divergent results during the training procedure? 2) Is there a sam ple selection approach that can better integrate prediction disagreements than the small loss strategy? Motivated by traditional multiview learning [ 3,26] and multilabel learning [ 24], we propose a new noisylabel learn ing method that aims to answer the two questions above. Our method, named Asymmetric Coteaching (AsyCo) and de picted in Fig. 1, is based on two models trained with different learning strategies to maximise their prediction disagreement. One model, the classiÔ¨Åcation net , is trained with conven tional multiclass learning by minimising a cross entropy loss and provide singleclass prediction, and the other, the reference net , is trained with a binary cross entropy loss to enable multilabel learning that is used to estimate the top ranked labels that represent the potentially clean candidate labels for each training sample. The original training labels and the predictions by the training and reference nets enable the formation of three label views for each training sample, allowing us to formulate the multiview consensus that is tightly integrated with the prediction disagreement to select clean and noisy samples for training the multiclass model and to iteratively relabel samples with multiple topranked labels for training the multilabel model. In summary, our main contributions are: ‚Ä¢The new noisylabel coteaching method AsyCo de signed to maximise the prediction disagreement be tween the training of a multiclass and a multilabel model; and ‚Ä¢The novel multiview consensus that uses the disagree ments between training labels and model predictions to select clean and noisy samples for training the multiclass model and to iteratively relabel samples with multiple topranked labels for training the multilabel model. We conduct extensive experiments on both synthetic and realworld noisy datasets that show that AsyCo provides sub stantial improvements over previous stateoftheart (SOTA) methods. 2. Related Work "
394,Disparity Between Batches as a Signal for Early Stopping.txt,"We propose a metric for evaluating the generalization ability of deep neural
networks trained with mini-batch gradient descent. Our metric, called gradient
disparity, is the $\ell_2$ norm distance between the gradient vectors of two
mini-batches drawn from the training set. It is derived from a probabilistic
upper bound on the difference between the classification errors over a given
mini-batch, when the network is trained on this mini-batch and when the network
is trained on another mini-batch of points sampled from the same dataset. We
empirically show that gradient disparity is a very promising early-stopping
criterion (i) when data is limited, as it uses all the samples for training and
(ii) when available data has noisy labels, as it signals overfitting better
than the validation data. Furthermore, we show in a wide range of experimental
settings that gradient disparity is strongly related to the generalization
error between the training and test sets, and that it is also very informative
about the level of label noise.","Earlystopping using a separate validation set is one of the most popular techniques used to avoid under/over Ô¨Åtting deep neural networks trained with iterative methods, such as gradient descent [15, 42, 56]. The optimization is stopped when the performance of the model on a validation set starts to diverge from its performance on the training set. Early stopping requires an accurately labeled validation set, separated from the training set, to act as an unbiased proxy on the unseen test error. Obtaining such a reliable validation set can be expensive in many realworld applications as data collection is a timeconsuming process that might require domain expertise. Furthermore, deep learning is becoming popular in applications for which there is simply not enough available data [22, 46]. Finally, inexperienced label collectors, complex tasks (e.g., distinguishing a guinea pig from a hamster), and corrupted labels due for instance to adversarial attacks result in datasets that contain noisy labels [12]. Deep neural networks have the unfortunate ability to overÔ¨Åt to such small and/or noisy labeled datasets, an issue that cannot be completely solved by popular regularization techniques [59]. A signal of overÔ¨Åtting during training is therefore particularly useful, if it does notneed a separate, accurately labeled validation set, which is the purpose of this paper. LetS1andS2be two minibatches of points sampled from the available (training) dataset. Suppose that S1 is selected for an iteration (step) of the minibatch gradient descent (SGD), at the end of which the parameter vector is updated to w1. The average loss over S1(denoted by LS1(hw1)) is in principle reduced, given a suÔ¨Éciently small learning rate. The average loss LS2(hw1)over the other minibatch S2is not as likely to be reduced. It is more likely to remain larger than the loss LS2(hw2)computed over S2, if it wasS2instead of S1that had been selected for this iteration. The diÔ¨Äerence R2=LS2(hw1)"
392,Self-supervised learning for infant cry analysis.txt,"In this paper, we explore self-supervised learning (SSL) for analyzing a
first-of-its-kind database of cry recordings containing clinical indications of
more than a thousand newborns. Specifically, we target cry-based detection of
neurological injury as well as identification of cry triggers such as pain,
hunger, and discomfort. Annotating a large database in the medical setting is
expensive and time-consuming, typically requiring the collaboration of several
experts over years. Leveraging large amounts of unlabeled audio data to learn
useful representations can lower the cost of building robust models and,
ultimately, clinical solutions. In this work, we experiment with
self-supervised pre-training of a convolutional neural network on large audio
datasets. We show that pre-training with SSL contrastive loss (SimCLR) performs
significantly better than supervised pre-training for both neuro injury and cry
triggers. In addition, we demonstrate further performance gains through
SSL-based domain adaptation using unlabeled infant cries. We also show that
using such SSL-based pre-training for adaptation to cry sounds decreases the
need for labeled data of the overall system.","Crying is the primary means by which babies communicate with the world. Researchers have been interested in infant cry analysis since the early 1960s [1]. Cry characteristics may help us to under stand basic baby needs (hunger, pain, etc.) and, more importantly, can be analyzed for the early and noninvasive detection of various diseases [2]. For example, clinical research has reported that cer tain infant cry characteristics are correlated with birth asphyxia [3]. This multicausal condition frequently leads to severe health prob lems, including neurological injury and even death. Various methods based on signal processing [4], statistical modeling [5, 6] and deep learning [7‚Äì10] have been explored for Ô¨Ånding clinical and other in sights using cry recordings. One of the main challenges in baby cry analysis is data acqui sition. Today, cry sounds are not part of routine medical records, so obtaining a database requires targeted efforts such as a clinical study. These are expensive to conduct and typically require the col laboration of several hospital staff over the years. Most machine learning (ML) research on pathology detection from cry sounds was done using the Baby Chillanto [11] database, which contains only six patients diagnosed with birth asphyxia. From an ML problem point of view, cry classiÔ¨Åcation is anal ogous to general audio classiÔ¨Åcation, where deep convolutional neural networks (CNNs) have excelled as the stateoftheart. Re cently, [12] demonstrated that Pretrained Audio Neural Networks(PANNs)  large CNNs pretrained on generic audio  transferred to a wide range of audio pattern recognition tasks outperformed sev eral previous stateoftheart systems. Since then, PANNs have been widely adopted for various audio tasks, including emotion recogni tion from speech [13] and COVID19 detection from cough [14]. Another popular paradigm in audio classiÔ¨Åcation stateoftheart is selfsupervised learning (SSL)  a method to obtain highquality representations by training on unlabeled data. SSL has revolution ized the Ô¨Åelds of Natural Language Processing and Computer Vi sion and is currently widely adopted in audio processing [15]. A neural network (encoder) pretrained with SSL can be seen as a non linear mapping of an audio sequence to a hidden representation  an embedding. The embeddings can be used as input to a classi Ô¨Åer trained on a speciÔ¨Åc task with a supervised objective (using la beled data and conventional crossentropy loss). This approach is common for benchmarking various SSL models on multiple diverse audio tasks [16, 17]. Recently, a similaritybased contrastive learn ing method called SimCLR introduced in Computer Vision [18, 19] demonstrated good performance in multiple audio tasks [17, 20], in cluding music analysis [21, 22]. SimCLR maximizes the similarity between modiÔ¨Åed (distorted) views of the same object. For audio, such distortion can be done, for example, by mixing random audio samples [17], spectrogram masking [23] in [20], or/and reverbera tion, pitch shifting, etc [21]. In this paper, we experiment with PANNs using both super vised and selfsupervised pretraining to learn representations for two downstream tasks. The Ô¨Årst task is classifying brain injury (re sulting from birth asphyxia), and the second is predicting cry trig gers (pain, hunger, discomfort). The methods are tested on a unique clinical database of newborn cries collected by Ubenwa Health in collaboration with hospitals across three countries [24]. In addition, we evaluate the impact of SimCLRbased adap tation of PANNs using unlabeled cries inspired by selfsupervised domain adaptation in Speech [25] and Natural Language Process ing [26]. It should be noted that speech and audio SSL stateof theart frequently uses transformers instead of CNNs and relies on different learning objectives [15]. However, our preliminary exper iments with some popular pretrained speech and audio transform ers (speciÔ¨Åcally, Wav2Vec2.0 [27], HuBERT [28], WavLM [29] and SSAST [30]) have not shown sufÔ¨Åcient improvements but generally required many parameters to be adapted and hyperparameters tuned. We, therefore, focus on CNN and SimCLR, which demonstrated a good balance of accuracy and adaptation complexity. 2. METHODOLOGY "
7,Improving patch-based scene text script identification with ensembles of conjoined networks.txt,"This paper focuses on the problem of script identification in scene text
images. Facing this problem with state of the art CNN classifiers is not
straightforward, as they fail to address a key characteristic of scene text
instances: their extremely variable aspect ratio. Instead of resizing input
images to a fixed aspect ratio as in the typical use of holistic CNN
classifiers, we propose here a patch-based classification framework in order to
preserve discriminative parts of the image that are characteristic of its
class. We describe a novel method based on the use of ensembles of conjoined
networks to jointly learn discriminative stroke-parts representations and their
relative importance in a patch-based classification scheme. Our experiments
with this learning procedure demonstrate state-of-the-art results in two public
script identification datasets. In addition, we propose a new public benchmark
dataset for the evaluation of multi-lingual scene text end-to-end reading
systems. Experiments done in this dataset demonstrate the key role of script
identification in a complete end-to-end system that combines our script
identification method with a previously published text detector and an
off-the-shelf OCR engine.","Script and language identication are important steps in modern OCR systems designed for multilanguage environments. Since text recognition al gorithms are languagedependent, detecting the script and language at hand allows selecting the correct language model to employ [1]. While script iden tication has been widely studied in document analysis [2, 3], it remains an almost unexplored problem for scene text. In contrast to document im ages, scene text presents a set of specic challenges, stemming from the high variability in terms of perspective distortion, physical appearance, variable illumination and typeface design. At the same time, scene text comprises typically a few words, contrary to longer text passages available in document images. Current endtoend systems for scene text reading [4, 5, 6] assume single script and language inputs given beforehand, i.e. provided by the user, or inferred from available metadata. The unconstrained text understanding problem for large collections of images from unknown sources has not been considered up to very recently [7, 8, 9, 10, 11]. While there exists some previ ous research in script identication of text over complex backgrounds [12, 13], such methods have been so far limited to video overlaidtext, which presents in general dierent challenges than scene text. This paper addresses the problem of script identication in natural scene images, paving the road towards true multilingual endtoend scene text 2Figure 1: Collections of images from unknown sources may contain textual information in dierent scripts. understanding. Multiscript text exhibits high intraclass variability (words written in the same script vary a lot) and high interclass similarity (certain scripts resemble each other). Examining text samples from dierent scripts, it is clear that some strokeparts are quite discriminative, whereas others can be trivially ignored as they occur in multiple scripts. The ability to distinguish these relevant strokeparts can be leveraged for recognising the corresponding script. Figure 2 shows an example of this idea. Figure 2: (best viewed in color) Certain strokeparts (in green) are discrimi native for the identication of a particular script (left), while others (in red) can be trivially ignored because are frequent in other classes (right). The use of state of the art CNN classiers for script identication is not straightforward, as they fail to address a key characteristic of scene text instances: their extremely variable aspect ratio. As can be seen in Figure 3, scene text images may span from single characters to long text sentences, and thus resizing images to a xed aspect ratio, as in the typical use of holistic CNN classiers, will deteriorate discriminative parts of the image that are characteristic of its class. The key intuition behind the proposed method is 3that in order to retain the discriminative power of stroke parts we must rely in powerful local feature representations and use them within a patchbased classier. In other words, while holistic CNNs have superseded patchbased methods for image classication, we claim that patchbased classiers can still be essential in tasks where image shrinkage is not feasible. Figure 3: Scene text images with the larger/smaller aspect ratio available in three dierent datasets: MLe2e(left), SIW13(center), and CVSI(right). In previously published work [10] we have presented a method combining convolutional features, extracted by sliding a window with a single layer Convolutional Neural Network (CNN) [14], and the NaiveBayes Nearest Neighbour (NBNN) classier [15] with promising results. In this paper we demonstrate far superior performance by extending our previous work in two dierent ways: First, we use deep CNN architectures in order to learn more discriminative representations for the individual image patches; Second, we propose a novel learning methodology to jointly learn the patch representa tions and their importance (contribution) in a global image to class proba bilistic measure. For this, we train our CNN using an Ensemble of Conjoined Networks and a loss function that takes into account the global classication error for a group of Npatches instead of looking only into a single image patch. Thus, at training time our network is presented with a group of N patches sharing the same class label and produces a single probability distri bution over the classes for all them. This way we model the goal for which the network is trained, not only to learn good local patch representations, 4but also to learn their relative importance in the global image classication task. Experiments performed over two public datasets for scene text classica tion demonstrate stateoftheart results. In particular we are able to reduce classication error by 5 percentage points in the SIW13 dataset. We also introduce a new benchmark dataset, namely the MLe2e dataset, for the eval uation of scene text endtoend reading systems and all intermediate stages such as text detection, script identication and text recognition. The dataset contains a total of 711 scene images, and 1821 text line instances, covering four dierent scripts (Latin, Chinese, Kannada, and Hangul) and a large variability of scene text samples. 2. Related Work "
40,Early-Learning Regularization Prevents Memorization of Noisy Labels.txt,"We propose a novel framework to perform classification via deep learning in
the presence of noisy annotations. When trained on noisy labels, deep neural
networks have been observed to first fit the training data with clean labels
during an ""early learning"" phase, before eventually memorizing the examples
with false labels. We prove that early learning and memorization are
fundamental phenomena in high-dimensional classification tasks, even in simple
linear models, and give a theoretical explanation in this setting. Motivated by
these findings, we develop a new technique for noisy classification tasks,
which exploits the progress of the early learning phase. In contrast with
existing approaches, which use the model output during early learning to detect
the examples with clean labels, and either ignore or attempt to correct the
false labels, we take a different route and instead capitalize on early
learning via regularization. There are two key elements to our approach. First,
we leverage semi-supervised learning techniques to produce target probabilities
based on the model outputs. Second, we design a regularization term that steers
the model towards these targets, implicitly preventing memorization of the
false labels. The resulting framework is shown to provide robustness to noisy
annotations on several standard benchmarks and real-world datasets, where it
achieves results comparable to the state of the art.","Deep neural networks have become an essential tool for classiÔ¨Åcation tasks [ 19,15,11]. These models tend to be trained on large curated datasets such as CIFAR10 [ 18] or ImageNet [ 9], where the vast majority of labels have been manually veriÔ¨Åed. Unfortunately, in many applications such datasets are not available, due to the cost or difÔ¨Åculty of manual labeling (e.g. [ 13,32,25,1]). However, datasets with lower quality annotations, obtained for instance from online queries [ 5] or crowdsourcing [ 49,53], may be available. Such annotations inevitably contain numerous mistakes or label noise . It is therefore of great importance to develop methodology that is robust to the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to Ô¨Årst Ô¨Åt the training data with clean labels during an early learning phase, before eventually memorizing the examples with 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.arXiv:2007.00151v2  [cs.LG]  22 Oct 2020Clean labels Wrong labels Cross Entropy Earlylearning Regularization Figure 1: Results of training a ResNet34 [ 15] neural network with a traditional cross entropy loss (top row) and our proposed method (bottom row) to perform classiÔ¨Åcation on the CIFAR10 dataset where 40% of the labels are Ô¨Çipped at random. The left column shows the fraction of examples with clean labels that are predicted correctly (green) and incorrectly (blue). The right column shows the fraction of examples with wrong labels that are predicted correctly (green), memorized (the prediction equals the wrong label, shown in red), and incorrectly predicted as neither the true nor the labeled class (blue). The model trained with cross entropy begins by learning to predict the true labels, even for many of the examples with wrong label, but eventually memorizes the wrong labels. Our proposed method based on earlylearning regularization prevents memorization, allowing the model to continue learning on the examples with clean labels to attain high accuracy on examples with both clean and wrong labels. false labels [ 3,54]. In this work we study this phenomenon and introduce a novel framework that exploits it to achieve robustness to noisy labels. Our main contributions are the following: ‚Ä¢In Section 3 we establish that early learning and memorization are fundamental phenomena in high dimensions, proving that they occur even for simple linear generative models. ‚Ä¢In Section 4 we propose a technique that utilizes the earlylearning phenomenon to counteract the inÔ¨Çuence of the noisy labels on the gradient of the cross entropy loss. This is achieved through a regularization term that incorporates target probabilities estimated from the model outputs using several semisupervised learning techniques. ‚Ä¢In Section 6 we show that the proposed methodology achieves results comparable to the state of the art on several standard benchmarks and realworld datasets. We also perform a systematic ablation study to evaluate the different alternatives to compute the target probabilities, and the effect of incorporating mixup data augmentation [55]. 2 Related Work "
208,Uncertainty-Aware Label Refinement for Sequence Labeling.txt,"Conditional random fields (CRF) for label decoding has become ubiquitous in
sequence labeling tasks. However, the local label dependencies and inefficient
Viterbi decoding have always been a problem to be solved. In this work, we
introduce a novel two-stage label decoding framework to model long-term label
dependencies, while being much more computationally efficient. A base model
first predicts draft labels, and then a novel two-stream self-attention model
makes refinements on these draft predictions based on long-range label
dependencies, which can achieve parallel decoding for a faster prediction. In
addition, in order to mitigate the side effects of incorrect draft labels,
Bayesian neural networks are used to indicate the labels with a high
probability of being wrong, which can greatly assist in preventing error
propagation. The experimental results on three sequence labeling benchmarks
demonstrated that the proposed method not only outperformed the CRF-based
methods but also greatly accelerated the inference process.","Linguistic sequence labeling is one of the funda mental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including partofspeech (POS) tagging, text chunking, and named entity recognition (NER). BeneÔ¨Åting from representation learning, neural networkbased approaches can achieve stateof theart performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. United            Arab            EmiratesBLOC          ILOC          ELOCInputTrue LabelDraft LabelRefinement......BORG          ILOC          EORGBLOC          IORG          ELOCX<latexit sha1_base64=""ex7UVx6LxzCcCSDqKZWFuZK2gBU="">AAACzXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoxp0V7APbIsl02obmxWQilKpbf8Ct/pb4B/oX3hlTUIvohCRnzr3nzNx73dj3EmlZrzljbn5hcSm/XFhZXVvfKG5uNZIoFYzXWeRHouU6Cfe9kNelJ33eigV3AtfnTXd0quLNGy4SLwov5Tjm3cAZhF7fY44k6qrDhpyNAkeMroslq2zpZc4COwMlZKsWFV/QQQ8RGFIE4AghCftwkNDThg0LMXFdTIgThDwd57hDgbQpZXHKcIgd0XdAu3bGhrRXnolWMzrFp1eQ0sQeaSLKE4TVaaaOp9pZsb95T7SnutuY/m7mFRArMST2L9008786VYtEH8e6Bo9qijWjqmOZS6q7om5ufqlKkkNMnMI9igvCTCunfTa1JtG1q946Ov6mMxWr9izLTfGubkkDtn+OcxY0KmX7oFy5OCxVT7JR57GDXezTPI9QxRlqqJN3iEc84dk4N1Lj1rj/TDVymWYb35bx8AFBiJNo</latexit>‚á•<latexit sha1_base64=""PcNBZ+k3dnNGojhYt75BSmjl47k="">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoRnBTwT6gLZJMp3VsXiYTsRZX/oBb/THxD/QvvDOmoBbRCUnOnHvPmbn3upEnEmlZrzljZnZufiG/WFhaXlldK65vNJIwjRmvs9AL45brJNwTAa9LIT3eimLu+K7Hm+7wWMWbNzxORBicy1HEu74zCERfMEcS1ehI4fPkoliyypZe5jSwM1BCtmph8QUd9BCCIYUPjgCSsAcHCT1t2LAQEdfFmLiYkNBxjnsUSJtSFqcMh9ghfQe0a2dsQHvlmWg1o1M8emNSmtghTUh5MWF1mqnjqXZW7G/eY+2p7jaiv5t5+cRKXBL7l26S+V+dqkWij0Ndg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWTPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ0albK9V66c7ZeqR9mo89jCNnZpngeo4gQ11Mn7Co94wrNxalwbt8bdZ6qRyzSb+LaMhw/935G5</latexit>‚á•<latexit sha1_base64=""PcNBZ+k3dnNGojhYt75BSmjl47k="">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoRnBTwT6gLZJMp3VsXiYTsRZX/oBb/THxD/QvvDOmoBbRCUnOnHvPmbn3upEnEmlZrzljZnZufiG/WFhaXlldK65vNJIwjRmvs9AL45brJNwTAa9LIT3eimLu+K7Hm+7wWMWbNzxORBicy1HEu74zCERfMEcS1ehI4fPkoliyypZe5jSwM1BCtmph8QUd9BCCIYUPjgCSsAcHCT1t2LAQEdfFmLiYkNBxjnsUSJtSFqcMh9ghfQe0a2dsQHvlmWg1o1M8emNSmtghTUh5MWF1mqnjqXZW7G/eY+2p7jaiv5t5+cRKXBL7l26S+V+dqkWij0Ndg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWTPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ0albK9V66c7ZeqR9mo89jCNnZpngeo4gQ11Mn7Co94wrNxalwbt8bdZ6qRyzSb+LaMhw/935G5</latexit>‚á•<latexit sha1_base64=""PcNBZ+k3dnNGojhYt75BSmjl47k="">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoRnBTwT6gLZJMp3VsXiYTsRZX/oBb/THxD/QvvDOmoBbRCUnOnHvPmbn3upEnEmlZrzljZnZufiG/WFhaXlldK65vNJIwjRmvs9AL45brJNwTAa9LIT3eimLu+K7Hm+7wWMWbNzxORBicy1HEu74zCERfMEcS1ehI4fPkoliyypZe5jSwM1BCtmph8QUd9BCCIYUPjgCSsAcHCT1t2LAQEdfFmLiYkNBxjnsUSJtSFqcMh9ghfQe0a2dsQHvlmWg1o1M8emNSmtghTUh5MWF1mqnjqXZW7G/eY+2p7jaiv5t5+cRKXBL7l26S+V+dqkWij0Ndg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWTPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ0albK9V66c7ZeqR9mo89jCNnZpngeo4gQ11Mn7Co94wrNxalwbt8bdZ6qRyzSb+LaMhw/935G5</latexit>Figure 1: Schematic of label reÔ¨Ånement framework (Cui and Zhang, 2019). The goal is reÔ¨Åning the label of ‚ÄúArab‚Äù using contextual labels and words, while the reÔ¨Ånement of other correct labels may be negatively impacted by incorrect draft labels. creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely on inefÔ¨Åcient Viterbi decoding. Many of the recent methods try to introduce label embeddings to manage longer ranges of dependencies, such as twostage label reÔ¨Ånement (Krishnan and Manning, 2006; Cui and Zhang, 2019) and seq2seq (Vaswani et al., 2016; Zhang et al., 2018) frameworks. In particular, Cui and Zhang (2019) introduced a hierarchicallyreÔ¨Åned representation of marginal label distributions, which predicts a sequence of draft labels in advance and then uses the wordlabel interactions to reÔ¨Åne them. Although these methods can model longer label dependencies, they are vulnerable to error propagation: if a label is mistakenly predicted during inference, the error will be propagated and the other labels conditioned on this one will be impacted (Bengio et al., 2015). As shown in Figure 1, the label attention network (LAN) (Cui andarXiv:2012.10608v1  [cs.CL]  19 Dec 2020Draft Uncertainty ReÔ¨Ånement #Tokens 4 0.018 4 √ô 8 39 8 0.524 8 √ô 4 54 Table 1: Results of LAN with uncertainty estimation evaluated on CoNLL2003 test dataset. 4refers to the correct prediction, and 8refers to the wrong prediction. We use Bayesian neural networks (Kendall and Gal, 2017) to estimate the uncertainty. We can see that the uncertainty value of incorrect prediction is 29 times larger than that of correct predictions, which can effectively indicate the incorrect predictions. Zhang, 2019) would negatively impact the correct predictions in the reÔ¨Ånement stage. There are 39 correct tokens that have been incorrectly modiÔ¨Åed (Table 1). Hence, the model should selectively correct the labels with high probabilities of being incorrect, not all of them. Fortunately, we Ô¨Ånd that uncertainty values estimated by Bayesian neural networks (Kendall and Gal, 2017) can effectively indicate the labels that have a high probability of being incorrect. As shown in Table 11, the average uncertainty value of incorrect prediction is 29 times larger than that of correct predictions for the draft labels. Hence, we can easily set an uncertainty threshold to only reÔ¨Åne the potentially incorrect labels and prevent side effects on the correct labels. In this work, we propose a novel twostage UncertaintyAware label reÔ¨Ånement Network (UANet). At the Ô¨Årst stage, the Bayesian neural networks take a sentence as input and yield all of the draft labels together with corresponding uncertainties. At the second stage, a twostream selfattention model performs attention over label embeddings to explicitly model the label dependencies, as well as context vectors to model the context representations. All of these features are fused to reÔ¨Åne the potentially incorrect draft labels. The above label reÔ¨Ånement operations can be processed in parallel, which can avoid the use of Viterbi decoding of the CRF for a faster prediction. Experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRFbased methods but also signiÔ¨Åcantly accelerated the inference process. The main contributions of this paper can be summarized as follows: 1) we propose the use of Bayesian neural networks to estimate 1We slightly modiÔ¨Åed the code using Bayesian neural networks.the uncertainty of predictions and indicate the potentially incorrect labels that should be reÔ¨Åned; 2) we propose a novel twostream selfattention reÔ¨Åning framework to better model different ranges of label dependencies and wordlabel interactions; 3) the proposed parallel decoding process can greatly speed up the inference process; and 4) the experimental results across three sequence labeling datasets indicate that the proposed method outperforms the other label decoding methods. 2 Related Work and Background "
245,Label-Noise Robust Multi-Domain Image-to-Image Translation.txt,"Multi-domain image-to-image translation is a problem where the goal is to
learn mappings among multiple domains. This problem is challenging in terms of
scalability because it requires the learning of numerous mappings, the number
of which increases proportional to the number of domains. However, generative
adversarial networks (GANs) have emerged recently as a powerful framework for
this problem. In particular, label-conditional extensions (e.g., StarGAN) have
become a promising solution owing to their ability to address this problem
using only a single unified model. Nonetheless, a limitation is that they rely
on the availability of large-scale clean-labeled data, which are often
laborious or impractical to collect in a real-world scenario. To overcome this
limitation, we propose a novel model called the label-noise robust
image-to-image translation model (RMIT) that can learn a clean label
conditional generator even when noisy labeled data are only available. In
particular, we propose a novel loss called the virtual cycle consistency loss
that is able to regularize cyclic reconstruction independently of noisy labeled
data, as well as we introduce advanced techniques to boost the performance in
practice. Our experimental results demonstrate that RMIT is useful for
obtaining label-noise robustness in various settings including synthetic and
real-world noise.","Imagetoimage translation is a problem in which the goal is to translate an image into the corresponding target image. Recently, this problem has been studied actively owing to its high potential for diverse applications, such as colorization [45, 94], super resolution [46, 43], image in painting [63, 29], photographic image synthesis [13, 85], and photo editing [99, 12, 33]. In particular, the introduc tion of generative adversarial networks (GANs) [21] has re sulted in signiÔ¨Åcant advances in this problem and allows for an imagetoimage translation model to be constructed in more challenging but practically important settings. Among them, a wellattended problem is multidomain imagetoimage translation where the goal is to learn map ping among multiple domains. This problem focuses on a dataset that contains multiple domains, such as the RaFD dataset [44] which contains eight facial expression labels (e.g., happy, angry, and sad) and the CelebA dataset [51] which includes 40 facial attribute labels (e.g., hair color, gender, and age). Given such a dataset, the aim of multi domain imagetoimage translation is to construct a genera tor that can translate an image among multiple domains ac cording to the given domain labels (e.g., expression labels and attribute labels). This problem is challenging in terms of scalability. In 1arXiv:1905.02185v1  [cs.CV]  6 May 2019particular, typical onetoone imagetoimage translation models (e.g., [77, 38, 100, 89, 50]) suffer from the difÔ¨Å culty because they require the learning of c(c"
214,NLNL: Negative Learning for Noisy Labels.txt,"Convolutional Neural Networks (CNNs) provide excellent performance when used
for image classification. The classical method of training CNNs is by labeling
images in a supervised manner as in ""input image belongs to this label""
(Positive Learning; PL), which is a fast and accurate method if the labels are
assigned correctly to all images. However, if inaccurate labels, or noisy
labels, exist, training with PL will provide wrong information, thus severely
degrading performance. To address this issue, we start with an indirect
learning method called Negative Learning (NL), in which the CNNs are trained
using a complementary label as in ""input image does not belong to this
complementary label."" Because the chances of selecting a true label as a
complementary label are low, NL decreases the risk of providing incorrect
information. Furthermore, to improve convergence, we extend our method by
adopting PL selectively, termed as Selective Negative Learning and Positive
Learning (SelNLPL). PL is used selectively to train upon expected-to-be-clean
data, whose choices become possible as NL progresses, thus resulting in
superior performance of filtering out noisy data. With simple semi-supervised
training technique, our method achieves state-of-the-art accuracy for noisy
data classification, proving the superiority of SelNLPL's noisy data filtering
ability.","Convolutional Neural Networks (CNNs) have improved the performance of image classiÔ¨Åcation signiÔ¨Åcantly [17, 8, 29, 11, 7, 38]. For this supervised task, huge dataset composed of images and their corresponding labels is re quired for training CNNs. CNNs are powerful tools for classifying images if the corresponding labels are correct. However, accurately labeling a large number of images is daunting and timeconsuming, occasionally yielding mis matched labeling. When the CNNs are trained with noisy data, it can overÔ¨Åt to such a dataset, resulting in poor classi Ô¨Åcation performance. Therefore, training CNNs properly with noisy data is of great practical importance. Many Figure 1: Conceptual comparison between Positive Learn ing(PL) and Negative Learning (NL). Regarding noisy data, while PL provides CNN the wrong information (red balloon), with a higher chance, NL can provide CNN the correct information (blue balloon) because a dog is clearly not a bird. approaches address this problem by applying a number of techniques and regularization terms along with Posi tive Learning (PL), a typical supervised learning method for training CNNs that ‚Äúinput image belongs to this la bel‚Äù [6, 2, 34, 20, 3, 39, 26, 30, 22, 33, 21]. However, when the CNN is trained with images and mismatched la bels, wrong information is being provided to the CNN. To overcome this issue, we suggest Negative Learning (NL), an indirect learning method for training CNN that ‚Äúinput image does not belong to this complementary la bel.‚Äù NL does not provide wrong information as frequently as PL (Figure 1). For example, when training CNN with noisy CIFAR10 using PL, if the CNN receives an image of a dog and the label ‚Äúcar‚Äù, the CNN will be trained to ac knowledge that this image is a car. In this case, the CNN is trained with wrong information. However, with NL, the CNN will be randomly provided with a complementary la bel other than ‚Äúcar,‚Äù for example, ‚Äúbird.‚Äù Training CNN to acknowledge that this image is not a bird is in some way an act of providing CNN the right information because a dog is clearly not a bird. In this manner, noisy data can contribute to training CNN by providing the ‚Äúright‚Äù infor mation with a high chance of not selecting a true label as a complementary label, whereas zero chance is provided inarXiv:1908.07387v1  [cs.LG]  19 Aug 2019PL. Our study demonstrates the effectiveness of NL as it prevents CNN from overÔ¨Åtting to noisy data. Furthermore, utilizing NL training method, we pro pose Selective Negative Learning and Positive Learning (SelNLPL), which combines PL and NL to take full ad vantage of both methods for better training with noisy data. Although PL is unsuitable for noisy data, it is still a fast and accurate method for clean data. Therefore, after train ing CNN with NL, PL begins to train CNN selectively using only training data of high classiÔ¨Åcation conÔ¨Ådence. Through this process, SelNLPL widens the gap between the conÔ¨Ådences of clean data and noisy data, resulting in excel lent performance for Ô¨Åltering noisy data from training data. Subsequently, by discarding labels of Ô¨Åltered noisy data and treating them as unlabeled data, we utilize semi supervised learning for noisy data classiÔ¨Åcation. Based on the superior Ô¨Åltering ability of SelNLPL, we demonstrate that stateoftheart performance on noisy data classiÔ¨Åca tion can be achieved with a simple semisupervised learn ing method. Although this is not the Ô¨Årst time that noisy data classiÔ¨Åcation has been addressed by Ô¨Åltering noisy data [2, 6, 24], the Ô¨Åltering results have not been promis ing owing to the use of PL for noisy data. The main contributions of this paper are as follows: We apply the concept of Negative Learning to the prob lem of noisy data classiÔ¨Åcation. We prove its applica bility by demonstrating that it prevents the CNN from overÔ¨Åtting to noisy data. Utilizing the proposed NL, we introduce a new frame work, called SelNLPL, for Ô¨Åltering out noisy data from training data. Following NL, by selectively applying PL only to training data of high conÔ¨Ådence, we can achieve accurate Ô¨Åltering of noisy data. We achieved stateoftheart noisy data classiÔ¨Åcation re sults with relatively simple semisupervised learning based on the superior noisy data Ô¨Åltering achieved by SelNLPL. Our method does not require any prior knowledge of the type or number of noisy data points. It does not require any tuning of hyperparameters that depend on prior knowledge, making our method applicable in real life. The remainder of this paper is organized as follows: Sec tion 3 describes the overall process of our method with de tailed explanations of each step. Section 4 demonstrates the superior Ô¨Åltering ability of SelNLPL. Section 5 describes the experiments for evaluating our method, and Section 6 describes the experiments to further analyze our method. Finally, we conclude the paper in Section 7.2. Related works "
125,Identification of Novel Classes for Improving Few-Shot Object Detection.txt,"Conventional training of deep neural networks requires a large number of the
annotated image which is a laborious and time-consuming task, particularly for
rare objects. Few-shot object detection (FSOD) methods offer a remedy by
realizing robust object detection using only a few training samples per class.
An unexplored challenge for FSOD is that instances from unlabeled novel classes
that do not belong to the fixed set of training classes appear in the
background. These objects behave similarly to label noise, leading to FSOD
performance degradation. We develop a semi-supervised algorithm to detect and
then utilize these unlabeled novel objects as positive samples during training
to improve FSOD performance. Specifically, we propose a hierarchical ternary
classification region proposal network (HTRPN) to localize the potential
unlabeled novel objects and assign them new objectness labels. Our improved
hierarchical sampling strategy for the region proposal network (RPN) also
boosts the perception ability of the object detection model for large objects.
Our experimental results indicate that our method is effective and outperforms
the existing state-of-the-art (SOTA) FSOD methods.","The adoption of deep neural network architectures in ob ject detection has led to a signiÔ¨Åcant method in determining the location and the category of objects of interest in an im age. In the presence of abundant training data, object de tection models based on the regionbased convolution neu ral networks (RCNN) architecture reach high accuracy on most object detection tasks. However, preparing largescale annotated training data can be a challenging task in some applications, e.g., miscellaneous disease analysis and indus trial defect detection. In the presence of insufÔ¨Åcient training data, these models easily overÔ¨Åt and fail to generalize well. In contrast, humans are able a novel object class very fast based on a few samples. As a result, it is extremely desirable to develop models that can learn object classes using only a Figure 1: FSOD methods pretrain a model on abundant base classes and then Ô¨Ånetune it on both base and novel classes. few samples, known as fewshot object detection (FSOD). Current FSOD methods are based on pretraining a suit able model on a set of base classes with abundant training data and then Ô¨Ånetuning the model on both the base classes and the novel classes for which only a few samples are ac cessible (see Figure 1). The primary approach in FSOD is to beneÔ¨Åt from ideas in transfer learning or metalearning to learn novel classes through the knowledge obtained dur ing the pretraining stage while maintaining good perfor mance in base classes. Despite recent advances in FSOD, current SOTA methods are still far from getting favorable results on novel classes similar to the base classes. Poten tial reasons for this performance gap include the confusion between visually similar categories, incorrect annotations (label noise), the existence of unseen novel objects during training, etc. Recent FSOD methods have focused on ad dressing these challenges for improved FSOD performance. We study the phenomenon that unlabeled novel object classes that do not belong to either of the base or the la beled novel classes can appear in the training data. For ex ample, we see in Figure. 1 that among baseclass training samples, there are a number of objects that remain unla beled, such as the cow in the image. These unlabeled ob jects can potentially belong to unseen novel classes. Our ex periments demonstrate that this phenomenon exists in PAS CAL VOC [4] and COCO [21] datasets.This phenomenon leads to the objectness inconsistency for the model whenarXiv:2303.10422v1  [cs.CV]  18 Mar 2023recognizing the novel objects: for the novel class, objects are treated as background if their annotations are missing, but they are treated as foreground where they are labeled. Such nonconformity of foreground and background con fuses the model when training the objectness and make the model hard to converge and degrades detection accuracy. To tackle the above challenge, we develop a semi supervised learning method to utilize the potential novel ob jects that appear during training to improve the ability of the model to recognize novel classes. We Ô¨Årst demonstrate the possibility of detecting these unlabeled objects. Our exper iment indicates that some unlabeled class objects are likely to be recognized if they are similar to the training base and novel classes. We collect the unlabeled novel objects from the background proposals by determining whether they are predicted as known classes, and then we give these propos als an extra objectness label in the region proposal network (RPN) so that the model could learn them. We also ana lyze the defect of the standard RPN in detecting objects of different sizes during training and propose a more balanced RPN sampling method so that objects are treated equally in all scales. We provide extensive experimental results to demonstrate the effectiveness of our method on the PAS CAL VOC and COCO datasets. Our contributions include: ‚Ä¢ We modify the anchor sampling strategy so that the anchors are evenly chosen from different layers of the feature pyramid layer in the RCNN architecture. ‚Ä¢ We design a ternary objectness classiÔ¨Åcation in the RPN layer which enables the model to recognize po tential novel class objects to improve consistency. ‚Ä¢ We use contrastive learning in the RPN layer to distin guish between the positive and the negative anchors. 2. Related works "
27,Hydra: an Ensemble of Convolutional Neural Networks for Geospatial Land Classification.txt,"We describe in this paper Hydra, an ensemble of convolutional neural networks
(CNN) for geospatial land classification. The idea behind Hydra is to create an
initial CNN that is coarsely optimized but provides a good starting pointing
for further optimization, which will serve as the Hydra's body. Then, the
obtained weights are fine-tuned multiple times with different augmentation
techniques, crop styles, and classes weights to form an ensemble of CNNs that
represent the Hydra's heads. By doing so, we prompt convergence to different
endpoints, which is a desirable aspect for ensembles. With this framework, we
were able to reduce the training time while maintaining the classification
performance of the ensemble. We created ensembles for our experiments using two
state-of-the-art CNN architectures, ResNet and DenseNet. We have demonstrated
the application of our Hydra framework in two datasets, FMOW and NWPU-RESISC45,
achieving results comparable to the state-of-the-art for the former and the
best reported performance so far for the latter. Code and CNN models are
available at https://github.com/maups/hydra-fmow","Land use is a critical piece of information for a wide range of applications, from humanitarian to military pur poses. For this reason, automatic land use classiÔ¨Åcation from satellite images has been drawing increasing attention from academia, industry and government agencies [1]. This research problem consists in classifying a target location in a satellite image as one of the classes of interest or as none of them. It is also common to have metadata associated with these images. Figure 1 illustrates a typical input data used for land use classiÔ¨Åcation. There are many factors that make the land classiÔ¨Åcation problem very challenging: Clutter: satellite images cover a large piece of land and may include a variety of elements ( e.g., objects, R. Minetto is with Universidade Tecnol¬¥ ogica Federal do Paran¬¥ a (UTFPR), Brazil. Email: rodrigo.minetto@gmail.com M. P . Segundo is with Universidade Federal da Bahia (UFBA), Brazil. Email: mauriciops@ufba.br S. Sarkar is with Department of Computer Science and Engineering, Uni versity of South Florida (USF), Tampa, FL, USA. Email: sarkar@usf.edu The research in this paper was conducted while the authors were at the Computer Vision and Pattern Recognition Group, USF.Spectral: 4/8 band multispectral images . . .  Temporal SpatialSpatialTextual metadata gsd: 0.44421135.. cloud cover: 25 date: 20130825 time 18:47:39 utm: 11S country: USA ... Textual metadata gsd: 0.57625807.. cloud cover: 0 date: 20170302 time 18:21:45 utm: 11S country: USA ... Fig. 1. Typical data associated with aerial images for land use classiÔ¨Å cation. It provides spatial, temporal, spectral and metadata information together with the annotation of target locations (example taken from the FMOW dataset [1]). buildings, and vegetation), making it hard to classify a target place. Viewpoint: aerial images can be acquired from dif ferent angles depending on the satellite location, which could considerably change the appearance of the imaged content. Occlusion: some parts of a target place may not be visible in aerial images due to viewpoint variations, cloud cover or shadows. Time: variations over time affect the appearance in different ways. Shortterm temporal variations include illumination changes and movable objects, such as vehicles and temporary facilities, while long term variations consist of major topographic changesarXiv:1802.03518v2  [cs.CV]  20 Mar 2019TO APPEAR IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, 2019 2 caused by construction, weather, among others. Scale: there is a huge variation in size between differ ent target types ( e.g., airport versus swimming pool) and sometimes even between multiple instances of the same target type ( e.g., a parking lot), which is not easy to process with a single approach. Functionality: sometimes targets with similar ap pearance may have completely different functions. For instance, a similar building structure could be used as an ofÔ¨Åce, a police station or an educational facility. Stateoftheart methods tackle these difÔ¨Åculties by train ing a single Convolutional Neural Network (CNN) classiÔ¨Åer over a large dataset of satellite images seeking to learn a generalizable model [1], [2], [3], [4], which were shown to outperform earlier works based on handcrafted features. Even though it is well known that ensembles of classi Ô¨Åers improve the performance of their individual counter parts [5], this solution was not exploited by these works, probably due to its high training cost for large datasets. However, ensembles tackle one of the most common prob lems in multiclass classiÔ¨Åcation, which is the existence of several critical points in the search space (saddle points and plateaus) that prioritize some classes over others and the eventual absence of a global minimum within the classiÔ¨Åer search space. An ensemble ends up expanding this space by combining multiple classiÔ¨Åers that converged to different endpoints and reaches a better global approximation. As our main contribution, we present a faster way of creating ensembles of CNNs for land use classiÔ¨Åcation in satellite images, which we called Hydra for its similarity in shape to the namesake mythical creature (see Figure 2). The idea behind Hydra is to create an initial CNN that is coarsely optimized but provides a good starting pointing for further optimization, which will serve as the Hydra‚Äôs body. Then, the obtained weights are Ô¨Ånetuned multiple times to form an ensemble of CNNs that represent the Hydra‚Äôs heads. We take certain precautions to avoid affecting the diversity of the classiÔ¨Åers, which is a key factor in ensemble performance. By doing so, we were able to cut the training time approximately by half while maintaining the classiÔ¨Å cation performance of the ensemble. Figure 2 illustrates this process, in which a black line represents the optimization of the body and the red lines represent the heads reaching different endpoints. To stimulate convergence to different endpoints during training and thus preserve diversity, we exploit different strategies, such as online data augmenta tion, variations in the size of the target region, and changing image spectra. 2 R ELATED WORK "
198,Instance-Aware Graph Convolutional Network for Multi-Label Classification.txt,"Graph convolutional neural network (GCN) has effectively boosted the
multi-label image recognition task by introducing label dependencies based on
statistical label co-occurrence of data. However, in previous methods, label
correlation is computed based on statistical information of data and therefore
the same for all samples, and this makes graph inference on labels insufficient
to handle huge variations among numerous image instances. In this paper, we
propose an instance-aware graph convolutional neural network (IA-GCN) framework
for multi-label classification. As a whole, two fused branches of sub-networks
are involved in the framework: a global branch modeling the whole image and a
region-based branch exploring dependencies among regions of interests (ROIs).
For label diffusion of instance-awareness in graph convolution, rather than
using the statistical label correlation alone, an image-dependent label
correlation matrix (LCM), fusing both the statistical LCM and an individual one
of each image instance, is constructed for graph inference on labels to inject
adaptive information of label-awareness into the learned features of the model.
Specifically, the individual LCM of each image is obtained by mining the label
dependencies based on the scores of labels about detected ROIs. In this
process, considering the contribution differences of ROIs to multi-label
classification, variational inference is introduced to learn adaptive scaling
factors for those ROIs by considering their complex distribution. Finally,
extensive experiments on MS-COCO and VOC datasets show that our proposed
approach outperforms existing state-of-the-art methods.","As a fundamental task in computer vision, multilabel image recog nition aims to accurately and simultaneously recognize multiple objects present in an image. Compared to singlelabel image clas sification, multilabel recognition is more challenging because of usually complex scene, more wide label space, and implicit corre lation of objects. In view of the natural cooccurrence of objects in the realworld scene, multilabel image classification is more practical than the singlelabel one, and has received wide attention [3, 10, 12, 20] in recent years. Numerous algorithms have been proposed for multilabel image classification. In early works, deep Convolutional Neural Networks ‚àóBoth authors contributed equally to this research. Sports  Ball Tennis  Racket Person Tie Bike Person Sports  Ball Tennis  Racket Person Tie Bike BikeP(Person )=1.0 P(Bike)=1.0Figure 1: We first construct the directed graph accord ing to the conditional probabilities of labels statistical co occurrence information from training set, and then predict the labels scores of regions extracted from each current im age. We apply the labels scores about regions to enhance their corresponding correlation in the directed graph, which means that the arrow lines among the predicted labels are bold. (CNNs) used for singlelabel recognition [ 13,14,25,26] are lever aged for the multilabel task by treating the multilabel recognition as a set of binary classification tasks. Although boosting the accu racy of multilabel classification, however, this type of methods is still limited due to the ignorance of cooccurrence among objects, which can be reflected in label correlation. To model label dependen cies, three lines of works have been proposed in recent literatures including attention mechanism based [ 26,30], recurrent neural network based [ 28], and graph based algorithms [ 6,18,19]. Specifi cally, in graph based methods, graph convolution is introduced to characterize label correlations by diffusing label dependencies with a label correlation matrix (LCM). With graph inference on labels, graph convolutional neural network (GCN) and its variants [ 6,29] have reported stateoftheart performances in recent literatures.arXiv:2008.08407v1  [cs.CV]  19 Aug 2020The success of GCN [ 16] indicates the significance of label cor relation captured through the LCM for promoting the multilabel classification. In previous GCN [ 6,29] based works, the LCM is global and datasetdependent as it is obtained through the statistic of accessible data. However, in view of the large variation among images, the prior knowledge of label correlation may not well suit for all samples. For instance, the statistical cooccurrence of the bike and person is low, which may mislead the classification for the images of riders. Therefore, individual characteristics of label correlation should also be considered, and used to adaptively mod ify the prior knowledge. Here, we attempt to construct an adaptive individual LCM for each image instance by utilizing the rough classification scores of ROIs. As shown in Figure 1, as an intuitive understanding, if all ROIs indicate high appearance probabilities of some labels, such as person and bike, then the correlation between person and bike in statistical LCM should be accordingly enhanced for this specific image. In this paper, we propose an instanceaware graph convolutional neural network (IAGCN) framework for multilabel classification. The core idea is to adaptively construct one imagedependent label correlation matrix (IDLCM) for each given image, which better favours the graph inference on labels. For framework construction, considering the previous success of GCNbased methods [ 6,29], we build two fused branches of subnetworks in the framework: a global branch modeling the whole image, and an additional region based branch inferring on ROIs. Moreover, graph inference on labels is conducted to inject labelawareness into the both branches. In this process, different from previous works using statistical LCM [ 6], an imagedependent LCM is constructed by fusing both the statistical LCM and an individual one of each image instance. Specifically, the individual LCM of each image is obtained by mining the label dependencies based on the scores of detected ROIs. Considering the contribution differences of ROIs to multilabel classification, during the generation of the individual LCM, variational inference [15] is introduced to learn adaptive scaling factors for those ROIs by considering their complex distribution. As a result, the image dependent LCM is flexible and benefits the proposed framework in adaptively propagating information on labels for each image instance. Finally, the learned features of both the global and local branches are fused and jointly modeled for the multilabel classifica tion. We test the performance on MSCOCO and VOC datasets, and the results show that our proposed approach outperforms existing stateoftheart methods. The main contributions of this paper are as follows: ‚Ä¢We propose a novel IAGCN framework for the multilabel classification task by jointly modeling the global context of the whole image and local dependencies of ROIs with adaptive information propagation on labels. ‚Ä¢A novel imagedependent LCM is constructed based on both the statistical LCM and an individual one of each image, which endows graph convolution flexibility to handle huge correlation variations among numerous image instances. ‚Ä¢We introduce variational inference to explore label depen dencies by considering the distribution of ROI appearances, which results in an adaptive LCM for each image instance.‚Ä¢We report the stateoftheart performances on both MS COCO and VOC datasets, which verifies the effectiveness of our framework. 2 RELATED WORK "
532,Transfer Learning in Conversational Analysis through Reusing Preprocessing Data as Supervisors.txt,"Conversational analysis systems are trained using noisy human labels and
often require heavy preprocessing during multi-modal feature extraction. Using
noisy labels in single-task learning increases the risk of over-fitting.
Auxiliary tasks could improve the performance of the primary task learning
during the same training -- this approach sits in the intersection of transfer
learning and multi-task learning (MTL). In this paper, we explore how the
preprocessed data used for feature engineering can be re-used as auxiliary
tasks, thereby promoting the productive use of data. Our main contributions
are: (1) the identification of sixteen beneficially auxiliary tasks, (2)
studying the method of distributing learning capacity between the primary and
auxiliary tasks, and (3) studying the relative supervision hierarchy between
the primary and auxiliary tasks. Extensive experiments on IEMOCAP and SEMAINE
data validate the improvements over single-task approaches, and suggest that it
may generalize across multiple primary tasks.","The sharp increase in uses of videoconferencing creates both a need and an opportunity to better understand these conversations (Kim et al., 2019a). In postevent applications, analyzing conversations can give feedback to improve communication skills (Hoque et al., 2013; Naim et al., 2015). In realtime applications, such systems can be useful in legal trials, public speaking, ehealth services, and more (Poria et al., 2019; Tanveer et al., 2015). Analyzing conversations requires both human expertise and a lot of time. However, to build au tomated analysis systems, analysts often require a training set annotated by humans (Poria et al., 2019). The annotation process is costly, thereby limiting the amount of labeled data. Moreover, thirdparty annotations on emotions are often noisy. Deep networks coupled with limited noisy labeleddata increases the chance of overÔ¨Åtting (James et al., 2013; Zhang et al., 2016). Could data be used more productively? From the perspective of feature engineering to analyze videoconferences, analysts often employ prebuilt libraries (Baltru≈°aitis et al., 2016; V oka turi, 2019) to extract multimodal features as inputs to training. This preprocessing phase is often com putationally heavy, and the resulting features are only used as inputs. In this paper, we investigate how the preprocessed data can be reused as aux iliary tasks which provide inductive bias through multiple noisy supervision (Caruana, 1997; Lip ton et al., 2015; Ghosn and Bengio, 1997) and consequently, promoting a more productive use of data. SpeciÔ¨Åcally, our main contributions are (1) the identiÔ¨Åcation of beneÔ¨Åcially auxiliary tasks, (2) studying the method of distributing learning capacity between the primary and auxiliary tasks, and (3) studying the relative supervision hierar chy between the primary and auxiliary tasks. We demonstrate the value of our approach through pre dicting emotions on two publicly available datasets, IEMOCAP (Busso et al., 2008) and SEMAINE (McKeown et al., 2011). 2 Related Works and Hypotheses "
182,Towards replacing precipitation ensemble predictions systems using machine learning.txt,"Precipitation forecasts are less accurate compared to other meteorological
fields because several key processes affecting precipitation distribution and
intensity occur below the resolved scale of global weather prediction models.
This requires to use higher resolution simulations. To generate an uncertainty
prediction associated with the forecast, ensembles of simulations are run
simultaneously. However, the computational cost is a limiting factor here.
Thus, instead of generating an ensemble system from simulations there is a
trend of using neural networks. Unfortunately the data for high resolution
ensemble runs is not available. We propose a new approach to generating
ensemble weather predictions for high-resolution precipitation without
requiring high-resolution training data. The method uses generative adversarial
networks to learn the complex patterns of precipitation and produce diverse and
realistic precipitation fields, allowing to generate realistic precipitation
ensemble members using only the available control forecast. We demonstrate the
feasibility of generating realistic precipitation ensemble members on unseen
higher resolutions. We use evaluation metrics such as RMSE, CRPS, rank
histogram and ROC curves to demonstrate that our generated ensemble is almost
identical to the ECMWF IFS ensemble.","Precipitation forecasting is an essential aspect of weather prediction, with signicant implica tions for various sectors, such as agriculture, transportation, water management, and disaster preparedness [23, 30]. In recent decades, the quality of numerical weather prediction has sig nicantly improved, with meteorological centers utilizing numerical models and reanalyses with grid spacing ranging from 10 to 80 km and updated in near realtime [7]. However, while these grid spacing are capable of resolving largescale weather patterns, they are insucient for accu rately representing precipitation in regions with subgridscale orographic variations [17]. Thus, precipitation intensity can exhibit signicant variations over short distances, with scales of 1 km or less, which is far ner than the typical resolution of global weather models. It is imperative 1arXiv:2304.10251v1  [physics.aoph]  20 Apr 2023to enhance the resolution of precipitation forecasts to accurately evaluate potential impacts, particularly for scenarios involving extreme rainfall. Moreover, ensemble weather prediction aims to quantify the dierent sources of uncertainty in numerical weather prediction models, cf. [20]. The most signicant sources of uncertainty are the initial conditions and errors in the numerical model formulation. To address these uncer tainties, an ensemble of perturbed forecasts is generated, in addition to the single deterministic weather forecast, with the overall divergence, or spread, of the ensemble ideally providing a measure of the uncertainty in the deterministic prediction. However, the main constraint in generating the ensemble is still computational, as each ensemble run requires signicant com putational resources, thereby limiting the total number of ensembles that can be computed on an operational basis, to typically less than 100. This article presents a generative deep learning approach to generating ensemble weather forecasts for highresolution precipitation forecasts, without being trained on highresolution data. We use generative adversarial networks (GANs) to learn the complex spatiotemporal patterns of precipitation and generate diverse and realistic precipitation elds. Thus, the main contributions of this paper are: ‚Ä¢Using machine learning to generate realistic precipitation ensemble members from using only the available control forecast; ‚Ä¢Showing that it is possible to generating realistic precipitation ensemble members on un seen (higher) resolutions using lower resolution training data. The remainder of this paper is organized as follows. In Section 2, we provide a brief overview of related work on deep learning in meteorology and ensemble forecasting for precipitation. Section 3 is devoted to the description of our proposed method using a generative deep learning approach. In Section 4 we present the results and verication of our approach. Finally, in Section 5, we present a summary of the work carried out and discuss future directions for research in this area. 2 Related work "
398,Meta Clustering Learning for Large-scale Unsupervised Person Re-identification.txt,"Unsupervised Person Re-identification (U-ReID) with pseudo labeling recently
reaches a competitive performance compared to fully-supervised ReID methods
based on modern clustering algorithms. However, such clustering-based scheme
becomes computationally prohibitive for large-scale datasets. How to
efficiently leverage endless unlabeled data with limited computing resources
for better U-ReID is under-explored. In this paper, we make the first attempt
to the large-scale U-ReID and propose a ""small data for big task"" paradigm
dubbed Meta Clustering Learning (MCL). MCL only pseudo-labels a subset of the
entire unlabeled data via clustering to save computing for the first-phase
training. After that, the learned cluster centroids, termed as meta-prototypes
in our MCL, are regarded as a proxy annotator to softly annotate the rest
unlabeled data for further polishing the model. To alleviate the potential
noisy labeling issue in the polishment phase, we enforce two well-designed loss
constraints to promise intra-identity consistency and inter-identity strong
correlation. For multiple widely-used U-ReID benchmarks, our method
significantly saves computational cost while achieving a comparable or even
better performance compared to prior works.","Ubiquitous cameras generate innumerable pedestrian data every day. Due to the growing demands on person reidentification (ReID) and its expensive labeling cost, unsupervised person ReID (UReID) [ 9, 12,14,22,30,31,33,40,49,63,65,67,78] has attracted increasing attention recently. There are mainly two categories in UReID. One is unsupervised domain adaptive (UDA) person ReID, which first pretrains a model on the labeled source dataset, and then finetunes the model on the unlabeled target dataset to reduce domain gap [ 11,35,55,58, 65,75,76]. Albeit effective, UDA ReID branch typically suffers from a complex adaptation process, and its success also relies on an assumption that the discrepancy between source and target domain is not significant. This motivates the exploration on the other branch, the clusteringbased unsupervised ReID [ 9,14,18,22, 36,54]. As the ‚ÄúPrevious work‚Äù shown in Figure 1, the works of this branch tend to perform an iterative optimization process of feature extraction‚Äìclustering‚Äìtrain . In this way, all unlabeled data can be explicitly leveraged with the pseudo labels generated by clustering. The focus of the recent clusteringbased methods lies in creating more reliable clusters and efficiently using them to learn discriminative representations, e.g., with the help of selfsimilarity grouping [ 18], hybrid memory bank with contrastive loss [ 22] or clusterlevel memory bank [ 9], multilabel classification [ 54], and online hierarchical cluster dynamics [66, 71]. However, these methods all neglect an important fact in prac tice: the clustering process costs an intolerable computational re sources due to its pairwise similarity calculation and neighboring samples searching. Taking the most common clustering algorithm DBScan [ 13] in UReID as example, its worst time complexity and space complexity are both ùëÇ(ùëõ2). When the size of unlabeled data is very large (as shown in Figure 1(a)), both of the memory and timearXiv:2111.10032v4  [cs.CV]  6 Aug 2022MM ‚Äô22, October 10‚Äì14, 2022, Lisboa, Portugal Xin Jin, Tianyu He, Xu Shen, Tongliang Liu, Xinchao Wang, Jianqiang Huang, Zhibo Chen, and XianSheng Hua Epoch0             10            20            30            40            50Ration of labeling errors (%) 0.0        0.2        0.4       0.6        0.8     Memory cost (MB) 0       5000    10000   15000   20000 0            15k          30k           45k          60k          75k Number of training imagesTime cost (s) 500     400       300       200       100         0 PersonX : 9.8k imgs 410IDs 449MB 33sMarket1501: 12.9k imgs 751IDs 723MB 43sMSMT17: 32.6k imgs 1041 IDs 4117MB 137sLaST : 71.2k imgs 5000IDs 22399MB 495s(a) (b)  (2) ClusteringPrevious  workOur work (2) Meta  Clustering Problems Figure 1: Motivation illustration for clusteringbased unsu pervised ReID: as the size of unlabeled data increases, the clustering process in previous works will (a) cost an intoler able computational resources in terms of memory and time costs, and (b) be more prone to be affected by pseudo label noise. Our work introduces a new metaclustering learning to achieve a satisfactory UReID performance while simul taneously tackling these two challenges. cost of clustering will rapidly increase. For example, performing clustering once on the LaST [ 45] (71.2k images) on the GPU follow ing previous works [ 9,22,30] will take a memory usage up to 22GB , which can not run on a 16GB Tesla V100. One may ask why not use the offline clustering (on CPU) or batchwise local clustering ( e.g., Kmeans) to avoid a large memory and time cost. This is due to the specificity of ReID: (1) the clusteringbased ReID needs iteratively perform the feature extraction and clustering in feature domain (on GPU) [ 9,22,54]; (2) the batchwise local clustering for ReID is suboptimal, which hinders the exploration and utilization of global relationship among largescale person data. In this paper, we attempt to achieve a largescale unsupervised ReID framework while taking the computational cost into account, which is challenging but valuable and meaningful to bridge the gap between ReID algorithms and practical applications. To this end, we propose a ‚Äúsmall data for big task‚Äù paradigm dubbed Meta Clustering Learning (MCL). Inspired by the other concept of Meta Learning [ 51‚Äì53] that are designed for ‚Äòlearning to learn‚Äô with the assistance of meta knowledge, our MCL first obtain the meta knowl edge on a part of the unlabeled person data and then softly extend the knowledge to the rest unlabeled ones. Therefore, it naturally avoids clustering the full/whole dataset before each training epoch and thus reduces the computation overhead. In addition, during the knowledge extension process, MCL further leverages a clustering free polishing step to enhance the discriminative representation learning while alleviating noisy label issue for ReID model. As illustrated in Figure 1, MCL consists of two phases of meta prototype optimization and prototypereferenced polishment (see Sec. 3.1, 3.2 for details). In the first phase, the features of the partialunlabeled images are extracted. This ratio can be flexibly deter mined according to the computing power of practical environment, as a byproduct of MCL. Then, a clustering algorithm, like DB Scan [ 13], is used to cluster features and generate pseudo ID labels. Based on them, the ReID model is trained with a memorybased optimization strategy [ 9,22,54]. Meanwhile, the clustered centroids (termed as metaprototypes) are stored in the memory and updated on the fly in a momentum manner [24]. The second prototypereferenced polishment is based on the learned metaprototypes in the previous phase, which are taken as a proxy annotator to mine the potential label information for the rest unlabeled data . For each unused person image, we get a soft real valued label likelihood vector by comparing it with metaprototypes reference. Based on such clusteringfree pseudo label, we further polish model by mining the relative comparative characteristic in person images. The reason why we call this phase as ‚Äúpolishment‚Äù is because ‚Äúpolish‚Äù has the meaning of try to perfect one‚Äôs skill, like here we promote the discriminative feature learning for ReID model with the rest unlabeled data . Another point should be noticed is that, the pseudo labeling it self no matter of clusteringbased or referencebased may generate wrong label predictions [ 20,30,54,60]. As shown in Figure 1(b), the larger size of unlabeled dataset, the more possible of generating noisy labels. To alleviate it, we further leverage two loss constraints for label denoising in MCL. One loss enforces instancelevel consis tency to reduce intraidentity variance and the other constructs a softweighted triplet constraint to promise interidentity correla tion. In this way, MCL could better investigate the discriminative information of data even with noisy pseudo labels. We summarize our main contributions as follows: ‚Ä¢To our best knowledge, this paper is the first to achieve the unsupervised largescale ReID training while considering the computational cost savings. A ‚Äúsmall data for big task‚Äù paradigm dubbed Meta Clustering Learning (MCL) is proposed. MCL per forms clusteringbased ReID training on partial unlabeled data, saving computing resources. ‚Ä¢To further leverage the rest unlabeled data, we take the learned prototypes from partial data as proxy annotator to pseudolabel them, and then polish model based on such pseudo labels with two welldesigned losses (as a minor contribution) to promise intraidentity consistency and interidentity strong correlation, which helps alleviate the noisy label issue. ‚Ä¢As the first attempt to handle the largescale unsupervised ReID, extensive experiments on multiple benchmarks show that MCL could significantly save computational cost while achieving a stateoftheart performance. In particular, MCL achieves ReID performance improvements of 4.8%, 2.9% in mAP on the large scale MSMT17 [ 58], LaST [ 45], but saves 71.8%/87.9% memory costs and 73.7%/85.7% time costs compared to the baselines. 2 RELATED WORK "
33,An Efficient Method of Training Small Models for Regression Problems with Knowledge Distillation.txt,"Compressing deep neural network (DNN) models becomes a very important and
necessary technique for real-world applications, such as deploying those models
on mobile devices. Knowledge distillation is one of the most popular methods
for model compression, and many studies have been made on developing this
technique. However, those studies mainly focused on classification problems,
and very few attempts have been made on regression problems, although there are
many application of DNNs on regression problems. In this paper, we propose a
new formalism of knowledge distillation for regression problems. First, we
propose a new loss function, teacher outlier rejection loss, which rejects
outliers in training samples using teacher model predictions. Second, we
consider a multi-task network with two outputs: one estimates training labels
which is in general contaminated by noisy labels; And the other estimates
teacher model's output which is expected to modify the noise labels following
the memorization effects. By considering the multi-task network, training of
the feature extraction of student models becomes more effective, and it allows
us to obtain a better student model than one trained from scratch. We performed
comprehensive evaluation with one simple toy model: sinusoidal function, and
two open datasets: MPIIGaze, and Multi-PIE. Our results show consistent
improvement in accuracy regardless of the annotation error level in the
datasets.","Recent development of deep neural network (DNN) re search allows us to solve many kinds of problems with veryhigh accuracy, such as classiÔ¨Åcation, regression, and obje ct detection. To enhance DNNs‚Äô accuracy, a straightforward method is just increasing depth and channel of the network, and a considerable amount of studies have been conducted on Ô¨Ånding a method to train deeper networks effectively. Their large memory and numerical costs, however, prohibits us to apply them to realworld solutions. To alleviate this problem, many techniques have been proposed, e.g., Ô¨Ånding efÔ¨Åcient network structures [16, 27], channel pruning [14] , and quantization of network weights [17]. Knowledge dis tillation is one of the most popular methods for this purpose which tries to mimic the behavior of deeper and larger mod els (teacher) by a smaller or compressed model (student) [6, 3, 15]. Although seminal work have followed to improve the technique, those work mainly focused on the classiÔ¨Åca tion problem. On the other hand, little attention has been given on regression problems which also have many appli cations, for example, estimating age [11], gaze angle [26], and so on. Though some techniques of the above work can also be applied to regression problems, it has another in herent difÔ¨Åculty, that is, the uncertainty of giving annota  tion. In general, regression problems treat continuous var i ables as annotation, and it is unavoidable to accept a certai n amount of annotation error which originates from human errors and limitations of measurement. Since information from teacher models can also be an origin of those errors, it is necessary to consider the treatment of these errors when developing knowledge distillation for regression problem s, which no existing work tackled as far as we know. To address the above problems, we propose a method to train a fast and accurate student networks for regression problems using a newly developed knowledge distillation method. Our contributions are as follows: ‚Ä¢We propose a new formulation for knowledge distillation for regression problems which solves regression problems using a multitask network (Section 3). ‚Ä¢We propose a new loss, socalled Teacher Outlier Re jection (TOR) Loss which allows the student models to avoid suffering from outliers with the help of teacher models (Section 4.1). ‚Ä¢We present insights into the nature of regression prob lems with noisy data which was obtained from com prehensive numerical experiments (Section 5). 2 Related Work "
439,Automatic Language Identification for Celtic Texts.txt,"Language identification is an important Natural Language Processing task. It
has been thoroughly researched in the literature. However, some issues are
still open. This work addresses the identification of the related low-resource
languages on the example of the Celtic language family.
  This work's main goals were: (1) to collect the dataset of three Celtic
languages; (2) to prepare a method to identify the languages from the Celtic
family, i.e. to train a successful classification model; (3) to evaluate the
influence of different feature extraction methods, and explore the
applicability of the unsupervised models as a feature extraction technique; (4)
to experiment with the unsupervised feature extraction on a reduced annotated
set.
  We collected a new dataset including Irish, Scottish, Welsh and English
records. We tested supervised models such as SVM and neural networks with
traditional statistical features alongside the output of clustering,
autoencoder, and topic modelling methods. The analysis showed that the
unsupervised features could serve as a valuable extension to the n-gram feature
vectors. It led to an improvement in performance for more entangled classes.
The best model achieved a 98\% F1 score and 97\% MCC. The dense neural network
consistently outperformed the SVM model.
  The low-resource languages are also challenging due to the scarcity of
available annotated training data. This work evaluated the performance of the
classifiers using the unsupervised feature extraction on the reduced labelled
dataset to handle this issue. The results uncovered that the unsupervised
feature vectors are more robust to the labelled set reduction. Therefore, they
proved to help achieve comparable classification performance with much less
labelled data.","Language IdentiÔ¨Åcation (LI) approaches the problem of automatic recognition of speciÔ¨Åc natural languages [Jauhiainen et al., 2018]. LI has multiple applications in the modern world. The original use case is routing source documents to languageappropriate Natural Language Processing (NLP) components, such as machine translation and dialogue systems [Zhang et al., 2018]. Most of the other NLP methods assume input text is monolingual; in that case, routing of the input documents is crucial. Moreover, LI is a valuable component of the corpus creation pipelines, especially for the lowresource languages. Text crawled from the web resources usually needs to be separated by language to create a meaningful corpus. LI can be a relatively easy task for the most popular languages with the abundance of available resources. Nevertheless, there are languages with fewer native speakers and less research focus; such languages are considered lowresource. Lowresource languages and related languages continuously pose a problem for language identiÔ¨Åcation [He et al., 2018,arXiv:2203.04831v1  [cs.CL]  9 Mar 2022Automatic Language IdentiÔ¨Åcation for Celtic Texts A P REPRINT Jauhiainen et al., 2018]. This paper focuses on applying language identiÔ¨Åcation methods to a family of lowresource languages on the example of the Celtic language group combining the two issues. The main problem with the lowresource languages is the unavailability of highquality corpora, which is costly to prepare. Language IdentiÔ¨Åcation faces this problem same as other NLP tasks. Thus, in our research, we created a corpus of three Celtic languages. It contains Irish, Scottish, and Welsh texts. Moreover, the corpus is extended with a small proportion of English samples because these languages are usually mixed in the common usage. This work also focuses on the preparation of a language identiÔ¨Åcation model alongside the evaluation and analysis of the different feature extraction methods. In particular, we explored the possibility of the application of the output of the unsupervised learning models as a feature representation for the classiÔ¨Åcation task. The unsupervised approach was promising in the case of scarce annotated data as here. The unsupervised models operate on the unlabelled data and so enhance the classiÔ¨Åcation model‚Äôs performance on a limited labelled dataset. To solve the lack of large corpora for lowresource languages, we proved that the features extracted from unsupervised methods ensure high performance on the reduced labelled set size. Consequently, this study validates that it is possible to save resources on data annotation, determine the language class of lowresource languages more efÔ¨Åciently, and ensure that the data for further processing is of high quality. In this study, our main contributions are: 1. collecting the dataset of three Celtic languages (Irish, Scottish, Welsh) and English (Section 3), 2.preparing a method to identify these languages from the Celtic family and differentiate them from English (Section 4), 3.evaluating the inÔ¨Çuence of different feature extraction methods and exploring the applicability of unsupervised models as a feature extraction technique (Section 6), 4. experimenting with unsupervised feature extraction on a reduced annotated set (Section 6.3). In the following sections, we discussed the related research (Section 2), the dataset creation process (Section 3), the proposed approach (Section 4), the results of the experiments (Section 6), the list of conclusions (Section 7) and directions for future research (Section 8). 2 Related Work "
357,Fully automatic scoring of handwritten descriptive answers in Japanese language tests.txt,"This paper presents an experiment of automatically scoring handwritten
descriptive answers in the trial tests for the new Japanese university entrance
examination, which were made for about 120,000 examinees in 2017 and 2018.
There are about 400,000 answers with more than 20 million characters. Although
all answers have been scored by human examiners, handwritten characters are not
labelled. We present our attempt to adapt deep neural network-based handwriting
recognizers trained on a labelled handwriting dataset into this unlabeled
answer set. Our proposed method combines different training strategies,
ensembles multiple recognizers, and uses a language model built from a large
general corpus to avoid overfitting into specific data. In our experiment, the
proposed method records character accuracy of over 97% using about 2,000
verified labelled answers that account for less than 0.5% of the dataset. Then,
the recognized answers are fed into a pre-trained automatic scoring system
based on the BERT model without correcting misrecognized characters and
providing rubric annotations. The automatic scoring system achieves from 0.84
to 0.98 of Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents
acceptable similarity of scoring between the automatic scoring system and the
human examiners. These results are promising for further research on end-to-end
automatic scoring of descriptive answers.","Descriptive answers are better to evaluate learner‚Äôs understanding and problem solving ability.  They encourage learners to think rather than select. However, scoring them requires large work  and time. In recent years, it was proposed to add descriptive questions in the new university  entrance common examinations in Japan as well as the current multiplechoice questions [1], but  given up due to the short period of scoring handwritten answers and the anxiety about reliable  scoring.   One approach is to score handwritten descriptive answers automatically and feedback scores to  examinees and examiners to correct scoring errors. Another approach is to apply automatic scoring  or clustering them for human examiners to score them efficiently and reliably [2], [3]. For both of  them, handwritten answers need to be recognized and scored.   A few datasets storing handwritten answers have been published and used in research on  handwriting recognition, such as SCUTEPT (Chinese handwritten answers) [4] and DsetMix,  which is artificially prepared by synthesized handwritten math answers [5]. Note that these  datasets are all fully labelled, ideally suited to train handwriting recognizers based on deep neural  networks.  The National Center for University Entrance Examinations (NCUEE) conducted trial tests for the  new university entrance common exams with 64,518 and 67,745 examinees in 2017 and 2018,  respectively. Three descriptive questions were included in the Japanese language test for each trial  test. Their handwritten answers were scored by human examiners. The scanned images and scores  are used here.  However, the offline images are only raw images and have not been segmented or labelled. It is  infeasible to label a whole set of scanned handwritten answers. On the other hand, automatic  pattern recognition methods, especially wellknown deep neural networks require largescale  labelled data for training. In this paper, we present normalization, segmentation, and handwriting  recognition from this handwritten answer set and their automatic scoring. In particular, we focus  mainly on training the handwriting recognizer to adapt to the actual data from the examinee‚Äôs  answers. In addition, we also incorporated the language model to rerank the predicted candidates  so that the ambiguous patterns are corrected by linguistic context. In summary, the main  contributions are as follows:  ‚Ä¢ We present an ensemble deep neural networkbased recognizer for offline Japanese  handwritten answer recognition.  ‚Ä¢ We propose a training procedure with multiple steps (pretraining, finetuning and  ensemble learning) to adapt the ensembled handwriting recognizer to real patterns.  ‚Ä¢ We evaluate the handwriting recognizer in combination with the latest automatic scoring  system.  ‚Ä¢ The combined architecture without correction of misrecognized characters and rubric  annotations scores handwritten answers as almost the same as human examiners.  2 Related works  "
379,Multi-label Learning Based Deep Transfer Neural Network for Facial Attribute Classification.txt,"Deep Neural Network (DNN) has recently achieved outstanding performance in a
variety of computer vision tasks, including facial attribute classification.
The great success of classifying facial attributes with DNN often relies on a
massive amount of labelled data. However, in real-world applications, labelled
data are only provided for some commonly used attributes (such as age, gender);
whereas, unlabelled data are available for other attributes (such as
attraction, hairline). To address the above problem, we propose a novel deep
transfer neural network method based on multi-label learning for facial
attribute classification, termed FMTNet, which consists of three sub-networks:
the Face detection Network (FNet), the Multi-label learning Network (MNet) and
the Transfer learning Network (TNet). Firstly, based on the Faster Region-based
Convolutional Neural Network (Faster R-CNN), FNet is fine-tuned for face
detection. Then, MNet is fine-tuned by FNet to predict multiple attributes with
labelled data, where an effective loss weight scheme is developed to explicitly
exploit the correlation between facial attributes based on attribute grouping.
Finally, based on MNet, TNet is trained by taking advantage of unsupervised
domain adaptation for unlabelled facial attribute classification. The three
sub-networks are tightly coupled to perform effective facial attribute
classification. A distinguishing characteristic of the proposed FMTNet method
is that the three sub-networks (FNet, MNet and TNet) are constructed in a
similar network structure. Extensive experimental results on challenging face
datasets demonstrate the effectiveness of our proposed method compared with
several state-of-the-art methods.","Facial attribute classiÔ¨Åcation is an important and fundamen tal research area in computer vision and pattern recognition. The task of facial attribute classiÔ¨Åcation is to predict the at tributes of a facial image, including gender, attraction, race, etc. Recently, facial attribute classiÔ¨Åcation has received increasing attention with a wide range of applications, such as face ver iÔ¨Åcation [1, 2, 3], face recognition [4, 5, 6], face retrieval [7]. However, it remains a challenging problem, because of the large facial appearance variations caused by pose, illumination and occlusion, etc. Early works on facial attribute classiÔ¨Åcation usually charac terize the facial attributes based on the histogram representation [2, 3, 8]. For example, Kumar et al. [2] propose to Ô¨Årstly ex tract the lowlevel features from di erent regions of a face, and then predict facial attributes with the Support Vector Machine (SVM) for face veriÔ¨Åcation. Cherniavsky et al. [8] develop a generative facial feature representation method based on the Haarlike features and investigate a semisupervised method to predict facial attributes with SVM. Corresponding author. Tel.: +865922580063 Email address: yanyan@xmu.edu.cn (Yan Yan)Recent research mainly focuses on using the Deep Neural Network (DNN) to predict facial attributes. Luo et al. [9] combine discriminative decision trees with the deep Sum Product Network (SPN) for facial attribute classiÔ¨Åcation. In [10, 11, 12], the authors Ô¨Årstly extract facial features using DNN and then classify facial attributes with SVM. Ehrlich et al. [13] learn the shared feature representation for facial attributes by directly operating on faces and facial landmark points. Rudd et al. [14] address the problem of imbalanced data to predict multiple facial attributes. Generally speaking, methods for facial attribute classiÔ¨Åca tion can be divided into two categories: singlelabel learn ing based methods [2, 8, 10, 11, 12] and multilabel learning based methods [13, 14]. The singlelabel learning based meth ods predict facial attributes separately and thus do not con sider the correlation between facial attributes. In contrast, the multilabel learning based methods, which attempt to predict facial attributes simultaneously by using labelled data, have drawn increasing attention. However, in realworld applica tions, only some commonly used attributes are provided with labelled information, while the other attributes have unlabelled data. Therefore, these methods [13, 14] fail to deal with the fa cial attribute classiÔ¨Åcation problem when unlabelled informa tion is available (recall that these methods are based on superarXiv:1805.01282v1  [cs.CV]  3 May 2018vised learning). Motivated by the above observations, we propose a novel facial attribute classiÔ¨Åcation method, which performs transfer learning based on multilabel learning. More speciÔ¨Åcally, we take advantage of the transfer DNN technique to predict facial attributes that do not have labelled information in the target domain. To e ectively exploit the labelled data in the source domain, we use the multilabel learning technique to predict multiple facial attributes simultaneously, considering the cor relation between facial attributes. Fig. 1 shows an illustration of the correlation between di erent facial attributes. For di er ent learning problems, some carefully designed networks are used, where these networks share the same structure at the for mer layers of the networks and they only di er at the latter lay ers. Therefore, the networks can be e ectively trained via Ô¨Åne tuning. In this paper, we propose an e ective deep transfer neu ral network method, termed FMTNet, which consists of three subnetworks for facial attribute classiÔ¨Åcation. The Ô¨Årst sub network is the Face detection Network (FNet) for face detec tion. FNet is initialized by using the model learned from a large scale ImageNet dataset [15], and then is Ô¨Ånetuned by using the facial images. The second subnetwork is the Multilabel learning Network (MNet) for facial attribute classiÔ¨Åcation with supervised learning, where multiple attributes are predicted si multaneously. Based on FNet, MNet is Ô¨Ånetuned by using labelled attributes in the source domain. The network struc tures at the former layers of both MNet and FNet are the same, whereas the main di erence is that multiple fullyconnected layers are independently constructed in MNet. The third sub network is the Transfer learning Network (TNet) for facial at tribute classiÔ¨Åcation, when labelled information is not available in the target domain. Based on MNet, TNet makes use of un supervised domain adaptation to improve the performance of facial attribute classiÔ¨Åcation. The main contributions of this paper are summarized as fol lows: (1) Instead of using singlelabel learning for each attribute [4, 7], the proposed method e ectively performs facial attribute classiÔ¨Åcation based on multilabel learning for the labelled at tributes in the source domain. Especially, we propose an ef fective loss weight scheme to explicitly exploit the correlation between facial attributes based on attribute grouping, which can signiÔ¨Åcantly improve the generalization performance of the proposed method. (2) Based on multilabel learning, the proposed method leverages transfer learning to predict facial attributes for the unlabelled attributes in the target domain. The transfer neural network successfully transfers the features from the source do main (with labelled information) to the target domain (without labelled information), even when the probability distributions between the two domains are signiÔ¨Åcantly di erent. Therefore, the proposed method alleviates the dependency on fully labelled training data, especially in the absence of labelled information for some attributes. The remainder of the paper is organized as follows: In Sec tion 2, some related work is discussed. In Section 3, the detailsof the proposed FMTNet method for facial attribute classiÔ¨Åca tion are described. In Section 4, the experimental results are reported. In Section 5, the conclusions are presented. 2. Related Work "
57,Multi-label Iterated Learning for Image Classification with Label Ambiguity.txt,"Transfer learning from large-scale pre-trained models has become essential
for many computer vision tasks. Recent studies have shown that datasets like
ImageNet are weakly labeled since images with multiple object classes present
are assigned a single label. This ambiguity biases models towards a single
prediction, which could result in the suppression of classes that tend to
co-occur in the data. Inspired by language emergence literature, we propose
multi-label iterated learning (MILe) to incorporate the inductive biases of
multi-label learning from single labels using the framework of iterated
learning. MILe is a simple yet effective procedure that builds a multi-label
description of the image by propagating binary predictions through successive
generations of teacher and student networks with a learning bottleneck.
Experiments show that our approach exhibits systematic benefits on ImageNet
accuracy as well as ReaL F1 score, which indicates that MILe deals better with
label ambiguity than the standard training procedure, even when fine-tuning
from self-supervised weights. We also show that MILe is effective reducing
label noise, achieving state-of-the-art performance on real-world large-scale
noisy data such as WebVision. Furthermore, MILe improves performance in class
incremental settings such as IIRC and it is robust to distribution shifts.
Code: https://github.com/rajeswar18/MILe","Largescale datasets with humanannotated labels have been central to the development of modern stateoftheart neural networkbased artiÔ¨Åcial perception systems [ 25,26, 34]. Improved performance on ImageNet [ 18] has led to remarkable progress in tasks and domains that leverage Im ageNet pretraining [ 12,45,73]. However, these weakly annotated datasets and models tend to project a rich, multi label reality into a paradigm that envisions one and only one label per image. This form of simpliÔ¨Åcation often hinders *Equal contribution. // Teacher Student K t K s BCE  loss multilabel  prediction bottleneck BCE  loss singlelabel  groundtruth Copy weights stop gradient Human Car Tree HouseFigure 1. Multilabel Iterated Learning (MILe) builds a multi label representation of the images from singlylabeled groundtruth. In this example, a model produces multilabel binary predictions for the next generation, obtaining Car andHouse for an image weakly labeled with House . model performance by asking models to predict a single la bel, when trained on realworld images that contain multiple objects. Given the importance of the problem, there is growing recognition of singlelabeled datasets as a form of weak supervision and an increasing interest in evaluating the lim its of these singlylabeled benchmarks. A series of recent studies [ 9,57,59,62,68] highlight the problem of label ambiguity in ImageNet. In order to obtain a better estimate of model performance, Beyer et al. [9]and Shankar et al. [57] introduced multilabel evaluation sets. They identiÔ¨Åed softmax crossentropy training as one of the main reasons for low multilabel performance since it promotes label ex clusiveness. They also showed that replacing the softmax with sigmoid activations and casting the output as a set of binary classiÔ¨Åers results in better multilabel validation per formance. Several other studies have explored ways to over come the shortcomings in existing validation procedures by improving the pipelines for gathering labels [6, 51, 61]. In order to obtain a more complete description of images from weaklysupervised or semisupervised data, a number 1arXiv:2111.12172v1  [cs.CV]  23 Nov 2021of methods leverage a noisy signal such as pseudolabels [ 68] or textual descriptions crawled from the web [ 50]. In this work, we observe that the process of building a rich repre sentation of data from a noisy source shares some properties with the process of language emergence studied in the cog nitive science literature. In particular, Kirby [31] proposed that structured language emerged from an intergenerational iterated learning process [ 31,32,33]. According to the theory, a compositional syntax emerges when agents learn by imitation from previous generations in the presence of a learning bottleneck. This bottleneck forces noisy fragments of the language to be forgotten when transmitted to new generations. Conversely, those fragments that can be reused and composed to enrich the language tend to be passed to subsequent generations. We show that the same procedure can be applied to settings that leverage a weak or noisy su pervisory signal such as [ 50,68] to build a richer description of images while reducing the noise. In this work, we propose multilabel iterated learning (MILe) to learn to predict rich multilabel representations from weakly supervised (singlelabeled) training data. We do so by introducing two different learning bottlenecks. First, we replace the standard convolutional neural network out put softmax with a hard multilabel binary prediction. Sec ond, we transmit these binary predictions through successive model generations, with a limited training iterations between each generation. In our experiments, we demonstrate that MILe allevi ates the label ambiguity problem by improving the F1 score of supervised and selfsupervised models on the ImageNet ReaL [ 9] multilabel validation set. In addition, experiments on WebVision [ 40] show that iterated learning increases ro bustness to label noise and spurious correlations. Finally, we show that our approach can help in continual learning scenarios such as IIRC [ 1] where newly introduced labels cooccur with known labels. Our contributions are: ‚Ä¢We propose MILe, a multilabel iterated learning algo rithm for image classiÔ¨Åcation that builds a rich multi label representation of data from weak single labels. ‚Ä¢We show that models trained with MILe are more robust to noise and perform better on ImageNet, ImageNet ReaL, WebVision, and multiple setups such as super vised learning (Section 4.1), outofdistribution gener alization (Section 4.2), selfsupervised Ô¨Ånetuning and semisupervised learning (Section 4.3), and continual learning. ‚Ä¢We provide insights on the predictions made by models trained with iterated learning (Section 4.4). 2. Related Work "
601,Cell Detection from Imperfect Annotation by Pseudo Label Selection Using P-classification.txt,"Cell detection is an essential task in cell image analysis. Recent deep
learning-based detection methods have achieved very promising results. In
general, these methods require exhaustively annotating the cells in an entire
image. If some of the cells are not annotated (imperfect annotation), the
detection performance significantly degrades due to noisy labels. This often
occurs in real collaborations with biologists and even in public data-sets. Our
proposed method takes a pseudo labeling approach for cell detection from
imperfect annotated data. A detection convolutional neural network (CNN)
trained using such missing labeled data often produces over-detection. We treat
partially labeled cells as positive samples and the detected positions except
for the labeled cell as unlabeled samples. Then we select reliable pseudo
labels from unlabeled data using recent machine learning techniques;
positive-and-unlabeled (PU) learning and P-classification. Experiments using
microscopy images for five different conditions demonstrate the effectiveness
of the proposed method.","Cell detection is an essential task in cell image analysis, which has been widely used for cell counting and cell tracking. Many image processing based methods have been proposed for automatically detecting cells e.g., using a threshold ing [13,24,3], region growing [25] and graph cuts [1]. These methods are usually designed on the basis of image characteristics, so they may only work under certain conditions. Recently, deep learningbased detection methods have shown to be eective for various types of cells if they are trained on enough training data for specic conditions [22,10,6,12]. Moreover, deep learningbased methods usually require fully annotating all the cells in an entire image for the network to learn both the foreground (cell) and the background area. If some of the cells are not anno tated ( i.e., imperfect annotation), the nonannotated cell regions are mistakenlyarXiv:2107.09289v2  [cs.CV]  21 Jul 20212 K. Fujii et al. treated as background regions in training. The detection performance signi cantly degrades due to such noisy labels. However, it is very costly to annotate all the cells in an image since there are as many as hundreds or thousands of cells in an image. Therefore, some of the current public datasets only provide partially annotated cells (imperfect annotation). The aim of our study is to make cell detection feasible from imperfect an notated data by using pseudo labeling. This would enable the use of imperfect datasets, which have already been made public, and reduce the annotation cost for biologists in real applications. In the proposed method, we rst create a mask that covers only the annotated cells, in which the loss is ignored outside the mask. The masked loss facilitates the reduction of false negatives but produces many false positives since it learns part of the foreground (cell) region but not the background. When the detector is applied to the dataset that contains par tially labeled cells (positive data), the detected positions excluding the labeled cell can be considered unlabeled data, which may contain both positive (cell) and negative (background) positions. To address the overdetection problem, we propose a semisupervised method that selects reliable background as pseudo negative labels and additional foreground regions as pseudo positive labels from the unlabeled data and adds these to the training data in the next step. We applied positive and unlabeled (PU) learning [9] to extract the optimal image features that separate the feature distribution of positive (cell) and negative la bel (background). In order to minimize the risk of selecting incorrect labels as the pseudo labels, we performed ranking learning using Pclassication [5] which aims to learn a ranking (scoring) function so that reliable positive samples are ranked higher. These processes are iteratively performed. This improves the per formance of the detector network by adding reliable labels for both negative and positive positions. The experiments using microscopy images for ve dierent conditions demonstrate the eectiveness of the proposed method. 2 Related work "
489,How to best use Syntax in Semantic Role Labelling.txt,"There are many different ways in which external information might be used in
an NLP task. This paper investigates how external syntactic information can be
used most effectively in the Semantic Role Labeling (SRL) task. We evaluate
three different ways of encoding syntactic parses and three different ways of
injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling
model. We show that using a constituency representation as input features
improves performance the most, achieving a new state-of-the-art for
non-ensemble SRL models on the in-domain CoNLL'05 and CoNLL'12 benchmarks.","Properly integrating external information into neu ral networks has received increasing attention re cently (Wu et al., 2018; Li et al., 2017; Strubell et al., 2018). Previous research on this topic can be roughly categorized into three classes: i) In put: The external information are presented as ad ditional input features (i.e., dense realvalued vec tors) to the neural network (Collobert et al., 2011). ii) Output : The neural network is trained to pre dict the main task and the external information in a multitask approach (Changpinyo et al., 2018). iii) Autoencoder : This approach, recently proposed by Wu et al. (2018), simultaneously combines the Input andOutput during neural models training. The simplicity of these methods allow them to ap ply to many NLP sequence tasks and various neu ral model architectures. However, previous studies often focus on inte grating wordlevel shallow features such as POS or chunk tags into the sequence labelling tasks. Syntactic information, which encodes the long range dependencies and global sentence structure, has not been studied as carefully. This paper Ô¨Ålls 1Our model source code is available in https:// github.com/GaryYufei/bestParseSRLthis gap by integrating syntactic information to the sequence labelling task. We address three ques tions: 1)How should syntactic information be en coded as wordlevel features? 2)What is the best way of integrating syntactic information? and3) What effect does the choice of syntactic represen tation have on the performance? We study these questions in the context of Se mantic Role Labelling (SRL). A SRL system ex tracts the predicateargument structure of a sen tence.2Syntax was an essential component of early SRL systems (Xue and Palmer, 2004; Pun yakanok et al., 2008). The stateoftheart neu ral SRL systems use a neural sequence labelling model without any syntax knowledge (He et al., 2018, 2017; Tan et al., 2018). We show below that injecting external syntactic knowledge into a neu ral SRL sequence labelling model can improve the performance, and our best model sets a new state oftheart for a nonensemble SRL system. In this paper we express the external syntac tic information as vectors of discrete features, be cause this enables us to explore different ways of injecting the syntactic information into the neural SRL model. SpeciÔ¨Åcally, we propose three dif ferent syntax encoding methods: a)a full con stituency tree representation ( FullC );b)an SRL speciÔ¨Åc span representation ( SRLC ); and c)a dependency tree representation ( Dep). For (a) we adapt the constituency parsing representation from (G ¬¥omezRodr ¬¥ƒ±guez and Vilares, 2018) and encode the tree structure as a set of features for word pairs. For (b), we use a categorical repre sentation of the constituency spans that are most relevant to SRL tasks based on (Xue and Palmer, 2004). Finally, (c)we propose a discrete vector representation that encodes the headmodiÔ¨Åer re lationships in the dependency trees. We evaluate the effectiveness of these encod ings using three different integration methods on 2who didwhat towhom ,where andwhenarXiv:1906.00266v1  [cs.CL]  1 Jun 2019the SRL CoNLL‚Äô05 andCoNLL‚Äô12 benchmarks. We show that using either of the constituency representations in either the Input or the Auto Encoder conÔ¨Ågurations produces the best perfor mance. These results are noticeably better than a strong baseline and set a new stateoftheart for nonensemble SRL systems. 2 Related Work "
278,Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition.txt,"This paper describes our approach to the DSTL Satellite Imagery Feature
Detection challenge run by Kaggle. The primary goal of this challenge is
accurate semantic segmentation of different classes in satellite imagery. Our
approach is based on an adaptation of fully convolutional neural network for
multispectral data processing. In addition, we defined several modifications to
the training objective and overall training pipeline, e.g. boundary effect
estimation, also we discuss usage of data augmentation strategies and
reflectance indices. Our solution scored third place out of 419 entries. Its
accuracy is comparable to the first two places, but unlike those solutions, it
doesn't rely on complex ensembling techniques and thus can be easily scaled for
deployment in production as a part of automatic feature labeling systems for
satellite imagery analysis.","The signiÔ¨Åcant increase of satellite imagery has given a radically improved understanding of our planet. Object recognition in aerial imagery enjoys growing interest to day, due to the recent advancements in computer vision and deep learning, along with important improvements in lowcost highperformance GPUs. The possibility of ac curately distinguishing different types of objects in aerial images, such as buildings, roads, vegetation and other cate gories, could greatly help in many applications, such as cre ating and keeping uptodate maps, improving urban plan ning, environment monitoring, and disaster relief. Besides the practical need for accurate aerial image interpretation systems, this domain also offers scientiÔ¨Åc challenges to the computer vision. In this paper, we describe and analyze these challenges for the speciÔ¨Åc satellite imagery dataset from a Kaggle com petition. We explore the challenges faced due to the small size of the dataset, the speciÔ¨Åc character of data, and super vised and unsupervised machine learning algorithms thatare suitable for this kind of problems. Our efforts can be summarized as follows: We adapted fully convolutional network to multispec tral input data and evaluated several data fusion strate gies on semantic segmentation task of satellite images. We introduced joint training objective that properly de Ô¨Ånes desired output for the segmentation task. We analyze local and global boundary effect on overall performance of the segmentation pipeline. 2. Related Work "
55,A Free Lunch to Person Re-identification: Learning from Automatically Generated Noisy Tracklets.txt,"A series of unsupervised video-based re-identification (re-ID) methods have
been proposed to solve the problem of high labor cost required to annotate
re-ID datasets. But their performance is still far lower than the supervised
counterparts. In the mean time, clean datasets without noise are used in these
methods, which is not realistic. In this paper, we propose to tackle this
problem by learning re-ID models from automatically generated person tracklets
by multiple objects tracking (MOT) algorithm. To this end, we design a
tracklet-based multi-level clustering (TMC) framework to effectively learn the
re-ID model from the noisy person tracklets. First, intra-tracklet isolation to
reduce ID switch noise within tracklets; second, alternates between using
inter-tracklet association to eliminate ID fragmentation noise and network
training using the pseudo label. Extensive experiments on MARS with various
manually generated noises show the effectiveness of the proposed framework.
Specifically, the proposed framework achieved mAP 53.4% and rank-1 63.7% on the
simulated tracklets with strongest noise, even outperforming the best existing
method on clean tracklets. Based on the results, we believe that building re-ID
models from automatically generated noisy tracklets is a reasonable approach
and will also be an important way to make re-ID models feasible in real-world
applications.","Person reidentiÔ¨Åcation (reID) is to match persons across nonoverlapping cameras. It is one of the core tech niques in intelligent surveillance analysis. Due to the ur gent demand for public safety, it has been an active research Ô¨Åeld over the years. Videobased person reID is the prob lem where subjects to be retrieved are presented as video sequences. Person reID has shown promising results in a fully supervised setting. This learning paradigm assumesthat there is a large number of labeled highquality cross camera training data. But it is of the high cost to collect such a largescale dataset, due to the exponential labeling cost. Besides, a welltrained reID model has been proved to perform much worse in a new domain. To overcome the drawbacks of supervised methods, in the last two years, several works have turned to study un supervised or weakly supervised person reID. SpeciÔ¨Åcally, we focus on unsupervised videobased person reID, where training data can be obtained without human labor by mul tiple object tracking [2] (MOT) algorithms, as shown in Ô¨Åg ure 1. Most of the existing unsupervised video reID meth ods still yield unsatisfactory results. Moreover, these meth ods operate on video reID datasets, such as MARS [28], iLIDSVID [17] and PRID 2011 [7]. It should be noted that, as shown in Ô¨Ågure 1, although these datasets are used in a unsupervised manner, i.e. video sequences without intra and intercamera ID association, the sequences themselves are clean and without noise, and the production of such clean sequences requires substantial human effort as well. The gap between such datasets and MOTgenerated track lets are the noise introduced by MOT algorithms, which is mainly ID fragmentation noise and ID switch noise, along side with detection [13] noise. Some methods have been proposed to exploit tracklets to build a reID model, but only ID fragmentation noise is considered, while ID switch and detection noise are ignored, resulting in a wider gap from being applicable to reallife scenarios. We propose a new trackletbased clustering and Ô¨Åne tuning framework to account for both ID fragmentation noise and ID switch noise in MOTgenerated tracklets. By analyzing characteristics of aforementioned noise, a multi stage clusteringbased method is proposed to reduce noise in the tracklets before feeding them into the unsupervised training pipeline, resulting in signiÔ¨Åcant performance boost. Since raw video of video reID datasets is generally unavail able, a novel algorithm is proposed to generate simulated tracklets from video reID datasets, to assist evaluating our method under various strength of noise.arXiv:2204.00891v1  [cs.CV]  2 Apr 2022Figure 1: Different categories of person reID methods. Supervised methods require full annotation of video sequences. Datasets used by unsupervised video reID methods require human labor to eliminate noise within tracklets generated by MOT algorithms. Our method uses MOT tracklets directly. We summarize our contribution as threefold. 1. Firstly, we propose taking raw tracklets generated by MOT algorithms as input of our method, removing the requirement of human effort completely, resulting in nearly zerocost training data preparation, moving a step closer into solving realistic problems. 2. Secondly, we analyze dominant noise categories in tracklets generated by MOT algorithms, i.e. ID frag mentation and ID switch noise, revealing character istics which is exploited in our novel noise reduction processing. We combine the noise reduction tech niques with a selftraining mechanism in our method, named Trackletbased Multilevel Clustering (TMC). 3. Thirdly, experiments show that our method achieves remarkable performance given that realistic tracklets with noise are used (mAP 55.3% rank1 68.2% on sim ulated tracklets), and, if clean video reID datasets are used instead, outperforms existing unsupervised video reID methods. 2. Related Work "
145,Reliability-Adaptive Consistency Regularization for Weakly-Supervised Point Cloud Segmentation.txt,"Weakly-supervised point cloud segmentation with extremely limited labels is
highly desirable to alleviate the expensive costs of collecting densely
annotated 3D points. This paper explores to apply the consistency
regularization that is commonly used in weakly-supervised learning, for its
point cloud counterpart with multiple data-specific augmentations, which has
not been well studied. We observe that the straightforward way of applying
consistency constraints to weakly-supervised point cloud segmentation has two
major limitations: noisy pseudo labels due to the conventional confidence-based
selection and insufficient consistency constraints due to discarding unreliable
pseudo labels. Therefore, we propose a novel Reliability-Adaptive Consistency
Network (RAC-Net) to use both prediction confidence and model uncertainty to
measure the reliability of pseudo labels and apply consistency training on all
unlabeled points while with different consistency constraints for different
points based on the reliability of corresponding pseudo labels. Experimental
results on the S3DIS and ScanNet-v2 benchmark datasets show that our model
achieves superior performance in weakly-supervised point cloud segmentation.
The code will be released.","Recently, 3D point cloud segmentation has achieved im pressive progresses [8, 24, 25, 29]. However, it is still ex tremely expensive and laborconsuming to collect abundant pointlevel annotations for the model training. Therefore, in order to alleviate huge labeling costs, it is highly de sirable to develop weaklysupervised point cloud segmen tation, which aims to train a satisÔ¨Åed segmentation model *Corresponding author: G. Lin (email: gslin@ntu.edu.sg ) (c) Selected PseudoLabels(b) TrueLabels (d) Discarded PseudoLabels(a) Point CloudsFigure 1. Illustrations of imperfect pseudo labels in the conven tional conÔ¨Ådencebased selection. We use a probability threshold of 0.7 to select highlyconÔ¨Ådent pseudo labels for the model train ing, while they are very noisy (b vs. c) and many discarded pseudo labels (d) are not exploited during training. with scarce labeled points but enormous unlabeled points. To exploit the unlabeled points, existing methods are mainly based on the consistency assumption [1, 37, 43, 44], where the model is encouraged to be consistent under various perturbations, to achieve the local distributional smoothness (LDS). For example, [28] utilizes the predic tions of weakly augmented data to guide the learning of strongly augmented versions, where they select reliable pre dictions as pseudo labels based on the prediction conÔ¨Å dence and use them to enforce the consistency constraints to regularize the model training. Such consistencybased regularization has not been well investigated for weakly supervised point cloud segmentation. For instance, the re cent 1T1C [17] model also leverages the conÔ¨Ådence scores to select reliable predictions as pseudo labels and uses themarXiv:2303.05164v1  [cs.CV]  9 Mar 2023to train the model iteratively, which however is not a con sistency constraint under diversiÔ¨Åed perturbations. This motivates us to study the intuitive idea of applying consistency constraints to improve weaklysupervised point cloud segmentation. The straightforward way is to directly extend the FixMatch [28] from images to point clouds, i.e., selecting conÔ¨Ådent predictions of the weakly augmented point clouds as pseudo labels and applying consistency con straints to guide the predictions of strongly augmented ones. However, such a scheme has two major limitations. First, it is unsatisÔ¨Åed to select reliable predictions based on their conÔ¨Ådences. The examples in Fig. 1 (b, c) illustrate that the scheme may generate highly conÔ¨Ådent but incorrect pseudo labels, which would lead to more noisy supervisions and confuse the model training. Second, for the large amounts of unlabeled points that are deemed unreliable (see Fig. 1 (d)), they are being discarded and not utilized during train ing [17, 28], resulting in a suboptimal performance. We would like to point out these limitations are particu larly noticeable for weaklysupervised point cloud segmen tation, while they might not be so signiÔ¨Åcant in the corre sponding image counterpart . This is because for weakly supervised point cloud segmentation, the human annota tions are much more scarce, e.g. the typical one thing one click (OTOC) setting [17], and the generated point cloud pseudo labels are much more noisy (due to the sparsity of point clouds and lack of neighbor support). Thus, the key questions for weaklysupervised point cloud segmentation are:how to select reliable pseudo labels and how to utilize a large number of unreliable pseudo labels? Our key idea in this work is to select more reli able pseudo labels by considering both prediction conÔ¨Å dence and model uncertainty, and utilize reliable predic tions as hard pseudo labels while using ambiguous predic tions as soft pseudo labels instead of throwing them away . SpeciÔ¨Åcally, we propose a simple yet effective Reliability Adaptive Consistency Network (RACNet), which enforces the consistency constraints on all unlabeled data adaptively based on their pseudo label reliability. To measure the re liability, we jointly use the prediction conÔ¨Ådence and un certainty to divide the initial predictions of unlabeled data into ambiguous and reliable sets, where the uncertainty is measured by computing the statistical variances among the predictions of different augmentations. Considering the am biguous predictions are unreliable, we treat them as soft pseudo labels and apply a consistency loss (KL Divergence) to encourage invariant results of augmented point clouds. Considering the reliable predictions are accurate, we con vert them into onehot pseudo labels and then apply a con sistency loss (Crossentropy Loss) to guide the learning of different augmented data. In addition, to further exploit the reliable set, we also generate mixaugmented point clouds by a pointwise interpolation among multiple offtheshelfbaseaugmentations and then use the onehot pseudo labels to facilitate the model training. We follow the public models [17, 37] to conduct experi ments on two largescale point cloud segmentation datasets: S3DIS [4] and ScanNetv2 [9]. Extensive experiments demonstrate that our RACNet is able to accurately select pseudo labels during the model training and achieves supe rior segmentation performance than other existing methods for weaklysupervised point cloud segmentation, e.g., out performing the DAT model [37] by 1.9% under the OTOC setting on the S3DIS dataset . Besides, our experimental re sults reveal that combining the local shape deformation like PointWolf [14] and the conventional augmentation ( e.g., AfÔ¨Åne Transformations) is able to achieve impressive per formance gains for weaklysupervised point cloud segmen tation. Overall, the contributions in this paper are threefold: ‚Ä¢ We consider the problem of applying consistency based regularization for weaklysupervised point cloud segmentation and identify the two main unique obsta cles: measuring pseudolabel reliability and utilizing unreliable pseudo labels. ‚Ä¢ We propose a novel RACNet to address the two is sues by incorporating uncertainty, which is computed as the discrepancy among different augmentations, to identify more reliable pseudolabels and adaptively applying different consistency constraints to different points based on their reliability. Moreover, we design a mixaugmentation module to generate mixaugmented point clouds to further exploit the reliable set. ‚Ä¢ Our RACNet achieves new stateoftheart perfor mance in weaklysupervised point cloud segmenta tion on S3DIS and ScanNetv2 datasets, with multiple baseaugmentations and mixaugmentations for adap tive consistency training. 2. Related Works "
37,Semi-supervised Skin Lesion Segmentation via Transformation Consistent Self-ensembling Model.txt,"Automatic skin lesion segmentation on dermoscopic images is an essential
component in computer-aided diagnosis of melanoma. Recently, many fully
supervised deep learning based methods have been proposed for automatic skin
lesion segmentation. However, these approaches require massive pixel-wise
annotation from experienced dermatologists, which is very costly and
time-consuming. In this paper, we present a novel semi-supervised method for
skin lesion segmentation by leveraging both labeled and unlabeled data. The
network is optimized by the weighted combination of a common supervised loss
for labeled inputs only and a regularization loss for both labeled and
unlabeled data. In this paper, we present a novel semi-supervised method for
skin lesion segmentation, where the network is optimized by the weighted
combination of a common supervised loss for labeled inputs only and a
regularization loss for both labeled and unlabeled data. Our method encourages
a consistent prediction for unlabeled images using the outputs of the
network-in-training under different regularizations, so that it can utilize the
unlabeled data. To utilize the unlabeled data, our method encourages the
consistent predictions of the network-in-training for the same input under
different regularizations. Aiming for the semi-supervised segmentation problem,
we enhance the effect of regularization for pixel-level predictions by
introducing a transformation, including rotation and flipping, consistent
scheme in our self-ensembling model. With only 300 labeled training samples,
our method sets a new record on the benchmark of the International Skin Imaging
Collaboration (ISIC) 2017 skin lesion segmentation challenge. Such a result
clearly surpasses fully-supervised state-of-the-arts that are trained with 2000
labeled data.","Skin cancer is currently one of the fastest growing cancers worldwide, and melanoma is the most deadly form of skin cancer, leading to an estimated 9,730 deaths in the United States in 2017 [24]. To improve the diagnostic performance of melanoma, dermoscopy has been proposed as a noninvasive imaging technique to enhance the visual effect of pigmented skin c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:1808.03887v1  [cs.CV]  12 Aug 20182 LI ET AL.: SEMISUPERVISED SKIN LESION SEGMENTATION Figure 1: Skin lesion cases with artifacts (the left two); examples of ambiguous (the middle two) and clearcut (the right two) labels. lesions. However, recognizing malignant melanoma by visual interpretation alone is time consuming and errorprone to inter and intraobserver variabilities. To assist dermatologists in the diagnosis, an automatic melanoma segmentation method is highly demanded in the clinical practice. Automatic melanoma segmentation is a very challenging task due to large variations in lesion size, location, shape and color over different patients and the presence of artifacts such as hairs and veins; see Figure 1. Traditional segmentation methods are mainly based on clustering, intensity thresholding, region growing, and deformable models. These methods, however, rely on handcrafted features, and have limited feature representation capability. Recently, convolutional neural networks (CNNs) have been widely used and achieved re markable success in a variety of vision recognition tasks. Many researchers advanced the skin lesion segmentation and showed decent results [2, 6, 17, 28]. For example, Yuan et al.[28] proposed a deep convolutional neural network (DCNN), trained it with multiple color spaces, and achieved the best performance in the ISIC 2017 skin lesion segmentation challenge. All the above methods, however, are based on fully supervised learning, which requires a large amount of annotated images to train the network for accuracy and robustness. Such pixellevel annotation is laborious and difÔ¨Åcult to obtain, especially for melanoma in the der moscopic images, which rely heavily on experienced dermatologists. Moreover, the limited amount of labeled data with pixelwise annotations also restricts the performance of deep networks. Lastly, there exists some cases that display ambiguous melanocytic or borderline features of melanoma. These cases are inherently difÔ¨Åcult to have an accurate annotation from the dermoscopic diagnosis [22]; see again Figure 1. Previous supervised learning based methods do not have speciÔ¨Åc schemes to deal with these ambiguous annotations, which may degrade the performance on those dermoscopic images with clearcut lesions. To alleviate the above issues, we address the skin lesion segmentation problem via semisupervised learn ing, which leverages both a limited amount of labeled and an arbitrary amount of unlabeled data. As a byproduct, our semisupervised method is robust and has a potential to be tol erant to ambiguous labels; see experiments in Section 4.2. There are some semisupervised approaches for dermoscopy images and other medical image processing [1, 10, 14, 18]. However, they either suffer from limited representation capacity of handcrafted features or may easily get into local minimum. In this paper, we present a novel semisupervised learning method for skin lesion seg mentation. The whole framework is trained with a weighted combination of the supervised loss and the unsupervised loss. To utilize the unlabeled data, our selfensembling method encourages the consistent prediction of the network for the same input data under differ ent regularizations ( e.g., randomized Gaussian noise, network dropout and randomized data transformation). In particular, we design our method to account for the challenging semi supervised segmentation task, in which pixellevel classiÔ¨Åcation is required to be predicted.LI ET AL.: SEMISUPERVISED SKIN LESION SEGMENTATION 3 We observe that in the segmentation problem, if one transforms ( e.g., rotate) the input image, the expected prediction should be transformed in the same manner. Actually, when the inputs of CNNs are rotated, the corresponding network predictions would not rotated in the same way [25]. In this regard, we take advantages of this property by introducing a transforma tion ( i.e., rotation, Ô¨Çipping) consistent scheme at the input and output space of our network. SpeciÔ¨Åcally, we design the unsupervised/regularization loss by minimizing the differences between the network predictions under different transformations of the same input. In summary, our work has the following achievements: We present a novel semisupervised learning method for the practical biomedical im age segmentation problem by taking advantage of a large amount of unlabeled data, which largely reduces annotation efforts for the dermatologists. To better utilize the unlabeled data for segmentation tasks, we propose a transforma tion consistent scheme in selfensembling model and demonstrate the effectiveness for semisupervised learning. We establish a new record with only 300 labeled data on the benchmark of ISIC 2017 skin lesion segmentation challenge, which excels the stateofthearts that are based on fully supervised learning with 2000 labeled data. 2 Related Work "
94,Semi-Supervised Noisy Student Pre-training on EfficientNet Architectures for Plant Pathology Classification.txt,"In recent years, deep learning has vastly improved the identification and
diagnosis of various diseases in plants. In this report, we investigate the
problem of pathology classification using images of a single leaf. We explore
the use of standard benchmark models such as VGG16, ResNet101, and DenseNet 161
to achieve a 0.945 score on the task. Furthermore, we explore the use of the
newer EfficientNet model, improving the accuracy to 0.962. Finally, we
introduce the state-of-the-art idea of semi-supervised Noisy Student training
to the EfficientNet, resulting in significant improvements in both accuracy and
convergence rate. The final ensembled Noisy Student model performs very well on
the task, achieving a test score of 0.982.","Plant pathology classiÔ¨Åcation is a very challenging and important task. In 2019, more than 20 %of all global crop productions were lost due to pests and other pathogens. This results in massive economic losses and food deÔ¨Åcits in many parts of the world. Therefore, it is very important to be able to develop systems that can detect such diseases. Such systems will have a signiÔ¨Åcant impact on the agricul tural sector, especially in areas where agriculture is a pri mary food source for millions of people. Figure 1: Examples of fungal diseases in apples [22]Currently, plant disease diagnosis is mostly done man ually by human examination. However, this is inefÔ¨Åcient, as it is tedious, timeconsuming, and errorprone. Recently, deep learning and computer vision techniques have shown great promise in this task. These systems have the ability to quickly and reliably make predictions, and have greatly contributed to plant classiÔ¨Åcation in the past few years. However, there are still many challenges. For instance, im ages of different leaves may look very similar with each other. Meanwhile, two images from the same leaf species may have very different images. Plant disease symptoms may also manifest in different ways due to age of infected tissues, genetic variations, and light conditions. In this report, we examine the use of pretrained deep learning models in plant pathology classiÔ¨Åcation. More speciÔ¨Åcally, we examine the stateoftheart EfÔ¨ÅcientNet [21] and a semisupervised approach called Noisy Student training [24]. We demonstrate that the Noisy Student EfÔ¨Å cientNet performs very well on our dataset, and it has sig niÔ¨Åcant improvements over previous attempts at the task. 2. Related Work "
98,Product Image Recognition with Guidance Learning and Noisy Supervision.txt,"This paper considers recognizing products from daily photos, which is an
important problem in real-world applications but also challenging due to
background clutters, category diversities, noisy labels, etc. We address this
problem by two contributions. First, we introduce a novel large-scale product
image dataset, termed as Product-90. Instead of collecting product images by
labor-and time-intensive image capturing, we take advantage of the web and
download images from the reviews of several e-commerce websites where the
images are casually captured by consumers. Labels are assigned automatically by
the categories of e-commerce websites. Totally the Product-90 consists of more
than 140K images with 90 categories. Due to the fact that consumers may upload
unrelated images, it is inevitable that our Product-90 introduces noisy labels.
As the second contribution, we develop a simple yet efficient \textit{guidance
learning} (GL) method for training convolutional neural networks (CNNs) with
noisy supervision. The GL method first trains an initial teacher network with
the full noisy dataset, and then trains a target/student network with both
large-scale noisy set and small manually-verified clean set in a multi-task
manner. Specifically, in the stage of student network training, the large-scale
noisy data is supervised by its guidance knowledge which is the combination of
its given noisy label and the soften label from the teacher network. We conduct
extensive experiments on our Products-90 and public datasets, namely Food101,
Food-101N, and Clothing1M. Our guidance learning method achieves performance
superior to state-of-the-art methods on these datasets.","This paper studies a crucial problem in realworld appli cation: recognize products from consumer photos without much supervision. More speciÔ¨Åcally, we want to recognize the Ô¨Ånegrained products taken by consumer‚Äôs mobile cam Figure 1. Example images from our Products90. We illustrate 5 different categories in column. Visually correct images are shown in the Ô¨Årst two rows. Visually confused or unrelated images are shown in the last two rows. eras, with unconstrained viewing directions, cluttered back ground, and different lighting conditions. One can imagine an application that you are recommended where the prod ucts can be found and what the prices are by recognizing your casuallycaptured product photos. To address this realworld product image recognition task, we build a novel largescale dataset, termed as Product90 , which consists of 90 generic product cate gories. Instead of collecting daily images by laborand timeintensive image capturing, we take advantage of the web and download images from the reviews of several e commerce websites where the images are casually captured by consumers. Totally, we collected more than 140k prod uct images from the customer reviews. The associated 90 categories are borrowed the categories of ecommerce web sites. Figure 1 shows some examples of this dataset. We can see there are several challenges brought by Product90 dataset: i)The visual contents in Product90 contains a wide 1arXiv:1907.11384v1  [cs.CV]  26 Jul 2019range of subjects. ii) Some categories are very similar in ap pearance, e.g. Hair Care vs. Body Care. iii) Some photos are not related to the category, which suggests a signiÔ¨Åcant level of noise exists in the dataset. To evaluate product image recognition algorithms on our Product90, we build a small manuallyclean subset for tra ditional training and testing, and remain the rest of Product 90 as noisy data which can be used for extra training. To take full advantage of the small clean training subset and the massive noisy labeled data for daily product recognition, we propose a novel guidance learning framework for noisy data learning. It mainly includes two training stages. At the Ô¨Årst stage, we train a baseline CNN model, or a teacher model, on the full Product90 dataset (without the clean test set). At the second stage, we train a student or target network on the largescale noisy set and the small clean training set with multitask learning. SpeciÔ¨Åcally, in the stage of stu dent training, the largescale noisy data is supervised by the guidance knowledge which consists of two supervision sig nals, namely the noisy ground truths and the soften labels from the teacher network. We fuse these onehot ground truths with the soften multihot labels, and optimize the net work by Kullback‚ÄìLeibler Divergence (KLDiv) loss. Our guidance learning framework considers the follow ing issues. The Ô¨Årst stage of our guidance learning ensures that we can obtain a powerful teacher model instead of us ing the noisy set or clean set only like in [25].We fuse both ground truths and soften labels for the largescale noisy data, since i) the teacher model provides useful informa tion but is far from perfect, and ii) there exist both false and correct labels in noisy labels. In summary, our contributions can be concluded as fol lows: We introduce a new task, i.e. daily product image recognition, and a novel largescale dataset, termed as Product90 which is collected from the reviews of e commerce websites. To advance the performance of daily product image recognition, we propose a generic guidance learning method to take full advantage the small clean subset and the largescale noisy data in Product90. We conduct comprehensive evaluations with our guid ance learning method on our Products90, Food 101 [1], Food101N [14], and Clothing1M [33], and achieve stateoftheart results. 2. Related Work "
490,ReINTEL Challenge 2020: A Multimodal Ensemble Model for Detecting Unreliable Information on Vietnamese SNS.txt,"In this paper, we present our methods for unrealiable information
identification task at VLSP 2020 ReINTEL Challenge. The task is to classify a
piece of information into reliable or unreliable category. We propose a novel
multimodal ensemble model which combines two multimodal models to solve the
task. In each multimodal model, we combined feature representations acquired
from three different data types: texts, images, and metadata. Multimodal
features are derived from three neural networks and fused for classification.
Experimental results showed that our proposed multimodal ensemble model
improved against single models in term of ROC AUC score. We obtained 0.9445 AUC
score on the private test of the challenge.","Recently, fake news detection have received much attention in both NLP and data mining re search community. This year, for the Ô¨Årst time, VLSP 2020 Evaluation Campaign organizers held ReINEL Challenge (Le et al., 2020) to encourage the development of algorithms and systems for de tecting unreliable information on Vietnamese SNS. In ReINTEL Challenge 2020, we need to deter mine a piece of information containing texts, im ages, and metadata is reliable or unreliable. The task is formalized as a binary classiÔ¨Åcation prob lem and training data with unreliable/reliable labels was provided by VLSP 2020 organizers. In this paper, we present a novel multimodal ensemble model for identifying unreiable informa tion on Vietnamese SNS. We use neural networks to obtain feature representations from different data types. Multimodal features are fused and put into a sigmoid layer for classiÔ¨Åcation. SpeciÔ¨Åcally, we use BERT model to obtain feature representations from texts, a multilayer perceptron to encode metadata and textbased features, and a Ô¨Ånetuned VGG 19 network to obtain feature representations from images. We combined two single models in order to improve the accuracy of fake news detection. Our proposed model obtained 0.9445 ROC AUC score on the private test of the challenge. 2 Related Work "
10,Adversarial Partial Multi-Label Learning.txt,"Partial multi-label learning (PML), which tackles the problem of learning
multi-label prediction models from instances with overcomplete noisy
annotations, has recently started gaining attention from the research
community. In this paper, we propose a novel adversarial learning model,
PML-GAN, under a generalized encoder-decoder framework for partial multi-label
learning. The PML-GAN model uses a disambiguation network to identify noisy
labels and uses a multi-label prediction network to map the training instances
to the disambiguated label vectors, while deploying a generative adversarial
network as an inverse mapping from label vectors to data samples in the input
feature space. The learning of the overall model corresponds to a minimax
adversarial game, which enhances the correspondence of input features with the
output labels in a bi-directional mapping. Extensive experiments are conducted
on multiple datasets, while the proposed model demonstrates the
state-of-the-art performance for partial multi-label learning.","In partial multilabel learning (PML), each training instance is assigned multiple candidate labels which are only partially relevant; that is, some irrelevant noise labels are assigned together with the groundtruth labels. As it is typically difÔ¨Åcult and costly to precisely annotate instances for multilabel data [ 22], the task of PML naturally arises in many realworld scenarios with crowdsource annotations. In such a scenario, in order to collect the complete set of positive labels for each data instance, one can gather all labels provided by multiple annotators to form the candidate label set, which is usually overcomplete and contains additional noisy labels beyond all the true labels, leading to the PML problem. Figure 1 presents such an example of overcompletely annotated training image for object recognition, where the candidate labels provided by crowdsource annotators cover all the ground truth labels (in black color) and some irrelevant noise labels (in red color). PML is much more challenging than standard multilabel learning as the true labels are hidden among irrelevant labels and the number of true labels is unknown. The goal of PML is to learn a good multilabel prediction model from such a partial label training set, and hence reduce the annotation cost. An intuitive strategy of PML is to treat all candidate labels as relevant ground truth, thus any offthe shelf multilabel classiÔ¨Åcation method can be adapted to induce an expected multilabel predictor [29]. This strategy, though simple, cannot work well since taking the noise labels as part of the true labels will mislead the multilabel training and induce inferior prediction models. The PML work in [ 22] assumes that each candidate label has a conÔ¨Ådence score of being a true label, and learns the conÔ¨Ådence scores and the classiÔ¨Åer in an alternative manner by minimizing a conÔ¨Ådence weighted ranking loss. Although this work yields some reasonable results, the estimation of label conÔ¨Ådence scores is errorprone, especially when noise labels dominate, which can seriously impair the classiÔ¨Åer‚Äôs performance. The recent work in [ 23] proposes to perform groundtruth label recovery and noise label identiÔ¨Åcation simultaneously by exploring the label correlations and the relationships between the noise labels and feature representations. Another recent work in [ 5] presents a two Preprint. Under review.arXiv:1909.06717v2  [cs.LG]  5 Jun 2020Figure 1: An annotated im age under the partial multilabel learning (PML) setting. Figure 2: The proposed PMLGAN model. It has four com ponent networks: generator G, disambiguator eD, predictor F, and discriminator D. stage PML method. It estimates the conÔ¨Ådence values of the candidate labels using iterative label propagation and then chooses the highly conÔ¨Ådent candidate labels as credible labels to induce a multi label prediction model. This work however suffers from the cumulative errors induced in propagation, which can impact the label conÔ¨Ådence estimation and consequently impair the prediction. In this paper, we propose a novel adversarial learning model, PMLGAN, under a generalized encoderdecoder framework to tackle the partial multilabel learning problem. The PMLGAN model comprises four component networks: a disambiguation network that predicts the probability of each candidate label being an additive noise for a training instance; a prediction network that predicts the disambiguated true labels of each instance from its input features; a generation network that generates samples in the feature space given latent vectors in the label space; and a discrimination network that separates the generated samples from the real data. The prediction network and disambiguation network together form an encoder that maps data samples in the input feature space to the disambiguated label vectors, while the generation network and discrimination network form a generative adversarial network (GAN) as an inverse decoding mapping from vectors in the multilabel space to samples in the input feature space. The learning of the overall model corresponds to a minimax adversarial game, which enhances the correspondence of input features with the output labels through the bidirectional encoderdecoder mapping mechanism, and consequently boosts multilabel prediction performance. To the best of our knowledge, this is the Ô¨Årst work that exploits a generative adversarial model based bidirectional mapping mechanism for PML. We conduct extensive experiments on multiple multilabel datasets under partial multilabel learning setting. The empirical results show the proposed PMLGAN yields the stateoftheart PML performance. 2 Related Work "
151,Orientation-aware Vehicle Re-identification with Semantics-guided Part Attention Network.txt,"Vehicle re-identification (re-ID) focuses on matching images of the same
vehicle across different cameras. It is fundamentally challenging because
differences between vehicles are sometimes subtle. While several studies
incorporate spatial-attention mechanisms to help vehicle re-ID, they often
require expensive keypoint labels or suffer from noisy attention mask if not
trained with expensive labels. In this work, we propose a dedicated
Semantics-guided Part Attention Network (SPAN) to robustly predict part
attention masks for different views of vehicles given only image-level semantic
labels during training. With the help of part attention masks, we can extract
discriminative features in each part separately. Then we introduce
Co-occurrence Part-attentive Distance Metric (CPDM) which places greater
emphasis on co-occurrence vehicle parts when evaluating the feature distance of
two images. Extensive experiments validate the effectiveness of the proposed
method and show that our framework outperforms the state-of-the-art approaches.","Vehicle reidentiÔ¨Åcation (reID) aims to match vehicle images in a camera net work. Recently, this task has drawn increasing attention due to practical appli cations such as urban surveillance and traÔ¨Éc Ô¨Çow analysis. While deep Convo lutional Neural Networks (CNN) have shown remarkable performance in vehicle reID over the years [22,23,33], various challenges still hinder the performance of vehicle reID. One of them is that a vehicle captured from diÔ¨Äerent viewpoints usually has dramatically diÔ¨Äerent visual appearances. On the other hand, two diÔ¨Äerent vehicles of the same color and car model are likely to have very sim ilar appearances. As illustrated in the left part of Fig. 1, it is challenging to distinguish vehicles by comparing the features extracted from the whole vehicle images. In such case, the minor diÔ¨Äerences in speciÔ¨Åc parts of vehicle such as decorations or license plates would be a great beneÔ¨Åt to identifying two vehicles. Furthermore, when two vehicles are presented in diÔ¨Äerent orientations, a desired vehicle reID algorithm should be able to focus on the parts (views) that botharXiv:2008.11423v2  [cs.CV]  12 Oct 20202 TsaiShien Chen, ChihTing Liu, ChihWei Wu, and ShaoYi Chien Globalbasedfeature spaceSideSidebasedfeature spaceNeg. Pair Pos. PairSideSemanticsguided Part Attention Network (SPAN) Fig. 1: Concept illustration of Semanticsguided Part Attention Net work. The example images show intraclass diÔ¨Äerence and interclass similarity in the vehicle reID problem. It is challenging to separate the negative images merely based on global feature due to the similar car model and viewpoint. In this example, it is easier to distinguish two vehicles by the sidebased feature. This motivates us to generate the part (view) attention maps and then emphasize the feature of the cooccurrence vehicle parts for better reID matching. appear in the two vehicle images. For example, in the right part of Fig. 1, it is easier to distinguish the vehicles by comparing their side views. To reach this idea, we divide it into two steps. The Ô¨Årst step is to extract the feature from speciÔ¨Åc parts of vehicle im ages. A number of work has been proposed to achieve this purpose by learning orientationaware features. Nonetheless, existing methods either rely on expen sive vehicle keypoints as guidance to learn an attention mechanism for each part of a vehicle [34,11] or use only viewpoint labels but produce noisy and unsteady attention outcome which will thus hinder the network to learn subtle diÔ¨Äerences between vehicles [42]. In this paper, we introduce the Semanticsguided Part At tention Network (SPAN) to generate attention masks for diÔ¨Äerent parts (front, side and rear views) of a vehicle. As shown in Fig. 1, our SPAN learns to produce meaningful attention masks. The masks not only help disentangle features of dif ferent viewpoints but also improve the interpretability of our learning framework. It is also worth noting that, instead of expensive keypoints or pixellevel labels for training, our SPAN requires only imagelevel viewpoint labels which are much easier to be derived from known camera pose and traÔ¨Éc direction. For the second step, we design a Cooccurrence Partattentive Distance Met ric (CPDM) to better utilize the part features when measuring the distance of images. The intuition of this metric is that the network should focus on the parts (views) that both appear in the compared vehicle images. Therefore, the proposed metric allows us to automatically adjust the importance of each part feature distance according to the part visibility in two compared vehicle images. We conduct experiments on two largescale vehicle reID benchmarks and demonstrate that our method outperforms current stateofthearts. AblationOrientationaware Vehicle ReID with Semanticsguided Part Attention Net 3 studies prove that the attention masks generated by SPAN extract helpful part features and our CPDM can better utilize the global and part features to im prove the reID performance. Moreover, qualitative results show that our SPAN can robustly generate meaningful attention maps on vehicles of diÔ¨Äerent types, colors, and orientations. We now highlight our contributions: (1) We propose a Semanticsguided Part Attention Network (SPAN) to generate robust part at tention masks which can be used to extract more discriminative features. (2) Our SPAN only needs imagelevel viewpoint labels instead of expensive keypoints or pixellevel annotations for training. (3) We introduce the Cooccurrence Part attentive Distance Metric (CPDM) to facilitate vehicle reID by focusing on the parts that jointly appear in the compared images. (4) Extensive experiments on public datasets validate the eÔ¨Äectiveness of each component and demonstrate that our method performs favorably against stateoftheart approaches. 2 Related Work "
265,Countering Adversarial Examples: Combining Input Transformation and Noisy Training.txt,"Recent studies have shown that neural network (NN) based image classifiers
are highly vulnerable to adversarial examples, which poses a threat to
security-sensitive image recognition task. Prior work has shown that JPEG
compression can combat the drop in classification accuracy on adversarial
examples to some extent. But, as the compression ratio increases, traditional
JPEG compression is insufficient to defend those attacks but can cause an
abrupt accuracy decline to the benign images. In this paper, with the aim of
fully filtering the adversarial perturbations, we firstly make modifications to
traditional JPEG compression algorithm which becomes more favorable for NN.
Specifically, based on an analysis of the frequency coefficient, we design a
NN-favored quantization table for compression. Considering compression as a
data augmentation strategy, we then combine our model-agnostic preprocess with
noisy training. We fine-tune the pre-trained model by training with images
encoded at different compression levels, thus generating multiple classifiers.
Finally, since lower (higher) compression ratio can remove both perturbations
and original features slightly (aggressively), we use these trained multiple
models for model ensemble. The majority vote of the ensemble of models is
adopted as final predictions. Experiments results show our method can improve
defense efficiency while maintaining original accuracy.","Adversarial attack presents a major challenge for the prevalent deep neural networks used for image classiÔ¨Åca tion and recognition [37]. Several countermeasures havebeen proposed against adversarial examples, mainly includ ing modelspeciÔ¨Åc hardening strategies and modelagnostic defenses. Typical modelspeciÔ¨Åc solutions like ‚Äúadversarial training‚Äù [17, 27, 34, 33, 29] can rectify the model param eters to mitigate the attacks by using the iterative retraining procedure or modifying the inner architecture. However, it is generally believed that, network‚Äôs architectural elements would matter little unless making them larger and deeper in improving adversarial robustness. In contrast, model agnostic solutions like input dimension reduction or direct JPEG compression [8, 4], become more feasible and prac tical, which attempt to remove adversarial perturbations by input transformations before feeding them into neural net work classiÔ¨Åers. For mitigating adversarial examples, standard JPEG compression has been explored in [8, 4]. But, in these works, they have shown that JPEG cannot achieve a good balance between countering adversarial examples and clas sifying benign images, i.e., lower quality factor (QF) for JPEG compression achieves better defense efÔ¨Åciency but causes a signiÔ¨Åcant feature loss on benign images. To re solve this problem, we Ô¨Årst optimize the JPEG based trans formation process in this work, to improve defense efÔ¨Å ciency against adversarial examples and maintain classiÔ¨Å cation accuracy on benign images. Firstly, we analyze the distributions of the DCT coefÔ¨Åcients for 6 color channels (i.e., R, G, B, Y , Cb, Cr) on both benign images and polluted images to Ô¨Ånd out adversarial perturbations‚Äô distribution at all 64 frequency bands. With the frequency analysis, we then divide the frequency coefÔ¨Åcients into two types, i.e., the original favored (OF) band and the adversarial favored (AF) band. Finally, the corresponding defensive quantiza tion parameters for these two bands are derived, where the 1arXiv:2106.13394v1  [cs.CV]  25 Jun 2021Model 1  Model 2  Model 3 Prediction  Finetune Test Model setModel 0 Model 4   Transform4 Transform3 Transform2  Adversarial   exampleTransform1 Gaussian noise Gaussian noise Gaussian noise Gaussian noiseOriginal imageFigure 1. Overview of our combination method. Different trans form modules represent different level of compression for the orig inal images. The initial model, i.e., model 0, is the pretrained model on benign images of ImageNet dataset, such as ResNet or Inceptionv4. number of the DCT coefÔ¨Åcients that should be included in each type of band is jointly optimized. With the purpose to further achieve both accuracy and robustness, we Ô¨Ånetune the model with our own pre processed images. Firstly, as a data augmentation, train ing images will be compressed using our compression al gorithm. This idea shares the similar spirit of the image croppingrescaling method proposed in [13], in which, the neural network retrained on randomly croppedrescaled images yields better performance than other input transfor mations. Secondly, Gaussian noise is added to the com pressed images to mimic the adversarial perturbations (de tailed in Section 3.4), which is based on the fact that strong adversaries are not necessarily needed during adversarial training as demonstrated in [33]. However, as the model is commonly noisy trained with compressed images of a cer tain level of quality, there still exists unavoidable tradeoff between robustness and accuracy. To achieve the best bal ance, we generate a number of classiÔ¨Åers by Ô¨Ånetuning the neural network using a variety of degrees of compression quality images during aforementioned preprocessing. Af ter having obtained a set of classiÔ¨Åers, the Ô¨Ånal prediction value if chosen to be the label maximizes the average conÔ¨Å dence (i.e., the output of Softmax layer) of each classiÔ¨Åer. Figure 1 shows an overview of our overall method, where we combine the input transformation and the noisy training. The preprocessing is implemented by using a pro posed compression followed by adding the general Gaus sian noise. The model set is realized by Ô¨Ånetuning the models with compressed images at different compressionlevel. The initial model used for Ô¨Ånetuning is the pre trained model on benign images. The models retrained with different compression levels are ultimately utilized together in an ensemble defense. Experimental results demonstrate the defense efÔ¨Åciency and the legitimate classiÔ¨Åcation efÔ¨Å ciency of the proposed algorithm against a variety of adver sarial examples in the graybox, blackbox and whitebox scenarios. The implementation code of the algorithm pro posed will be made publicly available. 2. Related Works "
62,Predicting Biomedical Interactions with Higher-Order Graph Convolutional Networks.txt,"Biomedical interaction networks have incredible potential to be useful in the
prediction of biologically meaningful interactions, identification of network
biomarkers of disease, and the discovery of putative drug targets. Recently,
graph neural networks have been proposed to effectively learn representations
for biomedical entities and achieved state-of-the-art results in biomedical
interaction prediction. These methods only consider information from immediate
neighbors but cannot learn a general mixing of features from neighbors at
various distances. In this paper, we present a higher-order graph convolutional
network (HOGCN) to aggregate information from the higher-order neighborhood for
biomedical interaction prediction. Specifically, HOGCN collects feature
representations of neighbors at various distances and learns their linear
mixing to obtain informative representations of biomedical entities.
Experiments on four interaction networks, including protein-protein, drug-drug,
drug-target, and gene-disease interactions, show that HOGCN achieves more
accurate and calibrated predictions. HOGCN performs well on noisy, sparse
interaction networks when feature representations of neighbors at various
distances are considered. Moreover, a set of novel interaction predictions are
validated by literature-based case studies.","ABiological system is a complex network of various molecular entities such as genes, proteins, and other biological molecules linked together by the interactions between these entities. The complex interplay between var ious molecular entities can be represented as interaction networks with molecular entities as nodes and their in teractions as edges. Such a representation of a biological system as a network provides a conceptual and intuitive framework to investigate and understand direct or indi rect interactions between different molecular entities in a biological system. Study of such networks lead to system level understanding of biology [1] and discovery of novel interactions including proteinprotein interactions (PPIs) [2], drugdrug interactions (DDIs) [3], drugtarget interactions (DTIs) [4] and genedisease associations (GDIs) [5]. Recently, the generalization of deep learning to the networkstructured data [6] has shown great promise across various domains such as social networks [7], recommen dation systems [8], chemistry [9], citation networks [10]. These approaches are under the umbrella of graph con volutional networks (GCNs). GCNs repeatedly aggregate feature representations of immediate neighbors to learn the informative representation of the nodes for link pre diction. Although GCN based methods show great suc cess in biomedical interaction prediction [3], [11], the issue with such methods is that they only consider information from immediate neighbors. SkipGNN [12] leverages skip graph to aggregate feature representations from direct and K. KC, R. Li, and A. Haake are with the Department of Computing and Information Sciences, Rochester Institute of Technology, Rochester, NY, 14623. Email:fkk3671, arhics, rxlics g@rit.edu F. Cui is with Thomas H. Gosnell School of Life Sciences, Rochester Institute of Technology, Rochester, NY, 14623. Email: fxcsbi@rit.edusecondorder neighbors and demonstrated improvements over GCN methods in biomedical interaction prediction. However, SkipGNN cannot be applied to aggregate infor mation from higherorder neighbors and thus fail to capture information that resides farther away from a particular interaction [13]. To address the challenge, we propose an endtoend deep graph representation learning framework named higherorder graph convolutional networks (HOGCN) for predicting interactions between pairs of biomedical entities. HOGCN learns a representation for every biomedical entity using an interaction network structure Gand/or features X. In particular, we deÔ¨Åne a higherorder graph convolution (HOGC) layer where the feature representations from k order neighbors are considered to obtain the representation of biomedical entities. The layer can thus learn to mix feature representations of neighbors at various distances for interaction prediction. Furthermore, we deÔ¨Åne a bilinear decoder to reconstruct the edges in the input interaction networkGby relying on feature representations produced by HOGC layers. The encoderdecoder approach makes HOGCN an endtoend trainable model for interaction pre diction. We compare HOGCN‚Äôs performance with that of state oftheart network similaritybased methods [14], network embedding methods [15], [16], and graph convolution based methods [10], [12], [17] for biomedical link prediction. We experiment with various interaction datasets and show that our method makes accurate and calibrated predic tions. HOGCN outperforms alternative methods based on network embedding by up to 30%. Furthermore, HOGCN outperforms graph convolutionbased methods by up to 6%, alluding to the beneÔ¨Åts of aggregating information from higherorder neighbors. We perform a case study on the DDI network andarXiv:2010.08516v1  [cs.LG]  16 Oct 2020JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 observe that aggregating information from higherorder neighborhood allows HOGCN to learn meaningful repre sentation for drugs. Moreover, literaturebased case studies illustrate that the novel predictions are supported by ev idence, suggesting that HOGCN can identify interactions that are highly likely to be a true positive. In summary, our study demonstrates the ability of HOGCN to identify potential interactions between biomedi cal entities and opens up the opportunities to use the biolog ical and physicochemical properties of biomedical entities for a followup analysis of these interactions. 2 R ELATED WORKS "
209,Data-Efficient and Interpretable Tabular Anomaly Detection.txt,"Anomaly detection (AD) plays an important role in numerous applications. We
focus on two understudied aspects of AD that are critical for integration into
real-world applications. First, most AD methods cannot incorporate labeled data
that are often available in practice in small quantities and can be crucial to
achieve high AD accuracy. Second, most AD methods are not interpretable, a
bottleneck that prevents stakeholders from understanding the reason behind the
anomalies. In this paper, we propose a novel AD framework that adapts a
white-box model class, Generalized Additive Models, to detect anomalies using a
partial identification objective which naturally handles noisy or heterogeneous
features. In addition, the proposed framework, DIAD, can incorporate a small
amount of labeled data to further boost anomaly detection performances in
semi-supervised settings. We demonstrate the superiority of our framework
compared to previous work in both unsupervised and semi-supervised settings
using diverse tabular datasets. For example, under 5 labeled anomalies DIAD
improves from 86.2\% to 89.4\% AUC by learning AD from unlabeled data. We also
present insightful interpretations that explain why DIAD deems certain samples
as anomalies.","Anomaly detection (AD) has numerous realworld applications, especially for tabular data, including detection of fraudulent trans actions, intrusions related to cybersecurity, and adverse outcomes Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA. ¬©2023 Copyright held by the owner/author(s). ACM ISBN 9798400701030/23/08. https://doi.org/10.1145/3580305.3599294in healthcare. When the realworld tabular AD applications are considered, there are various challenges constituting a fundamen tal bottleneck for penetration of fullyautomated machine learning solutions: ‚Ä¢Noisy and irrelevant features : Tabular data often contain noisy or irrelevant features caused by measurement noise, outlier fea tures and inconsistent units. Even a change in a small subset of features may trigger anomaly identification. ‚Ä¢Heterogeneous features : Unlike image or text, tabular data fea tures can have values with significantly different types (numeri cal, boolean, categorical, and ordinal), ranges and distributions. ‚Ä¢Small labeled data : In many applications, often a small portion of the labeled data is available. AD accuracy can be significantly boosted with the information from these labeled samples as they may contain crucial information on representative anomalies and help ignore irrelevant ones. ‚Ä¢Interpretability : Without interpretable outputs, humans cannot understand the rationale behind anomaly predictions, that would enable more trust and actions to improve the model performance. Verification of model accuracy is particularly challenging for high dimensional tabular data, as they are not easy to visualize for humans. An interpretable AD model should be able to identify important features used to predict anomalies. Conventional local explainability methods like SHAP [ 20] and LIME [ 25] are pro posed for supervised learning and may not be straightforward to generalize to unsupervised or semisupervised AD. Conventional AD methods fail to address the above ‚Äì their per formance often deteriorates with noisy features (Sec. 6), they cannot incorporate labeled data, and cannot provide interpretability. In this paper, we aim to address these challenges by propos ing a novel framework, Dataefficient Interpretable AD(DIAD ). DIAD‚Äôs model architecture is inspired by Generalized Additive Models (GAMs) and GA2M (see Sec. 3), that have been shown to obtain high accuracy and interpretability for tabular data [ 4,6,16], and have been used in applications like finding outlier patterns and auditing fairness [ 33]. We propose to employ intuitive notions of Partial Identification (PID) as an AD objective and learn them with a differentiable GA2M (NodeGA2M, Chang et al . [5]). Our design is based on the principle that PID scales to highdimensional features and handles heterogeneous features well, while the dif ferentiable GAM allows finetuning with labeled data and retain interpretability. In addition, PID requires clearcut thresholds like trees which are provided by NodeGA2M. While combining PIDarXiv:2203.02034v2  [cs.LG]  4 Jun 2023KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA. Chang, et al. Figure 1: Overview of the proposed DIAD framework. During training, first an unsupervised AD model is fitted employing interpretable GA2M models and PID loss with unlabeled data. Then, the trained unsupervised model is finedtuned with a small amount of labeled data using a differentiable AUC loss. At inference, both the anomaly score and explanations are provided, based on the visualizations of top contributing features. The example sample in the figure is shown to have an anomaly score, explained by the cell size feature having high value. with NodeGA2M, we introduce multiple methodological innova tions, including estimating and normalizing a sparsity metric as the anomaly scores, integrating a regularization for an inductive bias appropriate for AD, and using deep representation learning via finetuning with a differentiable AUC loss. The latter is crucial to take advantage of a small amount of labeled samples well and constitutes a more ‚Äòdataefficient‚Äô method compared to other AD approaches ‚Äì e.g. DIAD improves from 87.1% to 89.4% AUC with 5 labeled anomalies compared to unsupervised AD. Overall, our inno vations lead to strong empirical results ‚Äì DIAD outperforms other alternatives significantly, both in unsupervised and semisupervised settings. DIAD‚Äôs outperformance is especially prominent on large scale datasets containing heterogeneous features with complex relationships between them. In addition to accuracy gains, DIAD also provides a rationale on why an example is classified as anoma lous using the GA2M graphs, and insights on the impact of labeled data on the decision boundary, a novel explainability capability that provides both local and global understanding on the AD tasks. 2 RELATED WORK "
137,PASS: Peer-Agreement based Sample Selection for training with Noisy Labels.txt,"Noisy labels present a significant challenge in deep learning because models
are prone to overfitting. This problem has driven the development of
sophisticated techniques to address the issue, with one critical component
being the selection of clean and noisy label samples. Selecting noisy label
samples is commonly based on the small-loss hypothesis or on feature-based
sampling, but we present empirical evidence that shows that both strategies
struggle to differentiate between noisy label and hard samples, resulting in
relatively large proportions of samples falsely selected as clean. To address
this limitation, we propose a novel peer-agreement based sample selection
(PASS). An automated thresholding technique is then applied to the agreement
score to select clean and noisy label samples. PASS is designed to be easily
integrated into existing noisy label robust frameworks, and it involves
training a set of classifiers in a round-robin fashion, with peer models used
for sample selection. In the experiments, we integrate our PASS with several
state-of-the-art (SOTA) models, including InstanceGM, DivideMix, SSR, FaMUS,
AugDesc, and C2D, and evaluate their effectiveness on several noisy label
benchmark datasets, such as CIFAR-100, CIFAR-N, Animal-10N, Red Mini-Imagenet,
Clothing1M, Mini-Webvision, and Imagenet. Our results demonstrate that our new
sample selection approach improves the existing SOTA results of algorithms.","In deep neural networks (DNNs) and machine learning, it is commonly recognized that having an adequate amount of labeled training data and computational resources leads to exceptional outcomes in various Ô¨Åelds, such as com puter vision [6], natural language processing [62], and in *arpit.garg@adelaide.edu.aumedical domain [33]. Nonetheless, the aforementioned outcomes have predominantly been attained through the utilization of meticulously curated datasets possessing la bels of exceptional quality. The collection of such high caliber labels, particularly for large datasets, can prove to be exorbitantly costly in realworld scenarios [42, 60, 72]. Therefore, alternative cheaper labeling methods, includ ing crowdsourcing [60] and metadata mining [18], have gained traction, but they result in substandard labeling [20]. While these techniques aid in cost reduction and expedite the labeling process, they are susceptible to data mislabel ing [55]. Erroneous labels potentially degrade the performance of DNNs by inducing overÔ¨Åtting through the phenomenon of memorization [1, 39, 43, 50, 76]. This issue has prompted the development of innovative learning algorithms aimed at tackling the problem of noisy labeling. Within the domain of noisy labels, many methods have emerged [19, 39], each tailored to tackle the challenges posed by distinct settings of noise, namely instanceindependent noise (IIN) [22] and instancedependent noise (IDN) [68]. Early studies in the Ô¨Åeld of noisy label operated under the presumption that la bel noise was IIN, that is, mislabeling occurred irrespective of the information regarding the visual classes present in images [22]. In IIN, a transition matrix is generally em ployed, which comprises a predetermined probability of Ô¨Çipping between pairs of labels [72]. Nevertheless, recent studies have progressively redirected the Ô¨Åeld‚Äôs attention to ward the more realistic scenario of IDN [10, 19, 71], where label noise depends on both the true class label and the im age information. Previous techniques for mitigating the impact of noisy label samples frequently involve the manual selection of clean samples to form a clean validation set [56]. The difÔ¨Åculty in obtaining clean validation samples [26], par ticularly for problems with many classes, motivated recent studies to leverage semisupervised learning methods with out relying on clean validation sets [12, 39]. Other ap 1arXiv:2303.10802v1  [cs.CV]  20 Mar 2023A BSmalllossPASS AB CABC AB CBAA BFINE ABFigure 1. Illustration of the comparison between various sample selection strategies, including smallloss [22], featurebased tech niques [32], and our proposed PASS . In smallloss [22] we typically have two networks A & B, with each model using the loss values of the samples to select clean and noisy instances, which are exchanged between the two networks for training them. FINE [32] utilizes a featurebased selection approach based on the similarity to class speciÔ¨Åc eigenvectors in the representation space to select samples that are exchanged between two networks, A and B, for further training. Our PASS maintains three networks (A, B & C), where the prediction agreements between two networks will select the clean samples to be used for training the third network. Note that during training, the two networks used for sample selection will rotate through the three available networks. proaches incorporate robust loss functions [52], designed speciÔ¨Åcally to operate effectively with either clean or noisy labels, as well as probabilistic modeling approaches that model the data generation process [19, 71]. Furthermore, training regularization [43, 64] imposes a penalty term on the loss function during training, thereby reducing overÔ¨Åt ting and ameliorating generalization. Various techniques integrate sample selection strategies as a key algorithmic step [12, 13, 19, 39], allowing for the identiÔ¨Åcation and seg regation of noisy and clean labels. One popular criterion for this sample selection process is the loss value between the prediction of the trained classiÔ¨Åer and its label, whereby it is typically assumed that the noisy data exhibits a large loss [22, 30, 32, 64] or greater magnitude of the gradient during training [63]. Additionally, featurebased sample se lection techniques based on the similarity to principal com ponents of feature representations [32] or K nearest neigh bor (KNN) classiÔ¨Åcation in the feature space [18] have also been considered for sample selection criteria. Nonetheless, we empirically note that separating clean, but hard to clas sify samples from noisy label samples still remains chal lenging for these sample selection processes [64], partic ularly for problems of large noise rates. The use of peer classiÔ¨Åers for noisy label learning problems have been in vestigated for enforcing consistency in the training of the classiÔ¨Åers [22, 47], but not for selecting clean and noisy label samples. We argue in this paper that the prediction agreement between peer classiÔ¨Åers is more effective to se lect clean and noisy label samples than previous approaches because intuitively, such agreement is unlikely to happen, except when the classiÔ¨Åers agree on the clean label. In this paper, we propose a new sample selection criterion based on the predictive probability agreement be tween peer classiÔ¨Åers, which has been shown in computa tional linguistics to be a sample reliability measure [2]. In light of implementation, we train three classiÔ¨Åers simulta neously by using the agreement between two classiÔ¨Åers to select samples to train the remaining classiÔ¨Åer, as shown in Fig. 1. This sample selection relies on a thresholding algorithm [53] that distinguishes samples based on how high the agreement between the peer classiÔ¨Åcation predic tions is. Our proposed method, named as peeragreement based sample selection (PASS), can readily be integrated into existing models in noisy label learning, such as In stanceGM [19], DivideMix [39], SSR [18], FaMUS [70], AugDesc [51], and ContrasttoDivide (C2D) [77]. The pri mary contributions of our method can be delineated as fol lows: ‚Ä¢ We propose a new noisy label sample selection method, PASS, that differentiates clean and noisy la bel samples via the prediction agreement between peer classiÔ¨Åers. ‚Ä¢ We demonstrate that our method can be easily adapted to existing models, including InstanceGM [19], Di videMix [39], SSR [18], FaMUS [70], AugDesc [51], and C2D [77], where we show that PASS enhances the performance of various SOTA approaches on vari ous benchmarks, comprising both simulated and real world datasets, such as CIFAR100 [35], CIFAR N [66], Animal10N [59], Red MiniImagenet from Controlled Noisy Web Labels (CNWL) [70], Cloth ing1M [69], MiniWebvision [40], and Imagenet [16]. It is imperative to provide clarity that our proposition doesnot entail the introduction of a new algorithm for the classi Ô¨Åcation of noisy labels. Instead, we suggest a new method for selecting samples to substantially improve the efÔ¨Åcacy of preexisting noisy label learning algorithms, as shown by our experiments (Section 4). 2. Related Work "
93,Neural Paraphrase Identification of Questions with Noisy Pretraining.txt,"We present a solution to the problem of paraphrase identification of
questions. We focus on a recent dataset of question pairs annotated with binary
paraphrase labels and show that a variant of the decomposable attention model
(Parikh et al., 2016) results in accurate performance on this task, while being
far simpler than many competing neural architectures. Furthermore, when the
model is pretrained on a noisy dataset of automatically collected question
paraphrases, it obtains the best reported performance on the dataset.","Question paraphrase identiÔ¨Åcation is a widely use ful NLP application. For example, in questionand answer (QA) forums ubiquitous on the Web, there are vast numbers of duplicate questions. Identi fying these duplicates and consolidating their an swers increases the efÔ¨Åciency of such QA forums. Moreover, identifying questions with the same se mantic content could help Webscale question an swering systems that are increasingly concentrating on retrieving focused answers to users‚Äô queries. Here, we focus on a recent dataset published by the QA website Quora.com containing over 400K annotated question pairs containing binary para phrase labels.1We believe that this dataset presents a great opportunity to the NLP research commu nity and practitioners due to its scale and quality; it can result in systems that accurately identify dupli cate questions, thus increasing the quality of many QA forums. We examine a simple model family, thedecomposable attention model of Parikh et al. (2016), that has shown promise in modeling natural 1See https://data.quora.com/FirstQuoraDatasetRelease QuestionPairs.language inference and has inspired recent work on similar tasks (Chen et al., 2016; Kim et al., 2017). We present two contributions. First, to mitigate data sparsity, we modify the input representation of the decomposable attention model to use sums of character ngram embeddings instead of word embeddings. We show that this model trained on the Quora dataset produces comparable or better results with respect to several complex neural ar chitectures, all using pretrained word embeddings. Second, to signiÔ¨Åcantly improve our model perfor mance, we pretrain allour model parameters on the noisy, automatically collected questionparaphrase corpus Paralex (Fader et al., 2013), followed by Ô¨Ånetuning the parameters on the Quora dataset. This twostage training procedure achieves the best result on the Quora dataset to date, and is also sig niÔ¨Åcantly better than learning only the character ngram embeddings during the pretraining stage. 2 Related Work "
382,Named Entity Recognition in the Legal Domain using a Pointer Generator Network.txt,"Named Entity Recognition (NER) is the task of identifying and classifying
named entities in unstructured text. In the legal domain, named entities of
interest may include the case parties, judges, names of courts, case numbers,
references to laws etc. We study the problem of legal NER with noisy text
extracted from PDF files of filed court cases from US courts. The ""gold
standard"" training data for NER systems provide annotation for each token of
the text with the corresponding entity or non-entity label. We work with only
partially complete training data, which differ from the gold standard NER data
in that the exact location of the entities in the text is unknown and the
entities may contain typos and/or OCR mistakes. To overcome the challenges of
our noisy training data, e.g. text extraction errors and/or typos and unknown
label indices, we formulate the NER task as a text-to-text sequence generation
task and train a pointer generator network to generate the entities in the
document rather than label them. We show that the pointer generator can be
effective for NER in the absence of gold standard data and outperforms the
common NER neural network architectures in long legal documents.","Named Entity Recognition (NER) is the task of identifying the span and the class of a Named Entity (NE) in unstructured text. NEs typically include but are not limited to persons, companies, dates, and geographical locations (Sang and De Meulder, 2003). Legal NER is a central task in language process ing of legal documents, especially for extracting key information such as the name of the parties in a case, the court name or the case number, or references to laws or judgements, to name a few. The extracted NEs could be integrated in legal re search workÔ¨Çows for functionalities such as search,document anonymization or case summarization thereby enabling and expediting insights for legal professionals (Zhong et al., 2020). NER is commonly formalized as a sequence la beling task: each token of the document is assigned a single label that indicates whether the token be longs to an entity from a predeÔ¨Åned set of cate gories (Li et al., 2018). To create a training dataset in such a format the annotator is required to manu ally label each token in a sentence with the respec tive category. In this format, both the NE and the location of the NE in the source text are known. This format of training data is what we refer to hereafter as ‚Äúgold standard‚Äù data. Obtaining the re quired voluminous gold standard data to train such models is, therefore, a laborious and costly task. In this paper, we perform NER in Ô¨Åled lawsuits in US courts. SpeciÔ¨Åcally, we aim to identify the party names in each case, i.e. the names of the plaintiffs and the defendants, in a large collection of publicly available cases from more than 200 courts in different US jurisdictions. The party names have been identiÔ¨Åed by legal annotators but their exact location in the text is unknown. In this respect, we do not have access to ‚Äúgold standard‚Äù training data even though the target NEs are available. This feature of our dataset introduces a key difference of our task to most NER tasks. One solution to this problem is to generate the ‚Äúgold standard‚Äù training data by searching for the locations of the known NEs in the source text . By performing this additional transformation to our data, we would be able to train sequence labeling NER models. For the following reasons, this solu tion is nontrivial. First, as our source text is also extracted from scanned PDF Ô¨Åles (‚Äùimageonly‚Äù PDFs), it contains Optical Character Recognition (OCR) mistakes and/or typos which may not be present in the target NEs. Second, besides the po tential OCR errors at the character level, the closelyarXiv:2012.09936v1  [cs.CL]  17 Dec 2020spaced, twocolumn page layouts that can be often found as headers in the Ô¨Åled cases, represent an additional challenge for the OCR, which tends to concatenate the text across columns (Figure 1). In such cases, the tokens that make up the NEs in the source text may be intertwined with other words and/or sentences. Third, variations of the names may be also present in the source text and in our humangenerated labels, such as presence of Ô¨Årst and/or middle names whole or as initials and, to a lesser extent, typos. To address some of the challenges imposed by the format of our training data and inspired by the work in the Ô¨Åeld of abstractive summarization, we propose to reformulate the NER task, not as a se quence labeling problem, but as a texttotext se quence generation problem with the use of a pointer generator network (Gu et al., 2016; See et al., 2017; Gehrmann et al., 2018). With this reformulation, in contrast to sequence labeling, we do not require knowledge of the NE‚Äôs locations in the text as train ing labels. A recent study by Li et al. (2020) pro posed a different formulation of the NER task as a question answering task and achieved stateof theart performance in a number of published NER datasets (Li et al., 2020). In this study, we adopt a hybrid extractiveabstractive architecture, based on recurrent neural networks coupled with global (i.e. the entire input document) attention and copy ing (or pointing) attention mechanisms (Gehrmann et al., 2018). The proposed architecture can be suc cessfully used for abstractive summarization since it can copy words from the source text via point ing and can deal effectively with outofvocabulary (OOV) words ‚Äì words that have not been seen dur ing training. Our approach is conceptually sim ple but empirically powerful and we show that the pointer generator outperforms the typical NER ar chitectures in the case of noisy and lengthy inputs where the NE‚Äôs location in the text is not known. In addition, we examine how our approach can be used for the related NER task of case number extraction. The case number is a unique combi nation of letters, numbers and special characters as a single token and are, therefore, particularly challenging for NER models as they are often dealt with as OOV words by the model. As in the party names task discussed above, in the case number task we do not have ‚Äúgold standard‚Äù labels of the case number‚Äôs location in the text. We show that a character level sequence generation network candramatically increase our ability to extract case numbers from the source text, compared to a word level sequence generation network. The rest of the paper is organized as follows. In Section 2, we discuss related work in the Ô¨Åeld of NER in the legal domain. In Section 3, we describe our proposal of NER as a texttotext sequence gen eration task in the absence of gold standard data and formulate the task in two ways: (i) as a combi nation of automatically labeling the NE‚Äôs location and then using the conventional sequence labeling method for NER, and (ii) as a texttotext sequence generation task where the NEs are directly gener ated as text. Section 4 presents our experimental design, results and analysis. Section 5 presents the case number case study. Finally, we conclude and discuss directions for future work. 2 Related Work "
6,Empirical Error Modeling Improves Robustness of Noisy Neural Sequence Labeling.txt,"Despite recent advances, standard sequence labeling systems often fail when
processing noisy user-generated text or consuming the output of an Optical
Character Recognition (OCR) process. In this paper, we improve the noise-aware
training method by proposing an empirical error generation approach that
employs a sequence-to-sequence model trained to perform translation from
error-free to erroneous text. Using an OCR engine, we generated a large
parallel text corpus for training and produced several real-world noisy
sequence labeling benchmarks for evaluation. Moreover, to overcome the data
sparsity problem that exacerbates in the case of imperfect textual input, we
learned noisy language model-based embeddings. Our approach outperformed the
baseline noise generation and error correction techniques on the erroneous
sequence labeling data sets. To facilitate future research on robustness, we
make our code, embeddings, and data conversion scripts publicly available.","Deep learning models have already surpassed humanlevel performance in many Natural Lan guage Processing (NLP) tasks1. Sequence labeling systems have also reached extremely high accu racy (Akbik et al., 2019; Heinzerling and Strube, 2019). Still, NLP models often fail in scenarios, where nonstandard text is given as input (Heigold et al., 2018; Belinkov and Bisk, 2018). NLP algorithms are predominantly trained on errorfree textual data but are also employed to pro cess usergenerated text (Baldwin et al., 2013; Der czynski et al., 2013) or consume the output of prior Optical Character Recognition (OCR) or Auto matic Speech Recognition (ASR) processes (Miller et al., 2000). Errors that occur in any upstream 1GLUE benchmark (Wang et al., 2018a): https:// gluebenchmark.com/leaderboard Training Loss Sailing is a passion. Sailing 1s o passion. Seq2Seq ModelFigure 1: Our modiÔ¨Åcation of the NAT approach (green boxes). We propose a learnable seq2seqbased error generator and retrain FLAIR embeddings using noisy text to improve the accuracy of noisy neural sequence labeling. "
166,Audio Visual Speech Recognition using Deep Recurrent Neural Networks.txt,"In this work, we propose a training algorithm for an audio-visual automatic
speech recognition (AV-ASR) system using deep recurrent neural network
(RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal
Classification (CTC) objective function. The frame labels obtained from the
acoustic model are then used to perform a non-linear dimensionality reduction
of the visual features using a deep bottleneck network. Audio and visual
features are fused and used to train a fusion RNN. The use of bottleneck
features for visual modality helps the model to converge properly during
training. Our system is evaluated on GRID corpus. Our results show that
presence of visual modality gives significant improvement in character error
rate (CER) at various levels of noise even when the model is trained without
noisy data. We also provide a comparison of two fusion methods: feature fusion
and decision fusion.","Audiovisual automatic speech recognition (AVASR) is a case of multimodal analysis in which two modalities (audio and visual) complement each other to recognize speech. Incorporating visual features, such as speaker's lip movements and facial expressions, into automatic speech recognition (ASR) systems has been shown to improve their performances especially under noisy conditions. To this end several methods have been proposed which traditionally include variants of GMM/HMM models[4][2]. More recently AVASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed. Endtoend speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM. The RNN trained with CTC directly learns a mapping between audio feature frames and char acter/phoneme sequences. This method eliminates the need for intermediate 1Version (Aug 2016) accepted in 4th International Workshop on Multimodal pattern recognition of social signals in human computer interaction(MPRSS 2016), a satellite event of the International Conference on Pattern Recognition (ICPR 2016)arXiv:1611.02879v1  [cs.CV]  9 Nov 2016Audio Visual Speech Recognition using Deep Recurrent Neural Networks GMM/HMM training thereby simplifying the training procedure. To our knowl edge, so far AVASR systems based on RNN trained with CTC have not been explored. In this work, we design and evaluate an audiovisual ASR (AVASR) system using deep recurrent neural network (RNN) and CTC objective function. The design of an AVASR system includes the tasks of visual feature engineering, and audiovisual information fusion. Figure 1 shows the AVASR pipeline at test time. This work mainly deals with the visual feature extraction and pro cessing steps and training protocol for the fusion model. Proper visual features are important especially in the case of RNNs as RNNs are dicult to train. Bot tleneck features used in tandem with audio features are known to improve ASR performance [5][10][24]. We employ a similar idea to improve the discriminatory power of video features. We show that this helps the RNN to converge prop erly when compared with raw features. Finally, we compare the performances of feature fusion and decision fusion methods. The paper is organized as follows: Section 2 presents the prior work on AV ASR. Bidirectional RNN and its training using CTC objective function are discussed in Section 3. Section 4 describes the feature extraction steps for audio and visual modalities. In section 5 dierent fusion models are explained. Section 6 explains the training protocols and experimental results. Finally, we summarize our work in 7. Fig. 1. Pipeline of AVASR system using feature fusion method 2 Related Work "
203,Ensemble Neural Relation Extraction with Adaptive Boosting.txt,"Relation extraction has been widely studied to extract new relational facts
from open corpus. Previous relation extraction methods are faced with the
problem of wrong labels and noisy data, which substantially decrease the
performance of the model. In this paper, we propose an ensemble neural network
model - Adaptive Boosting LSTMs with Attention, to more effectively perform
relation extraction. Specifically, our model first employs the recursive neural
network LSTMs to embed each sentence. Then we import attention into LSTMs by
considering that the words in a sentence do not contribute equally to the
semantic meaning of the sentence. Next via adaptive boosting, we build
strategically several such neural classifiers. By ensembling multiple such LSTM
classifiers with adaptive boosting, we could build a more effective and robust
joint ensemble neural networks based relation extractor. Experiment results on
real dataset demonstrate the superior performance of the proposed model,
improving F1-score by about 8% compared to the state-of-the-art models.","Many NLP tasks have been built on different knowledge bases, such as Freebase and DBPedia. However, the knowl edge bases could not cover all the facts in the real world. Therefore, it is essential to extract more common relational facts automatically in open domain corpus. As known, rela tion extraction (RE) aims at extracting new relation instances that are not contained in the knowledge bases from the un structured open corpus. It aligns the entities in the open cor pus with those in the knowledge bases and retrieves the en tity relations from the real world. For example, if we aim to retrieve a relation from the raw text, ‚Äú Barack Obama mar ried Michelle Obama 10 years ago ‚Äù, a naive approach would be to search the news articles for indicative phrases, such as ‚Äúmarry ‚Äù or ‚Äú spouse ‚Äù. However, the result may be wrong since human language is inherently various and ambiguous. Corresponding authorPrevious supervised RE methods require a large amount of labelled relation training data by humanhand. To address this issue, Mintz et al. [Mintz et al. , 2009 ]proposed an ap proach via aligning the entity in KB for later extraction with out plenty of training corpus. However, their assumption  there is only one relation existing in a pair of entities, was ir rational. Therefore, later researches assumed more than one relation could exist between a pair of entities. Hoffmann et al.[Hoffmann et al. , 2011 ]proposed a multiinstance learn ing model with overlapping relations (MultiR) that combined a sentencelevel extraction model for aggregating the individ ual facts. Surdeanu et al. [Surdeanu et al. , 2012 ]proposed a multiinstance multilabel learning model (MIMLRE) to jointly model the instances of a pair of entities in text and all their labels. The major limitation of the above methods is that they cannot deeply capture the latent semantic infor mation from the raw text. It is also challenging for them to seamlessly integrate semantic learning with feature selection to more accurately perform RE. Recently, deep neural networks are widely explored for re lation extraction and have achieved signiÔ¨Åcant performance improvement [Zeng et al. , 2015; Lin et al. , 2016 ]. Compared with traditional shallow models, deep models can deeply cap ture the semantic information of a sentence. Zeng et al. [Lin et al. , 2016 ]employed CNN with sentencelevel attention over multiple instances to encode the semantics of sentences. Miwa and Bansal [Miwa and Bansal, 2016 ]used a syntax treebased long shortterm memory networks (LSTMs) on the sentence sequences. Ye et al. [Yeet al. , 2017 ]proposed a uni Ô¨Åed relation extraction model that combined CNN with a pair of ranking class ties. However, the main issue of existing deep models is that their performance may not be stable and could not effectively handle the quite imbalanced, noisy, and wrong labeled data in relation extraction even if a large number of parameters in the model. To address the above issues, in this paper we propose a novel ensemble deep neural network model to extract re lations from the corpus via an Adaptive Boosting LSTMs with Attention model (AdaLSTMs). SpeciÔ¨Åcally, we Ô¨Årst choose bidirectional long shortterm memory networks to embed forward and backward directions of a sentence for bet ter understanding the sentence semantics. Considering the fact that the words in a sentence do not contribute equally to the sentence representation, we import attention mechaarXiv:1801.09334v2  [cs.IR]  28 Apr 2018reweighting	the	gradient	ùõøfor	sample	withùê∑#(s&)ùêøùëÜùëáùëÄ,Œ≥,(ùë•)	Œ≥0(ùë•)	Œ≥1(ùë•)	Œ≥#2,(ùë•)	Œ≥#(ùë•)	ùõº,ùõº0ùõº1ùõº#2,UAttentionùëô,ùëô5ùëô62,ùëô6Ensenble	Model		Œ•words	ùëüùë§:of	the	sentence	ùë†1positions	of	ùëüùë§:feature LayerUUUUdropoutùëô0: LSTM networksU: twodirectionalLSTMs‚Äô unit: dropout: LSTMs‚Äô traditional forward: LSTMs‚Äô backward, update with ùê∑<(ùë†1)ùëô1: label of sentence ùë†1ùõº#	ùêøùëÜùëáùëÄùêøùëÜùëáùëÄ0ùêøùëÜùëáùëÄ1ùêøùëÜùëáùëÄ=2,ùêøùëÜùëáùëÄ#Symbols:Figure 1: The framework of AdaLSTMs contains three layers: feature layer, bidirectional Stacked LSTMs‚Äô layer with attention and adaptive boosting layer. siindicates the original input sentence with a pair of entities and their relation. nism to the bidirectional LSTMs. Next we construct multi ple such LSTM classiÔ¨Åers and ensemble their results as the Ô¨Ånal prediction result. Kim and Kang [Kim and Kang, 2010 ] showed that ensemble with neural networks perform better than one single neural network in prediction tasks. Motivated by their work, we import adaptive boosting and tightly cou ple it with deep neural networks to more effectively and ro bustly solve the relation extraction problem. The key role of adaptive boosting in our model is reweighting during the training process. The weight of incorrectly classiÔ¨Åed samples will increase. In other words, the samples classiÔ¨Åed wrongly gain more attention so that the classiÔ¨Åer is forced to focus on these hard examples. Note that attention can distinguish the different importance of words in the sentence, while adap tive boosting can use sample weights to inform the training of neural networks. In a word, the combination of the two can more precisely capture the semantic meaning of the sen tences and better represent them, and thus help us train a more accurate and robust model. We summarize the contributions of this paper as follows. We propose a Multiclass Adaptive Boosting Neural Networks model, which to our knowledge is the Ô¨Årst work that combines adaptive boosting and neural net works for relation extraction. We utilize adaptive boosting to tune the gradient descent in NN training. In this way, a large number of param eters in a single NN can be learned more robustly. The ensembled results on multiple NN models can achieve more accurate and robust relation extraction result. We evaluate the proposed model on a real data set. The results demonstrate the superior performance of the pro posed model which improves F1score by about 8% compared to stateoftheart models. 2 Related Work "
42,CorrDetector: A Framework for Structural Corrosion Detection from Drone Images using Ensemble Deep Learning.txt,"In this paper, we propose a new technique that applies automated image
analysis in the area of structural corrosion monitoring and demonstrate
improved efficacy compared to existing approaches. Structural corrosion
monitoring is the initial step of the risk-based maintenance philosophy and
depends on an engineer's assessment regarding the risk of building failure
balanced against the fiscal cost of maintenance. This introduces the
opportunity for human error which is further complicated when restricted to
assessment using drone captured images for those areas not reachable by humans
due to many background noises. The importance of this problem has promoted an
active research community aiming to support the engineer through the use of
artificial intelligence (AI) image analysis for corrosion detection. In this
paper, we advance this area of research with the development of a framework,
CorrDetector. CorrDetector uses a novel ensemble deep learning approach
underpinned by convolutional neural networks (CNNs) for structural
identification and corrosion feature extraction. We provide an empirical
evaluation using real-world images of a complicated structure (e.g.
telecommunication tower) captured by drones, a typical scenario for engineers.
Our study demonstrates that the ensemble approach of \model significantly
outperforms the state-of-the-art in terms of classification accuracy.","Inspecting faults (e.g. corrosion) is a major problem in industrial structures such as building roofs, pipes, poles, bridges, and telecommunication towers [1]. This is a vital service for several industrial sectors, especially manufacturing, where structures (assets) that are subject to corrosion due to their exposure to the weather are used to deliver critical products or services. The problem of corrosion may cost Australia up to $32 billion annually, which is greater than $1500 for every Australian each year [2]. Corrosion is not simply a nancial cost if left unattended; the endangerment of lives may also be a real risk. Without adopting to the latest in AIdriven solutions, businesses are losing millions in time and money to identify corrosion using methods that have changed little with heavy reliance on human judgement [3]. The timely and accurate detection of corrosion is a key way to improve the eciency of economy by instigating appropriately managed maintenance processes that will also safe lives. A fast and reliable inspection process for corrosion can ensure industrial assets are maintained in time to prevent regulatory breaches, outages or catas trophic disasters. In most cases, inspections of such assets are conducted man ually which can be slow, hazardous, expensive and inaccurate. Recently, drones have proven to be a viable and safer solution to perform such inspections in many adverse conditions by  ying upclose to the structures and take a very large number of highresolution images from multiple angles [4]. The images acquired through such process are stored and then subsequently reviewed man ually by expert engineers who decide about further actions. However, this causes a problem of plenty for highly qualied engineers to manually identify corrosion from the images which further leads to a high level of human error, inconsisten cies, high lead time and high costs in terms of manhours. Existing approaches for identifying structural corrosion from images are ei ther based on Computer Vision (CV) [5] or Deep Learning (DL) techniques [6, 7]. In recent CVbased techniques [8], nontrivial prior knowledge and ex tensive human eorts are required in designing high quality corrosion features from images. In addition, one cannot hope much on the performance (or the accuracy of corrosion detection) in the case that the corrosion features are some what incorrectly identied. Compared with computer vision/image processing [9] and vanilla machine learning approaches [10], DLbased methods, in partic ular Convolutional Neural Networks (CNNs) [11, 12] have shown the ability to automatically learn important features, outperforming stateoftheart vision based approaches [6, 7, 13] and achieving humanlevel accuracy. In this paper, we present a Deep Learning (DL)based framework named CorrDetector , for detecting corrosion from high resolution images captured by 2drones. As the key innovation, we propose and develop an ensemble of CNN models [14] which is capable of detecting corrosion in target structure (i.e. ob ject) from such high resolution images at signicantly higher accuracy than the current stateoftheart CNN models. More specically, the proposed framework is capable of providing i) industrial structure recognition  detect the industrial structure (i.e. object of interest) in the image captured by the drone (since the drone image is captured in a realworld environment that is lled with back ground noise) and; iii) localised detection of corrosion  detect which areas in the industrial structure contains corrosion. Most DLbased solutions for corro sion detection use image samples captured by DSLR (digital singlelens re ex), digital or mobile cameras with human involvement in taking pictures [7, 15] in more controlled environment. Such image samples are much lower in resolution than drone images. Moreover, these samples can be biased as they are captured specically to be utilised for experimental purposes at certain distances and an gles. Therefore, such images comprised of human judgements to focus in specic type of corrosion area within the image which can be easily isolated and distin guishable even in visual inspection [7]. Moreover, previous studies have mostly focused on corrosion identication only in metallic surfaces [4, 6, 7, 10]. To the best of our knowledge, this work is the rst attempt that utilises realworld highresolution unaltered images captured by drones in industrial and realworld settings to identify corrosion in industrial structure such as telecommunication tower. More specically, this paper makes the following contributions: ‚Ä¢Present a novel framework, CorrDetector with a 4layer architecture to detect industrial object and identify regions of corrosion in highresolution images of industrial assets captured by drones in a realworld setting from various positions, angles and distances. ‚Ä¢Present an innovative ensemble approach that combines two deep learning models; a deep learning model for recognising and separating targeted industrial structure from the background and a deep learning model to identify corrosion in specic regions of the industrial structure (localised). ‚Ä¢Present a systematic methodology for training our ensemble model us ing highresolution drone images that includes two types of annotation techniques namely gridbased and objectbased. ‚Ä¢A comprehensive evaluation using a realworld dataset (high resolution drone images of telecommunication towers) and comparison with current stateoftheart deep learning models for corrosion detection to demon strate the ecacy of the proposed CorrDetector . The rest of the paper is organised as follows. Section 2 provides a discussion of current stateoftheart in corrosion detection from images. Section 3 presents the systematic methodology for developing an ensemble of CNN models for corrosion detection. Section 4 presents the experimental domain, the empirical evaluation and a comprehensive analysis of our proposed approach against the currentstateoftheart and nally Section 5 concludes the paper. 32. Related Works "
535,Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss.txt,"The task of Fine-grained Entity Type Classification (FETC) consists of
assigning types from a hierarchy to entity mentions in text. Existing methods
rely on distant supervision and are thus susceptible to noisy labels that can
be out-of-context or overly-specific for the training sentence. Previous
methods that attempt to address these issues do so with heuristics or with the
help of hand-crafted features. Instead, we propose an end-to-end solution with
a neural network model that uses a variant of cross- entropy loss function to
handle out-of-context labels, and hierarchical loss normalization to cope with
overly-specific ones. Also, previous work solve FETC a multi-label
classification followed by ad-hoc post-processing. In contrast, our solution is
more elegant: we use public word embeddings to train a single-label that
jointly learns representations for entity mentions and their context. We show
experimentally that our approach is robust against noise and consistently
outperforms the state-of-the-art on established benchmarks for the task.","Finegrained Entity Type ClassiÔ¨Åcation (FETC) aims at labeling entity mentions in context with one or more speciÔ¨Åc types organized in a hier archy (e.g., actor as a subtype of artist , which in turn is a subtype of person ). Finegrained types help in many applications, including rela tion extraction (Mintz et al., 2009), question an swering (Li and Roth, 2002), entity linking (Lin et al., 2012), knowledge base completion (Dong et al., 2014) and entity recommendation (Yu et al., 2014). Because of the high cost in labeling large training corpora with Ô¨Ånegrained types, current FETC systems resort to distant supervision (Mintz et al., 2009) and annotate mentions in the train ing corpus with all types associated with the en tity in a knowledge graph. This is illustrated inFigure 1, with three training sentences about en titySteve Kerr . Note that while the entity be longs to three Ô¨Ånegrained types ( person ,athlete , andcoach ), some sentences provide evidence of only some of the types: person andcoach from S1,person andathlete from S2, and just person forS3. Clearly, direct distant supervision leads to noisy training data which can hurt the accuracy of the FETC model. One kind of noise introduced by distant super vision is assigning labels that are outofcontext (athlete inS1andcoach inS2) for the sentence. Current FETC systems sidestep the issue by ei ther ignoring outofcontext labels or using simple pruning heuristics like discarding training exam ples with entities assigned to multiple types in the knowledge graph. However, both strategies are in elegant and hurt accuracy. Another source of noise introduced by distant supervision is when the type isoverlyspeciÔ¨Åc for the context. For instance, ex ample S3does not support the inference that Mr. Kerr is either an athlete or acoach . Since existing knowledge graphs give more attention to notable entities with more speciÔ¨Åc types, overlyspeciÔ¨Åc labels bias the model towards popular subtypes in stead of generic ones, i.e., preferring athlete over person . Instead of correcting for this bias, most existing FETC systems ignore the problem and treat each type equally and independently, ignor ing that many types are semantically related. Besides failing to handle noisy training data there are two other limitations of previous FETC approaches we seek to address. First, they rely on handcrafted features derived from various NLP tools; therefore, the inevitable errors introduced by these tools propagate to the FETC systems via the training data. Second, previous systems treat FETC as a multilabel classiÔ¨Åcation problem: during type inference they predict a plausibility score for each type, and, then, either classify typesarXiv:1803.03378v2  [cs.CL]  14 Apr 2018Figure 1: With distant supervision, all the three mentions of Steve Kerr shown are labeled with the same types in oval boxes in the target type hierarchy. While only part of the types are correct: person andcoach forS1,person andathlete forS2, and just person forS3. with scores above a threshold (Mintz et al., 2009; Gillick et al., 2014; Shimaoka et al., 2017) or per form a topdown search in the given type hierarchy (Ren et al., 2016a; Abhishek et al., 2017). Contributions: We propose a neural network based model to overcome the drawbacks of exist ing FETC systems mentioned above. With pub licly available word embeddings as input, we learn two different entity representations and use bidi rectional longshort term memory (LSTM) with attention to learn the context representation. We propose a variant of cross entropy loss function to handle outofcontext labels automatically during the training phase. Also, we introduce hierarchical loss normalization to adjust the penalties for corre lated types, allowing our model to understand the type hierarchy and alleviate the negative effect of overlyspeciÔ¨Åc labels. Moreover, in order to simplify the problem and take advantage of previous research on hierar chical classiÔ¨Åcation, we transform the multilabel classiÔ¨Åcation problem to a singlelabel classiÔ¨Åca tion problem. Based on the assumption that each mention can only have one typepath depending on the context, we leverage the fact that type hier archies are forests, and represent each typepath uniquely by the terminal type (which might not be a leaf node). For Example, typepath root personcoach can be represented as just coach , while rootperson can be unambiguously repre sented as the nonleaf person . Finally, we report on an experimental validationagainst the stateoftheart on established bench marks that shows that our model can adapt to noise in training data and consistently outperform previ ous methods. In summary, we describe a single, much simpler and more elegant neural network model that attempts FETC ‚Äúendtoend‚Äù without postprocessing or adhoc features and improves on the stateoftheart for the task. 2 Related Work "
576,Treatment Learning Transformer for Noisy Image Classification.txt,"Current top-notch deep learning (DL) based vision models are primarily based
on exploring and exploiting the inherent correlations between training data
samples and their associated labels. However, a known practical challenge is
their degraded performance against ""noisy"" data, induced by different
circumstances such as spurious correlations, irrelevant contexts, domain shift,
and adversarial attacks. In this work, we incorporate this binary information
of ""existence of noise"" as treatment into image classification tasks to improve
prediction accuracy by jointly estimating their treatment effects. Motivated
from causal variational inference, we propose a transformer-based architecture,
Treatment Learning Transformer (TLT), that uses a latent generative model to
estimate robust feature representations from current observational input for
noise image classification. Depending on the estimated noise level (modeled as
a binary treatment factor), TLT assigns the corresponding inference network
trained by the designed causal loss for prediction. We also create new noisy
image datasets incorporating a wide range of noise factors (e.g., object
masking, style transfer, and adversarial perturbation) for performance
benchmarking. The superior performance of TLT in noisy image classification is
further validated by several refutation evaluation metrics. As a by-product,
TLT also improves visual salience methods for perceiving noisy images.","Although deep neural networks (DNNs) have surpassed humanlevel ‚Äúaccuracy‚Äù in many image recognition tasks [Ron neberger et al., 2015, He et al., 2016, Huang et al., 2017], current DNNs still implicitly rely on the assumption [Pearl, 2019] on the existence of a strong correlation between training and testing data. Moreover, increasing evidence and concerns [Alcorn et al., 2019] show that using the correlation association for prediction can be problematic against noisy images [Xiao et al., 2015], such as poseshifting of identical objects [Alcorn et al., 2019] or imperceptible perturbation [Goodfellow et al., 2015]. In practice, realworld image classiÔ¨Åcation often involves rich, noisy, and even chaotic contexts, intensifying the demand for generalization in the wild. Putting in a uniÔ¨Åed descriptive framework, To address machine perception against noisy images, we are inspired by how human performs visual recognition. Human‚Äôs learning processes are often mixed with logic inference (e.g., a symbolic deÔ¨Ånition from books) and rep resentation learning (e.g., an experience of viewing a visual pattern). One prominent difference between current DNNs and human recognition systems is the capability in causal inference. Mathematically, causal learning [Pearl, 1995a, Peters et al., 2014] is a statistical inference model that infers beliefs or probabilities under uncertain conditions, which aims to identify latent variables (called ‚Äúconfounders‚Äù) that inÔ¨Çuence both intervention and outcome. The unobserved confounders may be abstract in a cognitivelevel (e.g., concepts) but could be observed via their noisy view in the realworld (e.g., objects). For instance, as shown in Fig. 1 (a), confounder learning aims to model aarXiv:2203.15529v1  [cs.CV]  29 Mar 2022Treatment Learning Transformer for Noisy Image ClassiÔ¨Åcation A P REPRINT ICCV CGM Huck CH Yang March 2021 1 Introduction Zy X t (a) CGM Figure 1: Causal graphical model (CGM) for causal video multimodal sum marization (CVS) training phase (a) and testing phase (b). White nodes X (e.g., input data) and y(e.g., output label) are observable. Node Z, colored by grey, is a not observable and latent confounder from representation learning. Xis a noisy view on the hidden confounder Z, say the input text query and video. Node t, colored by grey in (b), is a treatment, e.g., visual or textual per turbation, referring to Section 3.5 for details, which is only observable during training. 1 (i) Train (ii) Test (clean) (iv) Context noise (iii) Additive noise  Figure 1: (a)An example of deployed causal graphical model (CGM), where Zdenotes unobservable confounder variable (e.g., the concept of ‚Äúcat‚Äù), Xdenotes a noisy observation of confounder (e.g., an image can still be recognized as a cat), ydenotes outcome (e.g., a label), and tdenotes the information of a binary treatment (e.g., the existence of extra semantic patterns or additive noise; thus, it is equal to 0or1), which is observable during training and unobservable during testing time. (b)Images with ‚Äúcat‚Äù labels, where (i) and (ii) share the same context of ‚Äúindoor‚Äù; (iii) shows a noisy setup of (ii) undergoing additive Gaussian perturbation; (iv) shows another setup of introducing extra noisy semantic patterns (e.g., ‚Äúwaterside‚Äù) in NICO [He et al., 2020] noisy images dataset. prediction process by Ô¨Ånding a representation (e.g., ‚Äúcat‚Äù) and avoiding relying on irrelevant patterns (e.g., ‚Äúwaterside‚Äù). Intuitively, with causal modeling and confounder inference, correct prediction can be made on noisy inputs, where the generative estimation process, such as causal effect variational autoencoder (CEV AE) [Louizos et al., 2017], affects multiple covariates for predicting data proÔ¨Åles. In this work, we aim to incorporate the effects of causal confounder learning to image classiÔ¨Åcation, as motivated by cognitive psychology for causal learning. SpeciÔ¨Åcally, we use the attention mechanism for noiseresilience inference from patterns. We design a novel sequencetosequence learning model, Treatment Learning Transformer (TLT) , which leverages upon the conditional querybased attention and the inference power from a variational causal inference model. Our TLT tackles noisy image classiÔ¨Åcation by jointly learning to a generative model of Zand estimating the effects from the treatment information ( t), as illustrated in Fig. 1 (a). This model consists of unobservable confounder variables Zcorresponding to the groundtruth but inaccessible information (e.g., the ontological concept [Trampusch and Palier, 2016] of a label), input data Xfrom a noisy view of Z(e.g., images), a treatment [Pearl et al., 2016] information tgiven XandZ(e.g., secondary information as visual patterns and additive noise without directly affecting our understanding the concept of ‚Äúcat‚Äù), and a classiÔ¨Åcation label yfrom the unobservable confounder. Built upon this causal graphical model, our contributions are: ‚Ä¢A transformer architecture (TLT) for noisy image classiÔ¨Åcation are presented, which is based on a treatment estimation architecture and a causal variational generative model with competitive classiÔ¨Åcation performance against noisy image. ‚Ä¢We further curated a new noisy images datasets, Causal Pairs (CPS), to study generalization under different artiÔ¨Åcial noise settings for general and medical images. ‚Ä¢We use formal statistical refutations tests to validate the causal effect of TLT, and show that TLT can improve visual saliency methods on noisy images. 2 Related Work "
348,The Devil is in the Labels: Noisy Label Correction for Robust Scene Graph Generation.txt,"Unbiased SGG has achieved significant progress over recent years. However,
almost all existing SGG models have overlooked the ground-truth annotation
qualities of prevailing SGG datasets, i.e., they always assume: 1) all the
manually annotated positive samples are equally correct; 2) all the
un-annotated negative samples are absolutely background. In this paper, we
argue that both assumptions are inapplicable to SGG: there are numerous ""noisy""
groundtruth predicate labels that break these two assumptions, and these noisy
samples actually harm the training of unbiased SGG models. To this end, we
propose a novel model-agnostic NoIsy label CorrEction strategy for SGG: NICE.
NICE can not only detect noisy samples but also reassign more high-quality
predicate labels to them. After the NICE training, we can obtain a cleaner
version of SGG dataset for model training. Specifically, NICE consists of three
components: negative Noisy Sample Detection (Neg-NSD), positive NSD (Pos-NSD),
and Noisy Sample Correction (NSC). Firstly, in Neg-NSD, we formulate this task
as an out-of-distribution detection problem, and assign pseudo labels to all
detected noisy negative samples. Then, in Pos-NSD, we use a clustering-based
algorithm to divide all positive samples into multiple sets, and treat the
samples in the noisiest set as noisy positive samples. Lastly, in NSC, we use a
simple but effective weighted KNN to reassign new predicate labels to noisy
positive samples. Extensive results on different backbones and tasks have
attested to the effectiveness and generalization abilities of each component of
NICE.","Scene Graph Generation (SGG), i.e., detecting all object instances and their pairwise visual relations, is a crucial steparXiv:2206.03014v1  [cs.CV]  7 Jun 2022towards comprehensive visual scene understanding. In gen eral, each scene graph is a visuallygrounded graph, where each node and edge refer to an object and visual relation, re spectively. Recently, with the release of several largescale SGG benchmarks ( e.g., Visual Genome (VG) [15]) and ad vanced object detectors [28, 1, 35], SGG has received un precedented attention [7]. However, due to the composi tional nature of pairwise visual relations, the number distri butions of different triplets in SGG datasets are much more imbalanced ( i.e., longtailed) than other recognition tasks. Accordingly, the performance of many stateoftheart SGG models [42, 2, 32, 23] degrades signiÔ¨Åcantly on the tailcat egories1compared to the head categories counterparts. Currently, the mainstream solutions to mitigate the long tailed problem in SGG can be coarsely categorized into two types: 1) Rebalancing strategy : It utilizes classaware sam ple resampling or loss reweighting to balance the propor tions of different predicate categories in the network train ing. The former attempts to balance the number of training samples in instancelevel2or imagelevel [17], and the latter leverages prior commonsense knowledge ( e.g., frequency of predicates [22], predicate correlations [39], or rulebased predicate priority [23, 14]) to reweight the contributions of different categories in loss calculations. 2) Biasedmodel based strategy : It inferences debiased predictions from pre trained biased SGG models. For instance, using counterfac tual causality to disentangle frequency biases [31], deriving more balanced loss weights for different predicates [41], or adjusting the probabilities of predicate predictions [4]. Although these methods have dominated performance on debiasing metrics ( e.g., mean Recall@K), it is worth noting that almost all existing models have taken two plausible as sumptions about the groundtruth annotations for granted: Assumption 1: All the manually annotated positive sam ples are equally correct. Assumption 2: All the unannotated negative samples are absolutely background. For the Ô¨Årst assumption, by ‚Äúequally‚Äù, we mean that the conÔ¨Ådence (or quality) of annotated groundtruth predicate label for each positive sample2is the same as others, i.e., all positive predicate labels are of high quality. Unfortunately, unlike other closeset classiÔ¨Åcation tasks where each sample has only a unique groundtruth label, a subjectobject pair in SGG sometimes has multiple reasonable predicates. This phenomenon has led to two inevitable annotation charac teristics in SGG datasets: 1) Commonprone : When these reasonable relations are in different semantic granularities, 1For brevity, we directly use ‚Äútail‚Äù, ‚Äúbody‚Äù, and ‚Äúhead‚Äù categories to represent the predicate categories in the tail, body, and head parts of the number distributions of different predicates in SGG datasets, respectively. 2We use ‚Äúinstance‚Äù to denote an instance of visual relation triplet, and we also use ‚Äúsample‚Äù to represent the triplet instance interchangeably.the annotators tend to select the most common predicate (or coarsegrained) as groundtruth. As shown in Figure 1(a), bothriding andonare ‚Äúreasonable‚Äù for man andbike , but the annotated groundtruth predicate is less informative oninstead of more convincing riding . And this charac teristic is very common in SGG datasets (more examples in Figure 1(a)). 2) Synonymrandom : When these reasonable relations are synonymous for the subjectobject pair, the an notators usually randomly choose one predicate as ground truth, i.e., the annotations for some similar visual patterns are inconsistent. For example, in Figure 1(b), both has and with denote ‚Äúbe dressed in‚Äù for man/woman andshirt , but the groundtruth annotations are inconsistent even in the same image. We further visualize thousands of sampled in stances ofhmanhas /with shirtiin VG, and these in stances are all randomly distributed in the feature space (cf. Figure 1(b)). Thus, we argue that all the positive samples are NOT equally correct, i.e., a part of positive samples are not highquality ‚Äî their labels can be more Ô¨Ånegrained (cf. commonprone) or more consistent (cf. synonymrandom). For the second assumption, although all SGG works have agreed that visual relations in existing datasets are always sparsely identiÔ¨Åed and annotated [25] (Figure 1(c)), almost all of them still train their models by regarding all the un annotated pairs as background ,i.e., there is no visual re lation between the subject and object. In contrast, we argue that all negative samples are NOT absolutely background, i.e., a part of negative samples are not highquality ‚Äî they are actually foreground with missing annotations. In this paper, we try to get rid of these two questionable assumptions, and reformulate SGG as a noisy label learning problem. To the best of our knowledge, we are the Ô¨Årst work to take a deep dive into the groundtruth annotation qualities of both positive and negative samples in SGG. SpeciÔ¨Åcally, we propose a novel modelagnostic NoIsy label CorrEction strategy, dubbed as NICE . NICE can not only detect numer ousnoisy samples, but also reassign more highquality pred icate labels to them. By ‚Äúnoisy‚Äù, we mean that these sam ples break these two assumptions. After the NICE training, we can obtain a cleaner version of dataset for SGG training. Particularly, we can: 1) increase the number of Ô¨Ånegrained predicates ( commonprone ); 2) decrease annotation incon sistency among similar visual patterns ( synonymrandom ); 3) increase the number of positive samples ( assumption 2 ). NICE consists of three components: negative noisy sam ple detection (NegNSD), positive noisy sample detection (PosNSD), and noisy sample correction (NSC). Firstly, in NegNSD, we reformulate the negative NSD as an outof distribution (OOD) detection problem, i.e., regarding all the positive samples as indistribution (ID) training data, and all the unannotated negative samples as OOD test data. In this way, we can detect the missing annotated (ID) samples with pseudo labels. Then, in PosNSD, we use a clusteringbasedalgorithm to divide all positive samples (including the out puts of NegNSD) into multiple sets, and regard samples in the noisiest set as noisy positive samples. The clustering re sults are based on the local density of each sample. Lastly, in NSC, we use a simple but effective weighted KNN to reassign new predicate labels to all noisy positive samples. We evaluate NICE on the most prevalent SGG bench mark: VG [15]. Since NICE only focuses on reÔ¨Åning noisy annotations of the dataset, it can be seamlessly incorporated into any SGG architecture to boost their performance. Ex tensive ablations have attested to the effectiveness and gen eralization abilities of each component of NICE. In summary, we make three contributions in this paper: 1. We are the Ô¨Årst to reformulate SGG as a noisy label learn ing problem, and point out the two plausible assumptions are not applicable for SGG, i.e., the devil is in the labels. 2. We propose a novel modelagnostic strategy NICE. Ex tensive ablations on several baselines, tasks, and metrics have demonstrated its excellent generalization abilities. 3. Each part of NICE can serve as an independent plugand play module to improve SGG annotation qualities3. 2. Related Work "
565,SplitNet: Learnable Clean-Noisy Label Splitting for Learning with Noisy Labels.txt,"Annotating the dataset with high-quality labels is crucial for performance of
deep network, but in real world scenarios, the labels are often contaminated by
noise. To address this, some methods were proposed to automatically split clean
and noisy labels, and learn a semi-supervised learner in a Learning with Noisy
Labels (LNL) framework. However, they leverage a handcrafted module for
clean-noisy label splitting, which induces a confirmation bias in the
semi-supervised learning phase and limits the performance. In this paper, we
for the first time present a learnable module for clean-noisy label splitting,
dubbed SplitNet, and a novel LNL framework which complementarily trains the
SplitNet and main network for the LNL task. We propose to use a dynamic
threshold based on a split confidence by SplitNet to better optimize
semi-supervised learner. To enhance SplitNet training, we also present a risk
hedging method. Our proposed method performs at a state-of-the-art level
especially in high noise ratio settings on various LNL benchmarks.","Deep Neural Networks (DNNs) generally rely on largescale training data with humanannotated good labels for achieving satisfactory perfor mance [24]. However, due to the high costs and complexity of labeling the data, the labels are oftencontaminatedbynoise,andthusmanyworks have strived to develop alternative methods that are robust to label noise, which is often called Learning with Noisy Labels (LNL) [37]. Recent studies for LNL, in general, have attempted to distinguish clean samples from the noisy dataset using handcrafted methods, e.g.,Gaussian Mixture Models (GMMs), and then use these clean samples as labeled samples in the SemiSupervised Learning (SSL) phase [29, 38]. However, the shape of the loss distribution often does not follow the Gaussian distribution [2], and data with loss values that are not large or small enoughcannotbeproperlydistinguished.Further more, the dominant approaches maintain multiple models to avoid the risk attributable to the ability ofDNNstoÔ¨Åtarbitrarylabels,butthisoftenleads to complicated training procedures [20]. More over, in the aforementioned LNL methodology that leverages SSL techniques, the weight of the 1arXiv:2211.11753v2  [cs.LG]  19 Dec 2022Springer Nature 2021 L ATEX template 2 Article Title SplitNetMain NetworkPrediction History,LabelClean Train Dataset,Split Confidence Fig. 1:The concept of our alternating update framework with SplitNet. When the main network outputs the prediction history and label,SplitNetusesthemtogenerateacleantrain ing dataset and deliver it to the main network, which is then used as the labeled data of SSL. These two phases are alternatively used to boost convergence and performance. unlabeled loss, one of the most substantial hyper parameter, must be adjusted carefully depending on the noise ratio to prevent the model from over Ô¨Åtting. However, the noise ratio is challenging to tease out in a realworld environment, proving to be an unrealistic approach. To overcome these limitations, we present a novel framework incorporating a learnable net work, called SplitNet, which splits the clean and noisy data in a datadriven manner. Contrary to conventional methods [29, 38] that Ô¨Åt GMMs solely based on persample loss distribution to select clean samples, our SplitNet can addition ally incorporate the prediction history as input, which allows us to better distinguish ambiguous samples that cannot be precisely distinguished by GMM. In addition, we use a split conÔ¨Ådence, a score indicating how conÔ¨Ådently SplitNet divides the samples, to determine whether to apply unsu pervisedloss,enablingmorestablelearningofSSL method in LNL settings. More speciÔ¨Åcally, our overall framework begins with a warmup and then iteratively learns the main network and SplitNet. As shown in Fig. 1, by formulating the main network and SplitNet in an iterative manner, the two learners are alter natelyupdated,eachusingthedatafromtheother network. For SplitNet training, the main network provides class prediction and loss distribution, while for the main network training, SplitNet provides split conÔ¨Ådences to Ô¨Çexibly adjust the threshold for its SSL procedure. By doing so,whereas previous stateoftheart methods [29, 38] require diÔ¨Äerent hyperparameter settings for dif ferent noise ratios, our proposed model achieves superior performance on all benchmarks, despite its simplicity, requiring only one model and a hyperparameter setting. In particular, taking into account the learn ing status of the main network and the estimated noise ratio of the data set, the thresholds are auto matically calculated to distinguish conÔ¨Ådently clean and noisy samples. This process which we dub risk hedging, results in a favorable learning environment for SplitNet to mitigate conÔ¨Årmation bias. As the number of conÔ¨Ådently clean and noisy sample increase throughout the process, SplitNet enjoys the beneÔ¨Åt of a natural curriculum with the aid of the gradually increasing number of hard samples. The key contributions of this method are as follows: ‚Ä¢Our method eÔ¨Äectively distinguishes clean sam ples from noisy datasets compared to other methods through a learnable network called SplitNet. ‚Ä¢As our method enables the learning curricu lum to adjust automatically depending on noise ratio, we propose the SSL method that is favorable to LNL by utilizing split conÔ¨Ådence obtained through SplitNet. ‚Ä¢Our method signiÔ¨Åcantly outperforms stateof theart results on numerous benchmarks with diÔ¨Äerent types and levels of label noise. 2 Related Work "
282,Noisy Label Detection for Speaker Recognition.txt,"The success of deep neural networks requires both high annotation quality and
massive data. However, the size and the quality of a dataset are usually a
trade-off in practice, as data collection and cleaning are expensive and
time-consuming. Therefore, automatic noisy label detection (NLD) techniques are
critical to real-world applications, especially those using crowdsourcing
datasets. As this is an under-explored topic in automatic speaker verification
(ASV), we present a simple but effective solution to the task. First, we
compare the effectiveness of various commonly used metric learning loss
functions under different noise settings. Then, we propose two ranking-based
NLD methods, inter-class inconsistency and intra-class inconsistency ranking.
They leverage the inconsistent nature of noisy labels and show high detection
precision even under a high level of noise. Our solution gives rise to both
efficient and effective cleaning of large-scale speaker recognition datasets.","The success of Deep Neural Networks (DNNs) heavily relies on largescale datasets with highquality annotations. How ever, in practice, the size and the quality of datasets are usu ally a tradeoff, as collecting reliable datasets is laborintensive and timeconsuming. This implies that datasets usually contain some hardtoestimate inaccurate annotations, often referred to as ‚Äùnoisy labels‚Äù. Noisy labels may overparameterize DNNs and lead to performance degradation due to the memorization effect. These pose bottlenecks in training and employing DNNs in realworld scenarios. Various strategies have been proposed to mitigate the neg ative effects of noisy labels, usually referred to as noisylabel learning (NLL) techniques [1]. Loss correction technique is one of the effective ways [2, 3, 4, 5, 6, 7]. It usually involves mod ifying the loss function to make it robust to the label noise, or correcting the loss values according to the label noise. Another approach is to let neural nets correct the labels during training. The correction signal can be computed from an external neural structure [8] or algorithm [9], or internal structure, i.e. a noise layer[10]. Coteaching is another way of label correction. It usually involves training two networks simultaneously through dataset codivide, label coreÔ¨Ånement, and coguessing, in order to Ô¨Ålter out label noise for each other [11, 12, 13]. The majority of such research studies are from the computer vision domain. For the audio and speech domain, much less re search on noisy labels has been conducted. [14] applied semi supervised techniques to learn from soft pseudo labels, and en sembles the predictions to overcome noisy data. [15] explored the extent of noisy labels‚Äô inÔ¨Çuences on trained xvector embeddings [16]. They showed that mislabeled data can severely damage the performance of speaker veriÔ¨Åcation systems on the NIST SRE 2016 dataset [17], and proposed regularization ap proaches to mitigate the damage. Later, [18] also demonstrated that label noise leads to signiÔ¨Åcant performance degradation for both the xvector frontend and PLDA backend on NIST SRE 2016, and proposed a few strategies to resist label noise. Contrary to previous results, [19] experimented with simulating noise on the V oxCeleb2 dataset [20], and showed that even high levels of label noise had only a slight impact on the ASV task, using either GE2E or CE loss functions. However, the noises that they investigate are all closedset noise [21], and never con sider another type of frequently occurring noise, openset noise. The most similar work to this work is [22], which proposed an iterative Ô¨Åltering method to remove noisy labels from the train ing set iteratively in order to improve the speaker recognition performance on the V oxCeleb. However, it requires manually setting a similarity threshold for the Ô¨Åltering and focuses on mitigating the noise effect. In this paper, instead of directly mitigating the effects of la bel noise, we try to address this problem from the root cause: we focus on the detection of noisy labels1. Detecting and Ô¨Åltering noisy labels beneÔ¨Åts not only the model training, but also the crowdsourcing data platforms, e.g. Amazon Mechanical Turk and Scale AI. The Ô¨Åltered noisy labels can be used for relabeling to build higher quality datasets, or identifying the noisy labels providers, i.e. cheaters. SpeciÔ¨Åcally, we investigate noisy label detection (NLD) under the context of speaker recognition, as this has been an underexplored topic, yet it plays an important role in collecting and correcting speaker identity annotations in largescale speech datasets. We are the Ô¨Årst to comprehensively compared various common metric learning loss functions used in the ASV task under a variety of noise settings, openset noise and closedset noise with different noise levels, on a relatively large scale for the Ô¨Årst time. Second, we propose two ranking based NLD methods based on the inconsistent nature of the noisy labels, and achieve high detection precision even under high level of noise. 2. Method "
484,Webly Supervised Image Classification with Metadata: Automatic Noisy Label Correction via Visual-Semantic Graph.txt,"Webly supervised learning becomes attractive recently for its efficiency in
data expansion without expensive human labeling. However, adopting search
queries or hashtags as web labels of images for training brings massive noise
that degrades the performance of DNNs. Especially, due to the semantic
confusion of query words, the images retrieved by one query may contain
tremendous images belonging to other concepts. For example, searching `tiger
cat' on Flickr will return a dominating number of tiger images rather than the
cat images. These realistic noisy samples usually have clear visual semantic
clusters in the visual space that mislead DNNs from learning accurate semantic
labels. To correct real-world noisy labels, expensive human annotations seem
indispensable. Fortunately, we find that metadata can provide extra knowledge
to discover clean web labels in a labor-free fashion, making it feasible to
automatically provide correct semantic guidance among the massive label-noisy
web data. In this paper, we propose an automatic label corrector VSGraph-LC
based on the visual-semantic graph. VSGraph-LC starts from anchor selection
referring to the semantic similarity between metadata and correct label
concepts, and then propagates correct labels from anchors on a visual graph
using graph neural network (GNN). Experiments on realistic webly supervised
learning datasets Webvision-1000 and NUS-81-Web show the effectiveness and
robustness of VSGraph-LC. Moreover, VSGraph-LC reveals its advantage on the
open-set validation set.","Deep convolutional neural networks (CNNs) are successful by virtue of largescale datasets with human annotation [ 23]. However, human annotation is extremely timeconsuming and expensive, which impedes the further expansion of those big datasets [ 42]. To overcome this limitation, researchers use web crawlers to collect billions of images and annotate them directly using text queries or hashtags [ 20,30]. However, due to the ambiguity or polysemy of the query fed into the search engine, label noise is subsequently introduced. Therefore, webly supervised learning, aiming at using huge scalable webcrawled data directly for networks training by suppressing label noise, has attracted great attention recently [1]. Early exploration in this direction relies on humanverified clean subsets. Representative methods using clean subsets include Men torNet [ 17] and CleanNet [ 24]. However, with the trend of rapid growth in the size of webly datasets, building clean subsets with manual verification becomes more infeasible, especially when the number of categories exceeds ten thousands [ 45]. Therefore, re cent works prefer models without clean subset dependencies, mak ing webly supervised learning fully automatic. To this end, some works strengthen networks‚Äô endurability against label noise using moving average of model predictions [ 40], loss function modifi cation [ 11,31] or regularization [ 34,54] . Coteaching uses two different networks to mutually detect label noise, doubly ensuring the model‚Äôs denoising ability [ 13]. Other works identify noisy sam ples based on some hypotheses including data density [ 12,14] or model confidence [50].arXiv:2010.05864v1  [cs.CV]  12 Oct 2020(a) Colors reflect web labels. Web label ‚ÄôDrumstick‚Äô shows rep resentative images corresponding to 5 regions of interest. Only Region5 corresponds to the correct concept (b) Prediction by Coteaching  (c) Prediction by VSGraphLC Figure 1: TSNE visualization [29] of WebVisionpretrained ResNet50 [15] features on 10 selected categories. Three ob servations are highlighted: (1) CNN models that trained from WebVision can distinguish different semantics within a category, even when semantics mismatch category defini tion. (2) Severe semantic label noise is a realworld problem, as majority images of class ‚Äòdrumsticks‚Äô deviate the true con cept of percussion mallets. (3) Coteaching fails to correct the majority semantic label noise, but our VSGraphLC is able to. Node brightness represents prediction confidence Although the aforementioned methods effectively enhance the model against label noise, especially for outliers, suppressing se mantic label noise is critical but untouched. To clarify, semantic label noise exists due to query‚Äôs polysemy or insufficient semantic resolution. Usually, semantic label noise would be severe in some category, which is composed of a large number of samples reflecting another semantic concept. As webly datasets come from realworld, those offtarget semantics are usually outofdistribution (i.e., devi ate from all semantic labels or out of interests in the test sets). For example, Figure 1a shows that although web label ‚Äòdrum sticks‚Äô has its correct semantic concept of ‚Äòpercussion mallets‚Äô ac cording to the test set, the majority of training samples actually belong to concepts of ‚Äòdrumstick trees/vegetable‚Äô and ‚Äòchicken legs‚Äô due to polysemy. These outofdistribution noisy samples clearly cluster themselves in the visual feature space. As a result, density assumption popularly adopted by previous methods [ 12,14] will re gard ‚Äòchicken legs‚Äô and ‚Äòdrumstick trees/vegetable‚Äô samples falsely as clean data. Figure 1b further shows the ineffectiveness of the rep resentative selftraining method Coteaching, where most samples belong to offtarget semantics are still predicted as positive. As webly dataset is crawled from the Internet, text metadata associated with web images has great potential to provide valuable Label Descripon from WordNet:Drums&ck: a light drums&ck with a rounded head that is used to strike such percussion instruments as chimes, ke8ledrums, marimbas, glockenspiels, etc. ‚úì‚úò‚úòSimilarDissimilarDissimilarFigure 2: Exemplar metadata associated with web images la beled as ‚ÄòDrumstick‚Äô. By comparing metadata with label de scription, images with semantic label noise will be detected information, which, however, has been ignored for a long time. In this paper, we aim to take advantage of extra knowledge provided by metadata to suppress label noise, especially semantic label noise. Figure 2 shows that information in metadata and label description can be used to detect semantic label noise. By using offtheshelf natural language processing (NLP) models [ 52], we convert the comparison between label description and metadata from human cognition level to text feature space, which ensures a fully auto mated process to precisely locate samples with correct semantic concepts, without the need for expensive manual annotations. Hence, we are motivated to build an automatic pipeline for webly supervised learning with metadata. Specifically, we propose a label corrector named VSGraphLC, which first selects anchor samples for each category through matching label description from Word Net [ 33] and metadata of every crawled image using a powerful NLP model XLNet [ 52]. To help those semantically correct anchors propagate their web labels towards more samples, we leverage a graph neural network (GNN) [ 18] training on ùëòNN visual feature graph of the entire training set. The corrected labels substitute the former noisy web labels for finetuning our final model. In summary, our contributions are mainly threefold: ‚Ä¢We explore two understudied but important factors under webly supervised learning setting: semantic label noise and text metadata. ‚Ä¢A humanlaborfree label correcting framework that fully exploits the merits of GNN and CNN is proposed as VSGraph LC, ignited by anchors automatically selected by metadata. ‚Ä¢The proposed framework is shown effective on NUS81Web and WebVision datasets, and reaches the stateoftheart result on WebVision1000. VSGraphLC produces more ap pealing results if the test set contains outofdistribution images1. 1Known as openset recognition task, introduced in [22]2 RELATED WORK "
579,Modelling Instance-Level Annotator Reliability for Natural Language Labelling Tasks.txt,"When constructing models that learn from noisy labels produced by multiple
annotators, it is important to accurately estimate the reliability of
annotators. Annotators may provide labels of inconsistent quality due to their
varying expertise and reliability in a domain. Previous studies have mostly
focused on estimating each annotator's overall reliability on the entire
annotation task. However, in practice, the reliability of an annotator may
depend on each specific instance. Only a limited number of studies have
investigated modelling per-instance reliability and these only considered
binary labels. In this paper, we propose an unsupervised model which can handle
both binary and multi-class labels. It can automatically estimate the
per-instance reliability of each annotator and the correct label for each
instance. We specify our model as a probabilistic model which incorporates
neural networks to model the dependency between latent variables and instances.
For evaluation, the proposed method is applied to both synthetic and real data,
including two labelling tasks: text classification and textual entailment.
Experimental results demonstrate our novel method can not only accurately
estimate the reliability of annotators across different instances, but also
achieve superior performance in predicting the correct labels and detecting the
least reliable annotators compared to state-of-the-art baselines.","In many natural language processing (NLP) ap plications, the performance of supervised machine learning models depends on the quality of the cor pus used to train the model. Traditionally, la bels are collected from multiple annotators/experts 1Code is available at https://github.com/ createmomo/instancelevelreliabilitywho are assumed to provide reliable labels. How ever, in reality, these experts may have varying levels of expertise depending on the domains, and thus may disagree on labelling in certain cases (Aroyo and Welty, 2013). A rapid and cost effective alternative is to obtain labels through crowdsourcing (Snow et al., 2008; Poesio et al., 2013, 2017). In crowdsourcing, each instance is presented to multiple expert or nonexpert anno tators for labelling. However, labels collected in this manner could be noisy, since some annotators could produce a signiÔ¨Åcant number of incorrect la bels. This may be due to differing levels of exper tise, lack of Ô¨Ånancial incentive and interest (Poesio et al., 2017), as well as the tedious and repetitive nature of the annotation task (Raykar et al., 2010; Bonald and Combes, 2017). Thus, in order to ensure the accuracy of the la belling and the quality of the corpus, it is crucial to estimate the reliability of the annotators automati cally without human intervention. Previous studies have mostly focused on evalu ating the annotators‚Äô overall reliability (Gurevych and Kim, 2013; Sheshadri and Lease, 2013; Poe sio et al., 2017). Measuring the reliability on a perinstance basis is however useful as we may expect certain annotators to have more expertise in one domain than another, and as a consequence certain annotation decisions will be more difÔ¨Åcult than others. This resolves a potential issue of mod els that only assign an overall reliability to each annotator, where such a model would determine an annotator with expertise in a single domain to be unreliable for the model, even though the anno tations are reliable within the annotator‚Äôs domain of expertise. Estimating perinstance reliability is also help ful for unreliable annotator detection and task allocation in crowdsourcing, where the cost of labelling data is reduced using proactive learnarXiv:1905.04981v1  [cs.CL]  13 May 2019ing strategies for pairing instances with the most costeffective annotators (Donmez and Carbonell, 2008; Li et al., 2017). Although reliability esti mation has been studied for a long time, only a limited number of studies have examined how to model the reliability of each annotator on a per instance basis. Additionally, these in turn have only considered binary labels (Yan et al., 2010, 2014; Wang and Bi, 2017), and cannot be extended to multiclass classiÔ¨Åcation in a straightforward manner. In order to handle both binary and multiclass labels, our approach extends one of the most popu lar probabilistic models for label aggregation, pro posed by Hovy et al. (2013). One challenge of extending the model is the deÔ¨Ånition of the label and reliability probability distributions on a per instance basis. Our approach introduces a classi Ô¨Åer which predicts the correct label of an instance, and a reliability estimator, providing the probabil ity that an annotator will label a given instance cor rectly. The approach allows us to simultaneously estimate the perinstance reliability of the annota tors and the correct labels, allowing the two pro cesses to inform each other. Another challenge is to select appropriate training methods to learn a model with high and stable performance. We in vestigate training our model using the EM algo rithm and cross entropy. For evaluation, we ap ply our method to six datasets including both syn thetic and realworld datasets (see Section 4.1). In addition, we also investigate the effect on the per formance when using different text representation methods and text classiÔ¨Åcation models (see Sec tion 4.2). Our contributions are as follows: Ô¨Årstly, we pro pose a novel probabilistic model for the simulta neous estimation of perinstance annotator relia bility and the correct labels for natural language labelling tasks. Secondly, our work is the Ô¨Årst to propose a model for modelling perinstance relia bility for both binary and multiclass classiÔ¨Åcation tasks. Thirdly, we show experimentally how our method can be applied to different domains and tasks by evaluating it on both synthetic and real world datasets. We demonstrate that our method is able to capture the reliability of each annotator on a perinstance basis, and that this in turn helps improve the performance when predicting the un derlying label for each instance and detecting the least reliable annotators.2 Related Work "
364,Asymmetric Tri-training for Unsupervised Domain Adaptation.txt,"Deep-layered models trained on a large number of labeled samples boost the
accuracy of many tasks. It is important to apply such models to different
domains because collecting many labeled samples in various domains is
expensive. In unsupervised domain adaptation, one needs to train a classifier
that works well on a target domain when provided with labeled source samples
and unlabeled target samples. Although many methods aim to match the
distributions of source and target samples, simply matching the distribution
cannot ensure accuracy on the target domain. To learn discriminative
representations for the target domain, we assume that artificially labeling
target samples can result in a good representation. Tri-training leverages
three classifiers equally to give pseudo-labels to unlabeled samples, but the
method does not assume labeling samples generated from a different domain.In
this paper, we propose an asymmetric tri-training method for unsupervised
domain adaptation, where we assign pseudo-labels to unlabeled samples and train
neural networks as if they are true labels. In our work, we use three networks
asymmetrically. By asymmetric, we mean that two networks are used to label
unlabeled target samples and one network is trained by the samples to obtain
target-discriminative representations. We evaluate our method on digit
recognition and sentiment analysis datasets. Our proposed method achieves
state-of-the-art performance on the benchmark digit recognition datasets of
domain adaptation.","With the development of deep neural networks in cluding deep convolutional neural networks (CNN) 1The University of Tokyo, Tokyo, Japan. Correspon dence to: Kuniaki Saito <ksaito@mi.t.utokyo.ac.jp >, Yoshi taka Ushiku <ushiku@mi.t.utokyo.ac.jp >, Tatsuya Harada <harada@mi.t.utokyo.ac.jp >.(Krizhevsky et al. ,2012 ), the recognition abilities of im ages and languages have improved dramatically. Train ing deeplayered networks with a large number of la beled samples enables us to correctly categorize samples in diverse domains. In addition, the transfer learning of CNN is utilized in many studies. For object detection or segmentation, we can transfer the knowledge of a CNN trained with a largescale dataset by Ô¨Ånetuning it on a relatively small dataset ( Girshick et al. ,2014 ;Long et al. , 2015a ). Moreover, features from a CNN trained on Ima geNet ( Deng et al. ,2009 ) are useful for multimodal learn ing tasks including image captioning ( Vinyals et al. ,2015 ) and visual question answering ( Antol et al. ,2015 ). One of the problems of neural networks is that although they perform well on the samples generated from the same distribution as the training samples, they may Ô¨Ånd it difÔ¨Å cult to correctly recognize samples from different distrib u tions at the test time. One example is images collected from the Internet, which may come in abundance and be fully la beled. They have a distribution different from the images taken from a camera. Thus, a classiÔ¨Åer that performs well on various domains is important for practical use. To real ize this, it is necessary to learn domaininvariantly discr im inative representations. However, acquiring such represe n tations is not easy because it is often difÔ¨Åcult to collect a large number of labeled samples and because samples from different domains have domainspeciÔ¨Åc characteristics. In unsupervised domain adaptation, we try to train a clas siÔ¨Åer that works well on a target domain on the condi tion that we are provided labeled source samples and un labeled target samples during training. Most of the pre vious deep domain adaptation methods have been pro posed mainly under the assumption that the adaptation can be realized by matching the distribution of features from different domains. These methods aimed to ob tain domaininvariant features by minimizing the diver gence between domains as well as a category loss on the source domain ( Ganin & Lempitsky ,2014 ;Long et al. , 2015b ;2016 ). However, as shown in ( BenDavid et al. , 2010 ), theoretically, if a classiÔ¨Åer that works well on both the source and the target domains does not exist, we can not expect a discriminative classiÔ¨Åer for the target domain . That is, even if the distributions are matched on the non discriminative representations, the classiÔ¨Åer may not wor kAsymmetric Tritraining for Unsupervised Domain Adaptati on !""#$$%&%'()*!""#$$%&%'(+* ,#'""'./$01(2'/$#34""'$ *!""#$$%&%'() *!""#$$%&%'(+ * 56""#'""'./7#(8'7/$#34""'$ * !""#$$/9 *!""#$$/9 *!""#$$%&%'(/7* :$'1.0;""#'""'./7#(8'7/$#34""'$ * Figure 1. Outline of our model. We assign pseudolabels to unla beled target samples based on the predictions from two class iÔ¨Åers trained on source samples. well on the target domain. Since directly learning discrimi  native representations for the target domain, in the absenc e of target labels, is considered very difÔ¨Åcult, we propose to assign pseudolabels to target samples and train target speciÔ¨Åc networks as if they were true labels. Cotraining and tritraining ( Zhou & Li ,2005 ) leverage multiple classiÔ¨Åers to artiÔ¨Åcially label unlabeled sample s and retrain the classiÔ¨Åers. However, the methods do not assume labeling samples from different domains. Since our goal is to classify unlabeled target samples that have different characteristics from labeled source samples, we propose asymmetric tritraining for unsupervised domain adaptation. By asymmetric , we mean that we assign differ ent roles to three classiÔ¨Åers. In this paper, we propose a novel tritraining method for unsupervised domain adaptation, where we assign pseudo labels to unlabeled samples and train neural networks uti lizing the samples. As described in Fig. 1, two networks are used to label unlabeled target samples and the remain ing network is trained by the pseudolabeled target sam ples. Our method does not need any special implementa tions. We evaluate our method on the digit classiÔ¨Åcation task, trafÔ¨Åc sign classiÔ¨Åcation task and sentiment analysi s task using the Amazon Review dataset, and demonstrate stateoftheart performance in nearly all experiments. I n particular, in the adaptation scenario, MNIST ‚ÜíSVHN, our method outperformed other methods by more than 10%. 2. Related Work "
299,Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings.txt,"Fine-grained entity type classification (FETC) is the task of classifying an
entity mention to a broad set of types. Distant supervision paradigm is
extensively used to generate training data for this task. However, generated
training data assigns same set of labels to every mention of an entity without
considering its local context. Existing FETC systems have two major drawbacks:
assuming training data to be noise free and use of hand crafted features. Our
work overcomes both drawbacks. We propose a neural network model that jointly
learns entity mentions and their context representation to eliminate use of
hand crafted features. Our model treats training data as noisy and uses
non-parametric variant of hinge loss function. Experiments show that the
proposed model outperforms previous state-of-the-art methods on two publicly
available datasets, namely FIGER (GOLD) and BBN with an average relative
improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one
dataset can be transferred to other datasets while using same model or other
FETC systems. These approaches of transferring knowledge further improve the
performance of respective models.","Entity type classiÔ¨Åcation is the task for assigning types or labels such as organization ,location to entity mentions in a document. This classiÔ¨Åca tion is useful for many natural language process ing (NLP) tasks such as relation extraction (Mintz et al., 2009), machine translation (Koehn et al.,2007), question answering (Lin et al., 2012) and knowledge base construction (Dong et al., 2014). There has been considerable amount of work on Named Entity Recognition (NER) (Collins and Singer, 1999; Tjong Kim Sang and De Meul der, 2003; Ratinov and Roth, 2009; Manning et al., 2014), which classiÔ¨Åes entity mentions into a small set of mutually exclusive types, such as Person ,Location ,Organization andMisc . How ever, these types are not enough for some NLP applications such as relation extraction, knowl edge base construction (KBC) and question an swering. In relation extraction and KBC, know ing Ô¨Ånegrained types for entities can signiÔ¨Åcantly increase the performance of the relation extrac tor (Ling and Weld, 2012; Koch et al., 2014; Mitchell et al., 2015) since this helps in Ô¨Åltering out candidate relation types that do not follow the type constrain. Finegrained entity types provide additional information while matching questions to its potential answers and signiÔ¨Åcantly improves performance (Dong et al., 2015). For example, Li and Roth (2002) rank questions based on their ex pected answer types (will the answer be food,ve hicle ordisease ). Typically, FETC systems use over hundred la bels, arranged in a hierarchical structure. An im portant aspect of FETC is that based on local con text, two different mentions of same entity can have different labels. We illustrate this through an example in Figure 1. All three sentences S1,S2, andS3mention same entity Barack Obama . How ever, looking at the context, we can infer that S1 mentions Obama as a person/author ,S2mentions Obama only as a person , and S3mentions Obama as aperson/politician . Available training data for FETC has noisy la bels. Creating manually annotated training data for FETC is time consuming, expensive, and er ror prone. Note that, a human annotator willarXiv:1702.06709v1  [cs.CL]  22 Feb 2017Figure 1: Noise introduced via distant supervision process. S1S3 indicates sentences where only a subset of labels for entity mention (bold typeface) are relevant given context, highlighted in T1T3. have to assign a subset of correct labels from a set of around hundred labels for each entity men tion in the corpus. Existing FETC systems use distant supervision paradigm (Craven and Kum lien, 1999) to automatically generate training data. Distant supervision maps each entity in the cor pus to knowledge bases such as Freebase (Bol lacker et al., 2008), DBpedia (Auer et al., 2007), YAGO (Suchanek et al., 2007). This method as signs same set of labels to all mentions of an entity across the corpus. For example, Barack Obama is a person, politician, lawyer, and author. If a knowledge base has these four matching la bels for Barack Obama, then distant supervision assigns all of them to every mention of Barack Obama. Training data generated with distant su pervision will fail to distinguish between mentions of Barack Obama in sentences S1,S2, and S3. Existing FETC systems have one or both of fol lowing drawbacks: 1. Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016) 2. Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016) We have observed that for real world datasets, more than twenty Ô¨Åve percent of training data has noisy labels. First drawback propagates this noise in training data to the FETC model. To extract hand crafted features various NLP tools are used. Since errors inevitably exist in such tools, the sec ond drawback propagates errors of these tools to FETC model. We propose a neural network based model to overcome the two drawbacks of existing FETC systems. First, we separate training data into clean andnoisy partitions using the same method as in AFET system (Ren et al., 2016). For these partitions, we use simple yet effective nonparametric variant of hinge loss function while training. To avoid use of hand crafted features, we learn repre sentations for given entity mention and its context. Additionally, we investigate effectiveness of us ing transfer learning (Pratt, 1993) for FETC task both at feature and model level. We show that feature level transfer learning can be used to im prove performance of other FETC system such as AFET, by up to 4.5% in microF1 score. Simi larly, model level transfer learning can be used to improve performance of the same model using dif ferent dataset by up to 3.8% in microF1 score. Our contributions can be summarized as fol lows: 1. We propose a simple neural network model that learns representations for entity mention and its context, and incorporate noisy label information using a variant of nonparametric hinge loss function. Experimental results on two publicly available datasets demonstrate the effectiveness of proposed model, with an average relative improvement of 2.69% in microF1 score. 2. We investigate the use of feature level and model level transferlearning strategies in the domain of the FETC task. The proposed transfer learning strategies further improve the stateoftheart on BBN dataset by 3.8% in microF1 score. 2 Related Work "
68,Learning from Noisy Labels with Coarse-to-Fine Sample Credibility Modeling.txt,"Training deep neural network (DNN) with noisy labels is practically
challenging since inaccurate labels severely degrade the generalization ability
of DNN. Previous efforts tend to handle part or full data in a unified
denoising flow via identifying noisy data with a coarse small-loss criterion to
mitigate the interference from noisy labels, ignoring the fact that the
difficulties of noisy samples are different, thus a rigid and unified data
selection pipeline cannot tackle this problem well. In this paper, we first
propose a coarse-to-fine robust learning method called CREMA, to handle noisy
data in a divide-and-conquer manner. In coarse-level, clean and noisy sets are
firstly separated in terms of credibility in a statistical sense. Since it is
practically impossible to categorize all noisy samples correctly, we further
process them in a fine-grained manner via modeling the credibility of each
sample. Specifically, for the clean set, we deliberately design a memory-based
modulation scheme to dynamically adjust the contribution of each sample in
terms of its historical credibility sequence during training, thus alleviating
the effect from noisy samples incorrectly grouped into the clean set.
Meanwhile, for samples categorized into the noisy set, a selective label update
strategy is proposed to correct noisy labels while mitigating the problem of
correction error. Extensive experiments are conducted on benchmarks of
different modalities, including image classification (CIFAR, Clothing1M etc)
and text recognition (IMDB), with either synthetic or natural semantic noises,
demonstrating the superiority and generality of CREMA.","Deep learning has achieved significant progress in the recognition of multimedia signals (e.g. images, text, speeches). The key to its success is the availability of largescale datasets with reliable manual annotations. Collecting such datasets, however, is timeconsuming and expensive. Some alternative ways to obtain la beled data, such as web crawling [46], inevitably yield samples with noisy labels, which are not appropriate to be directly utilized to train DNN since these com plex models can easily overfitting (i.e., memorizing) noisy labels [2,50]. To handle this problem, classical Learning with Noisy Label (LNL) ap proaches focus on either identifying and dropping noisy samples (i.e., sample selection) [10,14,49,45] or adjusting the objective term of each sample during training (i.e., loss adjustment) [29,48,37]. The former usually make use of small loss trick to select clean samples, and then take them to update DNNs. However, the procedure of sample selection cannot guarantee that the selected clean sam ples are completely clean. In contrast, as indicated in Fig. 1, division relied on statistic metrics can still involve some hard noisy samples in the training set, which will be treated equally as other normal samples in the following training stages. Thus the negative impact brought by wrongly grouped noisy samples can still confuse the optimization process and lower the test performance of DNNs [49]. On the other hand, the latter schemes reweight loss values or update labels by estimating the confidence on how clean a sample is. Typical meth ods include loss correction via an estimated noise transition matrix [29,8,12]. However, estimating an accurate noise transition matrix is practically challeng ing. Recently, there are approaches directly correcting the labels of all training samples [37,48]. However, we empirically find that unconstrained label correc tion in full data can do harm to clean samples and reversely hinder the model performance.CREMA: Coarsetofine learning with noisy labels 3 Towards the problems above, we propose a simple but effective method called CREMA (Coarsetofine sample cREdibility Modeling and Adaptive loss reweighting ), which adaptively reduces the impact of noisy labels via modeling the credibility (i.e., quality) of each sample. In the coarselevel, with the esti mated sample credibility by simple statistic metrics, clean and noisy samples can be roughly separated and handled in a divideandconquer manner. Since it is practically impossible to separate these samples perfectly, for the selected clean samples, we take their historical credibility sequences to adjust the con tribution of each sample to their objective, thus mitigating the negative impact of hard noisy samples (i.e, noisy samples incorrectly grouped into the clean set) in a finegrained manner. As for the separated noisy samples, some of them are actually clean (i.e., hard clean samples) and can be helpful for model training. Thus instead of discarding them as previous sample selection methods [10,45], we make use of them via a selective label correction scheme. The insight behind CREMA is from the observation on the loss value during training on noisy data (illustrated in Fig 1), it can be found that clean and noisy samples manifest distinctive statistical properties during train ing, where clean samples yield relatively smaller loss value [33]and more consistent prediction . Hence these statistical features can be utilized to coarsely model the sample credibility. However, Fig 1 also shows that the full data can not be perfectly separated by simple statistical metrics. This inspires us to adaptively cope with noises of different difficulty levels with more finegrained design. For easily recognized noisy samples, we can directly apply certain label correction schemes while avoiding erroneous correction on normal samples. For samples that fall into the confusing area and hybrid with clean ones, since the coarsely estimated credibility in the current epoch is not informative enough to identify noisy samples, CREMA applies a finegrained likelihood estimator of noisy samples by resorting the historical sequence of sample credibility. This is achieved by maintaining a historical memory bank along with the training process and estimating the likelihood function through a consistency metric and assumption of markov property of the sequence. CREMA is built upon a classic cotraining framework [45,10]. The fine grained sample credibility estimated by one network is used to adjust the loss term of credible samples for the other network. Extensive experiments are con ducted on benchmarks of different modality, including image classification (CI FAR, MNIST, Clothing1M etc) and text recognition (IMDB), with either syn thetic or natural semantic noises, demonstrating the superiority and generality of the proposed method. In a nutshell, the key contributions of this paper include: ‚Ä¢CREMA : a novel LNL algorithm that combats noisy labels via coarsetofine sample credibility modeling. In coarselevel, clean and noisy sets are roughly sep arated and handled respectively, in the spirit of the idea of divideandconquer. Easily recognized noisy samples are handled via a selective label update strategy; ‚Ä¢InCREMA , likelihood estimation of historical credibility sequence is pro posed to help identify hard noisy samples, which naturally plays as the dynamical weight to modulate loss term of each training sample in a finegrained manner;4 Boshen Zhang et al. Model1 Model2Training  DataCredibility Guided  Loss Modulation Selective Label  Distribution LearningSample Credibility  Modeling Module1 Sample Credibility  Modeling Module2ùëøùëøùíÑùíÑùüèùüèùëøùëøùíñùíñùüèùüè ùëøùëøùíÑùíÑùüêùüêùëøùëøùíñùíñùüêùüê ‚Ä¢ùëøùëøùíÑùíÑ:Samples withùíÑùíÑredible label . ‚Ä¢ùëøùëøùíñùíñ:Samples requiring labelùêÆùêÆpdating .ùëæùëæùüèùüè ùëæùëæùüêùüê ‚Ä¢ùëæùëæ:Sample credibility ùíòùíòeight . Fig. 2: The pipeline of CREMA .CREMA trains two parallel networks simultaneously. Clean samples (mostly clean) Xcand noisy samples (mostly noisy) Xuare separated via estimating the credibility of each training data. A selective label distribution learning scheme is applied for easily distinguishable noisy samples in Xu. As for the clean set Xc, likelihood estimation of historical credibility sequence is proposed to handle the hard noisy samples via adaptively modulating their loss term during training. ‚Ä¢CREMA is evaluated on six synthetic and realworld noisy datasets with different modality, noise type and strength. Extensive ablation studies and qual itative analysis are provided to verify the effectiveness of each component. 2 Related Works "
281,Analysis of an adaptive lead weighted ResNet for multiclass classification of 12-lead ECGs.txt,"Background: Twelve lead ECGs are a core diagnostic tool for cardiovascular
diseases. Here, we describe and analyse an ensemble deep neural network
architecture to classify 24 cardiac abnormalities from 12-lead ECGs.
  Method: We proposed a squeeze and excite ResNet to automatically learn deep
features from 12-lead ECGs, in order to identify 24 cardiac conditions. The
deep features were augmented with age and gender features in the final fully
connected layers. Output thresholds for each class were set using a constrained
grid search. To determine why the model made incorrect predictions, two expert
clinicians independently interpreted a random set of 100 misclassified ECGs
concerning Left Axis Deviation.
  Results: Using the bespoke weighted accuracy metric, we achieved a 5-fold
cross validation score of 0.684, and sensitivity and specificity of 0.758 and
0.969, respectively. We scored 0.520 on the full test data, and ranked 2nd out
of 41 in the official challenge rankings. On a random set of misclassified
ECGs, agreement between two clinicians and training labels was poor (clinician
1: kappa = -0.057, clinician 2: kappa = -0.159). In contrast, agreement between
the clinicians was very high (kappa = 0.92).
  Discussion: The proposed prediction model performed well on the validation
and hidden test data in comparison to models trained on the same data. We also
discovered considerable inconsistency in training labels, which is likely to
hinder development of more accurate models.","The 12lead electrocardiogram (ECG) provides critical information that assists in identifying cardiac abnormalities. The signal from each of the 12 leads corresponds to the heart's electrical activity from a distinct angle that can be mapped to the anatomy of thearXiv:2112.01496v1  [eess.SP]  1 Dec 2021Analysis of an adaptive lead weighted ResNet for multiclass classication of 12lead ECGs 2 heart. A skilled interpreter can therefore use ECG signals from multiple leads to localise the source of a cardiac abnormality. Expert cardiologists can identify abnormalities with high accuracy. A recent systematic review highlighted how the accuracy of human expert interpretation may be as high as 95% in a controlled setting in which the nal diagnosis was known [1]. However, expertlevel human ECG interpretation is limited by the availability of a trained cardiologist and the time required to synthesize information from the 12lead signal (and to document their ndings). In clinical practice, the absence of cardiologists means that other, nonspecialist, clinicians commonly make preliminary interpretations, but are demonstrably less accurate [2]. Computeraided methods for ECG interpretation have been suggested as one approach for circumventing these resource constraints. Historically, the accuracy of these methods has been poorer than humans [3]. For instance, Anh et. al's 2006 reported how 19% of atrial brillation were considered to be false positives when reviewed by a cardiologist [4]. Traditional machine learning approaches, in which salient features of the ECG signal are rst identied, have been successful for some use cases. As far back as 1991, the performance of some methods were almost as accurate as cardiologists, for a limited subset of clinical conditions [5]. However, such methods have frequently struggled to correctly interpret ECG with arrhythmias, conduction disorders and pacemaker rhythms [6]. Modern deep learning methods may be able to improve interpretation accuracy. Until recently, the use of such techniques for 12lead ECGs has been impractical due to the shortage of labelled training data. There remains room for improvement over initial promising results [7]. The public release of a new large labelled data set presents a fresh opportunity to revisit this challenging problem [8]. Here, we consider the task of cardiac abnormality classication from 12lead electrocardiogram (ECG) recordings of varying sampling frequency and duration. We have tackled this problem by developing a deep neural network architecture [9]. Our architecture acknowledges the importance of the spatial relationship between the ECG channels by using a squeezeandexcitation (SE) block. In this extended analysis, we present a deeper investigation into the strengths and weaknesses of this approach, including the use of expert clinical knowledge to determine why examples may be misclassied. 2. Methods "
134,Abstractive Text Classification Using Sequence-to-convolution Neural Networks.txt,"We propose a new deep neural network model and its training scheme for text
classification. Our model Sequence-to-convolution Neural Networks(Seq2CNN)
consists of two blocks: Sequential Block that summarizes input texts and
Convolution Block that receives summary of input and classifies it to a label.
Seq2CNN is trained end-to-end to classify various-length texts without
preprocessing inputs into fixed length. We also present Gradual Weight
Shift(GWS) method that stabilizes training. GWS is applied to our model's loss
function. We compared our model with word-based TextCNN trained with different
data preprocessing methods. We obtained significant improvement in
classification accuracy over word-based TextCNN without any ensemble or data
augmentation.","Ever since humans began to record information in the form of text, it was necessary to classify and manage information in a certain category to store and retrieve information efÔ¨Åciently. This need encouraged many researchers to develop a good text classiÔ¨Åcation technique that can assign predeÔ¨Åned categories to various kinds of text document such as emails, news articles, reviews, or patents. In commercial world, text classiÔ¨Åcation techniques such as Na√Øve Bayes classiÔ¨Åer[ 5], TFIDF[ 37], Support Vector Machines(SVM)[ 15] are already used in various Ô¨Åelds including spam Ô¨Åltering, news categorization, and sentiment analysis. Recent development in deep neural networks[ 17,39,19,38,7] are also achieving excellent results in extracting information from a text and classifying it into certain classes. As Convolutional Neural Networks(CNNs) achieved remarkable results in computer vision[ 32,35, 11,28], researchers also applied CNNs to text classiÔ¨Åcation[ 17,19,38,7] and showed excellent results. Training CNNs on top of pretrained word vectors[ 22,16,27] or characterlevel features[ 38,7] with hyperparameter tuning, they could get similar or outperforming results compared to other text classiÔ¨Åcation models. Although TextCNNs‚Äô performance in text classiÔ¨Åcation is remarkable, they can only be applied to data whose input has Ô¨Åxed size. Since the number of parameters in TextCNN is determined by the length of input text, researchers had to crop or pad input texts into a certain length to train their TextCNN. This can result information loss when classifying longer texts and cause performance degradation. In section 4.2, we show that performance of TextCNNs can be improved by training the model with summaries of input text. There are two ways to generate the summary of a text. One is extractive Preprint. Work in progress.arXiv:1805.07745v6  [cs.CL]  2 Jun 2020summarization, mere selection of a few existing sentences extracted from the source. The other is abstractive summarization, compressed paraphrasing of main contents of source, potentially using vocabulary unseen in the source. Both methods can change texts of various lengths into texts of Ô¨Åxed length still maintaining important features of source texts. TextRank[ 21] is a graphbased ranking model for extractive text summarization. TextRank gives a ranking over all sentences in a text allowing it to extract very short summaries without any training corpora. TextRank is widely used in summarizing structured text like news articles. Many researchers worked with Sequencetosequence Recurrent Neural Networks (Seq2seq RNNs)[ 34,24] to model abstractive text summarization. Using attention mechanism[ 6] that al lows neural networks to focus on different parts of their input, Seq2seq RNNs have been showing signiÔ¨Åcant results in the task of abstractive summarization[29, 24]. In this paper, we introduce Sequencetoconvolution Neural Networks(Seq2CNN) model that consists of two blocks: Sequence Block and Convolution Block. Sequence Block based on Attentional EncoderDecoder Recurrent Neural Networks[ 24] summarizes input texts and feeds them into Convo lution Block. Convolution Block based on TextCNN[ 17] classiÔ¨Åes input texts into certain classes using the summaries provided by Sequential Block. Both blocks share nonstatic word embedding layer, encouraging them to collaborate for performance improvement. Simply connecting two blocks and train them with single endtoend procedure cannot guarantee optimal results because Sequential Block doesn‚Äôt generate proper summaries in early stages of training. To solve this problem, we also propose a new training scheme that gradually shifts from Ô¨Åne tuning for summarization task to Ô¨Ånetuning for classiÔ¨Åcation task as training progresses. Our model is implemented with TensorÔ¨Çow[1]. Code is available at https://github.com/tgisaturday/Seq2CNN. 2 Related Work "
401,Positive-Unlabeled Learning with Uncertainty-aware Pseudo-label Selection.txt,"Positive-unlabeled (PU) learning aims at learning a binary classifier from
only positive and unlabeled training data. Recent approaches addressed this
problem via cost-sensitive learning by developing unbiased loss functions, and
their performance was later improved by iterative pseudo-labeling solutions.
However, such two-step procedures are vulnerable to incorrectly estimated
pseudo-labels, as errors are propagated in later iterations when a new model is
trained on erroneous predictions. To prevent such confirmation bias, we propose
PUUPL, a novel loss-agnostic training procedure for PU learning that
incorporates epistemic uncertainty in pseudo-label selection. By using an
ensemble of neural networks and assigning pseudo-labels based on
low-uncertainty predictions, we show that PUUPL improves the reliability of
pseudo-labels, increasing the predictive performance of our method and leading
to new state-of-the-art results in self-training for PU learning. With
extensive experiments, we show the effectiveness of our method over different
datasets, modalities, and learning tasks, as well as improved calibration,
robustness over prior misspecifications, biased positive data, and imbalanced
datasets.","Many realworld applications involve positive unlabeled (PU) datasets [ 1,2,3] in which only small parts of the data is labeled positive while the major ity is unlabeled. Learning from PU data can reduce development costs in many deep learning applicationsthat otherwise require annotations from experts such as medical image diagnosis [ 1], protein function predic tion [ 2], and it can even enable applications in settings where the measurement technology itself can not detect negative examples [3]. Approaches such as unbiased PU [ 4] and nonnegative PU [5] formulate this problem as costsensitive learn ing and sparked a stream of works on risk estima tors for PU learning (PUL). Others approach PUL as a twostep procedure, rst identifying and labeling some reliable negative examples, and then retraining the model based on this newly constructed labeled dataset [ 6]. These approaches show similarities with pseudolabeling in semisupervised classication set tings [ 7] and are generally able to reach higher perfor mance compared to other methods that use a single training iteration. Such pseudolabeling techniques are however especially vulnerable to incorrectly as signed labels of the selected examples as such errors will propagate and magnify in the retrained model, resulting in a negative feedback loop. This erroneous selection of unreliable pseudolabels occurs when wrong model predictions are associated with excessive model condence, which is accompanied by a distortion of the signal for the pseudolabel selection [ 8]. In recent literature on pseudolabeling, this problem, also re ferred to as conrmation bias [9], is recognized and successfully addressed by explicitly estimating the pre diction uncertainty [ 10,11]. While this is the case for semisupervised classication, recent selftraining ap proaches for PUL do not explore the use of uncertainty quantication for pseudolabeling in this context. Contributions: Motivated by this, we propose a 1arXiv:2201.13192v2  [stat.ML]  31 Aug 2022Train anensemblePredict with uncertainty Select with uncertainty PositiveunlabeledPseudolabeled FPRTPR(e) Ranking quality Network AUC: 0.55 Ensemble AUC: 0.81Figure 1: PUUPL is a pseudolabeling framework for PU learning that uses the epistemic uncertainty of an ensemble to select condent examples to pseudolabel. The ensemble can be trained with any PU loss for PU data while minimizing the crossentropy loss on the previously assigned pseudolabels. In a toy example, a single network is not very condent on most of the unlabeled data (a), resulting in many highcondence incorrect predictions and many lowcondence correct ones (c). The epistemic uncertainty of an ensemble is, on the other hand, very low on most of the unlabeled data (b), resulting in most correct predictions having low uncertainty and most incorrect predictions having high uncertainty (d). Thus, the estimated uncertainty by ensemble can be used more reliably to rank predictions and select correct ones (e). Retraining the model with an increased number of labeled samples will result in a slightly more accurate model, than can be used to predict new pseudolabels, which will further improve the model's performance, etc. novel framework for PUL that leverages uncertainty quantication to identify reliable examples to pseudo label (Fig. 1). In particular, 1.We introduce PUUPL (Positive Unlabeled, Uncer tainty aware PseudoLabeling), a novel framework that successfully overcomes the issue of conrma tion bias in PUL. 2.We evaluate our methods on a wide range of bench marks and PU datasets, achieving stateoftheart results in selftraining for PUL both with and without known positive class prior =p(y= 1). 3.Through extensive experiments we show that PU UPL results in very wellcalibrated predictions, is applicable to dierent data modalities such as im ages and text, can use any risk estimator for PUL and improve thereupon, and is robust to prior misspecication and class imbalance. These results show our framework to be highly reliable, extensible, and applicable in a wide range of real world scenarios.2 Related work "
507,DocLangID: Improving Few-Shot Training to Identify the Language of Historical Documents.txt,"Language identification describes the task of recognizing the language of
written text in documents. This information is crucial because it can be used
to support the analysis of a document's vocabulary and context. Supervised
learning methods in recent years have advanced the task of language
identification. However, these methods usually require large labeled datasets,
which often need to be included for various domains of images, such as
documents or scene images. In this work, we propose DocLangID, a transfer
learning approach to identify the language of unlabeled historical documents.
We achieve this by first leveraging labeled data from a different but related
domain of historical documents. Secondly, we implement a distance-based
few-shot learning approach to adapt a convolutional neural network to new
languages of the unlabeled dataset. By introducing small amounts of manually
labeled examples from the set of unlabeled images, our feature extractor
develops a better adaptability towards new and different data distributions of
historical documents. We show that such a model can be effectively fine-tuned
for the unlabeled set of images by only reusing the same few-shot examples. We
showcase our work across 10 languages that mostly use the Latin script. Our
experiments on historical documents demonstrate that our combined approach
improves the language identification performance, achieving 74% recognition
accuracy on the four unseen languages of the unlabeled dataset.","Language identification is a subfield of image classification and aims to recognize the language of printed and written text appearing in images. One exciting and less researched type of image is scans of historical documents. For instance, a significant and diverse collec tion of textbased historical documents compiled by several major European libraries can be accessed in the IMPACT [ 13] dataset. The creation of such a dataset, together with accompanying ground truth data for a smaller subset, is often connected to the goal of improving the analysis and digital processing of historical docu ments. Although many historical documents primarily contain text, such as text from books, articles, or newsletters, there are also doc uments whose main content is represented through large pictures. One specific example is arthistorical documents. These documents are written records that provide information about works of art and the artists who created them. They include manuscripts, letters, diaries, and other written materials that provide insights into a particular artwork‚Äôs creation, history, and significance. Being one of the broader discussed Computer Vision tasks, Optical Character Recognition ( OCR ) methods aim to convert the text in im ages into a digital format. The language of a document is a valuable and contextual information for the process of character recognition. For example, if the language of the document is known in prior, a languagespecific OCR engine can be selected. This can lead to reducing the amount of ambiguity in character recognition, likely improving recognition quality and speed. In previous years, deep learning methods have achieved signif icant performance improvements on various document analysis problems, such as image classification and OCR [9,11,21]. How ever, there are multiple aspects of document input images that still weaken the recognition performance of the above methods. A poor image resolution, small font sizes, and noise introduced by the input image itself or due to the scanning process are common problems [ 9,11,21] and, at the same time, common characteristics of arthistorical documents. Several supervised learning methodswere proposed to tackle these problems and improve recognition performance [ 11,21]. These methods usually incorporate large amounts of labeled data into the training of a neural network. How ever, most of today‚Äôs data is generated from diverse, unstructured sources, resulting in data without additional ground truth infor mation. Therefore, accurately assigning meaningful labels often requires contextual understanding and human expertise, which is usually timeconsuming and infeasible for large quantities of data. Motivated by this problem, in this paper, we propose DocLangID , a twostage training approach to tackle the problem of domain adaptation for language identification. Given two domains of im ages, one labeled and the other unlabeled, the task is to adapt a model trained on the labeled domain toward identifying the lan guages of the unlabeled domain. In this setting, the languages of both domains do not have to overlap. DocLangID is a simple yet effective fewshot learning method that employs a distancebased classifier, as shown by Chen et al. [2], to improve language identification in the unlabeled domain. We achieve this improvement by using only small amounts of manually labeled examples. Our contributions can be summarized as follows: (1)We achieve high language identification performance for art historical documents in a supervised setting, which was not accomplished previously for languages of the Latin script. (2)We develop a simple but effective method for domain adap tation, which utilizes large amounts of labeled data and few shot examples to transfer the model‚Äôs knowledge to a new domain of historical documents. (3)We analyze patch extraction, a commonly used procedure when training on image data, and the assessment of its im pact on our approach. (4)We implement and evaluate supervised variations of our main DocLangID approach, and we publish our work on Github1. 2 RELATED WORK "
25,Field Extraction from Forms with Unlabeled Data.txt,"We propose a novel framework to conduct field extraction from forms with
unlabeled data. To bootstrap the training process, we develop a rule-based
method for mining noisy pseudo-labels from unlabeled forms. Using the
supervisory signal from the pseudo-labels, we extract a discriminative token
representation from a transformer-based model by modeling the interaction
between text in the form. To prevent the model from overfitting to label noise,
we introduce a refinement module based on a progressive pseudo-label ensemble.
Experimental results demonstrate the effectiveness of our framework.","Formlike documents, such as invoices, paystubs and patient referral forms, are very common in daily business workÔ¨Çows. A large amount of hu man effort is required to extract information from forms every day. In form processing, a worker is usually given a list of expected form Ô¨Åelds (e.g., purchase_order ,invoice_number andtotal_amount in Figure 1), and the goal is to extract their corre sponding values based on the understanding of the form, where keys are generally the most important features for value localization. A Ô¨Åeld extraction system aims to automatically extract Ô¨Åeld values from redundant information in forms, which is cru cial for improving processing efÔ¨Åciency and reduc ing human labor. Field extraction from forms is a challenging task. Document layouts and text representations can be very different even for the same form type, if they are from different vendors. For example, invoices from different companies may have signiÔ¨Åcantly different designs (see Figure 3). Paystubs from different systems (e.g., ADP and Workday) have different representations for similar information. Recent methods formulate this problem as Ô¨Åeld value pairing or Ô¨Åeld tagging. Majumder et al. INVOICE #: 1234PO Number:Company LOGOField (purchase_order): [PO Number, PO #]Total:   100.00value00000001localized keyFigure 1: Field extraction from forms is to ex tract the value for each Ô¨Åeld, e.g., invoice_number , purchase_order andtotal_amount , in a given list. A key, e.g., INVOICE #, PO Number and Total, refers to a concrete text representation of a Ô¨Åeld in a form and it is an important indicator for value localization. (2020) propose a representation learning method that takes Ô¨Åeld and value candidates as inputs and utilizes metric learning techniques to enforce high pairing score for positive Ô¨Åeldvalue pairs and low score for negative ones. LayoutLM (Xu et al., 2020) is a pretrained transformer that takes both text and their locations as inputs. It can be used as a Ô¨Åeldtagger which predicts Ô¨Åeld tags for in put texts. These methods show promising results, but they require large amount of Ô¨Åeldlevel annota tions for training. Acquiring Ô¨Åeldlevel annotations of forms is challenging and sometimes even im possible since (1) forms usually contain sensitive information, so there is limited public data avail able; (2) working with external annotators is also infeasible, due to the risk of exposing private in formation and (3) annotating Ô¨Åeldlevel labels is timeconsuming and hard to scale. Motivated by these reasons, we propose a Ô¨Åeld extraction system that does not require Ô¨Åeldlevel annotations for training (see Figure 2). First, we bootstrap the training process by mining pseudo labels from unlabeled forms using simple rules. Then, a transformerbased architecture is used toarXiv:2110.04282v2  [cs.CV]  11 Apr 2022model interactions between text tokens in the form and predict a Ô¨Åeld tag for each token accordingly. The pseudolabels are used to supervise the trans former training. Since the pseudolabels are noisy, we propose a reÔ¨Ånement module to improve the learning process. SpeciÔ¨Åcally, the reÔ¨Ånement mod ule contains a sequence of branches, each of which conducts Ô¨Åeld tagging and generates reÔ¨Åned labels. At each stage, a branch is optimized by the labels ensembled from all previous branches to reduce label noise. Our method shows strong performance on real invoice datasets. Each designed module is validated via comprehensive ablation experiments. Our contribution is summarized as follows: (1) to the best of our knowledge, this is the Ô¨Årst work that addresses the problem of Ô¨Åeld extraction from forms without using Ô¨Åeldlevel labels; (2) we pro pose a novel training framework where simple rules are Ô¨Årst used to bootstrap the training process and a transformerbased model is used to improve per formance; (3) our proposed reÔ¨Ånement module is demonstrated as effective to improve model per formance when trained with noisy labels and (4) to facilitate future research, we introduce the INV CDIP dataset as a public benchmark. The dataset is available at https://github.com/salesforce/invcdip. 2 Related Work "
485,FIRED: a fine-grained robust performance diagnosis framework for cloud applications.txt,"To run a cloud application with the required service quality, operators have
to continuously monitor the cloud application's run-time status, detect
potential performance anomalies, and diagnose the root causes of anomalies.
However, existing models of performance anomaly detection often suffer from low
re-usability and robustness due to the diversity of system-level metrics being
monitored and the lack of high-quality labeled monitoring data for anomalies.
Moreover, the current coarse-grained analysis models make it difficult to
locate system-level root causes of the application performance anomalies for
effective adaptation decisions. We provide a FIne-grained Robust pErformance
Diagnosis (FIRED) framework to tackle those challenges. The framework offers an
ensemble of several well-selected base models for anomaly detection using a
deep neural network, which adopts weakly-supervised learning considering fewer
labels exist in reality. The framework also employs a real-time fine-grained
analysis model to locate dependent system metrics of the anomaly. Our
experiments show that the framework can achieve the best detection accuracy and
algorithm robustness, and it can predict anomalies in four minutes with F1
score higher than 0.8. In addition, the framework can accurately localize the
first root causes, and with an average accuracy higher than 0.7 of locating
first four root causes.","CLOUD environments provide elastic and ondemand resources for developing applications [1]. However, because of the inherent dynamism of clouds, performance anomalies of cloud applications such as degraded response time caused by resource saturation may severely affect the quality of the user experience. In addition, considering complex dependencies and multiple components in cloud applications, it‚Äôs difÔ¨Åcult for operators to detect perfor mance anomalies and identify root causes. Traditionally, operators perform diagnoses for cloud applications man ually, which is complicated and timeconsuming. Data of different monitoring metrics, e.g., CPU and memory usage, can be continuously collected, reÔ¨Çecting the runtime status of cloud applications [2]. Therefore, we could consider a performance diagnosis solution that leverages monitoring data and supports rapid recovery and loss mitigation of cloud applications. For a real cloud application, monitoring data can be identiÔ¨Åed as application and systemlevel data. Application level data such as response time can be used to detect performance anomalies when slow response time is deÔ¨Åned as an anomaly. However, it‚Äôs hard to capture the status of underlying cloud environments and exploit root causes of performance anomalies with singlevariate application level data. Meanwhile, underlying resources affect the per formance of cloud applications heavily. Systemlevel data mainly includes underlying resources, such as CPU, mem The authors are with Multiscale Networked Systems (MNS), University of Amsterdam, the Netherlands. Corresponding authors: Peng Chen (chenpeng@mail.xhu.edu.cn) and Zhiming Zhao (z.zhao@uva.nl). Peng Chen is also with School of Computer and Software Engineering, Xihua University, Chengdu, China. Manuscript received ; .ory, disk, and network. A single resource metric may not precisely reÔ¨Çect the health status of the whole application. Therefore, it‚Äôs reasonable to detect performance anomalies in cloud applications based on all systemlevel monitoring information, which is multivariate time series data. In addi tion, when a performance anomaly occurs, we need to Ô¨Åne grained pinpoint root causes on systemlevel data, which is helpful for the rapid recovery of a cloud application. Performance diagnosis is a process of detecting abnor mal performance phenomena, e.g., degradation, predicting anomalies to forestall future incidents, and localizing the causes of performance anomalies [3]. In recent years, studies for performance diagnosis have been developed and mainly focus on performance anomaly detection and root cause localization. For performance anomaly detection, numerous existing methods [4] [5] target improving detection accuracy, but their performance is inconsistent in cloud environments. For example, scaling of cloud infrastructures will change the data distributions of monitoring data, severely affect ing detection performance. Therefore, robust performance anomaly detection is necessary for performance diagnosis to keep performance consistency. As for root cause local ization, approaches are still developing [6] [7] and most of them are coarsegrained, which focuses on servicelevel or containerlevel faulty [8] [9]. To Ô¨Åll these gaps, we are motivated to develop a performance diagnosis, which can detect performance anomalies with good robustness and identify the root causes with Ô¨Ånegrained. As for the performance diagnosis framework, we iden tify several challenging requirements for performance anomaly detection and root cause localization based on real scenarios. Anomalies need to be detected accurately. The dearXiv:2209.01970v1  [cs.DC]  5 Sep 2022JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 tection should also have good robustness to perform consistently for different patterns in monitoring data. Multistep prediction of future anomalies is neces sary to effectively prevent potential application vio lations. When an anomaly occurs, root causes need to be localized in realtime with Ô¨Ånegrained, which is more helpful for efÔ¨Åcient application maintenance. Furthermore, the development of the diagnosis framework has to handle two data challenges: Monitoring data usually contains only fewer labels or no labels that can be immediately used to train a machine learningbased model because labeling data is often manual and timeconsuming. Collected monitoring data of cloud applications usu ally contains noise, which can inÔ¨Çuence the perfor mance of anomaly detection and root cause localiza tion. To address the two data challenges, we adopt weakly supervised learning [10] and provide methods to Ô¨Ålter data noise. In addition, to satisfy the three requirements, we consider the performance diagnosis framework should have good detection performance, multistep anomaly prediction ability, and Ô¨Ånegrained root cause localization. As for the performance anomaly detection and prediction, we consider integrating existing detection methods based on ensemble learning [11] to improve the detection performance. As for root cause localization, causality inference and graph methods [12] can be used. In this paper, we provide a FIne grained Robust p Erformance Diagnosis (FIRED) frame work. Our contributions can be summarized as follows: We design an integrated framework to implement performance diagnosis effectively by putting metrics selection which can Ô¨Ålter useless monitoring metrics, wellperformed performance anomaly detection and prediction, and Ô¨Ånegrained root cause localization together. We improve performance anomaly detection accu racy and robustness signiÔ¨Åcantly by developing a novel deep ensemble method. The method can also predict anomalies in four minutes with F1 score higher than 0.8. We propose a realtime and Ô¨Ånegrained root cause localization pipeline by building dependency graph and executing random walk automatically. It can accurately localize the Ô¨Årst root cause, and with the average localization accuracy higher than 0.7 of locating Ô¨Årst four root causes. The rest of the paper is organized as follows. In section 2, we review existing research about performance diagnosis and introduce anomaly detection and root cause localization methods. In section 3, a performance diagnosis framework and its main components are introduced in detail. In sec tion 4, experiments and results for each component of our diagnosis framework are provided. Finally, we provide discussion and conclusion in section 5 and section 6.TABLE 1 Unsupervised performance anomaly detection methods Type Algorithm Description DensitybasedLOF [5] Local Outlier Factor LOCI [19] Local Correlation Integral DistanceBasedKNN [20] K Nearest Neighbors LDOF [21] Local Distancebased Outlier Factor Kernelbased OCSVM [22] OneClass Support Vector Machines EnsembleIForest [23] Isolation Forest Feature bagging [24] Subset of features are used Deep learningAutoEncoder [25] Fully connected AutoEncoder VAE [26] Variational AutoEncoder 2 R ELATED WORKS "
515,PADDLES: Phase-Amplitude Spectrum Disentangled Early Stopping for Learning with Noisy Labels.txt,"Convolutional Neural Networks (CNNs) have demonstrated superiority in
learning patterns, but are sensitive to label noises and may overfit noisy
labels during training. The early stopping strategy averts updating CNNs during
the early training phase and is widely employed in the presence of noisy
labels. Motivated by biological findings that the amplitude spectrum (AS) and
phase spectrum (PS) in the frequency domain play different roles in the
animal's vision system, we observe that PS, which captures more semantic
information, can increase the robustness of DNNs to label noise, more so than
AS can. We thus propose early stops at different times for AS and PS by
disentangling the features of some layer(s) into AS and PS using Discrete
Fourier Transform (DFT) during training. Our proposed Phase-AmplituDe
DisentangLed Early Stopping (PADDLES) method is shown to be effective on both
synthetic and real-world label-noise datasets. PADDLES outperforms other early
stopping methods and obtains state-of-the-art performance.","Learning from noisy labels (LNL) [1] is an active area of research within the deep learning community [12,14,32,40, 57,60,62]. Noisy labels are common in realworld applica tions [44,49,54,59], and trustworthy AI should be robust to mislabelling. It has been argued that CNNs learn Ô¨Årst the actual pattern before over Ô¨Åtting the noise [3], which inspired many works in LNL [14,24,25,27,28,51,56]. A training strategy is early stopping (ES), which stops the gradientbased optimization at a speciÔ¨Åc early training step. Due to its effectiveness, ES is widely applied in current LNL models and has achieved promising performance [4, 24, 27, 33, 46]. The frequency and spatial domains are alternative codes *CoÔ¨Årst authors. 1Codes will be available upon acceptance.for depicting signal data such as images and text [36, 45]. Different frequency components contain different informa tion [7]. The amplitude spectrum (AS) quantiÔ¨Åes how much of each sinusoidal component is present, while the phase spectrum (PS) reveals the location of each sinusoidal com ponent within an image. Biological justiÔ¨Åcation and psy chological patterns testing [13, 42] demonstrate that the re sponse of cells in the primary visual cortex (V1) is closely related to the local AS for speciÔ¨Åc image patterns (fre quency and orientation). That is, the AS component usu ally represents the intensity of the patterns in the image. On the other hand, previous qualitative and quantitative stud ies [7, 13] indicate that the PS is the key to locating salient object areas and holds visible structured information for vi sion recognition [11, 23, 35], thus contains more semantic information than the AS. As a robust vision system, human vision focuses on se mantic parts during object recognition, and relies more on the image components related to the PS than the AS [8, 13, 23,35]. This system builds a strong connection between se mantic feature space and label space, helping humans ‚Äòun derstand‚Äô the actual correlation between objects and their corresponding identiÔ¨Åers (labels). The human visual system is very robust to label noise. However, CNNs proÔ¨Åt from human unperceivable highfrequency information in im ages [18, 50]. Without adequate regulations, CNNs model the correlation of objects and their labels mainly based on the connection between AS and the given annotations. Such overdependence is demonstrated as the leading cause of their sensitivity to image perturbation and overconÔ¨Ådence in outofdistribution (OOD) detection [8]. We argue that CNNs‚Äô overdependence of connection between the less se mantic AS and labels may spoil their recognition robust ness, resulting in their vulnerability to label noise. To investigate the impact of label noise on deep models trained with different image components, we generate sym metric label noise [14, 48] with a 50% noise rate and feed it with raw images, PS and AS to a ResNet18 [15] model separately. As shown in Figures 1a and 1b, the convergence speed of CNNs on AS and PS differs. When CNNs startarXiv:2212.03462v1  [cs.CV]  7 Dec 202225 50 75 100 Epochs0.51.01.52.0Training LossTrain_IM Train_AS Train_PS(a) Cleanly labeled examples 25 50 75 100 Epochs1.01.52.02.5Training LossTrain_IM Train_AS Train_PS (b) Wrongly labeled examples 25 50 75 100 Epochs020406080T est AccuracyTrain_IM Train_AS Train_PS (c) Test accuracy with noisy labels Figure 1. Results of training a ResNet18 model on CIFAR10 using original images, amplitude spectrum, and phase spectrum (‚ÄúTrain IM‚Äù, ‚ÄúTrain AS‚Äù, and ‚ÄúTrain PS‚Äù in the Figure) on cleanly and noisily labeled subsets. The curves are averaged across Ô¨Åve random runs. The dotted vertical lines indicate the best performance steps of different image components. The converging speed of the deep model trained on AS and PS differs, especially on wrongly labeled examples. Approaching the end of the training, when the wrong labels begin to be memorized, the model accelerates Ô¨Åtting to AS, resulting in an intersection on the training curves of AS and PS, shown in Figure 1b. Hence, PS can help the deep model become more resistant to label noises than AS. to overÔ¨Åt the noisy labels, they Ô¨Åt AS much faster than PS (Figure 1b). Meanwhile, the convergence speed on PS is slower than AS and the raw images, which indicates that PS can help the CNNs become more robust towards mislabels than AS or raw inputs. Note that the model trained with only AS or PS performs worse than the one trained with the raw images (Figure 1c). This is not surprising as either AS or PS could miss some information from the original image data. Therefore, an intuitive solution to improve the robust ness of the CNNs to the noisy labels is choosing different early stop points for AS and PS, during the training of the CNNs. In this way, we can suppress the overdependence of CNNs on AS while shift to utilize more PS components. Current CNNs are trained based on gradients update via backward propagation. The raw images are Ô¨Åxed and do not need gradient computing during the optimization. There fore, it is hard to control the model optimization on raw AS and PS directly. To tackle this challenge, we propose to use deep features to represent the ‚Äòimage‚Äô, as each ‚Äòpixel‚Äô of the feature map corresponds to an original image patch. Moreover, a similar study to that shown in Figure 1 for the deep features of ResNet blocks supports our solution. We observe that different frequency components from the deep features hold a similar property to those from the raw image (Please refer to the supplemental materials for this study). SpeciÔ¨Åcally, we propose to disentangle the deep im age features into AS and PS at different training steps by Discrete Fourier Transform (DFT). We Ô¨Årst detach the AS component from the gradient computational graph to stop its involvement in the model update, which can alleviate the potential negative effects of AS in the later training stage. With AS being detached, we continue train the deep model with PS components. The optimization on the PS compo nents will be stopped after a few training epochs. Noticethat the detached components will regenerate the deep fea tures in the spatial domain through inverse DFT (iDFT). This is efÔ¨Åcient as there is no modiÔ¨Åcation to the original architecture. Moreover, complete information is used for training. We call the proposed method as PhaseAmplituDe DisentangLed Early Stopping (PADDLES). To the best of our knowledge, PADDLES is the Ô¨Årst method to consider features learned with noisy labels in the frequency domain and thus is orthogonal to existing methods that mainly focus on the spatial domain. Our contributions are as follows: ‚Ä¢ We study learning with noise labels from the frequency domain and Ô¨Ånd that PS can help CNNs become more resistant to label noise than AS. ‚Ä¢ We propose to early stop training at different stages for AS and PS. We demonstrate that our proposed method can beneÔ¨Åt from the robustness of the PS without los ing information on AS during the training of CNNs. ‚Ä¢ Extensive experiments on benchmark datasets such as CIFAR10/100, CIFAR10N/100N, and Clothing1M validate the effectiveness of the proposed method. 2. Related Work "
121,ReSup: Reliable Label Noise Suppression for Facial Expression Recognition.txt,"Because of the ambiguous and subjective property of the facial expression
recognition (FER) task, the label noise is widely existing in the FER dataset.
For this problem, in the training phase, current FER methods often directly
predict whether the label of the input image is noised or not, aiming to reduce
the contribution of the noised data in training. However, we argue that this
kind of method suffers from the low reliability of such noise data decision
operation. It makes that some mistakenly abounded clean data are not utilized
sufficiently and some mistakenly kept noised data disturbing the model learning
process. In this paper, we propose a more reliable noise-label suppression
method called ReSup (Reliable label noise Suppression for FER). First, instead
of directly predicting noised or not, ReSup makes the noise data decision by
modeling the distribution of noise and clean labels simultaneously according to
the disagreement between the prediction and the target. Specifically, to
achieve optimal distribution modeling, ReSup models the similarity distribution
of all samples. To further enhance the reliability of our noise decision
results, ReSup uses two networks to jointly achieve noise suppression.
Specifically, ReSup utilize the property that two networks are less likely to
make the same mistakes, making two networks swap decisions and tending to trust
decisions with high agreement. Extensive experiments on three popular
benchmarks show that the proposed method significantly outperforms
state-of-the-art noisy label FER methods by 3.01% on FERPlus becnmarks. Code:
https://github.com/purpleleaves007/FERDenoise","Facial expression recognition (FER) has become a cru cial service in various realworld applications, such as healthcare [22], surveillance [1], and virtual reality [24]. It aims to recognize specific human emotions from the given facial images. However, for the largescale FER dataset collected from the Internet, it is difficult to achieve high quality annotations due to the subjectivity of annotators and Figure 1. Comparison of label noise suppression process of differ ent noisy label FER methods. The top is the current schemes and the bottom is ReSuP. ReSuP generates more reliable weights by noise modeling and suppresses unreliable weights by unreliability suppression design in addition to suppressing noisy labels. Trian gles represent unreliable weights. the ambiguity of facial expressions, and these lowquality annotations form label noises. Therefore, how to suppress label noises in FER tasks has become a research hotspot and attracted more and more attention[35, 30, 16, 48, 45, 42]. For this challenge, existing FER methods typically in corporate an importance learning branch to estimate the im portance weight of each image, which determines whether the label of the input image is noisy or not [35, 30, 48]. However, we argue that these methods suffer from the low reliability of such noise decision operation. Such opera tion usually generates unreliable weights and makes that some mistakenly abounded clean data are not utilized suf ficiently and some mistakenly kept noised data disturbing the model learning process. These unreliable weights orig inate from two perspectives: the noise decision process and the FER model itself. The former unreliable weights are due to overfitting of the importance learning branch as a result of the strong learning ability of deep neural net works (DNNs) [48]. Furthermore, such noise decision pro cess only considers information from a single sample [35] a batch [30], neglecting global information and resulting in unreliable decision making. In addition to the unreli able weights caused by the noise decision process, the FER model itself unavoidably produces some unreliable outputsarXiv:2305.17895v1  [cs.CV]  29 May 2023(the inputs of the importance learning branch), leading to further unreliable weights. These unreliable weights accu mulate during the entire learning process and affect current and future learning stages. Unfortunately, existing methods do not address how to mitigate the effects of these unreli able weights. Novel methods are required to address these limitations and improve the reliability and accuracy of noisy label FER. In this paper, we present a novel approach called Re Sup. The main objective of ReSup is to suppress noisy labels and unreliable weights, as illustrated in Figure 1. Specifically, instead of directly predicting noised or not, ReSup makes the noise decision by modeling he joint dis tribution of noise and clean labels. This is motivated by the memorization effect of deep neural networks (DNNs), where the model tends to memorize correctly labeled sam ples first [47, 3, 14], leading to noisy samples having higher loss during early epochs of training [2, 46]. But, to achieve optimal distribution modeling, ReSup propose to model the similarity (cosine similarity of predictions and targets) distribution of all samples rather than the loss, which re duces the confusion between noisy and clean distributions. The fitted noise model is then used to provide importance weights for each sample based on its similarity, without us ing neural network branches to avoid overfitting. In addi tion, the proposed scheme can take into account the global distribution of all samples. Furthermore, ReSup mitigate the effect of the unreliable weights by leveraging the agree ment maximization principles [6, 32], which suggest that two different networks would agree on most samples except for noisy samples [38] and thus can filter different types of errors. Inspired by the agreement maximization principles, ReSup employs two different networks to provide impor tance weights to each other, to prevent the accumulation of errors caused by unreliable weights. We also introduce a consistency loss that assigns large losses to samples with small agreement to prevent the model from fitting samples with unreliable weights, since the samples with small agree ments usually are noisy samples. In summary, our contribu tions include: ‚Ä¢ To avoid extra unreliable weights caused by the DNNbased importance learning branch, a novel label noise modeling method based on similarity distribu tion statistics is proposed to estimate the importance weights. ‚Ä¢ We propose ReSup to suppress label noise in FER. Re Sup satisfactorily mitigates the effect of the unreliable weights by leveraging a weight exchange strategy and a consistency loss. ‚Ä¢ Extensive experiment results demonstrate that the pro posed ReSup significantly outperforms stateoftheartnoisy label FER solutions on multiple FER bench marks with different levels of label noise. 2. Related Work "
577,IRNet: Iterative Refinement Network for Noisy Partial Label Learning.txt,"Partial label learning (PLL) is a typical weakly supervised learning, where
each sample is associated with a set of candidate labels. The basic assumption
of PLL is that the ground-truth label must reside in the candidate set.
However, this assumption may not be satisfied due to the unprofessional
judgment of the annotators, thus limiting the practical application of PLL. In
this paper, we relax this assumption and focus on a more general problem, noisy
PLL, where the ground-truth label may not exist in the candidate set. To
address this challenging problem, we propose a novel framework called
""Iterative Refinement Network (IRNet)"". It aims to purify the noisy samples by
two key modules, i.e., noisy sample detection and label correction. Ideally, we
can convert noisy PLL into traditional PLL if all noisy samples are corrected.
To guarantee the performance of these modules, we start with warm-up training
and exploit data augmentation to reduce prediction errors. Through theoretical
analysis, we prove that IRNet is able to reduce the noise level of the dataset
and eventually approximate the Bayes optimal classifier. Experimental results
on multiple benchmark datasets demonstrate the effectiveness of our method.
IRNet is superior to existing state-of-the-art approaches on noisy PLL.","PARTIAL label learning (PLL) [1], [2] (also called ambigu ous label learning [3], [4] and superset label learning [5], [6]) is a speciÔ¨Åc type of weakly supervised learning [7]. In PLL, each sample is associated with a set of candidate labels, only one of which is the groundtruth label. Due to the high monetary cost of accurately labeled data, PLL has become an active research area in many tasks, such as web mining [8], object annotation [4], [9] and ecological informatics [10]. Unlike supervised learning [11], the groundtruth label is hidden in the candidate set and invisible to PLL [12], [13], which increases the difÔ¨Åculty of model training. Researchers have proposed various approaches to address this problem. These methods can be roughly divided into averagebased [14], [15] and identiÔ¨Åcationbased methods [16], [17]. In averagebased methods, each candidate label has the same probability of being the groundtruth label. They are easy to implement but may be affected by false positive labels [18], [19]. To this end, researchers introduce identiÔ¨Åcationbased methods that treat the groundtruth label as a latent variable and maximize its estimated probability by the maximum margin criterion [20], [21] or the maximum likelihood crite rion [9], [18]. Due to their promising results, identiÔ¨Åcation based methods have attracted increasing attention recently. Zheng Lian, Lan Chen and Bin Liu are with National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Bejing, China, 100190. Email: lianzheng2016@ia.ac.cn; chen lan2016@ia.ac.cn; liubin@nlpr.ia.ac.cn. Licai Sun and Mingyu Xu are with the School of ArtiÔ¨Åcial Intelligence, University of Chinese Academy of Sciences, Beijing, China, 100049. E mail: sunlicai2019@ia.ac.cn; xumingyu2021@ia.ac.cn. Jianhua Tao is with Department of Automation, Tsinghua University, Bejing, China, 100084 and with the School of ArtiÔ¨Åcial Intelligence, University of Chinese Academy of Sciences, Beijing, China, 100049. E mail: jhtao@tsinghua.edu.cn. Manuscript received xxxxxxxx ; revised xxxxxxxx . (Corresponding author: Jianhua Tao, Bin Liu)The above PLL methods rely on a fundamental assump tion that the groundtruth label must reside in the candidate set. However, this assumption may not be satisÔ¨Åed in real world scenarios [22], [23]. Figure 1 shows some typical examples. In online object annotation (see Figure 1(a)), different annotators assign distinct labels to the same image. However, due to the complexity of the image and the unpro fessional judgment of the annotator, the groundtruth label may not be in the candidate set. Another typical application is automatic face naming (see Figure 1(b)). An image with faces is often associated with the text caption, by which we can roughly know who is in the image. However, it is hard to guarantee that all faces have corresponding names in the text caption. Therefore, we relax the assumption of PLL and focus on a more general problem, noisy PLL, where the groundtruth label may not exist in the candidate set. Due to its intractability, few works have studied this problem. The core challenge of noisy PLL is how to deal with noisy samples. To this end, we propose a novel framework called ‚ÄúIterative ReÔ¨Ånement Network (IRNet)‚Äù, which aims to pu rify noisy samples and reduce the noise level of the dataset. Ideally, we can approximate the performance of traditional PLL if all noisy samples are puriÔ¨Åed. IRNet is a multi round framework consisting of two key modules, i.e., noisy sample detection and label correction. To guarantee the performance of these modules, we start with warmup train ing and exploit data augmentation to further increase the reliability of prediction results. We also perform theoretical analysis and prove the feasibility of our proposed method. Qualitative and quantitative results on multiple benchmark datasets demonstrate that our IRNet outperforms currently advanced approaches under noisy conditions. The main contribution of this paper can be summarized as follows: Unlike traditional PLL where the groundtruth labelarXiv:2211.04774v5  [cs.CV]  8 Mar 20232 Annotation from user A:  Horse Ground Truth:  Donkey Annotation from user C:  Deer Annotation from user B:  Mule (a) online object annotation Phoebe is telling  Joey that she  knows who the father of  Rachel ‚Äôs baby is. Ground Truth:  Monica  (b) automatic face naming Fig. 1. Typical applications of noisy PLL. (a) Candidate labels can be provided by crowdsourcing. However, the groundtruth label may not be in the candidate set due to the unprofessional judgments of the annotators. (b) Candidate names can be extracted from the text caption. However, there are general cases of faces without names. must be in the candidate set, we focus on a seldomly discussed but vitally important task, noisy PLL, Ô¨Åll ing the gap of current works. To deal with noisy PLL, we design IRNet, a novel framework with theoretical guarantees. Experimental results on benchmark datasets show the effectiveness of our method. IRNet is superior to existing stateoftheart methods on noisy PLL. The remainder of this paper is organized as follows: In Section 2, we brieÔ¨Çy review some recent works. In Section 3, we propose a novel framework for noisy PLL with theoreti cal guarantees. In Section 4, we introduce our experimental datasets, comparison systems and implementation details. In Section 5, we conduct experiments to demonstrate the effectiveness of our method. Finally, we conclude this paper and discuss our future work in Section 6. 2 R ELATED WORKS "
488,System Identification of NN-based Model Reference Control of RUAV during Hover.txt,"UAV control system is a huge and complex system, and to design and test a UAV
control system is time-cost and money-cost. This paper considered the
simulation of identification of a nonlinear system dynamics using artificial
neural networks approach. This experiment develops a neural network model of
the plant that we want to control. In the control design stage, experiment uses
the neural network plant model to design (or train) the controller. We use
Matlab to train the network and simulate the behavior. This chapter provides
the mathematical overview of MRC technique and neural network architecture to
simulate nonlinear identification of UAV systems. MRC provides a direct and
effective method to control a complex system without an equation-driven model.
NN approach provides a good framework to implement MEC by identifying
complicated models and training a controller for it.","Unmanned aerial vehicles (UAVs)  are becoming more and more popular in a wide field of  applications nowadays. UAVs are used in number of military application for gathering  information and military attacks. In the future will likely see unmanned aircraft employed,  offensively, for bombing and ground attack. As a tool for research and rescue, UAVs can  help find humans lost in the wilderness, trapped in collapsed buildings, or drift at sea. It is  also used in civil application in fire station, police observation of crime disturbance and  natural disaster prevention, where the human ob server will be risky to fight the fire. There  is wide variety of UAV shapes, sizes, configuration and characteristics. Therefore, there is a  growing demand for UAV control systems, and many projects either commercial or  academic destined to design a UAV autopilot were held recently. A lot of impressive results  had already been achieved, and many UAVs, mo re or less autonomous, are used by various  organizations.  An Artificial Neural Network (ANN) [3] is an information processing paradigm that is  stimulated by the way biological nervous systems, such as the brain, process information.  The key element of this paradigm is the nove l structure of the information processing  system.  Basically, a neural network (NN) is composed of a set of nodes (Fig. 1). Each node is  connected to the others via a set of links. Info rmation is transmitted from the input to the  output cells depending of the strength of the links. Usually, neural networks operate in two  phases. The first phase is a learning phase where each of the nodes and links adjust their  strength in order to match with the desired outp ut. A learning algorithm is in charge of this  process. When the learning phase is complete , the NN is ready to recognize the incoming  information and to work as a pattern recognition system.  ANNs, like people, learn by example. An ANN is configured for a specific application, such  as pattern recognition or data classification , through a learning process. Learning in  biological systems involves adjustments to the synaptic connections that exist between t he  neurons.   www.intechopen.com A r t i f i c i a l  N e u r a l  N e t w o r k s    I n d u s t rial and Control Engineering Applications   396  In recent years, there is a wide momentum of ANNs in the contr ol system arena, to design  the UAVs. Any system in which input is not pr oportional to output is known as nonlinear  systems. The main advantages of ANNs are having the processing ability to model  nonlinear systems. ANNs are very suitable for identification of nonlinear dynamic systems.  Multilayer Perceptron model have been used to  model a large number of nonlinear plants.  We can vary the number of hidden layers to minimize the mean square error. ANNs has  been used to formulate a variety of control st rategies [1] [2]. The NN approach is a good  alternative for physical modeling techniques for nonlinear systems.      Fig. 1. General Neural Network Architecture  A fundamental difficulty of many nonlinear control systems, which potentially could  deliver better performance, is extremely difficul t to theoretically predict the behavior of a  system under all possible circumstances. In fact, even design envelope of a controller often  remains largely uncertain. Therefore, it become s a challenging task to verify and validate the  designed controller under all possible flight conditions. A practical solution to this pro blem  is extensive testing of the system. Possibly th e most expensive design items are the control  and navigation systems. Therefor e, one of main questions that each system designer has to  face is the selection of appropriate hardware for UAV system. Such hardware should satisfy  the main requirements without contravening th eir boundaries in terms of quality and cost.  In UAV design this kind of consideration is especially important due to the safety  requirements expressed in airw orthiness standards. Therefore question is how to find the  optimal solution. Thus, simulation  is necessary. Basically there are two type of simulation is  needed while designing UAVs sy stems, they are SoftwareInth eLoop (SIL) [5] simulation  and HardwareIntheLoop (HIL) simulation [4].  To utilize the SIL configuration, the uncompiled software source code, which normally runs  on the onboard computer, is comp iled into the simulation tool itself, allowing this software  to be tested on the simulation host computer. This allows the flight software to be tested  without the need to tieup the flight hardware , and was also used in selection of hardware.  HILS simulates (Fig. 2) a process such that  input and output signals show the time dependent values as realtime operating compon ents. It is possible to test embedded system  under real time with various test conditions. It provides the UAV developer to test many  aspects of autopilot hardware, finding the real time problems, test the reliability, and many  more.  The simulation can be done with the help of Matlab Simulink program environment. This  program can be considered as a facility fully competent for this task.  Simulink is the most    www.intechopen.comSystem Identification of NNbased Model Reference Control of RUAV during Hover     397    Fig. 2. UAV Architecture: Hardwareintheloop Simulation  popular tool, it was not only used for a SIL Simulation of the complete UAV system but also  to create the simulation code of a HI L Simulator that runs in real time.  The system identification is the first and crucial step for the design of the controller,  simulation of the system and so on. Frequently it is necessary to analyze the flight data in  the frequency domain to identify the UAV sy stem. This paper demonstrates how ANN can  be used for non linear identification and contro ller design. The simulation processes consists  of designing a simple system, and simulates th at system with the help of model reference  control block in Matlab/Simulink [6].  The paper is organized as follows: Section 2 describes some related work. Section 3 deals  with system identification and control on th e basis of NNs. Details of design and control  system with NNs approaches is describes in section 4. In section 5, simulations are  performed on RUAVs system and finally, conclusions are drawn in section 6.  2. Related work  "
574,Camera On-boarding for Person Re-identification using Hypothesis Transfer Learning.txt,"Most of the existing approaches for person re-identification consider a
static setting where the number of cameras in the network is fixed. An
interesting direction, which has received little attention, is to explore the
dynamic nature of a camera network, where one tries to adapt the existing
re-identification models after on-boarding new cameras, with little additional
effort. There have been a few recent methods proposed in person
re-identification that attempt to address this problem by assuming the labeled
data in the existing network is still available while adding new cameras. This
is a strong assumption since there may exist some privacy issues for which one
may not have access to those data. Rather, based on the fact that it is easy to
store the learned re-identifications models, which mitigates any data privacy
concern, we develop an efficient model adaptation approach using hypothesis
transfer learning that aims to transfer the knowledge using only source models
and limited labeled data, but without using any source camera data from the
existing network. Our approach minimizes the effect of negative transfer by
finding an optimal weighted combination of multiple source models for
transferring the knowledge. Extensive experiments on four challenging benchmark
datasets with a variable number of cameras well demonstrate the efficacy of our
proposed approach over state-of-the-art methods.","Person reidentiÔ¨Åcation (reid), which addresses the problem of matching people across different cameras, has attracted intense attention in recent years [8, 31, 53]. Much progress has been made in developing a variety of methods to learn features [17, 22, 23] or distance metrics by exploit ing unlabeled and/or manually labeled data. Recently, deep learning methods have also shown signiÔ¨Åcant performance Equal Contribution yThis work was done while AL was a visiting student at UC Riverside. ùê∂!ùê∂""ùê∂# ùê∂!ùê∂""ùê∂#ùëÄ""#ùëÄ!""ùëÄ!# ùê∂!ùê∂""ùê∂#ùëÄ""#ùëÄ!""ùëÄ!# ùê∂$ùëÄ$#= ?ùëÄ$""= ?ùëÄ$!= ?Source data used for pairwise training of existing networkProvided pairwise metrics without access to source data New limited labeled data (colors  indicate corresponding camera)New camera with new limited pairwise labeled data with existing camerasLearned models , source data discarded  Onboard new camera (ùê∂$)Figure 1: Consider a three camera ( C1,C2andC3) net work, where we have only three pairwise distance metrics (M12,M23andM13) available for matching persons, and no access to the labeled data due to privacy concerns. A new camera,C4, needs to be added into the system quickly, thus, allowing us to have only very limited labeled data across the new camera and the existing ones. Our goal in this pa per is to learn the pairwise distance metrics ( M41,M42and M43) between the newly inserted camera(s) and the existing cameras, using the learned source metrics from the existing network and a small amount of labeled data available after installing the new camera(s). improvement on person reid [1, 16, 33, 34, 46, 54]. How ever, with the notable exception of [26, 27], most of these works have not yet considered the dynamic nature of a cam era network, where new cameras can be introduced at any time to cover a certain related area that is not wellcovered by the existing network of cameras. To build a more scal able person reidentiÔ¨Åcation system, it is very essential to consider the problem of how to onboard new cameras into an existing network with little additional effort. Let us consider Knumber of cameras in a network for which we have learned"
435,Limited Gradient Descent: Learning With Noisy Labels.txt,"Label noise may affect the generalization of classifiers, and the effective
learning of main patterns from samples with noisy labels is an important
challenge. Recent studies have shown that deep neural networks tend to
prioritize the learning of simple patterns over the memorization of noise
patterns. This suggests a possible method to search for the best generalization
that learns the main pattern until the noise begins to be memorized.
Traditional approaches often employ a clean validation set to find the best
stop timing of learning, i.e., early stopping. However, the generalization
performance of such methods relies on the quality of validation sets. Further,
in practice, a clean validation set is sometimes difficult to obtain. To solve
this problem, we propose a method that can estimate the optimal stopping timing
without a clean validation set, called limited gradient descent. We modified
the labels of a few samples in a noisy dataset to obtain false labels and to
create a reverse pattern. By monitoring the learning progress of the noisy and
reverse samples, we can determine the stop timing of learning. In this paper,
we also theoretically provide some necessary conditions on learning with noisy
labels. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate
that our approach has a comparable generalization performance to methods
relying on a clean validation set. Thus, on the noisy Clothing-1M dataset, our
approach surpasses methods that rely on a clean validation set.","Noisy labels tend to affect the generalization performance of machine learning. Errors are often inevitable in manual annotation. Moreover, many datasets a re constructed by crawling images and labels from websites, and these often contain a consider able number of noisy labels (e.g., YFCC100M [1], Clothing1M [2]). Therefore, research on lear ning with noisy labels has great im portance. Deep neural networks (DNNs) have been applied to achieve bre akthroughs in many Ô¨Åelds. Many DNNbased methods have been proposed for learning with nois y labels. However, owing to the powerful Ô¨Åtting ability, DNNs may even memorize noise [3], w hich might hamper the generaliza tion of the main pattern (pattern of interest). However, a re cent work [4] further revealed that DNNs prioritize the learning of simple patterns over the memoriz ation of noise. During training, the gener alization performance of the main pattern increases Ô¨Årst an d then decreases. Traditional approaches [2, 5, 6] often employ a clean valida tion set to identify the best stop timing for learning, i.e., early stopping. However, these methods are almost sensitive to the validation sets. The quality of the validation set directly affects the generalization performance. In fact, it is cumbersome to produce a highquality validation set, and in practice applications, clean validation sets are sometimes not readily available.In this work, we focus on the learning of noisy labels without involving clean samples. To estimate the best stop timing of training, we propose a method called l imited gradient descent (LGD) based on the characteristic that a classiÔ¨Åer learns the main patte rn until the noise pattern begins to be memorized. This method hopes to monitor the learning progre sses of the main and noise patterns. Unfortunately, samples of different patterns cannot be ini tially distinguished. Thus, we randomly select a few samples from a noisy training s et as the reverse pattern, which is mutually exclusive to the main pattern. SpeciÔ¨Åcally, we shi ft the labels of the selected samples as reverse labels (as in label+1). It can be proved that the reverse labels are almost false. N ote that the samples of the main pattern are still unknown. We can obta in the training accuracies for the two parts of the samples: the reverse samples and leftover no isy samples. At the early stage of training, the accuracy of the leftover samples increases be cause the main pattern is learned Ô¨Årst, and the accuracy of the reverse samples does not increase (or may even decrease). We could therefore monitor the ratio of the two accuracies to predict the best ge neralization. We evaluate the performance on the CIFAR10, CIFAR100, and Clothing1M datasets. The exper imental results on CIFAR10 and CIFAR100 show that althoug h the accuracies with our method are comparable to those from corresponding traditional met hods, the variances of the experimental results are signiÔ¨Åcantly reduced, which shows that our meth od has better robustness. We further demonstrate stateoftheart performance on the noisy rea lworld Clothing1M dataset. The main contributions of the present study are as follows. F irst, we propose a weakly supervised method called limited gradient descent (LGD) that can learn the main pattern to the maximum extent possible from noisy labels. Second, we prepare a few samples with false labels for training, which no study has attempted thus far to the best of our knowledge. T hird, we theoretically prove some necessary conditions on LGD learning with noisy labels. Las tly, our method is free of models; thus, it can be applied to most DNNs and loss functions based on the s tochastic gradient descent (SGD) optimization. 2 Related Works "
423,Optimal transport meets noisy label robust loss and MixUp regularization for domain adaptation.txt,"It is common in computer vision to be confronted with domain shift: images
which have the same class but different acquisition conditions. In domain
adaptation (DA), one wants to classify unlabeled target images using source
labeled images. Unfortunately, deep neural networks trained on a source
training set perform poorly on target images which do not belong to the
training domain. One strategy to improve these performances is to align the
source and target image distributions in an embedded space using optimal
transport (OT). However OT can cause negative transfer, i.e. aligning samples
with different labels, which leads to overfitting especially in the presence of
label shift between domains. In this work, we mitigate negative alignment by
explaining it as a noisy label assignment to target images. We then mitigate
its effect by appropriate regularization. We propose to couple the MixUp
regularization \citep{zhang2018mixup} with a loss that is robust to noisy
labels in order to improve domain adaptation performance. We show in an
extensive ablation study that a combination of the two techniques is critical
to achieve improved performance. Finally, we evaluate our method, called
\textsc{mixunbot}, on several benchmarks and real-world DA problems.","Deep neural networks have reached stateoftheart performance on classiÔ¨Åcation problems due to their ability to Ô¨Åt complex dataset while also generalizing within a speciÔ¨Åc domain (He et al., 2016). However, in applications like computer vision, it is standard to have sameclass samples coming from different domains, for instance when they have different backgrounds or colorspaces. Unfortunately, the generalization of deep neural networks across different domains is poor, and the object of intense research. Diversifying the domains with new samples in the training dataset is also a challenging task. For instance in medicine, collecting and annotating data is timeconsuming and prone to errors. The problem of domain adaptation , tackles the case where we have access to two domains sharing the same classes where one has labeled data, called the source domain, and the other has unlabeled data, called the target domain. The purpose of domain adaptation problems is to classify the unlabeled target samples using the labeled source samples (Pan & Yang, 2010; Patel et al., 2015). A popular and efÔ¨Åcient method to solve this problem is to use thealignment strategy , where sameclass samples from different domains are aligned in an embedded space. To align the embedded source and target samples, several techniques exist such as adversarial training (Ganin et al., 2016; Long et al., 2018; Chen et al., 2020a) or optimal transport (Courty et al., 2017; Courty et al., 2017; Redko et al., 2017). Optimal Transport (Peyr ¬¥e & Cuturi, 2019) has become one of the most used methods to compare probability distri butions in machine learning. It has been popular for tasks such as generative models (Arjovsky et al., 2017; Genevay et al., 2018; Salimans et al., 2018; Burnel et al., 2021) or supervised learning problems (Frogner et al., 2015; Fatras et al., 2021a). In domain adaptation, it has been used to transport the source domain to the target domains (Courty et al., 2017), or to align the domains in a joint space of data and labels (Courty et al., 2017; Bhushan Damodaran et al., 2018). To reduce the OT cost, Bhushan Damodaran et al. (2018) used a minibatch approximation of OT (Fa tras et al., 2020), which led to nonoptimal connections between domains due to the minibatch samplings and the marginal constraints of exact OT. This phenomenon corresponds to negative transfer in domain adaptation problems. To mitigate the nonoptimal connections of minibatch OT, DEEPJDOT (Bhushan Damodaran et al., 2018) required large batch sizes‚Äîotherwise small batch sizes lead deep neural networks to overÔ¨Åt. Another workaround to mitigate the nonoptimal connections was to use Unbalanced Optimal Transport (UOT), an OT variant with relaxed marginals, as proposed in Fatras et al. (2021b). In this paper, we explain nonoptimal connections as assigning noisy labels to target samples. To avoid overÔ¨Åtting from noisy labels, we propose to regularize the neural networks and to use a noisy label robust loss in the transfer corresponding author: kilian.fatras@mila.quebec 1arXiv:2206.11180v1  [cs.CV]  22 Jun 2022Published at 1st Conference on Lifelong Learning Agents, 2022 term. We propose to couple two techniques: i)regularize the neural networks using the MixUp regularization (Zhang et al., 2018a) on both source and target domains, MixUp interpolates samples from the same distribution uniformly at random as well as their label when available; ii)Use the symmetric crossentropy loss in the transfer term, which has been proven to be robust to label noise (Wang et al., 2019). Our Ô¨Åndings show that it is the combination of the MixUp and symmetric crossentropy which leads to an increase in the performances of models as shown in an extensive ablation study, while the use of the techniques separately does not lead to any increase. It can be used for different optimal transport loss as we show in our experiments. This paper is structured as follows. In Section 2, we review the different methods to solve domain adaptation and we deÔ¨Åne optimal transport as well as its use in domain adaptation problems. In Section 3, we present our method MIXUNBOT as well as the components it is built upon such as DEEPJDOT , MixUp, and the symmetric crossentropy (SCE) loss. In Section 4, we present extensive domain adaptation and partial domain adaptation experiments. 2 R ELATED WORK ON DOMAIN ADAPTATION AND OPTIMAL TRANSPORT "
571,Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer.txt,"The success of Vision Transformer (ViT) in various computer vision tasks has
promoted the ever-increasing prevalence of this convolution-free network. The
fact that ViT works on image patches makes it potentially relevant to the
problem of jigsaw puzzle solving, which is a classical self-supervised task
aiming at reordering shuffled sequential image patches back to their natural
form. Despite its simplicity, solving jigsaw puzzle has been demonstrated to be
helpful for diverse tasks using Convolutional Neural Networks (CNNs), such as
self-supervised feature representation learning, domain generalization, and
fine-grained classification.
  In this paper, we explore solving jigsaw puzzle as a self-supervised
auxiliary loss in ViT for image classification, named Jigsaw-ViT. We show two
modifications that can make Jigsaw-ViT superior to standard ViT: discarding
positional embeddings and masking patches randomly. Yet simple, we find that
Jigsaw-ViT is able to improve both in generalization and robustness over the
standard ViT, which is usually rather a trade-off. Experimentally, we show that
adding the jigsaw puzzle branch provides better generalization than ViT on
large-scale image classification on ImageNet. Moreover, the auxiliary task also
improves robustness to noisy labels on Animal-10N, Food-101N, and Clothing1M as
well as adversarial examples. Our implementation is available at
https://yingyichen-cyy.github.io/Jigsaw-ViT/.","Vision Transformer (ViT) [ 1] is an architecture inherited from Natural Language Processing [ 2] while applied to image classiÔ¨Åcation with taking raw image patches as inputs. Different from classical Convolutional Neural Networks (CNNs), the architectures of ViTs are based on selfattention modules [ 2], which aim at modeling global interactions of all pixels in feature maps. More precisely, ViTs take sequential image patches as inputs, and the attention mechanism enables interaction and aggregation directly among patch information. Therefore, compared to CNNs where image features are progressively learnt from local to global context via reducing spatial resolution, ViT enjoys obtaining global information from the very beginning. Up till now, such convolutionfree networks have been achieving great success on various computer vision tasks, including image classiÔ¨Åcation [ 3,4,5,6,7,8], object detection [ 9,6,10], semantic segmentation [11, 9, 10] and image generation [12], etc. The fact that ViTs work on image patches makes it potentially relevant to one classical image patchbased learning task, that is, jigsaw puzzle solving. Solving jigsaw puzzle aims at reordering shufÔ¨Çed sequential image patches back to their original form. In practice, the problem is interesting for cultural heritage and archaeology to search the correct conÔ¨Åguration given numerous fragments of an art masterpiece [ 13]. However, in the Computer Vision community, thearXiv:2207.11971v2  [cs.CV]  5 Jan 2023Figure 1: Overview framework of our JigsawViT. (Top) We incorporate jigsaw puzzle solving (in blue Ô¨Çow) into the standard ViT for image classiÔ¨Åcation (in red). During the training, we jointly learn the two tasks. (Bottom) The details of our jigsaw puzzle Ô¨Çow. We drop several patches, i.e., patch masking, and remove positional embeddings before feeding to ViT. For each unmasked patch, the model should predict the class corresponding to the patch position. most interesting aspect could be that it provides offtheshelf annotations for free considering a given image. Despite its simplicity, it has shown effectiveness in diverse Computer Vision tasks based on CNNs such as: selfsupervised feature representation learning [ 14], domain generalization [ 15] and Ô¨Ånegrained classiÔ¨Åcation [ 16]. Motivated by the fact that both jigsaw puzzle solving and ViT share the same basis of learning from image patches, we consider incorporating solving jigsaw puzzle to ViT for image classiÔ¨Åcation tasks. In this paper, we explore leveraging the jigsaw puzzle solving problem as a selfsupervised auxiliary loss of a standard ViT, named JigsawViT. Precisely, as shown in Fig. 1, in addition to the standard classiÔ¨Åcation Ô¨Çow in the endtoend training, we add a jigsaw Ô¨Çow whose goal is to predict the absolute positions of the input patches by solving a classiÔ¨Åcation problem. Notably, we make two important modiÔ¨Åcations compared to the naive jigsaw puzzle when feeding input patches to ViTs: i)we get rid of the positional embeddings in the jigsaw Ô¨Çow, by which we prevent the model from cheating from explicit clues in the positional embeddings; ii)we randomly mask some input patches, i.e., patch masking , and then aim at predicting only the positions of those unmasked patches, hence making the prediction rely on global context rather than several particular patches. Despite its simplicity, we Ô¨Ånd that our JigsawViT is able to improve on both generalization and robustness over the standard ViT, which is usually rather a tradeoff [ 17]. To be speciÔ¨Åc, in terms of generalization, we observe a steady increase in classiÔ¨Åcation accuracy on ImageNet1K [ 18] that our jigsaw Ô¨Çow brings to the ViTs. As for robustness, we Ô¨Årst show that the proposed jigsaw Ô¨Çow provides consistent improvement against noisy labels on three important realworld benchmarks, i.e., Animal10N [ 19], Food101N [ 20] and Clothing1M [ 21]. Then, we show that our proposed JigsawViTs can effectively enhance the robustness of ViTs against adversarial attacks in both blackbox and whitebox attack settings. To summarize, our contributions are as follows: First , we propose to introduce the jigsaw puzzle solving task into ViTbased models, namely JigsawViT, with two techniques: removing positional embeddings, and randomly masking patches. Second , empirical results suggest that our jigsaw Ô¨Çow not only improves the generalization ability of ViTs on largescale image classiÔ¨Åcation, but also the robustness against label noise and adversarial examples. Our implementation is available at https://yingyichencyy.github.io/JigsawViT . 2 Related work "
157,NaturalFinger: Generating Natural Fingerprint with Generative Adversarial Networks.txt,"Deep neural network (DNN) models have become a critical asset of the model
owner as training them requires a large amount of resource (i.e. labeled data).
Therefore, many fingerprinting schemes have been proposed to safeguard the
intellectual property (IP) of the model owner against model extraction and
illegal redistribution. However, previous schemes adopt unnatural images as the
fingerprint, such as adversarial examples and noisy images, which can be easily
perceived and rejected by the adversary. In this paper, we propose
NaturalFinger which generates natural fingerprint with generative adversarial
networks (GANs). Besides, our proposed NaturalFinger fingerprints the decision
difference areas rather than the decision boundary, which is more robust. The
application of GAN not only allows us to generate more imperceptible samples,
but also enables us to generate unrestricted samples to explore the decision
boundary.To demonstrate the effectiveness of our fingerprint approach, we
evaluate our approach against four model modification attacks including
adversarial training and two model extraction attacks. Experiments show that
our approach achieves 0.91 ARUC value on the FingerBench dataset (154 models),
exceeding the optimal baseline (MetaV) over 17\%.","In the past few years, deep neural networks (DDNs) have been applied to a wide range of fields like autonomous driv ing[Gauerhof et al. , 2018 ], face recognition [Parkhi et al. , 2015 ], and intelligent healthcare [Esteva et al. , 2017 ]due to their outstanding performance. While DNNs are prevalent in our lives, training such a model is a nontrivial task as it re quires a large amount of annotated data and powerful comput ing resources. Thus, many companies provide the prediction of their trained model as APIs to profit, such as Machine LearningasaService (MLaaS). In other words, the model is becoming a critical business asset, which requires to be pro tected against illegal redistribution and model extraction. Negative Model Positive Model Source Model Class A Class BFigure 1: The intuition of our proposed NaturalFinger. It leverages GAN to fingerprint the decision difference areas (purple areas) be tween positive models and negative models. Generally, a type of common approach to protect the in tellectual property (IP) of DNN models is model fingerprint ing[Cao et al. , 2021; Yang et al. , 2022; Lukas et al. , 2019 ]. This kind of scheme first constructs a query set based on the source model (protected model) as the fingerprint. Then, the defender queries the suspect model with the query set. If the matching rate between the predicted labels of the suspected model and the query set labels exceeds a threshold, the de fender judges the suspect model as a stolen model. However, previous fingerprinting schemes suffer from the following two problems: 1) Stealthiness. They generate un natural samples to query the suspect model like noisy exam ples, which can be perceived and rejected by the adversary; 2) Robustness. Many fingerprinting schemes [Wang and Chang, 2021; Peng et al. , 2022 ]leverage various adversarial exam ples to fingerprint the decision boundary, which is not robust against adversarial defense. To solve those two problems, we propose NaturalFinger, which generates natural query samples with generative adver sarial networks (GANs) [Goodfellow et al. , 2014a ]. The in tuition of our method is shown in Fig. 1. Our proposed Natu ralFinger fingerprints the decision difference areas (purple ar eas) between positive models (stolen from the source model) and negative models (unrelated to the source model), rather than the decision boundary. Beneficial to this strategy, Natu ralFinger is resistant to various model modifications. What‚Äôs more, the utilization of GAN not only empowers us to gener ate more imperceptible samples, but also enables us to gen erate unrestricted samples to explore the decision boundary.arXiv:2305.17868v1  [cs.CV]  29 May 2023(a) Frog/Bird (b) Bird/Airplane (c) Horse/Deer (d) Airplane/Ship Figure 2: The examples of NaturalFinger. The first label and the second label are predicted by positive models and negative models, respectively. Fig. 2 shows some interesting examples of our scheme. As shown in Fig. 2, those samples contain some contentious fea tures, which causes positive models and negative models to produce different labels. For example, Fig. 2(a) is a bird with green color, positive models predict it as a bird while negative models predict it as a frog. However, simply applying GAN faces the following ques tions. 1) How to generate samples that fingerprint the deci sion difference areas? 2) How to generate natural query sam ples instead of lowquality images as we optimize the input noises? 3) How to reduce overfit when the number of trained models is small? To address the first question, considering samples belong ing to decision difference areas are predicted differently by positive models and negative models, we assign two differ ent labels for them during optimization. For positive models, we assign the original labels predicted by most positive mod els. For negative models, to ease the optimization, we assign them the closest labels to the original labels. In specific, we attack the negative models with a fast gradient sign method (FGSM) [Goodfellow et al. , 2014b ]attack to obtain the ad versarial labels. We then optimize the noises to ensure that all models can correctly predict the generated images. To handle the second question, we utilize the discriminator to constrain the generated samples by adding its loss to the total loss. To tackle the last question, inspired by data augmentation, we apply image transformation to the generated samples before feeding them into the model. In summary, we propose a stealthy, robust, and effective model fingerprinting scheme to protect the IP of the model owner. The main contributions are summarized as follows: ‚Ä¢ We propose NaturalFinger which generates natural and stealthy query samples with GAN. Besides, NaturalFinger fingerprints the decision difference areas instead of the de cision boundary, which is more robust against model mod ification and is more effective against model extraction. ‚Ä¢ To solve the challenges encountered by applying GAN, we propose three tricks including adversarial label, dis criminator loss, and input transformation. Ablation experi ments show that those tricks significantly improve the per formance of NaturalFinger from 0.79 to 0.91 in ARUC. ‚Ä¢ Experiments demonstrate that NaturalFinger is robust against four model modifications and is effective against two model extraction attacks. It attains a 0.91 ARUC value on the FingerBench dataset (154 models), exceeding the optimal baseline (MetaV [Panet al. , 2022 ]) over 17%.2 Related Work "
175,Clickbait Identification using Neural Networks.txt,"This paper presents the results of our participation in the Clickbait
Detection Challenge 2017. The system relies on a fusion of neural networks,
incorporating different types of available informations. It does not require
any linguistic preprocessing, and hence generalizes more easily to new domains
and languages. The final combined model achieves a mean squared error of
0.0428, an accuracy of 0.826, and a F1 score of 0.564. According to the
official evaluation metric the system ranked 6th of the 13 participating teams.","Clickbait refers to headlines of web content targeting the human ‚Äúcuriosity gap‚Äù [ 13]. The reader is typically lured into clicking a targetlink by raising interest into the advertised story mentioned in the teaser message, without providing enough details to satisfy the readers curiosity. Such clickbaitlinks often contain videos, picture galleries, or simple listings. The content is mostly of little journalistic quality, but spreads well in social media by referring to soft topics. Content describing such content ( e.g., gossip, food news, or sensational stories) is often observed in tabloid newspapers. The conversion of a newspaper into tabloid format (also referred to as tabloidization) is often considered problematic [ 19]. However, there are also online magazines that provide clickbait titles on more serious topics. According to an analysis all of the top 20 most proliÔ¨Åc English news publishers on Twitter occasionally publish clickbait headlines [ 16]. Depending on the newspaper, percentages of clickbait content ranges from 8 % to an astonishing 51 % In this publication we describe our approach in the Clickbait Detection Challenge 2017 [ 17] to detect clickbait headlines using neural networks. 2. RELATED WORK "
466,Digit Image Recognition Using an Ensemble of One-Versus-All Deep Network Classifiers.txt,"In multiclass deep network classifiers, the burden of classifying samples of
different classes is put on a single classifier. As the result the optimum
classification accuracy is not obtained. Also training times are large due to
running the CNN training on single CPU/GPU. However it is known that using
ensembles of classifiers increases the performance. Also, the training times
can be reduced by running each member of the ensemble on a separate processor.
Ensemble learning has been used in the past for traditional methods to a
varying extent and is a hot topic. With the advent of deep learning, ensemble
learning has been applied to the former as well. However, an area which is
unexplored and has potential is One-Versus-All (OVA) deep ensemble learning. In
this paper we explore it and show that by using OVA ensembles of deep networks,
improvements in performance of deep networks can be obtained. As shown in this
paper, the classification capability of deep networks can be further increased
by using an ensemble of binary classification (OVA) deep networks. We implement
a novel technique for the case of digit image recognition and test and evaluate
it on the same. In the proposed approach, a single OVA deep network classifier
is dedicated to each category. Subsequently, OVA deep network ensembles have
been investigated. Every network in an ensemble has been trained by an OVA
training technique using the Stochastic Gradient Descent with Momentum
Algorithm (SGDMA). For classification of a test sample, the sample is presented
to each network in the ensemble. After prediction score voting, the network
with the largest score is assumed to have classified the sample. The
experimentation has been done on the MNIST digit dataset, the USPS+ digit
dataset, and MATLAB digit image dataset. Our proposed technique outperforms the
baseline on digit image recognition for all datasets.","Despite signiÔ¨Åcant success in the area of knowledge discovery, conventional m a chine learning approaches can fail to perform well when they deal with complex data  if it is imbalanced, high dimensional, noisy, etc. This is due to the diÔ¨Éculty of these 2  techni ques in capturing various features and underlying data structure [ 1]. Hence, an  important research topic has evolved in data mining for eÔ¨Äectively building an  eÔ¨Écient knowledge discovery and mining model. Ensemble learning, being one r e search hotspot, aims at integration of fusion, modeling, and mining of data to form a  uniÔ¨Åed model. SpeciÔ¨Åcally, it extracts feature sets along with various transform a tions. Based on the features learnt, various algorithms produce weak predictions.  Then, ensemble learning fuses the information obtained from the same for kno w ledge discovery an d for better prediction with the help of adaptive voting [ 1].  Dong et. al. [ 1] presented an introduction to ensemble learning ap proaches. The  conducted research in [ 2] used weak multiclass ensembles which combine outputs of  different layer based transfer conditions in deep networks. Experiments have shown  that this reduce s the eÔ¨Äects of adverse feature transference of features in image rec ognition tasks [ 1].  The research done in [ 3] has also used weak multiclass ensembles  to reduce the cross domain error in domain adaptation for the task of sentiment ana l ysis. The work in [ 4] has designed an ensemble which uses AdaBoost [ 5] for adjus t ment of weights of source data and target data. It achieved good performance on UCI datasets for insufficient data. Subsequently more work has been done in [ 614] which  usually use ensembles of weak multiclass classifiers which do not give satisfactory results as will be discussed. One important type of ensemble which has potential and is unexplored is  OneVersus All (OVA) ensemble.   We propose to use a deep learning [15] based OVA ensemble approach for digit  image recognition. By doing this, an e ffort has been  made to incr ease the classifica tion acc uracy of deep n eural netwo rks on the classical digit image recognition task by  using them in an ensemble of binary classification deep  netw orks for the purpose  of  multiclass class ification. Handwritten digit recognition is an applied and an interes t ing research area. Many app roaches have been proposed for this, such as Convol u tional neural network based classification techniques [ 1618], reinforcement learning  [19], artificial neural networks [ 20,21], support vector m achines [22,23], etc. In spite  of the fact that many these techniques have achieved decent classification accuracy o n  larger, more complex, and realistic images   issues remain due to issues including  nonstandard writing of circle and hook patterns, e.g. in 4, 5, 7 and 9, and also image  perception issues such as skew, slant, blur, small size, etc. After applying the pr o posed approach to digit image recognition as demonstrated by the experiments, higher classification accuracy has been achieved as compared to that of conventional deep  network classifiers and other state ofart ensemble techniques  on all the datasets used .  Three  digit i mage datasets  namely the  MNIST digit image dataset [24], the USPS+  digit image dataset  [25] and the MATLAB digit image dataset  were used.  In one of  the experi ments which  used a subset of the MNIST  digit image dataset,  the accu racy of Binary  Classi fication Convolutio nal Neural Network  (BCCNN)  Ense mble  was found  to be 98.03%  which was highe r than that found a fter using a con vention al  Multiclass Con volutio nal N eural Netwo rk (MCNN) viz. 97.90%.  After struc tural  modifications  to the  ensemble, and subsequ ently t esting  it as well a s a conven tional  deep net work  on a another  subset  of MNIST,  it was found that the proposed technique   gave an accur acy of 98.50%  which  was higher  than that  of the  conventio nal deep  netwo rk viz. 98.4 875%.  3  The rest of the paper is structured as follows. Related works are discussed in Sec tion 2. Section 3  discusses the proposed a pproach. In Section 4 , experimentation d e tails are discussed. We conclude in Section 5 .    2. Related Works   "
15,Co-matching: Combating Noisy Labels by Augmentation Anchoring.txt,"Deep learning with noisy labels is challenging as deep neural networks have
the high capacity to memorize the noisy labels. In this paper, we propose a
learning algorithm called Co-matching, which balances the consistency and
divergence between two networks by augmentation anchoring. Specifically, we
have one network generate anchoring label from its prediction on a
weakly-augmented image. Meanwhile, we force its peer network, taking the
strongly-augmented version of the same image as input, to generate prediction
close to the anchoring label. We then update two networks simultaneously by
selecting small-loss instances to minimize both unsupervised matching loss
(i.e., measure the consistency of the two networks) and supervised
classification loss (i.e. measure the classification performance). Besides, the
unsupervised matching loss makes our method not heavily rely on noisy labels,
which prevents memorization of noisy labels. Experiments on three benchmark
datasets demonstrate that Co-matching achieves results comparable to the
state-of-the-art methods.","Deep Neural Networks (DNNs) have shown remarkable performance in a variety of applications [19, 24, 38]. How ever, the superior performance comes with the cost of re quiring a correctly annotated dataset, which is extremely timeconsuming and expensive to obtain in most realworld scenarios. Alternatively, we may obtain the training data with annotations efÔ¨Åciently and inexpensively through ei ther online key search engine [22] or crowdsourcing [49], but noisy labels are likely to be introduced consequently. Previous studies [2, 50] demonstrate that fully memorizing noisy labels affects accuracy of DNNs signiÔ¨Åcantly, hence it is desirable to develop effective algorithms for learning with noisy labels. To handle noisy labels, most approaches focus on esti mating the noise transition matrix [12, 30, 37, 44] or cor recting the label according to model prediction [26, 31, 39, 46]. However, it is challenging to estimate the noise transition matrix especially when the number of classes is large. Another promising direction of study proposes to train two networks on smallloss instances [7, 14, 43, 47], wherein Decoupling [27] and Coteaching+ [47] introduce the ‚ÄúDis agreement‚Äù strategy to keep the two networks diverged to achieve better ensemble effects. However, the instances selected by ‚ÄúDisagreement‚Äù strategy are not guaranteed to have correct labels [14, 43], resulting in only a small por tion of clean instances being utilized in the training process. Coteaching [14] and JoCoR [43] aim to reduce the diver gence between two different networks so that the number of clean labels utilized in each minibatch increases. In the be ginning, two networks with different learning abilities Ô¨Ålter out different types of error. However, with the increase of training epochs, two networks gradually converge to a con sensus and even make the wrong predictions consistently. To address the above concerns, it is essential to keep a balance between divergence and consistency of the two net works. Inspired by augmentation anchoring [5, 35] from semisupervised learning, we propose a method using weak (e.g. using only cropandÔ¨Çip) and strong (e.g. using Ran dAugment [9]) augmentations for two networks respec tively to address the consensus issue. SpeciÔ¨Åcally, one network produces the anchoring labels based on weakly augmented images. The anchoring labels are used as tar gets when the peer network is fed the stronglyaugmented version of the same images. Their difference is captured by an unsupervised matching loss. Stronger augmentation results in disparate predictions, which guarantees the diver gence between two networks, unless they have learned ro bust generalization ability. As earlylearning phenomenon shows that the networks Ô¨Åt training data with clean labels before memorizing the samples with noisy labels [2]. Co matching trains two networks with a loss calculated by in terpolating between two loss terms: 1) A supervised clas siÔ¨Åcation loss encourages learning from clean labels during the earlylearning phase. 2) An unsupervised matching loss limits the divergence of two networks and prevents mem orization of noisy labels after the earlylearning phase. In each training step, we use the smallloss trick to select the most likely clean samples, thus ensuring the error Ô¨Çow fromarXiv:2103.12814v1  [cs.CV]  23 Mar 2021the biased selection would not be accumulated. To show that Comatching improves the robustness of deep learning on noisy labels, we conduct extensive exper iments on both synthetic and realworld noisy datasets, in cluding CIFAR10, CIFAR100 and Clothing1M datasets. Experiments show that Comatching signiÔ¨Åcantly advances stateoftheart results with different types and levels of la bel noise. Besides, we study the impact of data augmen tation and provide ablation study to examine the effect of different components in Comatching. 2. Related work "
443,Leveraging Native Language Speech for Accent Identification using Deep Siamese Networks.txt,"The problem of automatic accent identification is important for several
applications like speaker profiling and recognition as well as for improving
speech recognition systems. The accented nature of speech can be primarily
attributed to the influence of the speaker's native language on the given
speech recording. In this paper, we propose a novel accent identification
system whose training exploits speech in native languages along with the
accented speech. Specifically, we develop a deep Siamese network-based model
which learns the association between accented speech recordings and the native
language speech recordings. The Siamese networks are trained with i-vector
features extracted from the speech recordings using either an unsupervised
Gaussian mixture model (GMM) or a supervised deep neural network (DNN) model.
We perform several accent identification experiments using the CSLU Foreign
Accented English (FAE) corpus. In these experiments, our proposed approach
using deep Siamese networks yield significant relative performance improvements
of 15.4 percent on a 10-class accent identification task, over a baseline
DNN-based classification system that uses GMM i-vectors. Furthermore, we
present a detailed error analysis of the proposed accent identification system.","Over the recent years, many of voicedriven technologies have achieved signiÔ¨Åcant robustness needed for mass deploy ment. This is largely due to signiÔ¨Åcant advances in automatic speech recognition (ASR) technologies and deep learning algorithms. However, the variability in speech accents pose a signiÔ¨Åcant challenge to stateoftheart speech systems. In particular, large sections of the Englishspeaking population in the world face difÔ¨Åculties interacting with voicedriven agents in English due to the mismatch in speech accents This work was carried out with the help of a research grant awarded by Microsoft Research India (MSRI) for the Summer Workshop on ArtiÔ¨Åcial Social Intelligence.seen in the training data. The accented nature of speech can be primarily attributed to the inÔ¨Çuence of the speaker‚Äôs native language. In this work we focus on the problem of accent identiÔ¨Åcation , where the user‚Äôs native language is au tomatically determined from their nonnative speech. This can be viewed as a Ô¨Årst step towards building accentaware voicedriven systems. Accent identiÔ¨Åcation from nonnative speech bears re semblance to the task of language identiÔ¨Åcation [1]. How ever, accent identiÔ¨Åcation is a harder task as many cues about the speaker‚Äôs native language are lost or suppressed in the nonnative speech. Nevertheless, one may expect that the speaker‚Äôs native language is reÔ¨Çected in the acoustics of the individual phones used in nonnative language speech, along with pronunciations of words and grammar. In this work, we focus on the acoustic characteristics of an accent induced by a speaker‚Äôs native language. Our main contributions: We develop a novel deep Siamese network based model which learns the association between accented speech and native language speech. We explore ivector features extracted using both an un supervised Gaussian mixture model (GMM) and a su pervised deep neural network (DNN) model. We present a detailed error analysis of the proposed system which reveals that the confusions among accent predictions are contained within the language family of the corresponding native language. Section 3 outlines the ivector feature extraction process. Section 4 describes our Siamese networkbased model for ac cent identiÔ¨Åcation. Our experimental results are detailed in Section 5 and Section 6 provides an error analysis of our pro posed approach. 2. RELATED WORK "
340,Combining Multi-level Contexts of Superpixel using Convolutional Neural Networks to perform Natural Scene Labeling.txt,"Modern deep learning algorithms have triggered various image segmentation
approaches. However most of them deal with pixel based segmentation. However,
superpixels provide a certain degree of contextual information while reducing
computation cost. In our approach, we have performed superpixel level semantic
segmentation considering 3 various levels as neighbours for semantic contexts.
Furthermore, we have enlisted a number of ensemble approaches like max-voting
and weighted-average. We have also used the Dempster-Shafer theory of
uncertainty to analyze confusion among various classes. Our method has proved
to be superior to a number of different modern approaches on the same dataset.","Deep Learning has brought a new era in machine learning. Being able to learn more complex features from images, problems such as classication, localiza tion, segmentation has seen remarkable progress especially for natural images. Previously most signicant research in the domain of natural image process ing was performed using some sort of pattern recognition over pixels [5,9,3]. The problem that has been dealt in this paper is semantic image segmentation. Image segmentation goes beyond tasks like object recognition or localization. In this problem we are mainly interested in precise segments which semanti cally separates one object from another. While pixel level algorithms [12,8,10] provide very ne level segmentation, superpixels [18] provide much lesser com putational complexity while not compromising performance. Superpixels refer to small patches of adjacent similar pixels grouped together. We have used these su perpixels for our algorithms thus providing realtime performance. ConvolutionalarXiv:1803.05200v1  [cs.CV]  14 Mar 20182 Aritra Das et al. neural networks(CNNs) have showed tremendous performance in the eld of natural image processing as well as segmentation. In our approach we have im plemented multiple convolutional neural networks to obtain results. Any classi cation problem can be associated with uncertainty in the decision process. We have used some ensemble methods as well as DempsterShafer Theory to han dle such uncertainty. The next section will give a brief review of some related works. Section 3 will explain the methodologies. In section 4 and 5 will cover the experimentations and discussions regarding obtained results. 2 Related Works "
153,KGNN: Harnessing Kernel-based Networks for Semi-supervised Graph Classification.txt,"This paper studies semi-supervised graph classification, which is an
important problem with various applications in social network analysis and
bioinformatics. This problem is typically solved by using graph neural networks
(GNNs), which yet rely on a large number of labeled graphs for training and are
unable to leverage unlabeled graphs. We address the limitations by proposing
the Kernel-based Graph Neural Network (KGNN). A KGNN consists of a GNN-based
network as well as a kernel-based network parameterized by a memory network.
The GNN-based network performs classification through learning graph
representations to implicitly capture the similarity between query graphs and
labeled graphs, while the kernel-based network uses graph kernels to explicitly
compare each query graph with all the labeled graphs stored in a memory for
prediction. The two networks are motivated from complementary perspectives, and
thus combing them allows KGNN to use labeled graphs more effectively. We
jointly train the two networks by maximizing their agreement on unlabeled
graphs via posterior regularization, so that the unlabeled graphs serve as a
bridge to let both networks mutually enhance each other. Experiments on a range
of well-known benchmark datasets demonstrate that KGNN achieves impressive
performance over competitive baselines.","Graphstructured data are ubiquitous in a wide range of domains. Examples include social networks [ 31], biological reaction networks [28], molecules [ 34]. For graphstructured data, one fundamental problem is graph classification, which aims at analyzing and pre dicting the property of the entire graph. Such a problem has various downstream applications, including predicting the properties of molecules [ 13] and analyzing the functionality of compounds [ 19]. Graph classification is typically formalized as a supervised learn ing task, and many recent works propose to use graph neural net works (GNNs) [ 23,24,40] to solve the problem. The basic idea is to learn effective graph representations with nonlinear message passing schemas. At each step, a node receives messages from all its neighbors, which are further aggregated to update the node representation. Finally, a readout function is applied to integrate all the node representations into a representation of the whole graph. With this shared message passing framework, the learned graph representations can implicitly capture the similarity between query graphs and labeled graphs in the latent space. Despite the good performance, GNNs usually require a large amount of labeled data for training and fail to leverage unlabeled data. However, data an notation often requires domain experts, which is highly expensive, especially in specific domains such as biomedicine [13]. This motivates us to study semisupervised graph classifica tion, i.e., using both labeled and unlabeled data for graph classi fication. The unlabeled data serve as a regularizer, which helps a model better explore the inherent graph semantic information even with a limited amount of labeled data. Indeed, there are a handful of works along this line [ 13,24,34], and they typically employ semisupervised learning techniques to train GNN models. These approaches usually integrate selftraining [ 22] or knowledge distillation [ 14] into GNNs. However, these methods suffer from two key limitations: (1) Unable to well explore graph similar ity. Graph classification relies on comparing the query graphs witharXiv:2205.10550v1  [cs.LG]  21 May 2022labeled graphs. Existing methods [ 13,24,34] typically compute the similarity of graphs in an implicit way by projecting graphs into a latent space with a GNN encoder. However, such implicit methods often cannot well explore the similarity of graphs. (2) Suffering from labeled data scarcity . Besides, experimental results show that the performance of existing methods is still unsatisfactory especially when labeled data are very scarce. The reason is that these methods are not able to obtain highquality annotated data to improve model training. Therefore, we are looking for an approach that is able to better consider the relationship among graphs and meanwhile overcome the challenge of scarce labeled data. In this paper, we propose such a method called the Kernelbased Graph Neural Network (KGNN). The key idea of KGNN is to en hance GNNs with graph kernels, which are able to explicitly mea sure the graph similarity of graphs. To leverage graph kernels effectively, we introduce two modules in a KGNN, i.e., a GNNbased network and a kernelbased network. The GNNbased network is parameterized by existing GNNs, which uses message passing mechanisms to learn useful graph representations for graph classi fication. In contrast, the kernelbased network employs a memory network, where the memory stores the given labeled graphs. Given a query graph, we compare it with all the graphs in the memory using graph kernels, and further integrate the labels of the most similar graphs to predict the label of the query graph. The GNNbased network and kernelbased network explore graph similarity from different angles, i.e., message passing and graph kernels respectively. Although they are naturally comple mentary, how to jointly train both networks which enables us to distill the knowledge between each other is nontrivial. We solve the problem with a novel posterior regularization framework [ 7], which encourages both networks to collaborate with each other and max imize their agreement on unlabeled data. Each training iteration of posterior regularization consists of two steps. In the first step, the kernelbased network is updated by projecting the GNNbased network into a regularized subspace, yielding a stronger kernel based network. In the second, we distill the knowledge learned by the kernelbased network into the GNNbased network, so that allowing the GNN to better explore graph similarity and overcome the challenge caused by the scarcity of labeled data. To summarize, the main contributions of this work are as follows: ‚Ä¢We propose a novel approach for semisupervised graph classification, which consists of a GNNbased network and a kernelbased network to fully capture the graph similarity and overcome the scarcity of labeled data. ‚Ä¢We develop a novel posterior regularization framework to combine the advantages of graph neural networks and graph kernels in a principled way, such that they can mutually enhance each other via optimizing the two modules with an EMstyle algorithm. ‚Ä¢We conduct extensive experiments on a range of wellknown benchmarks to demonstrate the effectiveness of our KGNN. 2 RELATED WORK "
369,Guided Labeling using Convolutional Neural Networks.txt,"Over the last couple of years, deep learning and especially convolutional
neural networks have become one of the work horses of computer vision. One
limiting factor for the applicability of supervised deep learning to more areas
is the need for large, manually labeled datasets. In this paper we propose an
easy to implement method we call guided labeling, which automatically
determines which samples from an unlabeled dataset should be labeled. We show
that using this procedure, the amount of samples that need to be labeled is
reduced considerably in comparison to labeling images arbitrarily.","Deep learning has gained a lot of interest over the last few years because the methods perform very well on a wide range of machine learning tasks. One class of especially successful deep learning methods are convolutional neural networks (CNNs) for image classiÔ¨Åcation. Unfortunately, CNNs need a large amount of labeled training data to perform well. In many cases, this label ing is performed by humans. A common approach is to use some form of crowd based labeling. For example, Amazon Mechanical Turk [20] was used for labeling the ImageNet dataset [3]. Data can also be obtained as a side effect of some human interaction with an online system. For exam ple, CAPTCHA [21] challenges to prevent bots using online services can be set up to produce labeled data as a side effect of the veriÔ¨Åcation procedure [5]. Alas, simply labeling all available samples is a very in efÔ¨Åcient use of human labor, since not all samples will be of equal value. On the one hand, adding a sample which is similar to samples already in the dataset will not be very usefull. On the other hand, in most cases, not all classes will have the same difÔ¨Åculty and it might make sense to add more samples of the difÔ¨Åcult classes to the dataset. There fore, it would be advantageous to label a sample whichwould maximize the classiÔ¨Åcation accuracy of a system. Unfortunately, how much the quality of a dataset would in crease by adding a speciÔ¨Åc labeled sample can only be de termined after labeling and training on the resulting dataset. This is obviously not useful if we want to decide which sam ples should be labeled in the Ô¨Årst place. We propose to use the classiÔ¨Åcation conÔ¨Ådence of a CNN while trying to predict the class of unlabeled images to de cide what to label next. The proposed procedure, together with extensive data augmentation, will be evaluated for two small neural networks on the MNIST [10] and CIFAR10 [9] dataset. In practice we propose the following workÔ¨Çow: A small, labeled dataset is used to train a neural network that is used to select a batch of the most confusing images from a set of unlabeled data. This batch is given to human workers who label the images, after which they are added to the training dataset and the process repeats. We call this pro cedure guided labeling . Our hypothesis is that, using this procedure, we are able to trade human for computational resources. 2. Related Work "
8,Learning from Noisy Labels via Dynamic Loss Thresholding.txt,"Numerous researches have proved that deep neural networks (DNNs) can fit
everything in the end even given data with noisy labels, and result in poor
generalization performance. However, recent studies suggest that DNNs tend to
gradually memorize the data, moving from correct data to mislabeled data.
Inspired by this finding, we propose a novel method named Dynamic Loss
Thresholding (DLT). During the training process, DLT records the loss value of
each sample and calculates dynamic loss thresholds. Specifically, DLT compares
the loss value of each sample with the current loss threshold. Samples with
smaller losses can be considered as clean samples with higher probability and
vice versa. Then, DLT discards the potentially corrupted labels and further
leverages supervised learning techniques. Experiments on CIFAR-10/100 and
Clothing1M demonstrate substantial improvements over recent state-of-the-art
methods.
  In addition, we investigate two real-world problems for the first time.
Firstly, we propose a novel approach to estimate the noise rates of datasets
based on the loss difference between the early and late training stages of
DNNs. Secondly, we explore the effect of hard samples (which are difficult to
be distinguished) on the process of learning from noisy labels.","Although deep neural networks (DNNs) have achieved great success for image classiÔ¨Åcation tasks [10, 16], their excellent performance mainly relies on largescale datasets with clean label annotations. However, it is extremely ex pensive and timeconsuming to label highquality datasets, *Equal Contribution. This work was done at Huawei Technologies. ‚Ä†Correspondence to: MinLing Zhang (zhangml@seu.edu.cn) and Xin Geng (xgeng@seu.edu.cn). 50 100 150 200 250 300 Epoch0.00.51.01.52.02.53.0LossClean Noisy(a) 50 100 150 200 250 300 Epoch0.02.55.07.510.012.515.017.520.022.5LossClean Noisy (b) Figure 1: Crossentropy loss on CIFAR10 under 50% sym metric label noise. (a) Training with crossentropy loss re sults in Ô¨Åtting the noisy labels. (b) Using DLT avoids Ô¨Åtting label noise. The bold lines represent the means of losses and the shaded areas are the ranges of all samples. thus deep models are usually trained on data with lots of corrupted labels. As a result, dealing with label noise is a common adverse scenario which requires attention and has been extensively studied these years [1, 9, 13, 19]. A recent study on the generalization capabilities of deep networks [39] demonstrates that DNNs can easily overÔ¨Åt to noisy labels and result in poor generalization performance. However, even though deep networks can Ô¨Åt everything in the end, they learn patterns Ô¨Årst [3], and this suggests that DNNs gradually memorize the data, moving from correct data to mislabeled data. As shown in Figure 1(a), DNNs Ô¨Åt the correctly labeled samples (clean samples) before Ô¨Åtting noisy samples, resulting in notably larger loss values for noisy samples in the early training stage. Existing methods for learning from noisy labels can be grouped into three main categories. The Ô¨Årst one is based on label correction which aims to correct noisy labels to the groundtruth ones [20, 32, 33]. For example, the recent proposed PENCIL [37] utilizes backpropagation to correct image labels and update the network parameters simultane ously in an endtoend manner. The second one is based on the robust loss function [21, 35, 41]. Ghoshet al. [6] proves that the loss functions which satisfy the symmetricarXiv:2104.02570v1  [cs.LG]  1 Apr 2021condition, such as Mean Absolute Error (MAE), would be inherently tolerant to both uniform and class conditional la bel noise. The third one is based on sample selection which involves selecting correctly labeled samples from a noisy training dataset [1, 11, 19]. Coteaching [9, 38] is a repre sentative framework on this line which trains two networks where each network selects smallloss samples to teach an other one. In light of these recent advances, we propose dynamic loss thresholding (DLT) to avoid Ô¨Åtting noisy labels when training deep models, as shown in Figure 1(b). SpeciÔ¨Åcally, DLT records the loss value of each sample during training and calculates loss thresholds dynamically. By comparing the loss value of each training sample with the current loss threshold, we expect to distinguish the potentially clean and noisy samples during training. Samples with smaller losses can be considered as correctly labeled samples and vice versa. Then, DLT discards these potentially corrupted la bels and treats the corresponding samples as unlabeled data, thus we can leverage semisupervised learning techniques likeMixup [40] to improve the performance. Experiments on various benchmarks demonstrate substantial improve ments over recent stateoftheart methods. Furthermore, DLT is also of excellent generalization and Ô¨Çexibility and we empirically demonstrate that our noisy label detection method is still effective when combining with other meth ods. In addition, the noise rates of datasets are always un known but crucial to many realworld situations due to the fact that numbers of existing methods need the noise rate as their prior knowledge. In this paper, we further propose a novel method to estimate the noise rate based on the pat terns of losses. In particular, our method calculates the loss difference between the early and late deep network train ing stages and dynamically Ô¨Åts a Gaussian Mixture Model (GMM) on persample loss difference to divide the training samples into a clean set and a noisy set. Accordingly, noise rate can be obtained by calculating the proportion of clean samples to total samples. In realworld situations, there exists a common class of samples which are always quite close to the decision bound ary and hence difÔ¨Åcult to be distinguished by DNNs. Intu itively, we call these samples hard samples . Hard samples can be deÔ¨Åned as a subset of clean samples but easy to be mistaken for other classes. However, nearly no research dis cussed hard samples in the Ô¨Åeld of learning from noisy la bels to our knowledge. By means of our proposed method, we investigate the effect of hard samples during deep net work training for the Ô¨Årst time. In summary, our main con tributions are as follows: ‚Ä¢ We propose a novel noisy label detection method DLT, which is based on dynamic loss thresholds . Combined with semisupervised learning techniques, our wholeframework achieves stateoftheart performance. We experimentally show that DLT is of excellent general ization and Ô¨Çexibility. ‚Ä¢ We provide a method to estimate the noise rates of datasets based on loss difference . This is a key com plement to eliminate the dependence on using noise rate as common prior knowledge. ‚Ä¢ We provide insights into the effect of hard samples on the training of DNNs. We do this for not only clari fying the effectiveness of our method, but also Ô¨Ånding out the patterns of hard samples when learning from noisy labels. 2. Related Work "
548,Exemplar Normalization for Learning Deep Representation.txt,"Normalization techniques are important in different advanced neural networks
and different tasks. This work investigates a novel dynamic
learning-to-normalize (L2N) problem by proposing Exemplar Normalization (EN),
which is able to learn different normalization methods for different
convolutional layers and image samples of a deep network. EN significantly
improves flexibility of the recently proposed switchable normalization (SN),
which solves a static L2N problem by linearly combining several normalizers in
each normalization layer (the combination is the same for all samples). Instead
of directly employing a multi-layer perceptron (MLP) to learn data-dependent
parameters as conditional batch normalization (cBN) did, the internal
architecture of EN is carefully designed to stabilize its optimization, leading
to many appealing benefits. (1) EN enables different convolutional layers,
image samples, categories, benchmarks, and tasks to use different normalization
methods, shedding light on analyzing them in a holistic view. (2) EN is
effective for various network architectures and tasks. (3) It could replace any
normalization layers in a deep network and still produce stable model training.
Extensive experiments demonstrate the effectiveness of EN in a wide spectrum of
tasks including image recognition, noisy label learning, and semantic
segmentation. For example, by replacing BN in the ordinary ResNet50,
improvement produced by EN is 300% more than that of SN on both ImageNet and
the noisy WebVision dataset.","Normalization techniques are one of the most essential components to improve performance and accelerate train ing of convolutional neural networks (CNNs). Recently, a family of normalization methods is proposed includ ing batch normalization (BN) [14], instance normalization Equal contribution (a) The learning dynamic of EN ratios of four categories in three layers. (b) Performance of EN and its counterparts on various CV tasks. Figure 1. (a) The proposed Exemplar Normalization (EN) enables different categories to learn to select different normalizers in dif ferent layers. The four categories of ImageNet ( i.e. Ô¨Çute, bald ea gle, newfoundland and castle) in three layers ( i.e. bottom, middle and top) of ResNet50 are presented. (b) EN outperforms its coun terparts on various computer vision tasks ( i.e. image classiÔ¨Åcation, noisysupervised classiÔ¨Åcation and semantic image segmentation ) by using different network architectures. Zoom in three times for the best view. (IN) [37], layer normalization (LN) [1] and group normal ization (GN) [40]. As these methods were designed for dif ferent tasks, they often normalize feature maps of CNNs from different dimensions. To combine advantages of the above methods, switch able normalization (SN) [23] and its variant [34] were pro posed to learn linear combination of normalizers for each convolutional layer in an endtoend manner. We term this normalization setting as static ‚Äòlearningtonormalize‚Äô. De spite the successes of these methods, once a CNN is op timized by using them, it employed the same combinationarXiv:2003.08761v2  [cs.CV]  20 Mar 2020ratios of the normalization methods for all image samples in a dataset, incapable to adapt to different instances and thus rendering suboptimal performance. As shown in Fig. 1, this work studies a new learn ing problem, that is, dynamic ‚Äòlearningtonormalize‚Äô, by proposing Exemplar Normalization (EN), which is able to learn arbitrary normalizer for different convolutional layers, image samples, categories, datasets, and tasks in an end toend way. Unlike previous conditional batch normaliza tion (cBN) that used multilayer perceptron (MLP) to learn datadependent parameters in a normalization layer, suffer ing from overÔ¨Åtting easily, the internal architecture of EN is carefully designed to learn datadependent normalization with merely a few parameters, thus stabilizing training and improving generalization capacity of CNNs. EN has several appealing beneÔ¨Åts. (1) It can be treated as an explanation tool for CNNs. The exemplarbased important ratios in each EN layer provide information to analyze the properties of different samples, classes, and datasets in various tasks. As shown in Fig. 1(a), by train ing ResNet50 [9] on ImageNet [6], images from different categories would select different normalizers in the same EN layer, leading to superior performance compared to the ordinary network. (2) EN makes versatile design of the normalization layer possible, as EN is suitable for vari ous benchmarks and tasks. Compared with stateofthe art counterparts in Fig. 1(b), EN consistently outperforms them on many benchmarks such as ImageNet [6] for im age classiÔ¨Åcation, Webvision [18] for noisy label learning, ADE20K [43] and Cityscapes [5] for semantic segmenta tion. (3) EN is a plug and play module. It can be in serted into various CNN architectures such as ResNet [9], Inception v2 [36], and ShufÔ¨ÇeNet v2 [26], to replace any normalization layer therein and boost their performance. The contributions of this work are threefold. (1) We present a novel normalization learning setting named dynamic ‚Äòlearningtonormalize‚Äô, by proposing Exemplar Normalization (EN), which learns to select different nor malizers in different normalization layers for different im age samples. EN is able to normalize image sample in both training and testing stage. (2) EN provides a Ô¨Çexible way to analyze the selected normalizers in different layers, the re lationship among distinct samples and their deep represen tations. (3) As a new building block, we apply EN to vari ous tasks and network architectures. Extensive experiments show that EN outperforms its counterparts in wide spectrum of benchmarks and tasks. For example, by replacing BN in the ordinary ResNet50 [9], improvement produced by EN is300% more than that of SN on both ImageNet [6] and the noisy WebVision [18] dataset.2. Related Work "
412,Aesthetic Image Captioning From Weakly-Labelled Photographs.txt,"Aesthetic image captioning (AIC) refers to the multi-modal task of generating
critical textual feedbacks for photographs. While in natural image captioning
(NIC), deep models are trained in an end-to-end manner using large curated
datasets such as MS-COCO, no such large-scale, clean dataset exists for AIC.
Towards this goal, we propose an automatic cleaning strategy to create a
benchmarking AIC dataset, by exploiting the images and noisy comments easily
available from photography websites. We propose a probabilistic
caption-filtering method for cleaning the noisy web-data, and compile a
large-scale, clean dataset ""AVA-Captions"", (230, 000 images with 5 captions per
image). Additionally, by exploiting the latent associations between aesthetic
attributes, we propose a strategy for training the convolutional neural network
(CNN) based visual feature extractor, the first component of the AIC framework.
The strategy is weakly supervised and can be effectively used to learn rich
aesthetic representations, without requiring expensive ground-truth
annotations. We finally show-case a thorough analysis of the proposed
contributions using automatic metrics and subjective evaluations.","Availability of large curated datasets such as MS COCO [41] ( 100Kimages), Flickr30K [64] ( 30Kimages) or Conceptual Captions [74] ( 3Mimages) made it possi ble to train deep learning models for complex, multimodal tasks such as natural image captioning (NIC) [81] where the goal is to factually describe the image content. Similarly, several other captioning variants such as visual question answering [5], visual storytelling [38], stylized captioning [56] have also been explored. Recently, the PCCD dataset (4200 images) [11] opened up a new area of research of describing images aesthetically. Aesthetic image captioning (AIC) has potential applications in the creative industries such as developing smarter cameras or webbased applica tions, ranking, retrieval of images and videos etc. How ever in [11], only six wellknown photographic/aesthetic attributes such as composition, color, lighting, etc. havebeen used to generate aesthetic captions with a small cu rated dataset. Hence, curating a largescale dataset to facil itate a more comprehensive and generalized understanding of aesthetic attributes remains an open problem. Largescale datasets have always been pivotal for re search advancements in various Ô¨Åelds [15, 41, 64, 67]. However, manually curating such a dataset for AIC is not only time consuming, but also difÔ¨Åcult due to its subjective nature. Moreover, a lack of unanimously agreed ‚Äòstandard‚Äô aesthetic attributes makes this problem even more challeng ing as compared to its NIC counterpart, where deep models are trained with known attributes/labels [41]. In this pa per, we make two contributions. Firstly, we propose an au tomatic cleaning strategy to generate a large scale dataset by utilizing the noisy comments or aesthetic feedback pro vided by users for images on the web. Secondly, for a CNN based visual feature extractor as is typical in NIC pipelines, we propose a weaklysupervised training strategy. By auto matically discovering certain ‚Äòmeaningful and complex aes thetic concepts‚Äô, beyond the classical concepts such as com position, color, lighting, etc., our strategy can be adopted in scenarios where Ô¨Ånding clean groundtruth annotations is difÔ¨Åcult (as in the case of many commercial applications). We elaborate these contributions in the rest of this section. To generate a clean aesthetic captioning dataset, we col lected the raw user comments from the Aesthetic Visual Analysis (A V A) dataset [58]. A V A is a widely used dataset for aesthetic image analysis tasks such as aesthetic rat ing prediction [44, 48], photographic style classiÔ¨Åcation [25, 34]. However, A V A was not created for AIC. In this pa per, we refer to the original A V A with raw user comments as A V A rawcaption. It contains 250;000photographs from dpchallenge.com and the corresponding user comments or feedback for each photograph ( 3billion in total). Typ ically, in Dpchallenge, users ranging from casual hobby ists to expert photographers provide feedback to the images submitted and describe the factors that make a photograph aesthetically pleasing or dull. Even though these captions contain crucial aestheticbased information from images, they cannot be directly used for the task of AIC. Unlike the well instructed and curated datasets [41], A V A rawcaptions are unconstrained usercomments in the wild with typos,arXiv:1908.11310v1  [cs.CV]  29 Aug 2019Training Strategy (a)Noisy Data & Su pervised CNN (NS)i like the angle and the compo sitioni like the colors and the compo sitioni like the composition and the lightingi like the composition and the bw (b)Clean Data & Su pervised CNN (CS)i like the idea , but i think it would have been better if the door was in focus .i like the colors and the water . the water is a little distracting .i like the way the light hits the face and the background .i like this shot . i like the way the lines lead the eye into the photo . (c) Clean Data & Weakly Supervised CNN (CWS)i like the composition , but i think it would have been better if you could have gotten a little more of the buildingi like the composition and the colors . the water is a little too bright .this is a great shot . i love the way the light is coming from the left .i like the composition and the bw conversion . Figure 1. Aesthetic image captions. We show candidates generated by three different frameworks discussed in this paper: (a)For NS, we use an ImageNet trained CNN and LSTM trained on noisy comments (b)For CS, we use an ImageNet trained CNN and LSTM trained on compiled A V ACaptions dataset (c)For CWS, we use a weaklysupervised CNN and LSTM trained on A V ACaptions grammatically inconsistent statements, and also containing a large number of comments occurring frequently without useful information. Previous work in AIC [11] acknowl edges the difÔ¨Åculty of dealing with the highly noisy captions available in A V A. In this work, we propose to clean the raw captions from A V A by proposing a probabilistic ngram based Ô¨Åltering strategy. Based on wordcomposition and frequency of occurrence of ngrams, we propose to assign an informa tiveness score to each comment, where comments with a little or vague information are discarded. Our resulting clean dataset, A V ACaptions contains230;000images and1:5Mcaptions with an average of 5comments per image and can be used to train the Long and Short Term Memory (LSTM) network in the image captioning pipeline in the traditional way. Our subjective study veri Ô¨Åes that the proposed automatic strategy is consistent with human judgement regarding the informativeness of a cap tion. Our quantitative experiments and subjective studies also suggest that models trained on A V ACaptions are more diverse and accurate than those trained on the original noisy A V AComments. It is important to note that our strategy to choose the largescale A V A rawcaption is motivated from the widely used image analysis benchmarking dataset, MS COCO, which is now used as an uniÔ¨Åed benchmark for di verse tasks such as object detection, segmentation, caption ing, etc. We hope that our cleaned dataset will serve as a new benchmarking dataset for various creative studies and aestheticsbased applications such as aesthetics based im age enhancement, smarter photography cameras, etc. Our second contribution in this work is a weakly super vised approach for training a CNN, as an alternative to the standard practice. The standard approach for most image captioning pipelines is to train a CNN on large annotateddatasets e.g. ImageNet [15], where rich and discriminative visual features are extracted corresponding to the physical properties of objects such as cars, dogs etc. These features are provided as input to an LSTM for generating captions. Although trained for classiÔ¨Åcation, these ImageNetbased features have been shown to translate well to other tasks such as segmentation [42], styletransfer [22], NIC. In fact, due to the unavailability of largescale, taskspeciÔ¨Åc CNN annotations, these ImageNet features have been used for other variants of NIC such as aesthetic captioning [11], styl ized captioning [56], product descriptions [82], etc. However, for many commercial/practical applications, availability of such datasets or models is unclear due to copyright restrictions [24, 37, 83]. On the other hand, col lecting taskspeciÔ¨Åc manual annotations for a CNN is ex pensive and time intensive. Thus the question remains open if we can achieve better or at least comparable performance by utilizing easily available weak annotations from the web (as found in A V A) and use them for training the visual fea ture extractor in AIC. To this end, motivated from weakly supervised learning methods [18, 69], we propose a strategy which exploits the large pool of unstructured rawcomments from A V A and discovers latent structures corresponding to meaningful photographic concepts using Latent Dirichlet Allocation (LDA) [10]. We experimentally observe that the weaklysupervised approach is effective and its perfor mance is comparable to the standard ImageNet trained su pervised features. In essence, our contributions are as follows: 1. We propose a caption Ô¨Åltering strategy and compile A V ACaptions, a largescale and clean dataset for aes thetic image captioning (Sec 3). 2. We propose a weaklysupervised approach for trainingthe CNN of a standard CNNLSTM framework (Sec 4) 3. We showcase the analysis of the AIC pipeline based on the standard automated metrics (such as BLEU, CIDEr, SPICE etc. [2, 62, 78]), diversity of captions and subjective evaluations which are publicly available for further explorations (Section 6). 2. Related Work "
301,Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels.txt,"Graph Neural Networks (GNNs) have shown their great ability in modeling graph
structured data. However, real-world graphs usually contain structure noises
and have limited labeled nodes. The performance of GNNs would drop
significantly when trained on such graphs, which hinders the adoption of GNNs
on many applications. Thus, it is important to develop noise-resistant GNNs
with limited labeled nodes. However, the work on this is rather limited.
Therefore, we study a novel problem of developing robust GNNs on noisy graphs
with limited labeled nodes. Our analysis shows that both the noisy edges and
limited labeled nodes could harm the message-passing mechanism of GNNs. To
mitigate these issues, we propose a novel framework which adopts the noisy
edges as supervision to learn a denoised and dense graph, which can down-weight
or eliminate noisy edges and facilitate message passing of GNNs to alleviate
the issue of limited labeled nodes. The generated edges are further used to
regularize the predictions of unlabeled nodes with label smoothness to better
train GNNs. Experimental results on real-world datasets demonstrate the
robustness of the proposed framework on noisy graphs with limited labeled
nodes.","Graph Neural Networks (GNNs) [ 15,22] have made remarkable achievements in modeling graphs from various domains such as social networks [ 15], financial system [ 35], and recommendation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM ‚Äô22, February 21‚Äì25, 2022, Tempe, AZ, USA ¬©2022 Association for Computing Machinery. ACM ISBN 9781450391320/22/02. . . $15.00 https://doi.org/10.1145/3488560.3498408 + Adversarial edge Deleted edge Added edge Noisy edge + L abeled nodes  2  Hop neighbors ? ? D istant nodes ? + ? ? ? ? ? ? ? ? + ? ? ? ?   + W rong predictions   Noisy Graph Learned  Graph  Down  weighted edgeFigure 1: An illustration of downweighting/removing noise edges and densifying the graph for better performance. system [ 36]. The success of GNNs relies on the messagepassing mechanism [ 15,22], where node representations are updated by aggregating the information from neighbors. With this mechanism, the node representations capture node features, information of neighbors and local graph structure, which facilitate various tasks, especially semisupervised node classification. Although GNNs have shown great ability in modeling graphs, their performance can degrade significantly when trained on graphs with noisy edges and/or limited labeled nodes .First, due to the mes sage passing, GNNs are vulnerable to adversarial or noisy edges. For example, as shown in Fig. 1, poisoning attacks [ 46] add/delete care fully chosen edges to the graph. These adversarial edges (shown in red) usually connect nodes of different labels or features, thus con taminating the neighborhoods of nodes, propagating noises/errors to node representations. In addition, inherent edge noises also exist in realworld graphs. For instance, in social networks, bots tend to build connections with normal users to spread misinformation [ 11], which can also harm the performance of GNNs for bot detection. Second , for many applications, graphs are often sparsely labeled such as cell phone network for fraud detection [ 13]. Label sparsity can severely reduce the involvement of unlabeled nodes during mes sage passing, leading to poor performance. Generally, in a ùêælayer GNN, a labeled node aggregates its ùêæhop neighborhood informa tion, thus making many unlabeled nodes in ùêæhop neighborhood participate in the training, which is one major reason that GNNs can leverage unlabeled nodes for semisupervised node classifica tion. However, as verified in our preliminary analysis in Fig. 2a of Sec. 3.3, when the number of labeled nodes decreases, the amount of unlabeled nodes participating in training drops quickly, making message passing less effective. These shortcomings of GNNs hinder the adoption of GNNs for many realworld applications. Thus, it is important to develop robust GNNs that can simultaneously handle noisy graphs with sparse labels.arXiv:2201.00232v1  [cs.LG]  1 Jan 2022However, developing robust GNNs for graphs with noisy edges and limited labeled nodes is challenging. First, the training graph itself is noisy, i.e., noisy edges are mixed with the normal edges. Thus, we need supervision in downweighting or eliminating noisy edges. Second , alleviating the limited label issue requires more la bels, while obtaining more labeled nodes is timeconsuming and expensive. Hence, we need alternative approaches to more effec tively utilize the limited labels. Some initial efforts [ 20,20,33,38] have been taken to alleviate the effects of the adversarial edges such as pruning edges by using node similarity [ 38], and adopting Gaussian distribution as node representations to absorb noises [ 43]. To address the problem of sparsely labeled graphs, some meth ods [ 24,30,32] propose to obtain better representations by training GNNs with selfsupervised learning tasks such as pseudo label pre diction [ 24,32] and global context predictions [ 30]. However, little efforts are taken for robust GNNs that can simultaneously handle noisy edges and label sparsity. Since both the noisy edges and limited labeled nodes harm the message passing of GNNs and message passing is directly related to the graph structure, we argue that learning a denoised and dense graph guided by the raw attributed graph is promising to facilitate message passing for robust GNNs. First, for many graphs such as social networks, nodes with similar features and labels tend to be linked [ 26], while noisy edges would link nodes of dissimilar fea tures [ 38]. Thus, we can use node attributes to predict the links. For existing links, the link predictor will assign small weights to links connecting nodes of dissimilar features while large weights to links connecting nodes of similar features, thus alleviating negative issue of noisy edges during message passing. Second , realworld graphs are usually very sparse, containing many missing edges. With the link predictor, nodes that are potentially to be linked could be iden tified. Densifying the graph by linking similar nodes would induce more unlabeled nodes to become neighbors of labeled nodes with the same labels as shown in Fig. 1, which can alleviate the label sparsity issue. In addition, since adjacent nodes tend to have the same labels, the predicted new links can be used to further regu larize the label predictions of unlabeled nodes. Though promising, the work on downweighting noisy edges and densifying graph for robust GNN on noisy graphs with sparse labels are rather limited. Therefore, in this paper, we investigate a novel problem of de veloping robust noiseresistant GNNs with limited labeled nodes by learning a denoised and densified graph. In essence, we need to solve two challenges: (i) how to effectively learn a link predictor from the noisy graph which can eliminate noisy edges and densify the graph; and (ii) how to simultaneously use the learned graph to learn a structural noiseresistant GNNs with limited labeled nodes. To address these challenges, we propose a novel framework named robust structural noiseresistant GNN (RSGNN)1. RSGNN adopts the node attributes and supervision from the noisy edges to denoise and dense graph, which can alleviate the negative effects of noisy edges and facilitate the message passing between unlabeled nodes and labeled nodes. The learned graph is used as input for learning a GNN. RSGNN also adopts the predicted edges to further explicitly regularize the predictions of unlabeled nodes to alleviate the label sparsity issue. In summary, our main contributions are: 1Codes are available at: https://github.com/EnyanDai/RSGNN‚Ä¢We study a new problem of learning robust noiseresistant GNNs with limited labeled nodes; ‚Ä¢We propose a novel framework RSGNN, which can simultane ously learn a denoised and densified graph and a robust GNN on noisy graphs with limited labeled nodes; and ‚Ä¢We conduct extensive experiments on realworld datasets to demonstrate the robustness of RSGNN on both noisy/clean graphs with limited labeled nodes. 2 RELATED WORK "
558,Dimensionality-Driven Learning with Noisy Labels.txt,"Datasets with significant proportions of noisy (incorrect) class labels
present challenges for training accurate Deep Neural Networks (DNNs). We
propose a new perspective for understanding DNN generalization for such
datasets, by investigating the dimensionality of the deep representation
subspace of training samples. We show that from a dimensionality perspective,
DNNs exhibit quite distinctive learning styles when trained with clean labels
versus when trained with a proportion of noisy labels. Based on this finding,
we develop a new dimensionality-driven learning strategy, which monitors the
dimensionality of subspaces during training and adapts the loss function
accordingly. We empirically demonstrate that our approach is highly tolerant to
significant proportions of noisy labels, and can effectively learn
low-dimensional local subspaces that capture the data distribution.","Deep Neural Networks (DNNs) have demonstrated excellent performance in solving many complex problems, and have been widely employed for tasks such as speech recognition (Hinton et al., 2012), computer vision (He et al., 2016) and gaming agents (Silver et al., 2016). DNNs are capable of learning very complex functions, and can generalize well even for a huge number of parameters (Neyshabur et al., 2014). However, recent studies have shown that DNNs may generalize poorly for datasets which contain a high propor tion noisy (incorrect) class labels (Zhang et al., 2017). It is important to gain a fuller understanding of this phenomenon, with a view to development of new training methods that can *Equal contribution1The University of Melbourne, Mel bourne, Australia2Tsinghua University, Beijing, China3National Institute of Informatics, Tokyo, Japan. Correspondence to: Yisen Wang <wangys14@mails.tsinghua.edu.cn >, Xingjun Ma <xingjun.ma@unimelb.edu.au >. Proceedings of the 35thInternational Conference on Machine Learning , Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).achieve good generalization performance in the presence of variable amounts of label noise. One simple approach for noisy labels is to ask a domain expert to relabel or remove suspect samples in a preprocess ing stage. However, this is infeasible for large datasets and also runs the risk of removing crucial samples. An alterna tive is to correct noisy labels to their true labels via a clean label inference step (Vahdat, 2017; Veit et al., 2017; Jiang et al., 2017; Li et al., 2017). Such methods often assume the availability of a supplementary labelled dataset contain ing preidentiÔ¨Åed noisy labels which are used to develop a model of the label noise. However, their effectiveness is tied to the assumption that the data follow the noise model. A different approach to tackle noisy labels is to utilize cor rection methods such as loss correction (Patrini et al., 2017; Ghosh et al., 2017), label correction (Reed et al., 2014), or additional linear correction layers (Sukhbaatar & Fergus, 2014; Goldberger & BenReuven, 2017). In this paper, we Ô¨Årst investigate the dimensionality of the deep representation subspaces learned by a DNN and pro vide a dimensionalitydriven explanation of DNN general ization behavior in the presence of (class) label noise. Our analysis employs a dimensionality measure called Local In trinsic Dimensionality (LID) (Houle, 2013; 2017a), applied to the deep representation subspaces of training examples. We show that DNNs follow twostage of learning in this scenario: 1) an early stage of dimensionality compression , that models lowdimensional subspaces that closely match the underlying data distribution, and 2) a later stage of di mensionality expansion , that steadily increases subspace dimensionality in order to overÔ¨Åt noisy labels. This second stage appears to be a key factor behind the poor general ization performance of DNNs for noisy labels. Based on this Ô¨Ånding, we propose a new training strategy, termed DimensionalityDriven Learning , that avoids the dimen sionality expansion stage of learning by adapting the loss function. Our main contributions are: We show that from a dimensionality perspective, DNNs exhibit distinctive learning styles with clean labels ver sus noisy labels. We show that the local intrinsic dimensionality canarXiv:1806.02612v2  [cs.CV]  31 Jul 2018DimensionalityDriven Learning with Noisy Labels be used to identify the stage shift from dimensionality compression to dimensionality expansion. We propose a DimensionalityDriven Learning strategy (D2L) that modiÔ¨Åes the loss function once the turning point between the two stages of dimensionality com pression and expansion is recognized, in an effort to prevent overÔ¨Åtting. We empirically demonstrate on MNIST, SVHN, CIFAR10 and CIFAR100 datasets that our DimensionalityDriven Learning strategy can ef fectively learn (1) lowdimensional representation subspaces that capture the underlying data distribution, (2) simpler hypotheses, and (3) highquality deep representations. 2. Related Work "
56,Verifying Inverse Model Neural Networks.txt,"Inverse problems exist in a wide variety of physical domains from aerospace
engineering to medical imaging. The goal is to infer the underlying state from
a set of observations. When the forward model that produced the observations is
nonlinear and stochastic, solving the inverse problem is very challenging.
Neural networks are an appealing solution for solving inverse problems as they
can be trained from noisy data and once trained are computationally efficient
to run. However, inverse model neural networks do not have guarantees of
correctness built-in, which makes them unreliable for use in safety and
accuracy-critical contexts. In this work we introduce a method for verifying
the correctness of inverse model neural networks. Our approach is to
overapproximate a nonlinear, stochastic forward model with piecewise linear
constraints and encode both the overapproximate forward model and the neural
network inverse model as a mixed-integer program. We demonstrate this
verification procedure on a real-world airplane fuel gauge case study. The
ability to verify and consequently trust inverse model neural networks allows
their use in a wide variety of contexts, from aerospace to medicine.","Neural networks have been used to solve a variety of nonlinea r inverse problems such as state estimation [20], inverse computational mechanics [2 4], inversemodel controller design [15], medical imaging [12], and seismic reÔ¨Çectivity estimation [16]. Neural networks are used because they can learn complex functions f rom data and compute outputs quickly, whereas an analytical inverse modeling ap proach may not work for nonlinear systems, and a samplingbased approach may be com putationally expensive. However, neural network inverse models lack accuracy guara ntees, limiting their use in safety critical applications such as transportation. To enable the adoption of these models, we present a method for verifying the accuracy of an i nverse model neural net work. Our approach uses recent advancements in neural netwo rk veriÔ¨Åcation to provide formal guarantees of the correctness of the model over its en tire domain. Inverse problems arise when trying to estimate an unobserve d state from a set of observations, where only the forward model is known (i.e., t he model that maps states to observations). This is in contrast to a Ô¨Åltering setting w here a dynamics model as well as an observation model is often available. Analytical solu tions to inverse problems may not exist for nonlinear systems, and may become intractable when the forward model2 C. Sidrane et al. is highdimensional or stochastic. Samplingbased approx imate inverse models may be slow to query and may not converge to a useable point estimate at all. As a result, data driven approaches may be applied. The forward model is used t o produce a dataset of stateobservation pairs that are then used to train a neural network to output a state for a given observation. Once a neural network inverse model is trained, we want guara ntees that the inverse mapping between observations and states has low error, but t his may be challenging due to the complexity of the forward model and neural network . Prior work [28] has approached this problem using sampling to get stochastic bo unds on the error of an inverse model network. Formal approaches [27] have also bee n applied to 1layer net works where the forward model is replaced with a lookup table so that the Lipschitz constants of the simple network and forward model may be easi ly calculated and the accuracy evaluated at grid points in the domain. In addition to these simplifying assump tions, this approach scales exponentially with the number o f dimensions in the domain, making it unsuitable for largescale problems. We verify ReLUbased inverse model neural networks using mi xedinteger linear programming, a technique that has seen signiÔ¨Åcant recent us age for neural network ver iÔ¨Åcation [1, 5, 7, 8, 19, 25]. This approach allows formal ver iÔ¨Åcation over the entire input domain without a dependence on gridding. Instead of re placing the complex for ward model that maps states to observations with a lookup tab le, we encode it into the mixedinteger linear program, preserving the integrity of the model. This is enabled by overapproximating the nonlinear functions in the forwar d model using a technique adapted from veriÔ¨Åcation of neural network control systems [23]. The nonlinear func tions are overapproximated using piecewise linear constra ints, which can be encoded into a mixedinteger linear program, alongside the inverse model neural network. We then maximize the error between the portion of the state that the inverse model re constructs and the original state values. This produces a ve riÔ¨Åed upper bound on the estimation error of the inverse model. Contributions Our contribution is to provide a new formal approach to verif ying in verse model neural networks by bringing ideas across discip line boundaries ‚Äì from reachability to inverse models. Compared to prior work, our work: 1. Can handle multilayer ReLU neural networks. 2. Does not require gridding the state space and incurring th e curse of dimensionality. 3. Can compare an inverse model neural network to the true non linear forward model, rather than a lookup table. 4. Can verify stochastic forward models with additive noise . 5. Employs intelligent parallelization in order to reduce v eriÔ¨Åcation time. 6. Demonstrates the approach on a realworld case study of ai rcraft fuel measurement. 2 Related Work "
527,Yelp Food Identification via Image Feature Extraction and Classification.txt,"Yelp has been one of the most popular local service search engine in US since
2004. It is powered by crowd-sourced text reviews and photo reviews. Restaurant
customers and business owners upload photo images to Yelp, including reviewing
or advertising either food, drinks, or inside and outside decorations. It is
obviously not so effective that labels for food photos rely on human editors,
which is an issue should be addressed by innovative machine learning
approaches. In this paper, we present a simple but effective approach which can
identify up to ten kinds of food via raw photos from the challenge dataset. We
use 1) image pre-processing techniques, including filtering and image
augmentation, 2) feature extraction via convolutional neural networks (CNN),
and 3) three ways of classification algorithms. Then, we illustrate the
classification accuracy by tuning parameters for augmentations, CNN, and
classification. Our experimental results show this simple but effective
approach to identify up to 10 food types from images.","Nowadays people really love taking photos, especially when they are in a fancy restaurant, in addition that smart mobile phones today are well equipped with highresolution cameras. So, it is not a surprise that you can see there would be thousands of pictures from someone‚Äôs phone album after a year. However, labeling and searching by words for these photos becomes a real hassle. For 1Wikipedia: https://en.wikipedia.org/wiki/Yelp 2Yelp Data Challenge: https://www.yelp.com/dataset/challenge Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). E511‚Äô18, December 2018, Bloomington, IN ¬©2018 Copyright held by the owner/author(s). ACM ISBN 978xxxxxxxxxx/YY/MM. https://doi.org/10.1145/nnnnnnn.nnnnnnnexample, imaging you are talking with a friend about your extraor dinary experience of eating a lobster in Bloomington IN, turning your phone upside down just want to share this picture, oops, you can not find it. This is because you forgot to label this picture and did not remember which day you took this picture. Not just for personal photo album management, this issue raises astonishing importance for companies like Yelp, which does show ing and researching business reviews. Digital photos identification poses a hard problem for those companies which rely on users‚Äô uploaded photos. Since they serve millions of customers and they may have billions of photos [ 4]. It would be almost infeasible to edit photo labels by human editors. So, it would be promising that devel oping some automatically identification solution for user uploaded pictures. More formally, in this work, we would like to build a model that can automatically classify a user uploaded food photo into a set of applicable categories, and the accuracy of prediction should be beyond average human guess. We first consider using pretrained features provided by Yelp Data Challenge and feeding them to traditional machine learning algorithms like Convolutional Neural Network, Support Vector Machine, Gradient Boosting, with cross validation. But unfortunately, the classification result was way below human eyeball. After double check, it becomes evident that the main issue lies on the features. As Yelp is a usergenerated content platform, many pictures in our training set are vague, off topic or mislabeled. Given so, the decision was to do it from scratch. We carefully select 30 pictures for each class, use argumentation methods to enlarge our train set, manually extract features, finally the classifi cation result is satisfactory. The rest of the paper is organized as follows. Section 2 discusses some related works, some of which either address similar issues or adopt similar solution methods that are related to the content of this work. Section 3 presents the overview of our solution, machine learning models, algorithms, and et al. Section 4 and 5 show the overview of the original datasets from Yelp and our experiments of showing the effectiveness of our implementation. Finally, this paper concludes the work and discusses some advantages and limitations of the work in Section 6. 2 RELATED WORK "
251,Joint Optimization Framework for Learning with Noisy Labels.txt,"Deep neural networks (DNNs) trained on large-scale datasets have exhibited
significant performance in image classification. Many large-scale datasets are
collected from websites, however they tend to contain inaccurate labels that
are termed as noisy labels. Training on such noisy labeled datasets causes
performance degradation because DNNs easily overfit to noisy labels. To
overcome this problem, we propose a joint optimization framework of learning
DNN parameters and estimating true labels. Our framework can correct labels
during training by alternating update of network parameters and labels. We
conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset.
The results indicate that our approach significantly outperforms other
state-of-the-art methods.","DNNs trained on largescale datasets have achieved impressive results on many classiÔ¨Åcation problems. Generally, accurate labels are necessary to eÔ¨Äectively train DNNs. However, many datasets are constructed by crawling images and labels from websites and often contain incorrect noisy labels ( e.g., YFCC100M [17], Clothing1M [21]). This study addresses the following question: how can we eÔ¨Äectively train DNNs on noisy labeled datasets without manually cleaning the data? The prominent issue in training DNNs on noisy la beled datasets is that DNNs can learn or memorize, any training dataset , and this implies that DNNs are subject to total overÔ¨Åtting on noisy data. To address this problem, commonly used regulariza tion techniques including dropout and early stopping are helpful. However, these methods do not guarantee optimization because they prevent the networks from reducing the training loss. Another method involves using prior knowledge, such as the confusion matrix between clean and noisy labels, which typically cannot be used in real settings. Figure 1. The concept of our joint optimization framework. Noisy labels are reassigned to the probability output by CNNs. Network parameters and labels are alternatively updated for each epoch. Consequently, we need a new framework of opti mization. In this study, we propose an optimization framework for learning on a noisy labeled dataset. We propose optimizing the labels themselves as opposed to treating the noisy labels as Ô¨Åxed. The joint opti mization of network parameters and the noisy labels corrects inaccurate labels and simultaneously improves the performance of the classiÔ¨Åer. Fig. 1 shows the con cept of our proposal. The main contributions are as follows. 1. We propose a joint optimization framework for learning on noisy labeled datasets. Our optimiza tionproblemhastwooptimizationnetworkparam eters and class labels that are optimized by an al ternating strategy. 2. We observe that a DNN trained on noisy labeled datasets does not memorize noisy labels and main tains high performance for clean data under a high learning rate. This reinforces the Ô¨Åndings of Arpit et al.[1]thatsuggestthatDNNsÔ¨Årstlearnssimple patterns and subsequently memorize noisy data. 3. We evaluate the performance on synthetic and real noisy datasets. We demonstrate stateofthe art performance on the noisy CIFAR10 dataset and a comparable performance on the Clothing1M dataset [21]. 1arXiv:1803.11364v1  [cs.CV]  30 Mar 20182. Related Works "
373,ForamViT-GAN: Exploring New Paradigms in Deep Learning for Micropaleontological Image Analysis.txt,"Micropaleontology in geosciences focuses on studying the evolution of
microfossils (e.g., foraminifera) through geological records to reconstruct
past environmental and climatic conditions. This field heavily relies on visual
recognition of microfossil features, making it suitable for computer vision
technology, specifically deep convolutional neural networks (CNNs), to automate
and optimize microfossil identification and classification. However, the
application of deep learning in micropaleontology is hindered by limited
availability of high-quality, high-resolution labeled fossil images and the
significant manual labeling effort required by experts. To address these
challenges, we propose a novel deep learning workflow combining hierarchical
vision transformers with style-based generative adversarial network algorithms
to efficiently acquire and synthetically generate realistic high-resolution
labeled datasets of micropaleontology in large volumes. Our study shows that
this workflow can generate high-resolution images with a high signal-to-noise
ratio (39.1 dB) and realistic synthetic images with a Frechet inception
distance similarity score of 14.88. Additionally, our workflow provides a large
volume of self-labeled datasets for model benchmarking and various downstream
visual tasks, including fossil classification and segmentation. For the first
time, we performed few-shot semantic segmentation of different foraminifera
chambers on both generated and synthetic images with high accuracy. This novel
meta-learning approach is only possible with the availability of
high-resolution, high-volume labeled datasets. Our deep learning-based workflow
shows promise in advancing and optimizing micropaleontological research and
other visual-dependent geological analyses.","  Throughout geological history,  foraminifera represent  one of the most exceptionally diverse  groups of marine microfossils, with an estimated number of current species between 8,966  and an estimated number of 40,888 fossil species in the geological record1.  This accounts  for approximately 2% of all animal species from the Cambrian to the present.  (Sen, 2003) .  The size of  fossil foraminifera is very diverse  ranging from  less than  100 microns to 20  centimeters and their shell can be made up of diverse compositions, such as calcite, aragonite ,  agglutinated particles , and other organic compounds.  In foraminifera , factors such as their  cosmopolitan nature and evolutionary diversification make them of particular interest to  provide a paleontological and stratigraphic record, whi ch is of significant  value to carry out                                                              1 https://www.marinespecies.org/foraminifera/  2   biostratigraphic correlations and paleoenvironmental interpretations (Jones, 2014; Sen,  2003) .     In both ancient and modern environments , the relative  abundance of specific species and their  corresponding  morphometric characteristics are used as a proxy for (paleo) temperature,  (paleo) oxygen concentration , and (paleo) oceanic salinity  and paleoproductivity . In addition,  foraminifera are often used as the building block to define biofacies for paleo bathym etric  studies, as an aid in the characterization of sedimentary subenvironments. With the  progressive  change of different macro  and microfossils throughout the history of the Earth,  especially planktonic foram inifera  being  utilized as markers on the geological time scale and  the occurrence of specific events  in the stratigraphic record   (Jones, 2014; Marchant et al.,  2020; Sen, 2003) .    Detailed  identification  of both species and morphotypes and  producing high quality  photomicrographs  of foraminifera have been primarily dependent  on the  availability of high  end equipment  such as advanced stereomicroscopes and high resolution scanning systems.  Although some techniques have become staple in researc h institutes, some high end  equipment for digitizing specimens at high resolution s not widely accessible to  the  geoscientific community . This is exacerbated by the expertise needed to perform species and  genera classification. All of this leads to an issu e with standardization across various  laboratories and institutions, which severely limits the reproducibility of such classification  and its accessibility to non experts. As a result, there is an urgent need to develop an efficient,  automated approach or workflow for  improving the resolution of microfossil images and  obtaining labeled datasets without the need for high end equipment. Furthermore,  the widespread implementation of robust deep learning models for foraminifera classification  and morphological diversity distillation using advanced computer vision technology,  particularly deep convolutional neural networks, has yet to be  fully  investigated.      In the current literature , there are numerous works that leverage  computer vision  technology  to classify and segment microfossil specimens  (Beaufort & Dollfus, 2004; Marchant et al.,  2020) . From seminal implementations in which the algorithms used did not achieve  human  accuracy (Beaufort & Dollfus, 2004; Culverhouse et al., 1996; S. Liu et al., 1994)  to recent  works in which the algorithms exceed human accuracy and speed when classifying  microfossils (Marchant et al., 2020; Pedraza et al., 2017) . Recently, the use of Deep  Convolutional Neural Networks (CNNs) has  been notable in this research corpus , with CNNs  having  several advantages for these tasks  given th e reduced need for feature engineering, the  scalability to larger datasets, and exceptional  ability to  process grid like data (Carvalho et al.,  2020; Ho et al., 2023; Johansen et al., 2021; Koeshidayatullah et al., 2020; Marchant et al.,  2020; Mitra et al., 2019) . This is further supported by the accessibility  to powerful pretrained  architectures as backbones for a starting point during the training of new models ( Ho et al.,  2023; Koeshidayatullah et al., 2020; Koeshidayatullah et al., 2022; Pires de Lima & Duarte,  2021; Shoji et al., 2018) .     In geosciences, r ecent advances  of Generative Adversarial Networks (GANs) enable   geoscientists to generate additional synthetic data as an additional augmentation technique to 3   conventional ones, assisting in the improvement  of machine learning mode l performance   (Ferreira et al., 2022; Wang & Perez, 2017) . In addition, GANs  are effec tive at balancing  the  distribution of data within a particular  geological dataset, ensuring a better representation,  and reducing the risk of bias  during training (Ferreira et al., 2022; Koeshidayatullah, 2022;  Abdellatif et al., 2022) .    In recent research trends, Vision Transformers (ViT) have emerged from the Natural  Language Processing (NLP) corpus as a powerful technique for tackling visual tasks  (Vaswani et al 2017; Dosovitskiy  et al, 2020) . Unlike traditional CNNs, ViTs employ a  transformer ar chitecture to address both global and local relationships in an image, resulting  in more effective feature extraction and representation. Image Super Resolution (SR) is a  field of Computer Vision that focuses on enhancing low resolution images, making them   more visually appealing and informative. Recent improvements in CNN and ViT based  architectures have become the state oftheart for upscaling and restoring images  (Liang et  al, 2021; Lin et al, 2022) . This application could prove useful in preserving fin egrained  details and textural information for micropaleontological image analysis.     The primary goals of this research were to create and suggest new approaches to traditional  microfossil image scaling methods, and to investigate the application of end toend deep  learning for enhancing the quality and accurately representing the morphological diversity of  foraminifera images. Ferreira et al. (2022) successfully showcased this in their study using  petrographic datasets (Fig. 1a and b). Such tools have the potential to expand the variety of  micropaleontological datasets and provide synthetic digital counterparts for confidential data.     Figure 1 : Examples of the use of deep learning architectures in petrography (a) super  resolution imaging of sandstones (Y. Liu et al., 2020)  (b) generation of thin sections (Ferreira   et al.,  2022) .      2. Methodology   "
512,Is Attention always needed? A Case Study on Language Identification from Speech.txt,"Language Identification (LID), a recommended initial step to Automatic Speech
Recognition (ASR), is used to detect a spoken language from audio specimens. In
state-of-the-art systems capable of multilingual speech processing, however,
users have to explicitly set one or more languages before using them. LID,
therefore, plays a very important role in situations where ASR based systems
cannot parse the uttered language in multilingual contexts causing failure in
speech recognition. We propose an attention based convolutional recurrent
neural network (CRNN with Attention) that works on Mel-frequency Cepstral
Coefficient (MFCC) features of audio specimens. Additionally, we reproduce some
state-of-the-art approaches, namely Convolutional Neural Network (CNN) and
Convolutional Recurrent Neural Network (CRNN), and compare them to our proposed
method. We performed extensive evaluation on thirteen different Indian
languages and our model achieves classification accuracy over 98%. Our LID
model is robust to noise and provides 91.2% accuracy in a noisy scenario. The
proposed model is easily extensible to new languages.","From the inception of research in Natural Language Processing (NLP), researchers have speciÔ¨Åcally rely on ConvolutionNeuralNetworks(CNN)asitutilizeslocalfeatureseÔ¨Äectively.EarlierRecurrentNeuralNetwork(RNN) was eÔ¨Äectively used in diÔ¨Äerent NLP domains, but recent use of Transformer has shown promising results which outperforms all previous experimental results. Attention based models are capable of capturing the contentbased global interactions. Transformer in QuestionAnswering domain, researcher (Yamada et al., 2020) were able to outperform BERT (Devlin et al., 2019), SpanBERT (Joshi et al., 2020), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2020). In Machine Translation domain researcher (Takase and Kiyono, 2021; Gu et al., 2019; Chen and HeaÔ¨Åeld, 2020) usedTransformerandwereabletooutperformotherstateoftheart(sota)algorithms.InotherdomainlikeLanguage Modelling, Text ClassiÔ¨Åcation, Topic Modelling, Emotion ClassiÔ¨Åcation, Sentiment Analysis, etc Transformer has widely used. InthisworkwefocusedonusingdiÔ¨ÄerentapproachesforSpokenLanguageIdentiÔ¨Åcation.Humansarecapableof recognizingalmostimmediatelythelanguagebeingusedbyaspeakerforvoicinganutterance.Thetaskofautomatic language identiÔ¨Åcation (LID) is to automatically classify the language used by a speaker in his/her speech. In the era of Internet of Things , smart and intelligent assistants (e.g., Alexa1, Siri2, Cortona3, Google Assistant4, etc.) can interactwithhumanswithsomedefaultlanguagesettings(mostlyinEnglish)andthesesmartassistantsrelyheavilyon Automatic Speech Recognition (ASR). However, these virtual assistants fail to provide any assistance in multilingual <Corresponding author atanumandal0491@gmail.com (A. Mandal); santanu.pal2@wipro.com (S. Pal); indranildutta.lnl@jadavpuruniversity.in (I. Dutta); languagemahib@gmail.com (M. Bhattacharya); sudipkumar.naskar@jadavpuruniversity.in (S.K. Naskar) ORCID(s):0000000293855897 (A. Mandal); 0000000330796903 (S. Pal); 0000000185225302 (I. Dutta); 0000000315884665 (S.K. Naskar) 1https://developer.amazon.com/enUS/alexa/alexavoiceservice 2https://www.apple.com/in/siri/ 3https://www.microsoft.com/enin/windows/cortana 4https://assistant.google.com/ Mandal et al.: Preprint submitted to Elsevier Page 1 of 21Is Attention always needed? contexts.Tomakesuchsmartassistantsrobust,LIDcanbeusedsothatthesmartassistantscanautomaticallyrecognize the speaker‚Äôs language and change its language setting accordingly. Our approach of identifying spoken language is limited to Indian Languages only because India is world second populated and seventh largest country in landmass and also have dynamic culture. Currently, India has 28 states and 8 Union Territories, where each states and Union Territories has its own language, but none of the language is recognised as the national language of the country. Only, English and Hindi is used as oÔ¨Écial language according to the Constitution of India Part XVII Chapter 1 Article 3435. Currently, only 22 languages have been accepted as regional languages. Sl. No. Language Family Spoken in 1 Assamese IndoAryan Assam 2 Bengali IndoAryanAssam, Jharkhand, Tripura, West Bengal 3 Bodo SinoTibetan Assam 4 Dogri IndoAryan Jammu and Kashmir 5 Gujarati IndoAryanGujrat, Dadra and Nagar Haveli and Daman and Diu 6 Hindi IndoAryanAndaman and Nicobar Islands, Bihar, Chhattisgarh, Dadra and Nagar Haveli and Daman and Diu, Delhi, Haryana, Himachal Pradesh, Jammu and Kashmir, Jharkhand, Ladakh, Madhya Pradesh, Mizoram, Rajasthan, Uttar Pradesh, Uttarakhand 7 Kannada Dravidian Karnataka 8 Kashmiri IndoAryan Jammu and Kashmir 9 Konkani IndoAryanDadra and Nagar Haveli and Daman and Diu, Goa 10 Maithili IndoAryan Jharkhand 11 Malayalam Dravidian Kerala, Lakshadweep, Puducherry 12 Marathi IndoAryanDadra and Nagar Haveli and Daman and Diu, Goa, Maharashtra 13 Meitei SinoTibetan Manipur 14 Nepali IndoAryan Sikkim, West Bengal 15 Odia IndoAryan Jharkhand, Odisha 16 Punjabi IndoAryan Delhi, Haryana, Punjab 17 Sanskrit IndoAryan Himachal Pradesh 18 Santali Austroasiatic Jharkhand 19 Sindhi IndoAryan Rajasthan 20 Tamil Dravidian Tamil Nadu 21 Telugu DravidianAndhra Pradesh, Puducherry, Telangana 22 Urdu IndoAryanBihar, Delhi, Jammu and Kashmir, Jharkhand, Telangana, Uttar Pradesh Table 1 List of oÔ¨Écial languages as per the Eighth Schedule of the Constitution of India, as of 1 December 2007 with their language family and states spoken in. Table 1 describes the 22 languages designated as OÔ¨Écial language according to the Eighth Schedule of the ConstitutionofIndia,asof1December2007.MostoftheIndianlanguagesoriginatedfromIndoAryanandDravidian language family. 5https://www.mea.gov.in/Images/pdf1/Part17.pdf Mandal et al.: Preprint submitted to Elsevier Page 2 of 21Is Attention always needed? It can be seen from the Table 1 that diÔ¨Äerent languages are spoken in diÔ¨Äerent states, however, languages do not obey the geographical boundaries. Therefore, many of these languages, particularly in the neighboring regions, have multiple dialects which are amalgamation of two or more languages. SuchenormouslinguisticdiversitymakesitdiÔ¨ÉcultforthecitizenstocommunicateindiÔ¨Äerentpartsofthecountry. Bilingualism and multilingualism are the norm in India. In this context, an LID system becomes a crucial component foranyspeechbasedsmartassistant.ThebiggestchallengeandhenceanareaofactiveinnovationforIndianlanguage is the reality that most of these languages are under resourced. Every spoken language has its underlying lexical, speaker, channel, environment, and other variations. The likely diÔ¨Äerences among various spoken languages are in their phoneme inventories, frequency of occurrence of the phonemes, acoustics, the span of the sound units in diÔ¨Äerent languages, and intonation patterns at higher levels. The overlap between the phoneme set of two or more familial languages makes it a challenge for recognition. The lowresource status of these languages makes the training of machine learning models doubly diÔ¨Écult. Every spoken languagehasitsunderlyinglexical,speaker,channel,environment,andothervariations.ThelikelydiÔ¨Äerencesamong various spoken languages are in their phoneme inventories, frequency of occurrence of the phonemes, acoustics, the spanofthesoundunitsindiÔ¨Äerentlanguages,andintonationpatternsathigherlevels.Theoverlapbetweenthephoneme setoftwoormorefamiliallanguagesmakesitachallengeforrecognition.Thelowresourcestatusoftheselanguages makes the training of machine learning models doubly diÔ¨Écult. The purpose of our approach is yet to predict the correct spoken language regardless of the abovementioned constraints. In this work we proposed Language IdentiÔ¨Åcation method for Indian Languages using diÔ¨Äerent approaches. Our LID methods cover 13 Indian languages6. Additionally our method is language agnostic. The main contributions of this work can be summarized as follows: ‚Ä¢ThemethodusesConvolutionalNeuralNetwork(CNN),ConvolutionalRecurrentNeuralNetwork(CRNN),and attention based CRNN for the task of LID. We tested 13 Indian languages achieving stateoftheart accuracy. ‚Ä¢Our model also provides stateoftheart performance in languages that belong to the same language family as well as in noisy scenarios. 2. Related Works "
164,Autonomous Learning for Face Recognition in the Wild via Ambient Wireless Cues.txt,"Facial recognition is a key enabling component for emerging Internet of
Things (IoT) services such as smart homes or responsive offices. Through the
use of deep neural networks, facial recognition has achieved excellent
performance. However, this is only possibly when trained with hundreds of
images of each user in different viewing and lighting conditions. Clearly, this
level of effort in enrolment and labelling is impossible for wide-spread
deployment and adoption. Inspired by the fact that most people carry smart
wireless devices with them, e.g. smartphones, we propose to use this wireless
identifier as a supervisory label. This allows us to curate a dataset of facial
images that are unique to a certain domain e.g. a set of people in a particular
office. This custom corpus can then be used to finetune existing pre-trained
models e.g. FaceNet. However, due to the vagaries of wireless propagation in
buildings, the supervisory labels are noisy and weak.We propose a novel
technique, AutoTune, which learns and refines the association between a face
and wireless identifier over time, by increasing the inter-cluster separation
and minimizing the intra-cluster distance. Through extensive experiments with
multiple users on two sites, we demonstrate the ability of AutoTune to design
an environment-specific, continually evolving facial recognition system with
entirely no user effort.","Facial recognition and verification are key components of smart spaces, e.g., offices and buildings for determining who is where. Knowing this information allows a building management system to tailor ambient conditions to particular users, perform automated security (e.g., opening doors for the correct users without the need for a swipe card), and customize smart services (e.g., coffee dispens ing). A vast amount of research over the past decades has gone into designing tailored systems for facial recognition and with the advent of deep learning, progress has accelerated. As an example of a stateoftheart face recognizer, FaceNet achieves extremely high accuracies (e.g., 99.5%) on very challenging datasets through the use of a low dimensional embedding, allowing similar faces to be clustered through their Euclidean distance [ 14,26]. However, when directed transferred to operate in ‚Äòin the wild‚Äô, subject to variable lighting conditions, viewing angle and appearance changes, perfor mance of offtheshelf pretrained classifiers degrades significantly, with accuracies around 15% not being uncommon. The solution to this is to obtain a large, labelled corpus of data for a particular environment, with hundreds of annotated images per user. Given access to such a hypothetical dataset, it is then possible to finetune the pretrained classifier on outdomain data to adapt to the new environment and achieve excellent performance. However, the cost of labelling and updating the corpus (e.g. to enrol new users) is prohibitive for most critical applications and therefore, will naturally limit the use and uptake of facial recog nition as a ubiquitous technology in emerging Internet of Things (IoT) applications. On the other side, people often, but not always, carry smart devices (phones, fitness devices etc). Wang et al. [ 36] advocated that although these devices do not provide finegrained enough positioning capability to act as a proxy for presence, they can be used to indicate that a user might be present in an areaarXiv:1908.09002v1  [cs.CV]  14 Aug 2019with a colocated camera. In this work, we take a step forward and further utilize device presence as weak supervision signals for the purposes of finetuning a classifier. The goal now becomes how to take an arbitrary, pretrained recognition network and tune it from a generic classifier to a highly specific classifier, optimized for a certain environment and group of people. We note that the aim is to make the network better and better at this specific goal, but it would likely perform poorly if transferred directly to a different environment. This is the antithesis of the conventional view of generalized machine learning, but is ideally suited for the problem of environment specific facial recognition, as opposed to generic facial recognition. The technical challenge is that there is not a 1:1 mapping between a face and a wireless identity, rather we need to solve the association between a set of faces and a set of identities over many sessions or occasions. To further complicate the problem, the sets are not pure i.e. the set of faces can contain additional faces from people not of interest (e.g. visitors). Equally well, due to the vagaries of wireless transmission, the set of wireless identifiers will contain additional identifiers e.g. from people in the next office. Furthermore, it is also possible to have missing observations e.g. because a person was not facing the camera or because someone left their phone at home. In this work, we present AutoTune , a system which can be used to gradually improve the performance of facial recognition sys tems in the wild, with zero user effort, tailoring them to the visual specifics of a particular smart space. We demonstrate stateofthe art performance in realworld facial recognition through a number of experiments and trials. In particular, our contributions are: ‚Ä¢We observe and prove that wireless signals of users‚Äô de vices provide valuable, albeit noisy, clues for face recogni tion. Namely, wireless signals can serve as a weak label. Such weak labels can replace the human annotated face images in the wild to save intensive effort. ‚Ä¢We create AutoTune , a novel pipeline to simultaneously label face images in the wild and adapt the pretrained deep neural network to recognize the faces of users in new environments. The key idea is to repeat the faceidentity association and network update in tandem. To cope with observation noise, we propose a novel probabilistic framework in AutoTune and design a new stochastic center loss to enhance the robustness of network finetuning. ‚Ä¢We deployed AutoTune in two realworld environments and experimental results demonstrate that AutoTune is able to achieve >0.85F1score of image labeling in both environ ments, outperforming the best competing approach by >25%. Compared to the best competing approach, using the fea tures extracted from the finetuned model and training a classifier based on the crossmodality labeled images can give a‚àº19%performance gain for online face recognition. The rest of this paper is organized as follows. ¬ß2 introduces the background of this work. System overview is given in ¬ß3. We de scribe the AutoTune solution in ¬ß4 and ¬ß5. System implementation details are given in ¬ß6. The proposed approach is evaluated and compared with state of the art methods in ¬ß7. Finally, we discuss and outlook future work in ¬ß8 and conclude in ¬ß9.2 RELATED WORK "
402,Hand gesture detection in tests performed by older adults.txt,"Our team are developing a new online test that analyses hand movement
features associated with ageing that can be completed remotely from the
research centre. To obtain hand movement features, participants will be asked
to perform a variety of hand gestures using their own computer cameras.
However, it is challenging to collect high quality hand movement video data,
especially for older participants, many of whom have no IT background. During
the data collection process, one of the key steps is to detect whether the
participants are following the test instructions correctly and also to detect
similar gestures from different devices. Furthermore, we need this process to
be automated and accurate as we expect many thousands of participants to
complete the test. We have implemented a hand gesture detector to detect the
gestures in the hand movement tests and our detection mAP is 0.782 which is
better than the state-of-the-art. In this research, we have processed 20,000
images collected from hand movement tests and labelled 6,450 images to detect
different hand gestures in the hand movement tests. This paper has the
following three contributions. Firstly, we compared and analysed the
performance of different network structures for hand gesture detection.
Secondly, we have made many attempts to improve the accuracy of the model and
have succeeded in improving the classification accuracy for similar gestures by
implementing attention layers. Thirdly, we have created two datasets and
included 20 percent of blurred images in the dataset to investigate how
different network structures were impacted by noisy data, our experiments have
also shown our network has better performance on the noisy dataset.","There is increasing interest in developing computer tests that can completed remotely, away from the research centre. This provides convenience for participants, facilitates involvement in research for participants who live remotely and allows research studies to continue when restrictions on travel occur, such as during the COIVD 2019 pandemic. Our team are developing a new online test that analyses hand movement features associated with ageing that can be completed remotely from the research centre. Our test has included two different tests, the alternating hand movement test involves opening and closing the whole hand and the alternating Ô¨Ångertapping test involves opening and closing the Ô¨Ångers of each hand repeatedly. Both tests are considered an important test for evaluating motor function. Figure 1 shows how the Ô¨Ångertapping test is performed, the participants will be instructed to switch between ‚ÄòGesture 1‚Äô and ‚ÄòGesture 2‚Äô quickly and repeatedly. Since the video tests will request participants to open and close their Ô¨Ångers as fastarXiv:2110.14461v2  [cs.CV]  29 Oct 2021APREPRINT  NOVEMBER 1, 2021 as possible using relatively low fps laptop cameras, motion blur may be a signiÔ¨Åcant problem during the data collection process. Meanwhile, getting the participants to follow the instruction and recording the correct gestures are essential for our study as most participants will complete the tests unsupervised in their own homes. To obtain highquality video data in the hand movement tests, it is important to detect if users are following the instructions with the correct gestures. (a) Gesture 1  (b) Gesture 2 Figure 1: Finger tapping test demonstration Since the hand movement tests will request participants to open and close their Ô¨Ångers as fast as possible using relatively low fps laptop cameras, motion blur may be a signiÔ¨Åcant problem during the data collection process. Meanwhile, getting the participants to follow the instruction and recording the correct gestures are essential for the following study as most participants will complete the tests unsupervised in their own homes. To obtain highquality video data in the hand movement tests, it is important to detect if users are following the instructions with the correct gestures. In this case, hand gesture recognition will apply important implications in guiding participants to assist data collection for TAS Test hand movement tests. This paper has the following three contributions: 1.We have processed and labelled 7071 images, then built a hand gestures classiÔ¨Åcation dataset for dementia related gesture classiÔ¨Åcation and motion blur study 2. We have compared the performance of different network structures on the hand gesture classiÔ¨Åcation task 3.We proposed a novel approach and used the attention technique to increase the classiÔ¨Åcation performance on similar gestures. Our model is better than stateoftheart. 4.We have demonstrated how the quality of images will impact the performance on hand gesture classiÔ¨Åcation through two sets of experiments and have proven our network structure has better result than stateoftheart on noisy dataset. 2 Related Work "
256,Attend in groups: a weakly-supervised deep learning framework for learning from web data.txt,"Large-scale datasets have driven the rapid development of deep neural
networks for visual recognition. However, annotating a massive dataset is
expensive and time-consuming. Web images and their labels are, in comparison,
much easier to obtain, but direct training on such automatically harvested
images can lead to unsatisfactory performance, because the noisy labels of Web
images adversely affect the learned recognition models. To address this
drawback we propose an end-to-end weakly-supervised deep learning framework
which is robust to the label noise in Web images. The proposed framework relies
on two unified strategies -- random grouping and attention -- to effectively
reduce the negative impact of noisy web image annotations. Specifically, random
grouping stacks multiple images into a single training instance and thus
increases the labeling accuracy at the instance level. Attention, on the other
hand, suppresses the noisy signals from both incorrectly labeled images and
less discriminative image regions. By conducting intensive experiments on two
challenging datasets, including a newly collected fine-grained dataset with Web
images of different car models, the superior performance of the proposed
methods over competitive baselines is clearly demonstrated.",2. Related Work 2 
469,Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery.txt,"The trend towards higher resolution remote sensing imagery facilitates a
transition from land-use classification to object-level scene understanding.
Rather than relying purely on spectral content, appearance-based image features
come into play. In this work, deep convolutional neural networks (CNNs) are
applied to semantic labelling of high-resolution remote sensing data. Recent
advances in fully convolutional networks (FCNs) are adapted to overhead data
and shown to be as effective as in other domains. A full-resolution labelling
is inferred using a deep FCN with no downsampling, obviating the need for
deconvolution or interpolation. To make better use of image features, a
pre-trained CNN is fine-tuned on remote sensing data in a hybrid network
context, resulting in superior results compared to a network trained from
scratch. The proposed approach is applied to the problem of labelling
high-resolution aerial imagery, where fine boundary detail is important. The
dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and
Potsdam benchmark data sets.","Land use classication has been a longstanding research problem in remote sensing, and has historically been applied to coarse resolution multispectral imagery (for example, LANDSAT has 30m x 30m ground sampling distance (GSD), Quickbird 2.2m GSD). More recently, highresolution aerial imagery has become available with a GSD of 510 cm, so that objects such as cars and buildings are distinguishable. Pixel labelling becomes a richer semantic representation, but is more dicult. Now instead of classifying a spectral signature averaged over a large area (one pixel to many objects), individual objects must be segmented (one object to many pixels). Spectral properties alone may not be sucient to distinguish objects (e.g. grass from trees, road from roof), and discriminative appearancebased features are needed. The negrained classication of image pixels is termed semantic labelling . For such highresolution imagery, computer vision techniques for object segmentation and semantic la belling are eminently applicable. Recently deep convolutional neural networks (CNNs) have become the dominant method for visual recognition, achieving stateoftheart results on a number of problems includ ing semantic labelling [16, 3, 17]. CNNs have also been applied to remote sensing data [25], but usually on a patch level. For classication, this involves classifying a cropped out patch of imagery ( e.g.airport, forest). In the case of semantic labelling, the aim is usually to classify the pixel at the centre of the patch, and this classication is applied to overlapping patches densely over the image, resulting in a fully labelled output. In this work we apply the recentlydeveloped fully convolutional network [17] to semantic labelling of aerial imagery, achieving higher accuracy than the patchbased approach. By exploiting the convolutional nature of the CNN, the classier can be treated like a convolutional lter both during training and classication. The result is improved accuracy and computational eciency. However the FCN produces output at a signicantly 1arXiv:1606.02585v1  [cs.CV]  8 Jun 2016lower resolution than the input imagery due to poolinganddownsampling layers in the network. This is undesirable for complete labelling of remote sensing data because the ne boundary details are important. Here a novel nodownsampling network is presented to maintain the full resolution of the imagery at every layer in the FCN. The nodownsampling approach boosts the eective training sample size and achieves higher accuracy than a downsampling FCN, especially when the downsampling factor of the network is large. For aerial imagery, a semantic labelling pixel accuracy of around 75% can be achieved simply using the spectral and elevation information at each individual pixel [25]. To achieve higher accuracy on higher resolution images, the scene appearance must be exploited using discriminative texture features. We use pretrained convolutional features derived from ImageNet data to improve overhead semantic labelling. Pre trained networks only take 3band data as input. To make use of elevation data such as a digital surface model (DSM) we propose a hybrid network that combines the pretrained image features with DSM features that are trained from scratch. The hybrid network improves the labelling accuracy on the highestresolution imagery. The novel contributions of this work are: 1. the use of fullyconvolutional networks to semantically label aerial imagery; 2. a nodownsampling approach to FCNs to preserve output resolution; 3. a hybrid FCN architecture to combine imagery with elevation data; 4. the rst published results on the ISPRS Potsdam benchmark; and 5. stateoftheart semantic labelling accuracy for highresolution aerial imagery. The remainder of the paper is arranged as follows. Related work on semantic labelling using CNNs on remote sensing data and FCNs is reviewed in Section 2. The characteristics of highresolution aerial imagery are discussed in Section 3 and the data sets used for experimentation are introduced. Section 4 explains how FCNs are applied to remote sensing data and compares the approach experimentally to patchbased training. The nodownsampling FCN is introduced in Section 5 and compared with FCNs. Section 6 shows how pretrained image features can be combined with a custom FCN with DSM input in a hybrid architecture to improve segmentation of the veryhighresolution Potsdam data. The paper concludes with Section 7. Details of the experimental parameters are listed in the Appendix. 2 Related Work "
556,Weakly supervised CRNN system for sound event detection with large-scale unlabeled in-domain data.txt,"Sound event detection (SED) is typically posed as a supervised learning
problem requiring training data with strong temporal labels of sound events.
However, the production of datasets with strong labels normally requires
unaffordable labor cost. It limits the practical application of supervised SED
methods. The recent advances in SED approaches focuses on detecting sound
events by taking advantages of weakly labeled or unlabeled training data. In
this paper, we propose a joint framework to solve the SED task using
large-scale unlabeled in-domain data. In particular, a state-of-the-art general
audio tagging model is first employed to predict weak labels for unlabeled
data. On the other hand, a weakly supervised architecture based on the
convolutional recurrent neural network (CRNN) is developed to solve the strong
annotations of sound events with the aid of the unlabeled data with predicted
labels. It is found that the SED performance generally increases as more
unlabeled data is added into the training. To address the noisy label problem
of unlabeled data, an ensemble strategy is applied to increase the system
robustness. The proposed system is evaluated on the SED dataset of DCASE 2018
challenge. It reaches a F1-score of 21.0%, resulting in an improvement of 10%
over the baseline system.","Great attention has been paid to developing advanced  approaches to understand the sounds of everyday life in the  contexts of practical applications such as smart cars [1],  surveillance  [2, 3], healthcare [4]. Sound event  detection  (SED) has been studied to automatically  achieve the strong  temporal annotations for the occurrences of sound events in  an audio recording . As the rapid development in recen t  years, deep learning methods have become the main  approaches to solve the SED task, especially in the IEEE  audio event and scene detection challenges (DCASE) [1, 5]. Ideally strongly labeled audio data is preferable for SED  model development in terms of supervised learning manner  [6, 7]. However, the cost of producing strongly labeled  dataset is remarkably high for the data intensive de ep learning methods. Therefore, it is desirable to develop  weakly supervised SED methods t hat can exploit weakly  labeled data and  unlabeled data effectively. As the release of  Google AudioSet [8] that consists of approximately 2  million weakly labeled short audio clips, the progress of  weakly supervised learning SED approaches has been  significantly accelerated.   1.1 Sound event detection by weakly supervised l earning   Apart from supervised learning, the weakly supervised SED  approaches can be normally grouped into three domains:   (1) The methods developed under inexact supervision   [9] to predict strong annotations of sound events using  weakly labeled training data. In terms of using the weak  labels for SED, there are mainly four ways. One way is to  directly assign the clip level labels (weak labels) to all the  frame level segments (strong labels) to train the model [10,  11], which may introduce noise to the frame level labels.  Another way is to do the source separation at first and  obtain the frame level labels [12, 13] based on the separated  sources. The third way is to use the a ttention mechanisms  [14, 15] to learn the relationship  between frame level labels  and clips level labels in training process. The fourth way is  to formulate the weak label SED job as a multiple instance  learning (MIL) problem [1618], where the audio clips are  treated as bags and audio segments are treated as instances.   (2) The methods implemented as incomplete  supervision  [9] to make use of unlabeled training data to  increase performance. The semi supervised lear ning is the  major technique for this purpose [1921]. Moreover, virtual  adverserial training (VAT) can  also be used to process un  labelled data [20, 22]. Mean teacher algorithm can improve  the performance of semi supervised SED system by using  unlabeled data [23]. The application of ensemble  mechanisms is a good strategy to make the semi supervised  learning more reliable [24].    1 The code  in https://github.com/Cocoxili/DCASE2018Task2/    (3) The methods used to deal with the noisy labels of  training data in an inaccurate supervision  [9] manner. To  address the noisy label problem in SED, a practical idea is  to identify the mislabeled samples and make some  corrections. The majority voting strategy and sample re  weight  strategy [25] employed in the ensemble methods are  widely used techniques. Some effort is also made to  implement an iteratively fine tuning framework by means of  selfverifying the training observations [26].  1.2 Our contributions   In this paper, we aim to develop a scalable system based on  a CRNN framework using th e well developed neural  networks for weakly supervised SED. The main  contributions of our work can be summarized as: (1) In  order to make use of unlabe led training data, we use our  audio tagging system  (NUDT system)  ranked as the top in  the DCASE 2018 chal lenge [27] to contribute on predicting   the weak labels for unlabeled data more effectively. (2) We  explore to integrate well developed CNN architectures such  as ResNet  [28] and Xception  [29] into the C RNN  framework for more  effective feature extract ion. The SED  performance significantly  benefit s from finetuning these  pretrained CNN models . (3) To address influence of us ing  unlabeled  training  data on SED results, we make a  comparative study by respectively adding unlabeled data  with different confidence levels  into the training. (4) In  order to tackle the noisy label problem caused by  using   unlabeled training data, we app ly a model ensemble  technique to increase the robustness of proposed system .  Our code can be  referred to  https://github.com/Blank  Wang/DCASE2018 Task4 .    2. METHODOLOGY   "
525,Reddening-free Q indices to identify Be star candidates.txt,"Astronomical databases currently provide high-volume spectroscopic and
photometric data. While spectroscopic data is better suited to the analysis of
many astronomical objects, photometric data is relatively easier to obtain due
to shorter telescope usage time. Therefore, there is a growing need to use
photometric information to automatically identify objects for further detailed
studies, specially H{\alpha} emission line stars such as Be stars. Photometric
color-color diagrams (CCDs) are commonly used to identify this kind of objects.
However, their identification in CCDs is further complicated by the reddening
effect caused by both the circumstellar and interstellar gas. This effect
prevents the generalization of candidate identification systems. Therefore, in
this work we evaluate the use of neural networks to identify Be star candidates
from a set of OB-type stars. The networks are trained using a labeled subset of
the VPHAS+ and 2MASS databases, with filters u, g, r, H{\alpha}, i, J, H, and
K. In order to avoid the reddening effect, we propose and evaluate the use of
reddening-free Q indices to enhance the generalization of the model to other
databases and objects. To test the validity of the approach, we manually
labeled a subset of the database, and use it to evaluate candidate
identification models. We also labeled an independent dataset for cross dataset
evaluation. We evaluate the recall of the models at a 99% precision level on
both test sets. Our results show that the proposed features provide a
significant improvement over the original filter magnitudes.","In the big data era, free access to databases in dierent wavelength ranges, from gammarays to radio waves, together with machinelearning methods, has drastically incremented the possibility to study and identify dierent types of peculiar lineemission stars using photometric information (e.g., Vioque et al., 2019 [30]; Akras et al., 2019 [2]; P erezOrtiz et al., 2017 [21]). While spectroscopic techniques are excellent to perform accurate stellar clas sication and deepen into the study of various spectral features, the telescope time required to obtain such information is longer compared to obtaining pho tometric data. The goal of this work is then to use the potential of photometric data to search for emissionline star candidates. These can be later observed and be conrmed spectroscopically as such. Particularly, we are interested in detecting Be star candidates. Be stars are emissionline objects that rotate at high speed (Jaschek et al., 1981 [15]; Struve, O., 1931 [28]) and constitute unique astrophysical laboratories. They are of interest in various branches of stellar physics dedicated to the study of mechanisms of mass loss, angular momentum distribution, astroseismology, among others. The rest of this section describes Be stars in detail, classical techniques to detect plausible candidates as well as previous star candidate proposals based on machinelearning methods. 1.1 Be stars Be stars are dened as nonsupergiant spectral Btype stars that exhibit, or have exhibited, one or more hydrogen lines in emission (Jaschek et al., 1981 [15]; Collins, II, G., 1987 [7]), particularly the H line. In some cases, it is also possible to observe the presence of onceionized helium and metal lines in emission. Thus, this denition not only applies to Btype stars but also to late O and early Atype stars. The analysis of spectrophotometric observations of Be stars at dierent wave lengths, combined with interferometric and polarimetric data (Gies et al., 2007 [11]; Meilland et al., 2007 [18], among others), indicate that the dierent properties shown by these stars could be interpreted by the existence of an opticallythin gaseous circumstellar equatorial disk in Keplerian motion (see Rivinius et al., 2013 [25]). This suggests that the high rotation speed would play a signicant role in the development of the equatorial disk (e.g., Struve, O., 1931 [28]; Huang, S., 1972 [14]; Quirrenbach, A., 1993 [23]; Quirrenbach et al., 1994 [24]; Hirata, R., 1995 [13]). However, despite the increasing observational evidence that Be stars do not rotate at their critical rotational speed (Zorec et al. 2016 [31], Zorec et al. 2017 [32], Aidelman et al. 2018 [1], Cochetti et al., 2019 [6]), there is still no consensus on disk formation mechanism(s). Other observed eects induced by stellar rotation during the main sequence phase of hot stars, are the development of axisymmetric winds, the modicationReddeningfree Qindices to identify Be star candidates 3 in pulsation modes, changes in metallicity or the presence of magnetic elds (see Peters et al., 2020 [22]; Rivinius et al., 2013 [25]). These properties make Be stars perfect stellar laboratories, of interest in dierent astrophysical topics, as mentioned above. In this context, the discovery, classication and analysis of a considerable sample of Be stars in dierent environments are necessary to understand their nature. 1.2 Related work "
215,Symmetric Cross Entropy for Robust Learning with Noisy Labels.txt,"Training accurate deep neural networks (DNNs) in the presence of noisy labels
is an important and challenging task. Though a number of approaches have been
proposed for learning with noisy labels, many open issues remain. In this
paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting
to noisy labels on some classes (""easy"" classes), but more surprisingly, it
also suffers from significant under learning on some other classes (""hard""
classes). Intuitively, CE requires an extra term to facilitate learning of hard
classes, and more importantly, this term should be noise tolerant, so as to
avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we
propose the approach of \textbf{Symmetric cross entropy Learning} (SL),
boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy
(RCE). Our proposed SL approach simultaneously addresses both the under
learning and overfitting problem of CE in the presence of noisy labels. We
provide a theoretical analysis of SL and also empirically show, on a range of
benchmark and real-world datasets, that SL outperforms state-of-the-art
methods. We also show that SL can be easily incorporated into existing methods
in order to further enhance their performance.","Modern deep neural networks (DNNs) are often highly complex models that have hundreds of layers and millions of trainable parameters, requiring largescale datasets with clean label annotations such as ImageNet [2] for proper training. However, labeling largescale datasets is a costly and errorprone process, and even highquality datasets are likely to contain noisy (incorrect) labels. Therefore, training accurate DNNs in the presence of noisy labels has become a task of great practical importance in deep learning. Recently, several works have studied the dynamics of DNN learning with noisy labels. Zhang et.al [28] argued that DNNs exhibit memorization effects whereby they Ô¨Årst memorize the training data for clean labels and then subse quently memorize data for the noisy labels. Similar Ô¨Åndings Equal contribution. yCorrespondence to: Yisen Wang (eewangyisen@gmail.com) and Xingjun Ma (xingjun.ma@unimelb.edu.au). (a) CE  clean  (b) CE  noisy (c) LSR  noisy  (d)SL noisy Figure 1: The classwise test accuracy of an 8layer CNN on CIFAR10 trained by (a) CE on clean labels with class biased phenomenon, (b) CE on 40% symmetric/uniform noisy labels with ampliÔ¨Åed classbiased phenomenon and under learning on hard classes ( e.g., class 3), (c) LSR under the same setting to (b) with under learning on hard classes still existing, (d) our proposed SL under the same setting to (b) exhibiting overall improved learning on all classes. are also reported in [1] that DNNs Ô¨Årst learn clean and easy patterns and eventually memorize the wrongly assigned la bels. Further evidence is provided in [13] that DNNs Ô¨Årst learn simple representations via subspace dimensionality compression and then overÔ¨Åt to noisy labels via subspace dimensionality expansion. Different Ô¨Åndings are reported in [19], where DNNs with a speciÔ¨Åc activation function (i.e., tanh) undergo an initial label Ô¨Åtting phase then a sub sequent representation compression phase where the over Ô¨Åtting starts. Despite these important Ô¨Åndings, a complete understanding of DNN learning behavior, particularly their learning process for noisy labels, remains an open question. In this paper, we provide further insights into the learn ing procedure of DNNs by investigating the learning dy namics across classes. While Cross Entropy (CE) loss is the most commonly used loss for training DNNs, we have found that DNN learning with CE can be classbiased :arXiv:1908.06112v1  [cs.LG]  16 Aug 2019(a) CE  clean  (b) CE  noisy  (c) SL  noisy Figure 2: Visualization of learned representations on CIFAR10 using tSNE 2D embeddings of deep features at the last second dense layer with (a) CE on clean labels, (b) CE on 40% symmetric noisy labels, (c) the proposed SL on the same setting to (b). some classes (‚Äúeasy‚Äù classes) are easy to learn and con verge faster than other classes (‚Äúhard‚Äù classes). As shown in Figure 1a, even when labels are clean, the classwise test accuracy spans a wide range during the entire training pro cess. As further shown in Figure 1b, this phenomenon is ampliÔ¨Åed when training labels are noisy: whilst easy classes (e.g., class 6) already overÔ¨Åt to noisy labels, hard classes (e.g., class 3) still suffer from signiÔ¨Åcant under learning (class accuracy signiÔ¨Åcantly lower than clean label setting). SpeciÔ¨Åcally, class 3 (bottom curve) only has an accuracy of60% at the end, considerably less than the >90% ac curacy of class 6 (top curve). Label Smoothing Regular ization (LSR) [21, 17] is a widely known technique to ease overÔ¨Åtting issues, as shown in Figure 1c, which still exhibits signiÔ¨Åcant under learning on hard classes. Comparing the overall test accuracy (solid red curve) in Figure 1, a low test accuracy (under learning) on hard classes is a barrier to high overall accuracy. This is a different Ô¨Ånding from previous belief that poor performance is simply caused by overÔ¨Åtting to noisy labels. We also visualize the learned representa tions for the noisy label case in Figure 2b: some clusters are learned comparably well to those learned with clean la bels (Figure 2a), while some other clusters do not have clear separated boundaries. Intuitively, CE requires an extra term to improve its learning on hard classes, and more importantly, this term needs to be tolerant to label noise. Inspired by the sym metric KLdivergence, we propose such a noise tolerant term, namely Reverse Cross Entropy (RCE), which com bined with CE forms the basis of the approach Symmetric cross entropy Learning (SL). SL not only promotes sufÔ¨Å cient learning (class accuracy close to clean label setting) of hard classes, but also improves the robustness of DNNs to noisy labels. As a preview of this, we can inspect the improved learning curves of classwise test accuracy and representations in Figure 1d and 2c. Under the same 40% noise setting, the variation of classwise test accuracy has been narrowed by SL to 20% with 95% the highest and 75% the lowest (Figure 1d), and the learned representations are of better quality with more separated clusters (Figure 2c), both of which are very close to the clean settings. Compared to existing approaches that often involve ar chitectural or nontrivial algorithmic modiÔ¨Åcations, SL isextremely simple to use. It requires minimal intervention to the training process and thus can be straightforwardly incorporated into existing models to further enhance their performance. In summary, our main contributions are: We provide insights into the classbiased learning pro cedure of DNNs with CE loss and Ô¨Ånd that the under learning problem of hard classes is a key bottleneck for learning with noisy labels. We propose a Symmetric Learning (SL) approach, to simultaneously address the hard class under learning problem and the noisy label overÔ¨Åtting problem of CE. We provide both theoretical analysis and empirical un derstanding of SL. We empirically demonstrate that SL can achieve better robustness than stateoftheart methods, and can be also easily incorporated into existing methods to sig niÔ¨Åcantly improve their performance. 2. Related Work "
235,Plug-and-Play Pseudo Label Correction Network for Unsupervised Person Re-identification.txt,"Clustering-based methods, which alternate between the generation of pseudo
labels and the optimization of the feature extraction network, play a dominant
role in both unsupervised learning (USL) and unsupervised domain adaptive (UDA)
person re-identification (Re-ID). To alleviate the adverse effect of noisy
pseudo labels, the existing methods either abandon unreliable labels or refine
the pseudo labels via mutual learning or label propagation. However, a great
many erroneous labels are still accumulated because these methods mostly adopt
traditional unsupervised clustering algorithms which rely on certain
assumptions on data distribution and fail to capture the distribution of
complex real-world data. In this paper, we propose the plug-and-play
graph-based pseudo label correction network (GLC) to refine the pseudo labels
in the manner of supervised clustering. GLC is trained to perceive the varying
data distribution at each epoch of the self-training with the supervision of
initial pseudo labels generated by any clustering method. It can learn to
rectify the initial noisy labels by means of the relationship constraints
between samples on the k Nearest Neighbor (kNN) graph and early-stop training
strategy. Specifically, GLC learns to aggregate node features from neighbors
and predict whether the nodes should be linked on the graph. Besides, GLC is
optimized with 'early stop' before the noisy labels are severely memorized to
prevent overfitting to noisy pseudo labels. Consequently, GLC improves the
quality of pseudo labels though the supervision signals contain some noise,
leading to better Re-ID performance. Extensive experiments in USL and UDA
person Re-ID on Market-1501 and MSMT17 show that our method is widely
compatible with various clustering-based methods and promotes the
state-of-the-art performance consistently.","Person reidentification (ReID), which aims to associate person images captured by disjoint cameras, is of great practical value. Recently, unsupervised learning2 T. Yan et al. Clustering ùí¥ùë°+1 ‡∑®ùí¥ùë°+1Conventional Clustering based Methods Our Method Clustering ùêπùë° Feature  Extraction Network GLC ùëÆùú∂ Feature  Extraction Network ùêπùë°ùí¥ùë°+1 (a) 100 200 300 400 500 Iterations:0.700.720.740.760.780.80NMI Scores Initial Pseudo Labels Corrected Pseudo Labels (b) Fig. 1: (a) Conventional clusteringbased methods alternate between the genera tion of pseudo labels yt+1via clustering the features Ft, and the optimization of network in ( t+1)th epoch. This selftraining manner can produce a great many label noises, substantially hindering the training of EŒ∏. We propose GLC GŒ±as a postprocessing module to refine the initial pseudo labels after each clustering, improving the performance of EŒ∏. Note that GLC is not required during the testing phase. (b) NMI Scores of pseudo labels at different iterations in the GLC training. We early stop the training of GLC (The dotted line), which prevents the overfitting to the label noise, improving the initial noisy pseudo labels. (USL) person ReID [5, 2] and unsupervised domain adaptive (UDA) person ReID [4, 18, 28], which relax the requirement on labeled training data, have received a lot of research interests. Nowadays, clusteringbased approaches [4, 8, 2, 28, 4, 31], which alternate between the generation of pseudo labels and the optimization of feature extraction network as shown in fig. 1 (a), dominate the community of USL and UDA person ReID. Although the accuracy of pseudo labels are gradually improved, there are inevitable pseudo label noises, which will be accumulated during training, leading to degraded the reID performance. Recently, a series of researches have been presented to reduce the adverse impact of noisy pseudo labels, and can be roughly divided into two categories, reliable pseudo label selection methods [5, 18, 27, 23] and pseudo label refinement methods [4, 27]. The former utilize the estimated reliability of pseudo labels to select credible samples or downweight unreliable ones to train the feature ex tractor. The latter achieve label refinement by adopting mutual learning scheme [4], optimal transport algorithm [29] or label propagation [31, 21]. The existing methods mostly adopt the traditional unsupervised clustering algorithms, such as Kmeans [14] and DBSCAN [6], to generate pseudo labels. These algorithms all rely on certain assumptions on data distribution ( e.g., convex shape, similar size and same density of clusters), failing to capture the distribution of complex realworld data thus generating many erroneous pseudo labels. This motivates us to tackle the noisy pseudo label issue by applying the supervised clustering framework, which can perceive the data distribution from the training samples, and generate better pseudo labels. In this paper, we propose a plugandplay graphbased pseudo label correc tion network (GLC) to improve the accuracy of pseudo labels for USL and UDAAbbreviated paper title 3 person reID. GLC is trained to capture the data distribution at each epoch with the supervision of pseudo labels generated by any clustering methods. Al though the supervision signals contain some noise, GLC can learn to correct the initial noisy pseudo labels by means of the relationship constraint between samples on the graph and the earlystop training strategy. GLC is an addon component to any clusteringbased methods which adopt an iterative twostage training scheme. As shown in fig.1(a), GLC acts as a postprocessing module in a plugandplay way, which is optimized separately from the feature extrac tion network to rectify current pseudo labels after each clustering. Thus GLC is widely compatible with the existing pseudo label refinement approaches, and adding it can further reduce the remaining pseudo label noise. Specifically, we first construct a kNN graph with each node denoting a per son image and being connected to its k nearest neighbours. Then we formulate the image clustering task as a problem of the link prediction on the graph, by applying several Graph Convolutional Network layers to aggregate node features and a classification layer to predict whether two nodes should be linked under the supervision of pseudo labels. After the GLC training, each node feature is refined by the similarity to its neighbors and the supervisory information, pos sessing more robustness to the initial noise pseudo labels. Besides, considering that deep neural networks first fit the training data with clean labels3before eventually memorizing the examples with false labels when trained on noisy labels [13], we adopt an earlystop strategy during the GLC training to avoid overfitting to the noisy pseudo labels as illustrated in fig.1(b). Finally, in the GLC inference, we conduct link prediction on the whole graph, and the links with low confidence are cut off. Dynamic clusters are generated to fit the cur rent data distribution, and there are more chances for those false positive image links to be cut off and those false negative image links and outliers to be linked. To further unleash the potential of GLC, on the one hand, we jointly utilize sample features and ID classification scores to measure the node similarity and link more positive image pairs together during the kNN graph construction, considering that classification scores are more robust to the data distribution gap caused by factors like camera variations than raw features [22]. On the other hand, we reinitialize the parameters of GLC at each epoch to reduce the accumulated errors inherited by the network parameters, and we also retrain the feature extractor from scratch after some epochs to start another selftraining process with the corrected pseudo labels, to get rid of the adverse impact of previous noisy pseudo labels. Our main contributions are summarized as follows. (1) We are the first to adopt a neuralnetworkbased supervised clustering framework for USL/UDA person reID, which perceives the data distribution from training samples and adapts better to the dynamic features at each epoch. (2) We propose the plug andplay GLC to learn to refine the initial noisy pseudo labels with the re lationship constrains between samples on the graph and the early stop train 3The clusteringbased methods have a basic assumption that most samples from the same person are given the same pseudo label, and in our practice, GLC first fit them.4 T. Yan et al. ing strategy. (3) Extensive experiments in unsupervised and UDA person ReID on Market1501 and MSMT17 demonstrate the wide compatibility and consis tent performance promotion of our proposed method to various stateoftheart clusteringbased methods. 2 Related Work "
434,Hybrid Contrastive Learning with Cluster Ensemble for Unsupervised Person Re-identification.txt,"Unsupervised person re-identification (ReID) aims to match a query image of a
pedestrian to the images in gallery set without supervision labels. The most
popular approaches to tackle unsupervised person ReID are usually performing a
clustering algorithm to yield pseudo labels at first and then exploit the
pseudo labels to train a deep neural network. However, the pseudo labels are
noisy and sensitive to the hyper-parameter(s) in clustering algorithm. In this
paper, we propose a Hybrid Contrastive Learning (HCL) approach for unsupervised
person ReID, which is based on a hybrid between instance-level and
cluster-level contrastive loss functions. Moreover, we present a
Multi-Granularity Clustering Ensemble based Hybrid Contrastive Learning
(MGCE-HCL) approach, which adopts a multi-granularity clustering ensemble
strategy to mine priority information among the pseudo positive sample pairs
and defines a priority-weighted hybrid contrastive loss for better tolerating
the noises in the pseudo positive samples. We conduct extensive experiments on
two benchmark datasets Market-1501 and DukeMTMC-reID. Experimental results
validate the effectiveness of our proposals.","Person ReidentiÔ¨Åcation (ReID) is a popular and important task in pattern recogni tion and computer vision, aiming to Ô¨Ånd the images of the same pedestrian in gallery to match the given probe image. The common approaches are to sort the gallery im ages according to the similarity between the probe image and the images in the gallery. Early works are usually based on supervised learning, which trains a deep model with a large amount of labeled data. However, the performance of the supervised ReID model will often seriously degenerate when facing the openworld data because the models are usually trained with limited data with supervision information. Thus it is crucial to exploit the hidden guidance information from the images without supervision. In recent years, unsupervised methods for person ReID have attracted a lot of atten tion. In unsupervised setting, the most popular methods [5‚Äì7, 27] are based on training a deep neural network with pseudo labels, which are generated by clustering algorithm (e.g.,kmeans, DBSCAN [3]). For instance, kmeans is used in [5] to generate the pseudo labels for different part of the images and DBSCAN is used in [6, 7, 27].arXiv:2201.11995v2  [cs.CV]  14 Apr 20222 H. Sun et al. The basic assumption behind the pseudo labelsbased unsupervised methods is that the samples in the same cluster are more likely with the same class label. Unlike the groundtruth labels, however, the pseudo labels obtained via a clustering algorithm are unavoidably noisy. Thus it is critic to tackle the noises in pseudo labels. For example, in [6], a mutual learning strategy via a temporal mean net is leveraged; in [5], a multi branch network from [19] is adopted to perform clustering with different part of images. Besides, some works [15, 24] attempt to exploit the neighborhood relationship instead of using traditional clustering methods. More recently, in [7], contrastive learning is introduced to unsupervised person ReID, in which a hybrid memory bank is used to store all the features and a uniÔ¨Åed contrastive loss based on the similarity of inputs and all features is adopted to train a deep neural network. While remarkable improvements in performance are reported, all these methods depend upon performing clustering method with a delicate hyper parameter (e.g., the neighborhood ratio parameter din DBSCAN). Unfortunately, the performance might dramatically degenerate if an improper hyperparameter is used. In this paper, we present a simple yet effective contrastive learningbased frame work for unsupervised person ReID, in which the noisy pseudo labels are used to deÔ¨Åne a hybrid contrastive loss‚Äîwhich aims to ‚Äúattract‚Äù the pseudo positive samples in the current cluster and at the meantime ‚Äúdispel‚Äù all the remaining samples (i.e., the pseudo negative samples) with respect to the current cluster. Moreover, we introduce a cluster ensemble strategy to generate multigranularity clustering information‚Äîwhich is en coded into priority weights, and adopt the priority weights to deÔ¨Åne a weighted hybrid contrastive loss. The cluster ensemble strategy aims to alleviate the sensitivity of using a single hyperparameter in clustering algorithm by using a range of the hyperparameter to perform clustering ensemble instead; whereas the priorityweighting mechanism in the contrastive loss aims to better tolerate the noises in pseudo labels. Paper Contributions. The contributions of the paper can be summarized as follows. ‚ÄìWe propose a novel hybrid contrastive paradigm for unsupervised person ReID, which is able to better exploit the noisy pseudo labels. ‚ÄìWe adopt a multigranularity clustering ensemble strategy to depict the conÔ¨Ådence of positive samples and hence present a priorityweighted hybrid contrastive loss for better tolerating the noises in pseudo positive samples. ‚ÄìWe conduct extensive experiments on two benchmark datasets and the experimental results validate the effectiveness of our proposals. 2 Related works "
171,Is your noise correction noisy? PLS: Robustness to label noise with two stage detection.txt,"Designing robust algorithms capable of training accurate neural networks on
uncurated datasets from the web has been the subject of much research as it
reduces the need for time consuming human labor. The focus of many previous
research contributions has been on the detection of different types of label
noise; however, this paper proposes to improve the correction accuracy of noisy
samples once they have been detected. In many state-of-the-art contributions, a
two phase approach is adopted where the noisy samples are detected before
guessing a corrected pseudo-label in a semi-supervised fashion. The guessed
pseudo-labels are then used in the supervised objective without ensuring that
the label guess is likely to be correct. This can lead to confirmation bias,
which reduces the noise robustness. Here we propose the pseudo-loss, a simple
metric that we find to be strongly correlated with pseudo-label correctness on
noisy samples. Using the pseudo-loss, we dynamically down weight
under-confident pseudo-labels throughout training to avoid confirmation bias
and improve the network accuracy. We additionally propose to use a confidence
guided contrastive objective that learns robust representation on an
interpolated objective between class bound (supervised) for confidently
corrected samples and unsupervised representation for under-confident label
corrections. Experiments demonstrate the state-of-the-art performance of our
Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets
including curated data synthetically corrupted with in-distribution and
out-of-distribution noise, and two real world web noise datasets. Our
experiments are fully reproducible github.com/PaulAlbert31/SNCF","Standard supervised datasets for image classification us ing deep learning [ 15,7,20,14] are constituted by large amounts of images gathered from the web which have been True label guess  Turtle Turtle Turtle  Classification loss minimization  True label guess  Turtle Turtle Turtle  Alligator  Classification loss minimization Alligator Alligator Alligator  Pseudo loss filtering  xLabel noise robust algorithm Ours  Figure 1. Two stage label noise mitigation on detected noisy sam ples. Contrary to stateoftheart label noise robust algorithms, we filter out incorrect pseudolabels using the pseudoloss to avoid confirmation bias on incorrect corrections. heavily curated by multiple human annotators. In this paper, we propose to devise an algorithm which aims to train an accurate classification network on a web crawled dataset [ 19,32] where the human curation process was skipped. By doing so, the dataset creation time is greatly reduced but label noise becomes an issue [ 2] and can greatly degrade the classification accuracy [ 42]. To counter the effect of noisy annotations, previous contributions have fo cused on detecting the noisy samples using the natural robust ness of deep learning architectures to noise in early training stages [ 3,4]. These algorithms will identify noisy sam ples because they tend to be learned slower than their clean counterpart [ 17], because of incoherences with the labels of close neighbors in the feature space [ 23,18], a confident prediction from the neural net in a different class than the target class [ 38,21], inconsistent predictions across itera tions [ 22,34], and more. Once the noisy samples are identi fied, a corrected label is produced, yet ensuring that labels are correctly guessed is less studied in the label noise litera ture. Some propositions inspired by semisupervised learn ing [28,41] have been made recently by Li et al. [18] where only pseudolabels whose value in the max softmax bin (con fidence) is superior to a hyperparameter threshold value are kept or by Song et al. [29] where low entropy predictions indicate a confident pseudolabel. This paper proposes toarXiv:2210.04578v2  [cs.CV]  15 Oct 2022focus on the correction of noisy samples once they have been detected. We specifically propose a novel metric, the pseudo loss, which is able to retrieve correctly guessed pseudolabels and that we show to be superior to the pseudolabel confi dence previously used in the semisupervised literature. We find that incorrectly guessed pseudolabels are especially damaging to the supervised contrastive objectives that have been used in recent contributions [ 23,1,18]. We propose an interpolated contrastive objective between classconditional (supervised) for the clean or correctly corrected samples, where we encourage the network to learn similar represen tation for images belonging to the same class; and an unsu pervised objective for the incorrectly corrected noise. This results in P¬Øseudo L¬ØossS¬Øelection (PLS) a twostage noise detection algorithm where the first stage detects all noisy samples in the dataset while the second stage removes incor rect corrections. We then train a neural network to jointly minimize a classification and a supervised contrastive objec tive. We design PLS on synthetically corrupted datasets and validate our findings on two real world noisy web crawled datasets. Figure 1 illustrates our proposed improvement to label noise robust algorithms. Our contributions are: ‚Ä¢A twostage noise detection using a novel metric where we ensure that the corrected targets for noisy samples are accurate; ‚Ä¢A novel softly interpolated confidence guided con trastive loss term between supervised and unsupervised objective to learn robust features from all images; ‚Ä¢Extensive experiments of synthetically corrupted and webcrawled noisy datasets to demonstrate the perfor mance of our algorithm. 2. Related work "
131,Unsupervised Person Re-Identification with Wireless Positioning under Weak Scene Labeling.txt,"Existing unsupervised person re-identification methods only rely on visual
clues to match pedestrians under different cameras. Since visual data is
essentially susceptible to occlusion, blur, clothing changes, etc., a promising
solution is to introduce heterogeneous data to make up for the defect of visual
data. Some works based on full-scene labeling introduce wireless positioning to
assist cross-domain person re-identification, but their GPS labeling of entire
monitoring scenes is laborious. To this end, we propose to explore unsupervised
person re-identification with both visual data and wireless positioning
trajectories under weak scene labeling, in which we only need to know the
locations of the cameras. Specifically, we propose a novel unsupervised
multimodal training framework (UMTF), which models the complementarity of
visual data and wireless information. Our UMTF contains a multimodal data
association strategy (MMDA) and a multimodal graph neural network (MMGN). MMDA
explores potential data associations in unlabeled multimodal data, while MMGN
propagates multimodal messages in the video graph based on the adjacency matrix
learned from histogram statistics of wireless data. Thanks to the robustness of
the wireless data to visual noise and the collaboration of various modules,
UMTF is capable of learning a model free of the human label on data. Extensive
experimental results conducted on two challenging datasets, i.e., WP-ReID and
DukeMTMC-VideoReID demonstrate the effectiveness of the proposed method.","PERSON reidentiÔ¨Åcation is essentially a person retrieval task in a multicamera surveillance network. Given an image or video of a person that we are interested in, it aims to Ô¨Ånd out the images or videos of this person from a large data corpus captured by multiple surveillance cameras. The supervised person reidentiÔ¨Åcation [1], [2], [3], [4], [5], [6], [7], [8] requires massive and exhaustive identity labeling of the crosscamera data, which is laborious and suffers the scalability issue in realworld applications. To bypass the label requirement, unsupervised person reidentiÔ¨Åcation [9], [10], [11], [12], [13], [14], [15] is proposed to directly learn models from unlabeled data. Thanks to its favorable potential, it has received substantial attention from both academia and industry in recent years. The existing unsupervised person reidentiÔ¨Åcation meth ods rely on visual cues for model training. Although re markable progress has been made, the defects of the visual data limit their further improvements. In other words, the occlusion and blur lead to the loss of distinguishing parts of pedestrians. Besides, the changes in viewpoints and clothing cause signiÔ¨Åcant appearance changes in pedestrians. These visual noises can easily mislead existing methods that rely solely on visual clues. When the visual data is unreliable, the performance of these methods cannot be guaranteed. These defects of visual data force us to seek new supplementary information to increase the robustness of the system. Yiheng Liu, Wengang Zhou, Qiaokang Xie, and Houqiang Li are with CAS Key Laboratory of GIP AS, University of Science and Technology of China, Hefei, China. Wengang Zhou and Houqiang Li are also with Institute of ArtiÔ¨Åcial Intelligence, Hefei Comprehensive National Science Center. Email: flyh156, xieqiaokg@mail.ustc.edu.cn,fzhwg, lihqg@ustc.edu.cn. Corresponding authors: Wengang Zhou and Houqiang Li. Wireless  fragment Wireless positioning trajectory Camera monitoring area Sensing area of wireless  fragments Fig. 1. The problem we study in this work. The wireless positioning trajectories of pedestrians carrying mobile phones can be obtained by existing cellular networks and WiFi positioning. Video data are captured when pedestrians walk to the monitoring areas of cameras. We consider the area within a preset sensing radius centered on the camera as the sensing area of the wireless trajectory fragments. When a pedes trian carrying a mobile phone enters the sensing area of the wireless fragments, the fragment of the wireless positioning trajectory within the sensing area is the sensed wireless fragment. The wireless fragment may belong to the same pedestrian as one of the videos captured by this camera during its time range. For the person reidentiÔ¨Åcation task, there have been some attempts to use the multimodal data. Fan et al. [16] propose to use radar signals for supervised person re identiÔ¨Åcation, but additional radar equipment is required. Liu et al . [17] introduce the wireless position trajectories for crossdomain person reidentiÔ¨Åcation. Wireless posi tioning trajectories can be obtained through offtheshelfarXiv:2110.15610v2  [cs.CV]  5 Apr 2023JOURNAL OF LATEX CLASS FILES, VOL. **, NO. **, ** ** 2 equipment, such as cellular networks and WiFi positioning systems. The immutability of signal ID allows it to assist visionbased person reidentiÔ¨Åcation task. However, they need to label the GPS coordinate of each location of the monitoring areas to map the pixel coordinate to the world coordinate for calculating the distance between videos and wireless signals [17]. Such a paradigm suffers the scalability issue since in real scenarios, labeling and maintaining the location information of each position in all monitoring areas extremely consumes human labor. Based on the above discussion, as shown in Fig. 1, we propose a new setting to assist unsupervised person reidentiÔ¨Åcation with wireless trajectories to mitigate the effects of visual noise (Fig. 2). For pedestrians carrying mobile phones, their wireless positioning trajectories can be obtained through existing cellular networks and WiFi positioning. We deÔ¨Åne the circular area whose distance from the surveillance camera is less than a preset distance as the wireless fragment sensing area of this camera. Once a pedestrian carrying a mobile phone enters this area, we assume that a person enters the monitored area and a wireless fragment is sensed. The wireless fragment is the fragment of the wireless positioning trajectory within the circular sensing area. The video of the owner of the wireless trajectory should belong to the set of videos captured by the corresponding camera during the time range of this wireless fragment. This weak relationship allows us to use wireless data to assist person reidentiÔ¨Åcation. Different from [16], our setting does not require new equipment. Compared with [17] which assumes the GPS coordinate labeling on the entire monitoring areas, our weak scene labeling setting only needs to access the locations of cameras. Therefore, our setting is more feasible and scalable for largescale surveillance scenarios than previous settings [16], [17]. Our weak scene labeling makes the monitoring system easier to maintain and deploy, but it also brings great chal lenges. In each surveillance scene, since many pedestrians are carrying mobile phones, multiple videos and multi ple wireless fragments are captured simultaneously. For a wireless fragment, the video of its owner is mixed in the videos captured by the corresponding camera during its time range, but we don‚Äôt know which video corresponds to, since the association between videos and wireless fragments is unknown. This weak physical connection brings great challenges for us to use it to assist person reidentiÔ¨Åcation. In this work, we propose a new task, which is to use wireless positioning trajectories to assist unsupervised per son reidentiÔ¨Åcation under weak scene labeling. To handle the challenges in this new task, we devise a novel unsu pervised multimodal training framework (UMTF), which contains a multimodal data association strategy (MMDA) and a multimodal graph neural network (MMGN). MMDA constructs detailed multimodal data associations through an adaptive clustering method. MMGN passes multimodal messages in the video graph by adaptively learning the ad jacency matrix from multimodal data. By using these mod ules to mine the potential clues in unlabeled multimodal data, UMTF improves the model quality progressively. The experimental results conducted on two challenging datasets, i.e.WPReID and Campus4K, demonstrate the effectiveness of our method. Fig. 2. The occlusion, blur and clothing changes examples on existing person reidentiÔ¨Åcation datasets, which introduce many challenges for existing methods that rely on visual data. Each row in the Ô¨Ågure contains two video sequences belonging to the same person, each with three frames. 2 R ELATED WORKS "
494,"Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters.txt","Motivated by the success of ensembles for uncertainty estimation in
supervised learning, we take a renewed look at how ensembles of $Q$-functions
can be leveraged as the primary source of pessimism for offline reinforcement
learning (RL). We begin by identifying a critical flaw in a popular algorithmic
choice used by many ensemble-based RL algorithms, namely the use of shared
pessimistic target values when computing each ensemble member's Bellman error.
Through theoretical analyses and construction of examples in toy MDPs, we
demonstrate that shared pessimistic targets can paradoxically lead to value
estimates that are effectively optimistic. Given this result, we propose MSG, a
practical offline RL algorithm that trains an ensemble of $Q$-functions with
independently computed targets based on completely separate networks, and
optimizes a policy with respect to the lower confidence bound of predicted
action values. Our experiments on the popular D4RL and RL Unplugged offline RL
benchmarks demonstrate that on challenging domains such as antmazes, MSG with
deep ensembles surpasses highly well-tuned state-of-the-art methods by a wide
margin. Additionally, through ablations on benchmarks domains, we verify the
critical significance of using independently trained $Q$-functions, and study
the role of ensemble size. Finally, as using separate networks per ensemble
member can become computationally costly with larger neural network
architectures, we investigate whether efficient ensemble approximations
developed for supervised learning can be similarly effective, and demonstrate
that they do not match the performance and robustness of MSG with separate
networks, highlighting the need for new efforts into efficient uncertainty
estimation directed at RL.","OfÔ¨Çine reinforcement learning (RL), also referred to as batch RL (Lange et al., 2012), is a problem setting in which one is provided a dataset of interactions with an environ ment in the form of a Markov decision process (MDP), and the goal is to learn an effective policy exclusively from this Ô¨Åxed dataset. OfÔ¨Çine RL holds the promise of data efÔ¨Åciency through data reuse, and improved safety due to minimizing the need for policy rollouts. As a result, ofÔ¨Çine RL has been the subject of signiÔ¨Åcant renewed interest in the machine learning literature (Levine et al., 2020). One common approach to ofÔ¨Çine RL in the modelfree set ting is to use approximate dynamic programming (ADP) to learn aQvalue function via iterative regression to backed uptarget values . The predominant algorithmic philosophy with most success in ADPbased ofÔ¨Çine RL is to encour age obtained policies to remain close to the support set of the available ofÔ¨Çine data. A large variety of methods have been developed for enforcing such constraints, examples of which include regularizing policies with behavior cloningobjectives (Kumar et al., 2019; Fujimoto & Gu, 2021), per forming updates only on actions observed inside (Peng et al., 2019; Nair et al., 2020; Wang et al., 2020; Ghasemipour et al., 2021) or close to (Fujimoto et al., 2019) the ofÔ¨Çine dataset, and regularizing value functions to underestimate the value of actions not seen in the dataset (Wu et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021a). The need for such regularizers arises from inevitable inac curacies in value estimation when function approximation, bootstrapping, and offpolicy learning ‚Äì i.e. The Deadly Triad (Van Hasselt et al., 2018) ‚Äì are involved. In ofÔ¨Çine RL in particular, such inaccuracies cannot be resolved through additional interactions with the MDP. Thus, remaining close to the ofÔ¨Çine dataset limits opportunities for catastrophic in accuracies to arise. However, recent works have argued that the aforementioned constraints can be overly pessimistic, and instead opt for approaches that take into consideration theuncertainty about the value function (Buckman et al., 2020; Jin et al., 2021; Xie et al., 2021), thus refocusing the ofÔ¨Çine RL problem to that of deriving accurate lower conÔ¨Ådence bounds (LCB) of Qvalues.arXiv:2205.13703v1  [cs.LG]  27 May 2022Why So Pessimistic? OfÔ¨Çine RL through Ensembles, and Why Their Independence Matters In the empirical supervised learning literature, deep ensem bles1and their more efÔ¨Åcient variants have been shown to be the most effective approaches for uncertainty estimation, to wards learning calibrated estimates and conÔ¨Ådence bounds with modern neural network function approximators (Ova dia et al., 2019). Motivated by this, in our work we take a renewed look into Qensembles, and study how to leverage them as the primary source of pessimism for ofÔ¨Çine RL. In deep RL, a very popular algorithmic choice is to use an ensemble of Qfunctions to obtain pessimistic value es timates and combat overestimation bias (Fujimoto et al., 2018). SpeciÔ¨Åcally, in the policy evaluation procedure, all Qnetworks are updated towards a shared pessimistic tem poral difference target . Similarly in ofÔ¨Çine RL, in addition to the main ofÔ¨Çine RL objective that they propose, several existing methods use such Qensembles (Wu et al., 2019; Kumar et al., 2019; Agarwal et al., 2020; Smit et al., 2021; An et al., 2021; Lee et al., 2021; 2022; Ghasemipour et al., 2021). We begin by mathematically characterizing a critical Ô¨Çaw in the aforementioned ensembling procedure. SpeciÔ¨Åcally, we demonstrate that using shared pessimistic targets can paradoxically lead to Qestimates which are in fact opti mistic ! We verify our Ô¨Ånding by constructing pedagogical toy MDPs. These results demonstrate that the formulation of using shared pessimistic targets is fundamentally illformed. To resolve this problem, we propose Model Standard deviation Gradients (MSG), an ensemblebased ofÔ¨Çine RL algorithm. In MSG, each Qnetwork is trained indepen dently ,without sharing targets . Crucially, ensembles trained with independent target values will always provide pes simistic value estimates. The pessimistic lowerconÔ¨Ådence bound (LCB) value estimate ‚Äì computed as the mean minus standard deviation of the Qvalue ensemble ‚Äì is then used to update the policy being trained. Evaluating MSG on the established D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020) benchmarks for ofÔ¨Çine RL, we demonstrate that MSG matches, and in the more challenging domains such as antmazes, signiÔ¨Åcantly exceeds the prior stateoftheart. Additionally, through a series of ablation experiments on benchmark domains, we verify the signiÔ¨Åcance of our theoretical Ô¨Åndings, study the role of ensemble size, and highlight the settings in which ensembles provide the most beneÔ¨Åt. The use of ensembles will inevitably be a computational bottleneck when applying ofÔ¨Çine RL to domains requiring 1In the deep learning literature, deep ensembles refers to the setting where the same network architectures are trained using the same data and objective functions, with the only difference in ensemble members being the random weight initialization of the networks.large neural network models. Hence, as a Ô¨Ånal analysis, we investigate whether the favorable performance of MSG can be obtained through the use of modern efÔ¨Åcient ensemble approaches which have been successful in the supervised learning literature (Lee et al., 2015; Havasi et al., 2020; Wen et al., 2020; Ovadia et al., 2019). We demonstrate that while efÔ¨Åcient ensembles are competitive with the stateoftheart on simpler ofÔ¨Çine RL benchmark domains, similar to many popular ofÔ¨Çine RL methods they fail on more challenging tasks, and cannot recover the performance and robustness of MSG using full ensembles with separate neural networks. Our work highlights some of the unique and often over looked challenges of ensemblebased uncertainty estimation in ofÔ¨Çine RL. Given the strong performance of MSG, we hope our work motivates increased focus into efÔ¨Åcient and stable ensembling techniques directed at RL, and that it highlights intriguing research questions for the community of neural network uncertainty estimation researchers whom thus far have not employed sequential domains such as ofÔ¨Çine RL as a testbed for validating modern uncertainty estimation techniques. 2. Related Work "
14,TextileNet: A Material Taxonomy-based Fashion Textile Dataset.txt,"The rise of Machine Learning (ML) is gradually digitalizing and reshaping the
fashion industry. Recent years have witnessed a number of fashion AI
applications, for example, virtual try-ons. Textile material identification and
categorization play a crucial role in the fashion textile sector, including
fashion design, retails, and recycling. At the same time, Net Zero is a global
goal and the fashion industry is undergoing a significant change so that
textile materials can be reused, repaired and recycled in a sustainable manner.
There is still a challenge in identifying textile materials automatically for
garments, as we lack a low-cost and effective technique for identifying them.
In light of this, we build the first fashion textile dataset, TextileNet, based
on textile material taxonomies - a fibre taxonomy and a fabric taxonomy
generated in collaboration with material scientists. TextileNet can be used to
train and evaluate the state-of-the-art Deep Learning models for textile
materials. We hope to standardize textile related datasets through the use of
taxonomies. TextileNet contains 33 fibres labels and 27 fabrics labels, and has
in total 760,949 images. We use standard Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs) to establish baselines for this dataset. Future
applications for this dataset range from textile classification to optimization
of the textile supply chain and interactive design for consumers. We envision
that this can contribute to the development of a new AI-based fashion platform.","Clothing and textiles are ubiquitous in our daily lives. Online shopping elevates individuals to a new level of purchasing experience  customised shopping with tons of options from a global market, simple checkout procedures, convenient delivery and returns. According to market estimates, the global fashion emarket is worth $752.5 billion in 2020 [ 9]. This enormous economic value indicates an increased demand for ecommerce services by individuals. These rising demands in the fashion industry motivate the use of Machine Learning (ML) techniques to facilitate lowlevel pixel recognition, midlevel fashion comprehension and highlevel fashion applications [61]. Higherlevel applications, such as outÔ¨Åt recommendations and virtual tryons [ 23], are supported by lowerlevel fashion tasks, e.g.parsing (segmentation) [ 18,32], landmark detection [ 43],etc.A number of works have then developed apparel related datasets for all levels of fashion tasks, including landmark annotations [ 43,32,18], category classiÔ¨Åcation [21, 73, 67], attributes labelling [32, 26], recognitionbased retrieval [31, 23, 22], etc. Despite the development of ML techniques in the fashion industry, the textile industry still faces challenges in its chase of a more sustainable model to reduce the enormous volumes of textile wastes and to meet the global Net Zero goal [ 16,4]. Textile materials play a critical role in garments due to the fact that it is selected based on their particular properties, which may include the level of comfort they provide and the degree to which they can be recycled, etc.[40]. Millions of tons of garments end up in landÔ¨Åll every year [ 48]. Textiles circularity, a novel conceptual model of the circular economy, demonstrates an option for the fashion industry to reduce its carbon footprint and costs, maximise the life of textiles, and minimise waste. Textiles can bearXiv:2301.06160v1  [cs.DL]  15 Jan 2023GARMENTS RAW   MATERIAL FIBRE FABRIC TEXTILES ManufacturePlant Animal Synthetic RegeneratedFibre types cotton ... wool ... polyeste ... viscose ...FIBRE FABRICWoven Knitted NonwovenTwill ...Fabric types Jersey ... Felt ...Outerwear Top Skirt ... Wool Aramids Flax Milk Casein Denim Tweed Velvet TaffetaPlant / Cellulose Animal / Protein Petroleum WasteFigure 1: The general production Ô¨Çow of textiles. Textilesis an umbrella term and it includes raw material, Ô¨Åbre, fabric and garments etc(indicated in dashed box). The dataset labels are generated from two textile material taxonomies: a Ô¨Åbre taxonomy (Figure 2) and a fabric taxonomy (in Supplementary material). Fibres have four macrotypes and we show several Ô¨Åbre examples (Ô¨Çax, wool, aramids, and milk casein) in the Ô¨Ågure. Fabrics have three macrotypes of production methods, we also show several fabric examples (denim, tweed, velvet and taÔ¨Äeta) . reused at many levels, being regenerated into new Ô¨Åbres or utilised textile wastes as energy fuel etc., thereby reducing the carbon footprint [ 45]. It is a recommended practice to ensure that textiles can be traced back to their original source so that the recycling process can be guaranteed. Yet, nowadays, textiles are mostly sorted manually [ 51], despite recent research raise using near infrared spectroscopy (NIR spectroscopy) to recognize textiles for automated garments sorting line [ 10].A lowcost, higheÔ¨Éciency technique for the automatic identiÔ¨Åcation of textile materials in garments is missing, so that the digitised fashion sector would be able to retrieve the materials they are composed of. This would help reduce a large amount of textile wastes and carbon emissions [15]. Given the signiÔ¨Åcance of textile material identiÔ¨Åcation in clothing, it is worth noticing that this identiÔ¨Åcation process can be complicated because Ô¨Åbre and fabric refer to diÔ¨Äerent textile materials. Fibre is the material to make fabric, however, most existing fashion datasets in ML mixed them in the same class. No dataset presently contains organized textile material labels; they do not provide a systematic picture of materialrelated labels [43,21]. They contain partial textile material attributes; their annotation scheme lacks a rationale and is not systematically reviewed by material scientists. Here we propose TextileNet, a material taxonomybased fashion textile dataset to close the gap in current research on textile material identiÔ¨Åcation in clothing. We developed the TextileNet dataset (illustrated in Figure 1 and the detailed illustration is in Figure 2) based on Ô¨Åbre and fabric labels. We achieved this through carefully designing two taxonomies, one for Ô¨Åbres and another for fabrics in close collaboration with material domain experts. We discuss the design details in Section 2.3. The goal of this TextileNet dataset is to contribute to textile material identiÔ¨Åcation in the fashion industry and imagebased textile material retrieval, at the same time, standardize the digitized textile material labelling . TextileNet can be deployed in various domains, including material science, fashion design, retails and the textile supply chain, etc. Our contributions are: ‚Ä¢We present a Ô¨Åbre taxonomy and a fabric taxonomy created in collaboration with material domain experts; thesetaxonomiescontainmacrotypesoftextilesandareextendableforfuturenewÔ¨Åbre/fabric types; ‚Ä¢Using the labels from these taxonomies, we collect and build material taxonomybased fashion datasets for Ô¨Åbre and fabric. The built datasets, named TextileNet, TextileNetÔ¨Åbre contains 33 Ô¨Åbre labels, 27 fabric labels in TextileNetfabric, and have 760,949 images; 2‚Ä¢We present and report two baseline models (CNNs and Vision Transformers) for Ô¨Åbre and fabric classiÔ¨Åcation, both models achieve >80%top5 accuracy on our datasets. 2 Related work "
414,Noise2Blur: Online Noise Extraction and Denoising.txt,"We propose a new framework called Noise2Blur (N2B) for training robust image
denoising models without pre-collected paired noisy/clean images. The training
of the model requires only some (or even one) noisy images, some random
unpaired clean images, and noise-free but blurred labels obtained by predefined
filtering of the noisy images. The N2B model consists of two parts: a denoising
network and a noise extraction network. First, the noise extraction network
learns to output a noise map using the noise information from the denoising
network under the guidence of the blurred labels. Then, the noise map is added
to a clean image to generate a new ""noisy/clean"" image pair. Using the new
image pair, the denoising network learns to generate clean and high-quality
images from noisy observations. These two networks are trained simultaneously
and mutually aid each other to learn the mappings of noise to clean/blur.
Experiments on several denoising tasks show that the denoising performance of
N2B is close to that of other denoising CNNs trained with pre-collected paired
data.","Image denoising, which aims to restore a highquality image from its degraded observation, is a fundamental problem in image processing. In many imaging systems [11, 20, 34, 15], image noise comes from multiple sources, such as capturing instruments, data transmission media or subsequent postprocessing. Such complicated generation processes makes it difÔ¨Åcult to estimate the latent noise model and recover the clean image from the noisy observation. A large variety of denoising algorithms have been developed to deal with image noise. Traditional denoising methods ( e.g.BM3D [8], WNNM [13]) exploit a property of the noise or image structure to help denoising. These methods require accurate image model deÔ¨Ånitions, thus performance is limited in realworld cases. Modern denoising methods often employ convolutional neural networks (CNNs) to learn the mapping function from noise to clean on a large collection of noisy/clean image pairs. The performance of CNN denoisers are highly dependent on whether the distributions of training noise and test noise are well matched. Since pairs of real noisy/clean images are difÔ¨Åcult to obtain, CNN denoisers are mostly trained on synthesized data. In addition, the real noise degradation process is usually complex or unknown, so that the synthesized noise distribution can deviate severely from the 1arXiv:1912.01158v2  [eess.IV]  14 May 2020(a) SSIM, PSNR j0.981, 39.34dB  (b) 0.976, 38.25dB  (c)0.828, 28.95dB  (d) 0.968, 37.26dB Figure 1. Comparison of different training schemes. (a) Traditionally, the training of denoising network requires a large amount of paired noisy/clean images. (b) Noise2Noise [22] trains the network using pairs of independent noisy measurements of the same target. (c) Noise2V oid [17] uses just individual noisy images as training data. (d) Our Noise2Blur needs some unpaired noise and clean images, as well as blurred counterparts of the noisy images to generate supervision. real noise distribution. As a result, CNN denoisers are easily overÔ¨Åtted to the speciÔ¨Åc synthetic noise (e.g.Gaussian noise, Poisson noise) and generalize poorly to the realworld noisy images. In this paper, we propose Noise2Blur (N2B), a training scheme that overcomes the above problems. The training of N2B model does not need access to estimation of noise and precollected paired data. It only requires some unpaired noisy images and clean images, which is easy to implement in most practical applications. Although the images we have are unpaired, we can extract information from them to generate supervision for the denoising process. Our N2B model consists of two subnetworks, i.e.denoising and noise extraction. The noisy inputs are Ô¨Årst transformed into noisefree but blurred images by general Ô¨Åltering techniques ( e.g. Gaussian Ô¨Ålter, median Ô¨Ålter). We use the noisy/blurred image pair to guide the noise extraction network to roughly extract noise from its input (‚Äúnoiseto blur‚Äù). The denoising network is then trained using a new noisy/clean image pair obtained by adding the extracted noise to a random clean image (‚Äúnoisetoclean‚Äù). With a simple gradient interruption operation, the denoising network eventually converges to the ‚Äúnoisetoclean‚Äù objective, while the noise extraction network learns to Ô¨Ånely extract the noise. On the other hand, the training of N2B model has no requirement on the number of noisy images; even if only one noisy image of size 512512is available, a denosing network with strong generalization can be trained. Through experiments on several datasets, we demonstrate the effectiveness of N2B. Although we train with unpaired data, the denoising performance of the N2B model is close to that of other denoising CNNs trained with precollected paired data. 2. Related work "
178,Robustness study of noisy annotation in deep learning based medical image segmentation.txt,"Partly due to the use of exhaustive-annotated data, deep networks have
achieved impressive performance on medical image segmentation. Medical imaging
data paired with noisy annotation are, however, ubiquitous, but little is known
about the effect of noisy annotation on deep learning-based medical image
segmentation. We studied the effects of noisy annotation in the context of
mandible segmentation from CT images. First, 202 images of Head and Neck cancer
patients were collected from our clinical database, where the organs-at-risk
were annotated by one of 12 planning dosimetrists. The mandibles were roughly
annotated as the planning avoiding structure. Then, mandible labels were
checked and corrected by a physician to get clean annotations. At last, by
varying the ratios of noisy labels in the training data, deep learning-based
segmentation models were trained, one for each ratio. In general, a deep
network trained with noisy labels had worse segmentation results than that
trained with clean labels, and fewer noisy labels led to better segmentation.
When using 20% or less noisy cases for training, no significant difference was
found on the prediction performance between the models trained by noisy or
clean. This study suggests that deep learning-based medical image segmentation
is robust to noisy annotations to some extent. It also highlights the
importance of labeling quality in deep learning","Deep supervised networks have achieved impressive performance in medical image segmentation partly due to the  use of high quali ty exhaustive annotated data (Hesamian  et al. , 2019 ; Chen  et al. , 2019 ; Liu et al. , 2017 ). However,  in radiation oncology, it is hard or impossible to conduct sufficient high quality image annotation. Besides potential  hurdles of funding acquisitions, time cost and patient privacy, accurate ann otation of medical images always requires  scarce and  expensive medical expertise (Greenspan  et al. , 2016 ) and thereby, medical imaging data paired with noisy  annotation is prevalent, particularly in radiation oncology.    2                                                     S. Yu  et al.   2   An increasing attention has been  paid to the issue of label noise in deeply supervised image classification (Han et  al., 2019 ; Hendrycks  et al. , 2018 ; Tanaka  et al. , 2018 ). These approaches to tackle the label noise could be generally  categorized into two g roups. One tends to analyze the label noise and to develop deep networks with noise robust  loss functions. Reed et al propose d a generic way to tackle inaccurate labels by augmenting the prediction objective  function with a noti on of perceptual consistency  (Reed  et al. , 2014 ). The consistency was defined as the confidence  of predicted labels between different objective estimation computed from the same input data. Further, the authors   introduce d a convex combination of the known labels and predicted labels as the training target in self learning.  Patrini et al present ed two procedures for loss function correction , and both the application domain and the network  architecture  were unknown  (Patrini  et al. , 2017 ). The computing cost is at most a matrix inversion and multiplication.  Both procedures were  prove n to be robust to the noisy data , and importantly, the Hessian of the loss function was  found independent from label noise for the ReLU networks. By generalizing t he categorical cross entropy, Zhang and  Sabuncu develop ed a theoretically grounded set of noise robust loss functions (Zhang and Sabuncu, 2018 ). These  functions could be embedded into any deep ne tworks to yield good performance in a wide range of noisy label  scenarios. And n otably, Luo et al design ed a variance regularization term to penali ze the Jacobian norm of a deep  netwo rk on the whole training set (Luo et al. , 2019 ). Both theoretical ly deduc ed and experimental results show ed that  the regul arization term can decrease the subspace dimensionality, improve the robustness , and generalize well to  label noise.  However,  these approaches require prior knowledge or an accurate estimation of the label noise  distribution , which is not practical in real world applications. The other group tends to figure out and to remove or  correct noisy labels by using a small set of clean data. Misra et al demonstrate d that noisy labels from human centric  annotation are statistically dep endent on the data , and thus, clean labels could be learnt to decouple this kind of  human reporting bias and to improve i mage captioning performance (Misra  et al. , 2016 ). Xiao et al introduce d a  general framework to train deep networks with a limited number of clean samples an d massive  noisy samples (Xiao   et al. , 2015 ). The relationships among  images, class labels , and label noises were  quantified with a probabilistic  graphical model , which was further integrated into an end toend deep learning system. Mirikharaji et al propose d a  practical framework to learn from a limited number of clean samples in the training phase that assign ed higher  weights to pixels with gradient directions closer to tho se of clean data in a meta learning approach (Mirikharaji  et al. ,  2019 ). This kind of approaches  is feasible  but significantly increase s the computing complexity.     Many efforts have been made to tackle label noise in image classification, while little is known about the effect of  annotation quality on object segmentation. Object segmentation can be viewed as pixel wise image classification and  requires highquality exhaustive annotated data for algorithm training . However, in radiation oncology, some organs  atrisk (OARs)  may be roughly  annotated due to the trade off between time spent  and radiati on treatment planning   quality . Such  rough annotation s can mislead deep network  training and result in ambiguous localization of  anatomical structures. This study concerns t he effect of annotation quality on medical image segmentation . It  involves  medical  image  data annotated by dosimetrists in radiation treatment planning and differ s from the 3                                                     S. Yu  et al.   3 aforementioned studies , which  artificially generate noisy labels  and do not reflect reallife scenarios. The primary  purpose of this study is to investigate  whether a deep network trained with noisy data can achieve comparative  performance as that trained with clean data. Specifically, the effect of different ratios  of noisy cases in the training  data is investigated in the context of deep learning based mandi ble image segmentation.     2. Methods and M aterials   "
19,Two Routes to Scalable Credit Assignment without Weight Symmetry.txt,"The neural plausibility of backpropagation has long been disputed, primarily
for its use of non-local weight transport $-$ the biologically dubious
requirement that one neuron instantaneously measure the synaptic weights of
another. Until recently, attempts to create local learning rules that avoid
weight transport have typically failed in the large-scale learning scenarios
where backpropagation shines, e.g. ImageNet categorization with deep
convolutional networks. Here, we investigate a recently proposed local learning
rule that yields competitive performance with backpropagation and find that it
is highly sensitive to metaparameter choices, requiring laborious tuning that
does not transfer across network architecture. Our analysis indicates the
underlying mathematical reason for this instability, allowing us to identify a
more robust local learning rule that better transfers without metaparameter
tuning. Nonetheless, we find a performance and stability gap between this local
rule and backpropagation that widens with increasing model depth. We then
investigate several non-local learning rules that relax the need for
instantaneous weight transport into a more biologically-plausible ""weight
estimation"" process, showing that these rules match state-of-the-art
performance on deep networks and operate effectively in the presence of noisy
updates. Taken together, our results suggest two routes towards the discovery
of neural implementations for credit assignment without weight symmetry:
further improvement of local rules so that they perform consistently across
architectures and the identification of biological implementations for
non-local learning mechanisms.","Backpropagation is the workhorse of modern deep learning and the only known learning algorithm that allows multi layer networks to train on largescale tasks. However, any exact implementation of backpropagation is inherently non local, requiring instantaneous weight transport in which backward errorpropagating weights are the transpose of the forward inference weights. This violation of locality is biologically suspect because there are no known neural mechanisms for instantaneously coupling distant synaptic weights. Recent approaches such as feedback alignment (Lillicrap et al., 2016) and weight mirror (Akrout et al., 2019) have identiÔ¨Åed circuit mechanisms that seek to ap proximate backpropagation while circumventing the weight transport problem. However, these mechanisms either fail to operate at largescale (Bartunov et al., 2018) or, as we demonstrate, require complex and fragile metaparameter scheduling during learning. Here we present a unifying framework spanning a space of learning rules that allows for the systematic identiÔ¨Åcation of robust and scalable alter natives to backpropagation. To motivate these rules, we replace tied weights in back propagation with a regularization loss on untied forward and backward weights. The forward weights parametrize the global cost function, the backward weights specify a descent direction, and the regularization constrains the relationship between forward and backward weights. As the system iterates, forward and backward weights dynamically align, giving rise to a pseudogradient. Different regularization terms are possible within this framework. Critically, these regularization terms decompose into geometrically natural primitives, which can be parametrically recombined to con struct a diverse space of credit assignment strategies. This space encompasses existing approaches (including feedback alignment and weight mirror), but also elucidates novel learning rules. We show that several of these new strategiesarXiv:2003.01513v2  [qbio.NC]  25 Jun 2020Two Routes to Scalable Credit Assignment without Weight Symmetry are competitive with backpropagation on realworld tasks (unlike feedback alignment), without the need for complex metaparameter tuning (unlike weight mirror). These learn ing rules can thus be easily deployed across a variety of neural architectures and tasks. Our results demonstrate how highdimensional errordriven learning can be robustly per formed in a biologically motivated manner. 2. Related Work "
35,Identification of 1H-NMR Spectra of Xyloglucan Oligosaccharides: A Comparative Study of Artificial Neural Networks and Bayesian Classification Using Nonparametric Density Estimation.txt,"Proton nuclear magnetic resonance (1H-NMR) is a widely used tool for chemical
structural analysis. However, 1H-NMR spectra suffer from natural aberrations
that render computer-assisted automated identification of these spectra
difficult, and at times impossible. Previous efforts have successfully
implemented instrument dependent or conditional identification of these
spectra. In this paper, we report the first instrument independent
computer-assisted automated identification system for a group of complex
carbohydrates known as the xyloglucan oligosaccharides. The developed system is
also implemented on the world wide web (http://www.ccrc.uga.edu) as part of an
identification package called the CCRC-Net and is intended to recognize any
submitted 1H-NMR spectrum of these structures with reasonable signal-to-noise
ratio, recorded on any 500 MHz NMR instrument. The system uses Artificial
Neural Networks (ANNs) technology and is insensitive to the instrument and
environment-dependent variations in 1H-NMR spectroscopy. In this paper,
comparative results of the ANN engine versus a multidimensional Bayes'
classifier is also presented.","1HNMR  spectroscopy.  NMR  (nuclear magnetic  resonance) spectroscopy  is  a widely used  tool  for chemical analysis.  It is  used  to identify  materials,  determine  the  chemical structure of organic compounds, and can be usedto  quantify  chemical  substituents  or  the components of chemical mixtures.  The proton (1H)  is  the  nuclide  that  is  most  frequently observed by NMR.  When a sample is placed in a strong magnetic field, the magnetically active nuclei become aligned.   The resulting  sample magnetization can be manipulated by applying a very brief magnetic field pulse that oscillates at radio frequency (RF).  Such RF pulses perturb the  sample  magnetization,  which  can  be observed  via  its  induction  of  a  current  in  a detector coil as the magnetization relaxes back to  its  equilibrium  state.   The  resulting  ""free induction  decay""  (FID)  contains  information regarding  the  chemical  environment  of  nuclei within  the  chemical  sample,  and  thus  can  be used  to  identify  and  quantitate  individual chemical components of the sample.  The FID consists of a mixture of sinusoidal oscillations in the timedomain with decaying amplitudes. The timedomain signal is normally transformed (usually  using  Fourier  transform)  into  the frequency domain.  Figure 1 (located at the end of  this  article),  illustrates  two  examples  of  a frequency  domain  signal  (spectrum)  of  a xyloglucan  oligosaccharide.  1HNMR spectra, in general, suffer from environmental, instrumental, and other types of variations that manifest themselves in a variety of aberrations.  Low signaltonoise ratio [1, 2, 4], baseline drifts [3, 4, 7], frequency shifts dueto temperature variations,  line broadening and negative  peaks  due  to  phasing  problems,  and malformed peaks (or overlapped peaks) due to inaccurate  shimming,  are  among  the  most prominent  and  common  aberrations.   For example, Figure 1, shows two 1HNMR spectra of  a  complex  carbohydrate.   The  spectrum labeled (B) in this figure suffer from a variety of the  above  mentioned  aberrations,  and contamination by lactate, frequently introduced by  touching  laboratory  glassware  with  bare hands.   It  is  important  to  realize  that  this spectrum by no means represents a worst case scenario, and it does not represent the level of complexity present in the problem of instrument independent identification of 1HNMR spectra of xyloglucan oligosaccharides.  Spectrum (B) is merely  a  demonstration  of  some  types  of possible aberrations. For  the  purpose  of  automated identification of these spectra, elimination of the above mentioned aberrations becomes essential, as they can lead to erroneous identification [1 7].  A variety of signal processing techniques have  been  applied  to  ""clean  up""  1HNMR spectra.  For instance, signal averaging1 [4] and apodization2 [4] have become standard ways of improving the signaltonoise ratio.  To correct baseline problems, a number of techniques have been used such as parametric modeling using a priori  knowledge  [3,  5],  optimal  associative memory  (OAM)  [5],  and  spectral  derivatives [6].  Other mathematical techniques have also 1 In signal averaging a spectrum is recorded several times.  Each recorded signal is referred to as a ‚Äútransient.‚Äù  The final spectrum is the arithmetic average of all the transients.  The hope is that by doing  so the  zero  mean  components  of  the  noise present in the signal will be averaged out. 2 Apodization is a type of low (high) pass filtering  performed  in  the  timedomain. Apodization  is  performed  by  speeding  up,  or slowing  down  the  rate  of  decay  of  timedomain exponential  functions.   This  is  accomplished  by multiplying  the  timedomain  signal  by  another function.  This technique allows the improvement of signaltonoise ratio in exchange for the reduction of signal resolution (or visa versa).been introduced to address each specific type of aberration encountered in 1HNMR spectra.  Although many of these signal processing techniques  have  enjoyed  success  in  specific applications,  they remain solutions  to specific types  of  aberrations.   In  order to  produce an overall  ‚Äúclean‚Äù  spectrum,  one  needs  to  use several  of  these  methods  to  eliminate  the aberrations  present  in  a  real  spectrum. Furthermore, most of these techniques produce side effects that are magnified when improperly processed  by  a  second  signal  processing algorithm.  Furthermore, after the initial signal processing steps have been taken, the task of identifying  the  processed  spectrum  remains. This  is  not  a  trivial  task  as  many  times  the quality of the processed spectrum remains poor, requiring a sophisticated identification system. In  this  paper  we  show  that  instead  of eliminating  all  the  present  aberrations  by  a signal processing procedure as a preprocessor, it is  possible  to  eliminate some  of  them  in  the processing  step,  and  some  in  the  actual identification  step.   Here,  we  show  that  an adaptive  identification  system  can  learn  to effectively ignore some forms of aberrations. Xyloglucan  Oligosaccharides.   Complex carbohydrates  are important  biomolecules  that play a role in many biological functions such as providing physical strength (connective tissue in animals and woody tissue in plants) and as a source of energy reserves (glycogen in animals and starch in plants).  These molecules are also known  to  be  directly  and  widely  involved  in biological recognition and regulatory processes in normal growth and development as well as in disease processes.  The recent discovery of the role  of  complex  carbohydrates  in  disease processes,  and  therefore  drug  development, among others has triggered a large number of studies in order to better understand the role of abnormal  (structurally  altered)  complex carbohydrates in disease development.  For this reason,  an  automated  identification  system  of complex carbohydrates can eliminate the many manhours  wasted  in  duplicated  efforts  in structural  characterization  of  known carbohydrates.A specific group of these molecules from plant  cell  wall  are  called  xyloglucan oligosaccharides.  The 1HNMR spectra of these molecules resemble each other to a great degree, and the experiments in developing an automated identification system for these spectra is a good indicator for the success of such future projects for automated identification of other molecules. 2. Method: "
329,Deep Double Descent via Smooth Interpolation.txt,"The ability of overparameterized deep networks to interpolate noisy data,
while at the same time showing good generalization performance, has been
recently characterized in terms of the double descent curve for the test error.
Common intuition from polynomial regression suggests that overparameterized
networks are able to sharply interpolate noisy data, without considerably
deviating from the ground-truth signal, thus preserving generalization ability.
At present, a precise characterization of the relationship between
interpolation and generalization for deep networks is missing. In this work, we
quantify sharpness of fit of the training data interpolated by neural network
functions, by studying the loss landscape w.r.t. to the input variable locally
to each training point, over volumes around cleanly- and noisily-labelled
training samples, as we systematically increase the number of model parameters
and training epochs. Our findings show that loss sharpness in the input space
follows both model- and epoch-wise double descent, with worse peaks observed
around noisy labels. While small interpolating models sharply fit both clean
and noisy data, large interpolating models express a smooth loss landscape,
where noisy targets are predicted over large volumes around training data
points, in contrast to existing intuition.","The ability of overparameterized deep networks to interpolate noisy data, while at the same time showing good generalization performance (Belkin et al., 2018; Zhang et al., 2018), has been recently characterized in terms of the double descent curve of the test error (Belkin et al., 2019; Geiger et al., 2019). Within this framework, as model size increases, the test error follows the classical biasvariance tradeoÔ¨Ä curve (Geman etal.,1992), peakingasmodelsbecomelargeenoughtoperfectlyinterpolatethetrainingdata, anddecreasing as model size grows further (Belkin et al., 2019). This phenomenon, largely studied in the context of regression (Bartlett et al., 2020; Muthukumar et al., 2020) and random features (Belkin et al., 2020), at present lacks a precise characterization relating interpolation to generalization for deep networks. Current intuition from linear and polynomial regression suggests that, under some hypothesis on the training sample, large overparameterized models are able to perfectly interpolate both cleanly and noisilylabeled samples, without considerably deviating from the groundtruth signal, thus showing good performance de spite overÔ¨Åtting the training data (Muthukumar et al., 2020; Bartlett et al., 2020; Nakkiran et al., 2019a). 1Source code to reproduce our results available at https://github.com/magamba/double_descent 1arXiv:2209.10080v4  [cs.LG]  8 Apr 2023Published in Transactions on Machine Learning Research (04/2023) 0  2 x2 1 012sin(x)+ Prediction Ground Truth Observations (a) 0  2 x2 1 012sin(x)+ Prediction Ground Truth Observations (b) x2 x1LosspathœÄ1 pathœÄ2 pathœÄ3K=1 K=2 K=3 training sample (c) x2 x1Loss (d) Figure 1: Intuition from overparameterized regression. a) Polynomial of large degree, trained with gradient descent to Ô¨Åt noisy scalar data, reproducing the polynomial regression experiment of Nakkiran et al. (2019a), and reÔ¨Çecting common intuition on double descent, suggesting that the generalization ability of large interpolating models is tied to sharply Ô¨Åtting of noisy data, thus resulting in models that do not deviate considerably from the ground truth signal. b) In this work we show that, contrary to intuition, deep networks smoothly interpolate both clean and noisy data, and that improved generalization in the interpolating regime is tied to smoothness of the loss w.r.t. the input variable. Geodesic MC integration. For each base training point, we generate Pgeodesic paths by connecting a sequence of augmentations of increasingstrength,whichweusetocovervolumesofincreasingsizeinthelosslandscapearoundeachtraining point. We compare points that are c) sharply interpolated from those that are d) smoothly interpolated. Figure 1a illustrates this phenomenon, showing a polynomial of large degree that perfectly Ô¨Åts the training data, with predictive function sharply interpolating noisy samples (intuitively corresponding to a spike at each training point), while overall remaining close to the datagenerating function. In this work, we study the emergence of double descent for the test error of deep networks (Nakkiran et al., 2019b) through the lens of smoothness of interpolation of the training data, as model size as well as the number of training epochs vary, for models trained in practice. To quantify smoothness of interpolation, we conduct an empirical exploration of the loss landscape w.r.t. the input variable, by providing explicit measures of sharpness of the loss, focusing on image classiÔ¨Åcation. Due to the inherently noisy nature of Euclidean estimators in pixel space, and following the manifold hy pothesisPope et al. (2020); Bengio (2013); Narayanan & Mitter (2010), postulating that natural data lies on a combination of manifolds of lower dimension than the input data‚Äôs ambient dimension, we constrain our measures to the support of the data distribution, locally to each training point. Our empirical study shows that the polynomial intuition in Figure 1a does not hold in practice for deep networks, which instead smoothly interpolate both clean and noisy data (Figure 1b). SpeciÔ¨Åcally, smooth interpolation ‚Äì emerging both for large overparameterized networks and prolonged training ‚Äì results in large models conÔ¨Ådently predicting the (noisy) training targets over large volumes around each training point. Contributions ‚Ä¢We present the Ô¨Årst systematic empirical study of smoothness of the loss landscape of deep networks in relation to overparameterization and interpolation for natural image datasets. ‚Ä¢Starting from inÔ¨Ånitesimal smoothness measures from prior work, we introduce volumetric measures that capture loss smoothness when moving away from training points. ‚Ä¢We develop a geodesic Monte Carlo integration method for constraining our measures to a local approximation of the data manifold, in proximity of each training point. 2Published in Transactions on Machine Learning Research (04/2023) ‚Ä¢We present an empirical study of modelwise and epochwise double descent for neural networks trained without confounders (explicit regularization, data augmentation, batch normalization), as well as for commonlyfound training settings. By decoupling smoothness from generalization, we empirically show that overparameterization promotes inputspace smoothness of the loss landscape. Particularly, we produce practical examples in which smoothness of the learned function of deep networks does not result in improved generalization, highlighting that the implicit regularization eÔ¨Äectofoverparameterizationshouldbestudiedintermsofreducedvariationofthelearnedfunction. 2 Related work "
4,A Simple yet Effective Baseline for Robust Deep Learning with Noisy Labels.txt,"Recently deep neural networks have shown their capacity to memorize training
data, even with noisy labels, which hurts generalization performance. To
mitigate this issue, we provide a simple but effective baseline method that is
robust to noisy labels, even with severe noise. Our objective involves a
variance regularization term that implicitly penalizes the Jacobian norm of the
neural network on the whole training set (including the noisy-labeled data),
which encourages generalization and prevents overfitting to the corrupted
labels. Experiments on both synthetically generated incorrect labels and
realistic large-scale noisy datasets demonstrate that our approach achieves
state-of-the-art performance with a high tolerance to severe noise.","Recently deep neural networks (DNNs) have achieved re markable performance on many tasks, such as speech recog nition [ 1], image classiÔ¨Åcation [ 8], object detection [ 25]. However, DNNs usually need a largescale training dataset to generalize well. Such largescale datasets can be collected by crowdsourcing, web crawling and machine generation with a relative low price, but the labeling may contain er rors. Recent studies [ 34,2] reveal that mislabeled exam ples hurt generalization. Even worse, DNNs can memorize the training data with completely randomlyÔ¨Çipped labels, which indicates that DNNs are prone to overÔ¨Åt noisy training data. Therefore, it is crucial to develop algorithms robust to various amounts of label noise that still obtain good general ization. To address the degraded generalization of training with noisy labels, one direct approach is to reweigh training exam ples [ 24,12,7,17], which is related to curriculum learning. The general idea is to assign important weights to examples with a high chance of being correct. However, there are two major limitations of existing methods. First, imagine an ideal weighting mechanism. It will only focus on the Work done during an internship in Google Cloud AI.selected clean examples. For those incorrectly labeled data samples, the weights should be near zero. If a dataset is under 80% noise corruption, an ideal weighting mechanism assigns nonzero weights to only 20% examples and aban dons the information in a large amount of 80% examples. This leads to an insufÔ¨Åcient usage of training data. Second, previous methods usually need some prior knowledge on the noise ratio or the availability of an additional clean unbiased validation dataset. But it is usually impractical to get this extra information in real applications. Another approach is correctionbased, i.e. estimating the noisy corruption ma trix and correcting the labels [ 22,23,6]. But it is often difÔ¨Åcult to estimate the underlying noise corruption matrix when the number of classes is large. Further, there may not be an underlying ground truth corruption process but an open set of noisy labels in the real world. Although many complex approaches [ 12,24,7] have been proposed to deal with label noise, we Ô¨Ånd that a simple yet effective baseline can achieve surprisingly good performance compared to the strong competing methods. In this paper, we propose to minimize the predictive vari ance, which is an unbiased estimator of Jacobian norm. A model with simpler hypothesis and smoother decision bound aries is assumed to generalize better. Our method is simple yet effective which can take advantage of the whole dataset including the noisy examples to improve the generalization. Our main contributions are: We propose a new strong baseline method for robust ness to noisy labels, which greatly mitigates overÔ¨Åtting and should not be omitted in the label noise community. A thorough empirical evaluation on various datasets (e.g., CIFAR10, CIFAR100, ImageNet) is conducted and demonstrates signiÔ¨Åcant improvements over previ ous competing methods. We also apply our method to a largescale realworld noisy dataset, Webvision [ 15], and establish the new stateoftheart results. We show that the variancebased regularizer is an un biased estimator of Jacobian norm and analyze the re liability of this estimator. Its good performance is due to that Jacobian norm correlates with generalization. 1arXiv:1909.09338v2  [cs.LG]  27 Sep 2019The method can be applied to any neural network archi tecture. Additional knowledge on the clean validation dataset is not required. Empirically we Ô¨Ånd that our method learns a model with lower subspace dimensionality and lower complexity, which are the indicators of better generalization. 2. Related work "
581,Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource Sequence Labeling.txt,"In this paper, we address the problem of effectively self-training neural
networks in a low-resource setting. Self-training is frequently used to
automatically increase the amount of training data. However, in a low-resource
scenario, it is less effective due to unreliable annotations created using
self-labeling of unlabeled data. We propose to combine self-training with noise
handling on the self-labeled data. Directly estimating noise on the combined
clean training set and self-labeled data can lead to corruption of the clean
data and hence, performs worse. Thus, we propose the Clean and Noisy Label
Neural Network which trains on clean and noisy self-labeled data simultaneously
by explicitly modelling clean and noisy labels separately. In our experiments
on Chunking and NER, this approach performs more robustly than the baselines.
Complementary to this explicit approach, noise can also be handled implicitly
with the help of an auxiliary learning task. To such a complementary approach,
our method is more beneficial than other baseline methods and together provides
the best performance overall.","For many lowresource languages or domains, only small amounts of labeled data exist. Raw or unlabeled data, on the other hand, is usually avail able even in these scenarios. Automatic annota tion or distant supervision techniques are an option to obtain labels for this raw data, but they often require additional external resources like human generated lexica which might not be available in a lowresource context. Selftraining is a popu lar technique to automatically label additional text. There, a classiÔ¨Åer is trained on a small amount of labeled data and then used to obtain labels for xThis work was started while the authors were at Saarland University.unlabeled instances. However, this can lead to unreliable or noisy labels on the additional data which impede the learning process (Pechenizkiy et al., 2006; Nettleton et al., 2010). In this pa per, we focus on overcoming this slowdown of selftraining. Hence, we propose to apply noise reduction techniques during selftraining to clean the selflabeled data and learn effectively in a low resource scenario. Inspired by the improvements shown by the Noisy Label Neural Network ( NLNN , Bekker and Goldberger (2016)), we can directly apply NLNN to the combined set of the existing clean data and the noisy selflabeled data. However, such an ap plication can be detrimental to the learning pro cess (Section 6). Thus, we introduce the Clean and Noisy Label Neural Network ( CNLNN ) that treats the clean and noisy data separately while training on them simultaneously (Section 3). This approach leads to two advantages over NLNN (Section 6 and 7) when evaluating on two sequencelabeling tasks, Chunking and Named Entity Recognition. Firstly , when adding noisy data, CNLNN is robust showing consistent im provements over the regular neural network, whereas NLNN can lead to degradation in per formance. Secondly , when combining with an indirectnoise handling technique, i.e. with an auxiliary target in a multitask fashion, CNLNN complements better than NLNN in the multitask setup and overall leads to the best performance. 2 Related Work "
446,The Group Loss++: A deeper look into group loss for deep metric learning.txt,"Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that ""similar objects
should belong to the same group"", the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We design a set of inference strategies tailored towards our
algorithm, named Group Loss++ that further improve the results of our model. We
show state-of-the-art results on clustering and image retrieval on four
retrieval datasets, and present competitive results on two person
re-identification datasets, providing a unified framework for retrieval and
re-identification.","MEasuring object similarity is at the core of many important machine learning problems like clustering and object retrieval. For visual tasks, this means learning a distance function over images. With the rise of deep neural networks, the focus has rather shifted towards learning a feature embedding that is easily separable using a simple distance function, such as the Euclidean distance. In essence, objects of the same class (similar) should be close by in the learned manifold, while objects of a different class (dissimi lar) should be far away. Historically, the best performing approaches get deep feature embeddings from the socalled siamese networks [1], which are typically trained using the contrastive loss [1] or the triplet loss [2], [3]. A clear drawback of these losses is that they only consider pairs or triplets of data points, missing key information about the relationships between all members of the minibatch. On a minibatch of size n, despite that the number of pairwise relations between sam ples isO(n2), contrastive loss uses only O(n=2)pairwise relations, while triplet loss uses O(2n=3)relations. Addi tionally, these methods consider only the relations between objects of the same class (positives) and objects of other classes (negatives), without making any distinction that negatives belong to different classes. This leads to not taking into consideration the global structure of the embedding space, and consequently results in lower clustering and retrieval performance. To compensate for that, researchers I.E, J.S, L.W and L.L.T are with Dynamic Vision and Learning Group at the Technical University of Munich, S.V, A.T and M.P are at Ca‚Äô Foscari University of Venice Corresponding authors: Ismail Elezi (ismail.elezi@tum.de), Laura Leal Taix¬¥ e (leal.taixe@tum.de) * denotes equal contributionrely on other tricks to train neural networks for deep metric learning: intelligent sampling [4], multitask learning [5] or hardnegative mining [6]. Recently, researchers have been increasingly working towards exploiting in a principled way the global structure of the embedding space [7], [8], [9], [10], typically by designing ranking loss functions instead of following the classic triplet formulations. In a similar spirit, we propose Group Loss , a novel loss function for deep metric learning that considers the sim ilarity between all samples in a minibatch. To create the minibatch, we sample from a Ô¨Åxed number of classes, with samples coming from a class forming a group . Thus, each minibatch consists of several randomly chosen groups, and each group has a Ô¨Åxed number of samples. An iterative, fullydifferentiable label propagation algorithm is then used to build feature embeddings which are similar for samples belonging to the same group, and dissimilar otherwise. At the core of our method lies an iterative process called replicator dynamics [11], [12], that reÔ¨Ånes the local information, given by the softmax layer of a neural network, with the global information of the minibatch given by the similarity between embeddings. The driving rationale is that the more similar two samples are, the more they affect each other in choosing their Ô¨Ånal label and tend to be grouped together in the same group, while dissimilar samples do not affect each other on their choices. We then study the embedding space generated by net works trained with our Group Loss, resulting in a few observations that we exploit by introducing a set of in ference strategies. We call this model, the Group Loss++ and show that reaches signiÔ¨Åcantly better results than the Group Loss , making clustering and image retrieval easier. Finally, we show that our proposed model can be used to train networks in the Ô¨Åeld of person reidentiÔ¨Åcation,arXiv:2204.01509v1  [cs.CV]  4 Apr 20222 thus providing a similarity learning uniÔ¨Åed framework that works both for retrieval and reidentiÔ¨Åcation. Our contribution in this work is Ô¨Åvefold: We propose the Group Loss , a novel loss function to train neural networks for deep metric embedding that takes into account the local information of the samples, as well as their similarity. We propose a differentiable labelpropagation iter ative model to embed the similarity computation within backpropagation, allowing endtoend train ing with our new loss function. We introduce a set of inference strategies, resulting in Group Loss++ that improve the results of our model. We show stateoftheart qualitative and quantita tive results in four standard clustering and retrieval datasets. We show competitive results on two person re identiÔ¨Åcation datasets, thus providing a uniÔ¨Åed framework for similarity learning. 2 R ELATED WORK "
39,Towards Quantifying Intrinsic Generalization of Deep ReLU Networks.txt,"Understanding the underlying mechanisms that enable the empirical successes
of deep neural networks is essential for further improving their performance
and explaining such networks. Towards this goal, a specific question is how to
explain the ""surprising"" behavior of the same over-parametrized deep neural
networks that can generalize well on real datasets and at the same time
""memorize"" training samples when the labels are randomized. In this paper, we
demonstrate that deep ReLU networks generalize from training samples to new
points via piece-wise linear interpolation. We provide a quantified analysis on
the generalization ability of a deep ReLU network: Given a fixed point
$\mathbf{x}$ and a fixed direction in the input space $\mathcal{S}$, there is
always a segment such that any point on the segment will be classified the same
as the fixed point $\mathbf{x}$. We call this segment the $generalization \
interval$. We show that the generalization intervals of a ReLU network behave
similarly along pairwise directions between samples of the same label in both
real and random cases on the MNIST and CIFAR-10 datasets. This result suggests
that the same interpolation mechanism is used in both cases. Additionally, for
datasets using real labels, such networks provide a good approximation of the
underlying manifold in the data, where the changes are much smaller along
tangent directions than along normal directions. On the other hand, however,
for datasets with random labels, generalization intervals along mid-lines of
triangles with the same label are much smaller than those on the datasets with
real labels, suggesting different behaviors along other directions. Our
systematic experiments demonstrate for the first time that such deep neural
networks generalize through the same interpolation and explain the differences
between their performance on datasets with real and random labels.","In recent years, deep neural networks have improved the state of the art performance substantially in computer vision [He et al., 2016a; Krizhevsky et al. , 2012; Salman and Liu, 2019], machine translation [Sutskever et al. , 2014], speech recog nition [Graves et al. , 2013], healthcare [Miotto et al. , 2017; Salman et al. , 2019] and game playing [Silver et al. , 2017] among other applications. However, the underlying mecha nisms that enable them to perform well are still not well un derstood. Even though they typically have more parameters than the training samples and exhibit very large capacities, they generalize well on real datasets trained via stochastic gradient descent or its variants. In an insightful paper, Zhang et al. [2016] have identiÔ¨Åed a number of intriguing phenom ena of such networks. In particular, they demonstrate that overparametrized neural networks can achieve 100% accu racy trained on datasets with the original labels and general ize well. At the same time, the exact same neural network architectures can also achieve 100% accuracy on the datasets with random labels, and therefore ‚Äúmemorize‚Äù the training samples. Clearly, this is not consistent with statistical learn ing theory [Vapnik, 1998] and biasvariance tradeoff [Geman et al. , 1992], where models should match the (unknown) ca pacity of the underlying processes in order to generalize well. Understanding and explaining this typical behavior of deep neural networks has attracted a lot of attention recently with the hope of revealing the underlying mechanisms of how deep neural networks generalize. Fundamentally, while training, deep neural networks iter atively minimize a loss function deÔ¨Åned as the sum of the loss on the training samples. The parameters in the trained network depend on the initial parameter values, the optimiza tion process, and training data. As 100% accuracy on the training samples can be achieved even with random labels, Ô¨Ånding good solutions for such networks that minimize the loss is therefore not a key issue. While regularization tech niques can affect the parameters of trained networks, Zhang et al. [2016] have demonstrated that their effects are typi cally small, suggesting that they are not a key component. Therefore, the generalization performance of a trained over parametrized network should depend on the training data and network architecture. In this paper, we focus on deep ReLU networks. We show that such networks generalize consis tently and reliably by interpolating among the training points.arXiv:1910.08581v1  [cs.LG]  18 Oct 2019Using generalization intervals deÔ¨Åned as the range of the data that have the same classiÔ¨Åcation along a direction, we dis cover that pairwise generalization intervals on datasets with real and random labels are almost identical for high dimen sional inputs (e.g., MNIST and CIFAR10 samples). Further more, we show that pairwise interpolations approximate the underlying manifold in the data well, enabling the networks to generalize well. We show that the properties are remark ably consistent among networks with different architectures and on different datasets. The properties enable us to char acterize the generalization performance of neural networks based on their behaviors on the training sets only, which we call intrinsic generalization. This notion of generalization is very different from the typical deÔ¨Ånition of the performance gap on the training set and test set. While intrinsic generaliza tion of a network on a training set can be quantiÔ¨Åed through generalization intervals, the gapbased generalization perfor mance can not be studied without having a validation set or test set. Furthermore, the gapbased deÔ¨Ånition is extrinsic as it can vary when a different validation set is used. In other words, for the Ô¨Årst time, we demonstrate the underlying mechanisms that enable overparametrized networks to gen eralize well when all the training samples are classiÔ¨Åed cor rectly. The systematic results demonstrate the effectiveness of the proposed method and therefore validate the proposed solution. The rest of the paper is organized as follows. In the next section, we review recent works that are closely related to our study. After that, we present the theoretical foundation of the generalization mechanism via interpolation for deep ReLU networks. We introduce a novel notation called generalization interval (GI) to quantify the generalization of such networks. Then, we illustrate our proposed ideas on intrinsic generaliza tion behavior of deep ReLU networks with systematic experi ments on representative datasets such as MNIST and CIFAR 10 along with a twodimensional synthetic dataset. Finally, we discuss correlations between generalization intervals on training sets and validation accuracy and whether there ex ists a mechanism in deep neural networks that supports deep memorization. We conclude the paper with a brief summary and plan for future work. 2 Related Work "
383,Seq-UPS: Sequential Uncertainty-aware Pseudo-label Selection for Semi-Supervised Text Recognition.txt,"This paper looks at semi-supervised learning (SSL) for image-based text
recognition. One of the most popular SSL approaches is pseudo-labeling (PL). PL
approaches assign labels to unlabeled data before re-training the model with a
combination of labeled and pseudo-labeled data. However, PL methods are
severely degraded by noise and are prone to over-fitting to noisy labels, due
to the inclusion of erroneous high confidence pseudo-labels generated from
poorly calibrated models, thus, rendering threshold-based selection
ineffective. Moreover, the combinatorial complexity of the hypothesis space and
the error accumulation due to multiple incorrect autoregressive steps posit
pseudo-labeling challenging for sequence models. To this end, we propose a
pseudo-label generation and an uncertainty-based data selection framework for
semi-supervised text recognition. We first use Beam-Search inference to yield
highly probable hypotheses to assign pseudo-labels to the unlabelled examples.
Then we adopt an ensemble of models, sampled by applying dropout, to obtain a
robust estimate of the uncertainty associated with the prediction, considering
both the character-level and word-level predictive distribution to select good
quality pseudo-labels. Extensive experiments on several benchmark handwriting
and scene-text datasets show that our method outperforms the baseline
approaches and the previous state-of-the-art semi-supervised text-recognition
methods.","Text recognition has garnered a great deal of attention in recent times [ 38], owing primarily to its commercial applica tions. Since the introduction of deep learning, great strides [6,9,14,33,36,37,53,54,63,64,71] have been made in recognition accuracy on various publicly available bench mark datasets [ 28,29,31,39,44,45,49,50,65]. These models, however, are heavily reliant on a large amount of labeled data with complete charactersequence as labels, which is laborious to obtain. Aside from fullysupervised text recognition, very few attempts have been made to utilize unlabelled data samples to improve the model‚Äôs performance [1, 27, 76]. n r u w[s] n r u w n r u w n r u w n r u w H i g h L o wFigure 1: An overview of BeamSearch inference (beam width = 2) on recognizing an unlabeled textimage and pop ulating the hypotheses set. The framework considers all the accumulated hypotheses to approximate the total uncertainty (Utotal) by importancesampling. Semisupervised learning paradigms have been developed to address the preceding issues, and primarily pseudolabel based semisupervised learning (PLSSL) methods have caught much attention. In a PLSSL configuration, a smaller labeled set is used to train an initial seed model and then applied to a larger amount of unlabeled data to generate hypotheses. Furthermore, the unlabeled data along with its most reliable hypothesis as the label is combined with the training data for retraining, this methodology utilizes both the labeled and unlabelled datapoints to retrain the com plete model, allowing the entire network to exploit the latent knowledge from unlabelled datapoints as well. However, on the other hand, PLSSL is sensitive to the quality of the selected pseudolabels and suffers due to the inclusion of erroneous highly confident pseudolabels generated from poorly calibrated models, resulting in noisy training [ 51]. Moreover, for imagebased text recognition, that requires predictions of characters at each timestep for each input image, pseudolabeling is much more challenging due to the combinatorial vastness of the hypotheses space and the fact that a single incorrect character prediction renders the entirearXiv:2209.00641v2  [cs.CV]  6 Oct 2022predicted sequence false. Additionally, in the PLSSL setup, handling erroneous predictions from the model trained with a relatively small amount of labeled data, and being able to exclude them in the beginning of the training cycle is highly essential. Therefore, correct hypotheses generation and se lection are of massive importance in such a framework. This work proposes a reliable hypotheses generation and selection method for PLSSL for imagebased text recog nition. We suggest a way to estimate the uncertainty as sociated with the prediction of character sequences for an input image that gives a firm estimate of the reliability of the pseudolabel for an unlabelled data sample and then based on the estimated uncertainty, select the examples which have a low likelihood of an incorrectly assigned pseduolabel. Our methodology stems from the two primary observations that suggest (a) for pseudolabeling based SSL schemes, choos ing predictions with low uncertainty reduces the effect of poor calibration, thus improving generalization [ 51] and (b) a high positive correlation exists between the predictive un certainty and the token error rate, for a deep neural network based language model, suggesting if the model produces a high uncertainty for an input image, its highly likely that the prediction used as the pseudolabel is incorrect [62]. Nevertheless, the majority of the unsupervised uncertainty estimation methods have concentrated on conventional un structured prediction tasks, such as image classification and segmentation. Uncertainty estimation of an input image for text recognition, inherently a sequence prediction task, is highly nontrivial and poses various challenges [ 42]: (a) Recognition models do not directly generate a distribution over an infinite set of variablelength sequences, and (b) an autoregressive sequence prediction task, such as text recognition, does not have a fixed hypotheses set; therefore, debarring expectation computation on the same. To circumvent these challenges we use BeamSearch in ference (Figure 1) to yield high probability hypotheses for each unlabelled datapoint on the seed model trained with the given labeled data. Moreover, the hypotheses obtained are used to approximate the predictive distribution and obtain expectations over the set. We term this process as deter ministic inference , which generates definite and a distinct hypotheses set for each image that aid in approximating the predictive distribution. Furthermore, to compute the uncer tainty associated with an input, we take a Bayesian approach to ensembles as it produces an elegant, probabilistic and interpretable uncertainty estimates [42]. We use MonteCarloDropout (MCDropout) [ 20], which alleviates the need to train multiple models simultaneously and allows us to utilize Dropout to virtually generate multiple models (with different neurons dropped out of the original model) as MonteCarlo samples and perform inferences on the sampled models, on each of the sequences in the hypothe ses set by teacherforcing [67], terming it as stochastic infer ence. Our motivation to utilize teacherforcing in the pseudo labeling phase is to enforce prediction consistency across allthe sampled models in the ensemble such that we can esti mate the predictive distribution of every hypothesis obtained with deterministicinference . Finally, the predictive posterior for each hypothesis is obtained by taking the expectation over all the sampled models. Furthermore, the obtained pre dictive posterior is used to compute an informationtheoretic estimate of the uncertainty, which estimates the total uncer tainty [22], considering both the characterlevel and word level predictive posteriors and serves as a robust selection criterion for the pseudolabels. Figure 1 shows an intuitive idea behind BeamSearch inference to generate multiple hypotheses for a normalized uncertainty estimate. Finally, we test our method on several handwriting and scenetext datasets, comparing its performance to stateoftheart text recognition methods in semisupervised setting. Moreover, we demonstrate the robustness of our uncertainty estimation using prediction rejection curves [41,43] based on the Word Error Rate (WER). In sum the keypoints are: (a) We propose an uncertainty aware pseudolabelbased semisupervised learning frame work, that utilizes BeamSearch inference for pseudolabel assignment, and a character and sequence aware uncertainty estimate for sample selection. (b) We utilize teacherforcing [67], primarily employed to train sequencemodels, in the pseudolabeling phase to enforce prediction consistency across all the sampled models in the ensemble, to estimate the predictive distribution.(c) Finally, the methods are eval uated on several challenging handwriting and scenetext datasets in the SSL setting. 2. Related Work "
444,Vehicle Shape and Color Classification Using Convolutional Neural Network.txt,"This paper presents a module of vehicle reidentification based on make/model
and color classification. It could be used by the Automated Vehicular
Surveillance (AVS) or by the fast analysis of video data. Many of problems,
that are related to this topic, had to be addressed. In order to facilitate and
accelerate the progress in this subject, we will present our way to collect and
to label a large scale data set. We used deeper neural networks in our
training. They showed a good classification accuracy. We show the results of
make/model and color classification on controlled and video data set. We
demonstrate with the help of a developed application the re-identification of
vehicles on video images based on make/model and color classification. This
work was partially funded under the grant.","The objective of the vehicle reidentiÔ¨Åcation module based on make/model and color classiÔ¨Åca tion is to recognize a vehicle within a large image or video data set based on its make/model and color attributes. There are a number of challenges related to this task, that need to be addressed: There are more than 150 car manufacturers worldwide with approximately 2.000 mod els. Each model of a make generally has a longer history while model upgrades appear every few years. The appearance of a model of a make varies not only due to its model year but also dif fers strongly depending on the perspective.The same vehicle looks very different from the front than from the rear or from the side view. Video data frequently contains objects at low resolution, this aggravates the classiÔ¨Åcation. Occlusion in case vehicles are close each to other. In this work, we used convolutional neural networks (CNNs) to learn the vehicle‚Äôs make/model and color descriptors. We used TensorÔ¨Çow as framework. Our Training is applied on the detected region of interest (ROI) of the vehicle. 2. Related Works "
82,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro.txt,"The main contribution of this paper is a simple semi-supervised pipeline that
only uses the original training set without collecting extra data. It is
challenging in 1) how to obtain more training data only from the training set
and 2) how to use the newly generated data. In this work, the generative
adversarial network (GAN) is used to generate unlabeled samples. We propose the
label smoothing regularization for outliers (LSRO). This method assigns a
uniform label distribution to the unlabeled images, which regularizes the
supervised model and improves the baseline. We verify the proposed method on a
practical problem: person re-identification (re-ID). This task aims to retrieve
a query person from other cameras. We adopt the deep convolutional generative
adversarial network (DCGAN) for sample generation, and a baseline convolutional
neural network (CNN) for representation learning. Experiments show that adding
the GAN-generated data effectively improves the discriminative ability of
learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and
DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1
precision over the baseline CNN, respectively. We additionally apply the
proposed method to fine-grained bird recognition and achieve a +0.6%
improvement over a strong baseline. The code is available at
https://github.com/layumi/Person-reID_GAN.","Unsupervised learning can serve as an important auxil iary task to supervised tasks [14, 29, 11, 28]. In this work, we propose a semisupervised pipeline that works on the original training set without an additional data collectio n process. First, the training set is expanded with unlabeled data using a GAN. Then our model minimizes the sum of the supervised and the unsupervised losses through a new ‚àóTo whom all correspondence should be addressed. Figure 1. The pipeline of the proposed method. There are two components: a generative adversarial model [27] for unsupe rvised learning and a convolutional neural network for semisuper vised learning. ‚ÄúReal Data‚Äù represents the labeled data in the giv en training set; ‚ÄúTraining data‚Äù includes both the ‚ÄúReal Data‚Äù and the generated unlabeled data. We aim to learn more discrimin ative embeddings with the ‚ÄúTraining data‚Äù. regularization method. This method is evaluated with per son reID, which aims to spot the target person in different cameras. This has been recently viewed as an image re trieval problem [50]. This paper addresses three challenges. First, current re search in GANs typically considers the quality of the sam ple generation with and without semisupervised learning in vivo [24, 32, 27, 7, 26, 41]. Yet a scientiÔ¨Åc problem re mains unknown: moving the generated samples out of the box and using them in currently available learning frame works. To this end, this work uses unlabeled data produced by the DCGAN model [27] in conjunction with the labeled training data. As shown in Fig. 1, our pipeline feeds the newly generated samples into another learning machine (i.e . a CNN). Therefore, we use the term ‚Äú in vitro ‚Äù to differenti ate our method from [24, 32, 27, 7]; these methods perform semisupervised learning in the discriminator of the GANs (in vivo ). Second, the challenge of performing semisupervised learning using labeled and unlabeled data in CNNbased methods remains. Usually, the unsupervised data is used as a pretraining step before supervised learning [28, 11, 14] . Our method uses all the data simultaneously. In [25, 18, 24, 32], the unlabeled/weaklabeled real data are assignedlabels according to predeÔ¨Åned training classes, but our method assumes that the GAN generated data does not be long to any of the existing classes. The proposed LSRO method neither includes unsupervised pretraining nor la bel assignments for the known classes. We address semi supervised learning from a new perspective. Since the unla beled samples do not belong to any of the existing classes, they are assigned a uniform label distribution over the trai n ing classes. The network is trained not to predict a particul ar class for the generated data with high conÔ¨Ådence. Third, in person reID, data annotation is expensive, be cause one has to draw a pedestrian bounding box and as sign an ID label to it. Recent progress in this Ô¨Åeld can be attributed to two factors: 1) the availability of largesca le re ID datasets [49, 51, 44, 19] and 2) the learned embedding of pedestrians using a CNN [8, 10]. That being said, the number of images for each identity is still limited, as shown in Fig. 2. There are 17.2 images per identities in Market 1501 [49], 9.6 images in CUHK03 [19], and 23.5 images in DukeMTMCreID [30] on average. So using additional data is nontrivial to avoid model overÔ¨Åtting. In the literature , pedestrian images used in training are usually provided by the training sets, without being expanded. So it is unknown if a larger training set with unlabeled images would bring any extra beneÔ¨Åt. This observation inspired us to resort to the GAN samples to enlarge and enrich the training set. It also motivated us to employ the proposed regularization to implement a semisupervised system. In an attempt to overcome the abovementioned chal lenges, this paper 1) adopts GAN in unlabeled data gen eration, 2) proposes the label smoothing regularization fo r outliers (LSRO) for unlabeled data integration, and 3) re ports improvements over a CNN baseline on three person reID datasets. In more details, in the Ô¨Årst step, we train DCGAN [27] on the original reID training set. We gen erate new pedestrian images by inputting 100dim random vectors in which each entry falls within [1, 1]. Some gen erated samples are shown in Fig. 3 and Fig. 5. In the second step, these unlabeled GANgenerated data are fed into the ResNet model [13]. The LSRO method regular izes the learning process by integrating the unlabeled data and, thus, reduces the risk of overÔ¨Åtting. Finally, we eval u ate the proposed method on person reID and show that the learned embeddings demonstrate a consistent improvement over the strong ResNet baseline. To summarize, our contributions are: ‚Ä¢the introduction of a semisupervised pipeline that in tegrates GANgenerated images into the CNN learning machine in vitro ; ‚Ä¢an LSRO method for semisupervised learning. The integration of unlabeled data regularizes the CNN learning process. We show that the LSRO method is Figure 2. The image distribution per class in the dataset Mar ket 1501 [49], CUHK03 [19] and DukeMTMCreID [30]. We observe that all these datasets suffer from the limited images per cl ass. Note that there are only a few classes with more than 20 images . superior to the two available strategies for dealing with unlabeled data; and ‚Ä¢a demonstration that the proposed semisupervised pipeline has a consistent improvement over the ResNet baseline on three person reID datasets and one Ô¨Åne grained recognition dataset. 2. Related Work "
385,Virus-MNIST: A Benchmark Malware Dataset.txt,"The short note presents an image classification dataset consisting of 10
executable code varieties and approximately 50,000 virus examples. The
malicious classes include 9 families of computer viruses and one benign set.
The image formatting for the first 1024 bytes of the Portable Executable (PE)
mirrors the familiar MNIST handwriting dataset, such that most of the
previously explored algorithmic methods can transfer with minor modifications.
The designation of 9 virus families for malware derives from unsupervised
learning of class labels; we discover the families with KMeans clustering that
excludes the non-malicious examples. As a benchmark using deep learning methods
(MobileNetV2), we find an overall 80% accuracy for virus identification by
families when beneware is included. We also find that once a positive malware
detection occurs (by signature or heuristics), the projection of the first 1024
bytes into a thumbnail image can classify with 87% accuracy the type of virus.
The work generalizes what other malware investigators have demonstrated as
promising convolutional neural networks originally developed to solve image
problems but applied to a new abstract domain in pixel bytes from executable
files. The dataset is available on Kaggle and Github.","For classifying handwriting, t he popularity of the Modified National Institute of Standards and Technology   dataset (MNIST)  contin ues to dominate the early exploration of new algorithms  [13]. Its extensions to  other domains have included foreign languages  [48], medical dia gnoses  [9], overhead  imagery  [10], and  retail objects  [11].  With modern deep learning an d convolutional neur al networks, t he accuracy for multi ple  classification  challenges typically exceed 90% ac ross all classes  [12]. Recent interest  in applying the same  techniques to anti virus and malware detectors  [1319] motivates the present work to sco re a similar  formatted problem  and compare the algorithmic perfo rmance  with existing me thods . Intel Labs and  Microsoft  Threat Protection Intelligence T eam recently launched t heir static malware collaboration called  STAMINA: S calable Deep Learning  Appro ach for Malware Classific ation  [20]. The contribution  of this short note  is to reformulate  the malware image  problem as a familiar MNIST variant ,  to generate the 9 virus  clusters based  on byte similarities,  and then to  identify the  virus famil y based on a  greyscale thumbnail image (32 x 32) .  Figure 1 shows the abstract images  derived for each of the 10 classes, with  ‚Äú0‚Äù as the only one that is non  malicious.       Figure 1 Virus MNIST showing 10 classes . The ‚Äú0‚Äù class represents non  malicious examples. The  other 9 virus families were clustered using a K  means method to match with  the standard MNIST format  and multi class  solut ions. 2. METHODS   "
162,Feature Diversity Learning with Sample Dropout for Unsupervised Domain Adaptive Person Re-identification.txt,"Clustering-based approach has proved effective in dealing with unsupervised
domain adaptive person re-identification (ReID) tasks. However, existing works
along this approach still suffer from noisy pseudo labels and the unreliable
generalization ability during the whole training process. To solve these
problems, this paper proposes a new approach to learn the feature
representation with better generalization ability through limiting noisy pseudo
labels. At first, we propose a Sample Dropout (SD) method to prevent the
training of the model from falling into the vicious circle caused by samples
that are frequently assigned with noisy pseudo labels. In addition, we put
forward a brand-new method referred as to Feature Diversity Learning (FDL)
under the classic mutual-teaching architecture, which can significantly improve
the generalization ability of the feature representation on the target domain.
Experimental results show that our proposed FDL-SD achieves the
state-of-the-art performance on multiple benchmark datasets.","Person reidentication (ReID) aims to match person images across multiple nonoverlapping cameras, which has achieved attention from both industry and academia. Most of the existing person ReID models along the supervised approach [20, 28, 21, 29, 30] have achieved satisfactory performance. However, these models generally perform less well in real applications because they have never been trained to adapt to the application scenes. To address this issue, a new problem referred as to unsu pervised domain adaptation becomes a hot topic in ReID task [47, 38, 23], which focuses on how to adapt a pretrained model from a labeled source domain to an unlabeled target domain. The main challenge of unsupervised domain adaptive person ReID lies in learning feature represen tation with unlabeled target domain data. To solve this challenge, one major line attempts to assign pseudo labels for target samples based on the pretrained model trained with labeled source samples [47, 10, 3, 43], and then netunes the model using the target samples with pseudo labels. Obviously, following this approach, the person ReID performance highly depends on the quality of pseudo labels. Therefore some works focus on obtaining highly dependable pseudo labels. Some of these works con centrate on the clustering process [10, 3, 19], in which target samples are assigned with pseudo labels based on dierent metrics. This process can improve the clustering result. Other works aim at how to eectively utilize target samples based on the clustering results [43, 44]. During the training pro cess of these methods, the pseudo labels with dierent reliability are assigned with dierent weights. However, due to the clustering results are unsatisfactory, all of these methods suer from noisy pseudo labels. Experimental results revealed that a small proportion of the samples are assigned with wrong pseudo labels frequently. These samples can be regarded as hard samples for the unsupervised domain adaptive person ReID task. As is well known, the performance of a welltrained model relies more on hard samples than easy samples. For the same reason, hard samples with stubborn wrong pseudo labels will limit the performance of the ReID model heavily. Unfortunately, existing unsupervised domain adaptive ReID methods can hardly solve this problem. Meanwhile, most of these models only apply supervised loss functions such as the Cross Entropy and Triplet loss based on the pseudo labels to train the model, but neglect unsupervised feature learning without groundtruth labels or pseudo labels, which hiddenly limits the generalization ability of the learned feature representation. To address these two problems, we propose a novel solution FDLSD to resist hard samples and learn features with better generalization ability in a united framework. In order to limit the ill in uence 1arXiv:2201.10212v1  [cs.CV]  25 Jan 2022of these hard samples, we propose a simple but powerful method which is called Sample Dropout (SD) to smooth the distribution of noisy pseudo labels. For most clusteringbased unsupervised domain adaptive ReID works, assigning pseudo labels for all target samples is employed before each ne tuning iterator. But in this paper, we randomly discard a proportion of target samples before each epoch of the training, through which the vicious circle of iterative training caused by hard samples can be broken. In addition to the proposed SD, we also present a new architecture to realize Feature Diversity Learning (FDL) in an unsupervised way, which is believed to suppress the ill eect of wrong pseudo labels and enhance the generalization ability of the feature representation. Overall, the main contributions of this paper can be summarized in three aspects: (1) We propose the Sample Dropout (SD) method to reduce the adverse eect of hard samples on domain adaptation, which can prevent some hard samples from being assigned with wrong pseudo labels all the time, thus breaking the vicious circle caused by these hard samples. (2) We propose the Feature Diversity Learning (FDL) and embed it into a dualbranch architecture to learn feature diversity representation in an unsupervised fashion, which can boost the generalization ability of model on target domain. (3) Extensive experiments on multiple benchmark datasets show that our proposed FDLSD achieves the stateoftheart performance, which demonstrates the eectiveness of our proposed approach. 2 Related Work "
570,Mining Significant Microblogs for Misinformation Identification: An Attention-based Approach.txt,"With the rapid growth of social media, massive misinformation is also
spreading widely on social media, such as microblog, and bring negative effects
to human life. Nowadays, automatic misinformation identification has drawn
attention from academic and industrial communities. For an event on social
media usually consists of multiple microblogs, current methods are mainly based
on global statistical features. However, information on social media is full of
noisy and outliers, which should be alleviated. Moreover, most of microblogs
about an event have little contribution to the identification of
misinformation, where useful information can be easily overwhelmed by useless
information. Thus, it is important to mine significant microblogs for a
reliable misinformation identification method. In this paper, we propose an
Attention-based approach for Identification of Misinformation (AIM). Based on
the attention mechanism, AIM can select microblogs with largest attention
values for misinformation identification. The attention mechanism in AIM
contains two parts: content attention and dynamic attention. Content attention
is calculated based textual features of each microblog. Dynamic attention is
related to the time interval between the posting time of a microblog and the
beginning of the event. To evaluate AIM, we conduct a series of experiments on
the Weibo dataset and the Twitter dataset, and the experimental results show
that the proposed AIM model outperforms the state-of-the-art methods.","With the rapid growth of social media, such as Facebook, Twitter, and Weibo, people are sharing information and expressing their attitudes publicly. Social media brings great convenience to users, and information can be spread rapidly and widely nowadays. However, misinformation can also be spread on the Internet more easily. Misinformation brings significant harm to daily life, social harmony, or even public security. With the growth of the Internet and social media, such harm Author‚Äôs addresses: Q. Liu, F. Yu, S. Wu and L. Wang, the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA) and the University of Chinese Academy of Sciences (UCAS), Beijing 100080, China; emals: {qiang.liu, feng.yu, shu.wu, wangliang}@nlpr.ia.ac.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ¬©2009 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. 21576904/2010/3ART39 $15.00 https://doi.org/0000001.0000001 ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 39. Publication date: March 2010.arXiv:1706.06314v1  [cs.IR]  20 Jun 201739:2 Qiang Liu, Feng Yu, Shu Wu, and Liang Wang will also grow greater. For instance, as the loss of MH370 has drawn worldwide attention, a great amount of rumors has spread on social media, e.g., MH370 has landed in China1, the loss of MH370 is caused by terrorists2, and Russian jets are related to the loss of MH3703. These rumors about MH370 mislead public attitudes to a wrong direction and delay the search of MH370. Up to March 15, 2017, on the biggest Chinese microblog website Sina Weibo4, 32,076 rumors have been reported and collected in its misinformation management center5. Accordingly, it is crucial to evaluate information credibility and to detect misinformation on social media. Nowadays, to automatically identify misinformation on social media, some methods have been recently proposed. Usually, and event, which may be misinformation or true information, contains a group of microblogs. Thus, most of existing methods identify misinformation at the microblog level [3,11,30] or the event level [ 18,25,44]. Some studies investigate the aggregation of credibility from the microblog level to the event level [ 14]. On the contrary, considering dynamic information, some work designs temporal features based on the prorogation properties over time [ 18] or trains a model with features generated from different time periods [ 25]. Recently, Recurrent Neural Networks (RNN) [ 26] have been incorporated for misinformation identification, and the Gated Recurrent Unit (GRU) structure [ 7] is proved to have satisfactory performance [ 24]. Moreover, some methods take usage of users‚Äô feedbacks (comments and attitudes) to evaluate the credibility [9, 32, 44]. For instance, [ 44] takes out signal tweets, which indicates users‚Äô skepticism about factual claims for detecting misinformation. Though above methods succeed in misinformation identification, they have severe drawbacks. Among these methods, some identify misinformation according to global statistical features of an event or a time window, some calculate the credibility of each microblog and then aggregate them to the the credibility of the whole event. However, information on social media is full of noisy and outliers, which should be alleviated. Moreover, most of microblogs about an event have little contribution to the identification of misinformation. As shown in the example of misinformation on Sina Weibo in Table 1, most users are simply reposting the fake news, or credence to the misinformation. Only a few users express their questions about the misinformation. Thus, it is important to select those significant microblogs, and obtain a reliable misinformation identification method. Fortunately, the attention mechanism [ 13] is suitable for selecting most significant components of information. Via the attention mechanism, components which contribute more to a specific task have large weights for satisfying the objective as much as possible. The attention mechanism has succeed in multiple tasks, such as visual object detection [ 1], image caption [ 39], machine translation [ 2], text summarization [ 33] and text classification [ 35]. Accordingly, with the attention mechanism, we are able to mine signification microblogs for identifying misinformation, and design a reliable automatic detection method. Moreover, misinformation early detection is another important and practical task, in which we need to detect misinformation as early as possible [ 24,44]. Thus, we can take immediate actions at the beginning stage of spreading of misinformation, and minimize the baneful influence. For early detection, we need to identify misinformation with the first several microblogs. And with 1http://www.fireandreamitchell.com/2014/03/07/rumormalaysiaairlinesmh370landedchina/ 2http://www.csmonitor.com/World/AsiaPacific/2014/0310/MalaysiaAirlinesflightMH370Chinaplaysdownterrorism theoriesvideo 3http://www.inquisitr.com/1689765/malaysiaairlinesflightmh370russianjetsinbalticmayholdcluetohowflight 370vanished/ 4http://weibo.com 5http://service.account.weibo.com/?type=5&status=4 ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 39. Publication date: March 2010.An Attentionbased Approach for Identification of Misinformation 39:3 Table 1. An example of misinformation on Sina Weibo. posting time content 2014/03/20 23:55Hearing from an Australian friend: The plane has been found in the international waters near Perth. It is proven to be MH370 according to a major component of the plane. 2014/03/01 23:56 May God bless them! 2014/03/20 23:57 Reposting 2014/03/20 23:58 It is serious to spread rumors! 2014/03/20 23:59 Reposting 2014/03/21 00:00 Hopefully it‚Äôs not true. 2014/03/21 00:02 Reposting 2014/03/21 00:03 Really??? 2014/03/21 00:04 Waiting for official confirmation tomorrow. 2014/03/21 00:06 Reposting 2014/03/21 00:07 Reposting 2014/03/21 00:17 LetƒÖ≈ïs watch the exact news tomorrow morning. Anyway, may God bless them! 2014/03/21 00:18 Reposting 2014/03/21 00:21 What a bad news! 2014/03/21 00:22 Reposting 2014/03/21 00:32 Reposting 2014/03/21 00:35 Reposting unreliable information, what an expert! 2014/03/21 00:46 Reposting 2014/03/21 00:51 No! No! No! 2014/03/21 01:06 Dare to post misinformation! 2014/03/21 01:09 Reposting 2014/03/21 01:25 Is it reliable? 2014/03/21 01:32 Reposting the attention mechanism, we can identify misinformation with several significant microblogs. Accordingly, the attention mechanism is naturally suitable for misinformation early detection. In this work, we propose an Attentionbased approach for Identification of Misinformation (AIM ). First, for each microblog belonging to an event, we calculate corresponding attention value based on its textual features. This attention value is named as content attention. Second, considering microblogs posted at different time have distinct significance for the event, we calculate dynamic attention for each microblog. Dynamic attention can be determined related to the time interval between the posting time of a microblog and the beginning of the event. Then, we aggregate the content attention and the dynamic attention, and obtain the final attention weights for each microblog belonging to an event. Weighted sum of these microblogs can be performed to generate the representation of the whole event. Finally, the prediction of misinformation or true information can be made based on the event representation. In summary, the main contributions of this work are listed as follows: ‚Ä¢We incorporate the attention mechanism for misinformation identification on social media, which mines the most significant microblogs. ‚Ä¢We design both content attention and dynamic attention, for capturing different aspects of significance of microblogs for misinformation identification. ‚Ä¢Experiments conducted on two realworld datasets, i.e., the Weibo dataset and the Twitter dataset, show that AIM is effective and outperforms stateoftheart methods significantly. ‚Ä¢Visualization of the leaned attention mechanism in AIM demonstrates the rationality of our proposed method. The rest of this paper is organized as follows. In section 2, we review some related work on misinformation identification and attention mechanism. Then we detail the proposed AIM model in section 3. In section 4, we conduct and analyze experiments on two realworld datasets, and compare with several stateoftheart methods. In section 5, we illustrate some visualization examples of the leaned attention mechanism. Section 6 concludes this work and discusses future research directions. ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 39. Publication date: March 2010.39:4 Qiang Liu, Feng Yu, Shu Wu, and Liang Wang 2 RELATED WORK "
71,Weakly Supervised Learning with Side Information for Noisy Labeled Images.txt,"In many real-world datasets, like WebVision, the performance of DNN based
classifier is often limited by the noisy labeled data. To tackle this problem,
some image related side information, such as captions and tags, often reveal
underlying relationships across images. In this paper, we present an efficient
weakly supervised learning by using a Side Information Network (SINet), which
aims to effectively carry out a large scale classification with severely noisy
labels. The proposed SINet consists of a visual prototype module and a noise
weighting module. The visual prototype module is designed to generate a compact
representation for each category by introducing the side information. The noise
weighting module aims to estimate the correctness of each noisy image and
produce a confidence score for image ranking during the training procedure. The
propsed SINet can largely alleviate the negative impact of noisy image labels,
and is beneficial to train a high performance CNN based classifier. Besides, we
released a fine-grained product dataset called AliProducts, which contains more
than 2.5 million noisy web images crawled from the internet by using queries
generated from 50,000 fine-grained semantic classes. Extensive experiments on
several popular benchmarks (i.e. Webvision, ImageNet and Clothing-1M) and our
proposed AliProducts achieve state-of-the-art performance. The SINet has won
the first place in the classification task on WebVision Challenge 2019, and
outperformed other competitors by a large margin.","In recent years, the computer vision community has witnessed the signicant success of Deep Neural Networks (DNNs) on several benchmark datasets of image classication, such as ImageNet [1] and MSCOCO [22]. However, obtaining largescale data with clean and reliable labels is expensive and timeconsuming. When noisy labels are introduced in training data, it is widely known that the performance of a deep model can be signicantly degraded [2,36,3,23], which prevents deep models from being quickly employed in realworld noisy scenarios.arXiv:2008.11586v2  [cs.CV]  4 Sep 20202 L. Cheng et al. A common solution is to collect a large amount of image related side infor mation (e.g. surrounding texts, tags and descriptions) from the internet, and directly take them as the groundtruth for model training. Though this solution is more ecient than manual annotation, the obtained labels usually contain noise due to the heterogeneous sources. Therefore, improving the robustness of deep learning models against noisy labels has become a critical issue. To estimate the noise in labels, some works propose new layers [26,27] or loss functions [28,29,30,18] to correct the noisy label during training. However, these works rely on a strict assumption that there is a single transition prob ability between the noisy labels and the groundtruth labels. Owning to this assumption, these methods may show good performance on handcrafted noisy datasets but are inecient on real noisy datasets such as Clothing1M [36]. In some situations, it is possible to annotate a small fraction of training samples as additional supervision. By using additional supervision, works like [31,11,32] could improve the robustness of deep networks against label noises. But still, the requirement on clean samples make them less  exible to apply in large scale realworld scenarios. Many data cleaning algorithms [33,34,35] are developed to discard those sam ples with wrong label ahead of the training procedure. The major diculty of these algorithms is how to distinguish informative hard samples from harmful mislabeled ones. CleanNet [11] achieves stateoftheart performance on the real world noisy dataset Clothing1M [36]. CleanNet generates a single representative sample (class prototype) for each class and uses it to estimate the correctness of sample labels. With the observation that samples have widespread distribution in noisy classes, SMP [20] takes multiple prototypes to represent a noisy class instead of single prototype in CleanNet. In both CleanNet and SMP, extra clean supervision is required to train models. In most of previous works, image related side information or annotations (e.g. titles and tags) from web are commonly regarded as noisy labels. These works may not fully take advantage of the side information. Based on our observations, these image related side information reveal underlying similarity among images and classes, which has great potential to help tackle label noises. By analyzing the label structure and text descriptions, we explore an weaklysupervised learning strategy to deal with noisy samples. For example, the label \apple"" may refer to a fruit or an Apple mobile phone. When acquiring images from web using the label \apple"", images of apple fruits and Apple mobile phones will be wrongly put under a same class. Fortunately, titles or text descriptions about the images could imply the misplacement. In this paper, we propose an ecient weakly supervised learning strategy to evaluate the correctness of each image sample in each class by exploiting the label structure and label descriptions. Moreover, we release a large scale negrained product dataset to facilitate further research on visual recognition. To our knowledge, the proposed product dataset contains the largest number of product categories by now. Our contributions in this paper are summarized as follows:Weakly Supervised Learning with Side Information for Noisy Labeled Images 3 phalarope horseman candie d    apple tulipa  gesnerianacorrect labels noisy labels Fig. 1. Images of WebVision 2019 dataset [37] from the categories of phalarope, horse man, candied apple, tulipa, gesneriana . The dataset was collected from the Internet by textual queries generated from 5, 000 semantic concepts in WordNet. Obviously, each category includes a lot of noisy images as shown above. 1) A weakly supervised learning with side information network (SINet) is proposed for noisy labeled image classication. SINet infers the relationship be tween images and labels without any human annotation, and enable us to train highperformance and robust CNN models against large scale label noises. 2) A noisy and negrained product dataset called AliProducts is released, which contains more than 2 :5 million web images crawled from the Internet by using queries generated from the 50 ;000 negrained semantic classes. In addition, side information (e.g., hierarchical relationships between classes) are also provided for the convenience of extending research. 3) Extensive experiments are conducted on a number of benchmarks, includ ing WebVision, ImageNet, Clothing1M and AliProducts, in which the proposed SINet obtains the stateoftheart performance. Our SINet also won the rst place on the WebVision Challenge 2019, and outperforms the other competitors by a large margin. 2 Related Work "
433,Learning from Long-Tailed Noisy Data with Sample Selection and Balanced Loss.txt,"The success of deep learning depends on large-scale and well-curated training
data, while data in real-world applications are commonly long-tailed and noisy.
Many methods have been proposed to deal with long-tailed data or noisy data,
while a few methods are developed to tackle long-tailed noisy data. To solve
this, we propose a robust method for learning from long-tailed noisy data with
sample selection and balanced loss. Specifically, we separate the noisy
training data into clean labeled set and unlabeled set with sample selection,
and train the deep neural network in a semi-supervised manner with a balanced
loss based on model bias. Extensive experiments on benchmarks demonstrate that
our method outperforms existing state-of-the-art methods.","Deep neural networks have made great successes in machine learning applications [He et al. , 2016; Vaswani et al. , 2017] but require wellcurated data for training. These data, such as ImageNet [Russakovsky et al. , 2015] and MSCOCO [Lin et al. , 2014], are usually artificially balanced across classes with clean labels obtained by manual labeling, which is costly and timeconsuming. However, the data in realworld applications are longtailed and noisy, since data from specific classes are difficult to acquire and labels are usually collected without expert annotations. To take WebVision dataset as an example, it exhibits longtailed distribution, where the sample size of each class varies from 362 (Windsor tie) to 11,129 (ashcan), and contains about 20% noisy labels [Li et al. , 2017]. Thus, developing robust learning methods for longtailed noisy data is a great challenge. Many methods have been proposed for longtailed learning or learning with noisy labels. In terms of longtailed learning, resampling methods [Chawla et al. , 2002; Jeatrakul et al. , 2010], reweighting methods [Cui et al. , 2019; Cao et al. , 2019; Menon et al. , 2021], transfer learning methods [Liu et al. , 2019; Kim et al. , 2020b] and twostage methods [Kang et al. , 2019; Cao et al. , 2019] are included; in terms of learning with noisy labels, designing noiserobust loss functions [Ghosh et al. , 2017; Zhang and Sabuncu, 2018], constructing unbiased loss terms with the transition matrix [Patrini et al. , 2017; Hendrycks et al. , 2018], filtering clean samples based on smallloss criterion [Han et al. , 2018; Li et al. , 2020] and correcting the noisy labels [Tanaka et al. , 2018; Yi and Wu, 2019] are included. Despite learning from longtailed or noisy data has been well studied, these methods cannot tackle longtailed noisy data in realworld applications. A few methods are proposed to deal with longtailed noisy data, which mainly focus on learning a weighting function in a metalearning manner [Shu et al., 2019; Jiang et al. , 2021]. However, these methods simultaneously require additional unbiased data which may be inaccessible in practice. To deal with longtailed noisy data, an intuitive way is to select clean samples with smallloss criterion and then apply longtailed learning methods. For the sample selection process, Gui et al. ‚àóCorresponding author Preprint. Under review.arXiv:2211.10906v3  [cs.LG]  28 May 2023[2021] revealed that the losses of samples with different labels are incomparable and chose each class a threshold for applying the smallloss criterion. For longtailed learning, existing methods are commonly based on label frequency to prevent head classes from dominating the training process. However, the model bias on different classes may not be directly related to label frequency (see Appendix E), and the true label frequency is also unknown under label noise. In this paper, we propose a robust method for learning from longtailed noisy data. Specifically, we separate the noisy training data into clean labeled set and unlabeled set with classaware sample selection and then train the model with a balanced loss based on model bias in a semisupervised manner. Experiments on the longtailed versions of CIFAR10 and CIFAR100 with synthetic noise and the longtailed versions of miniImageNetRed, Clothing1M, Food101N, Animal10N and WebVision with realworld noise demonstrate the superiority of our method. 2 Related Work "
246,Deep pNML: Predictive Normalized Maximum Likelihood for Deep Neural Networks.txt,"The Predictive Normalized Maximum Likelihood (pNML) scheme has been recently
suggested for universal learning in the individual setting, where both the
training and test samples are individual data. The goal of universal learning
is to compete with a ``genie'' or reference learner that knows the data values,
but is restricted to use a learner from a given model class. The pNML minimizes
the associated regret for any possible value of the unknown label. Furthermore,
its min-max regret can serve as a pointwise measure of learnability for the
specific training and data sample. In this work we examine the pNML and its
associated learnability measure for the Deep Neural Network (DNN) model class.
As shown, the pNML outperforms the commonly used Empirical Risk Minimization
(ERM) approach and provides robustness against adversarial attacks. Together
with its learnability measure it can detect out of distribution test examples,
be tolerant to noisy labels and serve as a confidence measure for the ERM.
Finally, we extend the pNML to a ``twice universal'' solution, that provides
universality for model class selection and generates a learner competing with
the best one from all model classes.","In the common situation of supervised machine learning, a trainset consisting of Npairs of examples is given, zN=f(xi;yi)gN i=1, wherex2Xis the data or the feature and y2Yis the label. Then, a new xis given and the task is to predict its corresponding label y. The formal deÔ¨Ånition of the learning problem includes a loss function that measures the accuracy of the prediction. Here we assume that the learner assigns a probability q(jx)to the test label, and we use the logloss to evaluate the performance of the predictor `(q;x;y) ="
268,Video Representation Learning and Latent Concept Mining for Large-scale Multi-label Video Classification.txt,"We report on CMU Informedia Lab's system used in Google's YouTube 8 Million
Video Understanding Challenge. In this multi-label video classification task,
our pipeline achieved 84.675% and 84.662% GAP on our evaluation split and the
official test set. We attribute the good performance to three components: 1)
Refined video representation learning with residual links and hypercolumns 2)
Latent concept mining which captures interactions among concepts. 3) Learning
with temporal segments and weighted multi-model ensemble. We conduct
experiments to validate and analyze the contribution of our models. We also
share some unsuccessful trials leveraging conventional approaches such as
recurrent neural networks for video representation learning for this
large-scale video dataset. All the codes to reproduce our results are publicly
available at https://github.com/Martini09/informedia-yt8m-release.","Ranging from booming personal video collections, surveillance recordings, and professional video documen tary archives, we have witnessed an unprecedented growth of a wide range of video data. Numerous methods have been invented to understand video contents and enable searching over huge volumes of accumulated video data. Recently re leased largescale video datasets such as Google‚Äôs YouTube 8 Million (Youtube8M) video collection bring advance ments in video understanding tasks and create new pos sibilities for many emerging applications such as person alized assistant like Google Home and Microsoft Cortana. Youtube 8M is a multilabel video classiÔ¨Åcation benchmark composed of preextracted Inceptionv3 features [23], la bels and their hierarchy in the knowledge graph over more than 8 million videos. The quantity makes Youtube8M a unique video classi 1https://github.com/Martini09/ informediayt8mreleaseÔ¨Åcation testbed. There are 5.7 millions training videos, 1.6 millions validation videos, and 0.8 testing videos respec tively. The length of videos range from 120 to 500 seconds. Framelevel features are extracted under 1 frame per second (FPS) sampling rate. Video level features are meanpooled from framelevel features. The size of topic theme pool is 4,716. Each video is with 3.4 labels on average. In compar ison to other weaklylabeled datasets [11], the precision is reasonably good (85%) while recall remains poor. Learning an effective model for video understanding at this scale is challenging for the three reasons: First, al though effort in extracting features at a scale of 8 million is alleviated, the provided framelevel features are prepos sessed with some unknown PCA and followed by a simple mean pooling to generate videolevel representation. We propose to learn an attentive pooling kernel followed by a reÔ¨Åned representation learning module to further boost model performance. Second, labels (classes/concepts) are assumed to be independent in the Youtube 8M dataset, which fails to capture the authentic underlying relationship (such as cooccurrence, exclusion and hierarchy) between concepts. We address this issue by learning and incorpo rating latent concepts for multilabel classiÔ¨Åcation. Third, multimodel ensemble at this scale is underexplored. We design a systematic model ensemble scheme and quantify the importance over heterogeneous models. Our contribution in this paper is threefold: 1) We in vestigate feasible neural architectures to enhance mixture of experts (MoE) model with reÔ¨Åned representation learn ing via residual links and hypercolumns. 2) We introduce a novel latent concept learning layer to capture relationships among concepts 3) We incorporate temporal segment data augmentation and leaveoneout ensemble to further boost classiÔ¨Åcation accuracy. 2. Related work "
573,Unsupervised Person Re-identification via Multi-label Classification.txt,"The challenge of unsupervised person re-identification (ReID) lies in
learning discriminative features without true labels. This paper formulates
unsupervised person ReID as a multi-label classification task to progressively
seek true labels. Our method starts by assigning each person image with a
single-class label, then evolves to multi-label classification by leveraging
the updated ReID model for label prediction. The label prediction comprises
similarity computation and cycle consistency to ensure the quality of predicted
labels. To boost the ReID model training efficiency in multi-label
classification, we further propose the memory-based multi-label classification
loss (MMCL). MMCL works with memory-based non-parametric classifier and
integrates multi-label classification and single-label classification in a
unified framework. Our label prediction and MMCL work iteratively and
substantially boost the ReID performance. Experiments on several large-scale
person ReID datasets demonstrate the superiority of our method in unsupervised
person ReID. Our method also allows to use labeled person images in other
domains. Under this transfer learning setting, our method also achieves
state-of-the-art performance.","Recent years have witnessed the great success of person reidentiÔ¨Åcation (ReID), which learns discriminative fea tures from labeled person images with deep Convolutional Neural Network (CNN) [43, 14, 9, 27, 16, 26, 28, 29]. Be cause it is expensive to annotate person images across mul tiple cameras, recent research efforts start to focus on un supervised person ReID. Unsupervised person ReID aims to learn discriminative features from unlabeled person im ages. Compared with supervised learning, unsupervised learning relieves the requirement for expensive data anno tation, hence shows better potential to push person ReID towards real applications. The challenge of unsupervised person ReID lies in learn ing discriminative features without true labels. To conquer Multi label Classification input CNN memory single labelscores multi label MMCL MPLP Multi label Classification input CNN memory single class labelscores multi class  label MMCL MPLP Figure 1. Illustrations of multilabel classiÔ¨Åcation for unsuper vised person ReID. We target to assign each unlabeled person im age with a multiclass label reÔ¨Çecting the person identity. This is achieved by iteratively running MPLP for prediction and MMCL for multilabel classiÔ¨Åcation loss computation. This procedure guides CNN to produce discriminative features for ReID. this challenge, most of recent works [37, 46, 22, 19, 30] de Ô¨Åne unsupervised person ReID as a transfer learning task, which leverages labeled data on other domains for model initialization or label transfer. Among them, some works assign each image with a singleclass label [46]. Some oth ers leverage spatiotemporal cues or additional attribute an notations [22, 19, 30]. Detailed review of existing meth ods will be presented in Sec. 2. Thanks to the above ef forts, the performance of unsupervised person ReID has been signiÔ¨Åcantly boosted. However, there is still a con siderable gap between supervised and unsupervised per son ReID. Meanwhile, the setting of transfer learning leads to limited Ô¨Çexibility. For example, as discussed in many works [21, 35, 31], the performance of transfer learning is closely related to the domain gap, e.g., large domain gap degrades the performance. It is nontrivial to estimate the domain gap and select suitable source datasets for transfer learning in unsupervised person ReID. This paper targets to boost unsupervised person ReID without leveraging any labeled data. As illustrated in Fig. 1, we treat each unlabeled person image as a class and train the ReID model to assign each image with a multiclass label. In other words, the ReID model is trained to classify eacharXiv:2004.09228v1  [cs.CV]  20 Apr 2020image to multiple classes belonging to the same identity. Because each person usually has multiple images, multi label classiÔ¨Åcation effectively identiÔ¨Åes images of the same identity and differentiates images from different identities. This inturn facilitates the ReID model to optimize inter and intra class distances. Compared with previous meth ods [20, 34], which classify each image into a single class, the multilabel classiÔ¨Åcation has potential to exhibit better efÔ¨Åciency and accuracy. Our method iteratively predicts multiclass labels and updates the network with multilabel classiÔ¨Åcation loss. As shown in Fig. 1, to ensure the quality of predicted labels, we propose the Memorybased Positive Label Prediction (MPLP), which considers both visual similarity and cycle consistency for label prediction. Namely, two images are assigned with the same label if they a) share large similar ity and b) share similar neighbors. To further ensure the accuracy of label prediction, MPLP utilizes image features stored in the memory bank, which is updated with aug mented features after each training iteration to improve fea ture robustness. Predicted labels allow for CNN training with a multi label classiÔ¨Åcation loss. Since each image is treated as a class, the huge number of classes makes it hard to train clas siÔ¨Åers like Fully Connected (FC) layers. As shown in Fig. 1, we adopt the feature of each image stored in the memory bank as a classiÔ¨Åer. SpeciÔ¨Åcally, a Memorybased Multi label ClassiÔ¨Åcation Loss (MMCL) is introduced. MMCL accelerates the loss computation and addresses the vanish ing gradient issue in traditional multilabel classiÔ¨Åcation loss [39, 5] by abandoning the sigmoid function and enforc ing the classiÔ¨Åcation score to 1 or 1. MMCL also involves hard negative class mining to deal with the imbalance be tween positive and negative classes. We test our approach on several largescale person ReID datasets including Market1501 [42], DukeMTMC reID [25] and MSMT17 [31] without leveraging other la beled data. Comparison with recent works shows our method achieves competitive performance. For instance, we achieve rank1 accuracy of 80.3% on Market1501, signif icantly outperforming the recent BUC [20] and DBC [4] by 14.1% and 11.1%, respectively. Our performance is also better than the HHL [45] and ECN [46], which use extra DukeMTMCreID [25] for transfer learning. Our method is also compatible with transfer learning. Leverag ing DukeMTMCreID for training, we further achieve rank 1 accuracy of 84.4% on Market1501. In summary, our method iteratively runs MPLP and MMCL to seek true labels for multilabel classiÔ¨Åcation and CNN training. As shown in our experiments, this strat egy, although does not leverage any labeled data, achieves promising performance. The maintained memory bank reinforces both label prediction and classiÔ¨Åcation. Ourwork also shows that, unsupervised training has potential to achieve better Ô¨Çexibility and accuracy than existing transfer learning strategies. 2. Related Work "
170,APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning.txt,"Practical natural language processing (NLP) tasks are commonly long-tailed
with noisy labels. Those problems challenge the generalization and robustness
of complex models such as Deep Neural Networks (DNNs). Some commonly used
resampling techniques, such as oversampling or undersampling, could easily lead
to overfitting. It is growing popular to learn the data weights leveraging a
small amount of metadata. Besides, recent studies have shown the advantages of
self-supervised pre-training, particularly to the under-represented data. In
this work, we propose a general framework to handle the problem of both
long-tail and noisy labels. The model is adapted to the domain of problems in a
contrastive learning manner. The re-weighting module is a feed-forward network
that learns explicit weighting functions and adapts weights according to
metadata. The framework further adapts weights of terms in the loss function
through a combination of the polynomial expansion of cross-entropy loss and
focal loss. Our extensive experiments show that the proposed framework
consistently outperforms baseline methods. Lastly, our sensitive analysis
emphasizes the capability of the proposed framework to handle the long-tailed
problem and mitigate the negative impact of noisy labels.","Deep Neural Networks (DNNs) have become the default modeling choice for complex problem with largescale labeled data. They have been remark ably successful in supervised learning across a vari ety of domains such as natural language processing and computer vision. Their success relies on the availability of a large amount of labelled data with high quality. In practice, it is usually expensive to acquire clean labels at scale. It either requires multiple blind passes and adjudicators decision or needs quality assurance by auditor. Both are laborintensive and timeconsuming. Recent progress on Ô¨Åne tuning (Devlin et al., 2019; Cui et al., 2018), domain adaptation (Tzeng et al., 2017; Xu et al., 2020; Ganin and Lempitsky, 2015; Xu et al., 2021) and fewshot learning (Brown et al., 2020) alleviate the demand for large volume labeled data. Deep learning models remain dependent on accurate la beled data, in spite of those progress. Another factor that compromises model gener alization is data distribution shift. Data shift can occur from various sources. Longtailed problem is a common example. For example, in order to train a model to classify shopping items, it is difÔ¨Åcult to obtain sufÔ¨Åcient images for rare products (He and McAuley, 2016). It is also a challenge to train a dialog model that is exposed to sufÔ¨Åcient less frequent topics or user intents. When training a model on an imbalanced dataset, model training becomes biased towards the majority classes. With higher number of examples available to learn from, the model learns to perform well on the majority classes but due to the lack of enough examples the model fails to learn meaningful patterns that could aid it in learning the minority classes. The model performance on those tail classes bottlenecks the applications of deep neural networks in practice and thus it is critical to improve on such cases. A number of studies have proposed approaches to mitigate noisy label or longtailed class prob lem. To alleviate impact of noisy label, sample selection (Jiang et al., 2018), label correction (Pa trini et al., 2017; Sanchez et al., 2019), and noise aware losses (Liu et al., 2022; Castells et al., 2020) have been studied. Dataset resampling such as SMOTE (Chawla et al., 2002) is a popular method by selecting a proportion of data to train a network or by learning a weight for each example. The weights are optimized by minimizing the training loss. It is applied in multiple wellknown algo rithms such as AdaBoost (Freund and Schapire, 1997), selfpaced learning (Kumar et al., 2010),arXiv:2302.03488v2  [cs.CL]  2 May 2023and algorithm that emphasizes high variance sam ples (Jiang et al., 2018; Chang et al., 2017). To address problem of longtailed distributions, some studies have modiÔ¨Åed sampling algorithm to en sure all classes are represented equally (Kub√°t and Matwin, 1997; Chawla et al., 2002). Other popular approaches include adjusting loss function (Menon et al., 2021) biased to minor class, and posthoc correction (Kang et al., 2020). However, those methods have contradicting as sumptions. On the one hand, we assign higher weight to clean labelled data in order to mitigate noisy label problem. On the other hand, algo rithms for longtailed problems emphasize minor ity classes that more likely have higher training loss. Therefore, those methods can not handle the problem of concurrent noisy label and longtailed classes. In order to handle noisy label and long tailed problem simultaneously, some works (Shu et al., 2021; Ren et al., 2018) propose a meta learning paradigm that follows a more natural as sumption that the best example weighting should minimize the loss of clean data. Those methods learn instance weights from a small clean dataset and show promising results. Nevertheless, we ar gue that the metalearning paradigm is not general enough, and remains expensive due to the require ment of a balanced meta validation dataset. In our work, we propose the Adaptive Pre training and Adaptive Meta Learning method (APAM), a general framework to handle the concur rent problems of noisy label and longtailed classes together. It naturally subsumes the aforementioned metalearning paradigm (Shu et al., 2021; Ren et al., 2018) as a special case. Our method does not require a balanced meta data to guide reweighting. This fact reduces the amount of clean data needed in meta learning and thus is more feasible and less expensive. Furthermore, we introduce a stage of domain adaptive pretraining (Gururangan et al., 2020) through contrastive learning (Gao et al., 2021) to improve model robustness. On the one hand, the proposed adaptive procedure give lower weights on noisy samples; on the other hand, APAM give higher weights to simultaneously handle the noise and longtailed problems. We further evaluate APAM on two datasets in which it outperforms other methods (Shu et al., 2021; Lin et al., 2017; Cui et al., 2019; Devlin et al., 2019; Gao et al., 2021). In addition, we conduct comprehensive ablation study and sensitivity analysis. Across those experiments, we observe APAM consistently outperforms other methods. To summarize, the main contributions of our work are listed as follows: ‚Ä¢To cope with the concurrent problems of longtail and noisy label in text classiÔ¨Åca tion, we propose a general twostage deep learning framework including domain adap tive pretraining stage and supervised Ô¨Åne tuning through adaptive reweighting stage. It outperforms the stateoftheart methods in evaluation datasets. ‚Ä¢We demonstrate that the proposed adaptive weighting method does not require balanced meta data and thus alleviate the dependence to large amount of meta data in longtailed problem. ‚Ä¢Our experiment results illustrate that do main adaptive contrastive learning consis tently leads to improvement of performance in APAM framework. ‚Ä¢Through a holistic ablation study and sensitiv ity analysis, we demonstrate the contribution of each components of the proposed method and the effectiveness of this method to long tailed problem with noisy label. 2 Related Work "
604,SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels.txt,"Deep neural networks are prone to overfitting noisy labels, resulting in poor
generalization performance. To overcome this problem, we present a simple and
effective method self-ensemble label correction (SELC) to progressively correct
noisy labels and refine the model. We look deeper into the memorization
behavior in training with noisy labels and observe that the network outputs are
reliable in the early stage. To retain this reliable knowledge, SELC uses
ensemble predictions formed by an exponential moving average of network outputs
to update the original noisy labels. We show that training with SELC refines
the model by gradually reducing supervision from noisy labels and increasing
supervision from ensemble predictions. Despite its simplicity, compared with
many state-of-the-art methods, SELC obtains more promising and stable results
in the presence of class-conditional, instance-dependent, and real-world label
noise. The code is available at https://github.com/MacLLL/SELC.","The recent success of deep neural networks (DNNs) for vi sion tasks owes much to the availability of largescale, cor rectly annotated datasets. However, obtaining such high quality datasets can be extremely expensive, and sometimes even impossible. The common approaches, such as web queries [Liet al. , 2017 ]and crowdsourcing [Song et al. , 2019 ], can easily provide extensive labeled data, but unavoid ably introduce noisy labels . Existing studies [Arpit et al. , 2017; Zhang et al. , 2021 ]have demonstrated that DNNs can easily overÔ¨Åt noisy labels, which deteriorates the generaliza tion performance. Thus, it is essential to develop noiserobust algorithms for learning with noisy labels. Given a noisy training set consisting of clean samples and mislabeled samples, a common category of approaches [Reed et al. , 2015; Arazo et al. , 2019; Zhang et al. , 2020 ]to mit igating the negative inÔ¨Çuence of noisy labels is to identify and correct the mislabeled samples. However, the correction procedure in these methods only updates the noisy labels us ing the model prediction from the most recent training epoch directly, thus it may suffer from the false correction as themodel predictions for noisy samples tend to Ô¨Çuctuate. Take a bird image mislabeled as an airplane as an example. Dur ing the training, the clean bird samples would encourage the model to predict a given bird image as a bird, while the bird images with airplane labels regularly pull the model back to predict the bird as an airplane. Hence, the model prediction gathered in one training epoch may change back and forth between bird and airplane, resulting in false correction. We investigate the reason for performance degradation by analyzing the memorization behavior of the DNNs models. We observe that there exists a turning point during training. Before the turning point, the model only learns from easy (clean) samples, and thus model prediction is likely to be con sistent with clean samples. After the turning point, the model increasingly memorizes hard (mislabeled) samples. Hence model prediction oscillates strongly on clean samples. Trig gered by this observation, we seek to make the model retain the earlylearning memory for consistent predictions on clean samples even after the turning point. In this paper, we propose selfensemble label correction (SELC), which potentially corrects noisy labels during train ing thus preventing the model from being affected by the noisy labels. SELC leverages the knowledge provided in the model predictions over historical training epochs to form a consensus of prediction (ensemble prediction) before the turning point. We demonstrate that combining ensemble pre diction with the original noisy label leads to a better target. Accordingly, the model is gradually reÔ¨Åned as the targets be come less noisy, resulting in improving performance. How ever, it is challenging to Ô¨Ånd the turning point. Existing works estimate the turning point based on a test set or noise infor mation, which are unobservable in practice. We propose a metric to estimate the turning point only using training data, allowing us to select a suitable initial epoch to perform SELC. Overall, our contributions are summarized as follows: ‚Ä¢ We propose a simple and effective label correction method SELC based on selfensembling. ‚Ä¢ We design an effective metric based on unsupervised loss modeling to detect the turning point without requir ing the test set and noise information. ‚Ä¢ SELC achieves superior results and can be integrated with other techniques such as mixup [Zhang et al. , 2018 ] to further enhance the performance.arXiv:2205.01156v1  [cs.CV]  2 May 2022(a) Training accuracy  (b) Test accuracy  (c) CE  (d) CE  (e) SELC  (f) SELC Figure 1: Plots (a) and (b) show the training and test accuracy on CIFAR10 with different ratios of label noise using crossentropy (CE) loss. We investigate the memorization behavior of DNNs on CIFAR10 with 60% label noise using CE loss and SELC. Plot (c) and (e) show the fraction of clean samples that are predicted correctly (blue) and incorrectly (black). Plot (d) and (f) show the fraction of mislabeled samples that are predicted correctly (blue), memorized (i.e. the prediction equals to the wrong label, shown in red), and incorrectly predicted as neither the true nor the given wrong label (black). Compared to CE, SELC effectively prevents memorization of mislabeled samples and reÔ¨Ånes the model to attain correct predictions on both clean and mislabeled samples. 2 Related Work "
291,FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity.txt,"Federated noisy label learning (FNLL) is emerging as a promising tool for
privacy-preserving multi-source decentralized learning. Existing research,
relying on the assumption of class-balanced global data, might be incapable to
model complicated label noise, especially in medical scenarios. In this paper,
we first formulate a new and more realistic federated label noise problem where
global data is class-imbalanced and label noise is heterogeneous, and then
propose a two-stage framework named FedNoRo for noise-robust federated
learning. Specifically, in the first stage of FedNoRo, per-class loss
indicators followed by Gaussian Mixture Model are deployed for noisy client
identification. In the second stage, knowledge distillation and a
distance-aware aggregation function are jointly adopted for noise-robust
federated model updating. Experimental results on the widely-used ICH and
ISIC2019 datasets demonstrate the superiority of FedNoRo against the
state-of-the-art FNLL methods for addressing class imbalance and label noise
heterogeneity in real-world FL scenarios.","Federated Learning (FL), allowing individual clients to train a deep learning model collaboratively without data sharing, has been widely studied in privacyconscious occasions, e.g. medical [Kaissis et al. , 2020 ]and Ô¨Ånancial [Zheng et al. , 2021 ]applications. Most existing federated learning frame works are based on the paradigm of fully supervised learning [McMahan et al. , 2017; Li et al. , 2020b ], which implicitly as sumes that each participant‚Äôs data is labeled correctly. How ever, building a completely clean dataset with highquality annotation is costly in realistic medical scenarios, as label ing medical data is timeconsuming and laborintensive re quiring expertise. Consequently, it would unavoidably intro duce noisy labels when hiring nonprofessionals to label or using automatic labeling techniques [Irvin and others, 2019 ]. Corresponding author 1Code is available at https://github.com/wnn2000/FedNoRo.Therefore, it is more common that the labels of some clients are clean while others are not in realworld FL scenarios. Due to the existence of noisy clients, developing a noise robust FL framework is of great importance, where accurately identifying the noisy clients is the Ô¨Årst and the most cru cial step. Existing methods for noisy client detection pro pose to calculate an average indicator ( e.g. loss) over all samples of each client as its feature and Ô¨Ålter out the clients with abnormal features as noisy clients, which assumes clean clients‚Äô features are independent and identically distributed (IID) while noisy clients‚Äô are outliers following the small loss trick [Han et al. , 2018 ]. SpeciÔ¨Åcally, Xu et al. [2022 ] calculated the average LID value [Houle, 2013 ]of each client and identiÔ¨Åed the clients with larger average LID values as noisy clients. Similarly, Wang et al. [2022 ]replaced the LID value with the conÔ¨Ådence score and identiÔ¨Åed noisy clients with smaller conÔ¨Ådence scores. Unfortunately, these indica tors can be less effective to deal with noisy clients in real world FL scenarios, due to the following observations: 1. Data is highly classimbalanced from the global perspec tive [Wang et al. , 2021; Shang et al. , 2022 ]. Under class imbalance, using a global indicator for all classes is highly sensitive to clients‚Äô label distributions which may vary dramatically. 2. Data is heterogeneous across clients [Liet al. , 2020b ]. As each client collects its own data independently under FL, there may exist severe data variations, affecting the calculation of indicators across clients. 3. Label noise is heterogeneous across clients where the heterogeneity of noise varies in both strength and pat tern. The former represents different noise rates across clients, and the latter indicates various forms of label noise related to clients‚Äô local data distributions. Suffering from class imbalance, the federated model in FL can bias to the global majority classes [Kang et al. , 2020; Zhou et al. , 2020; Cao et al. , 2019; Menon et al. , 2021 ], re sulting in large indicator variations across classes and making noisy clients‚Äô indicators less distinguishable. For instance, in clinical scenarios, given one cancerspecialized hospital A and one general hospital B,Awould enroll more malignant cases and Bwill enroll more healthy cases. Therefore, Ais more likely to produce an abnormal clientwise feature ( e.g.,arXiv:2305.05230v1  [cs.LG]  9 May 2023large loss values similar to noisy clients) due to class imbal ance ( i.e., healthymalignant). In addition to data hetero geneity, the clientwise feature can also be affected by het erogeneous label noise, making noisy client detection harder. For instance, in clinical scenarios, given two hospitals Cand Dwith different patient diversity ( i.e., various label distribu tions), one benign case is more likely to be wrongly diag nosed as healthy by Cand malignant by D. Though both la bels are wrong, the loss values ( i.e., indicators for noisy client detection) produced by Cwould be much smaller than D, due to class imbalance ( i.e., healthymalignant). In summary, class imbalance would divert the clientwise feature from be ing only related to whether a client is noisy to being related to both its local data distribution and label noise pattern, mak ing the above identiÔ¨Åcation methods based on global indicator struggle in realistic FL scenarios. To address this, one straightforward way is to combine classbalancing [Kang et al. , 2020; Zhou et al. , 2020; Cao et al. , 2019; Menon et al. , 2021 ]with noisy label learning. However, those methods can only mitigate the bias in class prior probability while the bias in learning difÔ¨Åculty of class speciÔ¨Åc features [Yiet al. , 2022 ]is unsolved. Due to rela tively limited training data, the classspeciÔ¨Åc features of mi nority classes would be more difÔ¨Åcult to learn compared to those of the majority classes, resulting in imbalance in the feature space. When adopting feature learning [Karthik et al., 2021; Yi et al. , 2022 ]to alleviate the bias in learning difÔ¨Åculty, it can be constrained by each client‚Äôs limited lo cal data and computing resources. Till now, how to address class imbalance in federated noisy label learning (FNLL) is underexplored. In this paper, we Ô¨Årst formulate a new FNLL problem to model more realistic FL scenarios under class imbalance, and then propose a twostage framework FedNoRo for noise robust learning. SpeciÔ¨Åcally, in the Ô¨Årst stage, instead of us ing a global indicator, we propose to identify noisy clients according to the clientwise perclass average loss values cal culated by a warmup model trained by FedAvg [McMahan et al. , 2017 ]. As each class is considered independently, this detection method will be less affected by class imbalance and heterogeneity, leading to better detection performance. In the second stage, different learning strategies are employed for clean and noisy clients respectively, where crossentropy loss is adopted for training on clean clients and knowledge dis tillation (KD) [Hinton et al. , 2015 ]is employed to minimize the negative inÔ¨Çuence of noisy labels on noisy clients. In ad dition to clientlevel training, a distanceaware aggregation function is proposed to better balance the importance of clean and noisy clients for global model updating in the server. In the local training phase of both the two stages, logit adjust ment (LA) [Menon et al. , 2021 ]is imposed to Ô¨Åght against data heterogeneity and class imbalance. The main contribu tions are summarized as follows: ‚Ä¢ A new FNLL problem where both classimbalanced global data and heterogeneous label noise are considered to model real FL scenarios. ‚Ä¢ A new label noise generation approach for multisource data, where the synthetic label noise is heterogeneousand instancedependent. ‚Ä¢ A twostage FL framework, named FedNoRo, to address both class imbalance and label noise heterogeneity. In FedNoRo, noisy clients are identiÔ¨Åed based on abnormal perclass loss values, and noiserobust training strategies are used for effective federated model updating. ‚Ä¢ Superior performance against the stateoftheart FNLL approaches on realworld multisource medical datasets. 2 Related Work "
550,Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding.txt,"We investigate the usage of convolutional neural networks (CNNs) for the slot
filling task in spoken language understanding. We propose a novel CNN
architecture for sequence labeling which takes into account the previous
context words with preserved order information and pays special attention to
the current word with its surrounding context. Moreover, it combines the
information from the past and the future words for classification. Our proposed
CNN architecture outperforms even the previously best ensembling recurrent
neural network model and achieves state-of-the-art results with an F1-score of
95.61% on the ATIS benchmark dataset without using any additional linguistic
knowledge and resources.","The slot Ô¨Ålling task in spoken language understanding (SLU) is to assign a semantic concept to each word in a sentence. In the sentence I want to Ô¨Çy from Munich to Rome , an SLU system should tag Munich as the departure city of a trip and Rome as the arrival city. All the other words, which do not correspond to real slots, are then tagged with an artiÔ¨Åcial class O. Tradi tional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random Ô¨Åelds (CRF) [2, 3]. More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8]. Overall, RNNs outperformed other NN models and achieved the stateoftheart results on the ATIS benchmark dataset [9]. Furthermore, bidirectional RNNs have worked best so far showing that information from both the past and the future is important in predicting the semantic label of the current word. It is, however, well known that it is difÔ¨Åcult to train an RNN due to the vanishing gradient problem [10]. Introducing long short term memory (LSTM) [11] or other variants of LSTM such as the gated recurrent unit (GRU) can solve this problem but, in turn increases the number of parameters signiÔ¨Åcantly. Previous results reported in [8] did not show any improvement on the ATIS data set using LSTM or GRU. In contrast to previous papers which reported stateofthe art results with RNNs, we explore the usage of convolutional neural networks for a sequence labeling task like slot Ô¨Ålling. Previous research in [6] showed promising results on the slot Ô¨Ålling task. The motivation behind this is to allow the model to search for patterns in order to predict the label of the cur rent word independent of the feature representation of the pre vious word. Moreover, CNNs provide several advantages: itpreserves the word order information, it is faster and easier to train and does not mix up the word sequence and therefore it is able to interpret the features learnt for the current task to some extent. This study investigates the usage of CNNs for a sequential labeling task like slot Ô¨Ålling with the following contributions: (1) We propose a novel CNN architecture for sequence la beling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context. (2) We extend the proposed CNN model to a bidirectional sequential CNN (bisCNN) which combines the information from past and future words for prediction. (3) We compare the impact of two different ranking objec tive functions on the recognition performance and analyze the most important ngrams for semantic slot Ô¨Ålling. (4) On the ATIS benchmark dataset, the proposed bi directional sequential CNN outperforms all RNN related mod els and deÔ¨Ånes a new startoftheart F1score of 95.61%. 2. Related Work "
243,Label Refinement Network for Coarse-to-Fine Semantic Segmentation.txt,"We consider the problem of semantic image segmentation using deep
convolutional neural networks. We propose a novel network architecture called
the label refinement network that predicts segmentation labels in a
coarse-to-fine fashion at several resolutions. The segmentation labels at a
coarse resolution are used together with convolutional features to obtain finer
resolution segmentation labels. We define loss functions at several stages in
the network to provide supervisions at different stages. Our experimental
results on several standard datasets demonstrate that the proposed model
provides an effective way of producing pixel-wise dense image labeling.","We consider the problem of semantic image segmenta tion, where the goal is to densely label each pixel in an image according to the object class that it belongs to. We propose a convolutional neural network architecture called thelabel reÔ¨Ånement network (LRN) that performs semantic segmentation in a coarsetoÔ¨Åne fashion. Deep convolutional neural networks (CNNs) have been successfully applied to a wide variety of visual recognition problems, such as image classiÔ¨Åcation [ 13], object detec tion [ 19], action recognition [ 24], etc. CNNs extract deep feature hierarchies using alternating layers of operations, such as convolution, pooling, etc. Features from the top lay ers of CNNs tend to be invariant to ‚Äúnuisance factors‚Äù such as pose, illumination, small translations, etc. The invariance properties of these features make them particularly useful for vision tasks such as whole image classiÔ¨Åcation. However, these features are not well suited for other vision tasks (e.g. semantic segmentation) that require precise pixelwise infor mation. There have been some recent efforts [ 1,4,8,9,16,17,18] on adapting CNNs for semantic segmentation. Some of these approaches (e.g. [ 9,16,17]) are based on combining the convolutional features from multiple layers in a CNN. Another popular approach (e.g. [ 1,18] is to use upsampling (also known as deconvolution) to enlarge the spatial dimensions of the feature map at the top layer of a CNN, e.g. to the same spatial dimensions as the original image, then predict the pixelwise labels from the enlarged feature map. In both the cases, the Ô¨Ånal fullsized semantic segmentation result is obtained in a ‚Äúsingle shot‚Äù at the very end of the network architecture. In this paper, we introduce a novel CNNbased architec ture for semantic segmentation. Different from previous approaches, our proposed model predicts semantic labels at several different resolutions in a coarsetoÔ¨Åne fashion. We use resized groundtruth segmentation labels as the super vision at each resolution level. The segmentation labels at a coarse scale are combined with convolutional features to produce segmentation labels at a Ô¨Åner scale. See Fig. 1 for an illustration of our model architecture. We make threefold contributions in this paper which are as follows: We introduce a new perspective on the semantic seg mentation (or more generally, pixelwise labeling) prob lem. Instead of predicting the Ô¨Ånal segmentation result in a single shot, we propose to solve the problem in a coarsetoÔ¨Åne fashion by Ô¨Årst predicting a coarse label ing, then progressively reÔ¨Åne the result to get the Ô¨Åner scale results. We propose an endtoend CNN architecture to learn to predict the segmentation labels at multiple resolu tions. Unlike most of the previous methods that only have supervision at the end of their network, our model has supervision at multiple resolutions in the network. Although we focus on semantic image segmentation in this paper, our network architecture is general enough to be used for any pixelwise labeling task. We perform extensive experiments on several standard datasets to demonstrate the effectiveness of our pro posed model. 1arXiv:1703.00551v1  [cs.CV]  1 Mar 2017Figure 1. Overall architecture of the Label ReÔ¨Ånement Network (LRN). LRN is based on encoderdecoder framework. The encoder network produces a sequence of feature maps with decreasing spatial dimensions. The decoder network produces label maps with increasing spatial dimensions. A larger label map is obtained by combining the previous (smaller) label map and the corresponding convolutional features from a layer in the encoder network indicated by the solid line. We use downsampled groundtruth label maps to provide the supervision at each stage of the decoder network. The rest of the paper is organized as follows. Section 2 presents related work. Section 3 discusses the essential background of encoderdecoder architecture. Section 4 intro duces our label reÔ¨Ånement network. Section 5 describes the experiment details and also presents some analysis. Finally, we conclude the paper in Section 6. 2. Related Work "
415,A Fairness Analysis on Private Aggregation of Teacher Ensembles.txt,"The Private Aggregation of Teacher Ensembles (PATE) is an important private
machine learning framework. It combines multiple learning models used as
teachers for a student model that learns to predict an output chosen by noisy
voting among the teachers. The resulting model satisfies differential privacy
and has been shown effective in learning high-quality private models in
semisupervised settings or when one wishes to protect the data labels.
  This paper asks whether this privacy-preserving framework introduces or
exacerbates bias and unfairness and shows that PATE can introduce accuracy
disparity among individuals and groups of individuals. The paper analyzes which
algorithmic and data properties are responsible for the disproportionate
impacts, why these aspects are affecting different groups disproportionately,
and proposes guidelines to mitigate these effects. The proposed approach is
evaluated on several datasets and settings.","The availability of large datasets and inexpensive computational resources has rendered the use of machine learning (ML) systems instrumental for many critical decisions involving individuals, including criminal assessment, landing, and hiring, all of which have a profound social impact. A key concern for the adoption of these system regards how they handle bias and discrimination and how much information they leak about the individuals whose data is used as input. Dierential Privacy (DP) [ 5] is an algorithmic property that bounds the risks of disclosing sensitive information of individuals participating in a computation. It has become the paradigm of choice in privacypreserving machine learning systems and its deployments are growing at a fast rate. However, it was recently observed that DP systems may induce biased and unfair outcomes for di erent groups of individuals [1, 19, 28]. The resulting outcomes can have signiÔ¨Åcant societal and economic impacts on the involved individuals: classiÔ¨Åcation errors may penalize some groups over others in important determinations including criminal assessment, landing, and hiring [ 1] or can result in disparities regarding the allocation of critical funds and beneÔ¨Åts [ 19].While these surprising observations are becoming increasingly common, their causes are largely understudied and not fully understood. This paper makes a step toward this important quest, and studies the disparate impacts arising when training a model using Private Aggregation of Teacher Ensembles (PATE) [ 17] an important and popular privacypreserving machine learning framework. It combines multiple agnostic learning models used as teachers for a student model that learns to predict an output chosen by noisy voting among the teachers. The resulting model satisÔ¨Åes di erential privacy and has been shown e ective in learning high quality private models in semisupervised settings or when one wishes to protect the data labels. Preprint. Under review.arXiv:2109.08630v1  [cs.LG]  17 Sep 2021The paper analyzes which properties of the algorithm and the data are responsible for the dispro portionate impacts, why these aspects are a ecting di erent individuals or groups of individuals disproportionately, and proposes a solution that may aid mitigating these e ects. In summary, the paper makes the following contributions: 1.It uses a fairness notion that relies on the concept of excessive risk, and measures the direct impact of privacy to the model accuracy for individuals or groups. 2. It analyzes this fairness notion in PATE, a stateoftheart privacypreserving ML framework. 3.It isolates key components of the model parameters and the data properties which are responsible for the observed disparate impacts. 4.It studies when and why these components a ect di erent individuals or groups disproportionately. 5.Finally, based on these Ô¨Åndings, it proposes a method that may aid mitigating these unfairness eects while retaining high accuracy. To the best of the authors knowledge, this work represents a Ô¨Årst e ort toward understanding the reasons of the disparate impacts in privacypreserving ensemble models. 2 Related Work "
80,Can Ground Truth Label Propagation from Video help Semantic Segmentation?.txt,"For state-of-the-art semantic segmentation task, training convolutional
neural networks (CNNs) requires dense pixelwise ground truth (GT) labeling,
which is expensive and involves extensive human effort. In this work, we study
the possibility of using auxiliary ground truth, so-called \textit{pseudo
ground truth} (PGT) to improve the performance. The PGT is obtained by
propagating the labels of a GT frame to its subsequent frames in the video
using a simple CRF-based, cue integration framework. Our main contribution is
to demonstrate the use of noisy PGT along with GT to improve the performance of
a CNN. We perform a systematic analysis to find the right kind of PGT that
needs to be added along with the GT for training a CNN. In this regard, we
explore three aspects of PGT which influence the learning of a CNN: i) the PGT
labeling has to be of good quality; ii) the PGT images have to be different
compared to the GT images; iii) the PGT has to be trusted differently than GT.
We conclude that PGT which is diverse from GT images and has good quality of
labeling can indeed help improve the performance of a CNN. Also, when PGT is
multiple folds larger than GT, weighing down the trust on PGT helps in
improving the accuracy. Finally, We show that using PGT along with GT, the
performance of Fully Convolutional Network (FCN) on Camvid data is increased by
$2.7\%$ on IoU accuracy. We believe such an approach can be used to train CNNs
for semantic video segmentation where sequentially labeled image frames are
needed. To this end, we provide recommendations for using PGT strategically for
semantic segmentation and hence bypass the need for extensive human efforts in
labeling.","Semantic segmentation is an extensively studied problem which has been widely addressed using convolutional neural networks (CNNs) recently. CNNs have been shown to perform extremely well on datasets such as Pascal VOC [9], NYUD [33], CityScapes [7], etc. For ecient performance of CNNs, there are certain characteristics of training data which are required: i) the ground truth (GT) training data needs dense pixelwise annotations which requires an enormous amount of human eort. For instance, an image in the Cityscapes dataset takes arXiv:1610.00731v1  [cs.CV]  3 Oct 20162 Mustikovela, Yang, Rother about 1:5hfor dense annotation [7], ii) the training data has to be diverse in the sense that highly similar images do not add much information to the network. Such diversity in training data helps better modelling of the distribution of test scenarios. For semantic video segmentation, continuous annotation of consecutive frames is helpful rather than annotations of discrete and temporally separated frames. In such a case it is again extremely expensive to obtain dense pixelwise anno tation of consecutive images in the video. To this end, we arrive at an impor tant question: can auxiliary ground truth training data obtained by using label propagation help in better performance of a CNNbased semantic segmentation framework? In this work, we explore the possibility of using auxiliary GT, to produce more training data for CNN training. We use the CamVid dataset [5] as an example, which contains video sequences of outdoor driving scenarios. But the methodology can be easily applied to other relevant datasets. The CamVid has training images picked at 1 fpsfrom a 30fpsvideo, leading to one GT training frame for every 30 frames. We propagate the GT labels from these images to the subsequent images using a simple CRFbased, cue integration framework leading to pseudo ground truth (PGT) training images. It can be expected that the new PGT is noisy and has lower quality compared to the actual GT labeling as a result of automaitc label propagation. We train the semantic segmentation network FCN [24] using this data. In this regard, we explore three factors of how the PGT has to be used to enhance the performance of a CNN. 1.Quality  The PGT labeling has to be of good quality in the sense that there should not be too much of wrong labeling. 2.Diversity  The PGT training images have to be dierent compared to the GT images, in order to match the potential diverse test data distribution. 3.Trust  During the error propagation, the PGT has to be weighted with a trust factor in the loss function while training. Further, we systematically analyze the aforementioned dimensions through extensive experimentation to nd the most in uential dimension which improves the performance of the CNN. We perform experiments with two main settings. First, where equal number of PGT and GT training samples are present. Second, the number of samples of PGT is multiple folds larger than GT training samples. Our baseline is obtained by training the FCN only on the GT training images which stands at 49 :6%. From our experiments, we have found that adding PGT to the GT data and training the FCN helps in enhancing the accuracy by 2 :7% to 52:3%. The main contributions of this work are: {We perform exhaustive analysis to nd the in uential factors among Quality, Diversity andTrust which aect the learning in the presence of PGT data. We conclude that PGT images have to be diverse from the GT images in addition to their labeling to be of good quality. Trust on PGT data should be suciently low when there is multiple folds of PGT than GT data.Can Label Propagation help Semantic Segmentation? 3 {We provide application specic recommendations to use PGT data, taking the above factors into account. In the case of semantic video segmentation, when PGT is multiple folds larger than GT, it is advisable to have a low trust on PGT data. In case of image semantic segmentation, diverse high quality PGT data helps in improving the performance. Detailed discussions are further presented in experiments section (sec. 4). 2 Related Work "
418,Language Identification on Massive Datasets of Short Message using an Attention Mechanism CNN.txt,"Language Identification (LID) is a challenging task, especially when the
input texts are short and noisy such as posts and statuses on social media or
chat logs on gaming forums. The task has been tackled by either designing a
feature set for a traditional classifier (e.g. Naive Bayes) or applying a deep
neural network classifier (e.g. Bi-directional Gated Recurrent Unit,
Encoder-Decoder). These methods are usually trained and tested on a huge amount
of private data, then used and evaluated as off-the-shelf packages by other
researchers using their own datasets, and consequently the various results
published are not directly comparable. In this paper, we first create a new
massive labelled dataset based on one year of Twitter data. We use this dataset
to test several existing language identification systems, in order to obtain a
set of coherent benchmarks, and we make our dataset publicly available so that
others can add to this set of benchmarks. Finally, we propose a shallow but
efficient neural LID system, which is a ngram-regional convolution neural
network enhanced with an attention mechanism. Experimental results show that
our architecture is able to predict tens of thousands of samples per second and
surpasses all state-of-the-art systems with an improvement of 5%.","Language IdentiÔ¨Åcation (LID) is the Natural Language Processing (NLP) task of automatically recognizing the language that a document is written in. While this task was called ‚Äùsolved‚Äù by some authors over a decade ago, it has seen a resurgence in recent years thanks to the rise in pop ularity of social media (Jauhiainen et al., 2018; Jaech et al., 2016), and the corresponding daily creation of millions of new messages in dozens of different languages including rare ones that are not often included in language identi Ô¨Åcation systems. Moreover, these messages are typically very short (Twitter messages were until recently limited to 140 characters) and very noisy (including an abundance ofspelling mistakes, nonword tokens like URLs, emoticons, or hashtags, as well as foreignlanguage words in messages of another language), whereas LID was solved using long and clean documents. Indeed, several studies have shown that LID systems trained to a high accuracy on traditional documents suffer signiÔ¨Åcant drops in accuracy when ap plied to short socialmedia texts (Lui & Baldwin, 2012; Carter et al., 2013). Given its massive scale, multilingual nature, and popular ity, Twitter has naturally attracted the attention of the LID research community. Several attempts have been made to construct LID datasets from that resource. However, a ma jor challenge is to assign each tweet in the dataset to the correct language among the more than 70 languages used on the platform. The three commonlyused approaches are to rely on human labeling (Lui & Baldwin, 2014; Tromp & Pechenizkiy, 2011), machine detection (Tromp & Pech enizkiy, 2011; Jurgens et al., 2017), or user geolocation (Carter et al., 2013; Blodgett et al., 2017; Bergsma et al., 2012). Human labeling is an expensive process in terms of workload, and it is thus infeasible to apply it to create a massive dataset and get the full beneÔ¨Åt of Twitter‚Äôs scale. Automated LID labeling of this data creates a noisy and im perfect dataset, which is to be expected since the purpose of these datasets is to create new and better LID algorithms. And user geolocation is based on the assumption that users in a geographic region use the language of that region; an assumption that is not always correct, which is why this technique is usually paired with one of the other two. Our Ô¨Årst contribution in this paper is to propose a new approach to build and automatically label a Twitter LID dataset, and to show that it scales up well by building a dataset of over 18 million labeled tweets. Our hope is that our new Twitter dataset will become a benchmarking standard in the LID literature. Traditional LID models (Lui & Baldwin, 2012; Carter et al., 2013; Gamallo et al., 2014) proposed different ideas to design a set of useful features. This set of features is then passed to traditional machine learning algorithms such as Naive Bayes (NB). The resulting systems are capable of la beling thousands of inputs per second with moderate accu racy. Meanwhile, neural network models (Kocmi & Bojar,arXiv:1910.06748v1  [cs.CL]  15 Oct 2019Language IdentiÔ¨Åcation on Massive Datasets of Short Message using an Attention Mechanism CNN Table 1. Summary of literature results Paper Input Algorithm Metric Results langid.py Tromp & Pechenizkiy (2011) Character ngrams Graph Accuracy 0.975 0.941 Carter et al. (2013) Social network information Prior probabilities Accuracy 0.972 0.886 Gamallo et al. (2014) Words Dictionary F1score 0.733 N/A Jaech et al. (2016) Words LSTM F1score 0.912 0.879 Kocmi & Bojar (2017) Character ngrams GRU Accuracy 0.955 0.912 Jurgens et al. (2017) Character ngrams Encoderdecoder Accuracy 0.982 0.960 2017; Jurgens et al., 2017) approach the problem by design ing a deep and complex architecture like gated recurrent unit (GRU) or encoderdecoder net. These models use the message text itself as input using a sequence of character embeddings, and automatically learn its hidden structure via a deep neural network. Consequently, they obtain better results in the task but with an efÔ¨Åciency tradeoff. To allevi ate these drawbacks, our second contribution in this paper is to propose a shallow but efÔ¨Åcient neural LID algorithm. We followed previous neural LID (Kocmi & Bojar, 2017; Jurgens et al., 2017) in using character embeddings as in puts. However, instead of using a deep neural net, we pro pose to use a shallow ngramregional convolution neural network (CNN) with an attention mechanism to learn input representation. We experimentally prove that the ngram regional CNN is the best choice to tackle the bottleneck problem in neural LID. We also illustrate the behaviour of the attention structure in focusing on the most impor tant features in the text for the task. Compared with other benchmarks on our Twitter datasets, our proposed model consistently achieves new stateoftheart results with an improvement of 5% in accuracy and F1 score and a com petitive inference time. The rest of this paper is structured as follows. After a back ground review in the next section, we will present our Twit ter dataset in Section 3. Our novel LID algorithm will be the topic of Section 4. We will then present and analyze some experiments we conducted with our algorithm in Section 5, along with benchmarking tests of popular and literature LID systems, before drawing some concluding remarks in Section 6. Our Twitter dataset and our LID algorithm‚Äôs source code are publicly available1. 2. Related Work "
564,Skeptical Deep Learning with Distribution Correction.txt,"Recently deep neural networks have been successfully used for various
classification tasks, especially for problems with massive perfectly labeled
training data. However, it is often costly to have large-scale credible labels
in real-world applications. One solution is to make supervised learning robust
with imperfectly labeled input. In this paper, we develop a distribution
correction approach that allows deep neural networks to avoid overfitting
imperfect training data. Specifically, we treat the noisy input as samples from
an incorrect distribution, which will be automatically corrected during our
training process. We test our approach on several classification datasets with
elaborately generated noisy labels. The results show significantly higher
prediction and recovery accuracy with our approach compared to alternative
methods.","Generally, label noise comes from the stochastic process that the labels subject to before being presented to the learning algorithm (Angluin and Laird 1988). Typically, the noise is brought to the dataset during annotating. In recent years, deep neural networks have achieved great success in clas siÔ¨Åcation tasks, especially those with large perfectly labeled datasets. In some applications, however, it is very costly to annotate such large datasets by expert level annotators. Ac cepting amateur annotators or crowdsourcing are good solu tions, but those labels should be less credible, which result in label noise. Moreover, some untrusted annotators may label maliciously (Paudice, Mu ÀúnozGonz ¬¥alez, and Lupu 2018), which is a tricky problem. We aim for the cases that no any other prior information has been obtained except the noisylabeled dataset. Neither a set of clean labels nor the model of label noise is avail able. The problem is, since their validation sets are also with noisy labels in these cases, tuning can encounter an obsta cle. Therefore, it is important for a solution to gain the con Ô¨Ådence of the users. The method should be adaptive in order to face the various type of label noise. Also, methods with both good theoretical and empirical result will gain more application value. Our goal is to Ô¨Ånd the best method that meets the criteria above. There are several challenges here. First, we need to Ô¨Ånd a framework that accommodates most kinds of noise types. Second, we should derive a practical method that is robust tovarious types of label noise. Also, we need massive datasets with label noise that is difÔ¨Åcult for a deep neural network. We should Ô¨Ågure out how to generate those confusing labels from existed perfect datasets. In this paper, we Ô¨Årst introduce an approach named distri bution correction. We assume that the noisy dataset is sam pled from an incorrect distribution. Then, we derive how the expectation of values in the correct distribution can be repre sented in the noisy distribution. Also, We use this approach to explain the forward loss correction (Patrini et al. 2017). We implement the distribution correction by skeptical learn ing, where we substitute the correct distribution by the ex pressions with the model‚Äôs predictions. Although deep neural networks are usually more toler ant to the massive label noise (Rolnick et al. 2017) com pared to other machine learning algorithms such as SVMs (Nettleton, OrriolsPuig, and Fornells 2010), many previ ous works (Van Rooyen, Menon, and Williamson 2015; Xiao et al. 2015; Goldberger and BenReuven 2016; Ghosh, Kumar, and Sastry 2017; Patrini et al. 2017; Sukhbaatar et al. 2014) have shown that the mere logistic loss is not the best option. Typically, they either changed the loss function, or added dynamic processes to each label. People can also adopt heuristic solutions such as trick (Li and Long 2000) orbound (Cristianini and ShaweTaylor 2000), which use the times of prediction failures in the training process as a measure of the conÔ¨Ådence to each data entry. We believe these heuristic solutions are based on the generalization abil ity of a model and force the model to be more consistent with itself. However, when it comes to complex classiÔ¨Åca tion tasks of multiple labels, it usually takes many iterations to converge to the extent that we have enough conÔ¨Ådence in its predictions. As the dataset we have is polluted, we usually do not know when the predictions have become referable, so the solution should be adaptive. Finally, our solution com bines these two aspects into an adaptive training process. We also propose a solution to generate a noisylabeled dataset that is confusing to a model from an existed dataset, by sampling the incorrect label from the predictions of a trained deep neural networks. The type of label noise can be categorized as a completely random, a label relevant, and both label and feature dependent random process (Fr ¬¥enay and Verleysen 2014). Usually, The latter two types better de scribe the noise in reality, while they are difÔ¨Åcult to be genarXiv:1811.03821v3  [cs.LG]  13 Jan 2019erated manually. Indeed, researchers can collect actual noisy labels (Vahdat 2017), but it is still costly to collect enough such datasets to do massive tests. Our solution generates noisy datasets of high quality in large quantities, which can make the empirical experiments more convincing. 2 Related Works "
284,Enhancing Certifiable Robustness via a Deep Model Ensemble.txt,"We propose an algorithm to enhance certified robustness of a deep model
ensemble by optimally weighting each base model. Unlike previous works on using
ensembles to empirically improve robustness, our algorithm is based on
optimizing a guaranteed robustness certificate of neural networks. Our proposed
ensemble framework with certified robustness, RobBoost, formulates the optimal
model selection and weighting task as an optimization problem on a lower bound
of classification margin, which can be efficiently solved using coordinate
descent. Experiments show that our algorithm can form a more robust ensemble
than naively averaging all available models using robustly trained MNIST or
CIFAR base models. Additionally, our ensemble typically has better accuracy on
clean (unperturbed) data. RobBoost allows us to further improve certified
robustness and clean accuracy by creating an ensemble of already certified
models.","The lack of robustness in deep neural networks (DNNs) has motivated recent research on verifying and improv ing the robustness of DNN models (Katz et al., 2017; Dvijotham et al., 2018c; Gehr et al., 2018; Singh et al., 2018; Madry et al., 2018; Raghunathan et al., 2018b; Wong and Kolter, 2018; Zhang et al., 2018). Improv ing the robustness of neural networks, or designing the ‚Äúdefense‚Äù to adversarial examples, is a challenging problem. Athalye et al. (2018); Uesato et al. (2018) showed that many proposed defenses are broken and do not signiÔ¨Åcantly increase robustness under adaptive attacks. So far, stateoftheart defense methods in clude adversarial training (Madry et al., 2018; Sinha et al., 2018) and optimizing a certiÔ¨Åed bound on ro bustness (Wong and Kolter, 2018; Raghunathan et al., This is an extended version of ICLR 2019 Safe Machine Learning Workshop paper, ‚ÄúRobBoost: A provable approach to boost the robustness of deep model ensemble‚Äù. May 6, 2019. New Orleans, LA, USA2018a; Wang et al., 2018a; Mirman et al., 2018), but there is still a long way to go to conquer the adversarial example problem. On MNIST at perturbation = 0:3, adversarially trained models (Madry et al., 2018) are resistant to strong adversarial attacks but cannot be eÔ¨Éciently certiÔ¨Åed using existing neural network ver iÔ¨Åcation techniques. On the other hand, certiÔ¨Åable training methods usually suÔ¨Äer from high clean and veriÔ¨Åed error; in (Wong et al., 2018), the best model achieves 43.1% veriÔ¨Åed error and 14.9% clean error, which is much higher than ordinary MNIST models. Mostoftheseexistingdefensemethodsonlyfocusonim proving the robustness of a single model. Traditionally, a model ensemble has been used to improve prediction accuracy of weak models. For example, voting or boot strap aggregating (bagging) can be used to improve prediction accuracy in many occasions. Furthermore, boosting based algorithms, including AdaBoost (Fre und and Schapire, 1997), LogitBoost (Friedman et al., 2000), gradient boosting (Friedman, 2001, 2002) and many other variants, are designed to minimize an up per bound (surrogate loss) of classiÔ¨Åcation error, which provably increases the model‚Äôs accuracy despite the fact that each base model can be very weak. Inspired by the successful story of model ensembles, our question is that if a similar technique can be used to build a robust model with better provable robustness via an ensemble of certiÔ¨Åable base models? Intuitively, attacking an ensemble seems to be harder than attacking a single model, because an adversary must fool all models simultaneously. Some works on using a model ensemble to defend against adversarial examples (Abbasi and Gagn√©, 2017; Strauss et al., 2017; Liu et al., 2018; Pang et al., 2019; Kariyappa and Qureshi, 2019) show promising results that they can indeed increase the required adversarial distortion for a successful attack and improve robustness. However, none of these works attempt to propose a provable (orcertiÔ¨Åable ) method to improve model robustness via an ensemble, so there is no guarantee that these methods work in all situations. For example, He et al. (2017) reported that attacking a specialist ensemble only increases the required adversarial distortion by as little as 6% compared to a single model. In this paper, we propose a new algorithm, RobBoost , that can provably enhance the robustness certiÔ¨Åcate ofarXiv:1910.14655v1  [stat.ML]  31 Oct 2019Enhancing CertiÔ¨Åable Robustness via a Deep Model Ensemble a deep model ensemble. First, we consider the setting where a set of pretrained robust models are given, and we aim to Ô¨Ånd the optimal weights for each base classi Ô¨Åer that maximize provable robustness. We select the weight for each base model iteratively, to maximize a lower bound on the classiÔ¨Åcation margin of the neu ral network ensemble classiÔ¨Åer. Given a set of Tbase models which are individually certiÔ¨Åable, we formulate a certiÔ¨Åed robustness bound for the model ensemble and show that solving the optimal ensemble leads to an optimization problem. We propose a coordinate de scent based algorithm to iteratively solve the RobBoost objective. Second, we consider training each base classi Ô¨Åer sequentially from scratch, in a setting more similar to traditional gradient boosting, where each model is sequentially trained to improve the overall robustness of the current ensemble. Our experiments show that RobBoost can select a set of good base classiÔ¨Åers and weight them optimally, outperforming a naive average of all base models; on MNIST with `1perturbation of= 0:3, RobBoost reduces veriÔ¨Åed error from 36% (averaging models) to 34%using 12 certiÔ¨Åable base models trained individually. 2 Related Work "
119,Gaussian Mixture Variational Autoencoder with Contrastive Learning for Multi-Label Classification.txt,"Multi-label classification (MLC) is a prediction task where each sample can
have more than one label. We propose a novel contrastive learning boosted
multi-label prediction model based on a Gaussian mixture variational
autoencoder (C-GMVAE), which learns a multimodal prior space and employs a
contrastive loss. Many existing methods introduce extra complex neural modules
like graph neural networks to capture the label correlations, in addition to
the prediction modules. We find that by using contrastive learning in the
supervised setting, we can exploit label information effectively in a
data-driven manner, and learn meaningful feature and label embeddings which
capture the label correlations and enhance the predictive power. Our method
also adopts the idea of learning and aligning latent spaces for both features
and labels. In contrast to previous works based on a unimodal prior, C-GMVAE
imposes a Gaussian mixture structure on the latent space, to alleviate the
posterior collapse and over-regularization issues. C-GMVAE outperforms existing
methods on multiple public datasets and can often match other models' full
performance with only 50% of the training data. Furthermore, we show that the
learnt embeddings provide insights into the interpretation of label-label
interactions.","In many machine learning tasks, an instance can have sev eral labels. The task of predicting multiple labels is known as multilabel classiÔ¨Åcation (MLC). MLC is common in domains like computer vision (Wang et al., 2016), natural language processing (Chang et al., 2019) and biology (Yu et al., 2013). Unlike the singlelabel scenario, label corre 1Department of Computer Science, Cornell University, Ithaca, USA. Correspondence to: Shufeng Kong <sk2299@cornell.edu >. Proceedings of the 39thInternational Conference on Machine Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy right 2022 by the author(s).lations are more important in MLC. Early works capture the correlations through classiÔ¨Åer chains (Read et al., 2009), Bayesian inference (Zhang & Zhou, 2007), and dimension ality reduction (Bhatia et al., 2015). Thanks to the huge capacity of neural networks (NN), many previous methods can be improved by their neural exten sions. For example, classiÔ¨Åer chains can be naturally en hanced by recurrent neural networks (RNN) (Wang et al., 2016). The nonlinearity of NN alleviates the complex design of feature mapping and many deep models can there fore focus on the loss function, featurelabel and labellabel correlation modeling. One trending direction is to learn a deep latent space shared by features and labels. The encoded samples from the latent space are then decoded to targets. One typical example is C2AE (Yeh et al., 2017), which learns latent codes for both features and labels. The latent codes are passed to a decoder to derive the target labels. C2AE minimizes an `2distance between the feature and label codes, together with a relaxed orthogonality regularization. However, the learnt determin istic latent space lacks smoothness and structures. Small perturbations in this latent space can lead to totally different decoding results. Even if the corresponding feature and la bel codes are close, we cannot guarantee the decoded targets are similar. To address this concern, MPV AE (Bai et al., 2020) proposes to replace the deterministic latent space with a probabilistic space under a variational autoencoder (V AE) framework. The Gaussian latent spaces are aligned with KLdivergence, and the sampling process enforces smooth ness. Similar ideas can be found in (Sundar et al., 2020). However, these methods assume a unimodal Gaussian latent space, which is known to cause overregularization and pos terior collapse (Dilokthanakul et al., 2016; Wu & Goodman, 2018). A better strategy would be to learn a multimodal latent space. It is more reasonable to assume the observed data are generated from a multimodal subspace rather than a unimodal one. Another popular group of methods focuses on better label correlation modeling. Their idea is straightforward: some la bels should be more correlated if they coappear often while others should be less relevant. Existing methods adopt pair wise ranking loss, covariance matrices, conditional randomarXiv:2112.00976v2  [cs.LG]  10 Jun 2022Gaussian Mixture V AE with Contrastive Learning for MultiLabel ClassiÔ¨Åcation Ô¨Åelds (CRF) or graph neural nets (GNN) to this end (Zhang & Zhou, 2013; Bi & Kwok, 2014; Belanger & McCallum, 2016; Lanchantin et al., 2019; Chen et al., 2019b). These methods often either constrain the learning through a pre deÔ¨Åned structure (which requires a larger model size), or aren‚Äôt powerful enough to capture the correlations (such as pairwise ranking loss). Our idea is simple: we learn embeddings for each label class and the inner products between embeddings should reÔ¨Çect the similarity. We further learn feature embeddings whose inner products with label embeddings correspond to featurelabel similarity and can be used for prediction. We assume these embeddings are generated from a probabilistic multimodal latent space shared by features and labels, where we use KLdivergence to align the feature and label latent distributions. On the other hand, one might be concerned that embeddings alone won‚Äôt capture both labellabel and labelfeature correlations, which were usually modeled by extra GNN and covariance matrices in prior works (Lan chantin et al., 2019; Bai et al., 2020). To this end, we stress on the loss function terms rather than extra structure to cap ture these correlations. Intuitively, if two labels coappear often, their embeddings should be close. Otherwise, if two labels seldom coappear, their embeddings should be distant. A tripletlike loss could be naturally applied in this scenario. Nonetheless, its extension, contrastive loss, has shown to be even more effective than the triplet loss by introducing more samples rather than just one triplet. We show that con trastive loss can pull together correlated label embeddings, push away unrelated label embeddings (see Fig. 3), and even perform better than GNNbased or covariancebased methods. Our new model for MLC, contrastive learning boosted Gaus sian mixture variational autoencoder (CGMV AE), allevi ates the overregularization and posterior collapse concerns, and also learns useful feature and label embeddings. C GMV AE is applied to nine datasets and outperforms the existing methods on Ô¨Åve metrics. Moreover, we show that using only 50% of the data, our results can match the full performance of other stateoftheart methods. Ablation studies and interpretability of learnt embeddings will also be illustrated in the experiments. Our contributions can be summarized in three aspects: (i)We propose to use con trastive loss instead of triplet or ranking loss to strengthen the label embedding learning. We empirically show that by using a contrastive loss, one can get rid of heavyduty label correlation modules (e.g., covariance matrices, GNNs) while achieving even better performances. (ii)Though con trastive learning is commonly applied in selfsupervised learning, our work shows that by properly deÔ¨Åning anchor, positive and negative samples, contrastive loss can leverage label information very effectively in the supervised MLC scenario as well. (iii)Unlike prior probabilistic models,CGMV AE learns a multimodal latent space and integrates the probabilistic modeling (V AE module) with embedding learning (contrastive module) synergistically. 2. Methods "
228,Quantifying uncertainty for deep learning based forecasting and flow-reconstruction using neural architecture search ensembles.txt,"Classical problems in computational physics such as data-driven forecasting
and signal reconstruction from sparse sensors have recently seen an explosion
in deep neural network (DNN) based algorithmic approaches. However, most DNN
models do not provide uncertainty estimates, which are crucial for establishing
the trustworthiness of these techniques in downstream decision making tasks and
scenarios. In recent years, ensemble-based methods have achieved significant
success for the uncertainty quantification in DNNs on a number of benchmark
problems. However, their performance on real-world applications remains
under-explored. In this work, we present an automated approach to DNN discovery
and demonstrate how this may also be utilized for ensemble-based uncertainty
quantification. Specifically, we propose the use of a scalable neural and
hyperparameter architecture search for discovering an ensemble of DNN models
for complex dynamical systems. We highlight how the proposed method not only
discovers high-performing neural network ensembles for our tasks, but also
quantifies uncertainty seamlessly. This is achieved by using genetic algorithms
and Bayesian optimization for sampling the search space of neural network
architectures and hyperparameters. Subsequently, a model selection approach is
used to identify candidate models for an ensemble set construction. Afterwards,
a variance decomposition approach is used to estimate the uncertainty of the
predictions from the ensemble. We demonstrate the feasibility of this framework
for two tasks - forecasting from historical data and flow reconstruction from
sparse sensors for the sea-surface temperature. We demonstrate superior
performance from the ensemble in contrast with individual high-performing
models and other benchmarks.","1.1 Motivation Datadriven surrogate modeling research has shown great promise in improving the predictability and efÔ¨Åciency of computational physics applications. Among various algorithms, deep learningbased models have been observed to show signiÔ¨Åcant gains in accuracy and timetosolution over classical numericalmethods based techniques. However, the widespread adoption of deep learning models is still limited by its blackbox nature. To that end, uncertainty Email: rmaulik@anl.govarXiv:2302.09748v1  [cs.LG]  20 Feb 2023Neural architecture search ensembles for scientiÔ¨Åc machine learning A P REPRINT quantiÔ¨Åcation methods have been developed to overcome the challenges associated with the blackbox nature of the deeplearning models and establish trustworthiness by providing uncertainty estimates along with the predictions. The data or aleatoric uncertainty is attributed to the noise in the data, for example, low resolution sensors and sparse measurements; the model or epistemic uncertainty is attributed to the lack of training data. The former is inherent to the data and cannot be reduced by collecting more data; the latter is used to characterize the model‚Äôs predictive capability with respect to the training data and it can reduced by collecting appropriate data. In many computational physics applications, it is crucial to effectively quantify both data and modelform uncertainties in predictions from deep learning models. QuantiÔ¨Åcation of such uncertainties alleviates the risks associated with the deployment of such blackbox datadriven models for realworld tasks. Deep ensembles is a promising approach for uncertainty quantiÔ¨Åcation. In this approach, a ensemble of neural networks (NNs) are trained independently but they differ in the way in which they are trained. Consequently, the weights of the neural network parameters will be different. The prediction from these models are then used to improve prediction and estimate uncertainty. Despite its simplicity, deepensemblesbased uncertainty estimation has achieved superior performance over more sophisticated uncertainty quantiÔ¨Åcation methods on a number of benchmarks. Crucial to the effectiveness of uncertainty quantiÔ¨Åcation in the deepensemblebased methods is the diversity of the highperforming models. SpeciÔ¨Åcally, if the models are signiÔ¨Åcantly different from each other and also equally high performing, then the prediction and the uncertainty estimates become accurate [Egele et al., 2022]. However, this poses additional challenges due to the large overhead associated with the manual design of models. Despite one time cost, the development overhead signiÔ¨Åcantly increases the ofÔ¨Çine design and training costs of the ensemble model development and increase amortization time (i.e., the time required to offset ofÔ¨Çine costs). To that end, we explore an integrated automated deep ensemble approach that not only discovers highperforming models but also makes ensemble predictions with quantiÔ¨Åed uncertainty in a scalable manner. SpeciÔ¨Åcally, the highlights of this article are as follows: ‚Ä¢We demonstrate a uniÔ¨Åed strategy for discovering highperforming deep learning models for dynamical systems forecasting and signal recovery with epistemic and aleatoric uncertainty quantiÔ¨Åcation that leverages distributed computing. ‚Ä¢For uncertainty quantiÔ¨Åcation, we use the law of decomposition of variance from members of an ensemble to separately estimate both the aleatoric and epistemic uncertainty. ‚Ä¢We validate our proposed approach for forecasting Ô¨ÇowÔ¨Åelds as well as instantaneous state reconstructions for a realworld, highdimensional scientiÔ¨Åc machine learning problem with complex dynamics, given by the NOAA Optimum Interpolation Data Set. 1.2 Related work "
159,Unsupervised Anomaly Detection in Stream Data with Online Evolving Spiking Neural Networks.txt,"Unsupervised anomaly discovery in stream data is a research topic with many
practical applications. However, in many cases, it is not easy to collect
enough training data with labeled anomalies for supervised learning of an
anomaly detector in order to deploy it later for identification of real
anomalies in streaming data. It is thus important to design anomalies detectors
that can correctly detect anomalies without access to labeled training data.
Our idea is to adapt the Online evolving Spiking Neural Network (OeSNN)
classifier to the anomaly detection task. As a result, we offer an Online
evolving Spiking Neural Network for Unsupervised Anomaly Detection algorithm
(OeSNN-UAD), which, unlike OeSNN, works in an unsupervised way and does not
separate output neurons into disjoint decision classes. OeSNN-UAD uses our
proposed new two-step anomaly detection method. Also, we derive new theoretical
properties of neuronal model and input layer encoding of OeSNN, which enable
more effective and efficient detection of anomalies in our OeSNN-UAD approach.
The proposed OeSNN-UAD detector was experimentally compared with
state-of-the-art unsupervised and semi-supervised detectors of anomalies in
stream data from the Numenta Anomaly Benchmark and Yahoo Anomaly Datasets
repositories. Our approach outperforms the other solutions provided in the
literature in the case of data streams from the Numenta Anomaly Benchmark
repository. Also, in the case of real data files of the Yahoo Anomaly Benchmark
repository, OeSNN-UAD outperforms other selected algorithms, whereas in the
case of Yahoo Anomaly Benchmark synthetic data files, it provides competitive
results to the results recently reported in the literature.","Unsupervised anomaly discovery in stream data is a research topic that has important practical applications. For example, an Internet system administra tor may be interested in recognition of abnormally high activity on a web page potentially caused by a hacker attack. Unexpected spiking usage of a CPU unit in a computer system could be another example of anomalous behaviour that mayrequireinvestigation. CorrectdetectionandclassiÔ¨Åcationofsuchanomalies may enable optimization of the performance of the computer system. However, in many cases, it is not easy to collect enough training data with labeled anoma lies for supervised learning of an anomaly detector in order to use it later for identiÔ¨Åcation of real anomalies in streaming data. It is thus particularly impor tant to design anomalies detectors that can correctly detect anomalies without access to labeled training data. Moreover, since the characteristic of an input data stream may be changing, the anomaly detector should learn in an online mode. In order to design an eÔ¨Äective anomaly detection system, one can consider adaptation of evolving Spiking Neural Networks (eSNNs) (Kasabov, 2014; Lobo et al., 2018, 2020b; MaciƒÖg et al., 2020) to the task. eSNN is a neural network with an evolving repository of output neurons, in which learning processes, neuronal communication and classiÔ¨Åcation of data instances are based solely on transmission of spikes from input neurons to output neurons (Kasabov, 2014). The spikes increase so called postsynaptic potential values of output neurons, and directly inÔ¨Çuence the classiÔ¨Åcation results. The input layer of neurons in eSNN transforms input data instances into spikes. Depending on the type of input data, the transformation can be carried out by means of the temporal encoding methods such as StepForward or ThresholdBased (Petro et al., 2019; MaciƒÖg et al., 2019) or, alternatively, with the use of Gaussian Receptive Fields (Lobo et al., 2018). The distinctive feature of the eSNN is that its repository of output neurons evolves during the training phase based on candidate output neurons that are created for every new input data sample (Kasabov et al., 2013; Kasabov, 2015). More speciÔ¨Åcally, for each new input value, a new candidate output neuron is created and is either added to the output repository or, based on the provided similarity threshold, is merged with one of the output neurons contained in the repository. Recently, an online variant OeSNN of eSNN was proposed for classiÔ¨Åcation of stream data (Lobo et al., 2018). Contrary to the eSNN architecture, the size of the evolving repository of output neurons in OeSNN is limited. When the repository of output neurons is full and a new candidate output neuron is signiÔ¨Åcantly diÔ¨Äerent from all of the neurons in the repository, an oldest neuron is replaced with the new candidate output neuron. It was claimed in (Lobo 2et al., 2018) that OeSNN is able to make fast classiÔ¨Åcation of input stream data, while preserving restrictive memory limits. Considering all the positive features of eSNN and OeSNN, in this article, we oÔ¨Äer a novel Online evolving Spiking Neural Network for Unsupervised Anomaly Detection (OeSNNUAD) in stream data. Our main contributions presented in this article are as follows: ‚Ä¢We oÔ¨Äer a new OeSNNUAD anomaly detector working online in an unsu pervised way. It adapts the architecture of OeSNN, which also works in an online way, but, unlike OeSNNUAD, requires supervised training. The main distinction between our detector and OeSNN lies in applying diÔ¨Äer ent models of an output layer and diÔ¨Äerent methods of learning and input values classiÔ¨Åcation. While output neurons of OeSNN are divided into separate decision classes, there is no such separation of output neurons in our approach. Rather than that, each new output neuron is assigned an output value, which is Ô¨Årst randomly generated based on recent input values and then is updated in the course of learning of OeSNNUAD. ‚Ä¢As a part of the proposed OeSNNUAD detector, we oÔ¨Äer a new anomaly classiÔ¨Åcation method, which classiÔ¨Åes an input value as anomalous only in the following two cases: 1. if none of output neurons in the repository Ô¨Åres, or otherwise, 2. if an error between an input value and its OeSNNUAD prediction is greater than the average prediction error plus usergiven multiplicity of the standard deviation of the recent prediction errors. This twostep approach to classiÔ¨Åcation of an input value as anomalous or not enables more eÔ¨Äective detection of anomalies in input stream data and to the best of our knowledge was not previously used in the literature. ‚Ä¢We derive the important theoretical property of the OeSNN neuronal model that shows that the values of postsynaptic potential thresholds of all output neurons are the same. This property is inherited by our OeSNNUAD detector. The obtained result eliminates the necessity of recalculation of these thresholds when output neurons of OeSNN, as well as of OeSNNUAD, are updated in the course of the learning process, and increases the speed of classiÔ¨Åcation of input stream data. Moreover, we also prove that Ô¨Åring order values of input neurons do not depend on val ues ofTSandŒ≤parameters, which were previously used in OeSNNs for input value encoding with Gaussian Receptive Fields. ‚Ä¢We prove experimentally that in the case of stream data from the Nu menta Anomaly Benchmark repository (Ahmad et al., 2017b) as well as from the Yahoo Anomaly Datasets repository (Yahoo! Webscope, 2015) the proposed OeSNNUAD detects anomalies in unsupervised way more eÔ¨Äectively than other stateoftheart unsupervised and semisupervised detectors proposed in the literature. 3‚Ä¢Eventually, we argue that the proposed OeSNNUAD is able to make fast detection of anomalies among data stream input values and works eÔ¨Éciently in environments with imposed restrictive memory limits. The paper is structured as follows. In Section 2, we overview the related work. In Section 3, we present the architecture of Online evolving Spiking Neural Networks, whose adaptation proposed by us will be then used in OeSNN UAD. In Section 4, we provide new theoretical properties of neuronal model and input layer encoding of OeSNN, which enable more eÔ¨Äective and eÔ¨Écient detection of anomalies in our OeSNNUAD approach. In Section 5, we oÔ¨Äer our online method to unsupervised anomaly detection in stream data OeSNNUAD. Section 6 presents and discusses the proposed OeSNNUAD algorithm in detail. In Section 7, we present the results of comparative experimental evaluation of the proposed OeSNNUAD detector and stateoftheart unsupervised and semisupervised detectors of anomalies. We conclude our work in Section 8. 2. Related Work "
263,Memorization-Dilation: Modeling Neural Collapse Under Label Noise.txt,"The notion of neural collapse refers to several emergent phenomena that have
been empirically observed across various canonical classification problems.
During the terminal phase of training a deep neural network, the feature
embedding of all examples of the same class tend to collapse to a single
representation, and the features of different classes tend to separate as much
as possible. Neural collapse is often studied through a simplified model,
called the unconstrained feature representation, in which the model is assumed
to have ""infinite expressivity"" and can map each data point to any arbitrary
representation. In this work, we propose a more realistic variant of the
unconstrained feature representation that takes the limited expressivity of the
network into account. Empirical evidence suggests that the memorization of
noisy data points leads to a degradation (dilation) of the neural collapse.
Using a model of the memorization-dilation (M-D) phenomenon, we show one
mechanism by which different losses lead to different performances of the
trained network on noisy data. Our proofs reveal why label smoothing, a
modification of cross-entropy empirically observed to produce a regularization
effect, leads to improved generalization in classification tasks.","The empirical success of deep neural networks has accelerated the introduction of new learning algorithms and triggered new applications, with a pace that makes it hard to keep up with profound theoretical foundations and insightful explanations. As one of the few yet particularly appealing theo retical characterizations of overparameterized models trained for canonical classiÔ¨Åcation tasks, Neural Collapse (NC) provides a mathematically elegant formalization of learned feature representations Papyan et al. (2020). To explain NC, consider the following setting. Suppose we are given a balanced datasetD=n (x(k) n;yn)o k2[K];n2[N]XY in the instance space X=Rdand label spaceY= [N] := f1;:::;Ng, i.e. each class n2[N]has exactlyKsamples x(1) n;:::;x(K) n. We consider network architectures commonly used in classiÔ¨Åcation tasks that are composed of a feature engineering part g:X!RM(which maps an input signal x2X to its feature representation g(x)2RM) and a linear classiÔ¨Åer W() +bgiven by a weight matrix W2RNMas well as a bias vector b2RN. Letwndenote the row vector of Wassociated with class n2[N]. During training, both classiÔ¨Åer components are simultaneously optimized by minimizing the crossentropy loss. *These authors contributed equally to this work. 1arXiv:2206.05530v3  [cs.LG]  4 Apr 2023Published as a conference paper at ICLR 2023 Denoting the feature representations g(x(k) n)of the sample x(k) nbyh(k) n, the class means and the global mean of the features by hn:=1 KKX i=1h(k) n;h:=1 NNX n=1hn; NC consists of the following interconnected phenomena (where the limits take place as training progresses): (NC1) Variability collapse. For each class n2[N], we have1 KPK k=1   h(k) n"
417,Towards Robust Neural Networks via Random Self-ensemble.txt,"Recent studies have revealed the vulnerability of deep neural networks: A
small adversarial perturbation that is imperceptible to human can easily make a
well-trained deep neural network misclassify. This makes it unsafe to apply
neural networks in security-critical applications. In this paper, we propose a
new defense algorithm called Random Self-Ensemble (RSE) by combining two
important concepts: {\bf randomness} and {\bf ensemble}. To protect a targeted
model, RSE adds random noise layers to the neural network to prevent the strong
gradient-based attacks, and ensembles the prediction over random noises to
stabilize the performance. We show that our algorithm is equivalent to ensemble
an infinite number of noisy models $f_\epsilon$ without any additional memory
overhead, and the proposed training procedure based on noisy stochastic
gradient descent can ensure the ensemble model has a good predictive
capability. Our algorithm significantly outperforms previous defense techniques
on real data sets. For instance, on CIFAR-10 with VGG network (which has 92\%
accuracy without any attack), under the strong C\&W attack within a certain
distortion tolerance, the accuracy of unprotected model drops to less than
10\%, the best previous defense technique has $48\%$ accuracy, while our method
still has $86\%$ prediction accuracy under the same level of attack. Finally,
our method is simple and easy to integrate into any neural network.","Deep neural networks have demonstrated their success in many machine learning and computer vision applications, including image classication [14,7,35,9,34], object recognition [30] and image captioning [38]. Despite having nearperfect prediction performance, recent studies have revealed the vulnerability of deep neural networks to adversarial examples|given a correctly classied image, a carefully designed perturbation to the image can make a welltrained neural network misclassify. Algorithms crafting these adversarial images, called attack algorithms, are designed to minimize the perturbation, thus making adversarial images hard to be distinguished from natural images. This leads to securityarXiv:1712.00673v2  [cs.LG]  1 Aug 20182 X. Liu, M. Cheng, H. Zhang and CJ. Hsieh concerns, especially when applying deep neural networks to securitysensitive systems such as selfdriving cars and medical imaging. To make deep neural networks more robust to adversarial attacks, several defense algorithms have been proposed recently [23,40,17,16,39]. However, recent studies showed that these defense algorithms can only marginally improve the accuracy under the adversarial attacks [4,5]. In this paper, we propose a new defense algorithm: Random SelfEnsemble (RSE). More specically, we introduce the new \noise layer"" that fuses input vector with randomly generated noise, and then we insert this layer before each convolution layer of a deep network. In the training phase, the gradient is still computed by backpropagation but it will be perturbed by random noise when passing through the noise layer. In the inference phase, we perform several for ward propagations, each time with dierent prediction scores due to the noise layers, and then ensemble the results. We show that RSE makes the network more resistant to adversarial attacks, by virtue of the proposed training and testing scheme. Meanwhile, it will only slightly aect test accuracy when no at tack is performed on natural images. The algorithm is trivial to implement and can be applied to any deep neural networks for the enhancement. Intuitively, RSE works well because of two important concepts: ensemble andrandomness . It is known that ensemble of several trained models can improve the robustness [29], but will also increase the model size by kfolds. In contrast, without any additional memory overhead, RSE can construct innite number of models f, whereis generated randomly, and then ensemble the results to improve robustness. But how to guarantee that the ensemble of these models can achieve good accuracy? After all, if we train the original model without noise, yet only add noise layers at the inference stage, the algorithm is going to perform poorly. This suggests that adding random noise to an pre trained network will only degrade the performance. Instead, we show that if the noise layers are taken into account in the training phase, then the training procedure can be considered as minimizing the upper bound of the loss of model ensemble, and thus our algorithm can achieve good prediction accuracy. The contributions of our paper can be summarized below: {We propose the Random SelfEnsemble (RSE) approach for improving the robustness of deep neural networks. The main idea is to add a \noise layer"" before each convolution layer in both training and prediction phases. The algorithm is equivalent to ensemble an innite number of random models to defense against the attackers. {We explain why RSE can signicantly improve the robustness toward adver sarial attacks and show that adding noise layers is equivalent to training the original network with an extra regularization of Lipschitz constant. {RSE signicantly outperforms existing defense algorithms in all our experi ments. For example, on CIFAR10 data and VGG network (which has 92% accuracy without any attack), under C&W attack the accuracy of unpro tected model drops to less than 10%; the best previous defense technique has 48% accuracy; while RSE still has 86 :1% prediction accuracy under theRSE for Robust Neural Networks 3 same strength of attacks. Moreover, RSE is easy to implement and can be combined with any neural network. 2 Related Work "
28,Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition.txt,"Noisy label Facial Expression Recognition (FER) is more challenging than
traditional noisy label classification tasks due to the inter-class similarity
and the annotation ambiguity. Recent works mainly tackle this problem by
filtering out large-loss samples. In this paper, we explore dealing with noisy
labels from a new feature-learning perspective. We find that FER models
remember noisy samples by focusing on a part of the features that can be
considered related to the noisy labels instead of learning from the whole
features that lead to the latent truth. Inspired by that, we propose a novel
Erasing Attention Consistency (EAC) method to suppress the noisy samples during
the training process automatically. Specifically, we first utilize the flip
semantic consistency of facial images to design an imbalanced framework. We
then randomly erase input images and use flip attention consistency to prevent
the model from focusing on a part of the features. EAC significantly
outperforms state-of-the-art noisy label FER methods and generalizes well to
other tasks with a large number of classes like CIFAR100 and Tiny-ImageNet. The
code is available at
https://github.com/zyh-uaiaaaa/Erasing-Attention-Consistency.","Facial Expression Recognition (FER) has wide applications in the real world, such as driver fragile detection, service robots, and humancomputer interac tion [35]. The most common paradigm for FER is the endtoend supervised manner, whose performance largely relies on the massive highquality annotated data. However, collecting largescale datasets with fully precise annotations is usuallyexpensiveandtimeconsuming,sometimesevenimpossible.Furthermore, facialexpressionimageshaveinherentinterclasssimilarity(allclassesarehuman faces) and annotation ambiguity (some expression images are quite confusing), making noisy label FER more challenging than traditional noisy label classifica tion tasks. On the other hand, it is wellknown that deep neural networks have enough capacity to memorize largescale data with even completely random la bels, leading to poor performance in generalization [2,19,48]. Therefore, robustarXiv:2207.10299v2  [cs.CV]  20 Sep 20222 Y. Zhang et al. FER with noisy labels has become an essential and challenging task in computer vision [4,7,9,18,35,38,47,49,50]. Mainstream noisy label FER methods can be mainly classified into two cat egories, sample selection and label ensembling. SCN [38] and RUL [50] can be viewed as sample selection methods, which learn more from clean samples and then relabel the noisy samples. SCN [38] uses a fullyconnected layer to learn an importance weight for each sample and suppresses uncertain samples during the trainingphase.RUL[50]learnsuncertaintyweightsthroughcomparisonbetween different samples. IPA2LT [35] and DMUE [35] are label ensembling methods, which provide several labels for a single sample to better mine the latent truth. IPA2LT [35] assigns each sample more than one labels with human annotations or model predictions while DMUE [35] uses a multibranch model to better mine the latent distribution in the label space. All the aforementioned methods get good performances under noisy label FER while they still have defects. Specif ically, sample selection methods are based on the smallloss assumption [2,48], which might confuse hard samples and noisy samples as both of them have large loss values during the training process. Sample selection methods also need the noise rate, which is nontrivial in largescale realworld datasets. Label ensem bling methods provide different views of the same sample using several networks, similar to crowdsourcing in real FER applications. However, the extra informa tiongaintheybringmightbenoisy.Labelensemblingmethodsmightbringgreat computation overhead, making them less preferable in real applications. Thus, the noisy label FER problem demands better methods that do not need to know the noise rate or train several models to perform well. In this paper, instead of following the traditional path to detect noisy sam ples according to their loss values and then suppress them, we view noisy label learning from a new featurelearning perspective and propose a novel framework to deal with all the aforementioned defects. We find that the FER model remem bers noisy samples by focusing on a part of the features that can be considered related to the noisy labels, shown in Figure 1. The image in the first column is labeled as sad, while its latent truth is surprise. SCN [38] remembers this noisy sample by focusing on the frown feature which can be considered related to the noisy label of the sad expression. However, it neglects the open mouth feature, which is vital for the correct classification as an open mouth combined with a frown leads to the latent truth surprise instead of the noisy label sad. From the attention regions of the noisy samples, we conclude that the FER model only observes a part of the features that can be considered related to the noisy la bels to remember noisy samples. It is intuitive as remembering noisy samples by focusing on a part of the features that can be considered related to the noisy labels does not contradict the other learned features from the clean samples. Inspired by this finding, we propose to deal with noisy label FER from a new featurelearning perspective. If the model can not focus on a part of the features and always learns from the whole features, then it cannot remember the noisy samples. Learning from the whole features from all training samples also meansLearn From All 3 Fig.1: (a) shows the attention regions of the noisy samples learned by SCN and EAC (Ours). NL represents the noisy label, LT represents the latent truth. The prediction results are shown under the images. SCN only focuses on a part of the features that can be considered related to the noisy labels to remember the noisysamples.(b)showsSCNpredictsdifferentlyontheflippedimage.OurEAC forces the model to focus on similar parts before and after the flip to prevent the model from remembering noisy labels. the model does not need to filter out largeloss samples like traditional methods which might confuse useful hard samples with noisy samples. In this paper, we use Attention Consistency to implement the consistency regularization. Attention Consistency [11] assumes that the learned attention maps should follow the same transformation as the input images to achieve better multilabel classification performance. The attention maps denote the features that the model based on to make the predictions. We find that the flip semantic consistency of facial expression images can help to detect noisy labels. Flip semantic consistency means the original image and its flipped counterpart should be classified into the same category. However, if we train a FER model with a noisy sample, the model might remember the noisy sample while it still predicts the latent truth on its flipped counterpart, shown as the images in the first row of Figure 1. Inspired by that, we propose an imbalanced framework to prevent the model from remembering noisy samples. Specifically, we onlycompute classification loss on the original images and com pute consistency loss between the attention maps extracted from the original images and their flipped counterparts. We utilize the consistency loss to prevent the model from remembering a part of the features of the original images. Such an imbalanced framework cannot help the model totally get rid of the noisy labels as the model can still gradually overfit the attention maps of the flipped images to keep the consistency loss small, which degrades the regularization ef fect. We further propose Erasing Attention Consistency (EAC) to increase the performance of the imbalanced framework. Before flipping, we first randomly erase the input images during the whole training phase. During the training4 Y. Zhang et al. phase, the dynamic changing of the erased area ensures that the model can not simply remember the attention maps before and after the flip to get small con sistency loss values. When the model starts to overfit the noisy original samples by focusing on a part of the features related to the noisy labels, the attention maps of the original images will deviate largely from the attention maps of their flipped counterparts, which will lead to large consistency loss values. We set the weight of the consistency loss larger enough to ensure the model first optimizes the consistency loss. Thus, to get small consistency loss values, the model will automatically quit overfitting the noisy samples. The main contributions of our work are as follows: 1. Instead of using traditional methods which deal with noisy labels from high level smallloss selection, we cope with noisy labels from middlelevel feature learning, which does not require the noise rate to perform well. 2. We propose a novel method named Erasing Attention Consistency (EAC) which automatically prevents the model from memorizing noisy samples. 3. We experimentally show that EAC significantly advances stateoftheart results on multiple FER benchmarks with different levels of label noise. EAC also generalizes well to image classification tasks with a large number of classes. 2 Related Work "
295,Part-based Pseudo Label Refinement for Unsupervised Person Re-identification.txt,"Unsupervised person re-identification (re-ID) aims at learning discriminative
representations for person retrieval from unlabeled data. Recent techniques
accomplish this task by using pseudo-labels, but these labels are inherently
noisy and deteriorate the accuracy. To overcome this problem, several
pseudo-label refinement methods have been proposed, but they neglect the
fine-grained local context essential for person re-ID. In this paper, we
propose a novel Part-based Pseudo Label Refinement (PPLR) framework that
reduces the label noise by employing the complementary relationship between
global and part features. Specifically, we design a cross agreement score as
the similarity of k-nearest neighbors between feature spaces to exploit the
reliable complementary relationship. Based on the cross agreement, we refine
pseudo-labels of global features by ensembling the predictions of part
features, which collectively alleviate the noise in global feature clustering.
We further refine pseudo-labels of part features by applying label smoothing
according to the suitability of given labels for each part. Thanks to the
reliable complementary information provided by the cross agreement score, our
PPLR effectively reduces the influence of noisy labels and learns
discriminative representations with rich local contexts. Extensive experimental
results on Market-1501 and MSMT17 demonstrate the effectiveness of the proposed
method over the state-of-the-art performance. The code is available at
https://github.com/yoonkicho/PPLR.","Person reidentification (reID) aims to retrieve a per son corresponding to a given query across disjoint camera views or different time stamps [59, 69]. Thanks to the dis criminative power of deep neural networks, supervised ap proaches [20‚Äì22,53] have achieved impressive performance in this task. Unfortunately, they require a large amount of labeled data that demands costly annotations, limiting their practicality in largescale realworld reID problems. Dueto this issue, unsupervised methods that learn the discrimi native features for person retrieval from unlabeled data have recently received much attention. Prior works on unsupervised person reID have utilized pseudolabels obtained by knearest neighbor search [25, 47,60] or unsupervised clustering [7,24] for training. These approaches alternate a twostage training scheme: the label generation phase that assigns pseudolabels and the train ing phase that trains a model with generated labels. Among these approaches, the clusteringbased methods [2, 9] have especially demonstrated their effectiveness with stateof theart performance. However, inherent noises in pseudo labels significantly hinder the performance of these unsu pervised methods. To tackle this problem, many efforts have been made to improve the accuracy of pseudolabels by performing robust clustering [9, 62] or pseudolabel refinement [25, 64]. Re cent techniques [8, 63] significantly reduce the label noise through the model ensemble in a peerteaching manner by using predictions from an auxiliary network as refined la bels for the target network. Nevertheless, training multi ple backbones as teacher networks ( e.g., dual ResNet in MMT [8], and single DenseNet, ResNet, and Inceptionv3 in MEBNet [63]) requires high computational costs. Fur thermore, labels refined by these methods consider only global features and neglect the finegrained clues essential to person reID, leading to insufficient performance. To address the aforementioned problems, we propose Partbased Pseudo Label Refinement (PPLR) , a novel unsu pervised reID framework that effectively handles the label noise using part features in a selfteaching manner. Sev eral studies [42,67] demonstrate that the finegrained infor mation from part features improves the reID performance. Our key idea is that this finegrained information can pro vide not only useful cues for better representation learning but also robustness against label noises. In contrast to the globalshape information that has large variations due to significant changes in poses and viewpoints, part features can capture the localtexture information that provides a more crucial clue to reidentifying a person [70]. We argue that the complementary relationship between 1the global and part features can be used to refine the label noise in each of their feature spaces. However, some of the global and part features from the same image capture very different semantic information, and using the comple mentary relationship na ¬®ƒ±vely can result in noisy and even incorrect information. For instance, images may contain ir relevant parts ( e.g., occlusions or backgrounds) that provide unreliable complementary information, and it is desirable to exclude them from the training. Therefore, it is essential to identify whether the information of global and part features are reliable with each other to properly exploit their com plementary relationship. To address this issue, we design a cross agreement score based on the similarity between the knearest neighbors of global and part features. Based on the cross agreement, we propose two pseudolabel refine ment methods ‚Äì partguided label refinement (PGLR) and agreementaware label smoothing (AALS) . PGLR refines the pseudolabels of global features by aggregating the pre dictions of part features, guiding the global features to learn from rich local contexts. AALS refines the pseudolabels of part features by smoothing the label distributions, thus calibrating the predictions of part features. Our contributions can be summarized as follows: ‚Ä¢ We propose a partbased pseudolabel refinement framework that operates in a selfensemble manner without auxiliary networks. To the best of our knowl edge, this is the first work to handle the label noise using the part feature information for person reID. ‚Ä¢ We design a cross agreement score to capture reli able complementary information, which is computed by the similarity between the knearest neighbors of the global and part features. ‚Ä¢ Extensive experimental results with superior perfor mance against the stateoftheart methods demon strate the effectiveness of the proposed method. 2. Related Work "
144,INN: A Method Identifying Clean-annotated Samples via Consistency Effect in Deep Neural Networks.txt,"In many classification problems, collecting massive clean-annotated data is
not easy, and thus a lot of researches have been done to handle data with noisy
labels. Most recent state-of-art solutions for noisy label problems are built
on the small-loss strategy which exploits the memorization effect. While it is
a powerful tool, the memorization effect has several drawbacks. The
performances are sensitive to the choice of a training epoch required for
utilizing the memorization effect. In addition, when the labels are heavily
contaminated or imbalanced, the memorization effect may not occur in which case
the methods based on the small-loss strategy fail to identify clean labeled
data. We introduce a new method called INN(Integration with the Nearest
Neighborhoods) to refine clean labeled data from training data with noisy
labels. The proposed method is based on a new discovery that a prediction
pattern at neighbor regions of clean labeled data is consistently different
from that of noisy labeled data regardless of training epochs. The INN method
requires more computation but is much stable and powerful than the small-loss
strategy. By carrying out various experiments, we demonstrate that the INN
method resolves the shortcomings in the memorization effect successfully and
thus is helpful to construct more accurate deep prediction models with training
data with noisy labels.","Learning deep neural networks (DNNs) has achieved impressive successes in many research Ô¨Åelds but has suffered from collecting massive cleanannotated training samples such as ImageNet [ 1] and MSCOCO [ 2]. Since annotating procedures are usually done manually by human experts, it is expensive and timeconsuming to get large clean labeled data, which prevents deep learning models from being trained successfully. On the other hand, it is feasible to access numerous data through internet search engines [ 3,4,5,6] or hashtags, whose labels are easy to collect but relatively inaccurate. Thus it becomes to get a spotlight to exploit data sets with corrupted labels instead of clean ones to solve classiÔ¨Åcation tasks with DNNs, which is called the noisy label problem . There have been many kinds of literature dealing with noisy labeled data, and a majority of methods exploited socalled the memorization effect , which is a special characteristic of DNNs that DNNs memorize data eventually (i.e. perfectly classify training data) but memorize clean labeled samples Preprint. Under review.arXiv:2106.15185v1  [cs.LG]  29 Jun 2021earlier and noisy samples later [ 7,8]. Hence, we can identify clean data from the given training data contaminated with noisy labels by choosing samples with small loss values. Due to its simplicity and superiority, many followup studies have been proposed based on the smallloss strategy and achieved great success ([9] and references therein). Figure 1: An illustration of the INN method. Circle and square are inputs with clean label and noisy label, respectively. Numbered dots are the nearest inputs. Each graph presents the value of a given prediction model along the dashed line. The INN method takes an average of the areas under each of the graphs.But the smallloss strategy has several weak nesses. First, during the training phase, it is difÔ¨Åcult to know a training epoch (or iteration) where the discrepancy of loss values between clean data and noisy data is large since it heav ily depends on various factors including data set, model architecture, optimizer type and even learning schedule. Second, it becomes hard to identify cleanannotated samples from training data via the smallloss strategy when the training labels are heavily polluted. Besides, the memo rization effect may not appear when we analyze the data with imbalanced label distribution. As we can obtain imbalanced data frequently in many realworld domains, this shortcoming can be an obstacle for the smallloss strategy applied in many industry Ô¨Åelds. To tackle these issues about the memoriza tion effect, we develop a novel and powerful method called INN (Integration with the Nearest Neighbors). We start with a new and interesting observation that the output values of a trained DNN atneighbor regions of labeled and noisy samples are consistently much different regardless of training epochs. We call this phenomenon the consistency effect . Motivated by the consistency effect, the INN method takes averages of the output values of neighbor regions of a given sample and decides it as noisy if the average is small. See Figure 1 for an illustration of the INN method. In fact, the INN requires more computation than the smallloss method. Still, this additional expense deserves to pay since the INN successfully overcomes the smallloss method‚Äôs limitations. The INN works well even when the training labels are heavily contaminated or has imbalanced distribution, while the smallloss method is in trouble for the situations. The stability and superiority make the INN easily applicable to various supervised learning tasks without much effort. We can also combine the INN with an existing noisylabelproblemsolving learning method based on the smallloss strategy (e.g. DivideMix [ 10]) to construct deep networks of high accuracy. We replace the parts where the memorization effect and loss information are used with the consistency effect and the INN information. We show that these modiÔ¨Åcations enhance prediction performances much, especially when training labels have many noises or imbalanced distribution. This paper is organized as follows. In Section 2, we provide brief reviews for related studies dealing with noisy labels, and detailed descriptions of the INN are given in Section 3. Various experimental analyses including performance test and ablation study are given in Section 4 and Ô¨Ånal concluding remarks follow in Section 5. The key contributions of this work are as follows. ‚Ä¢We Ô¨Ånd a new observation called the consistency effect, that the output values of a trained DNN at neighbor regions of labeled and noisy samples are consistently much different regardless of training epochs. ‚Ä¢Built on the consistency effect, we propose a method called the INN to identify clean annotated data from a given training data. ‚Ä¢We empirically demonstrate that the INN can separate clean and noisy samples accurately and stably even under the heavy label corruption and imbalanced label distribution, and also helpful to construct superior prediction models. 22 Related works "
275,Generative Knowledge Transfer for Neural Language Models.txt,"In this paper, we propose a generative knowledge transfer technique that
trains an RNN based language model (student network) using text and output
probabilities generated from a previously trained RNN (teacher network). The
text generation can be conducted by either the teacher or the student network.
We can also improve the performance by taking the ensemble of soft labels
obtained from multiple teacher networks. This method can be used for privacy
conscious language model adaptation because no user data is directly used for
training. Especially, when the soft labels of multiple devices are aggregated
via a trusted third party, we can expect very strong privacy protection.","Neural network based language models (LMs) are used in many Ô¨Åelds such as speech recognition, chatbot, sentence completion and machine translation (Mikolov et al., 2010; Serban et al., 2015; Spithourakis et al., 2016; Mirowski & Vlachos, 2015; Cho et al., 2014). Training such LMs re quires a large amount of training data. A straightforward way of gathering a large amount of training data is to col lect user data through mobile or the internet connected de vices. However, since device users are increasingly reluc tant to leak their privacy, it becomes important to collect data while protecting the privacy. Even after training the LM once, it needs to be updated for the purpose of user adaptation or adding new expression. However, it is not desired to use the user data directly. Instead of collecting sensitive user data, the model pa rameters of a neural network adapted to a user can be used for training a new model by the knowledge transfer method (Hinton et al., 2014). However, this approach can also cause an unwanted privacy violation by an adversary 1Department of Electrical and Computer Engineering, Seoul National University, Seoul, 08826 Korea. Correspondence to: Sungho Shin <sungho.develop@gmail.com >, Wonyong Sung <wysung@snu.ac.kr >.through a machine learning model attack. If the adversary can access the machine learning model, the output of the model can be used to restore the face of the individual used in the training (Fredrikson et al., 2015). In the case of text data, similar attacks are possible because an LM can be used as a text generator for generating the sensi tive user data used for training the model (Sutskever et al., 2011; Graves, 2013). Therefore, even the model parame ters trained with sensitive data should not allow direct ac cess to the adversary. Ensemble of knowledge aggregation can increase the se curity of personal data. Ensemble methods combine the results of multiple classiÔ¨Åers to improve the performance of machine learning algorithms (Dietterich, 2000). Sev eral studies trained private classiÔ¨Åers to produce a Ô¨Ånal distributable classiÔ¨Åer in various ways, such as averaging the model parameters of teacher networks (Pathak et al., 2010), training hard labels by voting the ensemble of all the teachers (Papernot et al., 2016), or using soft labels (Hamm et al., 2016). In this process, the Ô¨Ånal classiÔ¨Åer mixes ran dom noise, such as Laplacian or Gaussian noise, to hide information about a speciÔ¨Åc person and achieve strong dif ferential privacy (Dwork et al., 2006; 2014). In this paper, we propose a method that efÔ¨Åciently trans fers personal text data information for training a recurrent neural network (RNN) based LM while minimizing privacy infringement. This method sends the soft labels generated by the teacher networks instead of sending personal data or model parameters, and the soft outputs obtained from the individual users are aggregated for training a student net work by a reliable third party. The proposed GKT trains the student network using only the generated data and la bels without the original training data, by operating RNN LMs as a generative model. The text generation can be conducted by either the teacher or the student network. This paper is composed as follows. Section 2 introduces the related work and Section 3 describes the proposed GKT. Section 4 describes the GKT by ensemble of multiple LMs, Section 5 shows the experimental results, and Section 6 concludes this paper.arXiv:1608.04077v3  [cs.LG]  28 Feb 2017Generative Knowledge Transfer for Neural Language Models (a) Teacherdriven generative knowledge transfer (TDGKT) (b) Studentdriven generative knowledge transfer (SDGKT) Figure 1. Two different schemes of GKT. Text sequence genera tion (green lines) can be produced by the teacher (TDGKT) or the student network (SDGKT). 2. Related Work on Knowledge Transfer "
445,Signal Combination for Language Identification.txt,"Google's multilingual speech recognition system combines low-level acoustic
signals with language-specific recognizer signals to better predict the
language of an utterance. This paper presents our experience with different
signal combination methods to improve overall language identification accuracy.
We compare the performance of a lattice-based ensemble model and a deep neural
network model to combine signals from recognizers with that of a baseline that
only uses low-level acoustic signals. Experimental results show that the deep
neural network model outperforms the lattice-based ensemble model, and it
reduced the error rate from 5.5% in the baseline to 4.3%, which is a 21.8%
relative reduction.","Multilingual speech recognition is an important feature for modern speech recognition systems allowing users to speak in more than a single, preset language. In Google multilin gual speech recognition service [1] users are allowed to se lect two, or more, languages simultaneously as prior infor mation (Fig. 1). When the microphone is enabled, the sys tem works by running several speech recognziers in paral lel, along with an acoustic language identiÔ¨Åcation (LangID) module [2]. After the system decides the language of the ut terance, the recognition result of the corresponding language will be used and the language decision can be propagated to downstream systems (e.g. a text to speech module). In our previous work [1], the Ô¨Ånal language identiÔ¨Åcation de cision is predominantly taken by the acoustic LangID mod ule, which generates a probability score for each of the lan guage based on the audio. Such approach however, ignores other potentially useful information returned by the individ ual speech recognizers such as the accumulated language or acoustic model score. The author did this work during his internship at Google. Email: shengye@ucsd.edu ‚Ä†Email: fliwan, yyuyy, elnota g@google.com Utterance AudioAcousticLangID Model2+ Speech RecognizersHighlevel LangIDSignal Combinationlangalangblangc‚Ä¶lang ‚ùå lang ‚úÖ lang ‚ùå ‚Ä¶LanguageDecisionLanguageCandidatesFig. 1 .The ‚ÄúLangID‚Äù pipeline. The system takes an audio clip of an utterance and a list of candidate languages, and it predicts the language of the utterance. In this work, we explore a few alternative approaches to improve the language identiÔ¨Åcation accuracy by using signals that can be easily computed by most speech recognition sys tems, including the conÔ¨Ådence, acoustic and language model scores computed by the recognizers (Table 1). Without signal combination, an acoustic LangID model provides a baseline with 5 :5% error rate. Using a lattice based ensemble model [3], we were able to reduce the clas siÔ¨Åcation error rate to 4 :5%, a relative 23 :7% reduction. We continued exploring new methods and found a deep neural network model outperforms the latticebased model: the er ror rate further reduced to 4 :3%, which is a 21 :8% relative reduction from the original baseline. This paper is structured as follows. Section 2 discusses related work in LangID and signal combination. Section 3 presents our method with a latticebased ensemble model and a deep neural networkbased improvement. We also explore methods in deep learning that work well in the signal com bination problem. Section 4 shows the experimental results. Finally, Section 5 concludes the paper. 2. RELATED WORK "
150,Robustness of convolutional neural networks to physiological ECG noise.txt,"The electrocardiogram (ECG) is one of the most widespread diagnostic tools in
healthcare and supports the diagnosis of cardiovascular disorders. Deep
learning methods are a successful and popular technique to detect indications
of disorders from an ECG signal. However, there are open questions around the
robustness of these methods to various factors, including physiological ECG
noise. In this study we generate clean and noisy versions of an ECG dataset
before applying Symmetric Projection Attractor Reconstruction (SPAR) and
scalogram image transformations. A pretrained convolutional neural network is
trained using transfer learning to classify these image transforms. For the
clean ECG dataset, F1 scores for SPAR attractor and scalogram transforms were
0.70 and 0.79, respectively, and the scores decreased by less than 0.05 for the
noisy ECG datasets. Notably, when the network trained on clean data was used to
classify the noisy datasets, performance decreases of up to 0.18 in F1 scores
were seen. However, when the network trained on the noisy data was used to
classify the clean dataset, the performance decrease was less than 0.05. We
conclude that physiological ECG noise impacts classification using deep
learning methods and careful consideration should be given to the inclusion of
noisy ECG signals in the training data when developing supervised networks for
ECG classification.","(a) Deep learning and physiological ECG signal noise Electrocardiogram (ECG) signals have long been used to support the diagnosis of cardiovascular disorders. Deep learning methods show encouraging results in ECG classiÔ¨Åcation tasks and have recently seen a rapid increase in popularity [1]. Noise and interference on the ECG signal are established causes of error in ECG diagnosis and interpretation [2] and have been noted to affect both manual (clinician) and automated (machine learning) detection of ECG abnormalities [3]. A desirable property of a deep network is that the performance of the network is robust to perturbations in the input data. Network robustness to ECG noise of deep learning methods used to detect cardiovascular disorders is not well understood and there have been no studies directly addressing the issue. Sources of noise that degrade the quality of a dataset include both label noise (in terms of mislabelled data) and ECG signal noise (in terms of physiological noise on the signal). Here we focus on the impact of ECG signal noise, to gain an understanding of how physiological ECG noise impacts the robustness of deep learning methods. (b) Transfer learning with deep networks While custom network architectures can be developed and trained from scratch to classify ECG signals [1], transfer learning is a popular method for utilising pretrained deep networks with new data. Transfer learning refers to the retraining of a pretrained network, for example a network pretrained using ImageNet [4] data can be retrained using ECG data to classify ECG data. This training method is useful when there is a lack of data, computational resources or time, or to prototype models and carry out exploratory analysis. ECG datasets often contain fewer than the large number of samples required to train a deep network from scratch, and in this case transfer learning is an attractive option. Furthermore, many well known network architectures have a demonstrated record of high performance. The focus of this study is to evaluate the robustness of deep learning to physiological ECG noise, rather than to optimise ECG classiÔ¨Åcation performance of a custom architecture. Using an established pretrained network provides a solid foundation for this focus. Convolutional neural networks (CNNs) are a class of deep network that is widely used for image classiÔ¨Åcation and, alongside recurrent neural networks (RNNs), are commonly used for ECG classiÔ¨Åcation [1]. Both 1D CNNs applied to the raw ECG signal and 2D CNNs applied to ECG image transforms have been used to detect cardiovascular disorders from the ECG signal. (c) Detecting cardiovascular disorders from the ECG signal Extensive work has been carried out to develop methods, including deep networks, that extract information from an ECG signal to support clinical decision making [1,5]. ECG image transformations are methods that convert a 1D ECG signal to a 2D image which can then be passed to a 2D CNN for training and classiÔ¨Åcation. The use of ECG image transforms allows both the use of 2D CNNs pretrained on the popular image dataset ImageNet [6], and the exploration of the impact of these image transforms on network robustness to noise. ECG image transforms capture frequency domain or morphology information that describe the underlying signal. Existing ECG image transforms and their applications include: the continuous wavelet transform (scalogram) for biometrics [7], gray level cooccurence matrix for morphological arrhythmia detection [8], recurrence plot to classify arrhythmias [9], distance distribution matrix to identify congestive heart failure [10] and the Symmetric Projection Attractor Reconstruction (SPAR) method for genetic mutation detection [11].3rsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Networks trained to classify ECG image transforms are less common than networks trained to classify the ECG signal directly and their utility for pathology classiÔ¨Åcation is still being explored. In particular, the impact of using ECG image transforms on network robustness is unclear. (d) Objectives The main objectives of this study are to: (i) Study the impact of the inclusion of physiological ECG noise in the input data on classiÔ¨Åcation performance of a CNN (the robustness); (ii) Assess the impact of the inclusion of physiological noise in the training data on the robustness of a CNN; (iii) Determine whether different ECG image transforms increase or decrease robustness to different noise types. 2. Methods "
449,Towards a Robust Differentiable Architecture Search under Label Noise.txt,"Neural Architecture Search (NAS) is the game changer in designing robust
neural architectures. Architectures designed by NAS outperform or compete with
the best manual network designs in terms of accuracy, size, memory footprint
and FLOPs. That said, previous studies focus on developing NAS algorithms for
clean high quality data, a restrictive and somewhat unrealistic assumption. In
this paper, focusing on the differentiable NAS algorithms, we show that vanilla
NAS algorithms suffer from a performance loss if class labels are noisy. To
combat this issue, we make use of the principle of information bottleneck as a
regularizer. This leads us to develop a noise injecting operation that is
included during the learning process, preventing the network from learning from
noisy samples. Our empirical evaluations show that the noise injecting
operation does not degrade the performance of the NAS algorithm if the data is
indeed clean. In contrast, if the data is noisy, the architecture learned by
our algorithm comfortably outperforms algorithms specifically equipped with
sophisticated mechanisms to learn in the presence of label noise. In contrast
to many algorithms designed to work in the presence of noisy labels, prior
knowledge about the properties of the noise and its characteristics are not
required for our algorithm.","To avoid exhausting engineering, Neural Architecture Search (NAS) has emerged as a leading mechanism for automatic design and wiring of neural networks. NAS has been successfully moving forward with diverse ap proaches to achieve a robust automatic architecture search e.g., evolutionbased NAS [11, 30, 31, 37], optimization based NAS [10, 23, 24, 34, 43], and Reinforcement Learning (RL) based NAS [29, 62, 63]. Specifically, a gradientbased method with a continuous architecture space called Differ entiable ARchiTecture Search (DARTS) has attracted sig nificant attention in NAS because of a reduced cost and complexity of searching for high performance architectures.In this paper, we go beyond merely learning with NAS under regular assumptions of clean labels. Supervised learning with neural networks often leads to a performance degradation due to overfitting, especially in the presence of label noise which often emerges due to data corruption and/or human annotation errors especially promi nent in large scale datasets. As a result, neural networks fail to generalize well to previously unseen data and achieve sub optimal classification results. Given the importance of such problems, existing stateoftheart methods are specifically designed to deal with the data noise by correcting labels [42], employing dedicated loss functions [5, 26, 46, 58], reweight ing samples [17, 33], selecting samples [15], and modeling a transition matrix [28, 40, 48]. However, designing robust neural networks that mitigate overfitting due to label noise is still unexplored. To this end, we propose a structural ap proach, that is a method that requires no explicit changes to neither the loss functions nor the input samples nor the final outputs (predictions) but the neural network structure. More over, our approach does not require specific assumptions on the amount or the type of label noise. Our primary focus1is to investigate and advance a robust method to search an architecture in the presence of label noise. To this end, we aim to answer the following questions: ‚Ä¢Motivational Curiosity. As handcrafted neural net works ( e.g., Inception [38]) overfit to noisy labels [55] (even the pretext labeling in selfsupervised UnNAS is imperfect), is the test performance of neural networks constructed by NAS also degraded by the label noise? ‚Ä¢Research Curiosity. Can we design an operator2that is robust to noisy labels and helps existing NAS methods (e.g., DARTS [23]) to perform well in the presence of label noise? The answer to the first question is affirmative as shown in Fig. 1. Firstly, we highlight that the performance of vanilla 1Note that training robust NAS under the label noise is not the same problem as using static network for classification under the label noise. 2An operator is an operation connecting two nodes in NAS e.g., the standard NAS uses conv., max or averagepooling, skip connections, etc. 10% 50% Noise Rate5060708090Accuracy (%)DARTS + our op. Vanilla DARTS94.895.1 85.7 69.0(a) 265457 232852 Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.CountDARTS + our op. Vanilla DARTS Vanilla DARTS 50%Sym. Noise 50%Sym. Noise Clean Labels 60 40 20 060 40 20 060 40 20 0 (b) 265457 232852 Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.CountDARTS + our op. Vanilla DARTS Vanilla DARTS 50%Sym. Noise 50%Sym. Noise Clean Labels 60 40 20 060 40 20 060 40 20 0 (c) 265457 232852 Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.CountDARTS + our op. Vanilla DARTS Vanilla DARTS 50%Sym. Noise 50%Sym. Noise Clean Labels 60 40 20 060 40 20 060 40 20 0 (d) Figure 1: (a) Testing accuracy of vanilla vs.our approach (CIFAR10, clean labels vs.50%symmetric label noise). The histogram of found operators (five runs) of (b) the normal cell of vanilla DARTS under 50%symmetric label noise, (c) the normal cell of DARTS with nConv (our noise injecting operator) searched on CIFAR10 (50%symmetric noisy labels), and (d) the normal cell of vanilla DARTS with clean labels. The normal cell of vanilla DARTS under label noise is constructed poorly due to the large number of parameterless operations. It is apparent the network thus looses the learning capacity in an attempt to prevent overfitting to the noise by selecting parameterless operators. In contrast, DARTS with our proposed operator mitigates such a poor cell design as highlighted by the larger number of convolutional operators being selected in place of a parameterless operator. DARTS [23] suffers when subjected to noisy labels as shown in Fig. 1 (a). Furthermore, the architecture searched by vanilla DARTS under label noise results in a bad architec ture as shown in Fig. 1 (b) while our approach shown in Fig. 1 (c) produces a better designed cell. To answer the second question, we analyze the task of learning deep neural networks under noisy labels using Information Bottleneck. As a result of this analysis, we introduce a variant of the convolutional operation by injecting noise into the pipeline. Upon learning the parameters of the noise from data, we will empirically show that robustness against label corruption can be attained. In essence, we will show later that the noisy convolution regularizes and limits the gradient of the noisy samples during training. In short, our key contributions are: i.We show that the search through noisy labels degrades the classification performance of standard NAS. ii.We provide an information theoretic framework to tackle learning noisy labels. Our proposed noise injection op erator performs implicit regularization during learning preventing overfitting to noisy labels. iii.The proposed operator is included into the NAS search space with the goal of preventing overfitting during train ing with noisy labels. A noise injection on the input of the operator turns activations of hidden units into the socalled stochastic activations. iv.We show experimentally that architectures emerging from the NAS search with our proposed noise injecting operator outperform under fewer parameters the state of the art, especially in the case of no prior knowledge given e.g., the lack of the noise type/its rates.2. Related work "
204,Tracking by Prediction: A Deep Generative Model for Mutli-Person localisation and Tracking.txt,"Current multi-person localisation and tracking systems have an over reliance
on the use of appearance models for target re-identification and almost no
approaches employ a complete deep learning solution for both objectives. We
present a novel, complete deep learning framework for multi-person localisation
and tracking. In this context we first introduce a light weight sequential
Generative Adversarial Network architecture for person localisation, which
overcomes issues related to occlusions and noisy detections, typically found in
a multi person environment. In the proposed tracking framework we build upon
recent advances in pedestrian trajectory prediction approaches and propose a
novel data association scheme based on predicted trajectories. This removes the
need for computationally expensive person re-identification systems based on
appearance features and generates human like trajectories with minimal
fragmentation. The proposed method is evaluated on multiple public benchmarks
including both static and dynamic cameras and is capable of generating
outstanding performance, especially among other recently proposed deep neural
network based approaches.","Multiperson localisation and tracking is one of the most active research areas in computer vision as it en ables a variety of applications including sports analysis [18,39,65], robot navigation [10,11] and autonomous driv ing [12, 15, 48]. Despite the impact of deep learning across a multitude of computer vision domains in recent years, within the track ing space it has been applied in a somewhat piecemeal man ner, with it often used for only a speciÔ¨Åc part of the tracking pipeline. For example, techniques such as [34, 58, 62] use DCNNs to model subject appearance within a probabilis tic tracker. We note that to date, complete deep learning solutions for both localisation and tracking have been limited [41]. We believe this is due to the scarcity of training data which is large enough to train a complete deep neu ral network based platform, as well as the complex, vari able length nature of multi person trajectories. We combine this with a deep tracking framework that utilises Long Short Term Memory networks (LSTMs) to capture pedestrian dy namics in the scene and track objects via predicting it‚Äôs fu ture trajectory. In this paper we contribute a novel light weight person detection framework based on Generative Adversarial Net works (GANs) [23], which can be easily trained on the lim ited data available for multi person localisation. We extend the general GAN framework to temporal sequences and ren der a probability map for pedestrians in the given sequence. The temporal structure of the proposed GAN allows us to identify pedestrians more effectively, regardless of the mo tion of other foreground objects in the scene. As illustrated in [42], multi person tracking consists of two subproblems: data association (i.e assigning a unique identiÔ¨Åer to the corresponding targets) and inferring the tra jectories of the targets. In most data association paradigms the researchers utilise an appearance based model [3, 9, 53, 60, 61] to reidentify the targets in the next frame. Yet in crowded environments with a high likelihood of occlu sions, noisy detections, and poor image resolution, appear ance models often fail to generate correct identiÔ¨Åcation of the targets. This results in an unrealistic trajectory gener ation from the tracking process. To counter this problem we propose a novel tracking framework where the object detections in the next frame are associated with targets via considering their predicted short term and long term trajec tories. The trajectory prediction method accounts for the motion of the pedestrian as well as the motion of other peo ple in the local neighbourhood which allows the modelling of the interactions among them. This enables intelligence in the data association process generating human like tra jectories even in the presence of occlusions and other image artefacts. To achieve this, we build upon recent advances [16, 41] in pedestrian trajectory modelling approaches andarXiv:1803.03347v1  [cs.CV]  9 Mar 2018propose a method to capture relationships with neighbour hood dynamics as well as the long term dependencies within the scene context. The major contributions of the proposed work can be summarised as follows: We introduce a novel pedestrian detection platform based on Generative Adversarial Networks (GANs). We develop a robust light weight algorithm for data association in multi person tracking problems with the aid of trajectory prediction. We generate human like trajectory estimates via the as sociation of neighbourhood and scene context in the trajectory prediction framework. We comprehensively evaluate the proposed models on publicly available benchmarks including videos from both static and dynamic cameras. We achieve outstanding performance in the MOT chal lenge benchmark datasets, especially among deep neu ral network based approaches. 2. Related work "
492,Attention-Aware Noisy Label Learning for Image Classification.txt,"Deep convolutional neural networks (CNNs) learned on large-scale labeled
samples have achieved remarkable progress in computer vision, such as
image/video classification. The cheapest way to obtain a large body of labeled
visual data is to crawl from websites with user-supplied labels, such as
Flickr. However, these samples often tend to contain incorrect labels (i.e.
noisy labels), which will significantly degrade the network performance. In
this paper, the attention-aware noisy label learning approach ($A^2NL$) is
proposed to improve the discriminative capability of the network trained on
datasets with potential label noise. Specifically, a Noise-Attention model,
which contains multiple noise-specific units, is designed to better capture
noisy information. Each unit is expected to learn a specific noisy distribution
for a subset of images so that different disturbances are more precisely
modeled. Furthermore, a recursive learning process is introduced to strengthen
the learning ability of the attention network by taking advantage of the
learned high-level knowledge. To fully evaluate the proposed method, we conduct
experiments from two aspects: manually flipped label noise on large-scale image
classification datasets, including CIFAR-10, SVHN; and real-world label noise
on an online crawled clothing dataset with multiple attributes. The superior
results over state-of-the-art methods validate the effectiveness of our
proposed approach.","CNNs have triumphed over many vision tasks. However, the overwhelming performances of CNNs heavily rely on largescale highquality labeled data, e.g., ImageNet [19], which are typically laborious and costly to collect and an Examples of noisy labels   airplane    ant   car   dog   watch    brain   elephant  scissor  apple  watch  panda  snail  Figure 1. Examples of noisy labels. The image annotations that are manually labeled by amateurs or automatically generated by a machine are not reliable. Worse, the noisy labels and the number of mislabeled images are not speciÔ¨Åed. notate. Nevertheless, there are millions of freely available images with usersupplied labels that can be easily accessed from the web. Although utilizing web images has become a popular research direction in the Ô¨Åeld of large scale im age recognition, the performance is obviously inferior to its counterpart on Ô¨Ånely labeled data. Directly using image sets with a high proportion of noisy labels (e.g., Fig. 1) can even degrade the performance of Ô¨Ånelytrained CNN mod els [6, 24, 26]. Thus, it is highly desired to design a network that is able to mitigate the impact of noisy labels. There are already several label noiserobust algorithms being developed in recent years. Some researchers propose robust loss functions speciÔ¨Åcally for noisy image classiÔ¨Åca tion [16, 21], others try to improve the quality of training data by predicting the label noise type or removing the mis labeled samples [24, 22]. However, these methods either work worse under large proportions of label noise or re quire prior knowledge on the patterns of label noise. There are also CNNbased methods explicitly modeling the noisy distributions by a noise layer [20, 12, 5]. However, these methods are usually based on the assumption that the dis turbance of all samples in the same class is equal, thus they are unable to acquire diverse noisy information, e.g., some 1arXiv:2009.14757v1  [cs.CV]  30 Sep 2020furry dogs are easy to be labeled as cats and some large ones as horses, which are very common for online images with usersupplied labels. In this work, we propose an attentionaware noisy label learning approach, termed A2NL, to improve the discrim inative capability of noiserobust network. In particular, a NoiseAttention (NA) model is proposed to explore multi ple distributions of label noise and a recursive learning strat egy is employed to further boost its learning ability. Both contributions can be applied to any conventional CNNs. To avoid confusion, the network with the NA model is infor mally called attention network in the following. Unlike pre vious works which describe the noisylabel information of an image set by one distribution, our NA model contains multiple units, each of which pays attention to a speciÔ¨Åc noisylabel distribution. The reasons for such an improve ment are two folds. 1) The noisy levels of different classes vary a lot, e.g., in the task of predicting clothes color, it is likely to mix the orange and brown while it is almost impossible to label a red one as a blue one. 2) The indi vidual images of the same class can even present different disturbance. For example, some dogs could be clearly rec ognized, some may be labeled as cats or horses. By mod eling multiple noisylabel distributions, the proposed NA model can not only portray the different noisy levels among classes, but also distinguish the diverse noisylabel distribu tions among images. The proposed recursive learning strategy is inspired by [25, 7, 3] that the soft predictions of a welltrained classiÔ¨Åer usually contain rich information. The soft outputs not only indicate the object class of the input image, but also reÔ¨Çect the relations among classes. For example, if a cloth sample is predicted as an orange one with the conÔ¨Ådence of 80%, a brown one with 15%, and 5% for other classes. The biggest Ô¨Ågure (80%) advocates that the input image contains a cloth in orange, and other Ô¨Ågures suggest that the orange is highly possible to be mixed up with the brown. Thus, to boost the learning ability of the proposed attention network, we re cursively train it by distilling the knowledge from a well trained attention network in the previous iteration. To be speciÔ¨Åc, the outputs of the attention network in the previ ous iteration, coupling with the given training labels, are combined as the training supervisions for the network in the current iteration. Different from directly combining multi ple network models, the recursive learning strategy is able to assemble the network knowledge in previous iterations without introducing more parameters. We conduct extensive experiments on both datasets with synthesized noisy labels (randomly Ô¨Çipping the labels) in cluding SVHN [17], CIFAR10 [13], and a realworld clothes dataset with multiple attributes [8] which naturally contains mislabeled samples. As considering both general and speciÔ¨Åc label noise simultaneously, the proposed framework shows excellent effectiveness and robustness to both synthesized and realworld label noise. Our main contribu tions are summarized as follows: A NoiseAttention model with multiple noisespeciÔ¨Åc units is proposed to explore various distributions of la bel noise, which can be applied to conventional CNNs. A recursive learning strategy, which could assemble the highlevel knowledge learned from multiple net works, is introduced to boost the attention network learning ability. Extensive experiments on both manually Ô¨Çipped label noise and realworld label noise demonstrate the ex cellent effectiveness and robustness of our attention aware noisy label learning framework. 2. Related Works "
240,Classifying and Segmenting Microscopy Images Using Convolutional Multiple Instance Learning.txt,"Convolutional neural networks (CNN) have achieved state of the art
performance on both classification and segmentation tasks. Applying CNNs to
microscopy images is challenging due to the lack of datasets labeled at the
single cell level. We extend the application of CNNs to microscopy image
classification and segmentation using multiple instance learning (MIL). We
present the adaptive Noisy-AND MIL pooling function, a new MIL operator that is
robust to outliers. Combining CNNs with MIL enables training CNNs using full
resolution microscopy images with global labels. We base our approach on the
similarity between the aggregation function used in MIL and pooling layers used
in CNNs. We show that training MIL CNNs end-to-end outperforms several previous
methods on both mammalian and yeast microscopy images without requiring any
segmentation steps.","High content screening (HCS) technologies that combine automated Ô¨Çuorescence microscopy with high throughput biotechnology have become powerful systems for studying cell biology and for drug screening [1]. These systems can produce more than 105images per day, making their success dependent on automated image analysis. Previous analysis pipelines heavily rely on handtuning the segmentation, feature extraction, and classiÔ¨Åcation steps for each assay. Although comprehen sive tools have become available [2] they are often optimized for mammalian cells and not directly applicable to model organisms such as yeast and C. elegans . Researchers studying these organisms often manually classify cellular patterns by eye [3]. Recent advances in deep learning have proven that deep neural networks trained endtoend can learn powerful feature representations and outperform classiÔ¨Åers built on top of extracted features [4, 5]. While object recognition models have been successfully trained using images with one or a few objects of interest at the center of the image, microscopy images often contain hundreds of cells from the label class, as well as a few outliers. Training similar recognition models on HCS screens is therefore challenging due to the lack of datasets labeled at the single cell level. In this work we describe a convolutional neural network (CNN) that is trained on full resolution microscopy images using multiple instance learning (MIL). The network is designed to produce feature maps for every output category, as proposed for segmentation tasks in [6]. We pose cellular phenotype classiÔ¨Åcation as a special case of MIL, where each element in a classspeciÔ¨Åc feature map is considered an instance and each full resolution microscopy image is considered a bag with a label. Typically binary MIL problems assume that a bag is positive if at least one instance within the bag is positive. This assumption does not hold for HCS images due to heterogeneities within cellular populations and imaging artifacts [7]. We explore the performance of several global pooling operators on this problem and propose a new operator capable of learning the proportion of instances necessary to activate a label. 1arXiv:1511.05286v1  [cs.CV]  17 Nov 2015The main contributions of our work are the following. We present a uniÔ¨Åed view of the classical MIL approaches as pooling layers in CNNs and compare their performances. We propose a novel MIL method, ‚Äúadaptive NoisyAND‚Äù, that is robust to outliers and large numbers of instances. We evaluate our proposed model on both mammalian and yeast datasets, and Ô¨Ånd that our model signiÔ¨Åcantly outperforms previously published results at phenotype classiÔ¨Åcation. Our model is capable of learning a good classiÔ¨Åer for full resolution microscopy images as well as individual cropped cell instances, even though it is only trained using whole image labels. We also demonstrate that the model can localize regions with cells in the full resolution microscopy images and that the model predictions are based on activations from these regions. 2 Related Work "
17,Are Labels Necessary for Neural Architecture Search?.txt,"Existing neural network architectures in computer vision -- whether designed
by humans or by machines -- were typically found using both images and their
associated labels. In this paper, we ask the question: can we find high-quality
neural architectures using only images, but no human-annotated labels? To
answer this question, we first define a new setup called Unsupervised Neural
Architecture Search (UnNAS). We then conduct two sets of experiments. In
sample-based experiments, we train a large number (500) of diverse
architectures with either supervised or unsupervised objectives, and find that
the architecture rankings produced with and without labels are highly
correlated. In search-based experiments, we run a well-established NAS
algorithm (DARTS) using various unsupervised objectives, and report that the
architectures searched without labels can be competitive to their counterparts
searched with labels. Together, these results reveal the potentially surprising
finding that labels are not necessary, and the image statistics alone may be
sufficient to identify good neural architectures.","Neural architecture search (NAS) has emerged as a research problem of searching for architectures that perform well on target data and tasks. A key mystery sur rounding NAS is what factors contribute to the success of the search. Intuitively, using the target data and tasks during the search will result in the least domain gap, and this is indeed the strategy adopted in early NAS attempts [36,27]. Later, researchers [37] started to utilize the transferability of architectures, which en abled the search to be performed on dierent data and labels ( e.g., CIFAR10) than the target ( e.g., ImageNet). However, what has not changed is that both the images and the (semantic) labels provided in the dataset need to be used in order to search for an architecture. In other words, existing NAS approaches perform search in the supervised learning regime. In this paper, we take a step towards understanding what role supervision plays in the success of NAS. We ask the question: How indispensable are labels in 3Code release: https://github.com/facebookresearch/unnasarXiv:2003.12056v2  [cs.CV]  3 Aug 20202 C. Liu et al. neural architecture search? Is it possible to nd highquality architectures using images only? This corresponds to the important yet underexplored unsupervised setup of neural architecture search, which we formalize in Section 3. With the absence of labels, the quality of the architecture needs to be esti mated in an unsupervised fashion during the search phase. In the present work, we conduct two sets of experiments using three unsupervised training methods [12,35,22] from the recent selfsupervised learning literature.4These two sets of experiments approach the question from complementary perspectives. In sample based experiments , we randomly sample 500 architectures from a search space, train and evaluate them using supervised vs. selfsupervised objectives, and then examine the rank correlation (when sorting models by accuracy) between the two training methodologies. In searchbased experiments , we take a wellestablished NAS algorithm, replace the supervised search objective with a selfsupervised one, and examine the quality of the searched architecture on tasks such as Ima geNet classication and Cityscapes semantic segmentation. Our ndings include: {The architecture rankings produced by supervised and selfsupervised pre text tasks are highly correlated . This nding is consistent across two datasets, two search spaces, and three pretext tasks. {The architectures searched without human annotations are comparable in performance to their supervised counterparts. This result is consistent across three pretext tasks, three pretext datasets, and two target tasks. There are even cases where unsupervised search outperforms supervised search. {Existing NAS approaches typically use labeled images from a smaller dataset to learn transferable architectures. We present evidence that using unlabeled images from a large dataset may be a more promising approach. We conclude that labels are not necessary for neural architecture search, and the deciding factor for architecture quality may hide within the image pixels. 2 Related Work "
308,Alternating Loss Correction for Preterm-Birth Prediction from EHR Data with Noisy Labels.txt,"In this paper we are interested in the prediction of preterm birth based on
diagnosis codes from longitudinal EHR. We formulate the prediction problem as a
supervised classification with noisy labels. Our base classifier is a Recurrent
Neural Network with an attention mechanism. We assume the availability of a
data subset with both noisy and clean labels. For the cohort definition, most
of the diagnosis codes on mothers' records related to pregnancy are ambiguous
for the definition of full-term and preterm classes. On the other hand,
diagnosis codes on babies' records provide fine-grained information on
prematurity. Due to data de-identification, the links between mothers and
babies are not available. We developed a heuristic based on admission and
discharge times to match babies to their mothers and hence enrich mothers'
records with additional information on delivery status. The obtained additional
dataset from the matching heuristic has noisy labels and was used to leverage
the training of the deep learning model. We propose an Alternating Loss
Correction (ALC) method to train deep models with both clean and noisy labels.
First, the label corruption matrix is estimated using the data subset with both
noisy and clean labels. Then it is used in the model as a dense output layer to
correct for the label noise. The network is alternately trained on epochs with
the clean dataset with a simple cross-entropy loss and on next epoch with the
noisy dataset and a loss corrected with the estimated corruption matrix. The
experiments for the prediction of preterm birth at 90 days before delivery
showed an improvement in performance compared with baseline and state
of-the-art methods.","The digitization of hospitals, by the adoption of Electroni c Health Record systems, promises to revo lutionize the future of healthcare. Several countries have achieved nearly 100% adoption. Therefore the complexity and size of EHR data is drastically increasin g. This is creating new challenges and opportunities to the research community of machine learnin g for healthcare. In this paper we con sider the clinical application of predicting preterm birth from EHR based on deep learning models. Between 10% to 15% of babies are born before 37 weeks of gestat ion [1]. Preterm birth is the leading cause of mortality and longterm disabilities in ne onates. It is also an important cause of developmental retardation. The cost of preterm deliveries and care exceed 26 billion dollars in the Machine Learning for Health (ML4H) Workshop at NeurIPS 2018 .US [2]. The goal of our application is to predict in advance th e risk for a preterm delivery [3, 4]. Developing such predictive model can be of high value for obs tetricians. The availability of large clinical EHR data should help in building accurate and inter pretable models. 2 Related Work "
254,Learning Debiased Classifier with Biased Committee.txt,"Neural networks are prone to be biased towards spurious correlations between
classes and latent attributes exhibited in a major portion of training data,
which ruins their generalization capability. We propose a new method for
training debiased classifiers with no spurious attribute label. The key idea is
to employ a committee of classifiers as an auxiliary module that identifies
bias-conflicting data, i.e., data without spurious correlation, and assigns
large weights to them when training the main classifier. The committee is
learned as a bootstrapped ensemble so that a majority of its classifiers are
biased as well as being diverse, and intentionally fail to predict classes of
bias-conflicting data accordingly. The consensus within the committee on
prediction difficulty thus provides a reliable cue for identifying and
weighting bias-conflicting data. Moreover, the committee is also trained with
knowledge transferred from the main classifier so that it gradually becomes
debiased along with the main classifier and emphasizes more difficult data as
training progresses. On five real-world datasets, our method outperforms prior
arts using no spurious attribute label like ours and even surpasses those
relying on bias labels occasionally.","Most supervised learning algorithms for classiÔ¨Åcation rely on the empirical risk minimization (ERM) principle [ 41]. However, ERM has been known to cause a learned classiÔ¨Åer to be biased toward spurious correlations between predeÔ¨Åned classes and latent attributes that appear in a majority of training data [ 12]. In the case of hair color classiÔ¨Åcation, for example, when most people with blondhair (i.e., target class) are female (i.e., latent attribute) in a dataset, a classiÔ¨Åer learned by ERM exploits female as a shortcut for the classiÔ¨Åcation due to its spurious correlation with blondhair , and often misclassiÔ¨Åes nonblondehaired women as blondhair in consequence. We call data with such spurious correlations and holding a majority of training data biasguiding samples , and the other biasconÔ¨Çicting samples , respectively. The issue of model bias has often been addressed by exploiting explicit spurious attribute labels [ 22,29,37,2,40,39,46] or knowledge about bias types given a priori [ 3]. However, these methods are impractical because such supervision and prior knowledge are costly, and the methods demand extensive post hoc analysis. Hence, a body of research has been conducted for learning debiased classiÔ¨Åers with no additional label for spurious attributes [ 43,28,30,33,23,27]. A common approach in this line of work is to employ an intentionally biased classiÔ¨Åer as an auxiliary module [ 30,33,23,27]. In this approach, samples that the biased classiÔ¨Åer has trouble handling are regarded as biasconÔ¨Çicting ones and assigned large weights when used for training the main classiÔ¨Åer to reduce the effect of biasguiding counterparts. Although it has driven remarkable success, this approach has drawbacks due to the use of a single biased classiÔ¨Åer. First, the quality of the biased classiÔ¨Åer could vary by hyperparameters [ 30] and its initial parameter values [ 11]. Further, data that the biased classiÔ¨Åer fails to handle could include not only biasconÔ¨Çicting samples but also biasguiding ones, which differs by the quality of the classiÔ¨Åer. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2206.10843v5  [cs.LG]  1 May 2023(a)  (b)  (c) Figure 1: Analysis on the instability of a single biased classiÔ¨Åer in mining and weighting bias conÔ¨Çicting samples. The experiments are conducted on the CelebA dataset, in which samples with blond andmale attributes are biasconÔ¨Çicting. (a) The ratio of biasconÔ¨Çicting samples to all incorrectly predicted by a single biased classiÔ¨Åer. The ratio highly Ô¨Çuctuates by the learning rate of the classiÔ¨Åer and varies up to 4%p due to different initialization even with a Ô¨Åxed learning rate, meaning that a single biased classiÔ¨Åer is sensitive to hyperparameters. (b) Disagreement on predictions of biased classiÔ¨Åers. For pairs of biased classiÔ¨Åers initialized differently, we measure the number of biasconÔ¨Çicting samples for which the classiÔ¨Åers predict differently. The results suggest that individual biased classiÔ¨Åers are sensitive to initialization. (c) Comparisons between a single biased classiÔ¨Åer and a committee of biased classiÔ¨Åers in terms of enrichment [ 30]. Higher enrichment implies more precise mining and weighting of biasconÔ¨Çicting samples; the formal deÔ¨Ånition of enrichment is given in Appendix A.2. The committee clearly outperforms the single biased classiÔ¨Åer in terms of the enrichment. These drawbacks limit the reliability and performance of debiasing methods depending on a single biased classiÔ¨Åer, as demonstrated in Figure 1. To overcome these limitations, we propose a new method using a committee of biased classiÔ¨Åers as the auxiliary module, coined learning with biased committee (LWBC). LWBC identiÔ¨Åes biasconÔ¨Çicting samples and determines their weights through consensus on their prediction difÔ¨Åculty within the committee. To this end, the committee is built as a bootstrapped ensemble, i.e., each of its classiÔ¨Åers is trained from a randomly sampled subset of the entire training dataset. This strategy not only guarantees the diversity among the classiÔ¨Åers, but also lets a majority of the classiÔ¨Åers be biased since random subsets of training data are highly likely to be dominated by biasguiding samples. Accordingly, a majority of the committee tends to classify biasguiding samples correctly and fail to deal with biasconÔ¨Çicting ones. The consensus on prediction difÔ¨Åculty within the committee thus gives a strong cue for identifying and weighting biasconÔ¨Çicting samples. Also, using the consensus of multiple classiÔ¨Åers enables LWBC to be robust to the varying quality of individual classiÔ¨Åers and consequently to focus more precisely on biasconÔ¨Çicting samples, as shown in Figure 1. Moreover, unlike the biased classiÔ¨Åer trained independently of the main classiÔ¨Åer in the previous work, the committee in LWBC is trained with knowledge of the main classiÔ¨Åer as well as the random subsets of training data to serve the main classiÔ¨Åer better. SpeciÔ¨Åcally, the knowledge is distilled in the form of classiÔ¨Åcation logits of the main classiÔ¨Åer [ 18], and each classiÔ¨Åer of the committee utilizes the knowledge as pseudo labels of training data other than its own training set. We expect that this strategy allows the committee to become debiased gradually so that it does not give large weights to easy biasconÔ¨Çicting samples, i.e., those already well handled by the main classiÔ¨Åer, and focuses more on difÔ¨Åcult ones. Note that, even with this strategy, the classiÔ¨Åers of the committee are still biased differently due to their different training sets with groundtruth labels. Finally, we further improve the proposed method by adopting a selfsupervised representation as the frozen backbone of the committee and the main classiÔ¨Åer. Since selfsupervised learning is not dependent on class labels, it is less affected by the spurious correlations between classes and latent attributes, leading to a robust and lessbiased representation. Also, by installing the committee and the main classiÔ¨Åer on top of the representation, the classiÔ¨Åers can be implemented efÔ¨Åciently in both space and time while enjoying the rich and biasfree features given by the backbone. LWBC is validated extensively on Ô¨Åve realworld datasets. It substantially outperforms existing methods using no bias label and even occasionally surpasses previous arts demanding bias labels. We also demonstrate that all of the main components, i.e., the use of the committee, its training with 2knowledge transfer, and the selfsupervised learning, contribute to the outstanding performance. The main contribution of this paper is fourfold: ‚Ä¢We present LWBC, a new approach to learning a debiased classiÔ¨Åer with no spurious attribute label. The use of consensus within the committee allows LWBC to address limitations of previous work relying on a single biased classiÔ¨Åer. ‚Ä¢We propose to learn the committee using knowledge of the main classiÔ¨Åer, unlike the previous work whose auxiliary modules do not consider the main classiÔ¨Åer. ‚Ä¢We investigate the potential of selfsupervised learning for debiasing, and Ô¨Ånd that it is a solid yet unexplored baseline for the task. ‚Ä¢LWBC demonstrates superior performance on Ô¨Åve realworld datasets. It outperforms existing methods using no additional supervision like ours and even surpasses those relying on spurious attribute labels occasionally. 2 Related work "
553,Multiple Expert Brainstorming for Domain Adaptive Person Re-identification.txt,"Often the best performing deep neural models are ensembles of multiple
base-level networks, nevertheless, ensemble learning with respect to domain
adaptive person re-ID remains unexplored. In this paper, we propose a multiple
expert brainstorming network (MEB-Net) for domain adaptive person re-ID,
opening up a promising direction about model ensemble problem under
unsupervised conditions. MEB-Net adopts a mutual learning strategy, where
multiple networks with different architectures are pre-trained within a source
domain as expert models equipped with specific features and knowledge, while
the adaptation is then accomplished through brainstorming (mutual learning)
among expert models. MEB-Net accommodates the heterogeneity of experts learned
with different architectures and enhances discrimination capability of the
adapted re-ID model, by introducing a regularization scheme about authority of
experts. Extensive experiments on large-scale datasets (Market-1501 and
DukeMTMC-reID) demonstrate the superior performance of MEB-Net over the
state-of-the-arts.","Person reidentication (reID) aims to match persons in an image gallery col lected from nonoverlapping camera networks [40], [14], [16]. It has attracted increasing interest from the computer vision community thanks to its wide ap plications in security and surveillance. Though supervised reID methods have achieved very decent results, they often experience catastrophic performance drops while applied to new domains. Domain adaptive person reID that can well generalize across domains remains an open research challenge. ?Corresponding author.arXiv:2007.01546v3  [cs.CV]  13 Jul 20202 Y. Zhai et al. Unsupervised domain adaptation (UDA) in reID has been studied exten sively in recent years. Most existing works can be broadly grouped into three categories. The rst category attempts to align feature distributions between source and target domains [35], [39], aiming to minimize the interdomain gap for optimal adaptation. The second category addresses the domain gap by employ ing generative adversarial networks (GAN) for converting sample images from a source domain to a target domain while preserving the person identity as much as possible [22], [5], [36], [24]. To leverage the target sample distribution, the third category adopts selfsupervised learning and clustering to predict pseudo labels of targetdomain samples iteratively to netune reID models [43], [15], [7], [37], [30], [8]. Nevertheless, the optimal performance is often achieved by en semble that integrates multiple subnetworks and their discrimination capability. However, ensemble learning in domain adaptive reID remains unexplored. How to leverage specic features and knowledge of multiple networks and optimally adapt them to an unlabelled target domain remains to be elaborated. In this paper, we present an multiple expert brainstorming network (MEB Net), which learns and adapts multiple networks with dierent architectures for optimal reID in an unlabelled target domain. MEBNet conducts iterative training where clustering for pseudolabels and models feature learning are al ternately executed. For feature learning, MEBNet adopts a mutual learning strategy where networks with dierent architectures are pretrained in a source domain as expert models equipped with specic features and knowledge. The adaptation is accomplished through brainstormingbased mutual learning among multiple expert models. To accommodate the heterogeneity of experts learned with dierent architectures, a regularization scheme is introduced to modulate the experts' authority according to their feature distributions in the target do main, and further enhances the discrimination capability of the reID model. The contributions of this paper are summarized as follows. {We propose a novel multiple expert brainstorming network (MEBNet) based on mutual learning among expert models, each of which is equipped with knowledge of an architecture. {We design an authority regularization to accommodate the heterogeneity of experts learned with dierent architectures, modulating the authority of experts and enhance the discrimination capability of reID models. {Our MEBNet approach achieves signicant performance gain over the state oftheart on commonly used datasets: Market1501 and DukeMTMCreID. 2 Related Works "
188,Scalable domain adaptation of convolutional neural networks.txt,"Convolutional neural networks (CNNs) tend to become a standard approach to
solve a wide array of computer vision problems. Besides important theoretical
and practical advances in their design, their success is built on the existence
of manually labeled visual resources, such as ImageNet. The creation of such
datasets is cumbersome and here we focus on alternatives to manual labeling. We
hypothesize that new resources are of uttermost importance in domains which are
not or weakly covered by ImageNet, such as tourism photographs. We first
collect noisy Flickr images for tourist points of interest and apply automatic
or weakly-supervised reranking techniques to reduce noise. Then, we learn
domain adapted models with a standard CNN architecture and compare them to a
generic model obtained from ImageNet. Experimental validation is conducted with
publicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred.
Results show that low-cost domain adaptation improves results compared to the
use of generic models but also compared to strong non-CNN baselines such as
triangulation embedding.","Many computer vision tasks, including object classiÔ¨Åcatio n and localization, as well as content based retrieval, are in  creasingly tackled with convolutionalneural network (CNN ) architectures. A lot of recent papers [1, 2] focus on improv ing CNN architectures and assume that largescale manually labeled datasets, such as ImageNet [3], are readily availab le. However, while large, the coverage of ImageNet is insufÔ¨Å cientinmanydomains. Forinstance,it illustratesonlya ve ry limited amount of named entities, including tourist points of interest or car brands and models. Such entities can never thelessbeofinterestindomainrelatedapplicationsandg ood qualityvisual resourceswhich illustrate them are assumed to be necessary. Manual resource enrichmentis tediousand not considered as scalable in terms of number of concepts and images. To overcome this issue, this paper focuses on domain adaptationand more precisely on automatic or weakly supervised methods to create the visual resources necessar y fordomainspeciÔ¨Åc CNN training. Due to resource scarcity, domain transfer recently re ceived a particular attention from the computer vision com munity. The authors of [4] have shown that activations of top layers of a CNN can be used effectively for image re trieval. More interestingly, they discoveredthat perform ance increases when the network is retrained with images similar to a speciÔ¨Åc domain (tourist points of interest). However, their method requires important manual intervention, sinc e theyverify200imagesforeachtouristpointofinterest(PO I) included in the training set. Equally important, they assum e thatthepresenceofnoisyimagesreducesthequalityofCNN modelsandcreatean unbalancedtrainingset (i.e. somePOIs are illustrated by 1000 images, while a majority of them are represented by only 100 images). In this paper, we focus on methodstobuildaCNNtrainingsetinafullyautomatedman ner and show that, contrary to the hypothesis of [4], a CNN can actually be trained from scratch with these images. This is probably due to the intrinsic quality of the training data set constitutedwithourmethodbutalsoto itslargersize. The automatic collection of groups of visually coherent images was already addressed in literature. The general ide a consistsofcollecting‚Äúnoisy‚Äùimageswhicharethenrerank ed according to a learned model [5] or use clustering to deter mine visually compact groups [6]. Beyond the fact these works did not aim at determining groups of image for CNN learning, we propose a simpler method, based on a Ô¨Åner do maincharacterization. Hence,ourapproachhasabettercom  putationaltractability,whilepreservingCNNmodelaccur acy. Experiments are carried out on three publicly available datasets which include images related to the tourism do main but have different visual properties. We test with Ox ford5k[7],INRIAHolidays[8]andDiv150Cred[9]andshow we obtain better results than other CNNbased method and amongthebestreportedintheliteratureonthese sets. 2. RELATED WORK "
225,Joint Ranking SVM and Binary Relevance with Robust Low-Rank Learning for Multi-Label Classification.txt,"Multi-label classification studies the task where each example belongs to
multiple labels simultaneously. As a representative method, Ranking Support
Vector Machine (Rank-SVM) aims to minimize the Ranking Loss and can also
mitigate the negative influence of the class-imbalance issue. However, due to
its stacking-style way for thresholding, it may suffer error accumulation and
thus reduces the final classification performance. Binary Relevance (BR) is
another typical method, which aims to minimize the Hamming Loss and only needs
one-step learning. Nevertheless, it might have the class-imbalance issue and
does not take into account label correlations. To address the above issues, we
propose a novel multi-label classification model, which joints Ranking support
vector machine and Binary Relevance with robust Low-rank learning (RBRL). RBRL
inherits the ranking loss minimization advantages of Rank-SVM, and thus
overcomes the disadvantages of BR suffering the class-imbalance issue and
ignoring the label correlations. Meanwhile, it utilizes the hamming loss
minimization and one-step learning advantages of BR, and thus tackles the
disadvantages of Rank-SVM including another thresholding learning step.
Besides, a low-rank constraint is utilized to further exploit high-order label
correlations under the assumption of low dimensional label space. Furthermore,
to achieve nonlinear multi-label classifiers, we derive the kernelization RBRL.
Two accelerated proximal gradient methods (APG) are used to solve the
optimization problems efficiently. Extensive comparative experiments with
several state-of-the-art methods illustrate a highly competitive or superior
performance of our method RBRL.","Traditional supervised singlelabel classication handles the task where each example is assigned to one class label. However, in many realworld classication applications, an example is often associated with a set of class labels. For instance, in text categorization, a document may belong to many labels such as \religion"" and \politics"". This brings the hot research interests of multilabel classication (MLC), which investigates the task where each example may be assigned to multiple class labels simultaneously. So far, MLC has witnessed its success in a wide range of research elds, such as function genomics [5, 9, 55], 2multimedia contents annotation [2, 29, 34], and NLP (e.g., text categorization [27, 31, 39], and information retrieval [52, 61, 16]). As a representative method for MLC, Ranking Support Vector Machine (RankSVM) [9] aims to minimize the empirical Ranking Loss while having a large margin and is enabled to cope with nonlinear cases with the kernel trick [25]. The classimbalance issue usually occurs in MLC, which mainly includes two aspects [53, 46]. On one hand, for a specic class label, the number of posi tive instances is greatly less than that of negative instances. On the other hand, for a specic instance, the number of relevant labels is usually less than that of irrelevant labels. Generally, the pairwise loss, which can be used to optimize imbalancespecic evaluation metrics such as the area under the ROC curve (AUC) and Fmeasure [6, 45], is more able to deal with the classimbalance issue than the pointwise loss. Thus, RankSVM can tackle the second aspect of the classimbalance issue in MLC by the minimization of the pairwise ap proximate ranking loss. Therefore, it can mitigate the negative in uence of the classimbalance issue in MLC. Nevertheless, apart from the rst ranking learn ing step, it needs another thresholding learning step, which is a stackingstyle way to set the thresholding function. Inevitably, each step has the estimation error. As a result, it may cause error accumulation and eventually reduce the nal classication performance for MLC. Therefore, it's better to nd a way to train the model in only one step. Although there are some methods proposed to tackle this issue, such as calibrated RankSVM [23] and RankSVMz [48], the basic idea of these methods is to introduce a virtual zero label for thresholding, 3which increases the number of the hypothesis parameter variables to raise the complexity of the model (i.e., the hypothesis set). Besides, while calibrated RankSVM [23] makes the optimization problem more computationally com plex, RankSVMz [48] makes it train more eciently. Moreover, there is little work to combine with Binary Relevance to address this issue. Binary Relevance (BR) [2] is another typical method, which transforms the MLC task into many independent binary classication problems. It aims to optimize the Hamming Loss and only needs onestep learning. Despite the intuitiveness of BR, it might have the classimbalance issue, especially when the label cardinality (i.e. the average number of labels per example) is low and the label space is large. Besides, it doesn't take into account label correlations, which plays an important role to boost the performance for MLC. Recently, to facilitate the performance of BR, many regularizationbased approaches [50, 51, 24, 47] impose a lowrank constraint on the parameter matrix to exploit the label correlations. However, little work has been done to consider the minimization of theRanking Loss to mitigate the negative in uence of the classimbalance issue and exploit the label correlations simultaneously. Moreover, these lowrank approaches are mostly linear models, which can't capture complex nonlinear relationships between the input and output. To address the above issues, in this paper we propose a novel multilabel classication model, which joints Ranking support vector machine and Binary Relevance with robust Lowrank learning (RBRL). Specically, we incorporate the thresholding step into the ranking learning step of RankSVM via Binary 4Relevance, which makes it train the model in only one step. It can also be viewed as an extension of BR, which aims to additionally consider the minimization of theRanking Loss to boost the performance. Hence, it can enjoy the advantages of RankSVM and BR, and tackle the disadvantages of both. Besides, the low rank constraint on the parameter matrix is employed to further exploit the label correlations. Moreover, to achieve nonlinear multilabel classier, we derive the kernelization of the linear RBRL. What's more, to solve the objective functions for the linear and kernel RBRL eciently, we use the accelerated proximal gradient methods (APG) with a fast convergence rate O(1=t2), wheretis the number of iterations. The contributions of this work are mainly summarized as follows: (1) We present a novel multilabel classication model, which joints RankSVM and BR with robust lowrank learning. (2) Dierent from existing lowrank approaches which are mostly linear mod els, we derive the kernelization RBRL to capture nonlinear relationships between the input and output. (3) For the linear and kernel RBRL, we use two accelerated proximal gradient methods (APG) to eciently solve the optimization problems with fast convergence. (4) Extensive experiments have conrmed the eectiveness of our approach RBRL over several stateoftheart methods for MLC. The rest of this paper is organized as follows. In Section 2, the related 5work about MLC is mainly reviewed. In Section 3, the problem of formulation and the model RBRL are presented in detail. The corresponding optimization algorithms are proposed in Section 4. In Section 5, experimental results are presented. Finally, Section 6 concludes this paper. 2. Related Work "
292,Noise-robust classification with hypergraph neural network.txt,"This paper presents a novel version of the hypergraph neural network method.
This method is utilized to solve the noisy label learning problem. First, we
apply the PCA dimensional reduction technique to the feature matrices of the
image datasets in order to reduce the ""noise"" and the redundant features in the
feature matrices of the image datasets and to reduce the runtime constructing
the hypergraph of the hypergraph neural network method. Then, the classic
graph-based semi-supervised learning method, the classic hypergraph based
semi-supervised learning method, the graph neural network, the hypergraph
neural network, and our proposed hypergraph neural network are employed to
solve the noisy label learning problem. The accuracies of these five methods
are evaluated and compared. Experimental results show that the hypergraph
neural network methods achieve the best performance when the noise level
increases. Moreover, the hypergraph neural network methods are at least as good
as the graph neural network.","During the last decade, the deep convolution neural network can be considered the current state of  the art method for various classification tasks such as image recognition [1], speech recognition [2], to name  a few. Recently, to deal with irregular data structures, data scientists have gained many interests in graph  convolution neural network method such as [3]. In this method, the pairwise relationships between objects  (samples) are used. In the other words, in this graph data structure, the edge of the graph can connect only  two vertices.   To overcome the information loss due to only considering the ‚Äúpairwise relationship between  objects ‚Äù of graph data structure [4, 5] have recently proposed the hypergraph neural network approach. In  this hypergraph data structure, an edge (hyperedge) can connect more than two vertices. In the other words,  the hy peredge is the subset of the set of vertices of the hypergraph. Recently, this hypergraph neural  network method has just been employed to solve classification tasks [4, 5] and outperforms the graph neural  network and can be considered the current state of the art method of semisupervised learning approach.  However, this method has also not been utilized to solve the noisy label learning problem.   Inspired from the idea combining the pagerank algorithm with the graph convolution neural network  in [6], in this paper, we propose the novel version of hypergraph neural network method combining the  classic hypergraph based semisupervised learning method [7, 8] with the hypergraph neural network  method [4, 5]. In the other words, we combine the propagation scheme utilizing the hypergraph model with  the hypergraph neural network which is the current state of the art method of semisupervised learnin g        ÔÅ≤  ISSN: 25024752  Indonesian J Elec Eng & Comp Sci, Vol. 21, No. 3, March 2021 :  1465  1 473 1466   approach. We find out that this proposed combination of the propagation scheme and the hypergraph neural  network method significantly improves the accuracy of the hypergraph neural network method alone even  when the noise presents in the labels.   In this paper, our contributions are threefolds:   a) In order to reduce the runtime constructing the graphs and the hypergraphs from the image datasets, we apply the dimensional reduction technique PCA to the image datasets. b) Propose the novel version of hypergraph neural network method combining the classic hypergraph based semisupervised learning method with the hypergraph neural network method. c) Compare the accuracy performance measures of the classic graph based semisupervised learning problem, the classic hypergraph based semisupervised learning problem, the graph neural network method, the hypergraph neural network method, and our proposed hypergraph neural network method when we apply these five methods to solve the noisy label learning problem. We will organize the paper as follows: Section 2 will discuss the related work. Section 3 will  introduce the novel version of hypergraph neural network method. Section 4 will describe the datasets and  present the experimental results. Section 5 will conclude this paper and the future direction of researches will  be discussed.   2. RELATED WORK "
543,Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion Classification.txt,"Deep neural networks (DNNs) have achieved great success in a wide variety of
medical image analysis tasks. However, these achievements indispensably rely on
the accurately-annotated datasets. If with the noisy-labeled images, the
training procedure will immediately encounter difficulties, leading to a
suboptimal classifier. This problem is even more crucial in the medical field,
given that the annotation quality requires great expertise. In this paper, we
propose an effective iterative learning framework for noisy-labeled medical
image classification, to combat the lacking of high quality annotated medical
data. Specifically, an online uncertainty sample mining method is proposed to
eliminate the disturbance from noisy-labeled images. Next, we design a sample
re-weighting strategy to preserve the usefulness of correctly-labeled hard
samples. Our proposed method is validated on skin lesion classification task,
and achieved very promising results.","Aiming to improve the performance of Deep Neural Net works (DNNs) on medical image analysis, the community is in the requirement of a huge amount of annotated im age data. Meanwhile, the huge capacity of DNNs makes it easily Ô¨Åt noisy labels. Incorrect in training labels can hurt the performance of DNNs on the test dataset [1]. Medical images‚Äôannotation quality is prone to experience, which re quires years of professional training and domain knowledge. For example, melanoma is the leading death cause of skin cancer, the accuracy of melanoma dermoscopy diagnosis in clinical is 50% to 82.3%; the unreliable image label issue can be very severe. With the high demanding of computeraided diagnosis of melanoma in clinical, it is of signiÔ¨Åcant impact to address the noisy label issue. Despite the label quality problem, DNNs are prone to other training set biases, espe cially class imbalance and hard samples [2]. An example of the typical hard samples in melanoma dermoscopy data is shown in Fig. 1, the appearance of benign and malignant Fig. 1 . Typical example of melanoma dermoscopy with clin ical diagnosis. cases can be vary similar. These hard samples are normally ambiguous and hence brings about extra challenges for iden tifying wronglabelled samples. In this study, we mainly focus on the noisy label issue, as the class imbalance issue can be solved easily during preprocessing or data collection. Although some approaches have been considered to ad dress the noisy label issue, it is still an ongoing challenge in deep learning for medical imaging. Aiming to simulate the relationship between noisy label and the latent clean la bel, Goldberger et al. [3] proposed to add a fully connected layer after softmax, where the updated weight represents the transition matrix between noisy and clean label. Patrini et al. [4] proposed a corrected loss by combining the noise transi tion matrix with traditional softmax cross entropy loss. These methods are heavily dependent on the accurate assumption of noise distribution, which is usually unknown in real practice. From the assumption that clean data will have a smaller loss than noisy data, Jiang et al. [5] proposed Mentornet, which learns small loss samples Ô¨Årst. Tanaka et al. [6] proposed to change the label of training data according to the softmax out put during training. Their methods have treated weak agree ment sample as noise, but the performance of these methods on medical image are degraded because of the hard samples that are usually presented in the medical image. There are also methods that are supervised by an extra group of clean data, such as a label clean network proposed by Veit et al. [7] and an adaptive weight learning method demonstrated by Ren et al. [8]. But these methods still need to maintain a set of expert annotated images. Those methods have demonstrated promising perfor mance in natural images. Not many studies have addressed the medical image noisy label issue. One pioneer work is [9]arXiv:1901.07759v2  [cs.CV]  24 Jan 2019Fig. 2 . The framework of the proposed learning approach.The network is jointly optimized by two types of losses: reweighted softmax loss and online uncertainty sample mining loss. by Dgani et al., they utilized the method of [3] on mammog raphy classiÔ¨Åcation task and outperforms standard training methods, but it is heavily dependent on the noise label distri bution assumption, and the hard samples and minority class in melanoma dataset will obstruct the assumption process. In this paper, we propose an iterative learning strategy with the aim of detecting noisy label in the training data and enhance the performance of the neural network. Notably, it is a tailormade strategy for medical images. SpeciÔ¨Åcally, an online uncertainty sample mining strategy is proposed to suppress the noisy samples, and an individual reweighting module is developed to preserve the hard samples and mi nority class. Extensive ablation studies demonstrate that the two components both contribute to the performance gain. The main contributions of this paper include: 1) An deep learning model based noisy label training strategy is proposed, which can enhance the model performance when the training data contains noisy labels; 2) A novel noisy label training loss is derived, which considers the hard samples as well as noisy labels. 2. METHOD "
305,Deep learning for class-generic object detection.txt,"We investigate the use of deep neural networks for the novel task of class
generic object detection. We show that neural networks originally designed for
image recognition can be trained to detect objects within images, regardless of
their class, including objects for which no bounding box labels have been
provided. In addition, we show that bounding box labels yield a 1% performance
increase on the ImageNet recognition challenge.","The task of separating objects from background is funda mental for many computer vision tasks. This has led to much research on localizing and classifying objects by us ing object segmentation, object detection, and region pro posals. Currently, most detectors are trained individually for each object class, which requires a class label and a bounding box for all images. Unfortunately, in this ap proach it is difÔ¨Åcult to transfer information from previously trained detectors to novel classes where bounding box la bels may not be available. This situation is common in current datasets, which often have many class labels but incomplete bounding box labels. In this work, we aim to overcome these challenges by training separately from bounding box labels and class labels, enabling our system to learn even when only one of these labels is available. This approach harnesses the notion of objectness (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013) to build a deep neural network (Krizhevsky et al., 2012) able to detect novel objects where bounding box labels have not been provided. One successful approach to object detection is to train a sin gle detector for each class of objects (for example, the De formable Parts Model (DPM) (Felzenszwalb et al., 2010)). In this approach, one discriminatively trains a set of detec Proceedings of the 31stInternational Conference on Machine Learning , Beijing, China, 2014. JMLR: W&CP volume 32. Copy right 2014 by the author(s).tors on each individual class. This strategy generally has proven useful on the Pascal VOC detection challenge due to the limited number of classes, each of which includes many bounding box labels. In other cases, however, where we may have an abundance of class labels, but few or no bounding box labels, it is not clear how to apply this same strategy. For example, the ImageNet dataset has 14 million class labels but only about 7%are labeled with bounding boxes (Deng et al., 2009). Recently, region proposal algorithms have shown good performance in object detection pipelines by proposing classgeneric locations for further classiÔ¨Åcation (Endres & Hoiem, 2010; Alexe et al., 2012; Girshick et al., 2013; Ui jlings et al., 2013). They attempt to measure objectness within an image by training on all bounding boxes labels, regardless of class, in hopes of building a single detector for all classes. While training from only bounding box labels potentially enables a detector to locate novel classes never seen be fore, it may perform poorly due to having too few train ing examples and failing to exploit the wealth of class la bels available in datasets like ImageNet. We propose to train a detector to localize objects while also exploiting ob ject class labels by separating the recognition and detec tion problems. We show that by pretraining our detector on class labels and then on object locations, we can increase its performance in detecting previously seen objects, while nearly retaining its ability to localize objects for which we have no bounding box labels. 2. Related works "
427,GPRAR: Graph Convolutional Network based Pose Reconstruction and Action Recognition for Human Trajectory Prediction.txt,"Prediction with high accuracy is essential for various applications such as
autonomous driving. Existing prediction models are easily prone to errors in
real-world settings where observations (e.g. human poses and locations) are
often noisy. To address this problem, we introduce GPRAR, a graph convolutional
network based pose reconstruction and action recognition for human trajectory
prediction. The key idea of GPRAR is to generate robust features: human poses
and actions, under noisy scenarios. To this end, we design GPRAR using two
novel sub-networks: PRAR (Pose Reconstruction and Action Recognition) and FA
(Feature Aggregator). PRAR aims to simultaneously reconstruct human poses and
action features from the coherent and structural properties of human skeletons.
It is a network of an encoder and two decoders, each of which comprises
multiple layers of spatiotemporal graph convolutional networks. Moreover, we
propose a Feature Aggregator (FA) to channel-wise aggregate the learned
features: human poses, actions, locations, and camera motion using
encoder-decoder based temporal convolutional neural networks to predict future
locations. Extensive experiments on the commonly used datasets: JAAD [13] and
TITAN [19] show accuracy improvements of GPRAR over state-of-theart models.
Specifically, GPRAR improves the prediction accuracy up to 22% and 50% under
noisy observations on JAAD and TITAN datasets, respectively","Accurate prediction of human trajectory, i.e., forecast ing pedestrians‚Äô future locations given their past (observed) frames in dynamic scenes, is critical for various applications such as autonomous driving [16], robotic navigation sys tems [18], and pedestrian tracking [21]. For the most part, challenges associated with predicting future trajectories are due to the presence of a multitude of features that may in Ô¨Çuence human future paths such as camera motion (ego Recognized action: walkingReconstructed poses and locationsfuture  trajectory PRARegomotion FA noisy human skeletons Figure 1: GPRAR addresses the problem of future trajec tory prediction given noisy pose observations by two novel subnetworks: (1) a human Pose Reconstruction and Action Recognition network (PRAR) to simultaneously reconstruct the noisy poses and recognize human actions, and (2) Fea ture Aggregator (FA) to channelwise aggregate the learned features: reconstructed poses and locations, actions, and egomotion to predict pedestrians‚Äô future locations. motion), human shapes (pose), past locations, and human actions. More importantly, these features are often noisy due to environmental and scene impediments, occlusions for example. This problem has signiÔ¨Åcantly degraded the performance of feature extractors, which in turn degrades the accuracy of the existing prediction models. Recent deeplearningbased methods[23, 25, 5, 1] have shown promising prediction results in ‚Äòperfect‚Äô settings, where ground truth (or complete) observations are given. Using ground truth observations helps model human motion more accurately and may improve the prediction accuracies. However, the ground truth data is unavailable during test time. This limits the potential applicability of these methodsarXiv:2103.14113v1  [cs.CV]  25 Mar 2021in practice. Other methods [20, 26] rely on preprocessing techniques to denoise the observations in advance of test ing. These approaches mainly focus on preprocessing (i.e. reconstructing or denoising) the human skeleton, an impor tant feature for prediction. However, they are easily prone to errors under harsh conditions, such as fast camera motion and occlusions, especially in dynamic scenes. In this work, the following challenges are addressed: (1) reconstruction of human pose, which is a nontrivial task in computer vi sion. To the best of our knowledge, none of existing meth ods successfully reconstruct human skeletons in dynamic video sequences by exploiting the structural properties of human skeletons spatially and temporally. (2) the use of lowlevel human pose features to learn the higherlevel ac tion features. So far, the skeletonbased action features have not been considered for prediction tasks. We design GPRAR to predict human future trajectory under noisy observations in dynamic video scenes by de vising solutions to the above challenges. It consists of two novel subnetworks: (1) a human pose reconstruction and action recognition network (PRAR) and (2) an encoder decoder based Feature Aggregator (FA), shown in Figure 1. The underlying idea of PRAR is to reconstruct human poses and learn action features simultaneously from the noisy pose detections. To best exploit the coherent and structural properties of human skeletons, PRAR is implemented with an encoder and two decoders, where each encoder and de coder is a multilayer spatiotemporal graph convolutional network operating on the naturally connected human joints (or pose graph). Furthermore, we propose an encoder decoder FA to channelwise aggregate the learned features: reconstructed poses and locations, actions, and camera mo tion using temporal convolutional networks (TCNs). The aggregated feature is then used to output the future trajec tory of a target pedestrian. In summary, the contributions of this paper are as follows: ‚Ä¢ We propose an efÔ¨Åcient and robust human trajectory prediction network (GPRAR) under noisy observa tions (Section 3). GPRAR consists of two novel subnetworks: a human pose reconstruction and ac tion recognition network (PRAR) and (2) an encoder decoder based Feature Aggregator (FA). ‚Ä¢ We evaluate our model on two commonly used datasets: TITAN and JAAD, and show that our method outperforms other methods with a large margin under noisy scenarios (Section 4). We also conduct ablation studies to demonstrate the effectiveness of each system component. 2. Related Work "
378,A Competitive Method for Dog Nose-print Re-identification.txt,"Vision-based pattern identification (such as face, fingerprint, iris etc.)
has been successfully applied in human biometrics for a long history. However,
dog nose-print authentication is a challenging problem since the lack of a
large amount of labeled data. For that, this paper presents our proposed
methods for dog nose-print authentication (Re-ID) task in CVPR 2022 pet
biometric challenge. First, considering the problem that each class only with
few samples in the training set, we propose an automatic offline data
augmentation strategy. Then, for the difference in sample styles between the
training and test datasets, we employ joint cross-entropy, triplet and
pair-wise circle losses function for network optimization. Finally, with
multiple models ensembled adopted, our methods achieve 86.67\% AUC on the test
set. Codes are available at https://github.com/muzishen/Pet-ReID-IMAG.","More and more families choose to keep some pets to ac company them in recent years. According to the GMI re port, global pet care market size surpassed 232 billion in 2020. With the rapid growth of pet economy, pet identiÔ¨Å cation is a challenging problem in many scenarios such as pet management, trading, insurance, medical treatment etc., unfortunately, there is no solution balanced accuracy, cost and usability well for this challenge up to now. In human biometrics, person/vehicle reidentiÔ¨Åcation (ReID) [4, 5, 8, 9, 11‚Äì14] methods based on deep learning have made a signiÔ¨Åcant process in recent years. Pet biomet ric challenge1is a workshop in the ECCV2020 conference. The challenge focuses on obtaining high area under curve (AUC) on a dog noseprint dataset. It is very challenging for dog noseprint reidentiÔ¨Åcation due to the adverse inÔ¨Çu 1https://www.vislab.ucr.edu/Biometrics2022/index.php Figure 1. The example of training data. Each column represents the same ID. ence of the sample class imbalance and lacking of labeled data, as shown in 1. However, we Ô¨Ånd that 1 vs 1 pet iden tity veriÔ¨Åcation by dog noseprint images is very similar to the pedestrian ReID task. The two tasks all need to train a model to extract features for each identity, and then com pare the extracted features to judge id information. Based on the pipeline of pedestrian ReID methods, we designed the framework of 1 vs 1 pet identity veriÔ¨Åcation. The rest of the paper is organized as follows. In Section 2, the proposed methods is introduced. The experimental results are presented in Section 3. And Ô¨Ånally Section 4 concludes the paper. 2. Methods "
326,Categorizing Items with Short and Noisy Descriptions using Ensembled Transferred Embeddings.txt,"Item categorization is a machine learning task which aims at classifying
e-commerce items, typically represented by textual attributes, to their most
suitable category from a predefined set of categories. An accurate item
categorization system is essential for improving both the user experience and
the operational processes of the company. In this work, we focus on item
categorization settings in which the textual attributes representing items are
noisy and short, and labels (i.e., accurate classification of items into
categories) are not available. In order to cope with such settings, we propose
a novel learning framework, Ensembled Transferred Embeddings (ETE), which
relies on two key ideas: 1) labeling a relatively small sample of the target
dataset, in a semi-automatic process, and 2) leveraging other datasets from
related domains or related tasks that are large-scale and labeled, to extract
""transferable embeddings"". Evaluation of ETE on a large-scale real-world
dataset provided to us by PayPal, shows that it significantly outperforms
traditional as well as state-of-the-art item categorization methods.","Online shopping websites have become extremely popular in recent years. In particular, marketplaces such as eBay.com and Amazon.com accept mil Corresponding author Email addresses: had.yonatan@gmail.com (Yonatan Hadar), shmueli@tau.ac.il (Erez Shmueli) 1arXiv:2110.11431v1  [cs.LG]  21 Oct 2021lions of new items every day (Cevahir & Murakami, 2016). To better cope with the enormous number of items, companies typically organize items into a predened set of categories (Shen et al., 2012). Such categorization of items1 is essential for enhancing user experience, where it allows users to search and navigate more easily between items, receive better recommendations for relevant items, and view customized descriptions of items. Categorization of items is also important for improving operational processes of the company, such as targeted advertising, determining shipping and handling fees, fraud detection, identication of duplicate items, and enforcement of company poli cies (e.g., which items are allowed or forbidden to be sold on the website) (Shen et al., 2012). While many item attributes can only be assigned by humans, the category of a given item can be inferred automatically from other attributes of that item. Automatic categorization of items is important both for saving the costs associated with manual labeling of millions of items every day and for ensuring the consistency of categories, which quickly becomes a serious prob lem when several individuals are involved in the labeling process (Kozareva, 2015). Indeed, many studies have investigated the problem of item categoriza tion as a machine learning text classication task. Earlier studies relied on representing text as a vector in a multidimensional space of identiers (e.g., single index terms or Ngrams) using various weighting schemes (e.g., TFIDF) (Ding et al., 2002; Yu et al., 2012; Mathivanan et al., 2018; Sun et al., 2014). However, these methods share the following limitations: 1) they typically produce training matrices which are very high dimensional and consequently very sparse; 2) the semantic meaning of words and their relationship to other words is typically ignored; and 3) the context of words (e.g., where they appear in the sentence, after which word, etc.) is rarely taken into account. With the growing popularity of deep learning, studies have started to show the applicability of deep learning also in the domain of item catego rization (Das et al., 2016; Ha et al., 2016; Cevahir & Murakami, 2016; Li et al., 2018; Krishnan & Amarthaluri, 2019; Chen et al., 2019). For ex 1While we name this task \item categorization"", it is important to note that a more accurate term would be \item classication"". Nevertheless, we stick with the term \item categorization"" as it is widely used in the literature, for example by Shen et al. (2012); Ha et al. (2016). 2ample, Ha et al. (2016) demonstrated how using multiple Recurrent Neural Network (RNN) layers for encoding textual item attributes can signicantly outperform traditional bagofwords methods. More recently, Krishnan & Amarthaluri (2019) showed how to combine both textual and nontextual item attributes as part of Long Short Term Memory network (LSTM) and Convolutional Neural Network (CNN) layers for improving classication per formance. However, a major limitation of deep learning methods is the need for very large labeled datasets to train on. Recent studies have tried to overcome the aforementioned limitation of deep learning models by using general purpose embeddings such as Word2Vec (Kozareva, 2015) and general purpose pretrained models such as BERT (Za hera & Sherif; Yang et al., 2020) and CamemBERT (Verma et al., 2020; Lee et al., 2020). In both cases, the idea is to train once deep learning models on a very large generalpurpose dataset, usually based on news articles, books, or Wikipedia pages. Then, the obtained embeddings or pretrained models are adjusted to the problem at hand, by using a considerably smaller labeled dataset from that problem's domain. For example, the two bestperforming solutions (Verma et al., 2020; Lee et al., 2020) in the ecommerce item cat egorization challenge as part of SIGIR 2020, used CamemBERT as a text encoder for item descriptions. However, as we demonstrate later in this pa per, the generalpurpose nature of these methods makes them less suitable for domain specic problems. In this paper, we focus on item categorization settings in which: 1) item descriptions are relatively short and noisy, and 2) labeled data for the tar get dataset is unavailable. Such settings entail that deep learning techniques cannot be applied directly on the target dataset, and generalpurpose embed ding might be less appropriate to use since the text distribution of the target dataset may dier greatly from that of the generalpurpose corpus they were trained with. To address such settings, we propose a novel learning framework, Ensem bled Transferred Embedding (ETE), which has four main steps: 1) manually label a small sample dataset; 2) extract embeddings from related largescale labeled datasets; 3) train transferred models using the extracted transferred embeddings and the labels of the sample dataset; and 4) build an ensemble to combine the outputs of the dierent transferred models into a single pre diction. We then show the applicability of the proposed framework to the item categorization task in settings for which item descriptions are noisy and short, and labels are not available. 3Extensive evaluation that we conducted, using a largescale realworld invoice dataset provided to us by PayPal, shows that our method signicantly outperforms all other considered traditional (e.g., TFIDF) as well as state oftheart (e.g., methods based on general purpose pretrained models such as BERT) item categorization methods. The contribution of this paper is threefold: ‚Ä¢We propose the ETE learning framework which relies on labeling a rel atively small sample of the target dataset, in a semiautomatic process, and leveraging other largescale labeled datasets from related domains or related tasks. ‚Ä¢We show the applicability of the proposed framework for the case of item categorization with short and noisy item descriptions. ‚Ä¢An extensive evaluation that we conducted demonstrates the superior ity of our method compared to traditional as well as stateoftheart text classication methods. The rest of this paper is structured as follows: In Section 2, we review the background and related work to this study. In Section 3, we describe the datasets we used in this study. In ection 4, we describe the proposed ETE learning framework and how it can be applied to our item categorization setting. Section 5 discusses the experimental setting and the results. Section 6 summarizes this paper and suggests directions for future work. 2. Related Work "
117,A Gift from Label Smoothing: Robust Training with Adaptive Label Smoothing via Auxiliary Classifier under Label Noise.txt,"As deep neural networks can easily overfit noisy labels, robust training in
the presence of noisy labels is becoming an important challenge in modern deep
learning. While existing methods address this problem in various directions,
they still produce unpredictable sub-optimal results since they rely on the
posterior information estimated by the feature extractor corrupted by noisy
labels. Lipschitz regularization successfully alleviates this problem by
training a robust feature extractor, but it requires longer training time and
expensive computations. Motivated by this, we propose a simple yet effective
method, called ALASCA, which efficiently provides a robust feature extractor
under label noise. ALASCA integrates two key ingredients: (1) adaptive label
smoothing based on our theoretical analysis that label smoothing implicitly
induces Lipschitz regularization, and (2) auxiliary classifiers that enable
practical application of intermediate Lipschitz regularization with negligible
computations. We conduct wide-ranging experiments for ALASCA and combine our
proposed method with previous noise-robust methods on several synthetic and
real-world datasets. Experimental results show that our framework consistently
improves the robustness of feature extractors and the performance of existing
baselines with efficiency. Our code is available at
https://github.com/jongwooko/ALASCA.","While deep neural networks (DNNs) have high expressive power that leads to promising performances, the success of DNNs heavily relies on the quality of training data, in par ticular, accurately labeled training examples. Unfortunately, labeling largescale datasets is a costly and errorprone pro cess, and even highquality datasets contain incorrect la bels (Nettleton, OrriolsPuig, and Fornells 2010; Zhang et al. 2017a). Hence, mitigating the negative impact of noisy la bels is critical, and many approaches have been proposed to improve robustness against noisy data for learning with noisy labels (LNL). Robustness to label noise is typically pursued by iden tifying noisy samples to reduce their contribution to the loss (Han et al. 2018; Mirzasoleiman, Cao, and Leskovec 2020), correcting labels (Yi and Wu 2019; Li, Socher, and *The two authors contributed equally. Copyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.Hoi 2020), utilizing a robust loss function (Zhang and Sabuncu 2018; Wang et al. 2019). However, one of the biggest challenges of LNL methods involves providing a de pendable criterion for distinguishing clean data from noisy data, such that clean data is fully exploited while Ô¨Åltering noisy data. While these existing methods are partially ef fective in mitigating label noise, their criterion for identify ing noisy examples uses biased posterior information from a linear classiÔ¨Åer or the penultimate layer of the corrupted network. These unpredictable biases can lead to a reduction in the network‚Äôs ability to separate clean and noisy instances (Nguyen et al. 2020; Kim et al. 2021a). To solve this undesired bias, several regularization meth ods (Xia et al. 2021; Cao et al. 2021) have been proposed to enhance the robustness of the feature extractor. However, while existing regularizationbased learning frameworks al leviate the degradation, these methods require multiple train ing stages and considerable computational costs and are dif Ô¨Åcult to apply in practice. Cao et al. (2021) used twostage training to compute the relative datadependent regulariza tion power to conduct Lipschitz regularization (LR) on inter mediate layers. Xia et al. (2021) identiÔ¨Åed and regularized the noncritical parameters that tend to Ô¨Åt noisy labels and require longer training time. Some studies (Zhang and Yao 2020; Zheltonozhskii et al. 2022) have designed contrastive learning frameworks to generate highquality feature extrac tors using unsupervised approaches, which require consid erable computations for high performance. To mitigate these impractical issues, we provide a sim ple yet effective learning framework for a robust feature ex tractor, Adaptive LAbelSmoothing via auxiliary ClAssiÔ¨Åer (ALASCA), with theoretical guarantee and small additional computation. Our proposed method is robust to label noise itself and can further enhance the performance of existing LNL methods. Our main contributions are as follows: ‚Ä¢ We theoretically explain that label smoothing (LS) im plicitly induces LR, which is known to enable robust training with noisy labels (Finlay et al. 2018; Cao et al. 2021). Through theoretical motivations, we empirically show that adaptive LS (ALS) can regularize noisy exam ples while fully exploiting clean examples. ‚Ä¢ To practically implement adaptive LR on the intermedi ate layers, we propose ALASCA, which combines ALS with auxiliary classiÔ¨Åers. To the best of our knowledge,arXiv:2206.07277v2  [cs.LG]  29 Nov 2022this is the Ô¨Årst study to apply auxiliary classiÔ¨Åers under label noise with theoretical evidence. ‚Ä¢ We experimentally demonstrate that ALASCA is uni versal by combining various LNL methods and vali dating that ALASCA consistently boosts robustness on benchmarksimulated and realworld datasets. ‚Ä¢ We verify that ALASCA effectively enhances the robust ness of feature extractors by comparing the quality of subsets on sampleselection methods and robustness to the hyperparameter selection of LNL methods. 2 Related Works "
106,Fine-Grained Named Entity Typing over Distantly Supervised Data Based on Refined Representations.txt,"Fine-Grained Named Entity Typing (FG-NET) is a key component in Natural
Language Processing (NLP). It aims at classifying an entity mention into a wide
range of entity types. Due to a large number of entity types, distant
supervision is used to collect training data for this task, which noisily
assigns type labels to entity mentions irrespective of the context. In order to
alleviate the noisy labels, existing approaches on FGNET analyze the entity
mentions entirely independent of each other and assign type labels solely based
on mention sentence-specific context. This is inadequate for highly overlapping
and noisy type labels as it hinders information passing across sentence
boundaries. For this, we propose an edge-weighted attentive graph convolution
network that refines the noisy mention representations by attending over
corpus-level contextual clues prior to the end classification. Experimental
evaluation shows that the proposed model outperforms the existing research by a
relative score of upto 10.2% and 8.3% for macro f1 and micro f1 respectively.","Named Entity Typing (NET) aims at classifying an entity mention to a set of entity types (e.g., person, location and organization) based on its context. It is one of the crucial components in NLP, as it helps in numerous down stream ing applications, e.g., information retrieval (Carlson et al. 2010), Knowledge Base Construction (KBC) (Dong et al. 2014), question answering (Lee et al. 2006), machine trans lation (Britz et al. 2017), etc. FineGrained Named En tity Typing (FGNET) is an extension of traditional NET to a much wide range of entity types (Corro et al. 2015; Ren et al. 2016a), typically over hundred types arranged in a hierarchical structure. It has shown promising results in different applications including KBC (Dong et al. 2014), re lation extraction (Mitchell et al. 2018), etc. In FGNET, an entity mention is labeled with multiple overlapping entity types based on the context. For instance, in the sentence: ‚ÄúAfter having recorded his role, Trump spent the whole day directing the movie. ‚Äù Trump can be an notated as both actor anddirector at the same time. Ow ing to a broad range of highly correlated entity types with M.A. ALi and W. Wang are the cocorresponding authors. Copyright c 2020, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.small contextual differences (Section 4.6), manual labeling is errorprone and timeconsuming, thus distant supervision is widely used to automatically acquire the training data. Distant supervision follows a twostep approach, i.e., detect ing the entity mentions followed by assigning type labels to the mentions using existing knowledge bases. However, it assigns type labels irrespective of the mention‚Äôs context, which results in high label noise (Ren et al. 2016b). This phenomenon is illustrated in Figure 1, where, for the sen tences denoted as: S1:S4, the entity mention ‚ÄúImran Khan‚Äù is labeled with all possible labels in the knowledgebase fperson, author, athlete, coach, politician g. Whereas, from the contextual perspective, in S1 the mention should be la beled asfperson, athleteg; in S2 it should be assigned labels fperson, authorg, etc. This label noise propagates in model learning, which hinders the improvement in performance. In an attempt to deal with the noisy training data, ex isting research on FGNET relies on the following differ ent approaches: (i) assume all labels to be correct (Ling and Weld 2012; Yogatama, Gillick, and Lazic 2015), which severely affects the model performance; (ii) apply differ ent pruning heuristics to prune the noisy labels (Gillick et al. 2014), however, these heuristics drastically reduce the size of training data; (iii) bifurcate the training data into two categories: clean and noisy, if the type labels corre spond to the same type path or otherwise (Ren et al. 2016a; Abhishek, Anand, and Awekar 2017), they ignore the fact that the labels, even corresponding to the same type path, may be noisy. For these approaches, it is hard to guarantee that underlying modeling assumptions will have a substan tial impact on alleviating the label noise. In addition, these approaches model the entity mentions entirely independent of each other, which hinders effective propagation of label speciÔ¨Åc contextual information across noisy entity mentions. In order to address the challenges associated with the noisy training data, we introduce a novel approach that puts an equal emphasis on analyzing the entity mentions w.r.t labelspeciÔ¨Åc corpuslevel context in addition to the sentencespeciÔ¨Åc context. SpeciÔ¨Åcally, we propose Fine Grained named Entity Typing with ReÔ¨Åned Representations (FGETRR), shown in Figure 2. FGETRR initially uses mention‚Äôs sentencespeciÔ¨Åc context to generate the noisy mention representation (PhaseI). Later, it uses corpuslevel contextual clues to form a sparse graph that surrounds a subarXiv:2004.03554v1  [cs.CL]  7 Apr 2020Entity	Mention:	Imran	Khan Candidate	types	via	Distant	Supervision: {person,	author ,	athlete,	coach,	politician} Entity	T ype	Hierarchy	( Yœà )S1:	The	former	cricket	great	Imran	khan	is	amongst	the	best	all rounders	of	his	time{ person,	athlete,	 author ,	coach,	politician } S2:	In	his	book	""Pakistan:	A	personal	history"",	Imran	khan focused	on	bilateral	ties{ person,	 athlete ,	 author ,	 coach,	politician } S3:	Former	cricketer	Imran	khan	is	selected	as	the	head	trainer	by Pakistan	cricket	board	{ person,	athlete,	 author , 	 coach, 	 politician } S4:	Imran	khan	founded	PTI,	a	centrist	political	party ,	in	1996. { person,	 athlete,	 author ,	coach, 	politician } (a) (b) (c)Root location person or ganization 		 author artist politician coach athleteFigure 1: (a) Entity mention and candidate entity types acquired via distant supervision, (b) Target Entity Type Hierarchy (c) Noisy training data with irrelevant entity types struckthrough. set of noisy mentions with a set of conÔ¨Ådent mentions hav ing high contextual overlap. And, performs edgeweighted attentive graph convolutions to recompute/reÔ¨Åne the repre sentation of noisy mention as an aggregate of the conÔ¨Ådent neighboring mentions lying at multiple hops (PhaseII). Fi nally, the reÔ¨Åned mention representation is embedded along with the type label representations for entity typing. We argue that the proposed framework has following ad vantages: (i) it allows appropriate information sharing by ef Ô¨Åcient propagation of corpuslevel contextual clues across noisy mentions; (ii) it analyzes the aggregated labelspeciÔ¨Åc context, which is more reÔ¨Åned compared with the noisy mentionspeciÔ¨Åc context; (iii) it effectively correlates the lo cal (sentencelevel) and the global (corpuslevel) context to reÔ¨Åne mention‚Äôs representation, required to perform the end task in a robust way. We summarize the major contributions of this paper as follows: We introduce FGETRR, a novel approach for FGNET that pays an equal importance on analyzing the entity mentions with respect to the corpuslevel context in addi tion to the sentencelevel context to perform entity typing in a performanceenhanced fashion. We propose an edgeweighted attentive graph convolution network to reÔ¨Åne the noisy mention representations. To the best of our knowledge, this is the Ô¨Årst work that, in contrast to the existing models that denoise the data at model‚Äôs input, reÔ¨Ånes the representations learnt over dis tantly supervised data. We demonstrate the effectiveness of the proposed model by comprehensive experimentation. FGETRR outper forms the existing research by a margin of upto 10.2% and 8.3% in terms of macrof1 and microf1 scores re spectively. 2 Related Work "
503,Residual Attention Network for Image Classification.txt,"In this work, we propose ""Residual Attention Network"", a convolutional neural
network using attention mechanism which can incorporate with state-of-art feed
forward network architecture in an end-to-end training fashion. Our Residual
Attention Network is built by stacking Attention Modules which generate
attention-aware features. The attention-aware features from different modules
change adaptively as layers going deeper. Inside each Attention Module,
bottom-up top-down feedforward structure is used to unfold the feedforward and
feedback attention process into a single feedforward process. Importantly, we
propose attention residual learning to train very deep Residual Attention
Networks which can be easily scaled up to hundreds of layers. Extensive
analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the
effectiveness of every module mentioned above. Our Residual Attention Network
achieves state-of-the-art object recognition performance on three benchmark
datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and
ImageNet (4.8% single model and single crop, top-5 error). Note that, our
method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69%
forward FLOPs comparing to ResNet-200. The experiment also demonstrates that
our network is robust against noisy labels.","Not only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures [34, 16, 23, 40]. At tention not only serves to select a focused location but also enhances different representations of objects at that loca tion. Previous works formulate attention drift as a sequen tial process to capture different attended aspects. However,as far as we know, no attention mechanism has been applied to feedforward network structure to achieve stateofart re sults in image classiÔ¨Åcation task. Recent advances of image classiÔ¨Åcation focus on training feedforward convolutional neural networks using ‚Äúvery deep‚Äù structure [27, 33, 10]. Inspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed atten tion mechanism in ‚Äúvery deep‚Äù structure. The Residual At tention Network is composed of multiple Attention Mod ules which generate attentionaware features. The attention aware features from different modules change adaptively as layers going deeper. Apart from more discriminative feature representation brought by the attention mechanism, our model also ex hibits following appealing properties: (1) Increasing Attention Modules lead to consistent perfor mance improvement, as different types of attention are cap tured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon. (2) It is able to incorporate with stateoftheart deep net work structures in an endtoend training fashion. Specif ically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network out performs stateoftheart residual networks on CIFAR10, CIFAR100 and challenging ImageNet [5] image classiÔ¨Åca tion dataset with signiÔ¨Åcant reduction of computation ( 69% forward FLOPs). All of the aforementioned properties, which are chal lenging to achieve with previous approaches, are made pos sible with following contributions: (1)Stacked network structure : Our Residual Attention Net work is constructed by stacking multiple Attention Mod ules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.arXiv:1704.06904v1  [cs.CV]  23 Apr 2017Origin imageFeature before maskSoftattentionmaskFeature after maskFeature before maskFeature after mask Lowlevel color feature  Sky maskHighlevel part feature Balloon instance maskClassificationInput Attention Attention mechanismSoftattentionmaskFigure 1: Left: an example shows the interaction between features and attention masks. Right: example images illustrating that different features have different corresponding attention masks in our network. The sky mask diminishes lowlevel background blue color features. The balloon instance mask highlights highlevel balloon bottom part features. (2)Attention Residual Learning : Stacking Attention Mod ules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mecha nism to optimize very deep Residual Attention Network with hundreds of layers. (3)Bottomup topdown feedforward attention : Bottomup topdown feedforward structure has been successfully ap plied to human pose estimation [24] and image segmenta tion [22, 25, 1]. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottomup fast feedforward process and topdown attention feedback in a single feedforward process which allows us to develop an endtoend trainable network with topdown attention. The bottomup topdown structure in our work differs from stacked hourglass network [24] in its intention of guiding feature learning. 2. Related Work "
583,Ensemble Neural Networks (ENN): A gradient-free stochastic method.txt,"In this study, an efficient stochastic gradient-free method, the ensemble
neural networks (ENN), is developed. In the ENN, the optimization process
relies on covariance matrices rather than derivatives. The covariance matrices
are calculated by the ensemble randomized maximum likelihood algorithm (EnRML),
which is an inverse modeling method. The ENN is able to simultaneously provide
estimations and perform uncertainty quantification since it is built under the
Bayesian framework. The ENN is also robust to small training data size because
the ensemble of stochastic realizations essentially enlarges the training
dataset. This constitutes a desirable characteristic, especially for real-world
engineering applications. In addition, the ENN does not require the calculation
of gradients, which enables the use of complicated neuron models and loss
functions in neural networks. We experimentally demonstrate benefits of the
proposed model, in particular showing that the ENN performs much better than
the traditional Bayesian neural networks (BNN). The EnRML in ENN is a
substitution of gradient-based optimization algorithms, which means that it can
be directly combined with the feed-forward process in other existing (deep)
neural networks, such as convolutional neural networks (CNN) and recurrent
neural networks (RNN), broadening future applications of the ENN.","  Artificial neural networks (ANN) are computing systems inspired by biological neural  networks that constitute animal brains. ANN is capable of approximating nonlinear functional  relationships between input and output variables  (Kim et al.,  2018). From a ma thematical  perspective, a neural network can model any function up to any given precision with a sufficiently  large number of basis functions  (Cybenko, 1989; Hornik, 1991). In addition, we can even use much  smaller models by constructing hierarchy neural n etworks (Delalleau & Bengio, 2011; Gal, 2016).  The basic processing elements of neural networks are neurons. A collection of neurons is referred  to as a layer, and the collection of interconnected layers forms the neural networks (Kim  et al. , 2018).  A four layer neural network is illustrated in Fig. 1 as an example. In a neuron, the output is calculated  by a nonlinear function of the sum of its inputs. The connections between different neurons from  adjacent layers are represented by the weights in a model. The weights adjust as learning proceeds ,  and they represent the strength of the signal at a connection. The nonlinear function is also called  the activation function, and the most popular choices are sigmoid, tansig, and ReLU (Li et al.,  2015). 2   ANN has bee n widely applied to solving real world engineering problems, and the following three  topics are significant for effective applications .        Fig. 1.  The structure of an artificial neural network.  ijm  denotes the weight between the ith neuron in a layer and  the jth neuron in the next layer. A neuron is a combination of a linear summation of inputs and an activation  function.     The first topic is uncertainty quantification. Uncertainty is inevitable in all kinds of prediction  models, in cluding neural networks. Predictive uncertainty results from data uncertainty caused by  noisy data, and model uncertainty comes from model parameters and model structure. Uncertainty  quantification determines how much confidence one has in a certain predic tion. This information is  desirable in numerous fields that have the possibility to directly or indirectly affect human life , and  control of them has been gradually handed over to automated systems  (Gal, 2016) , such as life  sciences (Herzog & Ostwald, 2013 ; Acharya et al., 2018 ) and autonomous vehicles (Widrow et al.,  1994 ; Tian et al., 2018 ).  The second topic concerns  data availability. Although data are the most precious resource in  machine learning, data collection is very expensive and time consuming in  many real world  engineering problems. For example, in the field of gas resource evaluation in petroleum engineering,  adsorbed gas content estimation is significant (Wu et al., 2014). However, an adsorption experiment  could take a week to collect a single pair of data, and it is normal to spend millions of dollars in  coring processes to obtain experimental material. Thus, most adsorbed shale gas datasets comprise  less than 100 data (Chen et al., 2017), which hinders the application of neural networks. Data  availability is especially important for deep learning , in which  tens of thousands of weights need to  be trained (He et al., 2016).   The final topic is not yet critical , but has the potential to greatly broaden the scope of neural  network applications. In s ome circumstances, it is desirable to have a gradient free optimization  method. For example, in the field of brain inspired computing, the Hodgkin Huxley (HH) model  (Hodgkin & Huxley, 1952) is utilized as the neuron model rather than the traditional McCull och Pitts (MCP) model (McCulloch & Pitts, 1943), in which the neuron structure is a linear combination  of inputs with an activation function. Although the HH model is much more elaborate and  biomimetic, and thus more accurate, it is described by a set of n onlinear differential equations and  obtaining the derivatives is challenging. A gradient free optimization method could be applied to  natural language processing, as well. Bilingual evaluation understudy is an algorithm for evaluating  3   the quality of text t hat has been machine translated (Papineni et al., 2002 ; Reiter, 2018 ). However,  it is difficult to build a loss function based on this evaluation criterion since it is not differentiable.  However, this will no longer pose a problem if we can find a gradient free optimization method.   Considering the aforementioned problems, a salient question is: are there any alternatives for  the optimization method in a neural network that are able to perform uncertainty analysis and  perform well with a small datase t, but do not rely on derivative calculations?   These obstacles are encountered in numerous engineering fields, such as petroleum  engineering. Uncertainty quantification is critical because underground geological parameters are  highly heterogeneous. High dimension models are always solved based on a small dataset due to  the expensive and time consuming data collection. It is also difficult to identify gradients of a target  variable with respect to model parameters because the corresponding physical models ar e highly  nonlinear and too complicated to solve analytically. In response to these problems, the ensemble  randomized maximum likelihood algorithm (EnRML) is proposed by Gu and Oliver (2007) in the  field of history matching in petroleum engineering. History  matching is an inverse modeling method,  which adjusts a model of a reservoir until it closely reproduces its past behavior (Oliver et al., 2008 ;  Stordal & N√¶vdal, 2018 ). It should be mentioned that the word ‚Äúensemble ‚Äù here indicates a different  meaning fr om that in ensemble averaging (Naftaly et al., 1997). In the former, it means the ensemble  of realizations generated from the same model, rather than multiple models in the latter. The most  prominent feature of the EnRML is that it constitutes a gradient free optimization method because  covariance matrices computed from the realizations are utilized for optimization instead of search  gradients.  Moreover , the EnRML is designed to solve high dimensional problems, which is an  advantage over other gradient free methods, such as the covariance matrix adaptation evolution  strategy (CMA ES) (Hansen  & Kern , 2004) . Chen  and Oliver  (2010) have successfully solved a  267300 dimension al SPE benchmark problem with the EnRML  based on 104 realizations.   The objective of this study is to find a method that is capable  to perform uncertainty analysis ,  perform  well with a small dataset , and do es not rely on derivative calculations. To achieve this  objective, ensemble neural networks (ENN) is proposed based on  the EnRML algorithm. In the  ENN, the feed forward process is the same as the common fully connected neural networks, but the  network training process is adjusted by substituting the EnRML for the traditional gradient descent  algorithm. Uncertainty quantif ication is straightforward in the ENN since it is based on the Bayesian  theorem. In addition, the ENN is not sensitive to data size, and its optimization process does not  necessitate the calculation of derivatives.    The ENN is a gradient free stochastic me thod, which combines the EnRML method of  historical matching with neural networks for the first time. The ENN method also shows that the  neural networks can be trained through correlation information from stochastic realizations  without  calculating the der ivative s. This study verifies the characteristics of the ENN through three  computational experiments , which are regression based on a toy dataset, sanity check based on a  highly nonlinear ideal dataset, and generalization test based on real world datasets .    2. Methodology   "
478,Integration of Autoencoder and Functional Link Artificial Neural Network for Multi-label Classification.txt,"Multi-label (ML) classification is an actively researched topic currently,
which deals with convoluted and overlapping boundaries that arise due to
several labels being active for a particular data instance. We propose a
classifier capable of extracting underlying features and introducing
non-linearity to the data to handle the complex decision boundaries. A novel
neural network model has been developed where the input features are subjected
to two transformations adapted from multi-label functional link artificial
neural network and autoencoders. First, a functional expansion of the original
features are made using basis functions. This is followed by an
autoencoder-aided transformation and reduction on the expanded features. This
network is capable of improving separability for the multi-label data owing to
the two-layer transformation while reducing the expanded feature space to a
more manageable amount. This balances the input dimension which leads to a
better classification performance even for a limited amount of data. The
proposed network has been validated on five ML datasets which shows its
superior performance in comparison with six well-established ML classifiers.
Furthermore, a single-label variation of the proposed network has also been
formulated simultaneously and tested on four relevant datasets against three
existing classifiers to establish its effectiveness.","ClassiÔ¨Åcation is one of the most popular topics in machine learning and is widely performed by various types of supervisedlearning approac hes. When discussing about classiÔ¨Åcation, the traditional approach is indicate d by default. Here, each data instance is associated with a single class, which is thu s termed as singlelabel classiÔ¨Åcation. However, real world scenarios follow a diÔ¨Äerent track of problems where data can be annotated with multiple labels at a time. This gave rise to a subdomain known as multilabel (ML) classiÔ¨Åcation [1]. Reallife examples of multilabel data can be easily seen around us. So cial media posts using various hashtags (labels) for a single image or text , movies categorized under diÔ¨Äerent genres, and so on. In each of these c ases, a set of labels is associated with a particular instance of the dataset which de termines the classes associated with them. However, due to more classes link ed with each data, there is an increase in complexity of decision space. The class b oundaries are much more convoluted and overlapping due to the increased gen erality of ML classiÔ¨Åcation. In a way, ML classiÔ¨Åcation can be seen as a superse t of singlelabel classiÔ¨Åcation where the classiÔ¨Åers need to be able to out put several predictions at once. Keeping the complex nature of the problem in mind, there are a few ap  proaches which researchers have been using to handle ML data. Th e existing classiÔ¨Åers present in literature can be divided into few groups: data transforma tion [2, 3], problem adaptation [4, 5] and ensemble classiÔ¨Åers [3, 6, 7]. Among these, theproblemadaptationtechniquesarethemostconvenien tandarewidely used. Some benchmark problem adaptation methods include the use of support vector machines (SVM) [8], multilayer perceptron (MLP) [9], knear est neigh bours [4], decisiontrees [10] and probabilistic classiÔ¨Åers [6]. While dea ling with the complex nature of ML classiÔ¨Åcation, most of these methods han dle the data as it is. However, it is seen that neural networks (NNs) are quite ca pable of han 2dling complexities without actually creating a bulky model. They are loos ely inspired by the working of a human brain and are one of the most popu lar and widely used tool for machine learning. They inherently bring nonlinea rity by increasing the number of layers in the model which are able to follow dis parate structures in the datasets. Although lot of work has been done in s inglelabel classiÔ¨Åcation using NN, it is to be noted that comparatively very few w orks include the usage of NNs for ML classiÔ¨Åcation. In this current work, the authors have attempted to construct a novel two layer transformation network, adapted from multilabel function al link artiÔ¨Åcial neural network (MLFLANN), previously proposed by the authors in [11], and the wellknown autoencoders (AE). This adaptation of MLFLANN an d AE has beenaptlynamedastheAutoEncoderintegratedMLFLANN(AEML FLANN). This network is capable of overcoming few drawbacks faced in multila bel clas siÔ¨Åcation previously. In the Ô¨Årst layer of our network, we perform functional expansion of features inspired from MLFLANN. The main motivation t o incor porate MLFLANN is its compact structure and complexity handling ca pability which seems suitable for multilabel classiÔ¨Åcation. The input feature s are func tionally expanded to a higher dimension, thus giving rise to a decision sp ace with increased separability. This introduces nonlinearity in the data , similar to that of multilayer perceptrons. It is an attempt to improve the input space, thereby, increasing the convergence. However, the transform ed data helps to improve the multilabel feature space only to some extent. The exp ansion of input space might not always give rise to an optimal representation. There is still some scope of further transforming the data which will lead to m ore im proved performance. Also, the functional expansion leads to the increase in the input dimension, which poses a problem for multilabel data. To handle both these issues, we introduce a second feature transformationcu mreduction layer incorporating autoencoders. The second layer is created from th e encoder sec tion of an AE, which is capable of transforming the features to a com paratively reduced and improved space. Autoencoders are widely known to imp licitly ex tract features for classiÔ¨Åcation tasks, while successfully transf orming the data 3[5]. It is capable of generating a suitable representation through un supervised learning. To concisely describe the network, it can be said that, the input fea tures are functionally expanded in the Ô¨Årst layer. In the second layer, these expanded features are passed through an AE which generates a favourable representation in a reduced feature space. These reduced and transformed fea tures are then mapped to the output layer. This AEMLFLANN model is capable of go od multilabel classiÔ¨Åcation as it can handle the complex decision space be tter by transforming the data through two consecutive layers. The prop osed work has highlighted the importance of the two transformation layers, which can be ex tendedfurthertobuild deepernetworks. Thisliesoutsidethescop eofthis paper and will be explored in future. AEMLFLANN has been experimentally s hown to perform better than six benchmark methods over Ô¨Åve multilabe l datasets. As per the knowledge of the authors, the proposed neural netwo rk model does not exist in literature, hence its application has been explored from m ultilabel domain to traditional singlelabel domain as well. This singlelabel vers ion of the proposed model, named AutoEncoder integrated singlelabel F LANN (AE SLFLANN), has been separately tested on four relevant dataset s to analyse its success. The contribution of this work can be highlighted as follows. ‚Ä¢IntroducinganoveltwolayernetworkbasedonMLFLANNandAEs pecif ically for multilabel classiÔ¨Åcation. ‚Ä¢Improving separability in multilabel data by Ô¨Årst applying functional ex pansion layer, followedby additional transformationby autoencod er layer. ‚Ä¢The increased feature dimension caused by the Ô¨Årst layer is reduce d by consecutive AE in the second layer. This maintains a balance between the feature space and sample size, which leads to a good training of t he classiÔ¨Åer with limited data. ‚Ä¢Introducing the singlelabel variation of this novel twolayer netw ork. 4‚Ä¢Experimental analysis of both singlelabel and multilabel classiÔ¨Åcat ion networks with benchmark datasets. The rest of the paper is organized as follows. In Section 2 related wo rks is discussed. In Section 3 some background on multilabel classiÔ¨Åcatio n followed by the proposed model, termed as AEMLFLANN is described. In Sec tion 4, results are put and Section 5 concludes the paper. 2. Related Works "
239,Deep Learning with Label Noise: A Hierarchical Approach.txt,"Deep neural networks are susceptible to label noise. Existing methods to
improve robustness, such as meta-learning and regularization, usually require
significant change to the network architecture or careful tuning of the
optimization procedure. In this work, we propose a simple hierarchical approach
that incorporates a label hierarchy when training the deep learning models. Our
approach requires no change of the network architecture or the optimization
procedure. We investigate our hierarchical network through a wide range of
simulated and real datasets and various label noise types. Our hierarchical
approach improves upon regular deep neural networks in learning with label
noise. Combining our hierarchical approach with pre-trained models achieves
state-of-the-art performance in real-world noisy datasets.","The robustness of deep learning has been studied from different aspects. One of the topics focuses on investigating whether deep neural networks can learn from noisy labels as it can be difÔ¨Åcult to collect data with clean annotations in many real applications [1], [2]. Although deep learning models enjoy certain generalizability for different tasks, they can be very sensitive to label noise as they tend to memorize noise during training due to their expressivity [3], [4]. In addition, there exist different types of label noise [5] and each of them may have its unique effects on the model performance. Deep learning models designed to mitigate label noise can be broadly categorized into two groups: modelbased and modelfree [6]. Modelbased methods depend on ex plicit assumptions about the distribution and the behavior of the noise. Popular techniques in modelbased settings in clude noisy channel [7], [8], [9], data pruning [10], [11], [12] and sample selection [13], [14], [15]. Modelfree methods, on the other hand, aim to improve robustness without ex plicitly modeling the label noise structure. Common model free methods include robust losses such as nonconvex loss [16], [17], [18], generalized crossentropy loss [19], meta learning [20], [21], [22], and regularization [23], [24], [25] among others [26], [27], [28]. We focus on the modelfree setting in this paper due to its broad applicability. Li Chen is Research Scientist with Meta AI. Email: lichen66@fb.com Ningyuan (Teresa) Huang and Cong Mu are PhD students in the Depart ment of Applied Mathematics and Statistics, Johns Hopkins University. Email: nhuang19@jhu.edu, cmu2@jhu.edu Hayden S. Helm, Kate Lytvynets, and Weiwei Yang are with Microsoft Research. Email: haydenshelm@gmail.com, kalytv@microsoft.com, wei wya@microsoft.com Carey E. Priebe is Professor in the Department of Applied Mathemat ics and Statistics (AMS), the Center for Imaging Science (CIS), and the Mathematical Institute for Data Science (MINDS), Johns Hopkins University. Email: cep@jhu.eduWe propose a simple and efÔ¨Åcient hierarchical approach that requires no change of the network architecture and the optimization mechanism. This is in contrast to the existing modelfree methods above that typically require either signiÔ¨Åcant change in network architecture (e.g., co teaching in [14], [29]) or in the optimization procedure (e.g., metalearning in [22], semisupervised learning in [30], [31]). Experiments on the benchmark datasets with synthetic noise suggest that our proposed hierarchical approach can statistically (and operationally) improve the performance of the deep learning model (see Figure 1). This result is similarly observed in a realworld dataset with inherent label noise. Fig. 1. Performance advantages obtained by our hierarchical model (HC) compared to the standard model (FLAT) on ICON94 dataset with uniform noise and noise ratio 2f0%;20%;30%;50%g. The accuracy gain (of HC over FLAT) is both statistically and operationally signiÔ¨Åcant. The rest of this paper is organized as follows. Section 2 provides background on label noise taxonomy and related work on the main directions of this topic. Section 3 intro duces our proposed hierarchical model. Section 4 provides experimental results on different datasets with various types of label noise. Section 5 conducts ablation study on our proposed method. Section 6 discusses our Ô¨Åndings and future work.arXiv:2205.14299v1  [cs.LG]  28 May 2022IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2 2 B ACKGROUND 2.1 Label Noise Taxonomy In this section, we provide different major categorizations of label noise. The simplest type of label noise is known as uniform label noise, where the ground truth labels are changed to the wrong labels uniformly with a probability p. A slightly more complicated type of label noise is class dependent. In classdependent settings there is a noise transition matrix TKK, whereKis the number of classes, that governs the probability of a groundtruth label getting switched to a different label. In particular, if an observation has a ground truth label ithen the probability that the observed class label is jisTij. Hence, the diagonal of the matrixTis proportion of trulyclass iremaining class i. The transition matrix Tneed not be symmetric. A yet more complicated setting is feature and classdependent. That is, the probability of transitioning from class ito class jis a function of both the feature vector and the ground truth labeli. This setting best mimics realworld label noise scenarios as the difÔ¨Åculty in labeling for human annotators is nonuniform for most groundtruth class conditionals. Some authors consider partdependent label noise where the noise only partially depends on an instance [32]. Also related to the label noise taxonomy is the perspec tive of [33] and [34] where they describe the label noise problem in the language of uncertainty. In their character ization, there are two types of uncertainty: aleatoric and epistemic. Aleatoric uncertainty persists in the data even as the number of samples goes to inÔ¨Ånity whereas epistemic uncertainty can be avoided with a sufÔ¨Åcient amount of data. The uniform label noise setting we consider herein falls under aleatoric uncertainty, and the classdependent noise setting under both aleatoric and epistemic uncertainty. 2.2 Related work "
36,CTRL: Clustering Training Losses for Label Error Detection.txt,"In supervised machine learning, use of correct labels is extremely important
to ensure high accuracy. Unfortunately, most datasets contain corrupted labels.
Machine learning models trained on such datasets do not generalize well. Thus,
detecting their label errors can significantly increase their efficacy. We
propose a novel framework, called CTRL (Clustering TRaining Losses for label
error detection), to detect label errors in multi-class datasets. It detects
label errors in two steps based on the observation that models learn clean and
noisy labels in different ways. First, we train a neural network using the
noisy training dataset and obtain the loss curve for each sample. Then, we
apply clustering algorithms to the training losses to group samples into two
categories: cleanly-labeled and noisily-labeled. After label error detection,
we remove samples with noisy labels and retrain the model. Our experimental
results demonstrate state-of-the-art error detection accuracy on both image
(CIFAR-10 and CIFAR-100) and tabular datasets under simulated noise. We also
use a theoretical analysis to provide insights into why CTRL performs so well.","Neural networks (NNs) have demonstrated success in nu merous classiÔ¨Åcation applications. Large and highquality datasets are essential for the success of NN training. In gen eral, it takes a lot of effort to label a large dataset manu ally. This process is also errorprone. Sometimes, it is not even feasible. Even wellknown humanannotated datasets have been found to have signiÔ¨Åcant labeling errors, e.g., Im ageNet (Deng et al. 2009) is known to have close to 6% labeling error in its validation set (Northcutt, Athalye, and Mueller 2021). Labels that are different from their true class are said to be noisy, else clean. In practice, each data in stance belongs to one or multiple hidden true classes and we are only provided with observed labels that may poten tially be erroneous. In fact, many popular datasets are not very clean (Northcutt, Athalye, and Mueller 2021). It is also known that deep NNs can easily Ô¨Åt random labels. Models overÔ¨Åtted on bad training data have poor predictive power on clean test sets (Zhang et al. 2021; Arpit et al. 2017). Deep learning models also display memorization ef fects. They Ô¨Årst memorize samples with clean labels (also 1CTRL is opensource: https://github.com/changyue/ctrlcalled ‚Äúearly learning‚Äù) and then start adapting to samples with noisy labels after sufÔ¨Åcient epochs of training. Large capacity models can eventually memorize all samples. This phenomenon is independent of the optimizations used dur ing training or the NN architectures employed (Zhang et al. 2021). Before overÔ¨Åtting, when all samples have close to zero losses, clean and noisy labels result in different loss curves due to the difference in how learning progresses for each type. This is exploited in many label error detection methods. MentorNet (Jiang et al. 2018) monitors the learn ing process and provides a curriculum to reweight sam ples. O2UNet (Huang et al. 2019) Ô¨Ånds label errors by sorting the average training loss of all samples. Wang et al. (Wang et al. 2022) detect label errors by classifying the loss curves. Arazo et al. (Arazo et al. 2019) model loss dis tribution in every epoch to infer a sample‚Äôs probability of being wrongly annotated. Xia et al. (Xia et al. 2020) prevent NNs from overÔ¨Åtting bad samples through early stopping. Early learning is theoretically analyzed in (Liu et al. 2020; Li, Soltanolkotabi, and Oymak 2020). Some methods demonstrate success against noisy labels by training NNs with selected samples. ConÔ¨Ådent learning (CL) (Northcutt, Jiang, and Chuang 2021) and O2UNet (Huang et al. 2019) train models over two rounds: Ô¨Årst, they train models on the noisy dataset to detect errors, then detect and remove samples with wrong labels, and Ô¨Ånally retrain the model on the clean data. Coteaching (Han et al. 2018) and Coteaching+ (Yu et al. 2019) perform dynamic sample selection during training. In this article, we present an effective framework, called CTRL, to detect noisy labels. It relies on the observation that training progresses differently for clean and noisy la bels. CTRL uses the Kmeans algorithm to classify labels as clean or noisy by clustering their training loss curves. The main contributions of this work are as follows. ‚Ä¢ We introduce a label error detection method that Ô¨Ånds noisy labels of samples by clustering their training loss trajectories. ‚Ä¢ After label error detection, we present label cleaning methods to enable retraining of the model on the cleaned dataset. ‚Ä¢ We verify the proposed method on popular benchmark datasets under simulated noise; our method achievesarXiv:2208.08464v1  [cs.LG]  17 Aug 2022stateoftheart label error detection accuracy and com parable model classiÔ¨Åcation accuracy to prior stateof theart. ‚Ä¢ To enable better understanding of the mechanism behind the method, we theoretically analyze a binary classiÔ¨Åca tion problem to demonstrate that the presence of a train ing loss gap between clean and noisy labels is highly probable. The article is organized as follows. We present related work in Section 2. We describe the methodology in Section 3. We present experimental results in Section 4. We conclude in Section 5, mentioning future directions. 2 Related Work "
9,Addressing Ambiguity of Emotion Labels Through Meta-Learning.txt,"Emotion labels in emotion recognition corpora are highly noisy and ambiguous,
due to the annotators' subjective perception of emotions. Such ambiguity may
introduce errors in automatic classification and affect the overall
performance. We therefore propose a dynamic label correction and sample
contribution weight estimation model. Our model is based on a standard BLSTM
model with attention with two extra parameters. The first learns a new
corrected label distribution, and is aimed to fix the inaccurate labels from
the dataset. The other instead estimates the contribution of each sample to the
training process, and is aimed to ignore the ambiguous and noisy samples while
giving higher weight to the clear ones. We train our model through an
alternating optimization method, where in the first epoch we update the neural
network parameters, and in the second we keep them fixed to update the label
correction and sample importance parameters. When training and evaluating our
model on the IEMOCAP dataset, we obtained a weighted accuracy (WA) and
unweighted accuracy (UA) of respectively 65.9% and 61.4%. This yielded an
absolute improvement of 2.5%, 2.7% respectively compared to a BLSTM with
attention baseline, trained on the corpus gold labels.","Automatic recognition of affect and emotion is important to enable a more natural and engaging communication between humans an d machines. In this work we concentrate on emotion recognitio n from speech, which is the task to estimate the emotional content o f a spo ken utterance. In the past, emotion recognition was performed by extractin g a set of lowlevel features from each frame of an audio sample. These features were then aggregated through various statistical aggrega tion function (mean, standard deviation, min, max, etc.) to a global utterancelevel vector representation [1], to be Ô¨Ånally fe d through a shallow classiÔ¨Åer such as Support Vector Machines (SVM) [2, 3]. However, in recent years, the accuracy of speech emotion rec ogni tion has dramatically improved with the introduction of Dee p Neural Networks (DNN). Initial DNNbased models [4] were still bas ed on the same utterancelevel feature extraction. However, in s ubsequent approaches, speech features extracted from each frame were used as inputs of more complex neural network architectures such as Con volutional Neural Networks (CNN) and Recurrent Neural Netw orks (RNN), and the accuracy was further improved [5, 6, 7]. Recen t years saw the application of novel methods developed from ot her AI Ô¨Åelds, such as selfattention models [8], Connectionist Temporal ClassiÔ¨Åcation (CTC) [9] and Dilated Residual Network (DRN) [10]. Even higher performance was achieved by employing multimo dal information, such as audio and image together with speech [1 1].While most of the effort concentrated on the development of more accurate classiÔ¨Åcation models, there were other aspec ts of emotion classiÔ¨Åcation regarding the data itself that were m ostly ignored, but that could help improving the performance. In m any datasets, the emotional labels are annotated based on human anno tators‚Äô perception and sensibility to emotion. Emotion per ception is highly subjective [12], therefore the labels often contain some noise due to humans‚Äô decision ambiguity. For instance, an annotat or may assign the label neutral not when the sample is actually neutral, but when he is unsure about the most correct emotion class. Likew ise, he may mistakenly recognize some loud enthusiastic speech a s an gry, while instead it is happy. Training a model on such noisy labels is likely the cause of some performance degradation, becaus e the model may become confused and may not clearly distinguish on e emotion from another. Another important issue is that, in many emotion recognitio n datasets, the numbers of utterances for each emotional cate gory are imbalanced. Generally, in the classiÔ¨Åcation task using the se category imbalanced dataset, accuracy of the small class is decrease d [13, 14], which in turn affects the overall accuracy. To overcome thes e prob lems, some methods were proposed to employ soft target appro aches to correct the annotation ambiguities [15], or to augment th e dataset with synthetic data to reduce the effect of data imbalance [1 6]. How ever, the former method only performs a static label contrib ution estimation based on the original annotation data, while the latter method is complex and the generated data might still be affec ted by the original labeling noise. In other domains, such as image recogni tion, similar issues were tackled by performing the label up date, not a priori but during training, by gradually tuning the estima tion [17]. Inspired by the achievements in image recognition [17], we propose a method to automatically tune the contribution of e ach data sample during training. We do this by alternately updat ing the parameters of a DNN emotion classiÔ¨Åcation model, and then us e the neural network prediction to correct the relative contr ibution and the target labels of each sample, in order to reduce the ov erall loss. The main purpose is to correct or ignore altogether the am biguously labeled utterances, while giving higher importa nce to the clear and unambiguous ones. Results obtained in the Interac tive Emotional Dyadic Motion Capture (IEMOCAP) dataset [18] sho w that our proposed method is effective in removing the annota tion noise. It achieves an improvement of 2.5% for weighted accur acy, and of 2.7% for unweighted accuracy compared to a stateoft heart BLSTM model trained on the original labels only [7]. 2. METHODOLOGY "
202,Image to Video Domain Adaptation Using Web Supervision.txt,"Training deep neural networks typically requires large amounts of labeled
data which may be scarce or expensive to obtain for a particular target domain.
As an alternative, we can leverage webly-supervised data (i.e. results from a
public search engine) which are relatively plentiful but may contain noisy
results. In this work, we propose a novel two-stage approach to learn a video
classifier using webly-supervised data. We argue that learning appearance
features and then temporal features sequentially, rather than simultaneously,
is an easier optimization for this task. We show this by first learning an
image model from web images, which is used to initialize and train a video
model. Our model applies domain adaptation to account for potential domain
shift present between the source domain (webly-supervised data) and target
domain and also accounts for noise by adding a novel attention component. We
report results competitive with state-of-the-art for webly-supervised
approaches on UCF-101 (while simplifying the training process) and also
evaluate on Kinetics for comparison.","Action recognition in videos is a wellstudied problem in computer vision with many important applications in ar eas such as surveillance, search, and humancomputer in teraction. Training deep neural networks typically requires a large labeled dataset. However, it may be difÔ¨Åcult to ob tain enough labeled data because it may be too scarce or too expensive to obtain. We can instead leverage webly supervised data (i.e. results from a public search engine) which are relatively plentiful but may be noisy. The highlevel overview of our model is shown in Fig ure 1. The noisy web image and web video domains are considered source domains that we want to domain adapt into the target domain. We present a twostage approach to Ô¨Årst learn an image model using a 2DCNN, transfer the learned spatial weights to a 3DCNN, and continue training a video model. Since our goal is to learn a video classiÔ¨Åer, we can potentially learn from web videos only, but we ar Noisy Web  Image  Noisy Web  Video Target Video 2D CNN  3D CNN Domain  Shift Domain Shift  Domain Shift Transfer Learned  Spatial Filters Figure 1: Given weblysupervised images and videos (source domains), we learn a video classiÔ¨Åer for the tar get domain. The model is learned in a twostage process by 1) learning an image model (2DCNN) and 2) transfer ring the spatial Ô¨Ålters to the video model (3DCNN) to con tinue training. The model also accounts for domain shift and noise present in the weblysupervised data. gue that our proposed twostage process is more appropriate for learning from noisy, weblysupervised data. Web videos are likely to be noisier than web images since web videos typically contain many frames that are irrelevant to the tar get concept. Thus it may be easier to learn spatial features Ô¨Årst, based on the relatively cleaner web images, and then learn the temporal features afterward. Previous work [25] has also hypothesized that it may be difÔ¨Åcult to learn both spatial and temporal features simultaneously. We present empirical results in Section 4 showing that our twostage process, which separates learning appearance and temporal features, outperforms a model that learns both jointly. In addition to the challenges of learning the appearance and motion, there are two additional issues with training on weblysupervised data. First, there is potential domain shift between the different domains. For example, compar ing web images and videos, many web images are typically highresolution and shot with highquality cameras, while web videos are typically lower resolution and may contain motion blur and other artifacts. Second, there may be noise present in weblysupervised data that may degrade perfor 1arXiv:1908.01449v1  [cs.CV]  5 Aug 201930  20  10  0 10 2030 20 10 0102030TSNE Plot (before DA) for BalanceBeam web image web video curated video 30  20  10  0 10 20 3020 10 0102030TSNE Plot (before DA) for LongJump web image web video curated video 20  10  0 10 20 3020 10 01020TSNE Plot (before DA) for Surfing web image web video curated video 30  20  10  0 10 20 3040 30 20 10 0102030TSNE Plot (before DA) for ThrowDiscus web image web video curated video 20  10  0 10 2020 10 0102030TSNE Plot (after DA) for BalanceBeam web image web video curated video 20  10  0 10 20 3020 10 01020TSNE Plot (after DA) for LongJump web image web video curated video 20  10  0 10 2020 10 01020TSNE Plot (after DA) for Surfing web image web video curated video 20  10  0 10 20 3030 20 10 0102030TSNE Plot (after DA) for ThrowDiscus web image web video curated videoFigure 2: TSNE Plots. We randomly sampled from the web image ( redpoints), web video ( green points) and target video (blue points) (UCF101 [23]) domains and show the TSNE [28] plots of 4 actions: balance beam, long jump, surÔ¨Ång, and throw discus. The Ô¨Årst row contains the TSNE plot before domain adaptation using pretrained RN34 [11] and the second row shows the same actions after the network has been domain adapted. Plot best viewed in color. mance. For example weblysupervised data may contain either the wrong concept entirely or a mix of relevant and irrelevant concepts (i.e. only a subset of frames in a video may correspond to the target concept). To account for domain shift, domain adaptation has been successfully used for tasks such as mapping from MNIST [14] to StreetView digits [27, 9], RGB to depth im ages [27] and webcam to product images [9]. In our work we incorporate an adversarial training component taken from Generative Adversarial Networks (GAN) [10]. To ac count for the noise present in weblysupervised data, we incorporate a novel attention component to reduce the ef fect of irrelevant examples, inspired by attention models for machine translation [1]. In this work, the target domain consists of curated videos, containing only a single concept or activity. We consider these curated videos to be a separate domain from web images and web videos. We assume there are relatively few irrelevant chunks from videos in the target domain com pared to web videos. For example, this setting may be ap propriate if the target domain was surveillance videos. To check whether there is indeed a difference be tween the separate domains, we extracted embeddings from random images/frames from each domain using ResNet 34 [11] and visualized TSNE [28] plots for four differ ent action categories from UCF101 [23]: Balance Beam, Long Jump, SurÔ¨Ång, Throw Discus. The top row corresponds to the embeddings before domain adaptation (DA) for curated video frames ( blue points), web video frames (green points), and web images ( redpoints). The bottom row corresponds to the embeddings after our DA (detail in Section 3). In the top row, before DA, there are visi bly distinct regions corresponding to the three domains of web images, web videos and curated videos (we used UCF 101 [23] videos), which may indicate domain differences. After DA, the different domains are packed closer together. To summarize, our contributions include: A novel twostage approach to Ô¨Årst learn spatial weights from a 2DCNN and then transfer these weights to a 3DCNN to learn temporal weights. A novel attention component to account for noise present in weblysupervised data. Results competitive with stateoftheart on UCF101 [23], while simplifying training. 2. Related Work "
572,Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting.txt,"Producing high-quality forecasts of key climate variables such as temperature
and precipitation on subseasonal time scales has long been a gap in operational
forecasting. Recent studies have shown promising results using machine learning
(ML) models to advance subseasonal forecasting (SSF), but several open
questions remain. First, several past approaches use the average of an ensemble
of physics-based forecasts as an input feature of these models. However,
ensemble forecasts contain information that can aid prediction beyond only the
ensemble mean. Second, past methods have focused on average performance,
whereas forecasts of extreme events are far more important for planning and
mitigation purposes. Third, climate forecasts correspond to a spatially-varying
collection of forecasts, and different methods account for spatial variability
in the response differently. Trade-offs between different approaches may be
mitigated with model stacking. This paper describes the application of a
variety of ML methods used to predict monthly average precipitation and two
meter temperature using physics-based predictions (ensemble forecasts) and
observational data such as relative humidity, pressure at sea level, or
geopotential height, two weeks in advance for the whole continental United
States. Regression, quantile regression, and tercile classification tasks using
linear models, random forests, convolutional neural networks, and stacked
models are considered. The proposed models outperform common baselines such as
historical averages (or quantiles) and ensemble averages (or quantiles). This
paper further includes an investigation of feature importance, trade-offs
between using the full ensemble or only the ensemble average, and different
modes of accounting for spatial variability.","Highquality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales, dened here as the time range between two weeks and two months, has long been a gap in operational forecasting [1]. Advances in weather forecasting on time scales of days to about a week [2, 3, 4, 5] or seasonal forecasts on time scales of two to nine months [6] do not translate to the challenging subseasonal regime. Skillful climate forecasts on subseasonal time scales would have immense value in agriculture, insurance, and economics. The importance of improved subseasonal predictions has been detailed in Ban et al. [1], Council [7]. The National Centers for Environmental Prediction (NCEP), part of the National Oceanic and Atmospheric Administration (NOAA), currently issues a \week 34 outlook"" for the continental U.S.1 The NCEP outlooks are constructed using a combination of dynamical and statistical forecasts, with statistical forecasts based largely on how the local climate in the past has varied (linearly) with indices of the El Ni~ noSouthern Oscillation (ENSO), MaddenJulian Oscillation (MJO), and global warming (i.e., the 30year trend). There exists great potential to advance subseasonal prediction using machine learning (ML) tech niques. A realtime forecasting competition called the Subseasonal Climate Forecast Rodeo [8], spon sored by the Bureau of Reclamation in partnership with NOAA, USGS, and the U.S. Army Corps of 1https://www.cpc.ncep.noaa.gov/products/predictions/WK34/ 1arXiv:2211.15856v1  [cs.LG]  29 Nov 2022Engineers, illustrated that teams using ML techniques can outperform forecasts from NOAA's opera tional seasonal forecast system. This paper focuses on developing MLbased forecasts that leverage ensembles of forecasts produced by NCEP in addition to observed data and other features. Past work, including successful methods in the Rodeo competition (e.g., Hwang et al. [9]), incorporated the ensemble average as a feature in their ML systems, but did not use any other information about the ensemble. In other words, variations among the ensemble members are not re ected in the training data or incorporated into the learned model. In contrast, this paper demonstrates that the full ensemble contains important information for subseasonal climate forecasting outside the ensemble mean. Specically, we consider the test case of predicting monthly 2meter temperatures and precipitation two weeks in advance over 3000 locations over the continental United States using physicsbased predictions, such as NCEPCFSv2 hindcasts [10, 11], using an ensemble of 24 distinct forecasts. We repeat this experiment for the Global Modeling and Assimilation Oce from the National Aeronautics and Space Administration (NASAGMAO) ensemble, which has 11 ensemble members [12]. In this context, this paper makes the following contributions: ‚Ä¢We train a variety of ML models (including neural networks, random forests, linear regres sion, and model stacking) that input all ensemble member predictions as features in addition to contemporaneous observations of geopotential heights, relative humidity, precipitation, and temperature from past months to produce new forecasts with higher accuracy than the ensemble mean; forecast accuracy is measured with a variety of metrics (Section 7). These models are considered in the context of regression, quantile regression, and tercile classication. Systematic experiments are used to characterize the in uence of individual ensemble members on predictive skill (Section 8.1). ‚Ä¢The collection of ML models employed allow us to consider dierent modes of accounting for spatial variability. ML models can account for spatial correlations among both features and responses; specically, when predicting Chicago precipitation, our models can leverage not only information about Chicago, but also about neighboring regions. Specically, we consider the following learning frameworks: (a) learning a predictive model for each spatial location indepen dently; (b) learning a predictive model that inputs the spatial location as a feature and hence can be applied to any single spatial location; (c) learning a predictive model for the full spa tial map of temperature or precipitation { i.e., predicting an outcome for all spatial locations simultaneously. Techniques such as positional encoding in ML models helps ensure that the right neighborhood information is used for each location, adapting to geographic features such as mountains or plains. In addition, ML models present a range of options for accounting for spatial variability, each with distinct advantages and disadvantages. Our application of model stacking allows our nal learned model to exploit the advantages of each method. ‚Ä¢We conduct a series of experiments to help explain the learned model and which features the model uses most to make its predictions. We systematically explore the impact of using lagged observational data in addition to ensemble forecasts and positional encoding to account for spatial variations (Section 8.3). ‚Ä¢The ensemble of forecasts from a physicsbased model (e.g., NCEPCFSv2 or NASAGMAO) contain information salient to precipitation and temperature forecasting besides their mean, and ML models that leverage the full ensemble generally outperform methods that rely on the ensemble mean alone (Section 8.1). ‚Ä¢Finally, we emphasize that the nal validation of our approach was conducted on data from 2011 to 2020 that was not used during any of the training, model development, parameter tuning, or model selection steps. We only conducted our nal assessment of predictive skill for 2011 to 2020 after we had completed all other aspects of this manuscript. Because of this, our nal empirical results accurately re ect the anticipated performance of our methods on new data. 22 Related work "
20,MGH: Metadata Guided Hypergraph Modeling for Unsupervised Person Re-identification.txt,"As a challenging task, unsupervised person ReID aims to match the same
identity with query images which does not require any labeled information. In
general, most existing approaches focus on the visual cues only, leaving
potentially valuable auxiliary metadata information (e.g., spatio-temporal
context) unexplored. In the real world, such metadata is normally available
alongside captured images, and thus plays an important role in separating
several hard ReID matches. With this motivation in mind, we
propose~\textbf{MGH}, a novel unsupervised person ReID approach that uses meta
information to construct a hypergraph for feature learning and label
refinement. In principle, the hypergraph is composed of camera-topology-aware
hyperedges, which can model the heterogeneous data correlations across cameras.
Taking advantage of label propagation on the hypergraph, the proposed approach
is able to effectively refine the ReID results, such as correcting the wrong
labels or smoothing the noisy labels. Given the refined results, We further
present a memory-based listwise loss to directly optimize the average precision
in an approximate manner. Extensive experiments on three benchmarks demonstrate
the effectiveness of the proposed approach against the state-of-the-art.","Given a person of interest for query, person reidentification(ReID) aims to search the same identities against gallery, it has been widely used in many realworld applications, such as video surveillance system, robotics, humancomputer interaction, etc. Due to the ex pensive annotation cost, recent researches focus on the unsuper vised person ReID, which requires no manual annotations. More over, ReID is generally carried out in a distributed camera network, where a wealth of auxiliary metadata (e.g. camera index, timestamp, etc.) is attached with the captured images. As shown in Figure 1, the metadata could provide auxiliary guidance to unsupervised person ReID [ 23,24,31,33,40,43,49,51,66,68], but how to adequatelyarXiv:2110.05886v1  [cs.CV]  12 Oct 2021GlobalDBSCANIntracameraDBSCANGlobalKNN(K=3)ùëí!ùëí""ùëí#ùëí$ùëí%ùëí&ùëí'HypergraphFigure 2: Illustration of hypergraph construction. We inves tigate different hyperedge construction strategies, i.e. KNN, clustering, cameraaware clustering. Global Clustering: per form global clustering and group the instances in a cluster as a hyperedge. Intracamera Clustering: camera informa tion is combined with clustering to perform intracamera Clustering. Global KNN: given a vertex (i.e., centroid), a hy peredge connects itself and its nearest neighbors. utilize such heterogeneous structure in unsupervised ReID is still an open problem. Recently, unsupervised person ReID approaches [ 9,12,60] follow clusteringandfinetune pipeline, which iteratively assigns pseudo labels for data then trains the feature extractor with the pseudo labels. In practice, since the ReID image data is generated from a distributed camera network, the correlation among images is usually diverse, heterogeneous, and complicated. Therefore, the relations are various, such as visual connections and camera con nections, which causes the unreliable pseudo labels in unsupervised person ReID. To solve this issue, a hypergraph representation [ 16] is an appropriate tool to model the multimodal and heterogeneous data correlation by means of a variety of hyperedges [ 10]. Due to the heterogeneous information in the metadata, the relation ship among instances are varied from the different viewpoints. To address this issuse, we propose three kinds of hypergraph construc tion techniques: ‚ÄúGlobal Clustering‚Äù, ‚ÄúIntracamera Clustering‚Äù, and ‚ÄúGlobal KNN‚Äù. As illustrated in Figure 2, ‚ÄúGlobal Clustering‚Äù means performing DBSCAN on all data and then grouping the in stances in a cluster as a hyperedge, this strategy aims to capture the global densitybased mode in the data. ‚ÄúIntracamera Clustering‚Äù considers the camera information and performs DBSCAN under an individual camera, this strategy concentrates on capture the cameraconditioned marginal density mode. ‚ÄúGlobal KNN‚Äù stands for building a hyperedge by connecting one vertex with its ùêænearest neighbors, this strategy focuses on local neighborhood to capture the smoothness locally. By means of merging several hyperedges created by different approaches, the hypergraph is able to perceivethe camera topology and model the heterogeneous data correla tion across cameras. With this hypergraph, we can generate more reliable pseudo labels in learning process. Given the generated pseudo labels, the ReID model could be finetuned iteratively. Several approaches related to mutual learn ing [ 3,11,44,58] and memorybased contrastive loss [ 43,68] are proposed to learn the robust feature representation with noisy labels. Although these proposed classification losses are verified to be effective, the end task such as average precision is not di rectly optimized in these approaches. To help address this issue, we propose a listwise loss combined with an instancelevel memory, which provides a finegrained supervision by directly optimize av erage precision in an approximate manner. Besides, we combine the proposed listwise loss and cameraaware contrastive loss as coarsetofine supervision for model updating. Based on the motivation above, we propose a novel method termed Metadata Guided Hypergraph (MGH) to mutually guide the process of label refinement and feature learning. Specifically, we construct a heterogeneous hypergraph based on the metadata to generate pseudo labels, and utilize coarsetofine memorybased supervision to update the model. To achieve this, we first generate the noisy pseudo labels by clustering the visual feature represen tations of all unlabeled images. Then utilize hypergraph for label refinement. Specifically, we construct the hyperedges based on a joint similarity matrix [ 40] by considering visual information and spatiotemporal context simultaneously. Hyperedges are grouped to generate a hypergraph, which models the complicated highorder relationships among the data. Then, we perform label propaga tion on the hypergraph structure, which rectifies the label errors caused by the previous clustering algorithm. For model updating, the instancelevel memory and cameraaware prototype memory are constructed to simultaneously capture local and global distribu tion. We propose to use a listwise loss based on the instancelevel memory, which is combined with a cameraaware contrastive loss to provide coarsetofine supervision. In summary, our main contributions are threefold as follows: ‚Ä¢We propose a heterogeneous hypergraph to model the com plicated data correlation among the metadata, which facil itates the unsupervised person ReID in the realworld sce nario. ‚Ä¢We propose a novel unsupervised person ReID model named MGH , which consists of label generation with hypergraph and model updating through memorybased coarsetofine supervision. ‚Ä¢On three public person ReID benchmarks with readily avail able metadata, i.e. camera index and timestamp, our proposed method outperforms the stateoftheart approaches. 2 RELATED WORK "
432,Bootstrapping the Relationship Between Images and Their Clean and Noisy Labels.txt,"Many state-of-the-art noisy-label learning methods rely on learning
mechanisms that estimate the samples' clean labels during training and discard
their original noisy labels. However, this approach prevents the learning of
the relationship between images, noisy labels and clean labels, which has been
shown to be useful when dealing with instance-dependent label noise problems.
Furthermore, methods that do aim to learn this relationship require cleanly
annotated subsets of data, as well as distillation or multi-faceted models for
training. In this paper, we propose a new training algorithm that relies on a
simple model to learn the relationship between clean and noisy labels without
the need for a cleanly labelled subset of data. Our algorithm follows a 3-stage
process, namely: 1) self-supervised pre-training followed by an early-stopping
training of the classifier to confidently predict clean labels for a subset of
the training set; 2) use the clean set from stage (1) to bootstrap the
relationship between images, noisy labels and clean labels, which we exploit
for effective relabelling of the remaining training set using semi-supervised
learning; and 3) supervised training of the classifier with all relabelled
samples from stage (2). By learning this relationship, we achieve
state-of-the-art performance in asymmetric and instance-dependent label noise
problems.","Supervised deep learning has had great success gen erating effective classification models from sets of la belled training data [24, 26]. Modern deep learning mod els require largescale datasets to achieve stateofthe art (SOTA) results [38, 39]. However, realworld large 1Supported by Australian Research Council through grants DP180103232 and FT190100525.scale datasets, such as those collected from search en gines or available from hospitals and clinics, tend to have a nonnegligible amount of instancedependent label noise (IDN) [32, 53]. Existing methods often attempt to address instanceindependent label noise (IIN), such as symmetric or asymmetric noise [15, 58, 65]. Handling the IDN present in largescale realworld datasets has become one of the main research problems in the field. When naively trained with noisylabelled data, deep learning models generalise poorly because they can easily overfit the incorrectly labelled samples [62]. Many meth ods have been developed for handling label noise, with SOTA approaches relying on sample relabelling mecha nisms. These strategies are based on techniques to estimate the relationship between images and clean labels, and after relabelling, the old noisy labels are discarded [29, 47, 65]. However, to model how different image features and noisy labels affect the mislabelling process in IDN, we need to estimate the relationship between images, clean labels and noisy labels [17, 65]. Some methods have attempted to model this relationship with noisetransition matrices and corrective layers for asymmetric noise [15, 37, 57] or part dependant noise in place of instancedependant noise [56], but they failed to achieve SOTA results. Rather than the usual noisylabel learning setting, where a set of cleanly annotated samples is not available, some methods assume the existence of a subset of training data containing images, clean labels and noisy labels [17, 21, 52]. By training a model that predicts clean labels from both images and noisy labels (see right of Figure 1), these meth ods are able to learn the relationship between image fea tures, noisy labels and clean labels, allowing them to model IDN and more effectively relabel noisy samples. However, it can be expensive, difficult and timeconsuming to obtain a clean subset of data with noisy labels and clean labels that is representative of the instancedependant noise in the dataset. Furthermore, these methods require distillation to a more standard model (such as the one on the left of Fig ure 1) for evaluation on samples without labels. 1arXiv:2210.08826v1  [cs.CV]  17 Oct 2022Figure 1. On the left, we show a ‚Äònormal‚Äô deep neural network model used for noisy label learning tasks. On the right, we present a ‚Äòmodified‚Äô model that can learn the relationship between images x, noisy labels Àúyand clean labels y, similar to those used by meth ods that have access to a clean set of data [17, 21, 52]. In this paper, we introduce a new algorithm to learn the relationship between images and their clean and noisy la bels without using any cleanlabel set. Our algorithm fol lows a 3stage process (see Fig. 2): 1) Bootstrapping: self supervised pretraining followed by an earlystopping train ing [62] of the classifier that receives images and ‚Äònull‚Äô la bels as input and predicts the noisy labels as output ‚Äì this stage forms a subset of predicted clean labels for the second stage of training; 2) Semisupervised Learning: use this pre dicted clean subset to learn the relationship between images, noisy labels and clean labels, which we exploit for an effec tive, explicit relabelling of the remaining training set; and 3)Final training: supervised training of the classifier using the relabelled samples. The main contributions of this paper are: ‚Ä¢ An effective threestage training algorithm designed to address instancedependent label noise by learning the relationship between images and their clean and noisy labels ‚Äì using a noisetransition sample balanc ing scheme, explicitly relabelling training samples and without requiring a cleanly annotated training set; ‚Ä¢ A method that reaches SOTA asymmetric and instance dependent label noise results using a simple single model architecture, unlike DivideMix [29] (and its derivatives such as [11, 22, 35, 42, 68]) that require a more complex 2model architecture. ‚Ä¢ A ‚Äòlabel dropping‚Äô strategy that removes the need for distillation to a standard model and allows predictions to be made on samples with andwithout noisy labels;2. Related Work "
614,NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing.txt,"Large-scale datasets in the real world inevitably involve label noise. Deep
models can gradually overfit noisy labels and thus degrade model
generalization. To mitigate the effects of label noise, learning with noisy
labels (LNL) methods are designed to achieve better generalization performance.
Due to the lack of suitable datasets, previous studies have frequently employed
synthetic label noise to mimic real-world label noise. However, synthetic noise
is not instance-dependent, making this approximation not always effective in
practice. Recent research has proposed benchmarks for learning with real-world
noisy labels. However, the noise sources within may be single or fuzzy, making
benchmarks different from data with heterogeneous label noises in the real
world. To tackle these issues, we contribute NoisywikiHow, the largest NLP
benchmark built with minimal supervision. Specifically, inspired by human
cognition, we explicitly construct multiple sources of label noise to imitate
human errors throughout the annotation, replicating real-world noise, whose
corruption is affected by both ground-truth labels and instances. Moreover, we
provide a variety of noise levels to support controlled experiments on noisy
data, enabling us to evaluate LNL methods systematically and comprehensively.
After that, we conduct extensive multi-dimensional experiments on a broad range
of LNL methods, obtaining new and intriguing findings.","Largescale labeled data has become indispensable in the notable success of deep neural networks (DNNs) in various domains and tasks (Russakovsky et al., 2015; Wang et al., 2019). Due to imper fect sources like crowdsourcing and web crawl ing (Xiao et al., 2015; Zhang et al., 2017b; Lee Corresponding author. 1The dataset is publicly available at https://github. com/tangminji/NoisywikiHow .Input Output (a) Take prescription weight loss medications. Losing Weight(b) Check calories on food packaging. (c) Include cultural and ethnic foods in your plan. (d) Talk about food differently. Table 1: Instances (a)‚Äì(d) depict examples of our task. Input : a procedural event. Output : a plausible inten tion toward that event. et al., 2018), datasets frequently include realworld label noise (Chen et al., 2021), which may induce model overÔ¨Åtting to noisy labels and hurt the gener alization of deep models (Zhang et al., 2017a; Wu et al., 2022a,b). To alleviate this issue, learning with noisy labels (LNL) methods for robustly train ing deep models have been studied extensively. Due to the lack of appropriate benchmarks, pre vious research often studied synthetic label noise to simulate realworld label noise (Zhang et al., 2018; Lukasik et al., 2020). As a general and real istic noise, realworld noise may have several noise sources (i.e., be heterogeneous ) (Northcutt et al., 2021) and be instancedependent (i.e.,P(~yjy;x), where the probability of an instance being assigned to the incorrect label ~ydepends on the original groundtruth label yand datax) (Han et al., 2021; Song et al., 2022). However, synthetic noise is generated from an artiÔ¨Åcial distribution and is thus instanceindependent (i.e.,P(~yjy)), which may not always work well in practice. Recently, various benchmarks for learning with realworld noisy labels have been proposed across Ô¨Åelds like computer vision (CV) (Li et al., 2017), audio signal processing (ASP) (Gemmeke et al., 2017), and natural language processing (NLP) (Hedderich et al., 2021). To fully evalu ate robust learning methods with realworld label noise, benchmarks should be as close to realworld scenarios as possible. Meanwhile, controlled exarXiv:2305.10709v1  [cs.CL]  18 May 2023periments are encouraged to verify whether LNL methods can remain effective over a wide range of noise levels (Jiang et al., 2020). Nevertheless, the noise levels in most datasets are Ô¨Åxed and unknown, resulting in uncontrolled label noise (Fonseca et al., 2019a; Song et al., 2019). Moreover, the noise therein often comes from the same or ambiguous sources (Li et al., 2017; Jiang et al., 2020), which conÔ¨Çicts with the heterogeneous characteristics of realworld noise. These problems prevent a better understanding of LNL methods. To bridge this gap, we present NoisywikiHow, a new NLP benchmark for evaluating LNL meth ods focusing on the intention identiÔ¨Åcation task. Intention identiÔ¨Åcation promotes numerous down stream natural language understanding tasks, from commonsense reasoning (Sap et al., 2019) to di alogue systems (Pepe et al., 2022). Additionally, the complexity of the task (total of 158 categories) facilitates a deeper investigation of the efÔ¨Åcacy of LNL approaches. The task form is shown in Ta ble 1. To make the benchmark more representative of realworld scenarios, we propose a practical as sumption: Realworld label noise in a dataset is mainly induced by human errors, regardless of whether the dataset‚Äôs construction is automated or crowdsourced. Existing psychological and cog nitive evidence further supports our hypothesis. It shows that different annotators have different pref erences and biases (Beigman and Klebanov, 2009; Burghardt et al., 2018), which means human la beling errors typically result from multiple noise sources. Furthermore, humans may make random labeling errors due to random attention slips. But they are more likely to produce label noise when labeling hard cases (Klebanov et al., 2008) (i.e., noise is instancedependent), such as instance (c) in Table 1. Motivated by this human cognition, we Ô¨Årst col lect data from the wikiHow website,2which con tains a collection of professionally edited howto guideline articles, providing a vast quantity of clean scripts and corresponding categories for free to help achieve controlled experiments and ensure benchmark quality. After that, we explicitly in ject a variety of noise sources into clean data to replicate human annotation errors, thus introducing realworld label noise into the benchmark. Notably, training samples in our benchmark exhibit a long 2https://www.wikihow.comtailed class distribution, which is in line with the facts, i.e., data in realworld applications is heav ily imbalanced (Van Horn et al., 2018; Liu et al., 2019b). Besides, we achieve minimal human su pervision by using a series of automated labeling procedures, saving lots of time and human effort. To evaluate NoisywikiHow, we carry out exten sive experimentation across various model archi tectures and noise sources, execute plentiful LNL methods on our benchmark, compare the more real istic realworld noise with the extensively studied synthetic noise, and investigate a case study and longtailed distribution characteristics. 2 Related Work "
465,Scene Labeling using Gated Recurrent Units with Explicit Long Range Conditioning.txt,"Recurrent neural network (RNN), as a powerful contextual dependency modeling
framework, has been widely applied to scene labeling problems. However, this
work shows that directly applying traditional RNN architectures, which unfolds
a 2D lattice grid into a sequence, is not sufficient to model structure
dependencies in images due to the ""impact vanishing"" problem. First, we give an
empirical analysis about the ""impact vanishing"" problem. Then, a new RNN unit
named Recurrent Neural Network with explicit long range conditioning (RNN-ELC)
is designed to alleviate this problem. A novel neural network architecture is
built for scene labeling tasks where one of the variants of the new RNN unit,
Gated Recurrent Unit with Explicit Long-range Conditioning (GRU-ELC), is used
to model multi scale contextual dependencies in images. We validate the use of
GRU-ELC units with state-of-the-art performance on three standard scene
labeling datasets. Comprehensive experiments demonstrate that the new GRU-ELC
unit benefits scene labeling problem a lot as it can encode longer contextual
dependencies in images more effectively than traditional RNN units.","Scene labeling is a fundamental task in computer vision. Its goal is to assign one of many predeÔ¨Åned category la Figure 1: CNNs have challenges in dealing with local tex tures in images as shown in the second row. With the help of Gated Recurrent Units, the model can make globally better prediction. However, GRUs still struggle in modeling Ô¨Åne structures in images due to the ‚Äúimpact vanishing‚Äù problem. Our GRUELC units can effectively model multi scale con textual dependencies in images and thus successfully pre serve local details in predictions, such as the windows and doors (even not annotated in ground truth) in Ô¨Årst row, and the trees and mountains in the second and third row. 1arXiv:1611.07485v2  [cs.CV]  28 Mar 2017bels to each pixel in an image. It is usually formulated as a pixelwise multiclass classiÔ¨Åcation problem. Mod ern scene labeling methods rely heavily on convolutional neural networks (CNNs) [7, 30, 49, 29]. CNNs are capa ble of learning scaleinvariant discriminative features for images. These features have proven more powerful than traditional handcrafted features on many computer vision tasks [20, 35, 30]. Specially designed CNN architectures [7, 30, 1, 48, 49] have shown superior performance on scene labeling by using endtoend training. However, CNNs have challenges in dealing with local textures and Ô¨Åne structures in images and tend to over segment or undersegment objects in images. To accurately segment small components and detect object boundaries, long range contextual dependencies in images are usually desired when designing scene labeling algorithm. Many works have exploited probabilistic graphical models such as conditional random Ô¨Åelds (CRFs) [19] to capture structural dependencies in images [49, 29]. However, CRFs usually require carefully designed potentials and their exact infer ence is usually intractable. In contrast, recurrent neural net works (RNN), as another category of powerful contextual dependency modeling methods, are free from these disad vantages and can learn contextual dependencies in a data driven manner. RNNs have Ô¨Årst proven effective in modeling data de pendencies in domains such as natural language process ing and speech recognition [11, 33]. Recently, there are some attempts at applying RNNs to images [4, 44, 38, 24]. Because the spatial relationship among pixels in 2D image data is fundamentally different from the temporal relation ship in the 1D data in NLP or speech recognition, variants of RNN architectures [17, 38, 44, 42, 12, 25, 41] have been proposed to handle 2D image data, which typically involve unfolding a 2D lattice grid into a 1D sequence. The un folded 1D sequence is usually much longer than the data sequence in NLP or speech recognition. Take a feature map of size 6464for example. Its unfolded 1D sequence is of length 6464 = 4096 . One major Ô¨Çaw of applying exist ing RNN units to sequences of such a long length is that the ‚Äúimpact vanishing‚Äù problem will raise and break the spatial dependencies in images. It is wellknown that long term dependency is hard to learn in RNN units due to the gradient exploding or van ishing problems and RNN units such as Long Short Term Memory (LSTM) or Gated Recurrent Unit (GRU) can ef fectively avoid these problems. However, in this work, we empirically show that even in LSTMs or GRUs, the depen dency in a extremely long range is still hard to capture due to the ‚Äúimpact vanishing‚Äù problem. First, this paper studies the ‚Äúimpact vanishing‚Äù problem when traditional RNNs are applied to image data. Then, we generalize traditional RNN units to RNN units with Explicit Longrange Conditioning (RNNELC) to overcome this problem. SpeciÔ¨Åcally, two variants, GRUELC and LSTMELC, are designed and discussed. Compared with existing works [17, 38, 44, 42, 12, 25, 41], RNNELCs can effectively capture long range con textual dependency in images with the help of their ex plicit long range conditioning architecture. In the RNN ELC units, the present variable is explicitly conditioned on multiple contextually related but sequentially distant vari ables. Intuitively, it is adding skip connection between hid den states. Adding skip connections in RNNs to help learn long term dependency Ô¨Årst appears in [26]. Our method generalizes the idea to model more complex 2D spatial de pendencies. The RNNELC unit is applicable to both raw pixels and image features. It can be naturally integrated in CNNs, thereby enabling joint endtoend training. In order to take the beneÔ¨Åts of the new RNN units for scene labeling tasks, we build a novel scene labeling system using the GRUELC units to model long range multiscale contextual dependencies in image features. In summary, our main contributions include: 1) An em pirical study of the ‚Äúimpact vanishing‚Äù problem, which commonly exists in traditional RNN units when they are applied to images; 2) A new RNN unit with Explicit Longrange Conditioning to alleviate the ‚Äúimpact vanish ing‚Äù problem; 3) A novel scene labeling algorithm based on GRUELCs. There are a few works utilizing GRUs for scene labeling. However, we show that our GRUELC units can actually achieve stateoftheart performances in scene labeling tasks; and 4) Improved performances on several standard scene labeling datasets. 2. Related Work "
441,The Neurally-Guided Shape Parser: Grammar-based Labeling of 3D Shape Regions with Approximate Inference.txt,"We propose the Neurally-Guided Shape Parser (NGSP), a method that learns how
to assign fine-grained semantic labels to regions of a 3D shape. NGSP solves
this problem via MAP inference, modeling the posterior probability of a label
assignment conditioned on an input shape with a learned likelihood function. To
make this search tractable, NGSP employs a neural guide network that learns to
approximate the posterior. NGSP finds high-probability label assignments by
first sampling proposals with the guide network and then evaluating each
proposal under the full likelihood. We evaluate NGSP on the task of
fine-grained semantic segmentation of manufactured 3D shapes from PartNet,
where shapes have been decomposed into regions that correspond to part instance
over-segmentations. We find that NGSP delivers significant performance
improvements over comparison methods that (i) use regions to group per-point
predictions, (ii) use regions as a self-supervisory signal or (iii) assign
labels to regions under alternative formulations. Further, we show that NGSP
maintains strong performance even with limited labeled data or noisy input
shape regions. Finally, we demonstrate that NGSP can be directly applied to CAD
shapes found in online repositories and validate its effectiveness with a
perceptual study.","The ability to semantically segment 3D shapes is im portant for numerous applications in vision, graphics, and robotics: reverseengineering the part structure of an object to support editing and manipulation; producing training data for structureaware generative shape models [13, 18, 28]; helping autonomous agents understand how to interact with objects in their environment [1]; and more. These appli cations often demand that the parts detected be finescale (e.g. wheels of an office chair) and hierarchicallyorganized (e.g. a cabinet door decomposes into a handle, door, and frame). Producing such segmentations has proved to be achallenging task, as it is expensive to gather large amounts of data at this granularity; PartNet [29] is the only existing largescale dataset of this type. Recent work on 3D shape semantic segmentation has mainly focused on endtoend approaches that operate on shape atoms (e.g. mesh faces, point cloud points, occupancy grid voxels), i.e. the lowestlevel geometric entity in the input representation [15, 33, 34, 44]. While these methods achieve impressive performance on many tasks, they do not often transfer well to domains with finegrained labels or when access to labeled data is limited. We postulate that one reason for this phenomenon is that attempting to label shape atoms directly results in a massive search space, allowing learningbased methods to overfit unless the ratio of labeled shape instances to the label set complexity is high. One way to address this issue is to design systems that make use of shape regions . When the number of shape regions becomes significantly smaller than the number of shape atoms , the label assignment problem becomes easier. Such a framing may allow methods to learn finegrained semantic segmentation when access to labeled data is limited. When shape regions are provided, they can be used in various ways: (i) as a postprocess aggregation on top of shape atom predictions, (ii) to formulate auxiliary selfsupervised objectives, or (iii) as the object to be labeled. Methods that operate within this last paradigm can more directly reason about relationships between regions, which can help improve finegrained segmentation performance by better considering the context of a region within the entire shape. The problem of decomposing a shape into regions useful for semantic segmentation is applicationdependent. For CAD shapes and scenes found in online repositories, this type of region decomposition is often produced as a by product of the modeling process, e.g. each part instance will be made out of one or more connected mesh compo nents [26, 39, 49]. Discovering region decompositions for shapes that do not already provide them is a wellstudied problem within computer vision and graphics. There has been considerable recent effort on unsupervised techniques that approximate 3D shapes with primitives [9,21,31,37,38], 1arXiv:2106.12026v3  [cs.CV]  22 Mar 2022and there is a long history of research on shape segmentation through purely geometric analysis [3, 19, 41]. There is even reason to believe that region decomposition solutions can generalize across shape categories, i.e. the way that shapes (especially manufactured objects) decompose into parts is largely categoryindependent [14, 50]. In this paper, we propose the NeurallyGuided Shape Parser (NGSP), a method that learns to assign finegrained labels from a semantic grammar to regions of a 3D shape. Our approach is based on maximum a posteriori (MAP) in ference in a model of the probability that a label assignment to the shape‚Äôs regions is correct. Our likelihood consists of a mixture of modules that each operate on some regions of the shape. One set of modules evaluates the validity of the implied geometry and spatial layout for each label in the semantic grammar. Another module evaluates groups of re gions formed by the label assignment. As this combinatorial search problem is too complex to solve with exhaustive enu meration, we employ a neural guide network to approximate the posterior. The guide network reasons locally, predicting the label probability for each region independently. Using the perregion probabilities produced by the guide network, NGSP importance samples a set of proposed label assign ments. To choose the best proposal out of this set, each label assignment is evaluated under the full likelihood, and the sample with highest posterior probability is chosen. We compare NGSP against methods that use shape re gions as a postprocess, a selfsupervisory signal, or assign labels to regions with different search strategies and likeli hood formulations. We evaluate each method on the task of finegrained semantic segmentation of manufactured 3D shapes from PartNet, where each method has access to re gions from the annotated part instance oversegmentations (e.g. each semantic part instance may consist of multiple regions). NGSP achieves the best semantic segmentation performance, even in paradigms where access to labeled data is limited or when the input shape regions are noisy. To validate our design decisions, we run an ablation study measuring the effect of each likelihood term and the neural guide network. Finally, we show that NGSP can find good semantic segmentations on ‚Äòin the wild‚Äô CAD shapes found from online repositories, and evaluate its performance with a forced choice perceptual study against comparison methods. Code for our method and experiments can be found at found at https://github.com/rkjones4/NGSP . In summary, our contributions are: (i)We present the NeurallyGuided Shape Parser (NGSP), a method that learns how to assign labels from a seman tic grammar to regions of a 3D shape. NGSP performs approximate MAP inference, using a guide network to find highprobability label assignments under a learned posterior probability of a label assignment conditioned on an input shape.(ii)We demonstrate that NGSP finds better finegrained semantic segmentations for manufactured shapes com pared with methods that use shape regions in alterna tive learning paradigms. 2. Related Work "
463,DeepUSPS: Deep Robust Unsupervised Saliency Prediction With Self-Supervision.txt,"Deep neural network (DNN) based salient object detection in images based on
high-quality labels is expensive. Alternative unsupervised approaches rely on
careful selection of multiple handcrafted saliency methods to generate noisy
pseudo-ground-truth labels. In this work, we propose a two-stage mechanism for
robust unsupervised object saliency prediction, where the first stage involves
refinement of the noisy pseudo labels generated from different handcrafted
methods. Each handcrafted method is substituted by a deep network that learns
to generate the pseudo labels. These labels are refined incrementally in
multiple iterations via our proposed self-supervision technique. In the second
stage, the refined labels produced from multiple networks representing multiple
saliency methods are used to train the actual saliency detection network. We
show that this self-learning procedure outperforms all the existing
unsupervised methods over different datasets. Results are even comparable to
those of fully-supervised state-of-the-art approaches. The code is available at
https://tinyurl.com/wtlhgo3 .","Object saliency prediction aims at Ô¨Ånding and segmenting generic objects of interest and help leverage unlabeled information contained in a scene. It can contribute to binary background/foreground segmentation, image caption generation (Show, 2015), semantic segmentation (Long et al., 2015), or object removal in scene editing (Shetty et al., 2018). In semantic segmentation, for example, the network trained on a Ô¨Åxed set of class labels can only identify objects belonging to these classes, while object saliency detection can highlight an unknown object (e.g., ""bear"" crossing a street). Existing techniques on the saliency prediction task primarily fall under supervised and unsupervised settings. The line of work of supervised approaches (Hou et al., 2017; Luo et al., 2017; Zhang et al., 2017b,c; Wang et al., 2017; Li et al., 2016; Wang et al., 2016; Zhao et al., 2015; Jiang et al., 2013b; Zhu et al., 2014) however, requires largescale clean and pixellevel humanannotated datasets, which are expensive and timeconsuming to acquire. Unsupervised saliency methods do not require any human annotations and can work in the wild on arbitrary datasets. These unsupervised methods are further categorized into traditional handcrafted salient object detectors (Jiang et al., 2013b; Zhu et al., 2014; Li et al., 2013; Jiang et al., 2013a; Zou & Komodakis, 2015) and DNNbased detectors (Zhang et al., 2018, 2017a). These traditional methods are based on speciÔ¨Åc priors, such as center priors (Goferman et al., 2011), global contrast prior (Cheng et al., 2014), and background connectivity assumption (Zhu et al., 2014). Despite their simplicity, these methods perform poorly due to the limited coverage of the handpicked priors. DNNbased approaches leverage the noisy pseudolabel outputs of multiple traditional handcrafted saliency models to provide a supervisory signal for training the saliency prediction network. Zhang et al. (2017a) proposes a method (SBF, ‚ÄôSupervision by fusion‚Äô) to fuse multiple saliency models to remove noise from the pseudogroundtruth labels. This method updates the pseudolabels with the predictions of the saliency detection network and yields very noisy saliency predictions, as shown in Fig. 7c. A slightly different approach (USD, ‚ÄôDeep unsupervised saliency detection‚Äô) is taken by Zhang et al. (2018) and introduces an explicit noise modeling module to capture the noise in pseudolabels of different handcrafted methods. The joint optimization, along with the noise module, enables the learning of the saliencyprediction network to generate the pseudonoisefree outputs. It does so by Ô¨Åtting different noise estimates on predicted saliency map, based on different noisy pseudogroundtruth labels. This method produces smooth predictions of salient objects, as seen in Fig. 7c since it employs a noise modeling module to counteract the inÔ¨Çuence of noise in pseudogroundtruth labels from handcrafted saliency models. Both DNNbased methods SBF and USD performs direct pseudo labels fusion on the noisy outputs of handcrafted methods. This implies that the poorquality pseudolabels are directly used for training saliency network. Hence, the Ô¨Ånal performance of the network primarily depends upon the quality of chosen handcrafted methods. On the contrary, a better way is to reÔ¨Åne the poor pseudolabels in isolation in order to maximize the strength of each method. The Ô¨Ånal pseudolabels fusion step 2Figure 3: Overview of the sequence of steps involved in our pipeline. Firstly, the training images are processed through different handcrafted methods to generate coarse pseudolabels. In the second step, which we refer to as interimages consistency , a deep network is learned from the training images and coarse pseudolabels to generate consistent label outputs, as shown in Fig. 2. In the next step, the label outputs are further reÔ¨Åned with our selfsupervision technique in an iterate manner. Lastly, the reÔ¨Åned labels from different handcrafted methods are fused for training the saliency prediction network. Details of the individual components in the pipeline are depicted in Fig. 4. to train a network should be performed on a set of diverse and highquality, reÔ¨Åned pseudolabels instead. More concretely, we propose a systematic curriculum to incrementally reÔ¨Åne the pseudolabels by substituting each handcrafted method with a deep neural network. The handcrafted methods operate on single image priors and do not infer highlevel information such as object shapes and perspective projections. Instead, we learn a function or proxy for the handcrafted saliency method that maps the raw images to pseudolabels. In other words, we train a deep network to generate the pseudolabels which beneÔ¨Åts from learning the representations across a broad set of training images and thus signiÔ¨Åcantly improve the pseudogroundtruth labels as seen in Fig. 2 (we refer this effect as interimages consistency). We further reÔ¨Åne our pseudolabels obtained after the process of interimage consistency to clear the remaining noise in the labels via the selfsupervision technique in an iterative manner. Instead of using pseudolabels from the handcrafted methods directly as Zhang et al. (2018, 2017a), we alleviate the weaknesses of each handcrafted method individually. By doing so, the diversity of pseudolabels from different methods is preserved until the Ô¨Ånal step when all reÔ¨Åned pseudolabels are fused. The large diversity reduces the overÔ¨Åtting of the network to the labels noise and results in better generalization capability. The complete schematic overview of our approach is illustrated in Fig. 3. As seen in the Ô¨Ågure, the training images are Ô¨Årst processed by different handcrafted methods to create coarse pseudolabels. In the second step, we train a deep network to predict the pseudolabels (Fig. 4a) of the corresponding handcrafted method using a imagelevel loss to enforce interimages consistency among the predic tions. As seen in Fig. 2, this step already improves the pseudolabels over handcrafted methods. In the next step, we employ an iterative selfsupervision technique (Fig. 4c) that uses historical moving averages (MV A), which acts as an ensemble of various historical models during training (Fig. 4b) to reÔ¨Åne the generated pseudolabels further incrementally. The described pipeline is performed for each handcrafted method individually. In the Ô¨Ånal step, the saliency prediction network is trained to predict the reÔ¨Åned pseudolabels obtained from multiple saliency methods using a mean imagelevel loss. Our contribution in this work is outlined as follows: we propose a novel systematic mechanism to reÔ¨Åne the pseudogroundtruth labels of handcrafted unsupervised saliency methods iteratively via selfsupervision. Our experiments show that this improved supervisory signal enhances the training process of the saliency prediction network. We show that our approach improves the saliency prediction results, outperforms previous unsupervised methods, and is comparable to supervised methods on multiple datasets. Since we use the reÔ¨Åned pseudolabels, the training behavior of the saliency prediction network largely resembles supervised training. Hence, the network has a more stable training process compared to existing unsupervised learning approaches. 2 Related work "
588,A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels.txt,"Learning with noisy labels is an important topic for scalable training in
many real-world scenarios. However, few previous research considers this
problem in the online setting, where the arrival of data is streaming. In this
paper, we propose a novel gradient-based approach to enable the detection of
noisy labels for the online learning of model parameters, named Online
Gradient-based Robust Selection (OGRS). In contrast to the previous sample
selection approach for the offline training that requires the estimation of a
clean ratio of the dataset before each epoch of training, OGRS can
automatically select clean samples by steps of gradient update from datasets
with varying clean ratios without changing the parameter setting. During the
training process, the OGRS method selects clean samples at each iteration and
feeds the selected sample to incrementally update the model parameters. We
provide a detailed theoretical analysis to demonstrate data selection process
is converging to the low-loss region of the sample space, by introducing and
proving the sub-linear local Lagrangian regret of the non-convex constrained
optimization problem. Experimental results show that it outperforms
state-of-the-art methods in different settings.","Online learning is a widely used learning framework for streaming data in many realworld scenarios. In recent years, online training of deep neural networks (DNNs) has garnered increased attention to enable largescale training [1 ‚Äì3], to face the challenge of increasingly large datasets. Such a largescale training process of DNNs, especially online DNNs training, is highly sensitive to the noisy labels in the datasets [4], which is more pronounced with the streaming and dynamically changing online data. The noisy label problem refers to the presence of incorrect or mislabeled annotations in a training dataset. Usually, the data samples with incorrect labels are defined as noisy data, and the correct one is called clean data. This issue has been identified as a common challenge in many datasets. For instance, researchers in [5] found 6% label errors in the Imagenet validation set and 10% label errors in the QuickDraw dataset. Similarly, up to 30% label errors were found in the Google Emotions Preprint. Under review.arXiv:2306.05046v1  [cs.LG]  8 Jun 2023dataset [6] and 37% errors in the MS COCO dataset [7]. Label errors vary across different datasets and appear with varying probabilities of occurrence in data streams at different time slots. In recent years, the robustness issue of training with noisy labels has been widely studied in different research areas [4, 8, 9]. Among various approaches, sample selection methods enjoy the flexibility to support any type of deep learning architecture and do not need to maintain additional neural networks. The concept of multiround sample selection for scalable models was first introduced in [10], where the authors proposed an iterative training loss minimization (ITLM) method that leverages samples selected at the beginning of each training epoch. Building on this idea, INCV [11] employs crossvalidation to detect noisy training data and remove largeloss samples. O2UNet [12] first repeats the entire training process to collect loss statistics, then retrains the neural network from scratch only with clean samples detected. These works proposed different methods to estimate the ratio of a dataset and using sorting methods to filter out noisy data based on that ratio, but they all follow the same idea of detecting clean samples based on a fixed preestimated scale parameter, which is hard to be set for streaming online datasets with changing clean ratios. In this paper, we introduce Online Gradientbased Robust Selection ( OGRS ), a novel gradientbased multiiteration sample selection approach that enables the online training of DNNs with dynamically changing proportions of noisy labels. Since clean data normally produces much lower training loss compared to noisy data based on the observations in [10], our proposed method capitalizes on the significant disparity in the gradient of the training loss at the clean and noisy data points respectively, which initially updates the data selection towards the clean region. To prevent the risk of overfitting, which may arise from the repeated selection of the same samples, we additionally propose a constraint function to mitigate the overlap of selected data. As a result, we formulate the problem as a nonconvex constrained optimization problem. This structure enables our approach to dynamically adapt to varying noise proportions, thereby boosting the robustness of the online DNNs training against noisy labels. In the realm of nonconvex constrained optimization, a critical unresolved issue pertains to providing a theoretical guarantee for convergence analysis. Since the recent decade, gradient descent optimization methods have been widely used to solve a wide variety of problems, like the controls of robotic systems [13], bayesian inference [14], recommendation systems [15, 16] and the training of DNNs [17‚Äì20, 45]. While [21] studied the constrained nonconvex optimization problem using quadratic approximations, a straightforward analysis for this problem remains elusive due to the computational intractability of minimizing standard regret in nonconvex cases. To address this challenge, we introduce a new metric for nonconvex constrained optimization, termed local Lagrangian regret. We conduct a detailed theoretical analysis to validate our approach and show that a constant number of updating steps ensure our method finds a balance between the sample selection performance and the computation expense. In the subsequent experimental evaluation, we incrementally input data selected by the OGRS method into various online training models. These results are then benchmarked against stateoftheart methods to demonstrate the effectiveness of our approach. In general, our main contributions are summarized below: ‚Ä¢We introduce a novel gradientbased sample selection approach designed to facilitate effec tive online DNNs training with dynamically varying clean ratios. ‚Ä¢We define a new local Lagrangian regret for the nonconvex optimization problem and propose an efficient algorithm that is specifically tailored to address the sample selection problem. ‚Ä¢We give a way for theoretical proof of the effectiveness and efficiency of our sample selection methods with our newly defined regret metric. ‚Ä¢We conduct experiments by simulating realworld online training cases and make compar isons between different sample selection methods. 2 Related Work "
191,Do We Really Need to Collect Millions of Faces for Effective Face Recognition?.txt,"Face recognition capabilities have recently made extraordinary leaps. Though
this progress is at least partially due to ballooning training set sizes --
huge numbers of face images downloaded and labeled for identity -- it is not
clear if the formidable task of collecting so many images is truly necessary.
We propose a far more accessible means of increasing training data sizes for
face recognition systems. Rather than manually harvesting and labeling more
faces, we simply synthesize them. We describe novel methods of enriching an
existing dataset with important facial appearance variations by manipulating
the faces it contains. We further apply this synthesis approach when matching
query images represented using a standard convolutional neural network. The
effect of training and testing with synthesized images is extensively tested on
the LFW and IJB-A (verification and identification) benchmarks and Janus CS2.
The performances obtained by our approach match state of the art results
reported by systems trained on millions of downloaded images.","The recent impact of deep Convolutional Neural Network (CNN) based methods on machine face recognition capabilities has been nothing short of revolutionary. The conditions under which faces can now be recognized and the numbers of faces which systems can now learn to identify improved to the point where some consider machines to be better than humans at this task. This remarkable advancement is partially due to the gradual improvement of new network designs which oer better performance. However, alongside developments in network architectures, it is also the underlying ability of CNNs to learn from massive training sets that allows these techniques to be so eective. Realizing that eective CNNs can be made even more eective by increasing their training data, many begun focusing eorts on harvesting and labeling large image collections to better train their networks. In [35], a standard CNN was trained by Facebook using 4.4 million labeled faces and shown to achieve what was, at the time, state of the art performance on the Labeled Faces in the Wild (LFW) benchmark [11]. Some time later, [24] proposed the VGGFace representation, trained on 2.6 million faces, and Face++ proposed its Megvii arXiv:1603.07057v2  [cs.CV]  11 Apr 20162 I. Masi, A.T. Tran, J. T. Leksut, T. Hassner and G. Medioni Dataset #ID #Img #Img =#ID Google [29] 8M 200M 25 Facebook [35] 4,030 4.4M 1K VGG Face [24] 2,622 2.6M 1K MegaFace [12] 690,572 1.02M 1.5 CASIA [44] 10,575 494,414 46 Aug. pose+shape 10,575 1,977,656 187 Aug. pose+shape+expr 10,575 2,472,070 234 (a)Face set statistics 1001011021031041050200040006000 Subjects (log scale)Images    CASIA WebFace Pose with Shapes Pose, Shapes, Expression(b)Images for subjects Fig. 1: (a) Comparison of our augmented dataset with other face datasets along with the average number of images per subject. (b) Our improvement by augmentation (Aug.) in the distribution of persubject image numbers in order to avoid the longtail eect of the CASIA set [44] (also shown in the last two rows of (a)). System [45], trained on 5 million faces. All, however, pale in comparison to the Google FaceNet [29] which used 200 million labeled faces for its training. Making networks better by collecting and labeling huge training sets is, un fortunately, not an easy game to play. The eort required to download, process and label millions of images from the Internet with reliable subject names is daunting. To emphasize this, the bigger sets, [35] and [29], required the eorts of large scale commercial organizations to assemble (Facebook and Google, resp.) and none of these sets was publicly released by its owners. By comparison, the largest face recognition training set which is publicly available is the CASIA WebFace collection [44] weighing in at a mere 495K images, several orders of magnitudes smaller than the two bigger commercial sets1. But downloading and labeling so many faces is more than just nancially challenging. Fig. 1a provides some statistical information on the larger face sets. Evidently, set sizes increase far faster than the numbers of images persubject. This may imply that nding many images veried as belonging to the same subjects is dicult even when resources are abundant. Regardless of the reason, this is a serious problem: face recognition systems should learn to model not just interclass appearance variations (dierences between dierent people) but also intraclass variations (dierences of appearance that do not change subject label) and so far this has been a challenge for data collection eorts. In light of these challenges, it is natural to ask: is there no alternative to this labor intensive, data download and labeling approach to pushing recognition per formance? Beyond potentially mitigating the challenges of data collection, this question touches a more fundamental issue. Namely, should images be processed with domain specic tools before being analyzed by CNNs, and if so, how? In answer to these questions, we make the following contributions. (1)We propose synthesizing data in addition to collecting it. We in ate the size of an existing training set, the CASIA WebFace collection [44], to several times its 1MegaFace [12] is larger than CASIA, but was designed as a testing set and so provides few images per subject. It was consequently never used for training CNN systems.Do We Really Need to Collect Millions of Faces 3 size using domain specic methods designed for face image synthesis (Fig. 1b). We generate images which introduce new intraclass facial appearance varia tions, including pose (Sec. 3.1), shape (Sec. 3.2) and expression (Sec. 3.3). (2) We describe a novel matching pipeline which uses similar synthesis methods at test time when processing query images. Finally, (3), we rigorously test our ap proach on the LFW [11], IJBA (verication and identication) and CS2 bench marks [14]. Our results show that a CNN trained using these generated faces matches state of the art performance reported by systems trained on millions of faces downloaded from the web and manually processed2. We note that our approach can be considered a novel face data augmenta tion method (Sec. 2): A domain specic data augmentation approach. Curiously, despite the success of existing generic augmentation methods, we are unaware of previous reports of applying the easily accessible approach described here to generate face image training data, or indeed training for any other class. 2 Related work "
78,Context-based Virtual Adversarial Training for Text Classification with Noisy Labels.txt,"Deep neural networks (DNNs) have a high capacity to completely memorize noisy
labels given sufficient training time, and its memorization, unfortunately,
leads to performance degradation. Recently, virtual adversarial training (VAT)
attracts attention as it could further improve the generalization of DNNs in
semi-supervised learning. The driving force behind VAT is to prevent the models
from overfitting data points by enforcing consistency between the inputs and
the perturbed inputs. This strategy could be helpful in learning from noisy
labels if it prevents neural models from learning noisy samples while
encouraging the models to generalize clean samples. In this paper, we propose
context-based virtual adversarial training (ConVAT) to prevent a text
classifier from overfitting to noisy labels. Unlike the previous works, the
proposed method performs the adversarial training at the context level rather
than the inputs. It makes the classifier not only learn its label but also its
contextual neighbors, which alleviates the learning from noisy labels by
preserving contextual semantics on each data point. We conduct extensive
experiments on four text classification datasets with two types of label
noises. Comprehensive experimental results clearly show that the proposed
method works quite well even with extremely noisy settings.","Deep neural networks (DNNs) have shown human level performance in various domains, such as image classiÔ¨Åcation, machine translation, and speech recog nition. To achieve such an ability, it is indisputably evident that we have to collect a large amount of train ing data. As labeling such data is laborious and ex pensive, previous works utilize search engine (Blum et al., 2003; Li et al., 2017) or crowdsourcing (Yan et al., 2014; Yu et al., 2018) to collect labeled dataset. Un fortunately, these lowcost approaches introduce low quality annotations with label noise . It causes DNNs to completely memorize such label noises (i.e., nearly 100% training accuracy) (Zhang et al., 2016), deterio rating generalization capability (Fr ¬¥enay and Verleysen, 2013; Sukhbaatar et al., 2014). To deal with label noises, recent works primar ily propose loss correction approaches. These meth ods directly correct loss function (e.g., crossentropy, mean squared error) or the probabilities used to com pute it. For example, (Zhang et al., 2016) learns sam ple weighting scheme via an auxiliary network (called MentorNet ) and applies it to noisy labels such that cor rupt data could get nearly zero sample weights on the loss function. On the one hand, (Han et al., 2018) utilizes an intervention between two same networks with smallloss samples which are considered as clean. However, there exists only a single work to handle label noise on natural language. (Jindal et al., 2019) utilize a noise transition matrix for text classiÔ¨Åcation. This method is useful at text classiÔ¨Åcation on label noises, but it turned out that estimating real noise transition matrix is difÔ¨Åcult (Jiang et al., 2018; Han et al., 2018) *Equal contribution.especially when the number of classes is large. Recently, adversarial training at attracts attention as it could prevent networks from misclassifying an image that contains a small perturbation (e.g., adver sarial examples). The at enforces the classiÔ¨Åer to make consistent predictions on synthetic inputs that have small, approximately worstcase perturbations. (Miyato et al., 2015) extends the idea of at to the semisupervised regime by removing label dependency on perturbations, which is called a virtual adversarial training (V AT). Adversarial training indirectly have a label smoothing effect as it prevents models from pre dicting given labels with high conÔ¨Ådence by anisotropi cally smoothing around each data point. Several recent works have shown that label smoothing is effective as a means of coping with label noise (Lukasik et al., 2020) and preventing memorization (Xie et al., 2016). Based on such observations, we further study whether the ad versarial training methods, which is an indirect label smoothing method, are useful at training a robust clas siÔ¨Åer on label noise. In this paper, we propose a contextbased virtual ad versarial training (ConV AT) to build a robust text clas siÔ¨Åer on label noise. We inherently follow a fundamen tal strategy of V AT. Unlike the previous work (Miayto et al., 2016), ConV AT performs the adversarial training on the contextlevel feature space not the wordlevel. To that end, we solve minmax optimization by follow ing two steps: formulating perturbation andsmooth ing. In the Ô¨Årst phase, we calculate the worstcase per turbation into an adversarial direction that could max imize the classiÔ¨Åcation loss on the given samples. We then minimize the distributional distance between a normal sample and a perturbed sample to learn robust classiÔ¨Åer on adversarial perturbations. This strategy alarXiv:2206.11851v1  [cs.CL]  29 May 2022(a)  (b) Figure 1: Overall training procedure in (a) virtual ad versarial training (b) contextbased virtual adversarial training (ConV AT). Dotted line indicates a duplicated propagation path to generate adversarial perturbation. lows us to train a label noiserobust classiÔ¨Åer without placing a burden in a network computation. In order to show the strength of the proposed method, we conduct extensive experiments on four dif ferent datasets with the different kinds of label noise. Comprehensive evaluation results clearly show that ConV AT outperforms the stateoftheart method in text classiÔ¨Åcation with noisy labels. The indepth anal ysis demonstrates that the proposed method have a strong advantage over previous adversarial methods in terms of time and memory complexity. Our code and dataset are publicly available1. 2. Related Works "
194,Label noise detection under the Noise at Random model with ensemble filters.txt,"Label noise detection has been widely studied in Machine Learning because of
its importance in improving training data quality. Satisfactory noise detection
has been achieved by adopting ensembles of classifiers. In this approach, an
instance is assigned as mislabeled if a high proportion of members in the pool
misclassifies it. Previous authors have empirically evaluated this approach;
nevertheless, they mostly assumed that label noise is generated completely at
random in a dataset. This is a strong assumption since other types of label
noise are feasible in practice and can influence noise detection results. This
work investigates the performance of ensemble noise detection under two
different noise models: the Noisy at Random (NAR), in which the probability of
label noise depends on the instance class, in comparison to the Noisy
Completely at Random model, in which the probability of label noise is entirely
independent. In this setting, we investigate the effect of class distribution
on noise detection performance since it changes the total noise level observed
in a dataset under the NAR assumption. Further, an evaluation of the ensemble
vote threshold is conducted to contrast with the most common approaches in the
literature. In many performed experiments, choosing a noise generation model
over another can lead to different results when considering aspects such as
class imbalance and noise level ratio among different classes.","Data quality is of great importance for ML applications and, in particular, for classiÔ¨Åcation tasks. Conventionally in these tasks, a training set of labeled instances is given as input to an ML algorithm, which will acquire useful knowledge to make predictions for new instances. In practice, realworld datasets frequently contain irregularities such as incompleteness, noise, and data inconsistencies that impact ML performance [Han et al., 2012]. In this light, noise detection and Ô¨Åltering are quite relevant techniques for ML [Zhu and Wu, 2004]. According to the literature, noise may occur in both attributes and classes [Zhu and Wu, 2004]. This work focuses on the latter problem, in which an unknown proportion of instances in a dataset are mislabeled because of different reasons. This is a relevant problem since label noise can harm the identiÔ¨Åcation of true class boundaries in a problem, increase the chance of overÔ¨Åtting, and affect learning performance in general [Frenay and Verleysen, 2014]. Previous works successfully adopted the classiÔ¨Åcation noise Ô¨Åltering method [Brodley and Friedl, 1999][Sluban and Lavra Àác, 2015][Guan et al., 2018] for label noise detection, widespread in the literature. In this approach, mislabeled instances in a dataset are identiÔ¨Åed according to the output results of a classiÔ¨Åer or an ensemble of classiÔ¨Åers. For example, in the majority vote for ensemble noise detection, an instance is marked as mislabeled if most classiÔ¨Åers in a pool incorrectly classify it. In the consensus vote for ensemble noise detection, a record is considered noisy if all classiÔ¨Åers in the pool misclassify it.arXiv:2112.01617v1  [cs.LG]  2 Dec 2021Label noise detection under the Noise at Random model with ensemble Ô¨Ålters A P REPRINT As with most ML tasks, empirical evaluation is also crucial in the context of noise detection techniques. Producing a groundtruth dataset for evaluation usually requires additional domain experts to decide which instances were mislabeled. This process can be costly, and experts are not always available. This problem is mitigated when artiÔ¨Åcial datasets are used or when simulated noise is injected into a dataset in a controlled way. The investigation of how noise inÔ¨Çuences the learning process is simpliÔ¨Åed when a systematic addition of noise is performed [Garcia et al., 2019]. Label noise can be injected into a dataset by assuming three distinct models of noise Frenay and Verleysen [2014]: (i) Noisy Completely at Random (NCAR), in which the probability of an instance being noisy is random, (ii) Noisy at Random (NAR), the probability of an instance being noisy depends on its label, and (iii) NonNoisy at Random (NNAR), the probability of an instance being noisy also depends on its attributes. In many previous works [Sluban and Lavra Àác, 2015] [Brodley and Friedl, 1999] [S√°ez et al., 2015] [Garcia et al., 2019], a single noise model is chosen over another to perform experiments. Nevertheless, it is usually not clear how this choice can affect experimental results. Additionally, other aspects, like class distribution, can impact the distribution of noise differently in a dataset depending on the noise type considered. For instance, a human supervisor may Ô¨Ånd it more difÔ¨Åcult to label records from the minority class than the majority class in some contexts. In this work, it is investigated how noise models can inÔ¨Çuence noise detection experiments under different aspects. In contrast to previous studies, the inÔ¨Çuence of distinct label noise models on ensemble noise detection is evaluated in this research under various contexts such as class imbalance, noise distribution, ensemble thresholds, and percentage of noise in data. It is shown that different results are achieved depending on the context. For instance, even under the same noise model (e.g., NAR), a detection technique may have quite distinct performance results if class imbalance changes (e.g., NAR with imbalanced vs. NAR with balanced class distributions). The remainder of this paper is organized as follows. In Section 2, an overview of label noise detection is presented. The proposed methodology is described in Section 3. Experiments are presented in Section 4. Finally, Section 5 summarizes the paper and presents future work. 2 Related Works "
557,Differencing based Self-supervised pretraining for Scene Change Detection.txt,"Scene change detection (SCD), a crucial perception task, identifies changes
by comparing scenes captured at different times. SCD is challenging due to
noisy changes in illumination, seasonal variations, and perspective differences
across a pair of views. Deep neural network based solutions require a large
quantity of annotated data which is tedious and expensive to obtain. On the
other hand, transfer learning from large datasets induces domain shift. To
address these challenges, we propose a novel \textit{Differencing
self-supervised pretraining (DSP)} method that uses feature differencing to
learn discriminatory representations corresponding to the changed regions while
simultaneously tackling the noisy changes by enforcing temporal invariance
across views. Our experimental results on SCD datasets demonstrate the
effectiveness of our method, specifically to differences in camera viewpoints
and lighting conditions. Compared against the self-supervised Barlow Twins and
the standard ImageNet pretraining that uses more than a million additional
labeled images, DSP can surpass it without using any additional data. Our
results also demonstrate the robustness of DSP to natural corruptions,
distribution shift, and learning under limited labeled data.","Scene change detection (SCD) is a critical perception task that helps to identify changes in a scene captured at different times. In recent years, SCD has been gaining popularity in the Ô¨Åeld of computer vision, robotics, and remote sensing (Hamaguchi et al., 2019; Sakurada et al., 2020; Alcantarilla et al., 2018) as its various real world applications such as ecosystem monitoring, urban expansion, remote surveillance, autonomous driving, and damage assessment have an immensely positive impact on society. For instance, in autonomous driving and robotics applications, the problem of generating and maintaining maps of everchanging environments is of utmost importance for dynamic localization, and robust operation of vehicles/robots in urban landscapes. SCD helps to alleviate the problem of mapping and efÔ¨Åcient maintenance by continuously monitoring the changes of the scene at different time instances (Alcantarilla et al., 2018). Thus, it plays an important role in many real world applications by perceiving the changes occurring in the environment. Generally, in SCD, the changed region is smaller than the unchanged region with uncertainty in change location and direction. Moreover, the changed region that needs to be detected depends on the nature of the application and is classiÔ¨Åed into semantic changes (relevant) and noisy changes (irrelevant). The structural changes caused by the appearance or disappearance of objects present in a scene are considered as semantic changes, while the changes induced by the radiometric (illumination, shadows, seasonal changes) and geometric variations (viewpoint differences caused by camera rotation) are considered as noisy changes (Alcantarilla et al., 2018; Sakurada & Okatani, 2015; Guo et al., 2018). A critical challenge in SCD is that these noisy changes are entangled with the semantic changes that alter the appearance of an image, thus degrading the change detection performance (Guo et al., 2018). Previous studies based on deep neural networks have proposed to extract multilevel feature representations from the input images to improve the performance of SCD against noisy changes (Guo et al., 2018; Alcantarilla et al., 2018; Varghese et al., 2018; Lei et al., 2020). However, the success of these stateoftheart methods hinges on a large quantity of annotated data. For instance, on average, it takes around 20 minutes and 156 minutes to annotate a single pair of images in the panoramic change detection (PCD) (Sakurada et al., 2013) and panoramic semantic change detection (PSCD) (Sakurada et al., 2020) dataset, respectively. Therefore, largescale labeled datasets for SCD are still scarce, and expensive to obtain (Shi et al., 2020). To address the dependency on labeled data, various SCD approaches initially pretrain their models on largescale datasets such as ImageNet (Deng et al., 2009) in a supervised Equal advising.1We open source our code at https://github.com/NeurAILab/DSP . 1arXiv:2208.05838v1  [cs.CV]  11 Aug 2022Published at 1st Conference on Lifelong Learning Agents, 2022 manner and then Ô¨Ånetune with a small quantity of pixellevel annotations on domainspeciÔ¨Åc dataset (Guo et al., 2018; Sakurada et al., 2020; Chen et al., 2021). However, there still exists the problem of (1) domain shift as the distribution of the ImageNet data widely differs from that of SCD datasets, (2) nature of feature representation learned by transfer learning models from classiÔ¨Åcation tasks is suboptimal for SCD. These problems lead to the degradation of change detection performance in SCD methods. 1% 10% 50% 100% Percentage of Labeled Data (%)01020304050607080F1score Rand Init SupIm Barlow Twins DSP Figure 1: Performance comparisons of supervised and the proposed selfsupervised pretraining (DSP) on VLCMU CD dataset under limited label scenario.To attenuate the reliance of SCD models on a large amount of dense pixellevel annotations and trans fer learning from largescale labeled outofdistribution data, we propose a novel selfsupervised pretraining ap proach that utilizes unlabeled data to learn taskspeciÔ¨Åc representations for the downstream task of SCD. Our method, DSP, uses feature differencing to learn discrim inatory representations corresponding to the changed re gions that are beneÔ¨Åcial for the downstream task of SCD. Furthermore, we propose invariant prediction (IP) ob jective and change consistency regularization (CR) , to gether referred to as temporal consistency (TC) loss, to reduce the effects of differences in the lighting con ditions or camera viewpoints by enhancing the image alignment between the temporal images in the decision and feature space, respectively. With extensive experi ments, we show that our proposed approach achieves re markable performance compared to ImageNet pretrain ing under limited labels scenario as seen in Figure 1. To the best of our knowledge, this is the Ô¨Årst work on SCD that relaxes the requirement of largescale annotated datasets and the need to pretrain on additional largescale labeled data in a computationally efÔ¨Åcient way. Our contribution can be summarized as follows: ‚Ä¢ We propose a differencing based selfsupervised pretraining (DSP) method that learns change representations (taskspeciÔ¨Åc) relevant for scene change detection. ‚Ä¢ We propose an invariant prediction (IP) objective and change consistency regularization (CR) to mitigate the effect of noisy changes across a pair of views. ‚Ä¢ We evaluated the proposed methods on two challenging SCD datasets. DSP surpasses the widely used Ima geNet pretraining without any additional data. Also, DSP pretraining enhances the SCD performance com pared to the standard Barlow Twins (Zbontar et al., 2021) method. ‚Ä¢ Current scene change detection models are vulnerable to severe performance impairments on images with natural corruptions, and the proposed selfsupervised pretraining signiÔ¨Åcantly enhances the robustness of the model to natural corruptions. ‚Ä¢ The effectiveness of the proposed selfsupervised pretraining under limited labels and generalization to out ofdistribution data is veriÔ¨Åed. 2 R ELATED WORK "
603,Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks.txt,"We present a novel learning-based approach to estimate the
direction-of-arrival (DOA) of a sound source using a convolutional recurrent
neural network (CRNN) trained via regression on synthetic data and Cartesian
labels. We also describe an improved method to generate synthetic data to train
the neural network using state-of-the-art sound propagation algorithms that
model specular as well as diffuse reflections of sound. We compare our model
against three other CRNNs trained using different formulations of the same
problem: classification on categorical labels, and regression on spherical
coordinate labels. In practice, our model achieves up to 43% decrease in
angular error over prior methods. The use of diffuse reflection results in 34%
and 41% reduction in angular prediction errors on LOCATA and SOFA datasets,
respectively, over prior methods based on image-source methods. Our method
results in an additional 3% error reduction over prior schemes that use
classification based networks, and we use 36% fewer network parameters.","Estimating the directionofarrival (DOA) of sound sources has been an important problem in terms of analyzing multichannel recordings [1, 2]. In these applications, the goal is to predict the azimuth and elevation angles of the sound source relative to the microphone, from a sound clip recorded in any multi channel setting. One of the simpler problems is the estima tion of the DOA on the horizontal plane [3]. More complex problems include DOA estimation in threedimensional space or the identiÔ¨Åcation of both direction and distance of an audio source. Even more challenging problems correspond to per forming these goals in noisy and reverberant environments. To analyze spatial information from sound recordings, at least two microphones with known relative positions must be used. In practice, various spatial recording formats including binaural, 5.1channel, 7.1channel, etc. have been applied to spatial audio related systems [4]. The Ambisonics format de composes a soundÔ¨Åeld using a spherical harmonic function ba sis [5]. Compared with its alternatives, Ambisonics has the ad vantage of being hardware independent‚Äìit does not necessarily encode microphone speciÔ¨Åcations into the recording. Recent work [6] has applied the Ambisonics format to DOA estimation and trained a CRNN classiÔ¨Åer that yields more ac curate predictions than a baseline approach using independent component analysis. While a regression formulation seems more natural for the problem of DOA estimation, some recent This work was supported in part by ARO grant W911NF181 0313 and Intel. Project page https://gamma.umd.edu/pro/ speech/doawork [3] suggests a regression formulation may yield worse per formance than that of the classiÔ¨Åcation formulation for multi layer perceptrons. In this work, we present a novel learning based approach for estimating DOA of a single sound source from ambisonic audio, building on an existing deep learning framework [6]. We present a CRNN which predicts DOA as a 3D Cartesian vector. We introduce a method to generate synthetic data using geometric sound propagation that models specular and diffuse reÔ¨Çections, which results in up to 43% er ror reduction compared with imagesource methods. We con duct a fourway comparison between the Cartesian regression network, two classiÔ¨Åcation networks trained with crossentropy loss, and a regression network trained using angular loss. Fi nally, we investigate results on two 3rdparty datasets: LO CATA [7] and SOFA [8], where our best model reduces angular prediction error by 43% compared to prior methods. Section 2 gives an overview of prior work. We propose our method in Section 3. Section 4 presents our results on two benchmarks and we conclude in Section 5. 2. Related Work "
612,Divergence Optimization for Noisy Universal Domain Adaptation.txt,"Universal domain adaptation (UniDA) has been proposed to transfer knowledge
learned from a label-rich source domain to a label-scarce target domain without
any constraints on the label sets. In practice, however, it is difficult to
obtain a large amount of perfectly clean labeled data in a source domain with
limited resources. Existing UniDA methods rely on source samples with correct
annotations, which greatly limits their application in the real world. Hence,
we consider a new realistic setting called Noisy UniDA, in which classifiers
are trained with noisy labeled data from the source domain and unlabeled data
with an unknown class distribution from the target domain. This paper
introduces a two-head convolutional neural network framework to solve all
problems simultaneously. Our network consists of one common feature generator
and two classifiers with different decision boundaries. By optimizing the
divergence between the two classifiers' outputs, we can detect noisy source
samples, find ""unknown"" classes in the target domain, and align the
distribution of the source and target domains. In an extensive evaluation of
different domain adaptation settings, the proposed method outperformed existing
methods by a large margin in most settings.","Deep neural networks (DNNs) have achieved impressive results with largescale annotated training samples, but the performance declines when the domain of the test data dif fers from the training data. To address this type of distribu tion shift between domains with no extra annotations, unsu pervised domain adaptation (UDA) has been proposed to learn a discriminative classiÔ¨Åer while there is a shift be tween training data in the source domain and test data in the target domain [1, 8, 9, 11, 25, 27, 27, 29, 33, 36]. Most existing domain adaptation methods assume that the source and target domains completely share the classes, but we do not know the class distribution of samples in the target domain in realworld UDA. Universal domain adap Figure 1: Problem setting of Noisy UniDA. Our proposed setting assumes that some source samples have corrupted labels, some classes of the source domain do not appear in the target domain, and the classes of some target samples are not shared by the source domain. tation (UniDA) [41] is proposed to remove the constraints on the label sets, where target samples may contain un known samples belonging to classes that do not appear in the source domain and some source classes may not appear in the target samples. However, UniDA is still an ideal sce nario, where existing UniDA methods require source sam ples with correct annotations to train the model. This re quirement limits the application of existing UniDA meth ods in real domain adaptation problems, where clean and highquality datasets are time consuming and expensive to collect. Data can more easily be collected from a crowd sourcing platform or crawled from the Internet or social me dia, but such data are inevitably corrupted with noise ( e.g. YFCC100M [35], Clothing1M [40], and ImageNet [3]). Hence, we consider a new realistic setting called ‚ÄúNoisy Universal Domain Adaptation‚Äù (Noisy UniDA), as shown in Fig. 1, which has the following properties: ‚Ä¢ Labeled data of the source domain contains noisy la bels.1 ‚Ä¢ Some classes of the source domain do not appear in the target domain, and these classes are named source 1The labels of target samples are not considered because they are not available in the setting of UDA. 1arXiv:2104.00246v1  [cs.CV]  1 Apr 2021private classes. ‚Ä¢ Some classes of the target domain are not shared by the source domain, and these classes are named target private classes. Some existing methods [28, 17, 30, 7, 41] aim to solve certain parts of Noisy UniDA. For example, [30] attempted to train domainadaptive models on noisy source data, [7] worked on the partial problem that the source private classes are absent from the target domain, [28] and [17] attempted to solve the openset problem of target private classes, and [41] addressed the settings with the partial problem and the openset problem together. However, a method that can solve all these problems at the same time does not exist. Instead of solving each problem separately, we focus on the divergence of DNNs to address all the problems of Noisy UniDA. Inspired by Cotraining for multiview learn ing and semisupervised learning [4, 31], when different models having different parameters are trained on the same data, they learn distinct views of each sample because they have different abilities to learn. As a result, different mod els in each view would agree on the labels of most samples, and it is unlikely for compatible classiÔ¨Åers trained on in dependent views to agree on a wrong label. We Ô¨Ånd this property can be effective in Noisy UniDA, where the noisy source samples have wrong labels, and target private sam ples can also be considered to have incorrect labels because their true label is not contained in the label set. When these data are input to different networks, the networks are more likely to output different results because they have differ ent parameters. Therefore, we utilize a twohead network architecture with two independent classiÔ¨Åers to detect all these unwanted samples simultaneously. The proposed twohead network consists of one common feature generator and two separate label classiÔ¨Åers for clas siÔ¨Åcation. The two classiÔ¨Åers are updated by the same data at the minibatch level, but they are initialized differently to obtain different classiÔ¨Åers. To detect noisy source samples in each minibatch, we calculate the divergence between the two classiÔ¨Åers‚Äô outputs on the source data, and only source samples with small divergences are chosen to update the network by supervised loss. Using the same principle, tar get samples with larger divergence are more likely to be tar get private samples, and we further separate the divergence of the classiÔ¨Åers on common and target private samples to reject target private samples. Consequently, we align the distributions of the clean samples from the common classes shared by both domains, where the methods that align the entire distribution are inÔ¨Çuenced by incorrect source labels, the source private classes and target private classes. We evaluated our method on a diverse set of domain adaptation settings. In many settings, our method outper forms existing methods by a large margin. We summarize the contributions of this paper as follows:Method Noisy labels Partial DA Openset DA DANN [10]    TCL [30]    ETN [7]    STA [17]    UAN [41]    DANCE [24]    Proposed    Table 1: Summary of recent related methods. UniDA con sists of Partial DA and Openset DA. Our proposed method is the only method that covers all the settings. ‚Ä¢ We propose a novel experimental setting and a novel training methodology for noisy universal domain adap tation (Noisy UniDA). ‚Ä¢ We propose a divergence optimization framework to detect noisy source samples, Ô¨Ånd target private sam ples, and align the distributions of the source and tar get domains according to the divergence of two label classiÔ¨Åers. ‚Ä¢ We evaluate our method across several realworld do main adaptation tasks. 2. Related Work "
497,High performing ensemble of convolutional neural networks for insect pest image detection.txt,"Pest infestation is a major cause of crop damage and lost revenues worldwide.
Automatic identification of invasive insects would greatly speedup the
identification of pests and expedite their removal. In this paper, we generate
ensembles of CNNs based on different topologies (ResNet50, GoogleNet,
ShuffleNet, MobileNetv2, and DenseNet201) altered by random selection from a
simple set of data augmentation methods or optimized with different Adam
variants for pest identification. Two new Adam algorithms for deep network
optimization based on DGrad are proposed that introduce a scaling factor in the
learning rate. Sets of the five CNNs that vary in either data augmentation or
the type of Adam optimization were trained on both the Deng (SMALL) and the
large IP102 pest data sets. Ensembles were compared and evaluated using three
performance indicators. The best performing ensemble, which combined the CNNs
using the different augmentation methods and the two new Adam variants proposed
here, achieved state of the art on both insect data sets: 95.52% on Deng and
73.46% on IP102, a score on Deng that competed with human expert
classifications. Additional tests were performed on data sets for medical
imagery classification that further validated the robustness and power of the
proposed Adam optimization variants. All MATLAB source code is available at
https://github.com/LorisNanni/.","Worldwide pest infestation is a major cause of crop and machine damage and, as a consequence, considerabl e reductions  in  grower  revenues  and economic growth [1, 2] . Invasive pests in the US , for example, were reported in 2016 to be racking  up  losses totaling  at least  seventy  billion USD per year  [3]. Rather than sending experts into the  field, a  common low cost method  for monitoring and controlling pest populations has been  to lure insects into traps with pheromones, color, and light. Once  specimens are collected  in this manner , they must be identified and counted, which is an expensive , timeconsuming task  requiring taxonomic expertise. Automatic identification of invasive insects from trap images is a solution that offers the  prospect of increased accuracy , reduced  costs, and large scale execution . However, species identification and th e  discrimination of pests that are harmful versus those that are safe is complicated by  the small size of many  insects, close     resemblance between species, discoloration caused by soap and alcohol solutions  contained within the traps , the pheromone  cap, the loss of legs in sticky traps, vegetative debris, and insect clumping  [46]. Some of t hese conditions can be overcome  by developing systems that automatically and continuously i nspect pests in the field , but the difficult problem of identifying  insects within complex backgrounds ‚Äîas they appear on different crops or on the ground with occlusions  and variations in  illumination ‚Äîremains [2, 710].   In a recent 2020 survey [4] of the literature on pest detection published between 2015 and 2019 , it was determined  that 63% of the studies were  based on convolutional neur al networks (CNNs)  [11], a powerful deep neural network model  designed specifically to recognize visual patterns from images with minimal preprocessing,  and 29% on more traditional  feature based approache s. An earlier survey published in 2018  [12] that focused on deep learners in pest identification found  that only 42% of papers employed  a CNN approach. These two surveys demonstrate that CNNs have become the classifier  of choice in pest classification, a finding that is not surprising given the power of CNNs, which we highlight  in section 4.   In this paper, we propose a method for classif ying insects that is based on CNN and some new Adam optimization  variants.  During  CNN  training,  millions of parameters are updated via a loss function, which attempts to minimize the  difference between current outputs and the  actual values in the training set. Traditionally, gradient descent  (GD) , especially  the stochastic gradient descent (SGD) [13] has been the preferred method  for minimiz ing an objective function . Although  popular, SGD tends towards the convergence of suboptimal local minima when minimizing highly non convex error  functions due to the fact that  the optimization landscape of SGD is not convex. To overcome this s hortcoming, many  variations of SGD  have been proposed [14] [1517]. To some degree, every SGD variant  utilizes the momentum to direct the  gradient [1416]. In [14] and [18], for instance, optimization is guided by its own inertia . In AdaGrad [15] and Adadelta [16],  which extends AdaGrad,  the learning rate of parameters  is decreased more with large partial derivatives tha n with smaller  partial derivatives.  The mechanism behind AdaGrad is the accumulat ion of  previously squared gradients, unlike Adadelt a,  which keeps  track of an exponentially decaying average of past squared gradients . Another SDG variant , Adam [18], not  only stores an exponentially decaying average of past squared gradients but also  an exponentially decaying average of past  gradients ; this information  is used to decrease the learning  rate of the parameters whose gradient changes more frequently ,  most especially in case s where the gradient changes its sign. Adam is well known for its realization of low minima of the  training loss  and is now frequently used with CNNs  [19].  Adam 's excellent ability to find low minima, however,  fails to  translate into better performance compared with SGD  [20]. As a consequence, many new var iants of Adam have been introduced that aim at  increasing its effectiveness  [17, 21,  22]. In [21], for example, the authors p ropose Nadam, which inserts into Adam the Nesterov momentum. More recently, new   Adam variants have been proposed that are based on the difference between present and past gradients, where the step size  is adjusted for each parameter. In [22], AMSGrad curtails  the step size to prevent it from  increas ing, while in diffGrad [17],  which has obtained state oftheart results,  the step size for every parameter is made proportional to the change in the gradient .  In [23], the performance of different Adam variants is compared with SGD and shown to introduce diversity  in ensembles of  CNNs. In addition, the author s propose two new  Adam optimization methods: 1) DGrad, which is a variant of diffGrad  that  uses a moving average of the squares of parameter gradients, and 2) two adjustments  of DGrad that apply different cycli c  learning rates [24].  In this study, we propose two new variants of DGrad [23]: Exp, which introduc es a scaling factor , and ExpLR,  which varies  Exp by adding an extra  step when calculating  the final learning rate. Sets of CNNs are then trained on two insect  benchmarks  (Deng [9] and IP102 [25]) using these and several other Adam variants. Fusions of these partially independent  classifiers are compared  and evaluated to discover t hose combinations that produc e the best classification results . Our best  method is shown to achieve state of  the art on both data sets. Additional  tests are performed on some medical  data to further  validat e the usefulness of the proposed  Adam optimization variants.   The organization of the remainder of this paper is as follows.  In section 2, a  short review of related papers in pest  classification is provided, followed , in section 3,  by a brief introduction to  the CNN topologies investigate d in this study . In  section 4, the Adam optimization variants tested in this work, along with  the new ones proposed here, are detailed . In section  5, the benchmark data set s are described , and experimental results  are presented. The paper concludes with some final remarks  and some suggestions for future research .    2 Related Work on Insect classification   "
3,Towards Early Prediction of Human iPSC Reprogramming Success.txt,"This paper presents advancements in automated early-stage prediction of the
success of reprogramming human induced pluripotent stem cells (iPSCs) as a
potential source for regenerative cell therapies.The minuscule success rate of
iPSC-reprogramming of around $ 0.01% $ to $ 0.1% $ makes it labor-intensive,
time-consuming, and exorbitantly expensive to generate a stable iPSC line.
Since that requires culturing of millions of cells and intense biological
scrutiny of multiple clones to identify a single optimal clone. The ability to
reliably predict which cells are likely to establish as an optimal iPSC line at
an early stage of pluripotency would therefore be ground-breaking in rendering
this a practical and cost-effective approach to personalized medicine. Temporal
information about changes in cellular appearance over time is crucial for
predicting its future growth outcomes. In order to generate this data, we first
performed continuous time-lapse imaging of iPSCs in culture using an ultra-high
resolution microscope. We then annotated the locations and identities of cells
in late-stage images where reliable manual identification is possible. Next, we
propagated these labels backwards in time using a semi-automated tracking
system to obtain labels for early stages of growth. Finally, we used this data
to train deep neural networks to perform automatic cell segmentation and
classification. Our code and data are available at
https://github.com/abhineet123/ipsc_prediction.","1.1 Motivation The goal of this work is to apply machine learning to automate the identification of human iPSCs that show promise for clinical cell therapies in regenerative medicine. IPSCs are ¬©2023 Singh,Jasra,Mouhammed,Dadheech,Ray and Shapiro. License: CCBY 4.0arXiv:2305.14575v1  [cs.CV]  23 May 2023Singh,Jasra,Mouhammed,Dadheech,Ray and Shapiro generated by reprogramming a patient‚Äôs own cells back in time to make more malleable cells with differentiation potential for generating any cells or tissues of interest. This tech nology has shown great potential for transforming regenerative cell therapies, drug and dis ease modelling, tissue repair and regeneration, and personalized genecorrected products. However, the pipeline for iPSC generation, characterization and cell banking is a highly laborintensive, timeconsuming and costly one. The monetary cost of researchgrade iPSC line generation is estimated at USD 10,00025,000 while that of clinicalgrade iPSC line is approximately USD 800,000 based on published reports (Huang et al., 2019). The entire process of optimal iPSC line generation and selection can take up to 35 days and requires a further 3 months to produce large scale iPSCs for therapeutic application in patients. Additionally, quality control techniques for growing iPSCs to limit inter or intrapatient iPSC line variability, which is currently assessed manually, remain imperfect in largescale biomanufacturing. The current solution relies on the judgement of an expert cell biolo gist, who determines precise iPSC induction, confirms pluripotency based on morphological changes and assesses molecular characterization for multiple clones  all tasks that remain highly effortintensive and subjectively biased. Manual cell quality control therefore cannot be used to scale up the production of iPSCs and derived products for therapeutic appli cations. An automated method enabling highthroughput surveillance and validation of cell identity, growth kinetics, and morphological features is desirable throughout the entire manufacturing process. The screening is multifold and needed to not only select optimal cells which have been fully converted to iPSCs during reprogramming stage but also to exclude unstable and pseudo iPSC contaminants during the expansion stage. Automating this process using machine learning would therefore be groundbreaking in improving iPSC bioprocess efficiency and yield, thereby drastically reducing the time and cost involved in the generation of iPSCbased products for therapeutic applications. This paper presents some early but promising steps in this direction. 1.2 Background 1.2.1 iPSC Reprogramming Takahashi and Yamanaka (2006) demonstrated that mouse embryonic or adult fibroblasts can be reprogrammed into pluripotent stem cells by introducing four genes encoding tran scription factors, namely Oct3/4, Sox2, Klf4, and cMYC (Takahashi et al., 2007; Ye et al., 2013). Generated stem cells showed similar morphological or functional behavior as embry onic pluripotent stem cells and were thus termed iPSCs. Soon thereafter, Takahashi et al. (2007) reported directed conversion of human fibroblasts into pluripotent stem cells, termed as human iPSCs. With the discovery of Yamanaka‚Äôs human iPSC technology, patient derived stem cells have huge potential in regenerative medicine (Takahashi and Yamanaka, 2013). Human iPSCs show merit not only in delivering any desired cell types for treating degenerative diseases, tissue repairing, disease modeling, and drug screening (Amabile and Meissner, 2009; Yamanaka, 2009), but they also solve two major problems associated with other pluripotent stem cells such as embryonic stem cells (Ye et al., 2013), namely immune tolrenace after transplantation and ethical concerns. However, there still exist technical and biomedical challenges including the risk of teratoma formation and the uncertainty of efficient nuclear reprogramming completeness due to variability and inconsistencies in the 2Towards Early Prediction of Human iPSC Reprogramming Success selection of optimal cells (Ye et al., 2013). There are two major problems to be solved be fore human iPSCs can be applied as a standardized technique, Firstly, manually monitoring the quality of growing iPSC colonies that is currently practiced does not scale. Secondly, only colonies that satisfy clinical good manufacturing practice (GMP) standards need to be identified for use in downstream applications. Hence, there is an urgent need for automated quality control, thereby also lending it an element of objectivity and standardization. 1.2.2 Machine learning in iPSC Recognition Though many applications of machine learning for iPSC recognition in images have been presented in the literature (Kusumoto et al., 2018; Waisman et al., 2019; Zhang et al., 2019a; Hirose et al., 2021; Coronnello and Francipane, 2021; Kusumoto et al., 2022; Lan et al., 2022), there are none that include both detection and classification or use time lapse imaging, which is the object of this study. To the best of our knowledge, Zhang et al. (2019b) presented the method that comes closest to this work though that too differs in several key respects. It utilizes fluorescence imaging and the commercial closedsource IMARIS software to segment cells and it captures 3D shape information that is the basis for extracting morphological features to train the classifier it uses. Our aim is to make open source cell segmentation possible without fluorescence and with only the 2D pixel data in standard phasecontrast microscpy images. 1.2.3 Deep Learning in Visual Recognition In the past decade, deep learning (Alzubaidi et al., 2021) has been applied extensively in computer vision (Chai et al., 2021), especially for recognition tasks like image classification (Byerly et al., 2022), object detection (Liu et al., 2020), instance segmentation (Gu et al., 2022), semantic segmentation (Mo et al., 2022), and object tracking (MarvastiZadeh et al., 2021; Jiao et al., 2021; Pal et al., 2021). It has likewise seen broad application in medical image analysis (Suganyadevi et al., 2021; Cai et al., 2020) including segmentation in general (Liu et al., 2021a) and cell segmentation in particular (Wen et al., 2022). The latter is the task most relevant to this work, though cell tracking (BenHaim and RiklinRaviv, 2022; Chen et al., 2021; Wang et al., 2020; Lugagne et al., 2020; Ulman et al., 2017) is also important here. More recently, the advent of transformers (Vaswani et al., 2017) has led to significant performance improvements (Liu et al., 2021b) over the convolutional neural network (CNN) based architectures that had been prominent earlier. Liu et al. (2021c) proposed the Swin transformer to improve the original vision transformer (Dosovitskiy et al., 2021) further using shifted windows. This currently appears to be the backbone of choice in most state of the art models, though that might change with the recent introduction of ConvNext (Liu et al., 2022) as a competitive CNNbased alternative. For this project, we needed instance segmentation models, preferably ones that could benefit from the temporal information in timelapse images. Therefore, we selected topperforming static and video segmentation models (sec. 2.1.4) with publicly available code from a popular leaderboard (Xiang, 2023). We also searched through the leaderboards on a couple of cell tracking and segmentation benchmark challenges (Ulman et al., 2017; Anjum and Gurari, 2020) but failed to find any models with publicly available code. 3Singh,Jasra,Mouhammed,Dadheech,Ray and Shapiro 2. Methodology "
11,Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels.txt,"Noisy training set usually leads to the degradation of generalization and
robustness of neural networks. In this paper, we propose using a theoretically
guaranteed noisy label detection framework to detect and remove noisy data for
Learning with Noisy Labels (LNL). Specifically, we design a penalized
regression to model the linear relation between network features and one-hot
labels, where the noisy data are identified by the non-zero mean shift
parameters solved in the regression model. To make the framework scalable to
datasets that contain a large number of categories and training data, we
propose a split algorithm to divide the whole training set into small pieces
that can be solved by the penalized regression in parallel, leading to the
Scalable Penalized Regression (SPR) framework. We provide the non-asymptotic
probabilistic condition for SPR to correctly identify the noisy data. While SPR
can be regarded as a sample selection module for standard supervised training
pipeline, we further combine it with semi-supervised algorithm to further
exploit the support of noisy data as unlabeled data. Experimental results on
several benchmark datasets and real-world noisy datasets show the effectiveness
of our framework. Our code and pretrained models are released at
https://github.com/Yikai-Wang/SPR-LNL.","Deep learning has achieved remarkable success on many topics of supervised learning with millions of labeled training data. The performance heavily relies on the quality of label annotation since neural networks are susceptible to noisy labels and even can easily memorize randomly labeled annotations [63], leading to the degradation of generalization and robustness. In many realworld scenarios, it is expensive and difÔ¨Åcult to obtain precise labels, exposing a realistic challenge for supervised deep models to learn with noisy data. *Corresponding author.There is a large literature for this challenge from various perspectives, including modifying the network architectures [6, 12, 13, 59] or loss functions [11, 27, 53, 65], or dynamically selecting clean data during training [5, 14, 17, 27,34,40,44,61]. Particularly, the dynamic sample selection methods adopt the spirit of providing only clean data for the training. Such a spirit can form a ‚Äòvirtuous‚Äô cycle between the noisy data elimination and network training: the elimination of noisy data can help the network training; and on the other hand, the improved network is empowered with a better ability in picking up clean data. As this virtuous cycle evolves, the performance can be improved. Typical principles to identify outliers include large loss [14], inconsistent prediction [67], and irregular feature representation [57]. The former two principles focus on the label space, while the last one focuses on the feature space of the same class. In this paper, we unify the label and feature space and assume linear relationship between the featurelabel pair (denoted as (xi;yi)) of dataiby yi=x> i+""; (1) wherexi2Rpis the feature vector, and yi2Rcis the onehot label vector; 2Rpcis the Ô¨Åxed (unknown) coefÔ¨Åcient matrix and ""2Rcis random noise. This linear relation is approximately established as the networks are trained to minimize the divergence between a (softmax) linear projection of the feature and onehot label vector. For a welltrained network, the output prediction of clean data is expected to be as similar to a onehot vector as possible, while for noisy data the output is dense. Intuitively, when the linear relation is wellapproximated without softmax operation, the corresponding data is likely to be clean data. The simplest way to identify the suspected outliers in the linear model is checking the predict error, or residual, ri=yi"
229,Latent Class-Conditional Noise Model.txt,"Learning with noisy labels has become imperative in the Big Data era, which
saves expensive human labors on accurate annotations. Previous
noise-transition-based methods have achieved theoretically-grounded performance
under the Class-Conditional Noise model (CCN). However, these approaches builds
upon an ideal but impractical anchor set available to pre-estimate the noise
transition. Even though subsequent works adapt the estimation as a neural
layer, the ill-posed stochastic learning of its parameters in back-propagation
easily falls into undesired local minimums. We solve this problem by
introducing a Latent Class-Conditional Noise model (LCCN) to parameterize the
noise transition under a Bayesian framework. By projecting the noise transition
into the Dirichlet space, the learning is constrained on a simplex
characterized by the complete dataset, instead of some ad-hoc parametric space
wrapped by the neural layer. We then deduce a dynamic label regression method
for LCCN, whose Gibbs sampler allows us efficiently infer the latent true
labels to train the classifier and to model the noise. Our approach safeguards
the stable update of the noise transition, which avoids previous arbitrarily
tuning from a mini-batch of samples. We further generalize LCCN to different
counterparts compatible with open-set noisy labels, semi-supervised learning as
well as cross-model training. A range of experiments demonstrate the advantages
of LCCN and its variants over the current state-of-the-art methods.","LARGE SCALE datasets with accurate labels have driven the success of deep neural networks (DNNs) in com puter vision [1], natural language processing [2], and speech recognition [3]. However, for many realworld applications, it is expensive to collect precisely annotated data in large volume. Instead, samples with noisy supervision, as an al ternative to alleviate the annotation burden, can be acquired inexhaustibly on social websites and have shown potential to many applications in the deep learning area [4‚Äì6]. However, it is challenging to train DNNs in the presence of noisy supervision due to the memorization effect [7]. To prevent the degeneration under this setting, several seminal works [8, 9] have been explored from the perspective of reg ularization to avoid overÔ¨Åtting or sample/label reÔ¨Ånement. For example, Arpit et al. [7] applied the dropout regulariza tion to decelerate noise memorization, and Reed et al. [10] explored the selfweighting mechanism via its predictions to weaken the noise effect. Different from the aforementioned methodologies, this study focuses on the third prosperous statistical perspective, learning via noise transition. This line of works place a transition matrix on top of the classiÔ¨Åer to disentangle the noise from the learning procedure. Previous works [11‚Äì13] presents a twostep solution: Ô¨Årst estimate the noise transition, and then freeze it to train the classiÔ¨Åer. However, it is usually impractical to have or approximately have an accurate anchor set [14] as the sufÔ¨Åcient statistics Jiangchao Yao, Zhihan Zhou and Ya Zhang are with Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai 200240, China. Jiangchao Yao and Ya Zhang are also with Shanghai AI Laboratory, Shanghai 200030, China. ymeans Ya Zhang is the corresponding author. Email:fSunarker, zhihanzhou, ya zhangg@sjtu.edu.cn Bo Han is with the Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Kowloon, Hong Kong, China. Email: bhanml@comp.hkbu.edu.hk Ivor W. Tsang is with the A*STAR Centre for Frontier AI Research, Singapore 138632. Email: ivor tsang@ihpc.astar.edu.sgof the noise transition. The alternative ways [15, 16] cast the noise transition as a neural layer and adapt it along with the training. Unfortunately, the current stochastic training as a neural layer to noise transition is illposed, yielding that the optimization depends on the empirically tuning to dodge the undesired local minimums. Actually, the reason of this phenomenon is the backpropagation applied in the neural layer could induce the arbitrary update to the noise transition by a minibatch of samples. The worst case is that some atypical minibatches containing the extreme noise could catastrophically destroy the well estimation regarding the noise transition parameters for the complete dataset. To solve this problem, we extend the conventional CCN into a latent counterpart, i.e.,Latent ClassConditional Noise model (LCCN). The intuition is that LCCN that param eterizes the noise transition under a Bayesian framework can explicitly characterize the dependency on the complete dataset instead of the minibatch of sample statistics. This fundamentally constrains the learning of the noise transition unlike the backpropagation applied in the neural layer, yielding a more stable estimation about the noise transition. Although it apparently builds a more sophisticated model, we show that the optimization for LCCN deduced by Gibbs sampling can be as efÔ¨Åcient as stochastic gradient descent. SpeciÔ¨Åcally, we propose a novel dynamic label regression method and illustrate its learning procedure in Fig. 1 (see the caption for more details). Note that, although our op timization method iteratively infers the latent true labels and applies them for the training of the classiÔ¨Åer and the noise modeling, only a small amount of extra computational cost is introduced. We theoretically provide the convergence analysis and the generalization bound about LCCN under the label noise, and prove that our optimization approach safeguards the update of noise transition by a minibatch. Beyond the aforementioned property of LCCN, we show that it is easy to extend LCCN under more broader set tings. Concretely, by taking the outofdistribution (OOD)arXiv:2302.09595v1  [cs.LG]  19 Feb 2023JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 imagesclassifier networkBayesian noise modelingsamplinglabels noisy labelsWolfCarMouseRose Safeguardedtransition updateLCCN Fig. 1. Dynamic label regression for LCCN. The images and noisy labels are respectively inputted to the classiÔ¨Åer and the safeguarded Bayesian noise modeling to compute the prediction and the conditional transition. Then, the latent labels are sampled based on their product, and then used for the training of the classiÔ¨Åer and the safeguarded Bayesian noise modeling. All components are trained endtoend in a stochastic fashion. samples into account [17], we increase the dimension of the latent label in LCCN to handle the openset label noise. With a slight modiÔ¨Åcation in the inference process of the latent true labels, the semisupervised learning paradigm is seamlessly integrated into the optimization. Considering the advantages of crossmodel training like coteaching and Dividemix [18, 19], we use two LCCNs to construct a divideLCCN model, which achieves the stateoftheart performance on a range of datasets. In summary, the con tribution of this paper can be categorized as follows. We propose a Latent ClassConditional Noise model to explicitly constrain the learning of the noise transi tion when jointly trained with deep neural networks. Unlike the backpropagation that arbitrarily updates the parameters of the transition layer, we introduce an efÔ¨Åcient dynamic label regression1to maintain the stable optimization in learning with noisy labels. We theoretically show the convergence property of LCCN by means of the dynamic label regression, and characterize the generalization bound of LCCN to uncover the factors for learning with label noise. Simultaneously, we prove that our optimization of the noise transition via a minibatch of samples is safely bounded compared to the backpropagation. We show how to extend the original LCCN without much effort to several variants, i.e., LCCNthat handles openset noisy labels when considering the OOD samples, and LCCN+ that is compatible with the semisupervised learning setting when some pre cisely annotated samples are available, and divideL CCN that leverages the advantage of the crossmodel training [18] to boost the performance. We conduct a range of experiments in the popular CIFAR 10, CIFAR100 datasets and large realworld noisy datasets, Clothing1M and WebVision17. Comprehensive results have demonstrated the superior performance of our model com pared with existing stateoftheart methods. 1. Note that, we use the word ‚Äúregression‚Äù to indicate the noisy label is progressively corrected to the groundtruth in the optimization.The rest part of this paper is organized as follows. Sec tion II brieÔ¨Çy reviews the related research of learning with noisy labels in deep learning. Then, we introduce our Latent ClassConditional model and the dynamic label regression method in Section III, where the corresponding theoretical analysis and the further extension of LCCN is also included. We validate the efÔ¨Åciency of our method over a range of experiments in Section IV . Section V concludes the paper. 2 R ELATED WORK "
355,Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data.txt,"Manually labeled corpora are expensive to create and often not available for
low-resource languages or domains. Automatic labeling approaches are an
alternative way to obtain labeled data in a quicker and cheaper way. However,
these labels often contain more errors which can deteriorate a classifier's
performance when trained on this data. We propose a noise layer that is added
to a neural network architecture. This allows modeling the noise and train on a
combination of clean and noisy data. We show that in a low-resource NER task we
can improve performance by up to 35% by using additional, noisy data and
handling the noise.","For training statistical models in a supervised way, labeled datasets are required. For many natural language processing tasks like partofspeech tag ging (POS) or named entity recognition (NER), every word in a corpus needs to be annotated. While the large effort of manual annotation is reg ularly done for English, for other languages this is often not the case. And even for English, the corpora are usually limited to certain domains like newspaper articles. For tasks in lowresource ar eas there tend to be no or only few labeled words available. Distant supervision and automatic labeling ap proaches are an alternative to manually creating labels. These exploit the fact that frequently large amounts of unannotated texts do exist in the tar geted domain, e.g. from web crawls. The la bels are then assigned using techniques like trans ferring information from highresource languages (Das and Petrov, 2011) or simple lookups inknowledge bases or gazetteers (Dembowski et al., 2017). Once such an automatic labeling system is set up, the amount of text to annotate becomes nearly irrelevant, especially in comparison to man ual annotation. Also, it is often rather easy to ap ply the system to different settings, e.g. by using a knowledge base in a different language. However, while easily obtainable in large amounts, the automatically annotated data usu ally contains more errors than the manually an notated. When training a machine learning algo rithm on such noisy training data, this can result in a low performance. Furthermore, the combination of noisy and clean training instances can perform even worse than just using clean data. In this work, we present an approach to training a neural network with a combination of a small amount of clean data and a larger set of automat ically annotated, noisy instances. We model the noise explicitly using a noise layer that is added to the network architecture. This allows us to di rectly optimize the network weights using stan dard techniques. After training, the noise layer is not needed anymore, removing any added com plexity. This technique is applicable to different classi Ô¨Åcation scenarios and in this work, we apply it to an NER task. To obtain a nonsynthetic, realistic source of noise, we use lookups from gazetteers for automatically annotating the data. In the low resource setting, we show the performance boost obtained from training with both clean and noisy instances and from handling the noise in the data. We also compare to another recent neural network noisehandling approach and we give some more insight into the impact of using additional noisy data and into the learned noise model.arXiv:1807.00745v2  [cs.LG]  22 Jul 20182 Related Work "
46,Weakly Supervised Training of Speaker Identification Models.txt,"We propose an approach for training speaker identification models in a weakly
supervised manner. We concentrate on the setting where the training data
consists of a set of audio recordings and the speaker annotation is provided
only at the recording level. The method uses speaker diarization to find unique
speakers in each recording, and i-vectors to project the speech of each speaker
to a fixed-dimensional vector. A neural network is then trained to map
i-vectors to speakers, using a special objective function that allows to
optimize the model using recording-level speaker labels. We report experiments
on two different real-world datasets. On the VoxCeleb dataset, the method
provides 94.6% accuracy on a closed set speaker identification task, surpassing
the baseline performance by a large margin. On an Estonian broadcast news
dataset, the method provides 66% time-weighted speaker identification recall at
93% precision.","Conventional speaker identiÔ¨Åcation models are usually trained on data where the speech segments corresponding to the target speakers are handannotated. However, the process of hand labelling speech data is expensive and doesn‚Äôt scale well, espe cially if a large set of speakers needs to be covered. This makes such models difÔ¨Åcult to manage and to deploy. For example, this problem is evident in the domain of media monitoring or indexing of large speech corpora where one might need to iden tify the speech segments of a wide and growing range of public Ô¨Ågures, such as politicians, scientists and celebrities. On the other hand, very large collections of audio and video documents are available online. The documents often come with metadata which may provide information about the per sons speaking in the document. For example, many items in the BBC radio archive1have a ‚ÄòContributors‚Äô section which lists the names of the speakers appearing in the programme, often together with names of other crew, such as producers and di rectors. Often, the names of the speakers appearing in the au dio/video document are given in free form text. For example, a video on YouTube that contains an interview with Elon Musk has probably the name ‚ÄòElon Musk‚Äô in the title and may further elaborate on the contents in the video description. However, the metadata rarely provides information about ‚Äòwho spoke when‚Äô, i.e., it doesn‚Äôt provide the necessary information for training speaker identiÔ¨Åcation models in a supervised way. This paper presents a method for training speaker identiÔ¨Å cation models in a weakly supervised manner, relying only on the possibly incomplete set of speakers occurring in each audio document in the training set. That is, the method does not need training data annotation at the speech segment level but rather 1http://www.bbc.co.uk/archive/at the recording level. The method relies on speaker diarization, ivectors [1] and deep neural networks. First, each recording in the training corpus is processed by a speaker diarization mod ule that partitions the recording into homogeneous segments, applies speech/nonspeech detection and clusters the result ing speech segments according to likely (anonymous) speaker turns. Second, ivectors are extracted from each segment and averaged across the speaker turns. Finally, a deep neural net work (DNN) is trained to perform speaker identiÔ¨Åcation, us ing ivectors as inputs and true speaker identities as outputs. Since we don‚Äôt know the true mapping between ivectors and speakers, we cannot use supervised learning using the cross entropy criterion to train the DNN, as it is usually done when training classiÔ¨Åcation models. Instead, we use a technique in spired by a method called expectation regularization [2]: rather than providing the true speaker label for each ivector, we pro vide a speaker distribution over allowed speaker labels for the ivectors of each recording and the training procedure is encour aged to Ô¨Ånd model parameters that predict a similar speaker dis tribution for each recording. That is, the loss function is deÔ¨Åned at the recording level, not at the speaker level. If different speak ers occur intermixed in different recordings in the training data, then the optimal solution to this objective function is a model that predicts the true label for each ivector. The method is sur prisingly robust and easy to implement: it requires deÔ¨Åning a loss function for the DNN that can be implemented in just a few lines of code in any modern deep learning framework. The evaluation of this technique is performed on two very different datasets: the V oxCeleb dataset of YouTube videos [3] and an Estonian broadcast news dataset. V oxCeleb con tains automatically retrieved videos corresponding to over 1000 different US celebrities, with around 18 videos per person. The videos do not have any metadata, other than the name of celebrity that was used for the corresponding YouTube search. The Estonian broadcast news dataset consists of over 6000 recordings of the main evening news program of Estonian na tional radio. Each recording is accompanied with metadata that lists all speakers (both news reporters and interviewees) speak ing in that programme. The two datasets are very different: while the V oxCeleb dataset is relatively heterogeneous, with unconstrained audio recording conditions, it contains roughly equal amount of speech for each speaker identity. The Estonian broadcast news database is much more homogeneous but the amount of speech from different speakers varies greatly: some news reporters appear in thousands of recordings while most speakers occur only once. We show that the proposed method works well for both datasets. The remainder of the paper is organized as follows. Section 2 presents related work. Section 3 describes the main idea of the training algorithm. Experimental results are shown in section 4. Section 5 concludes the paper.arXiv:1806.08621v1  [cs.SD]  22 Jun 20182. Related work "
231,Novel Uncertainty Framework for Deep Learning Ensembles.txt,"Deep neural networks have become the default choice for many of the machine
learning tasks such as classification and regression. Dropout, a method
commonly used to improve the convergence of deep neural networks, generates an
ensemble of thinned networks with extensive weight sharing. Recent studies that
dropout can be viewed as an approximate variational inference in Gaussian
processes, and used as a practical tool to obtain uncertainty estimates of the
network. We propose a novel statistical mechanics based framework to dropout
and use this framework to propose a new generic algorithm that focuses on
estimates of the variance of the loss as measured by the ensemble of thinned
networks. Our approach can be applied to a wide range of deep neural network
architectures and machine learning tasks. In classification, this algorithm
allows the generation of a don't-know answer to be generated, which can
increase the reliability of the classifier. Empirically we demonstrate
state-of-the-art AUC results on publicly available benchmarks.","Deep learning (DL) algorithms have successfully solved realworld classiÔ¨Åcation problems from a variety of Ô¨Åelds, including recognizing handwritten digits and identifying the presence of key diagnostic features in medical images [ 18,16]. A typical classiÔ¨Åcation challenge for a DL algorithm consists of training the algorithm on an example data set, then using a separate set of test data to evaluate its performance. The aim is to provide answers that are as accurate as possible, as measured by the true positive rate (TPR) and the true negative rate (TNR). Many DL classiÔ¨Åers, particularly those using a softmax function in the very last layer, yield a continuous score, h; A step function is used to map this continuous score to each of the possible categories that are being classiÔ¨Åed. TPR and TNR scores are then generated for each separate variable that is being predicted by setting a threshold parameter that is applied when mapping hto the decision. Values above this threshold are mapped to positive predictions, while values below it are mapped to negative predictions. The ROC curve is then generated from these pairs of TPR/TPN scores. The performance of binary 1arXiv:1904.04917v1  [stat.ML]  9 Apr 2019classiÔ¨Åers, performance is often evaluated by calculating the area under the ROC curve (AUC) [ ?, hanley1982meaning] the higher the AUC, the better the performance of the algorithm. Many studies show that the AUC achieved by DL algorithms is higher than most, if not all, of the alternative classiÔ¨Åers. Although they achieve high scores for metrics such as AUC, DL algorithms are notorious for being ‚Äúblack box‚Äù models, as it is diÔ¨Écult to obtain insight into how the algorithm arrived at its conclusion. This makes them less reliable, particularly for applications where databased decisions are a Ô¨Årm requirement. One way to mitigate this problem when applying DL algorithms to these datadriven applications is to provide a measure of the classiÔ¨Åcation uncertainty, or the conÔ¨Ådence one has in the classiÔ¨Åcation prediction, along with the prediction of the outcome. This can be accomplished by training a series of similar models that diÔ¨Äer in their initialization parameters, and then calculating the variance between the probabilities for each possible outcome across the diÔ¨Äerently initialized networks. The ground truth of the uncertainty, then, is the variance between these similar models. Reporting a conÔ¨Ådence level along side the prediction helps the model‚Äôs end user to interpret and build trust in the model‚Äôs performance. For Example, this is a critical requirement for algorithms deployed in a medical setting [ 2], where a moderately positive prediction with high uncertainty has very diÔ¨Äerent prognosis and treatment implications than does the same prediction when the uncertainty is low. We show an example of both low variance and high variance predictions, calculated using the ground truth as deÔ¨Åned , in Figure 1a. When assessing uncertainty in DL algorithms such classiÔ¨Åers are frequently permitted to return a ‚Äúdon‚Äôt know‚Äù answer for very low conÔ¨Ådence predictions in the test data. This allows the algorithm to be judged only on the responses in the ‚Äúdo know‚Äù portion of the data set. Consequently, the algorithm generates overall higher quality predictions, while leaving humans to interpret samples for which it would generate poor quality predictions, similar to a triage system The continuous variable hthat is output by the last layer of a DL with a softmax function provides the likelihood of an outcome. However, this likelihood estimation should not be mistaken for a conÔ¨Ådence level in the prediction (see [6]). Instead, recent studies have combined the DL regularization technique of dropout with Bayesian modeling to derive uncertainty estimates in DL classiÔ¨Åers [6,7]. Bayesian approaches provides a natural framework for estimating the uncertainty of the prediction. Furthermore, dropout is a regularization technique that uses an ensemble of models to create highperformance classiÔ¨Åers [ 22,15]. The interpretation of dropout through a Bayesian lens enables derivation of uncertainty estimates in DL classiÔ¨Åers. Within this framework the classiÔ¨Åcation score,h, describes the probability of predicting the correct class. The ROC curve for the model is measured on an averaged value of h, rather than a single value, and the uncertainty is estimated based on the variance of h. This uncertainty measure has been shown to correlate with the performance of the classiÔ¨Åer. Using the averaged hto predict the outcome was suÔ¨Écient to increase classiÔ¨Åcation accuracy in some cases [11]. In this paper, we present a novel method for estimating the uncertainty of 2a DL classiÔ¨Åer. We propose a framework that assigns probability distributions to thinned networks,(i.e., neural networks with a subset of neurons removed), based on the performance of their crossentropy loss function across the test data set. The main contributions of this work are threefold. First, we introduce a statisticalmechanics framework that assigns probability distributions over the ensemble of thinned networks of the dropout. This framework has a Ô¨Çexible variable,, which represents the inverse temperature, and a statistical Ô¨Çuctuation scale. When set to zero, it results in a uniform distribution assumption over the thinned networks and the framework collapses to a Gaussian process. In contrast, a Ô¨Ånite inverse temperature results in a nonuniform distribution, and the framework enables interpretation and reasoning regarding uncertainty. Second, we present a new algorithm, called Loss Variance Monte Carlo Estimate (LoVME), which is based on estimations of the loss variance in the case of a Ô¨Ånite through Monte Carlo sampling. Finally, we illustrate the beneÔ¨Åts of deriving uncertainty through the LoVME algorithm in scenarios where the classiÔ¨Åer can yield a don‚Äôtknow answer. We use the MNIST [ 14] and CIFAR [ 13] data sets to show the performance of our algorithm, and compare our results to stateof theart algorithms for uncertainty in DL. The rest of the paper is organized as follows: First, we introduce related work. We focus on two related bodies of knowledge: recent proposed methods to derive uncertainty measures for test data using the Bayesian interpretation of dropout, and the existing statisticalmechanics frameworks for analyzing distributions over an ensemble of models. Next, we introduce a new methodology for interpreting the ensemble of Neural Networks (NNs) generated by dropout. We present a derivation of a new algorithm, LoVME, which uses Monte Carlo sampling to estimate the loss variance; through this loss variance, it provides estimates of the predicted variable hand the uncertainty of the prediction. We evaluate the performance of the algorithm on multiple data sets and compare it to stateoftheart algorithms. We conclude with a discussion of the advantages of the proposed framework and algorithm, as well as potential extensions and open questions. We conclude with a discussion of the advantages of the proposed framework and algorithm, as well as potential extensions and open questions. 2 Related Work "
108,Training Deep Neural Networks on Noisy Labels with Bootstrapping.txt,"Current state-of-the-art deep learning systems for visual object recognition
and detection use purely supervised training with regularization such as
dropout to avoid overfitting. The performance depends critically on the amount
of labeled examples, and in current practice the labels are assumed to be
unambiguous and accurate. However, this assumption often does not hold; e.g. in
recognition, class labels may be missing; in detection, objects in the image
may not be localized; and in general, the labeling may be subjective. In this
work we propose a generic way to handle noisy and incomplete labeling by
augmenting the prediction objective with a notion of consistency. We consider a
prediction consistent if the same prediction is made given similar percepts,
where the notion of similarity is between deep network features computed from
the input data. In experiments we demonstrate that our approach yields
substantial robustness to label noise on several datasets. On MNIST handwritten
digits, we show that our model is robust to label corruption. On the Toronto
Face Database, we show that our model handles well the case of subjective
labels in emotion recognition, achieving state-of-the- art results, and can
also benefit from unlabeled face images with no modification to our method. On
the ILSVRC2014 detection challenge data, we show that our approach extends to
very deep networks, high resolution images and structured outputs, and results
in improved scalable detection.","Currently the predominant systems for visual object recognition and detection (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014) use purely supervised training with regularization such as dropout (Hinton et al., 2012) to avoid overÔ¨Åtting. These systems do not account for missing labels, subjective labeling or inexhaustively annotated images. However, this assumption often does not hold, especially for very large datasets and in highresolution images with complex scenes. For example, in recognition, the class labels may be missing; in detection, the objects in the image may not all be localized; in subjective tasks such as facial emotion recognition, humans may not even agree on the class label. As training sets for deep networks become larger (as they should), the problem of missing and noisy labels becomes more acute, and so we argue it is a fundamental problem for scaling up vision. In this work we propose a simple approach to hande noisy and incomplete labeling in weakly supervised deep learning, by augmenting the usual prediction objective with a notion of perceptual consistency. We consider a prediction consistent if the same prediction is made given similar per cepts, where the notion of similarity incorporates features learned by the deep network. One interpretation of the perceptual consistency objective is that the learner makes use of its rep resentation of the world (implicit in the network parameters) to match incoming percepts to known 1arXiv:1412.6596v3  [cs.CV]  15 Apr 2015Accepted as a workshop contribution at ICLR 2015 categories, or in general structured outputs. This provides the learner justiÔ¨Åcation to ‚Äúdisagree‚Äù with a perceptuallyinconsistent training label, and effectively relabel the data while training. More accurate labels may lead to a better model, which allows further label cleanup, and the learner bootstraps itself in this way. Of course, too much skepticism of the labels carries the risk of ending up with a delusional agent, so it is important to balance the tradeoff between prediction and the learner‚Äôs perceptual consistency. In our experiments we demonstrate that our approach yields substantial robustness to several types of label noise on several datasets. On MNIST handwritten digits (LeCun & Cortes, 1998) we show that our model is robust to label corruption. On the Toronto Face Database (Susskind et al., 2010) we show that our model handles well the case of subjective labels in emotion recognition, achieving stateoftheart results, and can also beneÔ¨Åt from unlabeled face images with no modiÔ¨Åcation to our method. On the ILSVRC2014 detection challenge data (Russakovsky et al., 2014), we show that our approach improves singleshot person detection using a MultiBox network (Erhan et al., 2014), and also improves performance in full 200way detection using MultiBox for region proposal and a deep CNN for postclassiÔ¨Åcation. In section 2 we discuss related work, in section 3 we describe our method along with a probabilistic interpretation and in section 4 we present our results. 2 R ELATED WORK "
346,TabGSL: Graph Structure Learning for Tabular Data Prediction.txt,"This work presents a novel approach to tabular data prediction leveraging
graph structure learning and graph neural networks. Despite the prevalence of
tabular data in real-world applications, traditional deep learning methods
often overlook the potentially valuable associations between data instances.
Such associations can offer beneficial insights for classification tasks, as
instances may exhibit similar patterns of correlations among features and
target labels. This information can be exploited by graph neural networks,
necessitating robust graph structures. However, existing studies primarily
focus on improving graph structure from noisy data, largely neglecting the
possibility of deriving graph structures from tabular data. We present a novel
solution, Tabular Graph Structure Learning (TabGSL), to enhance tabular data
prediction by simultaneously learning instance correlation and feature
interaction within a unified framework. This is achieved through a proposed
graph contrastive learning module, along with transformer-based feature
extractor and graph neural network. Comprehensive experiments conducted on 30
benchmark tabular datasets demonstrate that TabGSL markedly outperforms both
tree-based models and recent deep learning-based tabular models. Visualizations
of the learned instance embeddings further substantiate the effectiveness of
TabGSL.","Tabular data is a common data type in the real world [ 5]. Among various machine learning (ML) algorithms, gradient boosted decision trees (GBDTs) have been one type of the most competitive ML to handle tabular data for many years [ 18,27,19]. Recently, many studies also tried applying deep learning (DL) methods to tabular data to improve prediction performance, which has become popular in academics and the industry [ 18,24]. Studies found that DL methods designed for other domains are also helpful for tabular data, such as FiGNN [ 36], Table2Graph [ 62], and T2GFormer [ 57]. In these methods, feature interactions are taken into account. However, associations among instances are usually ignored in these approaches. Some instances may share similar patterns of features correlated with the prediction target, which can be modeled by a graph. Recently, studies on graphs have flourished in various domains [ 55]. Due to its powerful capability to learn latent representations from relational structure, DL methods are applied to modeling graphs and lead to a thriving research topic, graph neural networks (GNNs) [ 55,37]. GNNs learn latent representations for each node by aggregating information from the node‚Äôs neighbors on the given graph structure. However, graph structures are usually generated from complex systems and thus are inevitably noisy [ 63]. Specifically, there might be redundant information in a graph structure. Some connections might be missing or even incorrect [ 9]. Without a doubt, these noises on the graph structure are harmful to GNN‚Äôs performance considerably. Besides, GNN is not available when modeling data without an apparent graph structure, such as tabular data, which is very common in the real world [ 37]. Such issues lead to studies focusing on graph structure learning (GSL), which aims Preprint. Under review.arXiv:2305.15843v1  [cs.LG]  25 May 2023to learn a reliable graph structure with less noise for GNNs. Although there have been GSL methods proposed, they mainly focused on refining an existing graph structure. Not many discussed learning graph structures purely from tabular data, where there is no available graph initially [ 9,37,63]. GNNs can model the latent associations among instances, which cannot be captured by existing treebased models. To this end, learning a credible graph structure from the given tabular dataset is crucial. Most of the existing DL methods modeling tabular data consider feature interactions. Feature reconstruction, such as VIME [ 58] and TabNet [ 2], and contrastive learning, such as SubTab [ 49] and SCARF [ 3], are welladopted approaches. However, the latent associations among instances are almost ignored. Thus, patterns of features and the response variable that some similar instances (i.e., neighbors) share cannot be identified and exploited in tabular data learning. Data instances can be correlated with each other in terms of their features. For example, users with similar profiles or online behaviors tend to have similar preferences for ads or items [ 44,23,15]. Patients with similar clinical data or symptoms have a higher potential to suffer from similar diseases [ 10,26,6]. To better represent instances for downstream tasks, rather than solely employing each instance‚Äôs self features, it is crucial to model the correlation between instances. The key idea is to exploit such correlation to learn higherquality feature representations of instances, i.e., instances with similar labels are close to one another while those with different labels are pushed away from each other in the embedding space. With proper learning of the graph structure that depicts the relationships between instances, GNN would be a good fit to let instances learn to represent each other. That said, we require both GSL and GNNs to model instance association. Regarding the learning of graph structure for tabular data prediction, three challenges need to be tackled. First, existing GSL methods require a noisy graph as the input for refinement and adjustment, but tabular data contains no graph topology in essence. We need to learn the graph structure from scratch. Second, both modeling feature interactions and instance associations are essential in tabular data learning. How to simultaneously consider them both for tabular data in a unified framework is unclear. Third, in addition to learning the graph structure from tabular data, we further need to jointly train a graph neural network to obtain feature representations of instances for final predictions. In this work, we propose a novel graph machine learning model, TabularGraph Structure Learning (TabGSL ), for tabular data prediction. The key idea is to learn the graph structure from a given tabular dataset so that the latent correlation between instances can be modeled. We propose a novel contrastive learning mechanism to learn the graph structure from tabular data. The intuition is to construct a teacher, which possesses confident knowledge about label knowledge among instances, to guide a student graph learner to continuously adjust the graph structure. Besides, to further capture the interactions between features, we adopt the transformer with tokenized features to produce feature embeddings for GSL. The learning of tabular graph structure is jointly trained with a GNN module to generate final representations of instances. We summarize the contributions of this work as follows. (a)We revisit the tabular data prediction task from the perspective of graph structure learning and graph neural networks, and highlight the potential of modeling the associations between instances and capturing the interactions between features. (b)We present a new graphbased solution, Tabular Graph Structure Learning (TabGSL), to better perform tabular data prediction by simultaneously learning instance correlation and feature interaction in a united framework. A novel graph contrastive learning module is devised to fulfill the goal. (c)Experiments conducted on 30 benchmark tabular datasets exhibit that the proposed TabGSL significantly and consistently outperforms treebased models and recent deep learningbased tabular models. Visualization plots also show the effectiveness of the learned instance embeddings. This paper is organized as below. We review relevant studies in Section 2, followed by the description of the problem statement in Section 3. The methods and tasks involved in learning graph structures for tabular data prediction are presented in Section 4. We give the evaluation plan and discuss the experimental results in Section 5. Section 6 concludes this work. 2 Related Work "
190,Training Convolutional Networks with Noisy Labels.txt,"The availability of large labeled datasets has allowed Convolutional Network
models to achieve impressive recognition results. However, in many settings
manual annotation of the data is impractical; instead our data has noisy
labels, i.e. there is some freely available label for each image which may or
may not be accurate. In this paper, we explore the performance of
discriminatively-trained Convnets when trained on such noisy data. We introduce
an extra noise layer into the network which adapts the network outputs to match
the noisy label distribution. The parameters of this noise layer can be
estimated as part of the training process and involve simple modifications to
current training infrastructures for deep networks. We demonstrate the
approaches on several datasets, including large scale experiments on the
ImageNet classification benchmark.","In recent years, Convolutional Networks (Convnets) (LeCun et al., 1989; Lecun et al., 1998) have shown impressive results on image classiÔ¨Åcation tasks (Krizhevsky et al., 2012; Simonyan & Zisser man, 2014). However, this achievement relies on the availability of large amounts of labeled images, e.g. ImageNet (Deng et al., 2009). Labeling images by hand is a laborious task and impractical for many problems. An alternative approach is to use labels that can be obtained easily, such as user tags from social network sites, or keywords from image search engines. The catch is that these labels are not reliable so they may contain misleading information that will subvert the model during training. But given the abundance of tasks where noisy labels are available, it is important to understand the consequences of training a Convnet on them, and this is one of the contributions of our paper. For image classiÔ¨Åcation in realworld settings, two types of label noise dominate: (i) label Ô¨Çips , where an example has erroneously been given the label of another class within the dataset and (ii) outliers , where the image does not belong to any of the classes under consideration, but mistakenly has one of their labels. Fig. 1 shows examples of these two cases. We consider both scenarios and explore them on a variety of noise levels and datasets. Contrary to expectations, a standard Convnet model (from Krizhevsky et al. (2012)) proves to be surprisingly robust to both types of noise. But inevitably, at high noise levels signiÔ¨Åcant performance degradation occurs. Consequently, we propose a novel modiÔ¨Åcation to a Convnet that enables it to be effectively trained on data with high level of label noise. The modiÔ¨Åcation is simply done by adding a constrained linear ‚Äúnoise‚Äù layer on top of the softmax layer which adapts the softmax output to match the noise distribution. We demonstrate that this model can handle both label Ô¨Çip and outlier noise. As it is a linear layer, both it and the rest of the model can be trained endtoend with conventional back propagation, thus automatically learning the noise distribution without supervision. The model is also easy to implement with existing Convnet libraries (Krizhevsky, 2012; Jia et al., 2014; Collobert et al., 2011) and can readily scale to ImageNetsized problems. 1arXiv:1406.2080v4  [cs.CV]  10 Apr 2015Accepted as a workshop contribution at ICLR 2015 horsedogcathorsecatdogcatdog horsecatcathorsecatdogdogcatLabel Ô¨Çip noiseOutlier noise Figure 1: A toy classiÔ¨Åcation example with 3 classes, illustrating the two types of label noise encoun tered on real datasets. In the label Ô¨Çip case, the images all belong to the 3 classes, but sometimes the labels are confused between them. In the outlier case, some images are unrelated to the classiÔ¨Åcation task but possess one of the 3 labels. 2 R ELATED WORK "
169,Approximating Wisdom of Crowds using K-RBMs.txt,"An important way to make large training sets is to gather noisy labels from
crowds of non experts. We propose a method to aggregate noisy labels collected
from a crowd of workers or annotators. Eliciting labels is important in tasks
such as judging web search quality and rating products. Our method assumes that
labels are generated by a probability distribution over items and labels. We
formulate the method by drawing parallels between Gaussian Mixture Models
(GMMs) and Restricted Boltzmann Machines (RBMs) and show that the problem of
vote aggregation can be viewed as one of clustering. We use K-RBMs to perform
clustering. We finally show some empirical evaluations over real datasets.","There has been considerable amount of work on learning when labeling is expensive, such as tech niques on transductive inference and active learning. With the emergence of crowdsourcing services, like Amazon Mechanical Turk, labeling costs in many applications have dropped dramatically. Large amounts of labeled data can now be gathered at low price. Due to a lack of domain expertise and misaligned incentives, however, labels provided by crowdsourcing workers are often noisy. To overcome the quality issue, each item is usually simultaneously labeled by several workers, and then we aggregate the multiple labels with some manner, for instance, majority voting. An advanced approach for label aggregation is suggested by Dawid and Skene[1]. They assume that each worker has a latent confusion matrix for labeling. The offdiagonal elements represent the probabilities that a worker mislabels an arbitrary item from one class to another while the diagonal elements correspond to her accuracy in each class. Worker confusion matrices and true labels are jointly estimated by maximizing the likelihood of observed labels. One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4]. The method of DawidSkene (1979) implicitly assumes that a worker performs equally well across all items in a common class. In practice, however, it is often the case that one item is more difÔ¨Åcult to label than another. To address this heterogeneous issue, Zhou et al.(2012)[5] propose a minimax entropy principle for crowdsourcing. It results in that each item is associated with a latent confusion vector besides a latent confusion matrix for each worker. Observed labels are determined jointly by worker confusion matrices and item confusion vectors through an exponential family model. Moreover, it turns out that the probabilistic labeling model can be equivalently derived from a natural assumption of objective measurements of worker ability and item difÔ¨Åculty. Such kinds of objectivity arguments have been widely discussed in the literature of mental test theory [6][7]. All the above approaches are for aggregating multiclass labels and In many scenarios, the labels are ordinal. Zhou et. al. (2014)[8] proposed a work to aggregate votes using minimax conditional entropy for ordinal labels. Most of the methods use statistical methods to aggregate the observed labels by transforming them to some probability or entropy measures. But, there has been no work that operates directly on the observed labels. We present a method to learn the aggregates of the votes using clustering. We Ô¨Årst show the formulation that allows us to use clustering as an approximation of the vote aggregationarXiv:1611.05340v2  [cs.LG]  17 Nov 2016stratagem. We Ô¨Årst draw a parallel between the Restricted Boltzmann Machine (RBM) learning and the Expectation Maximization (EM) algorithm of the DavidSkene algorithm and then show that GaussianSoftmax RBMs[9] can be approximated by a Gaussian Mixture Model (GMM), whose speciÔ¨Åc conditions lead to a direct mapping to the traditional Kmeans algorithm[10][11]. To then elucidate the clustering paradigm, we perform clustering using the KRBM model as proposed in [14]. 2 Related Work "
323,Continuous Adaptation of Multi-Camera Person Identification Models through Sparse Non-redundant Representative Selection.txt,"The problem of image-base person identification/recognition is to provide an
identity to the image of an individual based on learned models that describe
his/her appearance. Most traditional person identification systems rely on
learning a static model on tediously labeled training data. Though labeling
manually is an indispensable part of a supervised framework, for a large scale
identification system labeling huge amount of data is a significant overhead.
For large multi-sensor data as typically encountered in camera networks,
labeling a lot of samples does not always mean more information, as redundant
images are labeled several times. In this work, we propose a convex
optimization based iterative framework that progressively and judiciously
chooses a sparse but informative set of samples for labeling, with minimal
overlap with previously labeled images. We also use a structure preserving
sparse reconstruction based classifier to reduce the training burden typically
seen in discriminative classifiers. The two stage approach leads to a novel
framework for online update of the classifiers involving only the incorporation
of new labeled data rather than any expensive training phase. We demonstrate
the effectiveness of our approach on multi-camera person re-identification
datasets, to demonstrate the feasibility of learning online classification
models in multi-camera big data applications. Using three benchmark datasets,
we validate our approach and demonstrate that our framework achieves superior
performance with significantly less amount of manual labeling.","Person identiÔ¨Åcation/recognition across cameras is an important problem in many surveillance and security applications, and requires the ability to handle very large data volumes. An automated solution of this problem is to use images of persons to pro vide identities to the images based on learned models that describe his/her appearance. Most existing solutions depend on learning a static model on tediously labeled training data. An example of such a task is person reidentiÔ¨Åcation [1, 2, 3, 4] which is the task of identifying and monitoring people moving across a number of nonoverlapping cameras. Considering the time and labor involved in labeling the training data manu ally, scalability to large numbers of persons remains an issue. Also, the learned models being static, cannot adapt to new images that may be available over time. Some recent semisupervised [5] and unsupervised methods [6, 7, 8] tried to ex plore saliency information or weighted features or learning a discriminative null space in a reidentiÔ¨Åcation scenario. However, none of these works assume a continuous learning setting. Moreover, it can be seen that unsupervised methods give signiÔ¨Åcantly lower performance compared to supervised methods [6, 7, 8]. A next alternative is to involve a human in the loop but at the same time efforts should be made to keep the human annotation to a minimum. Active learning [9] is a natural choice for reducing labeling effort by asking for labels only on a few but informative samples (called the active samples), rather than seeking labels either for samples chosen randomly from a set or for the whole set. In this paper, we explore the question of learning person identi Ô¨Åcation models online in a multicamera settings with limited labeling effort. We argue that, in order to truly reduce the labeling cost we need to choose a sparse but informa tive set of samples to be labeled. As only a small part of the whole data is annotated, the annotation effort is reduced considerably compared to annotating the whole dataset. Active learning has been successfully applied to many computer vision problems in 2cluding tracking [10], object detection [11], image [12] and video segmentation [13], image or scene classiÔ¨Åcation [14, 15, 16] and activity recognition [17, 18, 19] How ever, these methods deal with data coming from single source. It is not trivial to extend traditional active learning methods for an application scenario where multisensor data is involved. It is a natural challenge to select a few informative samples yet cover as much appearance variation as possible across multiple cameras in such a scenario. Apart from high cost of labeling the training data, all the data may not also be available at the very outset. A static pretrained model can not adapt to the changing dynamics of the incoming data. In this work, we will address both the abovementioned scenarios a). selection of a manageable set of informative samples for labeling and b). doing so in an online manner where all the training data is not available a priori. For this purpose, we propose an iterative framework which, starting with a pool of unlabeled images, progressively and judiciously selects the most informative set of images  termed as the ‚Äò representative ‚Äô images for labeling with minimal overlap with previously labeled images. Ideally, a set of representative images are ‚Äúrepresentatives‚Äù of a dataset because this set possesses most of the variabilities of the dataset within itself. On the other hand, without any label information, the representative images are some of the most confusing samples in the whole dataset by the same trait. Thus anno tating such representatives enriches the model by injecting valuable information with a reasonable labeling effort. We also use a structure preserving sparse reconstruction based classiÔ¨Åer to reduce the training burden typically seen in discriminative classiÔ¨Åers. The use of a sparse classiÔ¨Åer enables an online update of the identiÔ¨Åcation framework involving only new samples without requiring to train from scratch whenever new batch of data arrives. This pipeline leads to a framework for online update of the classiÔ¨Åers involving only the incorporation of new labeled data rather than any expensive training phase. Identifying and eliminating redundant samples is especially important in such an online scenario since reducing redundancy implies more information gain at the cost of less labeling effort. Thus the proposed work addresses the following question: Is it possible to select a sparse set of nonredundant training images progressively in an online setting for annotation from multisensor data while maintaining good identiÔ¨Å cation performance? 3We demonstrate the effectiveness of our proposed approach on datasets in per son reidentiÔ¨Åcation (although our problem setting is different than the traditional re identiÔ¨Åcation framework). There are many reasons for it. Using the reidentiÔ¨Åcation datasets allows us to demonstrate the effectiveness of the online representative selection framework where due to a multicamera setting, large intraperson variation is preva lent. Also, these datasets represent uncontrolled settings where we are not dependent on the availability of good quality facial shots. 1.1. Motivation behind the Proposed Approach The representatives or samples chosen iteratively for labeling can have two types of redundancies. Firstly, in each iteration, the chosen representatives may have many images of the same person. Secondly, representatives selected in subsequent iterations may also have overlap with the representatives chosen earlier for labeling. The Ô¨Årst type of redundancy is termed as the ‚Äòintraiteration redundancy‚Äô while the second type is termed as the ‚Äòinteriteration redundancy‚Äô. ‚ÄòIntraiteration redundancy‚Äô is restricted by exploiting the fact that redundant samples in any iteration are very close neighbors in the feature space. Without any feedback about the already chosen representatives, any representative selection strategy may select images of the same person as repre sentatives in subsequent iterations. Using a similar redundancy reduction strategy of looking for close neighbors as above, we will be able to Ô¨Ålter out samples redundant to the already labeled ones in previous iterations. However, using such a strategy to reduce ‚Äòinteriteration redundancy‚Äô will prohibit the information gain as images of a person from multiple cameras will be hard to come by. We tackle this situation by en forcing diversity among the selected representatives as information about the already chosen samples in previous iterations are fed back while choosing subsequent samples to be labeled. Variabilities are not only caused by the presence of different people but the same person may appear differently in different cameras. These two different types of variabilities make nonredundant representative selection a challenge in scenarios where there are multiple sources of data as is the case with multicamera person iden tiÔ¨Åcation. The proposed method exploits these variabilities by choosing diverse but small set of representatives from multiple cameras (ref. section 4.3.2) while discarding 4similar images of the same person which primarily comes from the same camera (ref. section 4.3.1). Such a representative selection problem is formulated as a convex optimization that minimizes the cost of representing an unlabeled pool with a sparse set of representa tives as well as one that minimizes the redundancy with the representatives selected earlier. Experiments on three benchmark datasets show that annotating the small but informative set of representative images reduces the labeling effort considerably, main taining reasonable identiÔ¨Åcation performance. Apart from the huge labeling effort, another factor that is a challenge for a scal able solution of the problem is the generally exponential increase of training time with the number of training samples for traditional discriminative classiÔ¨Åers ( e.g., SVM or random forest). These classiÔ¨Åers have to be retrained from scratch after each batch of representative selection and annotation in such repetitive active learning strategy. The generally super linear time complexity of the traditional discriminative classiÔ¨Åers makes them unsuitable for use in such a scenario. Though incremental learning based classiÔ¨Åers [20] can update the model without retraining from scratch, their performance is limited by the condition of knowing the number of classes from the start. Motivated by the recent progress of sparse coding based classiÔ¨Åers [21, 22], we employ a structure preserving sparse dictionary for classiÔ¨Åcation. Such a classiÔ¨Åcation strategy is helpful as updating the model with newly labeled data means simply adding the new samples with labels without making any changes to the existing dictionary elements made of the already labeled samples. This model update strategy not only helps in reducing the training time signiÔ¨Åcantly by avoiding the need for retraining but also enables the operation of the framework without assuming any knowledge of the number of classes. Thus, in summary, the proposed framework uses two convex optimization based strategies to select a few informative but nonredundant samples for labeling and to update a person identiÔ¨Åcation model online . The rest of the paper is organized as follows. Section 2 discusses the related works and our contributions. An overview of the proposed approach is given in section 3 . The details about approach, as nonredundant representative selection, and the use of structure preserving sparse coding based classiÔ¨Åcation are described in section 4. 5Experimental results and comparisons are shown in section 5. Finally, conclusions are drawn in section 6. 2. Related Works and Our Contributions "
244,Factorized Distillation: Training Holistic Person Re-identification Model by Distilling an Ensemble of Partial ReID Models.txt,"Person re-identification (ReID) is aimed at identifying the same person
across videos captured from different cameras. In the view that networks
extracting global features using ordinary network architectures are difficult
to extract local features due to their weak attention mechanisms, researchers
have proposed a lot of elaborately designed ReID networks, while greatly
improving the accuracy, the model size and the feature extraction latency are
also soaring. We argue that a relatively compact ordinary network extracting
globally pooled features has the capability to extract discriminative local
features and can achieve state-of-the-art precision if only the model's
parameters are properly learnt. In order to reduce the difficulty in learning
hard identity labels, we propose a novel knowledge distillation method:
Factorized Distillation, which factorizes both feature maps and retrieval
features of holistic ReID network to mimic representations of multiple partial
ReID models, thus transferring the knowledge from partial ReID models to the
holistic network. Experiments show that the performance of model trained with
the proposed method can outperform state-of-the-art with relatively few network
parameters.","Person ReidentiÔ¨Åcation is aimed at identifying the same person across videos captured from different cameras. It is a challenging task mainly due to factors such as back ground clutter, pose, illumination and camera point of view variations. As the prosperous of deep learning, handcrafte d features are replaced by features learned from data by deep convolutional neural networks (CNNs). With feature learn ing, deep network can build up attention mechanism to re duce interference of background clutter or occlusion, and extract discriminative poseinvariant features. It is generally believed that traditional networks extract  ing globally pooled features (like IDE [37]) can only learn Figure 1. SpeciÔ¨Åcation of 7 Views. Holistic : pedestrian image resize to 256 √ó128. Partial Group 1 : Uniformly divide image into 4 stripes to compose Partial Views: Up1(1/42/4), Mid1 (2/4 3/4),Dn1(3/44/4). Partial Group 2 : Uniformly divide image into 7 stripes to compose Partial Views: Up2(1/73/7), Mid2(3/7 5/7), Dn2(5/77/7). All partial images resize to 224 √ó224. to extract salient features during training, and it is hard t o extract local features due to their weak attention mecha nisms. In order to strengthen attention mechanisms in ReID networks, a lot of approaches have recently been proposed. These works commonly employ spatial partition, body parts detection, pose estimation and so on, pushing the perfor mance of ReID to a new level. However, with the complex network structures or huge models these approaches use, ReID models are hard to be commercially used in a large scale or deployed in mobile devices. Therefore, we want to address the problem in a different way. Why traditional ReID networks extracting globally pooled features are in low performance? As the number of identities in the training set is small relative to the numbe r of combinations of latent discriminative features, the com  monly used identity classiÔ¨Åcation loss prone to overfeat to a small subset of discriminative features which are salient . But identities in testing set are totally different from tra in ing set, as a result, the limited features that perfectly cla ssify training identities are not enough to distinguish testing i n 1dividuals. Even if some visual appearances of body parts in testing images are similiar to local areas in training image s, the model still prone to ignore them if they are inconspic uous, because these hard features need not to be leant to lower the loss fucntion. If we train partial ReID models with highresolution par tial images, each partial model can discover more discrim inative features in its restricted region than those found b y holistic model in the same area. Each partial ReID model is an expert for a speciÔ¨Åc region, so the total amount of knowl edge contained in the ensemble of separately trained partia l ReID models will exceed the holistic model. We aim to transfer the knowledge from multiple partial ReID models to a holistic model, while avoiding feature concatenation i n order not to make the dimension of student‚Äôs retrieval fea tures become unacceptably large when the number of teach ers is keep increasing. Our approach is: Firstly, we utilize several sepa rately trained teacher models (holistic or partial) to gen erate enhanced representation features, named Supervisory Representations ( SRs). Secondly, SRis regarded as Ô¨Åne grained highdimensional soft attributes, and the task of a t tributes training corresponding to each teacher is added to the holistic student‚Äôs training system to improve the featu re maps of the student. Thirdly, SRis regarded as anchors for each sample in different feature space of partial ReID rep resentation, and the role of metric learning is achieved by factorize the student model‚Äôs representation to each of the SR‚Äôs feature space and mimic these anchors. The contributions of this paper are: 1) We propose a novel knowledge transfer method, named Factorized Distillation (FD), which can train a holis tic student model by distilling an ensemble of partial mod els, using the way of factorization instead of feature con catination to result in compact retrieval features even whe n the number of teachers is large. 2) Trained by FD, ordinary holistic networks extracting global features can also generate strong attention mech anism, and can directly extract partial features from im ages of high pose variation without incorporating addition al bodyparts detection or pose estimation networks. 3) Extensive experiments on ReID datasets demonstrate that the proposed method can outperform stateoftheart with relatively few network parameters. 2. Related Work "
161,Noise-resistant Deep Metric Learning with Ranking-based Instance Selection.txt,"The existence of noisy labels in real-world data negatively impacts the
performance of deep learning models. Although much research effort has been
devoted to improving robustness to noisy labels in classification tasks, the
problem of noisy labels in deep metric learning (DML) remains open. In this
paper, we propose a noise-resistant training technique for DML, which we name
Probabilistic Ranking-based Instance Selection with Memory (PRISM). PRISM
identifies noisy data in a minibatch using average similarity against image
features extracted by several previous versions of the neural network. These
features are stored in and retrieved from a memory bank. To alleviate the high
computational cost brought by the memory bank, we introduce an acceleration
method that replaces individual data points with the class centers. In
extensive comparisons with 12 existing approaches under both synthetic and
real-world label noise, PRISM demonstrates superior performance of up to 6.06%
in Precision@1.","Commonly resulting from human annotation errors or imperfect automated data collection, noisy labels in training data degrade the predictive performance of models trained on them [11, 47, 16]. Manual inspection and correction of labels are labourintensive and hence scale poorly to large datasets. Therefore, training techniques that are robust to incorrect labels in training data play an important role in realworld applications of machine learning. To date, most works on noiseresistant neural networks [11, 17, 31, 52, 47, 48, 16] focus on image classiÔ¨Åcation. Little research effort has been devoted to noiseresistant deep metric learning (DML). The goal of DML is to learn a distance metric that maps similar pairs of data points close together and dissimilar pairs far apart, based on a predeÔ¨Ånednotion for similarity. DML Ô¨Ånds diverse applications such as image retrieval [18, 10, 33], landmark identiÔ¨Åcation [49], and selfsupervised learning [25]. Pairbased loss functions encourages DML networks to distinguish a similar pair of data points from one or more dissimilar pairs. Large batch sizes often lead to improved performance [6, 46, 5], as larger batches are more likely to contain informative examples. Pushing the idea of large batches to an extreme, [46] collects all positive and negative data samples from a memory bank. However, in the pres ence of substantial noise, indiscriminate use of all samples could lower performance. Alternatively, [26] uses learnable class centers to replace individual data samples in order to reduce computational complexity. Nonetheless, the cluster centers can also be sensitive to outliers and label noise. We propose a noiseresistant deep metric learning algo rithm, Probabilistic Rankingbased Instance Selection with Memory (PRISM), which works with both the memory bank approach and the classcenter approach. PRISM computes the probability that a label is clean based on the similarities between the data point and other data points using features extracted during the last several training iterations. This may be seen as modeling the posterior probability of the data label. For data points with high probability, we ex tract their features and insert them into the memory bank, which is used in subsequent model updates. In addition, we develop a smooth top R(sTRM) trick to adjust the thresh old for noisy data identiÔ¨Åcation as well as an acceleration technique that replaces individual data points with the class centers in the probability calculation. We perform extensive empirical evaluations on both syn thetic and real datasets. Inspired by the the ‚Äúnoise cluster‚Äù phenomenon observed from realworld data, we introduce the Small Cluster noise model to mimic openset noise in real data. Experimental results show that PRISM achieves superior performance compared to 12 existing DML and noiseresistant training techniques under symmetric noise, Small Cluster noise, and real noise. In addition, the accel 1arXiv:2103.16047v2  [cs.CV]  12 Apr 2021eration trick speeds up the algorithm by a factor of 6.9 on SOP dataset. The code and data are available at https: //github.com/alibabaedu/Rankingbased InstanceSelection . 2. Related Work "
320,Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification.txt,"Person re-identification (re-ID) aims at identifying the same persons' images
across different cameras. However, domain diversities between different
datasets pose an evident challenge for adapting the re-ID model trained on one
dataset to another one. State-of-the-art unsupervised domain adaptation methods
for person re-ID transferred the learned knowledge from the source domain by
optimizing with pseudo labels created by clustering algorithms on the target
domain. Although they achieved state-of-the-art performances, the inevitable
label noise caused by the clustering procedure was ignored. Such noisy pseudo
labels substantially hinders the model's capability on further improving
feature representations on the target domain. In order to mitigate the effects
of noisy pseudo labels, we propose to softly refine the pseudo labels in the
target domain by proposing an unsupervised framework, Mutual Mean-Teaching
(MMT), to learn better features from the target domain via off-line refined
hard pseudo labels and on-line refined soft pseudo labels in an alternative
training manner. In addition, the common practice is to adopt both the
classification loss and the triplet loss jointly for achieving optimal
performances in person re-ID models. However, conventional triplet loss cannot
work with softly refined labels. To solve this problem, a novel soft
softmax-triplet loss is proposed to support learning with soft pseudo triplet
labels for achieving the optimal domain adaptation performance. The proposed
MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and
16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT
unsupervised domain adaptation tasks. Code is available at
https://github.com/yxgeee/MMT.","Person reidentiÔ¨Åcation (reID) aims at retrieving the same persons‚Äô images from images captured by different cameras. In recent years, person reID datasets with increasing numbers of images were proposed to facilitate the research along this direction. All the datasets require timeconsuming an notations and are keys for reID performance improvements. However, even with such largescale datasets, for person images from a new camera system, the person reID models trained on exist ing datasets generally show evident performance drops because of the domain gaps. Unsupervised Domain Adaptation (UDA) is therefore proposed to adapt the model trained on the source image do main (dataset) with identity labels to the target image domain (dataset) with no identity annotations. Stateoftheart UDA methods (Song et al., 2018; Zhang et al., 2019b; Yang et al., 2019) for person reID group unannotated images with clustering algorithms and train the network with clustering generated pseudo labels. Although the pseudo label generation and feature learning with pseudo labels are conducted alternatively to reÔ¨Åne the pseudo labels to some extent, the training of the neural network is still substantially hindered by the inevitable label noise. The noise derives from the limited transferability of sourcedomain features, the unknown number of targetdomain identities, and the imperfect results of the clustering algorithm. The reÔ¨Ånery of noisy pseudo labels has crucial inÔ¨Çuences to the Ô¨Ånal performance, but is mostly ignored by the clusteringbased UDA methods. 1Code is available at https://github.com/yxgeee/MMT . 1arXiv:2001.01526v2  [cs.CV]  30 Jan 2020Published as a conference paper at ICLR 2020            Net1Net2Noisy Hard Pseudo Labels by ClusteringRobust Soft Pseudo Labels by MMT Figure 1: Person image A1andA2belong to the same identity while Bwith similar appearance is from another person. However, clusteringgenerated pseudo labels in stateoftheart Unsupervised Domain Adaptation (UDA) methods contain much noise that hinders feature learning. We propose pseudo label reÔ¨Ånery with online reÔ¨Åned soft pseudo labels to effectively mitigate the inÔ¨Çuence of noisy pseudo labels and improve UDA performance on person reID. To effectively address the problem of noisy pseudo labels in clusteringbased UDA methods (Song et al., 2018; Zhang et al., 2019b; Yang et al., 2019) (Figure 1), we propose an unsupervised Mutual MeanTeaching (MMT) framework to effectively perform pseudo label reÔ¨Ånery by optimizing the neural networks under the joint supervisions of offline reÔ¨Åned hard pseudo labels and online reÔ¨Åned soft pseudo labels. SpeciÔ¨Åcally, our proposed MMT framework provides robust soft pseudo labels in an online peerteaching manner, which is inspired by the teacherstudent approaches (Tarvainen & Valpola, 2017; Zhang et al., 2018b) to simultaneously train two same networks. The networks gradually capture targetdomain data distributions and thus reÔ¨Åne pseudo labels for better feature learning. To avoid training error ampliÔ¨Åcation, the temporally average model of each network is proposed to produce reliable soft labels for supervising the other network in a collaborative training strategy. By training peernetworks with such online soft pseudo labels on the target domain, the learned feature representations can be iteratively improved to provide more accurate soft pseudo labels, which, in turn, further improves the discriminativeness of learned feature representations. The classiÔ¨Åcation and triplet losses are commonly adopted together to achieve stateoftheart per formances in both fullysupervised (Luo et al., 2019) and unsupervised (Zhang et al., 2019b; Yang et al., 2019) person reID models. However, the conventional triplet loss (Hermans et al., 2017) can not work with such reÔ¨Åned soft labels. To enable using the triplet loss with soft pseudo labels in our MMT framework, we propose a novel soft softmaxtriplet loss so that the network can beneÔ¨Åt from softly reÔ¨Åned triplet labels. The introduction of such soft softmaxtriplet loss is also the key to the superior performance of our proposed framework. Note that the collaborative training strategy on the two networks is only adopted in the training process. Only one network is kept in the inference stage without requiring any additional computational or memory cost. The contributions of this paper could be summarized as threefold. (1) We propose to tackle the label noise problem in stateoftheart clusteringbased UDA methods for person reID, which is mostly ignored by existing methods but is shown to be crucial for achieving superior Ô¨Ånal per formance. The proposed Mutual MeanTeaching (MMT) framework is designed to provide more reliable soft labels. (2) Conventional triplet loss can only work with hard labels. To enable train ing with soft triplet labels for mitigating the pseudo label noise, we propose the soft softmaxtriplet loss to learn more discriminative person features. (3) The MMT framework shows exceptionally strong performances on all UDA tasks of person reID. Compared with stateoftheart methods, it leads to signiÔ¨Åcant improvements of 14.4% ,18.2% ,13.4% ,16.4% mAP on MarkettoDuke, DuketoMarket, MarkettoMSMT, DuketoMSMT reID tasks. 2 R ELATED WORK "
552,Learning with Noisy Labels for Sentence-level Sentiment Classification.txt,"Deep neural networks (DNNs) can fit (or even over-fit) the training data very
well. If a DNN model is trained using data with noisy labels and tested on data
with clean labels, the model may perform poorly. This paper studies the problem
of learning with noisy labels for sentence-level sentiment classification. We
propose a novel DNN model called NetAb (as shorthand for convolutional neural
Networks with Ab-networks) to handle noisy labels during training. NetAb
consists of two convolutional neural networks, one with a noise transition
layer for dealing with the input noisy labels and the other for predicting
'clean' labels. We train the two networks using their respective loss functions
in a mutual reinforcement manner. Experimental results demonstrate the
effectiveness of the proposed model.","It is well known that sentiment annotation or la beling is subjective ( Liu,2012 ). Annotators of ten have many disagreements. This is especially so for crowdworkers who are not well trained. That is why one always feels that there are many errors in an annotated dataset. In this paper, we study whether it is possible to build accurate sen timent classiÔ¨Åers even with noisylabeled training data. Sentiment classiÔ¨Åcation aims to classify a piece of text according to the polarity of the senti ment expressed in the text, e.g., positive ornega tive(Pang and Lee ,2008 ;Liu,2012 ;Zhang et al. , 2018 ). In this work, we focus on sentencelevel sentiment classiÔ¨Åcation (SSC) with labeling er rors. As we will see in the experiment section, noisy labels in the training data can be highly damag ing, especially for DNNs because they easily Ô¨Åt the training data and memorize their labels even ‚àóCorresponding authorwhen training data are corrupted with noisy labels (Zhang et al. ,2017 ). Collecting datasets annotated with clean labels is costly and timeconsuming as DNN based models usually require a large num ber of training examples. Researchers and practi tioners typically have to resort to crowdsourcing. However, as mentioned above, the crowdsourced annotations can be quite noisy. Research on learning with noisy labels dates back to 1980s ( Angluin and Laird ,1988 ). It is still vibrant today ( Mnih and Hinton ,2012 ; Natarajan et al. ,2013 ,2018 ;Menon et al. ,2015 ; Gao et al. ,2016 ;Liu and Tao ,2016 ;Khetan et al. , 2018 ;Zhan et al. ,2019 ) as it is highly challeng ing. We will discuss the related work in the next section. This paper studies the problem of learning with noisy labels for SSC. Formally, we study the fol lowing problem. Problem DeÔ¨Ånition : Given noisy labeled train ing sentences S={(x1,y1),...,(xn,yn)}, where xi|n i=1is theith sentence and yi‚àà {1,...,c}is the sentiment label of this sentence, the noisy labeled sentences are used to train a DNN model for a SSC task. The trained model is then used to classify sentences with clean labels to one of the csenti ment labels. In this paper, we propose a convolutional neural NETwork with ABnetworks (N ETAB) to deal with noisy labels during training, as shown in Figure 1. We will introduce the details in the subsequent sections. Basically, N ETABconsists of two convolutional neural networks (CNNs) (see Figure 1), one for learning sentiment scores to predict ‚Äòclean‚Äô1labels and the other for learning a noise transition matrix to handle input noisy labels. We call the two CNNs Anetwork and 1Here we use clean with single quotes as it is not com pletely clean. In practice, models can hardly produce com pletely clean labels.ABnetwork, respectively. The fundamental here is that (1) DNNs memorize easy instances Ô¨Årst and gradually adapt to hard instances as training epochs increase ( Zhang et al. ,2017 ; Arpit et al. ,2017 ); and (2) noisy labels are the oretically Ô¨Çipped from the clean/true labels by a noise transition matrix ( Sukhbaatar et al. ,2015 ; Goldberger and BenReuven ,2017 ;Han et al. , 2018a ,b). We motivate and propose a CNN model with a transition layer to estimate the noise transition matrix for the input noisy labels, while exploiting another CNN to predict ‚Äòclean‚Äô labels for the input training (and test) sentences. In training, we pretrain Anetwork in early epochs and then train A Bnetwork and Anetwork with their own loss functions in an alternating manner. To our knowledge, this is the Ô¨Årst work that ad dresses the noisy label problem in sentencelevel sentiment analysis. Our experimental results show that the proposed model outperforms the stateoftheart methods. 2 Related Work "
391,Attentive Prototypes for Source-free Unsupervised Domain Adaptive 3D Object Detection.txt,"3D object detection networks tend to be biased towards the data they are
trained on. Evaluation on datasets captured in different locations, conditions
or sensors than that of the training (source) data results in a drop in model
performance due to the gap in distribution with the test (or target) data.
Current methods for domain adaptation either assume access to source data
during training, which may not be available due to privacy or memory concerns,
or require a sequence of lidar frames as an input. We propose a single-frame
approach for source-free, unsupervised domain adaptation of lidar-based 3D
object detectors that uses class prototypes to mitigate the effect pseudo-label
noise. Addressing the limitations of traditional feature aggregation methods
for prototype computation in the presence of noisy labels, we utilize a
transformer module to identify outlier ROI's that correspond to incorrect,
over-confident annotations, and compute an attentive class prototype. Under an
iterative training strategy, the losses associated with noisy pseudo labels are
down-weighed and thus refined in the process of self-training. To validate the
effectiveness of our proposed approach, we examine the domain shift associated
with networks trained on large, label-rich datasets (such as the Waymo Open
Dataset and nuScenes) and evaluate on smaller, label-poor datasets (such as
KITTI) and vice-versa. We demonstrate our approach on two recent object
detectors and achieve results that out-perform the other domain adaptation
works.","The localization and categorization of objects in a 3D scene is a crucial component of perception systems in Ô¨Åelds like robotics and autonomous driving. In recent years, datadriven approaches using deep neural networks have achieved superior performance in various versions of this task [22, 27‚Äì29, 38, 43, 45], facilitated in part by the release Class  ROI featuresMisclassified ROI features  Outlier removal  Cluster centroid / class prototype  Class prototype from average of ROI features  Class prototype from weighted average    of attentive ROI features   Class boundary   Comparison with SOT A Average 3D mAP   Domain shift  Figure 1. Top row: Visual representations of prototype compu tation. On the left hand side is a depiction of a standard feature aggregation approach for prototype computation. In the case of noisy labels, features corresponding to mislabeled regions that are not discarded by outlier removal contribute to the class prototype. On the right is the proposed method of entropyweighted average of attentive region features which considers only salient regions for prototype computation. The opacity of the features represents the attention weights, and the width of the connecting lines repre sents the combination weights for computing the average. Bottom row: Comparison of our results on SECONDiou against recent stateoftheart methods for three domain shift scenarios. of numerous datasets and benchmarks [1, 5, 8, 10, 20, 31]. In practical scenarios, it is important for these object de tection frameworks to perform consistently well in different domain scenarios. However, deep neural networks tend to learn not only the valuable features that aid in performing the task at hand, but also the biases present in the data it is trained on. In the case of lidar datasets, the weather condi tions and the location of capture lead to biases in the dataset due to the speciÔ¨Åc dimensions of roads, vehicles, and the driving conventions of the area. Additionally, different li dar sensors possess different rates of return and producearXiv:2111.15656v2  [cs.CV]  1 Dec 2021(a) All predictions (b) thresh = 0.7   Figure 2. 3D bounding box predictions of the object detector [28] trained on Waymo [31] data and tested on KITTI [8]. Ground truth annotations are in green and predictions are in red. Thresholding (b) fails to remove all false positives present in (a). Selftraining with these pseudolabels leads to the enforcement of errors. pointclouds with varying densities, leading to another set of inherent biases. This leads to a distribution gap among various pointcloud datasets. Thus, an object detector trained on a particular dataset will drop in performance when eval uated on samples from a dataset with a different distribu tion. We call the training and test datasets in this scenario as the source and target domain datasets, respectively. One may argue that making use of a large, diverse source do main could solve this problem, however there will always be samples from an unseen distribution, and collecting ev ery possible type of lidar scene is impractical at best. Unsupervised domain adaptation (UDA) refers to the process of bridging this gap to improve the performance of sourcetrained networks on unlabelled target samples. There have been several recent works addressing this prob lem, for both 2D [12,19,25] and 3D [3,18,26,37,40] object detection. However, source samples are often unavailable during training due to limited memory capabilities or pri vacy reasons. This requires a sourcefree domain adapta tion approach, where only the sourcetrained model and un labelled target samples are used for adaptation. There exist several such approaches for various computer vision tasks on images [15, 17, 41]. Only SFUDA3D[26] attempts this setting for 3D object detection, but relies on a sequence of lidar frames as an input to the network. Self trainingbased methods have been successful in unsupervised and semisupervised domain adaptive works [3,36,46], but rely on conÔ¨Ådence thresholding to Ô¨Ålter noisy pseudolabels. As illustrated in the example from Figure 2, the use of high thresholds (as is general practice) results in training the model on easy samples and incorrect labels of high conÔ¨Ådence that contribute to the enforcement of errors during adaptation. We propose an unsupervised, sourcefree domain adap tation framework for 3D object detection that addresses the issue of incorrect, overconÔ¨Ådent pseudo labels during self training through the use of class prototypes. In the pres ence of label noise, standard feature aggregation methods of prototype computation [11,13,39,44] are insufÔ¨Åcient, since features corresponding to incorrectly labeled regions couldcontribute to the Ô¨Ånal prototype (see Figure 1, top row). Inspired by the high representative power of selfattention and recent works that make use of transformers to focus on salient inputs [7, 33], we calculate an attentive class proto type by using a transformer to identify salient regionsof interest and combine their associated feature vectors using prediction entropy weights that represent the uncertainty of the classiÔ¨Åcation branch for each sample. Class predictions corresponding to incorrect pseudo labels, which are identi Ô¨Åed by calculating the similarity with the class prototype, are downweighed to prevent reinforcing errors during self training. We demonstrate our result on several domain shift scenarios (see Figure 1). Our contributions are as follows ‚Ä¢ We propose the attentive prototype for learning repre sentative class features in the presence of label noise by leveraging selfattention through a transformer block and perform sourcefree unsupervised domain adaptation of 3D object detection networks that mitigates the effect of label noise during self training by Ô¨Åltering incorrect an notations. ‚Ä¢ We demonstrate our method on two recent object detec tors, SECONDiou [38], and PointRCNN [28] for six do main shift scenarios and outperform recent domain adap tation works. 2. Related Works "
513,Neural Network Pruning with Residual-Connections and Limited-Data.txt,"Filter level pruning is an effective method to accelerate the inference speed
of deep CNN models. Although numerous pruning algorithms have been proposed,
there are still two open issues. The first problem is how to prune residual
connections. We propose to prune both channels inside and outside the residual
connections via a KL-divergence based criterion. The second issue is pruning
with limited data. We observe an interesting phenomenon: directly pruning on a
small dataset is usually worse than fine-tuning a small model which is pruned
or trained from scratch on the large dataset. Knowledge distillation is an
effective approach to compensate for the weakness of limited data. However, the
logits of a teacher model may be noisy. In order to avoid the influence of
label noise, we propose a label refinement approach to solve this problem.
Experiments have demonstrated the effectiveness of our method (CURL,
Compression Using Residual-connections and Limited-data). CURL significantly
outperforms previous state-of-the-art methods on ImageNet. More importantly,
when pruning on small datasets, CURL achieves comparable or much better
performance than fine-tuning a pretrained small model.","Deep neural networks have now become the dominat ing method in various computer vision Ô¨Åelds, such as im age recognition [10, 20, 35] and object detection [5], and we have witnessed a great improvement in model accuracy. But, deploying a large CNN model on resource constrained devices like mobile phones is still challenging. Due to over parameterization, it is both storage and time consuming to run a cumbersome large model on small devices. Network pruning is a useful tool to obtain a satisfac tory balance between inference speed and model accuracy. Among these methods, Ô¨Ålter level pruning aims to remove This research was partially supported by the National Natural Science Foundation of China (61772256, 61921006). J. Wu is the corresponding author. ( a) bott len e ck (b)  h ou r g las s ( c) wall e tFigure 1. Illustration of residual block pruning with different strategies. (a) Bottleneck structure of residual blocks. (b) Only prune channels inside the bottleneck, generating an hourglass structure. (c) Prune channels both inside and outside the residual connection, generating a shape similar to an opened wallet. the whole unimportant Ô¨Ålters according to a certain crite rion. This strategy will not damage the original model struc ture and is attracting more and more attention recently. Although numerous Ô¨Ålter level pruning algorithms have been proposed, there are still several open issues. First, pruning residual connections is very difÔ¨Åcult. As illustrated in Fig. 1, most previous pruning methods only prune Ô¨Ål ters inside the residual connection, leaving the number of output channels unchanged. With a smaller target model (i.e., more Ô¨Ålters pruned), the original bottleneck structure will become an hourglass. Obviously, representation ability of middle layers inside the hourglass structure is severely handicapped. Therefore, pruning channels both inside and outside the residual connection is more preferred for accel erating networks. Then, the pruned block is still bottleneck or in an opened wallet shape. As illustrated in the experiments section, the wallet structure has more advantages compared with hourglass: 1) it is more accurate thanks to a larger pruning space; 2) it is faster even with the same number of FLOPs; 3) it can save more storage space because more weights will be pruned. The second issue is about pruning models with limited data. Most current pruning methods only report their results on toy datasets ( e.g., MNIST [21], CIFAR [19]) or large scale datasets ( e.g., ImageNet [33]), ignoring an important real application scenario: pruning models on small datasets which have few images per category. This is a very common 1arXiv:1911.08114v3  [cs.CV]  25 Apr 2020requirement, because we will not apply ImageNet in a real application. Directly pruning on a target dataset (which is usually small) is necessary. In order to get a small model on a target small dataset, there are two different ways: 1) compress the network us ing the large dataset (or using a small network trained from scratch on the large dataset), and then Ô¨Ånetune on the target small dataset; 2) directly prune the model without access to the large dataset. In many realworld scenarios, the only choice is to compress the network using the small dataset and Ô¨Ånetune on the same small dataset. But, the reality is that directly pruning on a small dataset usually has a signiÔ¨Åcantly lower accuracy than Ô¨Ånetuning a small model which is pruned or trained from scratch on the large scale dataset . This phenomenon widely exists in various networks and datasets. For example, as shown in ThiNet [29], Ô¨Ånetuning a pruned model which is com pressed on ImageNet is a better choice when transferring to other domains. They found that the accuracy of directly pruning on CUB200 [36] is only 66.90%, while Ô¨Ånetuning a pruned ImageNet model can achieve 69.43%. A dilemma is that directly pruning on the target dataset is often the case in realworld applications, where large datasets are either proprietary or too expensive to be used by ordinary users. In this paper, we propose CURL, namely Compression Using Residualconnections and Limiteddata, to address both issues. In order to prune the channels outside of the residual connection, we show that all the blocks in the same stage should be pruned simultaneously due to the short cut connection. We propose a KLdivergence based crite rion to evaluate the importance of these Ô¨Ålters. The chan nels inside and outside the residual connections will both be pruned, leading to a wallet shaped structure. Experi ments on ImageNet show that the proposed residual block pruning method outperforms the previous stateoftheart. To address the problem caused by the lack of enough train ing data, we propose to combine knowledge distillation [14] and mixup [40] together and enlarge the training dataset via image transformation. We also propose a novel method to correct the noise in the logits of the teacher model. All the techniques greatly improve the accuracy of directly pruning with limited data. Our contributions are summarized as follows. We propose a novel way to compress residual blocks. We prune not only channels inside the residual branch, but also channels of its output activation maps (both the identity branch and the residual branch). The re sulting walletshaped structure shows more advantages than previous hourglassshaped structure. Data augmentation is very effective in model Ô¨Åne tuning with limited data. We show that combining data augmentation and knowledge distillation can achievebetter performance. To avoid the inÔ¨Çuence of label noise, we propose a label reÔ¨Ånement strategy which can further improve the accuracy. 2. Related Work "
173,CascadeML: An Automatic Neural Network Architecture Evolution and Training Algorithm for Multi-label Classification.txt,"Multi-label classification is an approach which allows a datapoint to be
labelled with more than one class at the same time. A common but trivial
approach is to train individual binary classifiers per label, but the
performance can be improved by considering associations within the labels. Like
with any machine learning algorithm, hyperparameter tuning is important to
train a good multi-label classifier model. The task of selecting the best
hyperparameter settings for an algorithm is an optimisation problem. Very
limited work has been done on automatic hyperparameter tuning and AutoML in the
multi-label domain. This paper attempts to fill this gap by proposing a neural
network algorithm, CascadeML, to train multi-label neural network based on
cascade neural networks. This method requires minimal or no hyperparameter
tuning and also considers pairwise label associations. The cascade algorithm
grows the network architecture incrementally in a two phase process as it
learns the weights using adaptive first order gradient algorithm, therefore
omitting the requirement of preselecting the number of hidden layers, nodes and
the learning rate. The method was tested on 10 multi-label datasets and
compared with other multi-label classification algorithms. Results show that
CascadeML performs very well without hyperparameter tuning.","Inmultilabel classiÔ¨Åcation problems a datapoint can be assigned to more than one class, or label, simultaneously [12]. For example, an image can be clas siÔ¨Åed as containing multiple diÔ¨Äerent objects, or music can be labelled with more than one genre. This contrasts with multiclass classiÔ¨Åcation problems in which objects can only belong to a single class. Multilabel classiÔ¨Åcation algo rithms either break the multilabel problem down into smaller multiclass clas siÔ¨Åcation problems‚Äîfor example classiÔ¨Åer chains [23]‚Äîand are known as prob lem transformation methods‚Äîor modify multiclass algorithms to directly train ?This research was supported by Science Foundation Ireland (SFI) under Grant Num ber SFI/12/RC/2289.arXiv:1904.10551v1  [cs.LG]  23 Apr 20192 Pakrashi and Mac Namee on multilabel datasets‚Äîfor example BackPropagation in MultiLabel Learning (BPMLL) [37]‚Äîand are known as algorithm adaptation methods. Automatic machine learning [8], orAutoML, approaches have seen a recent resurgence of interest as researchers look for ways to automatically select opti mal algorithms, features, model architectures, and hyperparameters for machine learning tasks. The AutoML research community has, however, paid very little attention to multilabel classiÔ¨Åcation problems, although there have been some recent eÔ¨Äorts [25,26,33]. TheCascade2 algorithm [21] is an interesting neural network approach that learns model parameters and model architecture at the same time. In Cascade2, which is based on the cascade correlation neural network approach [7], train ing starts with a simple perceptron network, which is grown incrementally by adding new cascaded layers with skiplevel connections as long as performance on a validation dataset improves. Weights in each new layer are trained inde pendently of the overall network which greatly reduces the processing burden of this approach. This paper proposes CascadeML , a new AutoML solution for multilabel classiÔ¨Åcation problems, that is inspired by the Cascade2 algorithm and BPMLL. Improvements are made to both components leading to an implementation that requires minimal hyperparameter or network architecture tuning. In a series of evaluation experiments this approach has been shown to perform very well without the extensive hyperparameter tuning required by stateoftheart multi label classiÔ¨Åcation methods. To the best of authors‚Äô knowledge this is the Ô¨Årst automatic neural network architecture selection and training approach for multi label classiÔ¨Åcation methods. Theremainderofthepaperisstructuredasfollows.Section2discussestheex isting literature including a formal deÔ¨Ånition of multilabel classiÔ¨Åcation and the BPMLL algorithm. Section 2.3 describes the cascade neural network approach and, speciÔ¨Åcally, the Cascade2 algorithm. The proposed CascadeML method is then presented in Section 3. The design of an experiment to evaluate the perfor mance of the CascadeML algorithm, and benchmarking its performance against stateoftheart multilabel classiÔ¨Åcation approaches is described in Section 4. Section 5 presents and discusses the results of this experiment. Finally, Section 6 discusses future research directions and concludes the paper. 2 Related Work "
517,Bioresorbable Scaffold Visualization in IVOCT Images Using CNNs and Weakly Supervised Localization.txt,"Bioresorbable scaffolds have become a popular choice for treatment of
coronary heart disease, replacing traditional metal stents. Often,
intravascular optical coherence tomography is used to assess potential
malapposition after implantation and for follow-up examinations later on.
Typically, the scaffold is manually reviewed by an expert, analyzing each of
the hundreds of image slices. As this is time consuming, automatic stent
detection and visualization approaches have been proposed, mostly for metal
stent detection based on classic image processing. As bioresorbable scaffolds
are harder to detect, recent approaches have used feature extraction and
machine learning methods for automatic detection. However, these methods
require detailed, pixel-level labels in each image slice and extensive feature
engineering for the particular stent type which might limit the approaches'
generalization capabilities. Therefore, we propose a deep learning-based method
for bioresorbable scaffold visualization using only image-level labels. A
convolutional neural network is trained to predict whether an image slice
contains a metal stent, a bioresorbable scaffold, or no device. Then, we derive
local stent strut information by employing weakly supervised localization using
saliency maps with guided backpropagation. As saliency maps are generally
diffuse and noisy, we propose a novel patch-based method with image shifting
which allows for high resolution stent visualization. Our convolutional neural
network model achieves a classification accuracy of 99.0 % for image-level
stent classification which can be used for both high quality in-slice stent
visualization and 3D rendering of the stent structure.","Coronary heart disease is one of the most frequent causes of death despite being treatable. To treat the obstructive plaques, stenting is commonly used with mostly metallic stents being used in the past. As metal stents come with the risk of late stent thrombosis and instent restenosis,1bioresorbable scaolds such as bioresorbable vascular scaolds (BVS) have gained popularity recently. After implantation and in later followup examinations, the stents have to be assessed by the medical expert in order to detect malapposition or assess endothelialisation. Typically, intravascular optical coherence tomography (IVOCT) is used as an imaging modality for stent analysis2 as it provides high resolution images of the lumen and vessel walls. As a single IVOCT pullback contains hundreds of image slices to be assessed, manual evaluation is laborintensive and time consuming. Therefore, automatic stent detection and visualization methods have been proposed, mostly for metallic stents.3{5These methods largely rely on classic image processing to detect the highintensity metal stent struts. For bioresorbable scaolds, a classic approach has also been proposed.6However, since these struts are less pronounced in IVOCT images and dierent types of scaolds show dierent characteristics, recent approaches have used machine learning methods combined with handcrafted feature extraction for detection and visualization.7While showing promising results, these methods require pixellevel image annotations to learn local detection of stent struts within the slices. This Further author information: (Send correspondence to Nils Gessert) Nils Gessert: nils.gessert@tuhh.dearXiv:1810.09578v1  [cs.CV]  22 Oct 2018is, once again, time consuming and limits the potential dataset size and variability. Moreover, features need to be engineered for a specic stent type which might not be suitable for future stent variations which has already become evident during the transition from metal stents to bioresorbable scaolds.6,7 For this reason, we propose a novel deep learningbased method for stent visualization and potential detection using only imagelevel label annotations. A convolutional neural network (CNN) is trained to classify an IVOCT slice into the categories ""metal stent"", ""bioresorbable scaold"" and ""no device"". This way of imagelevel labeling has been successful for IVOCTbased deep learning8as it is fast and thus allows for larger datasets. Moreover, it is easily extensible to new stent types as a new class simply needs to be added to the learning problem and no new feature engineering is required. After training the model, we employ the concept of weakly supervised localization9to derive local stent strut information from the model. In particular, we compute saliency maps with guided backpropagation10which can be interpreted as a gradient image which shows the regions that were most important for the model's prediction. In our case, the trained network should have learned to focus on stent struts. However, saliency maps are generally diuse and high quality localization from global information only is very challenging.9For this reason, methods such as SmoothGrad11have been proposed which are targeted at improved saliency map quality. As we found this method to be insucient for the problem at hand, we propose a new patchbased approach with image shifting which leads to high resolution, high quality saliency maps that can serve as a visualization. The approach is used for regularization during model training and also for generation of stitched and averaged, smooth saliency maps. In this paper, we introduce our method for BVS visualization. We show that it is eective when visualizing stents for assessment after apposition with struts at the tissue surface as well as for followup review of, e.g. endothelialisation, where struts are starting to decay within the vessel tissue. Moreover, we show visualization of classic metal stents and the very recent Fantom Encore bioresorbable scaold. 2. METHODS AND MATERIALS "
267,AutoWS: Automated Weak Supervision Framework for Text Classification.txt,"Creating large, good quality labeled data has become one of the major
bottlenecks for developing machine learning applications. Multiple techniques
have been developed to either decrease the dependence of labeled data
(zero/few-shot learning, weak supervision) or to improve the efficiency of
labeling process (active learning). Among those, Weak Supervision has been
shown to reduce labeling costs by employing hand crafted labeling functions
designed by domain experts. We propose AutoWS -- a novel framework for
increasing the efficiency of weak supervision process while decreasing the
dependency on domain experts. Our method requires a small set of labeled
examples per label class and automatically creates a set of labeling functions
to assign noisy labels to numerous unlabeled data. Noisy labels can then be
aggregated into probabilistic labels used by a downstream discriminative
classifier. Our framework is fully automatic and requires no hyper-parameter
specification by users. We compare our approach with different state-of-the-art
work on weak supervision and noisy training. Experimental results show that our
method outperforms competitive baselines.","Text classiÔ¨Åcation is among the most popular Natural Language Processing (NLP) tasks, and has important applications in realworld, e.g., product categorization. The advent of Deep Learning has brought the stateoftheart to a wide variety of text classiÔ¨Åcation tasks (Mi naee et al., 2021). However, this comes with a price: deep learning based models usually require large amounts of annotated training data to achieve superior performance. In many scenarios, manual data annotation is expensive in terms of cost and eÔ¨Äort, especially whensubject matter experts (SME) must involve and/or classiÔ¨Åcation tasks are scaled to mil lions of instances with hundreds to thousands of classes. To reduce the manual annotation eÔ¨Äort, machine learning research have explored weak supervision approaches, i.e., possibilities to build prediction models using limited, noisy or imprecise labeled data. Weak supervision concerns special conditions of supervised learn ing in which annotated training data may be incomplete, inexact or inaccurate (Zhou, 2018). In this paper, we study incomplete supervision, i.e., data has ground truth labels but is too small to train a performant model, and focus on semisupervised learning techniques to lever age numerous amount of indomain unlabeled data to improve prediction performance. Prior studies have covered diÔ¨Äerent methods to assign noisy training labels to unlabeled data including crowdsourcing (Dalvi et al., 2013; Joglekar et al., 2015; Yuen et al., 2011; Zhang et al., 2016), distant supervision (HoÔ¨Ämann et al., 2011; Mintz et al., 2009; Smirnova and Cudr√©Mauroux, 2018; Takamatsu et al., 2012), heuristic rules (Awasthi et al., 2020; Ratner et al., 2017, 2016; Varma et al., 2017). Pre trained language models gain much attention recently because they can be Ô¨Ånetuned with little annotated data thanks to their great gen eralization capability (Perez et al., 2021; Rad ford et al., 2019). The above weak supervi sion sources are termed labeling functions (LF), which may vary in terms of error rate, coverage and probably generate conÔ¨Çict labels (Zhang et al., 2022, 2021). Researchers have developed label models that aggregate output of label ing functions to generate conÔ¨Ådenceweighted or probabilistic labels, which are consequently used to train an end model (e.g., Ô¨Ånal text classiÔ¨Åer). In this study, we propose AutoWS ‚Äì an auarXiv:2302.03297v1  [cs.CL]  7 Feb 2023tomated endtoend weak supervision frame work for text classiÔ¨Åcation. AutoWS provides a wide range of machinelearning based label ing functions ranging from statistical models to transformerbased language models, and dif ferent label models. Aiming to provide users a fully automated framework, AutoWS imple ments a simple yet eÔ¨Äective evaluation process even it is provided just a small labeled dataset. With our proposed evaluation process, only top labeling functions and the best label model are selected to produce Ô¨Ånal labels to unlabeled data. Contributions of our study are followings: ‚Ä¢AutoWSisafullyautomatedframeworkso that users neither need to provide any la beling heuristics/functions nor tune hyper parameters. ‚Ä¢Our experiments cover a wide variety of data domains including news, users‚Äô request, product titles, and emphasizes datasets with a large number of classes. While weak supervision has been studied for many years, its application on many class classiÔ¨Åcation tasks was not compre hensively evaluated. ‚Ä¢With a capability of selecting top labeling functions and best label models, AutoWS outperforms prior studies on benchmark data. 2 Related Work "
455,ML-KFHE: Multi-label ensemble classification algorithm exploiting sensor fusion properties of the Kalman filter.txt,"Despite the success of ensemble classification methods in multi-class
classification problems, ensemble methods based on approaches other than
bagging have not been widely explored for multi-label classification problems.
The Kalman Filter-based Heuristic Ensemble (KFHE) is a recent ensemble method
that exploits the sensor fusion properties of the Kalman filter to combine
several classifier models, and that has been shown to be very effective. This
work proposes a multi-label version of KFHE, ML-KFHE, demonstrating the
effectiveness of the KFHE method on multi-label datasets. Two variants are
introduced based on the underlying component classifier algorithm,
ML-KFHE-HOMER, and ML-KFHE-CC which uses HOMER and Classifier Chain (CC) as the
underlying multi-label algorithms respectively. ML-KFHE-HOMER and ML-KFHE-CC
sequentially trains multiple HOMER and CC multi-label classifiers and
aggregates their outputs using the sensor fusion properties of the Kalman
filter. Experiments and detailed analysis performed on thirteen multi-label
datasets and eight other algorithms, including state-of-the-art ensemble
methods, show that for both versions, the ML-KFHE framework improves the
ensembling process significantly with respect to bagging based combinations of
HOMER and CC, thus demonstrating the effectiveness of ML-KFHE. Also, the
ML-KFHE-HOMER variant was found to perform consistently and significantly
better than existing multi-label methods including existing approaches based on
ensembles.","A multiclass classiÔ¨Åcation task assigns an object to at most one class. Real world classiÔ¨Åcation problems exist, however, where an object can be assigned Corresponding author Email addresses: arjun.pakrashi@ucd.ie (Arjun Pakrashi), brian.macnamee@ucd.ie (Brian Mac Namee) Preprint submitted to Information Fusion March 11, 2021arXiv:1904.10552v3  [cs.LG]  10 Mar 2021to more than one class simultaneously [12]. In other words, an object can be labelledwith more than one class at the same time. For example, an image of a landscape may contain mountains, sea and sky and therefore it can be a member of each of the corresponding classes [4]. Similarly, music can be tagged with more than one genre. Such problems are known as multilabel classiÔ¨Åcation problems. Formally, multilabel classiÔ¨Åcation problems can be deÔ¨Åned as follows. Let xibe a datapoint from a ddimensional input space Xof real and/or categorical attributes. Also, let the set of all possible labels for a speciÔ¨Åc multilabel classi Ô¨Åcation problem be L=f1;2;:::;qg, from which a subset of labels, LiL, is applicable to the datapoint xi. Here labels inLiare called the relevant labels, and(L"
77,Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification.txt,"Employing clustering strategy to assign unlabeled target images with pseudo
labels has become a trend for person re-identification (re-ID) algorithms in
domain adaptation. A potential limitation of these clustering-based methods is
that they always tend to introduce noisy labels, which will undoubtedly hamper
the performance of our re-ID system. To handle this limitation, an intuitive
solution is to utilize collaborative training to purify the pseudo label
quality. However, there exists a challenge that the complementarity of two
networks, which inevitably share a high similarity, becomes weakened gradually
as training process goes on; worse still, these approaches typically ignore to
consider the self-discrepancy of intra-class relations. To address this issue,
in this paper, we propose a multiple co-teaching framework for domain adaptive
person re-ID, opening up a promising direction about self-discrepancy problem
under unsupervised condition. On top of that, a mean-teaching mechanism is
leveraged to enlarge the difference and discover more complementary features.
Comprehensive experiments conducted on several large-scale datasets show that
our method achieves competitive performance compared with the
state-of-the-arts.","Given a query image, person reidentiÔ¨Åcation (reID) aims to match the personofinterest across multiple nonoverlapped cameras distributed in different places. Encouraged by the remarkable success of deep learning methods and the availability of largescale datasets, reID research commu nity has achieved signiÔ¨Åcant progress during the past few years [Zheng et al. , 2016; Ye et al. , 2021 ]. However, as for pedestrian images from an unseen domain, even with a large diversity of training data, person reID model generally expe riences catastrophic performance drops because of the huge domain gaps or scene shifts, which cannot satisfy the need of application in real scenarios. To alleviate this problem, The corresponding author. person A  person Beasy hard  easy hard Figure 1: Illustration of the selfdiscrepancy of intraclass relations for UDA person reID tasks, which is caused by variations in pose, viewpoint and occlusion, etc. For each identity, some easy samples can be assigned with reliable pseudo labels. However, most of hard samples are always given with noisy pseudo labels. unsupervised domain adaptation (UDA) [Ganin and Lempit sky, 2015; Xiang et al. , 2020b; Saito et al. , 2018 ]is there fore proposed to employ the model trained on source dataset with identity labels to perform inference on the target domain. Nevertheless, it still remains an open research challenge in in dustry and academia due to the lack of identity annotations. Currently, there are two main categories of UDA meth ods in reID community. The Ô¨Årst category of imagelevel adaptation aims to eliminate the data distribution discrepancy across source and target domain, such as PTGAN [Wei et al., 2018 ]and SPGAN [Deng et al. , 2018 ]. Although these approaches achieve promising progress, their performance deeply relies on the images generation quality. The second category of clusteringbased adaptation [Song et al. , 2020; Fuet al. , 2019; Fan et al. , 2018 ]deploys clustering algo rithm to generate pseudolabels for unsupervised target im ages during training period. Unfortunately, their abilities are substantially hindered by the inevitable label noises caused by imperfect clustering algorithms. To alleviate this problem, some coteaching based reID approaches [Yang et al. , 2020; Geet al. , 2020; Zhao et al. , 2020; Zhai et al. , 2020 ]have been introduced for combating with noisy labels after clustering. Even though their optimal performance is often achieved by subnetwork‚Äôs discrimination ability, the selfdiscrepancy of intraclass relation (as shown in Figure 1) in target domain still remains unexplored. So a natural question then comes to our attention: how to leverage selfdiscrepancy features of multiple subnetwork, and then optically adapt them to un labelled domain, which has to be fully elaborated. Another challenge we observe is that, as the training process goes on,arXiv:2104.02265v5  [cs.CV]  7 Sep 2021two neural networks in traditional coteaching [Han et al. , 2018 ]tend to converge and unavoidably share a high simi larity, which weakens their complementarity and further im provement in terms of performance. To solve the challenges mentioned above, we propose a simple yet powerful Multiple Coteaching Network MCN that considerably explores the selfdiscrepancy of intraclass relation in target domain, consequently, person reID can be more effectively performed to resist with noisy labels in do main adaptation. In addition, we introduce a meanteaching mechanism to greatly enhance the complementarity and in dependence of collaborative networks, which, in turn, further improves the discriminability of learned representations in a progressive fashion. To the best of our knowledge, this is the Ô¨Årst research effort to exploit the potential of selfdiscrepancy among intraclass to address the UDA problem. Compared with existing coteaching based method [Geet al. , 2020; Zhao et al. , 2020; Zhai et al. , 2020 ], our MCN is different from them in terms of data input andmodel structure :(1) Our work proposes to adopt samples with different discrep ancy granularity ( T1Tn) as asymmetric inputs to multi ple networks, while previous methods applied same dataset as symmetric inputs during training; (2)MEBNet [Zhai et al., 2020 ]used DenseNet121 [Huang et al. , 2017 ], ResNet 50[Heet al. , 2016 ]and Inceptionv3 [Szegedy et al. , 2016 ]as backbone for enhancing the independence and complemen tary, [Geet al. , 2020; Zhao et al. , 2020 ]utilized random eras ing or random seeds for creating a difference, [Geet al. , 2020; Zhai et al. , 2020 ]also adopted symmetrical architecture with soft pseudo labels as well as hard pseudo labels in UDA reID tasks. In contrast, our MCN is only trained based on ResNet 50 with hard pseudo labels, which makes it more Ô¨Çexible and adaptable. In addition, our method can signiÔ¨Åcantly mine the selfdiscrepancy feature in target domain, and a novel mean teaching mechanism is also adopted to enhance the indepen dence and complementary between teacher network and stu dent networks, while previous asymmetric coteaching ap proach [Yang et al. , 2020 ]fails to meet these needs. In total, our contribution can be summarized as follows: 1. We propose a multiple coteaching network MCN to mine the selfdiscrepancy of intraclass relations in target do main for solving noisy labels. 2. A MeanTeaching mechanism is introduced to further enhance the output complementarity in a progressive manner based on proposed MCN method (‚ÄúMCNMT‚Äù for short). 3. Experimental results conducted on several benchmarks demonstrate the effectiveness of our proposed method. 2 Related works "
260,A Hierarchical Matcher using Local Classifier Chains.txt,"This paper focuses on improving the performance of current convolutional
neural networks in visual recognition without changing the network
architecture. A hierarchical matcher is proposed that builds chains of local
binary neural networks after one global neural network over all the class
labels, named as Local Classifier Chains based Convolutional Neural Network
(LCC-CNN). The signature of each sample as two components: global component
based on the global network; local component based on local binary networks.
The local networks are built based on label pairs created by a similarity
matrix and confusion matrix. During matching, each sample travels through one
global network and a chain of local networks to obtain its final matching to
avoid error propagation. The proposed matcher has been evaluated with image
recognition, character recognition and face recognition datasets. The
experimental results indicate that the proposed matcher achieves better
performance when compared with methods using only a global deep network.
Compared with the UR2D system, the accuracy is improved significantly by 1% and
0.17% on the UHDB31 dataset and the IJB-A dataset, respectively.","Visual recognition is one of the hottest topics in the Ô¨Åelds of computer vision and machine learning. In recent years, many deep learning models have been built to set new stateoftheart results in image classiÔ¨Åcation, object detection and many other visual recognition tasks [1, 2, 3]. Among these tasks, most of the breakthroughs are achieved with deep Convolutional Neural Networks (CNN) [4]. CNN was Ô¨Årst proposed in the late 1990s by LeCun et al. [4, 5]. It was quickly overwhelmed by the combination of other shallow descriptors (such as SIFT [6], HOG [7], bag of words [8]) with Support Vector Machine (SVM) [9]. In recent Preprint submitted to Elsevier May 8, 2018arXiv:1805.02339v1  [cs.CV]  7 May 2018Figure 1: An example of the proposed LCCCNN matcher is depicted with the combination of global model and local models. The matching paths of the two testing images are indicated in red and cyan, respectively. years, with the increase of image recognition data size and computation power, CNN is becoming more and more popular and dominant. Krizhevsky et al. [10] proposed the classic eight layer CNN model (AlexNet) with Ô¨Åve convolutional and three fully connected layers. The model is trained via backpropagation through layers and performs extremely well in domains with a large amount of training data. Since then, many new CNN models have been constructed with larger sizes and different architectures to improve the performance. A series of improvements were achieved by VGG [11], GooLeNet [12], ResNet [13, 14] and so on. How ever, a larger model creates a larger number of parameters and larger computa tional complexity. Methods for compressing network and accelerating training and testing computation have also been developed [15, 16, 17, 18]. Overall, the previous deep face networks have two limitations. (a) data size: training a larger size global model requires more training data, which can be costly and not applicable in certain applications. (b) local information: one deep neural network built over all the class labels may ignore the pairwise local correlations between different labels, which can be used to improve overall performance. This paper overcomes the limitations (a) and (b) by introducing a hierarchi cal matcher that builds chains of local binary CNN classiÔ¨Åers after the global CNN classiÔ¨Åer over all the class labels. Moreover, it is a method to improve face recognition performance without changing the architecture of the CNN network. 2Hereafter, these two types of classiÔ¨Åers are referred as local model and global model. The motivation behind this is that a global model focuses more on the global discriminative features over all the class labels and tends to misclassify samples from visually similar classes. With fewer labels, a local model can ex ploit more local discriminative features for the related labels and can be used to correct the matching result of the global model. Especially when the same train ing data and network architecture are used for both global and local models, each local model converges fast and achieves better accuracy than the global model. Also, local models can be trained in parallel, which avoids excessive increase in computational complexity. In addition, when data size is limited, a local model can explore more pairwise label correlations than the global model. To limit the complexity of the proposed matcher, only binary local models are built in this paper. Take CIFAR10 dataset [19] for example. Figure 1 depicts the intuition of the proposed matcher. It can be observed that for the ‚Äúdog‚Äù image, the binary local model between ‚Äúcat‚Äù label and ‚Äúdog‚Äù label is used to correct the mistake of the global model. Importantly, local models can be built one after another, which leads to a chain of local models to boost performance and avoid error propaga tion. For the ‚Äúdeer‚Äù image, a chain of two local models (between ‚Äúdog‚Äù label and ‚Äúhorse‚Äù label, ‚Äúdog‚Äù label and ‚Äúdeer‚Äù label) are built to improve the matching of the global model. The contributions of this paper are improving recognition performance by the following two techniques: (i) fully exploring training data by proposing a hier archical matcher, where the contributions of global model and local models are combined. (ii) making use of pairwise label information to local model chains, which adaptively select a small set of label pairs to build local models. The pair wise correlations between different labels are learned based on their relationships in the score matrices. These correlations are not well explored in global model based methods. Parts of this work on face recognition have been published in Zhang et al. [20]. In this paper, it is extended by providing: (i) a signature to store image information with global model and local model components; (ii) the Ô¨Ånetuning process of local models; (iii) more general applications of image recognition and character recognition; (iv) the evaluation on the poseinvariant 3Daided 2D face recognition system (UR2D) [21]. The rest of this paper is organized as follows: Section 2 presents related work. Section 3 and Section 4 describe the signature and the hierarchical matcher. The experimental design, results, and analysis are presented in Section 5. Section 6 concludes the paper. 32. Related work "
591,Robust Product Classification with Instance-Dependent Noise.txt,"Noisy labels in large E-commerce product data (i.e., product items are placed
into incorrect categories) are a critical issue for product categorization task
because they are unavoidable, non-trivial to remove and degrade prediction
performance significantly. Training a product title classification model which
is robust to noisy labels in the data is very important to make product
classification applications more practical. In this paper, we study the impact
of instance-dependent noise to performance of product title classification by
comparing our data denoising algorithm and different noise-resistance training
algorithms which were designed to prevent a classifier model from over-fitting
to noise. We develop a simple yet effective Deep Neural Network for product
title classification to use as a base classifier. Along with recent methods of
stimulating instance-dependent noise, we propose a novel noise stimulation
algorithm based on product title similarity. Our experiments cover multiple
datasets, various noise methods and different training solutions. Results
uncover the limit of classification task when noise rate is not negligible and
data distribution is highly skewed.","Product classiÔ¨Åcation is a quintessential E commerce machine learning problem in which product items are placed into their respective cate gories. With recent advancements of Deep Learn ing, various unimodal (i.e., text only) and multi modal (e.g., text and image) models have been de veloped to predict larger numbers of items and cate gories with better accuracy (Gao et al., 2020; Chen et al., 2021a; Brinkmann and Bizer, 2021). How ever, one of the fundamental assumptions behind such models is the availability of large and high quality labeled datasets. Access to such datasets is usually costly or infeasible in some settings. Large product datasets usually suffer from annotation errors, i.e., products are assigned to incorrect cat egories, partially due to complex category struc ture, confusing categories and similar titles. The problem of noisy labels is even more severe when product category distribution is highly imbalanced with heavytail (Shen et al., 2012; Das et al., 2016). Therefore, a text classiÔ¨Åer which is robust to noisy labels present in training data is critical for high performing product classiÔ¨Åcation applications. While machine learning in the presence of label noise has been studied for decades, most of prior studies experimented in computer vision domain (Gu et al., 2021; Song et al., 2022), and only a few research was conducted in text classiÔ¨Åcation (Jindal et al., 2019; Garg et al., 2021). Without an annotated dataset with manuallyidentiÔ¨Åed label noise, classical approaches for label noise stimula tion assume classconditional noise (CCN) where the probability of an item having label corrupted depends on the original and noisy labels. With this assumption, all products of ‚ÄúMen‚Äôs Watches‚Äù cat egory have the sample probability to be assigned ‚ÄúWomen‚Äôs Watches‚Äù label. This is not generally correct. For instance, product titles having phrase ‚Äúmen‚Äôs watches‚Äù are less likely mislabeled. Re cent research addresses more general label noise, i.e., instancedependent noise (IDN), that an item is mislabeled with a probability depending on its original label and features. In this paper, we present a comprehensive study on improving product title classiÔ¨Åcation in the pres ence of IDN. We develop a simple yet effective Deep Neural Network for text classiÔ¨Åcation and show that our model performs well on different product title datasets ranging from small to medium sizes, balanced to skewed distributions, and tens to over a hundred categories. To generate noisy labels for experiments, our Ô¨Årst contribution is an IDN stimulation algorithm which Ô¨Çips an item‚Äôs label based on its similarity to items of other categories. Noisy label data generated by our method is comarXiv:2209.06946v1  [cs.CL]  14 Sep 2022pared with prior IDN stimulation methods for their impact to model accuracy degradation. To make the model robust to label noise, our second contri bution is a data augmentation method that reduces noise rate and thus improves model‚Äôs accuracy. We compare three stateoftheart Deep Neural Net work training algorithms to train a classiÔ¨Åer on data with label noise generated by different meth ods. From experimental results we discuss lessons learned for product title classiÔ¨Åcation in produc tion. To the best of our knowledge, this work is the Ô¨Årst time that noiseresistance model training is studied in Ecommerce domain, which is our third contribution. 2 Related Work "
52,CrowdTeacher: Robust Co-teaching with Noisy Answers & Sample-specific Perturbations for Tabular Data.txt,"Samples with ground truth labels may not always be available in numerous
domains. While learning from crowdsourcing labels has been explored, existing
models can still fail in the presence of sparse, unreliable, or diverging
annotations. Co-teaching methods have shown promising improvements for computer
vision problems with noisy labels by employing two classifiers trained on each
others' confident samples in each batch. Inspired by the idea of separating
confident and uncertain samples during the training process, we extend it for
the crowdsourcing problem. Our model, CrowdTeacher, uses the idea that
perturbation in the input space model can improve the robustness of the
classifier for noisy labels. Treating crowdsourcing annotations as a source of
noisy labeling, we perturb samples based on the certainty from the aggregated
annotations. The perturbed samples are fed to a Co-teaching algorithm tuned to
also accommodate smaller tabular data. We showcase the boost in predictive
power attained using CrowdTeacher for both synthetic and real datasets across
various label density settings. Our experiments reveal that our proposed
approach beats baselines modeling individual annotations and then combining
them, methods simultaneously learning a classifier and inferring truth labels,
and the Co-teaching algorithm with aggregated labels through common truth
inference methods.","Labeled data is essential to guarantee the success of increasingly more com plex classiÔ¨Åers. Unfortunately obtaining large quantities of highqu ality labels can be costprohibitive for several Ô¨Åelds. For example, in the medic al domains, it may take a clinician several hours to annotate the health records of hundreds of patients. One alternative is to gather labels using crowdsourcing , where re motely located workers are utilized to perform the task of labeling th e data. Although these crowdworkersindividually may not be as accurate as an expert, constructing the true label from their aggregated opinions can ap proximate the accuracy of an expert. However, the subjectivity of annotator s and their diÔ¨Äer ent qualiÔ¨Åcations introduce noise to the labeling process. To model t his noise, ‚àóEmory University, Atlanta GA , USA Email: msotood@emory.ed u 1most studies either focus on modeling the reliability of annotatorsan d their cor relation and reÔ¨Çecting it in the label aggregation phase or combining c lassiÔ¨Åer training with learning the annotators‚Äô trust parameters. Yet, lea rning through crowdsourcingbasedmodels can still fail in the presence ofdiÔ¨Äerin g annotations and unreliable users [14]. A promising direction for dealing with noisy labels for training complex classiÔ¨Åers is Coteaching [5]. Under the Coteaching paradigm, two p eer neural networks are trained separately and speciÔ¨Åc samples are exchang ed between the networksto reduce the errorof the two models and yield a more acc uratemodel. As a result, Coteaching methods have shown great promise for co mputer vision problemswithnoisylabels. Coteachingcannaturallycounteractcr owdsourcing noise since it Ô¨Ålters out noisy samples in the beginning and only adds the m at later training stages when they will be valuable. However, Coteach ing treats each sample with the same weight. This can cause the classiÔ¨Åer to inco rrectly learn from samples that may have fewer annotations or diverging hu man labels. To address this limitation, we propose to leverage the certainty of s amples from the label aggregation phase to inform the selection process o f Coteaching, which has not been studied before. Our model, CrowdTeacher, use s a perturba tion scheme based on the uncertainty of the samples to improve the robustness of the Coteaching framework. Given the availability of samples‚Äô unc ertainty from the label aggregation step, our model uses this information t o counter the inherent noise by perturbing the input space. In addition, the fram ework pri oritizes the more conÔ¨Ådent samples of the classiÔ¨Åer during the learn ing process. Thus, we tackle the problem of classiÔ¨Åcation with features and crow dsourcing labels using three mechanisms: ‚Ä¢Estimation of the features‚Äô distributions to generate synthetic d ata which is then used to perturb each sample in an additive manner, proportion al to its estimated label‚Äôs uncertainty. ‚Ä¢EnhancingCoteachingbyknowledgedistillation, i.e. astudentteac hermodel of a simple and a complex network to accommodate smaller tabular dat a. ‚Ä¢Utilization of the perturbed samples as input to the above classiÔ¨Åer t o further diÔ¨Äerentiate uncertain and certain training points based on their los s in each epoch Next, we formally deÔ¨Åne the problem and summarize and delineate whe re and how CrowdTeacher ties into the relevant literature in crowdsou rcing, data augmentation, and learning with noisy labels. 1.1 Problem DeÔ¨Ånition: ClassiÔ¨Åcation with Crowdsourc ing Annotations In practice, there are numerous applications in which the ground tr uth of a classiÔ¨Åcation task is not available, or disputed. For instance in medicin e, mul tiple pathologists do not always necessarily agree on the malignancy s tatus of a tumor in an image [8], or multiple nurses do not all agree on the presen ce of hospitalacquired bedsores for a patient given their charts [15]. Similarly, obtaining ground truth from experts to train reliable classiÔ¨Åers can be expen sive, as in the case of content Ô¨Åltering and regulation of posts on so cial media, 2which are distributed among multiple nonexpert annotators to obt ain some good quality labels [9]. Formally, we deÔ¨Åne learning with crowdsourcing la bels as follows: DeÔ¨Ånition 1. (ClassiÔ¨Åcation with Crowdsourcing Annotations) Consider a set ofRannotators labeling Nsamples with Kpossible classes. Given an answer matrixA‚ààRN√óRwhere each element anrindicates the label for sample n provided by annotator r, and the training feature matrix Xtr‚ààRN√óM, the goal is to train a classiÔ¨Åer that accurately predicts the true labels f or the test data using only its feature matrix Xts. We useKto denote number of classes. Simulated data from the synthesizer used for perturbation is shown by Sand the perturbed samples are denoted with/tildewidestXtr. The set of continuous and discrete features are shown by FcandFd respectively. Table 1 summarizes the notations used throughout t his paper. Table 1: Summary of Notations. Symbol Description NNumber of Samples RNumber of Annotators KNumber of Classes Œ±Perturbation Fraction XtrTraining feature matrix AAnswer matrix of all annotators SSynthetic feature matrix /tildewidestXtrPerturbed training samples feature matrix FcSet of continuous features FdSet of all discrete features PClass probability matrix ciCertainty of ith 1.2 Related works "
437,BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter by Combining Deep Learning and Transfer Learning Models.txt,"The outbreak COVID-19 virus caused a significant impact on the health of
people all over the world. Therefore, it is essential to have a piece of
constant and accurate information about the disease with everyone. This paper
describes our prediction system for WNUT-2020 Task 2: Identification of
Informative COVID-19 English Tweets. The dataset for this task contains size
10,000 tweets in English labeled by humans. The ensemble model from our three
transformer and deep learning models is used for the final prediction. The
experimental result indicates that we have achieved F1 for the INFORMATIVE
label on our systems at 88.81% on the test set.","The rapid spread of the coronavirus (COVID19) has caused a global health crisis. This virus is haz ardous to people‚Äôs health and causes a big panic all over the world. Statistics show that each day there are 4 million tweets related to COVID19 on Twitter (Lamsal, 2020). Therefore, it is essential to keep track of the information associated with this disease. Along with the development of many social networking platforms such as Twitter and Facebook. This is the primary way that helps peo ple capture information about COVID19 regularly. However, there is much content appearing daily on these social media platforms. Most of them do not have information about the status of COVID19, such as the number of suspected cases or cases near the user‚Äôs area. In this article, we present our approach at WNUT2020 Task 2 (Nguyen et al., 2020) to iden tify Tweets containing information about COVID 19 on the social networking platform Twitter or not. A Tweet is believed to have information if it includes information such as recovered, suspected, conÔ¨Årmed, and death cases and location or travelhistory of the patients. SpeciÔ¨Åcally, we described the problem as follows. ‚Ä¢Input : Given English Tweets on the social networking platform. ‚Ä¢Output : One of two labels (INFORMATIVE and UNINFORMATIVE) predicted by our system. Several examples are shown in Table 1 Tweet Label A New Rochelle rabbi and a White Plains doctor are among the 18 con Ô¨Årmed coronavirus cases in Westchester. HTTPURL0 Day 5: On a family bike ride to pick up dinner at @USER Broadway, we encountered our preCOVID19 Land Park happy hour crew keeping up the tradition at an appropriate #SocialDis tance.HTTPURL1 Table 1: Several examples in the WNUT2020 Task 2 dataset. 0 and 1 stand for INFORMATIVE and UNIN FORMATIVE, respectively. In this paper, we have two main contributions as follows. ‚Ä¢Firstly, we implemented four different models based on neural networks and transformers such as BiGRUCNN, BERT, RoBERTa, XLNet to solve the WNUT2020 Task 2: IdentiÔ¨Åcation of informative COVID19 English Tweets. ‚Ä¢Secondly, we propose a simple ensemble model by combining multiple deep learning and transformer models. This model gives thearXiv:2009.02671v2  [cs.CL]  1 Apr 2021highest performance compared with the single models with F1 on the test set is 88.81% and on the development set is 90.65%. 2 Related work "
64,EEG-Based Emotion Recognition Using Regularized Graph Neural Networks.txt,"Electroencephalography (EEG) measures the neuronal activities in different
brain regions via electrodes. Many existing studies on EEG-based emotion
recognition do not fully exploit the topology of EEG channels. In this paper,
we propose a regularized graph neural network (RGNN) for EEG-based emotion
recognition. RGNN considers the biological topology among different brain
regions to capture both local and global relations among different EEG
channels. Specifically, we model the inter-channel relations in EEG signals via
an adjacency matrix in a graph neural network where the connection and
sparseness of the adjacency matrix are inspired by neuroscience theories of
human brain organization. In addition, we propose two regularizers, namely
node-wise domain adversarial training (NodeDAT) and emotion-aware distribution
learning (EmotionDL), to better handle cross-subject EEG variations and noisy
labels, respectively. Extensive experiments on two public datasets, SEED and
SEED-IV, demonstrate the superior performance of our model than
state-of-the-art models in most experimental settings. Moreover, ablation
studies show that the proposed adjacency matrix and two regularizers contribute
consistent and significant gain to the performance of our RGNN model. Finally,
investigations on the neuronal activities reveal important brain regions and
inter-channel relations for EEG-based emotion recognition.","EMOTION recognition focuses on the recognition of hu man emotions based on a variety of modalities, such as audiovisual expressions, body language, physiological signals, etc. Compared to other modalities, physiological signals, such as electroencephalography (EEG), electrocar diogram (ECG), electromyography (EMG), etc., have the advantage of being difÔ¨Åcult to hide or disguise. In recent years, due to the rapid development of noninvasive, easy touse and inexpensive EEG recording devices, EEGbased emotion recognition has received an increasing amount of attention in both research [1] and applications [2]. Emotion models can be broadly categorized into discrete models and dimensional models. The former categorizes emotions into discrete entities, e.g., anger, disgust, fear, happiness, sadness, and surprise in Ekman‚Äôs theory [3]. The latter describes emotions using their underlying di mensions, e.g., valence, arousal and dominance [4], which measures emotions from unpleasant to pleasant, passive to active, and submissive to dominant, respectively. EEG signals measure voltage Ô¨Çuctuations from the cortex in the brain and have been shown to reveal important information about human emotional states [5]. For example, greater relative left frontal EEG activity has been observed when experiencing positive emotions [5]. The voltage Ô¨Çuctu ations in different brain regions are measured by electrodes attached to the scalp. Each electrode collects EEG signals in one channel. The collected EEG signals are often analyzed P . Zhong, D. Wang and C. Miao are with the Joint NTUUBC Research Centre of Excellence in Active Living for the Elderly (LILY), Nanyang Technological University, Singapore. P . Zhong and C. Miao are also with the AlibabaNTU Singapore Joint Research Institute and the School of Computer Science and Engineering, Nanyang Technological University, Singapore. Email: peixiang001@e.ntu.edu.sg, fwangdi, ascymiaog@ntu.edu.sgin speciÔ¨Åc frequency bands, namely delta (14 Hz), theta (47 Hz), alpha (813 Hz), beta (1330 Hz), and gamma ( >30 Hz). Many existing EEGbased emotion recognition methods are primarily based on the supervised machine learning approach, wherein features are often extracted from prepro cessed EEG signals in each channel over a time window. Then, a classiÔ¨Åer is trained on the extracted features to recognize emotions. Wang et al. [6] compared power spec tral density features (PSD), wavelet features and nonlinear dynamical features with a Support Vector Machine (SVM) classiÔ¨Åer. Zheng and Lu [7] investigated critical frequency bands and channels using PSD, differential entropy (DE) [8] and PSD asymmetry features, and obtained robust accuracy using deep belief networks (DBN). However, most existing EEGbased emotion recognition approaches do not address the following three challenges: 1) the topological structure of EEG channels are not effectively exploited to learn more discriminative EEG representations; 2) EEG signals vary sig niÔ¨Åcantly across different subjects, which hinders the gener alizability of the trained classiÔ¨Åers in subjectindependent classiÔ¨Åcation settings; and 3) participants may not always generate the intended emotions when watching emotion eliciting stimuli. Consequently, the emotion labels in the collected EEG data may be noisy and inconsistent with the actual elicited emotions. There have been several attempts to address the Ô¨Årst challenge. Zhang et al. [9] and Zhang et al. [10] incorpo rated spatial relations in EEG signals using convolutional neural networks (CNN) and recurrent neural networks (RNN), respectively. However, their approaches require a 2D representation of EEG channels on the scalp, which may cause information loss during Ô¨Çattening because channels are actually arranged in the 3D space. In addition, their approach of using CNNs and RNNs to capture interchannelarXiv:1907.07835v4  [cs.CV]  13 May 20202 relations has difÔ¨Åculty in learning longrange dependencies [11]. Graph neural networks (GNN) has been applied in [12] to capture interchannel relations using an adjacency matrix. However, similar to CNNs and RNNs, the GNN approach [12] only considers relations between the nearest channels, which thus may lose valuable information between distant channels, such as the PSD asymmetry between channels on the left and right hemispheres in the frontal region, which has been shown to be informative in valence prediction [5]. In recent years, several studies [13], [14] attempted to tackle the second challenge by investigating the transfer ability of EEGbased emotion recognition models across subjects. Lan et al. [15] compared several domain adapta tion techniques such as maximum independence domain adaptation (MIDA), transfer component analysis (TCA), subspace alignment (SA), etc. They found that the subject independent classiÔ¨Åcation accuracy can be improved by around 10%. Li et al. [16] applied domain adversarial train ing to lower the inÔ¨Çuence of individual subject on EEG data and obtained improved performance as well. However, their adversarial training does not exploit any graph structure of the EEG signals and only leads to small performance improvement in our experiment (see Section 7.1). To the best of our knowledge, no attempt has been made to address the third challenge, i.e., noisy emotion labels, in EEGbased emotion recognition. In this paper, we propose a regularized graph neural network (RGNN) aiming to address all the three aforemen tioned challenges. Graph analysis for human brain has been studied extensively in the neuroscience literature [17], [18]. However, making an accurate connectome is still an open question and subject to different scales [18]. Inspired by [12], [19], we consider each EEG channel as a node in our graph. Our RGNN model extends the simple graph convolution network (SGC) [20] and leverages the topological structure of EEG channels. SpeciÔ¨Åcally, we propose a sparse adjacency matrix to capture both local and global interchannel rela tions based on the biological topology of human brain [19]. Local interchannel relations connect nearby groups of neu rons and may reveal anatomical connectivity at macroscale [18], [21]. Global interchannel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotionrelated functional connectivity [5], [16]. In addition, we propose a nodewise domain adversar ial training (NodeDAT) method to regularize RGNN for better generalization in subjectindependent classiÔ¨Åcation scenarios. Different from the domain adversarial training in [16], [22], our NodeDAT method provides a Ô¨Ånergrained regularization by minimizing the domain discrepancies be tween features in the source and target domains for each channel/node. Moreover, we propose an emotionaware distribution learning (EmotionDL) method to address the problem of noisy labels in the datasets. Prior studies have shown that noisy labels can adversely impact classiÔ¨Åcation accuracy [23]. Instead of learning the traditional singlelabel classiÔ¨Åcation, our EmotionDL method learns a distribution of labels of the training data and thus acts as a regularizer to improve the robustness of our model against noisy labels. Finally, we conduct extensive experiments to validate the effectiveness of our RGNN model and investigate emotionrelated informative neuronal activities. In summary, the main contributions of this paper are as follows: 1) We propose a regularized graph neural network (RGNN) model to recognize emotions based on EEG signals. Our biologically inspired model captures both local and global interchannel relations. 2) We propose two regularizers: nodewise domain adversarial training (NodeDAT) and emotionaware distribution learning (EmotionDL), to improve the robustness of our model against crosssubject varia tions and noisy labels, respectively. 3) We conduct extensive experiments in both subject dependent and subjectindependent classiÔ¨Åcation settings on two public EEG datasets, namely SEED [7] and SEEDIV [24]. Experimental results demon strate the effectiveness of our proposed model and regularizers. In addition, our RGNN model achieves superior performance over the stateoftheart mod els in most experimental settings. 4) We investigate the emotional neuronal activities and the results reveal that prefrontal, parietal and occipital regions may be the most informative re gions for emotion recognition. In addition, global interchannel relations between the left and right hemispheres are important, and local interchannel relations between (FP1, AF3), (F6, F8) and (FP2, AF4) may also provide useful information. 2 R ELATED WORK "
227,Five lessons from building a deep neural network recommender.txt,"Recommendation algorithms are widely adopted in marketplaces to help users
find the items they are looking for. The sparsity of the items by user matrix
and the cold-start issue in marketplaces pose challenges for the off-the-shelf
matrix factorization based recommender systems. To understand user intent and
tailor recommendations to their needs, we use deep learning to explore various
heterogeneous data available in marketplaces. This paper summarizes five
lessons we learned from experimenting with state-of-the-art deep learning
recommenders at the leading Norwegian marketplace FINN.no. We design a hybrid
recommender system that takes the user-generated contents of a marketplace
(including text, images and meta attributes) and combines them with user
behavior data such as page views and messages to provide recommendations for
marketplace items. Among various tactics we experimented with, the following
five show the best impact: staged training instead of end-to-end training,
leveraging rich user behaviors beyond page views, using user behaviors as noisy
labels to train embeddings, using transfer learning to solve the unbalanced
data problem, and using attention mechanisms in the hybrid model. This system
is currently running with around 20% click-through-rate in production at
FINN.no and serves over one million visitors everyday.","The common business model of marketplaces is to provide an online platform where sellers post their items for sale and buyers search for items they want to purchase. The items can range from low value ones such as secondhand textbooks and clothes to highvalue ones such as cars and real estate properties. Sellers can also post nontangible items such as job opportunities. In the marketplace industry, a classified is a post from a seller selling one or multiple items, and we refer to it as an ""ad"" in this paper. An example is Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). KDD‚Äô18 Deep Learning Day, August 2018, London, UK ¬©2018 Copyright held by the owner/author(s). ACM ISBN 978xxxxxxxxxx/YY/MM. https://doi.org/10.1145/nnnnnnn.nnnnnnn Figure 1: An example of FINN.no marketplace. It shows the screen shot of a search result page on the left and an ad detail page on the right. shown in Figure 1 with a search result page on the left and an ad detail page on the right. Marketplaces can have multiple verticals, but there are often specialized marketplaces on house, car and job to adapt to the specific characteristics of those categories. In this paper, we use FINN.no (https://finn.no/), the leading online marketplace in Norway, to understand the usercontent interaction mechanism on marketplace and to experiment our recommender system with real users. It is a multivertical marketplace, but for the work described here, we focus only on the torget vertical that contains mostly secondhand daily items such as books, clothes, furnitures and pets. In essence, the FINN.no torget marketplace can be viewed as a spe cial type of ecommerce site that provides secondhand items across many different categories from a very large and scattered non professional seller community. Recommendation for marketplace ads is more challenging than the standard ecommerce product recommendation for the following reasons: 1) Many marketplace sellers are nonprofessional individuals, and the information they provide to describe the items for sale is often of lower quality (incomplete or inaccurate) and less standardized. Thus it is more challenging to solely rely on the contentbased features to iden tify similar items. 2) Location proximity is often neglected in the offtheshelf recommender solutions, but it actually plays an impor tant role in marketplaces, since the potential buyers often preferarXiv:1809.02131v2  [cs.IR]  7 Oct 2018KDD‚Äô18 Deep Learning Day, August 2018, London, UK S. Eide, A. M. √òygard, and N. Zhou to complete the transaction in person to check if the item fits their expectation. 3) Different from ecommerce sites where each product usually has abundant supply, marketplace items are often second hand and therefore unique in some sense. As a result, item volatility in marketplaces is often much higher than in the ecommerce sce nario. While in traditional recommendation scenarios the number of items is much smaller than the number of users, in marketplaces the number of ""active"" items has the same order of magnitude of the number of users. 4) FINN.no does not use the auction method, and most sellers reserve their items for the first buyer whom they reach a deal with. Therefore freshness is also a factor to take into account in recommendation solutions, so that potential buyers can find newly listed items sooner. This highlights the importance of solving the cold start problem for newly listed items. We experimented with a group of hybrid recommenders in pro duction at FINN.no to solve the challenges mentioned above. Among various things we tried, the following five have shown significant improvements in the A/B tests with users: rich user behavior sig nals in Section 4.1, user behavior trained embeddings in Section 4.2, transfer learning for images in Section 4.3, staged training strategy in Section 4.4, and attention mechanism in Section 4.5. 2 RELATED WORK "
63,Automated Rip Current Detection with Region based Convolutional Neural Networks.txt,"This paper presents a machine learning approach for the automatic
identification of rip currents with breaking waves. Rip currents are dangerous
fast moving currents of water that result in many deaths by sweeping people out
to sea. Most people do not know how to recognize rip currents in order to avoid
them. Furthermore, efforts to forecast rip currents are hindered by lack of
observations to help train and validate hazard models. The presence of web cams
and smart phones have made video and still imagery of the coast ubiquitous and
provide a potential source of rip current observations. These same devices
could aid public awareness of the presence of rip currents. What is lacking is
a method to detect the presence or absence of rip currents from coastal
imagery. This paper provides expert labeled training and test data sets for rip
currents. We use Faster-RCNN and a custom temporal aggregation stage to make
detections from still images or videos with higher measured accuracy than both
humans and other methods of rip current detection previously reported in the
literature.","RipcurrentsarethemostsigniÔ¨Åcantsafetyrisktoswim mersalongthecoastlinesofoceans,seas,andlargelakes. [8, 7,13]. Themajorityofbeachgoersdonotknowhowtoiden tifyripcurrents,andthereisnorobustandreliablelocation independent method to identify them. Globally there are thousands of drownings each year due to rip currents [23, 42]. A 20 year study by the US Lifesaving Association re ports that 81.9% of the 37,000 beach rescues each year are duetoripcurrents[7]. Therehasbeennodeclineinthenum ber of associated drowning fatalities, despite warning signs and educational material. Rip currents are a wellstudied ocean phenomenon [3, 29, 31]. They are deÔ¨Åned as strong and narrow channels of fastmoving water that Ô¨Çow towards the sea from beaches. When waves break, they form a ‚Äúsetup‚Äù or an increase in mean water level. This setup can vary along a shoreline de pendingontheamountorheightofbreakingwaves. Ripcur rentsformaswatertendstoÔ¨Çowalongshorefromregionsof high setup (larger waves) to regions of lower setup (smaller waves) where currents converge to form a seaward Ô¨Çowing rip. The speed of seaward rips can be quite strong reaching 2 m/s, faster than an Olympic swimmer. There are multiple factorsthatdeterminethelocationandstrengthofrips,such as bathymetry, wave height and direction, tide, and beach shape. Rip currents may either be transient or persistent in space and time. Rips that are frequently found at the same location are usually indicative of a fairly stable bathymetric feature such as a sand bar or reef, or a hard structure such as rocky outcrop, jetty or pier. These bathymetric features results in variations in wave breaking and setup leading to channelizedripcurrentÔ¨Çow. TransientorÔ¨Çashripsareinde pendentofbathymetryandmaymoveupordownthebeach, and may appear or disappear. ORCID(s):Lifeguards are often trained to identify rip currents. However the majority of drownings occur on beaches with outtrainedpersonnel[1,4]. Postedsignscanprovideawarn ing,butthereisevidencethatmostpeopledonotÔ¨Åndexist ing signs helpful in actually identifying rip currents [6]. Experts at the National Oceanic and Atmospheric Ad ministration (NOAA) use images and video to gather statis tics about rip currents [19]. These data are supporting the validation of a rip current forecast model to alert people to potentialhazards[20]. Themostcommonlyusedmethodto visualizeripcurrentsfromvideoistimeaveraging,summa rizing a video as a single image [28]. In [43] boosted cas cade of simple Haar like features, a machine learning tech nique, was used to detect rip currents in time averaged im ages. Howeverthesetimeaverageswhenmanuallyassessed can be misinterpreted. Furthermore, they are not readily available nor interpretable by the average beachgoers, and the process of averaging removes available information. In recent years the coastal engineering community has successfullyuseddeepneuralnetworkstosolvemanyprob lems. ClassiÔ¨Åcation problems such as classifying wave breakingininfraredimagery[9],beachsceneandotherland scapeclassiÔ¨Åcation[11],automatedplanktonimageclassiÔ¨Å cation[41] andoceanfrontrecognition[36]wereformulated as deep learning problems using convolutional neural net works. Furthermore,someregressionproblemssuchasopti calwavegauging[10],trackingremotelysensedwaves[52], typhoon forecasting [33] were also solved using deep neu ral networks. In addition, generative adversarial networks, a type of deep neural networks, were used to improve the quality of downscaling of ocean remote sensing data [18]. Object detection with deep neural networks is well studied in the computer vision community. However most benchmarks and research focus on detecting physical objects with boundaries between what is and is not anobject[16,22,37]. Ripcurrentsareephemeral‚Äúobjects‚Äù de Silva et al.: Preprint submitted to Elsevier Page 1 of 9arXiv:2102.02902v1  [cs.CV]  4 Feb 2021Automated rip current detection Figure 1: A collection of beach scenes, some of which contain dangerous rip currents. Unfortunately these ‚Äúobjects‚Äù do not have clear shape, and most people Ô¨Ånd them hard to identify. This paper describes a computer vision system with detection accuracy higher than both existing published methods and human observers. which are not observable in every frame, and amorphous without clearly deÔ¨Åned boundaries even when observable. It is not obvious whether existing methods are applicable. Figure1providesasetofexamples,illustratingthediÔ¨Éculty of the problem. Our work is aimed at introducing this problem to the coastalengineeringcommunity,andshowingthatobjectde tection methods areapplicable. We gathered training data sets of rip currents and worked with experts at NOAA to ensure that test data were labeled correctly. We use Faster RCNN [50] with a custom temporal aggregation stage that allowed us to achieve detection accuracy that is higher than bothhumansandothermethodsofripcurrentdetectionpre viously reported in the literature. The contributions of this paper are: ‚Ä¢Evidence that region based convolutional neural net works (CNN) approach for object detection is appli cabletoamorphousandephemeralobjectssuchasrip currents ‚Ä¢Analysisshowingripcurrentdetectionaccuracyabove existing published methods ‚Ä¢Data sets of rip current images and video for training and testing The remainder of the paper is organized as follows. All therelatedworkissummarizedinSection2. Wediscusshow thedatawascollectedinSection3. Ourmethodisdiscussed in 4. Results are discussed in 5. Limitations and discussion areinSection6. InSection7weconcludeourpaper. Andin AppendixAweprovidethelinktothesupplementarymate rials. 2. Related Work "
302,Fair Evaluation of Graph Markov Neural Networks.txt,"Graph Markov Neural Networks (GMNN) have recently been proposed to improve
regular graph neural networks (GNN) by including label dependencies into the
semi-supervised node classification task. GMNNs do this in a theoretically
principled way and use three kinds of information to predict labels. Just like
ordinary GNNs, they use the node features and the graph structure but they
moreover leverage information from the labels of neighboring nodes to improve
the accuracy of their predictions. In this paper, we introduce a new dataset
named WikiVitals which contains a graph of 48k mutually referred Wikipedia
articles classified into 32 categories and connected by 2.3M edges. Our aim is
to rigorously evaluate the contributions of three distinct sources of
information to the prediction accuracy of GMNN for this dataset: the content of
the articles, their connections with each other and the correlations among
their labels. For this purpose we adapt a method which was recently proposed
for performing fair comparisons of GNN performance using an appropriate
randomization over partitions and a clear separation of model selection and
model assessment.","Graph neural networks (GNN) (Yang et al., 2016; Kipf and Welling, 2017; Defferrard et al., 2016) have become a tool of choice when modeling datasets whose observations are not i.i.d. but are comprised of entities interconnected according to a graph of relations. They can be used either for graph classiÔ¨Åcation, like molecule classiÔ¨Åcation (Dobson and Doig, 2003; Borgwardt et al., 2005), or for node classiÔ¨Åcation, like document classiÔ¨Åca tion in a citation network (Sen et al., 2008). The most common task is certainly semi supervised node classiÔ¨Åcation in which unlabeled nodes of a given subset are to be classiÔ¨Åed using a distinct subset of labeled nodes, the train set, fromthe same graph (Kipf and Welling, 2017; Deffer rard et al., 2016). Inductive classiÔ¨Åcation on the other hand refers to the most common setting in machine learning in which nodes to be labeled are not known ahead of time (Hamilton et al., 2017). A number of architectures have been proposed over the years which deal with speciÔ¨Åc issues oc curring with GNNs. Some combat oversmoothing, (which is the tendency for deep GNNs to predict the same labels for all nodes) (Klicpera et al., 2018), some deal with assortativity or heterophily (which refers to situations in which neighboring nodes are likely to have different labels) (Zhu et al., 2020, 2021; Bo et al., 2021) and others still try to learn the connection weights from data using an appropri ate attention mechanism (Veli Àáckovi ¬¥c et al., 2018). Despite their diversity, these models all have one important shortcoming. Namely they assume that labels can be predicted independently for each node in the graph. In other words they neglect la bel dependencies altogether. More recently Graph Markov Neural Networks (GMNN) (Qu et al., 2019) were introduced as genuine probabilistic models which include label correlations in graphs by combining the strength of GNNs and those of conditional random Ô¨Åelds (CRF) while avoiding their limitations. These are the models we shall focus on in this work. The accuracy of the GMNN model was evalu ated for node classiÔ¨Åcation and link prediction tasks in (Qu et al., 2019) on the classical benchmark datasets Cora, Pubmed and Citeseer (Sen et al., 2008) using the public splits deÔ¨Åned in (Yang et al., 2016). Under these settings a clear improvement was demonstrated when comparing the GMNN model to existing baselines that do not account for label dependencies. However, as a number of recent works (Shchur et al., 2018; Errica et al., 2020) have pointed out, a fair evaluation of the performance of GNNs requires a procedure which performs a systematic randomization over trainarXiv:2304.01235v1  [cs.LG]  3 Apr 2023validationtest set partitions and makes clear sepa ration between model selection and model assess ment. Our aim in this paper is to subject GMNN to such a rigorous performance analysis on a new, rel atively large graph of documents named WikiVitals . In a Ô¨Årst step we shall rigorously evaluate the effect on the accuracy of a classiÔ¨Åer of leveraging the graph structure. This is done by comparing a basic GNN with a graph agnostic baseline such as a MLP. In a second step we shall estimate the increase in accuracy that results from taking into account label correlations using a GMNN on top of a basic GNN. For completeness we also perform the same thor ough analysis on the classical benchmark datasets mentioned above. In summary, our contributions1are: ‚Ä¢We introduce a new dataset of interconnected documents named WikiVitals . Compared to the classical benchmark datasets this is a rela tively large graph comprising 48k nodes clas siÔ¨Åed into 32 categories and connected by 2.3M edges. ‚Ä¢We apply the fair comparison procedure pro posed in (Errica et al., 2020) to a GMNN which is a sophisticated node classiÔ¨Åcation model. So far only graph classiÔ¨Åcation mod els had been evaluated in this manner. ‚Ä¢We evaluate the respective contributions to the accuracy of classifying WikiVitals articles when Ô¨Årst including the graph structure infor mation using a common GNN and next when leveraging the label correlations information using a GMNN model on top of that GNN. 2 Related Work "
218,Unsupervised Learning of Visual Representations using Videos.txt,"Is strong supervision necessary for learning a good visual representation? Do
we really need millions of semantically-labeled images to train a Convolutional
Neural Network (CNN)? In this paper, we present a simple yet surprisingly
powerful approach for unsupervised learning of CNN. Specifically, we use
hundreds of thousands of unlabeled videos from the web to learn visual
representations. Our key idea is that visual tracking provides the supervision.
That is, two patches connected by a track should have similar visual
representation in deep feature space since they probably belong to the same
object or object part. We design a Siamese-triplet network with a ranking loss
function to train this CNN representation. Without using a single image from
ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train
an ensemble of unsupervised networks that achieves 52% mAP (no bounding box
regression). This performance comes tantalizingly close to its
ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We
also show that our unsupervised network can perform competitively in other
tasks such as surface-normal estimation.","What is a good visual representation and how can we learn it? At the start of this decade, most computer vision research focused on ‚Äúwhat‚Äù and used handdeÔ¨Åned features such as SIFT [32] and HOG [5] as the underlying visual representation. Learning was often the last step where these lowlevel feature representations were mapped to seman tic/3D/functional categories. However, the last three years have seen the resurgence of learning visual representations directly from pixels themselves using the deep learning and Convolutional Neural Networks (CNNs) [28, 24, 23]. At the heart of CNNs is a completely supervised learning paradigm. Often millions of examples are Ô¨Årst labeled us ing Mechanical Turk followed by data augmentation to cre ate tens of millions of training instances. CNNs are then trained using gradient descent and back propagation. But one question still remains: is strongsupervision necessary for training these CNNs? Do we really need millions of semanticallylabeled images to learn a good representation? ‚Ä¶ ‚Ä¶   ‚Ä¶ ‚Ä¶  Learning to Rank   Conv   Net Conv   Net Conv   Net  Query   (First Frame)  Tracked   (Last Frame)  Negative   (Random)  (a) Unsupervised Tracking in Videos   ùê∑  ,  ùê∑ ,  ùê∑ ,  ùê∑ ,  ùê∑: Distance in deep feature space   (b) Siamese triplet Network  (c) Ranking Objective  Figure 1. Overview of our approach. (a) Given unlabeled videos, we perform unsupervised tracking on the patches in them. (b) Triplets of patches including query patch in the initial frame of tracking, tracked patch in the last frame, and random patch from other videos are fed into our siamesetriplet network for train ing. (c) The learning objective: Distance between the query and tracked patch in feature space should be smaller than the distance between query and random patches. It seems humans can learn visual representations using little or no semantic supervision but our approaches still remain completely supervised. In this paper, we explore the alternative: how we can ex ploit the unlabeled visual data on the web to train CNNs (e.g. AlexNet [24])? In the past, there have been several at tempts at unsupervised learning using millions of static im ages [26, 44] or frames extracted from videos [56, 48, 34]. The most common architecture used is an autoencoder which learns representations based on its ability to recon struct the input images [35, 3, 49, 37]. While these ap proaches have been able to automatically learn V1like Ô¨Ål ters given unlabeled data, they are still far away from su pervised approaches on tasks such as object detection. So, what is the missing link? We argue that static images them selves might not have enough information to learn a good 1arXiv:1505.00687v2  [cs.CV]  6 Oct 2015visual representation. But what about videos? Do they have enough information to learn visual representations? In fact, humans also learn their visual representations not from mil lions of static images but years of dynamic sensory inputs. Can we have similar learning capabilities for CNNs? We present a simple yet surprisingly powerful approach for unsupervised learning of CNNs using hundreds of thou sands of unlabeled videos from the web. Visual tracking is one of the Ô¨Årst capabilities that develops in infants and often before semantic representations are learned1. Taking a leaf from this observation, we propose to exploit visual track ing for learning CNNs in an unsupervised manner. SpeciÔ¨Å cally, we track millions of ‚Äúmoving‚Äù patches in hundreds of thousands of videos. Our key idea is that two patches con nected by a track should have similar visual representation in deep feature space since they probably belong to the same object. We design a Siamesetriplet network with ranking loss function to train the CNN representation. This ranking loss function enforces that in the Ô¨Ånal deep feature space the Ô¨Årst frame patch should be much closer to the tracked patch than any other randomly sampled patch. We demon strate the strength of our learning algorithm using exten sive experimental evaluation. Without using a single image from ImageNet, just 100K unlabeled videos and VOC 2012 dataset, we train an ensemble of AlexNet networks that achieves 52% mAP (no bounding box regression). This per formance is similar to its ImageNetsupervised counterpart, an ensemble which achieves 54:4%mAP. We also show that our network trained using unlabeled videos achieves simi lar performance to its completely supervised counterpart on other tasks such as surface normal estimation. We believe this is the Ô¨Årst time an unsupervisedpretrained CNN has been shown so competitive; that too on varied datasets and tasks. SpeciÔ¨Åcally for VOC, we would like to put our re sults in context: this is the best results tilldate by using only PASCALprovided annotations (next best is scratch at 44%). 2. Related Work "
143,Adaptive Sample Selection for Robust Learning under Label Noise.txt,"Deep Neural Networks (DNNs) have been shown to be susceptible to memorization
or overfitting in the presence of noisily-labelled data. For the problem of
robust learning under such noisy data, several algorithms have been proposed. A
prominent class of algorithms rely on sample selection strategies wherein,
essentially, a fraction of samples with loss values below a certain threshold
are selected for training. These algorithms are sensitive to such thresholds,
and it is difficult to fix or learn these thresholds. Often, these algorithms
also require information such as label noise rates which are typically
unavailable in practice. In this paper, we propose an adaptive sample selection
strategy that relies only on batch statistics of a given mini-batch to provide
robustness against label noise. The algorithm does not have any additional
hyperparameters for sample selection, does not need any information on noise
rates and does not need access to separate data with clean labels. We
empirically demonstrate the effectiveness of our algorithm on benchmark
datasets.","The deep learning models, which are highly effective in many applications, need vast amounts of training data. Such largescale labelled data is often generated through crowd sourcing or automated labeling, which naturally cause ran dom labelling errors. In addition, subjective biases in hu man annotators too can cause such errors. The training of deep networks is adversely affected by label noise and hence robust learning under label noise is an important problem of current interest. In recent years many different approaches for robust learning of classifiers have been proposed, such as, robust loss functions [9, 6, 53, 42], loss correction [35], meta learning [25, 43], sample reweighting [38, 39, 41, 16, 11], etc. In this paper we present a novel algorithm that adap tively selects samples based on the statistics of observed 1Codes for reproducibility will be made available here: https://github.com/dbp1994/masters_thesis_codes/ tree/main/BAREloss values in a minibatch and achieves good robustness to label noise. Our algorithm does not use any additional sys tem for learning weights for examples, does not need extra data with clean labels and does not assume any knowledge of noise rates. The algorithm is motivated by curriculum learning and can be thought of as a way to design an adap tive curriculum. The curriculum learning [4, 21] is a general strategy of sequencing of examples so that the networks learn from the ‚Äòeasy‚Äô examples well before learning from the ‚Äòhard‚Äô ones. This is often brought about by giving different weights to different examples in the training set. Many of the recent algorithms for robust learning based on sample reweighting can be seen as motivated by a similar idea. A good justifica tion for this approach comes from some recent studies [50] that have shown that deep networks can learn to achieve zero training error on completely randomly labelled data, a phenomenon termed as ‚Äòmemorization‚Äô. Further studies such as [3, 29] have shown that the networks, when trained on noisilylabelled data, learn simpler patterns first before overfitting to the noisilylabelled data. In the last few years, several strategies have been pro posed that aim to select (or give more weightage to) ‚Äòclean‚Äô samples for obtaining a degree of robustness against label noise (e.g., [16, 11, 49, 47, 27, 38, 13]). All such methods essentially employ the heuristic of ‚Äòsmall loss‚Äô for sample selection wherein (a fraction of) smallloss valued samples are preferentially used for learning the network. Many of these methods use an auxiliary network to assess the loss of an example or to learn to map loss values to sample weights. Such methods need additional computing resources to learn multiple networks and may also need separate clean data (without label noise) and the methods involve careful choice of additional hyperparameters. In general, it is difficult to directly relate the loss value of an example with the relia bility of its label. Loss value of any specific example is it self a function of the current state of learning and it evolves with epochs. Loss values of even clean samples may change over a significant range during the course of learning. Fur ther, the loss values achievable by a network even on cleanarXiv:2106.15292v3  [cs.LG]  5 Dec 2022samples may be different for examples of different classes. Motivated by these considerations, we propose a sim ple, adaptive selection strategy called BAtch REweighting (BARE ). Our algorithm utilizes the statistics of loss values in a minibatch to compute the threshold for sample selec tion in that minibatch. Since, it is possible that this auto matically calculated threshold is different for different mini batches even within the same epoch, our method amounts to using a dynamic threshold which naturally evolves as learning proceeds. In addition, while calculating the batch statistics we take into consideration the class labels also and hence the dynamic thresholds are also dependent on the given labels of the examples. The main contribution of this paper is an adaptive sam ple selection strategy for robust learning that is simple to implement, does not need any clean validation data, needs no knowledge at all of the noise rates and also does not have any hyperparameters in the sample selection strategy. It does not need any auxiliary network for sample selec tion. We empirically demonstrate the effectiveness of our algorithm on benchmark datasets: MNIST [22], CIFAR10 [19], and Clothing1M [46] and show that our algorithm is much more efficient in terms of time and has as good or better robustness compared to other algorithms for different types of label noise and noise rates. The rest of the paper is organized as follows: Section 2 discusses related work, Section 3 discusses our proposed al gorithm. Section 4 discusses our empirical results and con cluding remarks are provided in Section 5. 2. Related Work "
242,Deep Self-Learning From Noisy Labels.txt,"ConvNets achieve good results when training from clean data, but learning
from noisy labels significantly degrades performances and remains challenging.
Unlike previous works constrained by many conditions, making them infeasible to
real noisy cases, this work presents a novel deep self-learning framework to
train a robust network on the real noisy datasets without extra supervision.
The proposed approach has several appealing benefits. (1) Different from most
existing work, it does not rely on any assumption on the distribution of the
noisy labels, making it robust to real noises. (2) It does not need extra clean
supervision or accessorial network to help training. (3) A self-learning
framework is proposed to train the network in an iterative end-to-end manner,
which is effective and efficient. Extensive experiments in challenging
benchmarks such as Clothing1M and Food101-N show that our approach outperforms
its counterparts in all empirical settings.","Deep Neural Networks (DNNs) achieve impressive re sults on many computer vision tasks such as image recog nition [13, 33, 34], semantic segmentation [22, 40, 24], ob ject detection [5, 30, 27, 18] and cross modality tasks [20, 21, 41]. However, many of these tasks require largescale datasets with reliable and clean annotations to train DNNs such as ImageNet [2] and MSCOCO [19]. But collect ing largescale datasets with precise annotations is expen sive and timeconsuming, preventing DNNs from being em ployed in realworld noisy scenarios. Moreover, most of the ‚Äúground truth annotations‚Äù are from human labelers, who also make mistakes and increase biases of the data. An alternative solution is to collect data from the Inter net by using different imagelevel tags as queries. These tags can be regarded as labels of the collected images. This solution is cheaper and more timeefÔ¨Åcient than human an notations, but the collected labels may contain noises. A lot of previous work has shown that noisy labels lead to an obvious decrease in performance of DNNs [38, 23, 26]. Therefore, attentions have been concentrated on how to im Prototype Decision Boundary Prototypes Prototypes Prototype Decision BoundaryFigure 1. An example of solving two classes classiÔ¨Åcation problem using different number of prototypes. Left: Original data distribu tion. Data points with the same color belong to the same class. Upper Right : The decision boundary obtained by using a single prototype for each class. Lower Right : The decision boundary obtained by two prototypes for each class. Two prototypes for each class leads to a better decision boundary. prove the robustness of DNNs against noisy labels. Previous approaches tried to correct the noisy labels by introducing a transition matrix [25, 9] into their loss functions, or by adding additional layers to estimate the noises [6, 32]. Most of these methods followed a simple assumption to simplify the problem: There is a single tran sition probability between the noisy label and groundtruth label, and this probability is independent of individual sam ples. But in real cases, the appearance of each sample has much inÔ¨Çuence on whether it can be misclassiÔ¨Åed. Due to this assumption, although these methods worked well on handcrafted noisy datasets such as CIFAR10 [12] with manually Ô¨Çipped noisy labels, their performances were lim ited on real noisy datasets such as Clothing1M [38] and Food101N [15]. Also, noisy tolerance loss functions [35, 39] have been developed to Ô¨Åght against label noises, but they had a simi lar assumption as the above noise correction approaches. So they were also infeasible for realworld noisy datasets. Fur thermore, many approaches [15, 17, 37] solved this probarXiv:1908.02160v2  [cs.CV]  20 Aug 2019lem by using additional supervision. For instance, some of them manually selected a part of samples and asked human labelers to clean these noisy labels. By using extra super vision, these methods could improve the robustness of deep networks against noises. The main drawback of these ap proaches was that they required extra clean samples, mak ing them expensive to apply in largescale realworld sce narios. Among all the above work, CleanNet [15] achieved the existing stateoftheart performance on realworld dataset such as Clothing1M [38]. CleanNet used ‚Äúclass prototype‚Äù (i.e. a representative sample) to represent each class cate gory and decided whether the label for a sample is correct or not by comparing with the prototype. However, CleanNet also needed additional information or supervision to train. To address the above issues, we propose a novel frame work of SelfLearning with MultiPrototypes (SMP), which aims to train a robust network on the real noisy dataset with out extra supervision. By observing the characteristics of samples in the same noisy category, we conjecture that these samples have widely spread distribution. A single class pro totype is hard to represent all characteristics of a category. More prototypes should be used to get a better represen tation of characteristics. Figure 1 illustrated the case and further exploration has been conducted in the experiment. Furthermore, extra information (supervision) is not neces sarily available in practice. The proposed SMP trains in an iterative manner which contains two phases: the Ô¨Årst phase is to train a network with the original noisy label and corrected label generated in the second phase. The second phase uses the network trained in the Ô¨Årst stage to select several prototypes. These prototypes are used to generate the corrected label for the Ô¨Årst stage. This framework does not rely on any assump tion on the distribution of noises, which makes it feasible to realworld noises. It also does not use accessorial neural networks nor require additional supervision, providing an effective and efÔ¨Åcient training scheme. The contributions of this work are summarized as fol lows. (1) We propose an iterative learning framework SMP to relabel the noisy samples and train ConvNet on the real noisy dataset, without using extra clean supervision. Both the relabeling and training phases contain only one single ConvNet that can be shared across different stages, mak ing SMP effective and efÔ¨Åcient to train. (2) SMP results in interesting Ô¨Åndings for learning from noisy data. For exam ple, unlike previous work [15], we show that a single pro totype may not be sufÔ¨Åcient to represent a noisy class. By extracting multiple prototypes for a category, we demon strate that more prototypes would get a better representa tion of a class and obtain better labelcorrection results. (3) Extensive experiments validate the effectiveness of SMP on different realworld noisy datasets. We demonstrate newstateoftheart performance on all these datasets. 2. Related Work "
258,Semi-supervised Acoustic Event Detection based on tri-training.txt,"This paper presents our work of training acoustic event detection (AED)
models using unlabeled dataset. Recent acoustic event detectors are based on
large-scale neural networks, which are typically trained with huge amounts of
labeled data. Labels for acoustic events are expensive to obtain, and relevant
acoustic event audios can be limited, especially for rare events. In this paper
we leverage an Internet-scale unlabeled dataset with potential domain shift to
improve the detection of acoustic events. Based on the classic tri-training
approach, our proposed method shows accuracy improvement over both the
supervised training baseline, and semisupervised self-training set-up, in all
pre-defined acoustic event detection tasks. As our approach relies on ensemble
models, we further show the improvements can be distilled to a single model via
knowledge distillation, with the resulting single student model maintaining
high accuracy of teacher ensemble models.","Acoustic event detection (AED) is the task of detecting whether certain events occur in an audio clip. It can be applied in many areas such as surveillance [1, 2], and recommenda tion systems [3]. Conventionally AED has been addressed with automatic speech recognition techniques, e.g. with fea tures such as melfrequency cepstrum coefÔ¨Åcients (MFCC) and classiÔ¨Åers based on hidden markov model (HMM). In recent years, with the advances in speech recognition [4] and image recognition [5] as well as size increasing of datasets [6][7], there are more deep learning based approaches applied to tackle AED tasks. For instance, the recently proposed Au dioset [6] comprises 1,789,621 10second audio segments from a wide domain of 632 categories. Convolutional neu ral network (CNN) [8, 9] or CNNbased approaches (e.g convolutional recurrent neural network [10, 11]) are used and have shown improvements over traditional approaches. Though accuracy has been much improved in many AED tasks, stateoftheart models often requires large number oflabeled training data. Labeled data can be be quite limited under certain scenarios (e.g., for rare events [12]). The focus of this paper is on leveraging unlabeled audios to improve accuracy for AED. Our main contributions include the following: (1). We propose an ensemble method based on the classic tritraining that shows improvements in all acoustic events we investigate in a realistic semisupervised setting (Internetscale unlabeled dataset with domain shift) (2). We show the improvements of the ensembled models can be distilled into a single model via knowledge distillation. As a result, there is no increase of computational costs during inference. 2. RELATED WORK "
566,Hierarchically-Refined Label Attention Network for Sequence Labeling.txt,"CRF has been used as a powerful model for statistical sequence labeling. For
neural sequence labeling, however, BiLSTM-CRF does not always lead to better
results compared with BiLSTM-softmax local classification. This can be because
the simple Markov label transition model of CRF does not give much information
gain over strong neural encoding. For better representing label sequences, we
investigate a hierarchically-refined label attention network, which explicitly
leverages label embeddings and captures potential long-term label dependency by
giving each word incrementally refined label distributions with hierarchical
attention. Results on POS tagging, NER and CCG supertagging show that the
proposed model not only improves the overall tagging accuracy with similar
number of parameters, but also significantly speeds up the training and testing
compared to BiLSTM-CRF.","Conditional random Ô¨Åelds (CRF) (Lafferty et al., 2001) is a stateoftheart model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, pre vious work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTMsoftmax gives better accuracies compared to BiLSTM CRF for partofspeech (POS) tagging. In addi tion, the stateoftheart neural Combinatory Cat egorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong rep resentation power of neural sentence encoders such as BiLSTMs allow models to capture im plicit longrange label dependencies from input TheycanfishandalsotomatoeshereInput PRP0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ MD0.7VB0.2‚Ä¶‚Ä¶ VB0.5NN0.4‚Ä¶‚Ä¶ CC0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ RB0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ NN0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ RB0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ PRP0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ MD0.3VB0.7‚Ä¶‚Ä¶ NN0.8VB0.2‚Ä¶‚Ä¶ CC0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ RB0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ NN0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ RB0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶PRPVBNNCCRBNNRB First LayerLast LayerOutputFigure 1: Visualization of hierarchicallyreÔ¨Åned Label Attention Network for POS tagging. The numbers in dicate the label probability distribution for each word. word sequences alone (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Teng and Zhang, 2018), thereby allowing the output layer to make local predictions. In contrast, though explicitly capturing output label dependencies, CRF can be limited by its Markov assumptions, particularly when being used on top of neural encoders. In addition, CRF can be computationally expensive when the number of labels is large, due to the use of Viterbi decoding. One interesting research question is whether there is neural alternative to CRF for neural se quence labeling, which is both faster and more ac curate. To this question, we investigate a neural network model for output label sequences. In par ticular, we represent each possible label using an embedding vector, and aim to encode sequences of label distributions using a recurrent neural net work. One main challenge, however, is that the number of possible label sequences is exponential to the size of input. This makes our task essen tially to represent a fullexponential search space without making Markov assumptions. We tackle this challenge using a hierarchically reÔ¨Åned representation of marginal label distribu tions. As shown in Figure 1, our model consists of a multilayer neural network. In each layer, each input words is represented together with itsarXiv:1908.08676v3  [cs.CL]  7 Nov 2019marginal label probabilities, and a sequence neu ral network is employed to model unbounded de pendencies. The marginal distributions space are reÔ¨Åned hierarchically bottomup, where a higher layer learns a more informed label sequence dis tribution based on information from a lower layer. For instance, given a sentence ‚ÄúThey 1can2Ô¨Åsh3 and4also 5tomatoes 6here 7‚Äù, the label distribu tions of the words can2andÔ¨Åsh3in the Ô¨Årst layer of Figure 1 have higher probabilities on the tags MD (modal verb) and VB (base form verb), re spectively, though not fully conÔ¨Ådently. The ini tial label distributions are then fed as the inputs to the next layer, so that longrange label depen dencies can be considered. In the second layer, the network can learn to assign a noun tag to Ô¨Åsh3 by taking into account the highly conÔ¨Ådent tag ging information of tomatoes 6(NN) , resulting in the pattern ‚Äú can2(VB) Ô¨Åsh 3(NN) ‚Äù. As shown in Figure 2, our model consists of stacked attentive BiLSTM layers, each of which takes a sequence of vectors as input and yields a sequence of hidden state vectors together with a sequence of label distributions. The model performs attention over label embeddings (Wang et al., 2015; Zhang et al., 2018a) for deriving a marginal label distributions, which are in turn used to calculate a weighted sum of label embeddings. Finally, the resulting packed label vector is used together with input word vectors as the hidden state vector for the current layer. Thus our model is named label attention network (LAN). For se quence labeling, the input to the whole model is a sentence and the output is the label distributions of each word in the Ô¨Ånal layer. BiLSTMLAN can be viewed as a form of multilayered BiLSTMsoftmax sequence labeler. In particular, a singlelayer BiLSTMLAN is iden tical to a singlelayer BiLSTMsoftmax model, where the label embedding table serves as the soft max weights in BiLSTMsoftmax, and the label attention distribution is the softmax distribution in BiLSTMsoftmax. The traditional way of mak ing a multilayer extention to BiLSTMsoftmax is to stack multiple BiLSTM encoder layers before the softmax output layer, which learns a deeper input representation. In contrast, a multilayer BiLSTMLAN stacks both the BiLSTM encoder layer and the softmax output layer, learning a deeper representation of both the input and can didate output sequences.On standard benchmarks for POS tagging, NER and CCG supertagging, our model achieves sig niÔ¨Åcantly better accuracies and higher efÔ¨Åcien cies than BiLSTMCRF and BiLSTMsoftmax with similar number of parameters. It gives highly competitive results compared with top performance systems on WSJ, OntoNotes 5.0 and CCGBank without external training. In addition to accuracy and efÔ¨Åciency, BiLSTMLAN is also more interpretable than BiLSTMCRF thanks to visualizable label embeddings and label distri butions. Our code and models are released at https://github.com/Nealcly/LAN . 2 Related Work "
429,Co-Seg: An Image Segmentation Framework Against Label Corruption.txt,"Supervised deep learning performance is heavily tied to the availability of
high-quality labels for training. Neural networks can gradually overfit
corrupted labels if directly trained on noisy datasets, leading to severe
performance degradation at test time. In this paper, we propose a novel deep
learning framework, namely Co-Seg, to collaboratively train segmentation
networks on datasets which include low-quality noisy labels. Our approach first
trains two networks simultaneously to sift through all samples and obtain a
subset with reliable labels. Then, an efficient yet easily-implemented label
correction strategy is applied to enrich the reliable subset. Finally, using
the updated dataset, we retrain the segmentation network to finalize its
parameters. Experiments in two noisy labels scenarios demonstrate that our
proposed model can achieve results comparable to those obtained from supervised
learning trained on the noise-free labels. In addition, our framework can be
easily implemented in any segmentation algorithm to increase its robustness to
noisy labels.","Recent years have witnessed an upsurge of interests in biomedical segmentation. Based on fully convolutional net  works, UNet [1] has been emerging as a classic model which concatenates multiscale features from the downsampling layers and the upsampling layers. By stacking two UNet architectures on top of each other, DoubleUnet [2] is an improved version of UNet aiming to achieve higher perfor mance on speciÔ¨Åc tasks. CENet [3] modiÔ¨Åed UNet structure by adopting pretrained ResNet blocks in the feature encodin g step to capture highlevel spatial information. However, t hese fully supervised learning algorithms are vulnerable to lab el noise and their performance may be hugely degenerated by noisy labels. Therefore, under noisy labels, it is importan t to identify and selectively learn from a clean and reliable sub set of samples which mainly include data with clean labels, rather than learning from the whole sample set. How to improve deep learning performance under noisy labels conditions has caught great attention [4, 5, 6, 7, 8, 9 ]. One direction is to estimate the mathematical model of a nois e distribution [6, 7]. In [7], two procedures were proposed fo r loss correction and noise transition matrix estimation. An  other direction is to directly train on clean samples [8, 9]. Co teaching [8] trains two networks simultaneously to pick cle an samples for each one. However, most current approaches fo cus on classiÔ¨Åcation tasks, which cannot be applied to the se g mentation where labels are spatially arranged in a dense man  ner. Finally, samplebased reweighting methods [8, 9, 10] j ust ignore or assign small weights on noisy samples, which can lead to severe overÔ¨Åtting, especially for small datasets. In this paper, we propose a novel deep learning frame work for image segmentation, namely CoSegmentation (Co Seg), to handle noisy labels. Our framework integrates the idea of selective training and label correction. In particu lar, we propose a robust training network to collaboratively lea rn and select samples with reliable labels. Then a label correc  tion scheme is proposed to enrich the reliable dataset and we retrain a new network on the updated dataset. Experimental results using CoSeg on noisy labels show performance com parable to supervised learning on noisefree labels. In sum  mary, this paper has the following contributions: (1) We develop an easilyimplemented yet effective frame work for image segmentation tasks with noisy labels. It can be easily applied to any deep learning segmentation model to increase learning ability under noisy labels. (2) We demonstrate that, in multiple noise settings, our mod el achieves comparable results to supervised training on nois e free datasets. 2. METHODOLOGY "
522,Admix: Enhancing the Transferability of Adversarial Attacks.txt,"Deep neural networks are known to be extremely vulnerable to adversarial
examples under white-box setting. Moreover, the malicious adversaries crafted
on the surrogate (source) model often exhibit black-box transferability on
other models with the same learning task but having different architectures.
Recently, various methods are proposed to boost the adversarial
transferability, among which the input transformation is one of the most
effective approaches. We investigate in this direction and observe that
existing transformations are all applied on a single image, which might limit
the adversarial transferability. To this end, we propose a new input
transformation based attack method called Admix that considers the input image
and a set of images randomly sampled from other categories. Instead of directly
calculating the gradient on the original input, Admix calculates the gradient
on the input image admixed with a small portion of each add-in image while
using the original label of the input to craft more transferable adversaries.
Empirical evaluations on standard ImageNet dataset demonstrate that Admix could
achieve significantly better transferability than existing input transformation
methods under both single model setting and ensemble-model setting. By
incorporating with existing input transformations, our method could further
improve the transferability and outperforms the state-of-the-art combination of
input transformations by a clear margin when attacking nine advanced defense
models under ensemble-model setting. Code is available at
https://github.com/JHL-HUST/Admix.","A great number of works [7, 2, 1] have shown that deep neural networks (DNNs) are vulnerable to adversarial ex amples [31, 7], i.e. the malicious crafted inputs that are *Corresponding author.indistinguishable from the legitimate ones but can induce misclassiÔ¨Åcation on the deep learning models. Such vul nerability poses potential threats to securitysensitive appli cations, e.g. face veriÔ¨Åcation [28], autonomous driving [6] and has inspired a sizable body of research on adversar ial attacks [22, 2, 21, 4, 16, 5, 38, 18]. Moreover, the ad versaries often exhibit transferability across neural network models [25], in which the adversarial examples generated on one model may also mislead other models. The adver sarial transferability matters because hackers may attack a realworld DNN application without knowing any informa tion of the target model. However, under whitebox set ting where the attacker has complete knowledge of the tar get model, existing attacks [2, 11, 1, 21] have demonstrated great attack performance but with comparatively low trans ferability against models with defense mechanisms [21, 33], making it inefÔ¨Åcient for realworld adversarial attacks. To improve the transferability of adversarial attacks, var ious techniques have been proposed, such as advanced gra dient calculations [4, 18, 35], ensemblemodel attacks [19, 15], input transformations [38, 5, 18, 10] and modelspeciÔ¨Åc methods [36]. The input transformation ( e.g. randomly re sizing and padding, translation, scale etc.) is one of the most effective approaches. Nevertheless, we observe that exist ing methods are all applied on a single input image. Since adversarial attacks aim to mislead the DNNs to classify the adversary into other categories, it naturally inspires us to ex plore whether we could further enhance the transferability by incorporating the information from other categories. Themixup operation, that linearly interpolates two ran dom images and corresponding labels, is Ô¨Årstly proposed as a data augmentation approach to improve the generaliza tion of standard training [41, 34, 40]. Recently, mixup is also used for inference [24] or adversarial training [12, 14] to enhance the model robustness. Since mixup adopts the information of a randomly picked image, we try to directly adopt mixup to craft adversaries but Ô¨Ånd that the attack per formance decays signiÔ¨Åcantly under whitebox setting with little improvement on transferability. To craft highly transarXiv:2102.00436v3  [cs.CV]  18 Aug 2021ferable adversaries with the information from other cate gories but not harm the whitebox attack performance, we propose a novel attack method called Admix that calculates the gradient on the admixed image combined with the orig inal input and images randomly picked from other cate gories. Unlike mixup that treats the two images equally and mixes their labels accordingly, the admix operation adds a small portion of the addin image from other categories to the original input but does not change the label. Thus Admix attack could obtain diverse inputs for gradient calculation. Empirical evaluations on standard ImageNet dataset [26] demonstrate that, compared with existing input transforma tions [38, 5, 18], the proposed Admix attack achieves sig niÔ¨Åcantly higher attack success rates under blackbox set ting and maintains similar attack performance under white box setting. By incorporating Admix with other input trans formations, the transferability of the crafted adversaries could be further improved. Besides, the evaluation of the integrated method under the ensemblemodel setting [19] against nine advanced defense methods [17, 37, 39, 20, 8, 3, 27, 23] demonstrates that the Ô¨Ånal integrated method, termed Admix TIDIM, outperforms the stateoftheart SI TIDIM [18] by a clear margin of 3.4% on average, which further demonstrates the high effectiveness of Admix . 2. Related Work "
140,Person Re-Identification with a Locally Aware Transformer.txt,"Person Re-Identification is an important problem in computer vision-based
surveillance applications, in which the same person is attempted to be
identified from surveillance photographs in a variety of nearby zones. At
present, the majority of Person re-ID techniques are based on Convolutional
Neural Networks (CNNs), but Vision Transformers are beginning to displace pure
CNNs for a variety of object recognition tasks. The primary output of a vision
transformer is a global classification token, but vision transformers also
yield local tokens which contain additional information about local regions of
the image. Techniques to make use of these local tokens to improve
classification accuracy are an active area of research. We propose a novel
Locally Aware Transformer (LA-Transformer) that employs a Parts-based
Convolution Baseline (PCB)-inspired strategy for aggregating globally enhanced
local classification tokens into an ensemble of $\sqrt{N}$ classifiers, where
$N$ is the number of patches. An additional novelty is that we incorporate
blockwise fine-tuning which further improves re-ID accuracy. LA-Transformer
with blockwise fine-tuning achieves rank-1 accuracy of $98.27 \%$ with standard
deviation of $0.13$ on the Market-1501 and $98.7\%$ with standard deviation of
$0.2$ on the CUHK03 dataset respectively, outperforming all other
state-of-the-art published methods at the time of writing.","In recent years, Person ReIdentiÔ¨Åcation(reID) has gained a lot of attention due to its foundational role in computer vision based video surveillance applications. Person reID is predominantly considered as a feature embedding problem. Given a query image and a large set of gallery images, person reID generates the feature embedding of each image and then ranks the similarity between query and gallery image vectors. This can be used to reidentify the person in photographs obtained by nearby surveillance cameras. Recently, Vision Transformer (ViT) as introduced by Dosovitskiy et al. [2020] is gaining substantial traction for image recognition problems. While some methods for image classiÔ¨Åcation [Dosovitskiy et al., 2020, Touvron et al., 2020], and for image retrieval [ElNouby et al., 2021] are focused only on theclassiÔ¨Åcation token , some approaches utilize the fact that local tokens , which are also outputs of the transformer encoder, can be used to improve performance of many computer vision applications including image segmentation [Wu et al., 2020, Wang et al., 2021, Chen et al., 2021], object detection [Beal et al., 2020, Wang et al., 2021] and even person reID [He et al., 2021]. Nevertheless, at present, approaches to make use of local and global tokens are an active area of research. Preprint. Under review.arXiv:2106.03720v2  [cs.CV]  8 Jun 2021In the words of Beal et al. [2020], ""The remaining tokens in the sequence are used only as features for the Ô¨Ånal class token to attend to. However, these unused outputs correspond to the input patches , and in theory, could encode local information useful for performing object detection"" .Beal et al. [2020] observed that the local tokens , although theoretically inÔ¨Çuenced by global information, also have substantial correspondence to the original input patches. One might therefore consider the possibility of using these local tokens as an enhanced feature representation of the original image patches to more strongly couple vision transformer encoders to fully connected (FC) classiÔ¨Åcation techniques. This coupling of local patches with FC classiÔ¨Åcation techniques is the primary intuition behind the LATransformer architectural design. Partbased Convolutional Baseline (PCB) [Sun et al., 2018] is a strong convolutional baseline technique for person reID and has inspired many stateoftheart models [Yao et al., 2018, Guo et al., 2019, Zheng et al., 2019]. PCB partitions the feature vector received from the backbone network into six vertical regions and constructs an ensemble of regional classiÔ¨Åers with a voting strategy to determine the predicted class label. A limitation of PCB is that each regional classiÔ¨Åer ignores the global information which is also very important for recognition and identiÔ¨Åcation. Nevertheless, PCB has achieved much success despite this limitation, and as such the design of LATransformer uses a PCBlike strategy to combine globally enhanced local tokens . Our work also improves on the recent results of He et al. [2021], who was the Ô¨Årst to employ Vision Transformers to person reID and achieved results comparable to the current stateoftheart CNN based models. Our approach extends He et al. [2021] in several ways but primarily because we aggregate the globally enhanced local tokens using a PCBlike strategy that takes advantage of the spatial locality of these tokens. Although He et al. [2021] makes use of Ô¨Ånegrained local tokens , it does so with a ShufÔ¨ÇeNet [Zhang et al., 2017] like Jigsaw shufÔ¨Çing step which does not take advantage of the 2D spatial locality information inherent in the ordering of the local tokens. LATransformer overcomes this limitation by using a PCBlike strategy to combine the globally enhanced local tokens while Ô¨Årst preserving their ordering in correspondence with the image dimension. An additional novelty of our approach is the use of blockwise Ô¨Ånetuning which we Ô¨Ånd is able to further improve the classiÔ¨Åcation accuracy of LATransformer for person reID. Blockwise Ô¨Ånetuning is viable as a form of regularization when training models with a large number of parameters over relatively small indomain datasets. Howard and Ruder [2018] advocate for blockwise Ô¨Ånetuning orgradual unfreezing particularly when training language models due to a large number of fully connected layers. As vision transformers also have high connectivity, we Ô¨Ånd that this approach is able to further improve the classiÔ¨Åcation accuracy for LATransformer. This paper is organized as follows: Firstly, we discuss related work involving Transformer archi tectures and other related methodologies in person reID. Secondly, we describe the architecture of LATransformer, including the novel locally aware network and blockwise Ô¨Ånetuning techniques. Finally, we present quantitative results of the person reID including mAP and rank1 analysis on the market1501 and CUHK03 datasets. 2 Related Work "
32,A Multi-cascaded Model with Data Augmentation for Enhanced Paraphrase Detection in Short Texts.txt,"Paraphrase detection is an important task in text analytics with numerous
applications such as plagiarism detection, duplicate question identification,
and enhanced customer support helpdesks. Deep models have been proposed for
representing and classifying paraphrases. These models, however, require large
quantities of human-labeled data, which is expensive to obtain. In this work,
we present a data augmentation strategy and a multi-cascaded model for improved
paraphrase detection in short texts. Our data augmentation strategy considers
the notions of paraphrases and non-paraphrases as binary relations over the set
of texts. Subsequently, it uses graph theoretic concepts to efficiently
generate additional paraphrase and non-paraphrase pairs in a sound manner. Our
multi-cascaded model employs three supervised feature learners (cascades) based
on CNN and LSTM networks with and without soft-attention. The learned features,
together with hand-crafted linguistic features, are then forwarded to a
discriminator network for final classification. Our model is both wide and deep
and provides greater robustness across clean and noisy short texts. We evaluate
our approach on three benchmark datasets and show that it produces a comparable
or state-of-the-art performance on all three.","In recent years, short text in the form of posts on microblogs, question answer forums, news headlines, and tweets is being generated in abundance [1]. Performing NLP tasks is relatively easier in longer documents (e.g. news articles) than in short texts (e.g. headlines) because, in longer documents, greater context is available for semantic understanding [2]. Moreover, in many cases, short texts (e.g. tweets) tend to use informal language (spelling variations, improper grammar, slang) compared to longer documents (e.g. blogs). Thus, the techniques tailored for formal and clean text do not perform well on informal one [3], which call for a need to develop an approach that can work in both settings (i.e., clean and noisy informal text) [4]. A paraphrase of a document is another document that can be dierent in syntax, but that expresses the same meaning in the same language. Automatically detecting paraphrases among a set of documents has many signicant applications in natural language processing (NLP) and information retrieval (IR) such as plagiarism detection [5], query ranking [6], duplicate question detection [7, 8], web searching [9], and automatic question answering [10]. Paraphrase detection is a binary classication problem in which pairs of texts are labeled as either positive (paraphrase) or negative (nonparaphrase). In this setting, pairs of texts are mapped into a xed dimensional featurespace, where a standard classier is learned. Feature maps based on lexical, syntactic Muhammad Haroon Shakeel Email addresses: 15030040@lums.edu.pk (Muhammad Haroon Shakeel), akarim@lums.edu.pk (Asim Karim), imdad.khan@lums.edu.pk (Imdadullah Khan) Preprint submitted to Journal December 30, 2019arXiv:1912.12068v1  [cs.CL]  27 Dec 2019and semantic similarities in conjunction with SVM are proposed in [3, 11]. More recently, it has been demonstrated that for short text, deep learningbased pairs representations and classication yield better accuracy [4]. Many deep learningbased schemes employ one or two Convolutional Neural Network (CNN) or Long Short Term Memory (LSTM) based models to learn features and make predictions on clean texts [7, 12, 13], while a recent model also incorporates linguistic features to detect paraphrases in both clean and noisy short texts [4]. For many NLP tasks involving short texts, it has been shown that developing wider models can yield signicant gains [14]. While deep models produce richer representations, they require large amounts of training data for a robust paraphrase detection system [3]. Thus, for small datasets, such as Microsoft Research Paraphrase (MSRP) corpus and SemEval2015 Twitter paraphrase dataset (SemEval), handcrafted features and SVM classier have been widely used [3, 15]. Labeling pairs of documents in a humanbased computation setting (e.g. crowdsourcing) is costly [16]. Therefore, [4] and [13] add to the training set each labeled pair also in the reversed order. However, this simple data augmentation strategy can be extended in a systematic manner by relying upon set and graph theory. For instance, consider four documents: ( a)How can I lose weight quickly? (b)How can I lose weight fast? (c)What are the ways to lose weight as soon as possible? (d)Will Trump win US elections? . If in the annotated corpus, documents aandband documents band care marked as paraphrases, then by transitive extension, documents aandccan also be considered as paraphrases. Similarly, if documents aandbare labeled as paraphrase, while documents banddare labeled as nonparaphrase, then a new nonparaphrase pair based on document aanddcan be inferred reliably. Such a strategy can be used to generate additional annotations in a sound and costeective manner, and potentially enhance the performance of deep learning models for paraphrase detection. In this paper, we propose a data augmentation strategy for generating additional paraphrase and non paraphrase annotations reliably from existing annotations. We consider notions of paraphrases and non paraphrases as binary relations over the set of documents. Representing the binary relation induced by the paraphrase labels as an undirected graph and performing transitive closure on this graph, we include addi tional paraphrase annotation in the training set. Similarly, by comparing paraphrase and nonparaphrase annotations we infer additional nonparaphrase annotations for inclusion in the training corpus. Our strat egy involves several steps and a parameter through which the data augmentation can be tuned for enhanced paraphrase detection. We also present a robust multicascaded deep learning model for paraphrase detection in short texts. Our model utilizes three independent CNN and LSTM (with and without soft attention) cascades for feature learning in a supervised manner. We also employ a number of additional linguistic features after corpus specic text preprocessing. All these features are fed into a discriminator network for nal classication. To show eectiveness of our approach we evaluate the data augmentation and deep model on three benchmark short text datasets (MSRP and Quora (clean), and SemEval (noisy)). We also perform extensive comparisons with the stateoftheart methods. We make the following key contributions in this work: We present an ecient strategy for augmenting existing paraphrase and nonparaphrase annotations in a consistent manner. This strategy generates additional annotations and enhances the performance of the datahungry deep learning models. We develop a multicascaded learning model for robust paraphrase detection in both clean and noisy texts. This model incorporates multiple learned and linguistic features in a wide and deep discriminator network for paraphrase detection. We address both clean and noisy texts in our presentation and show that the proposed model matches current best performances on benchmark datasets of both types. We analyze the impact of various data augmentation steps and dierent components of the multi cascaded model on paraphrase detection performance. The rest of the paper is organized as follows: We discuss the related work in paraphrase detection and data augmentation in Section 2. We present our data augmentation strategy in Section 3. Our multi 2cascaded model for paraphrase detection is presented in Section 4. Section 5 outlines our experimental evaluation setup including discussion of data augmentation. We present and discuss the results of our approach in Section 6. Finally, we present our concluding remarks in Section 7. 2. Related Work "
547,FabricNet: A Fiber Recognition Architecture Using Ensemble ConvNets.txt,"Fabric is a planar material composed of textile fibers. Textile fibers are
generated from many natural sources; including plants, animals, minerals, and
even, it can be synthetic. A particular fabric may contain different types of
fibers that pass through a complex production process. Fiber identification is
usually carried out through chemical tests and microscopic tests. However,
these testing processes are complicated as well as time-consuming. We propose
FabricNet, a pioneering approach for the image-based textile fiber recognition
system, which may have a revolutionary impact from individual to the industrial
fiber recognition process. The FabricNet can recognize a large scale of fibers
by only utilizing a surface image of fabric. The recognition system is
constructed using a distinct category of class-based ensemble convolutional
neural network (CNN) architecture. The experiment is conducted on recognizing
50 different types of textile fibers. This experiment includes a significantly
large number of unique textile fibers than previous research endeavors to the
best of our knowledge. We experiment with popular CNN architectures that
include Inception, ResNet, VGG, MobileNet, DenseNet, and Xception. Finally, the
experimental results demonstrate that FabricNet outperforms the
state-of-the-art popular CNN architectures by reaching an accuracy of 84% and
F1-score of 90%.","Textile Ô¨Åbers are the components that are used to construct fabrics. Commonly, the types of Ô¨Åbers are split into two categories: natural Ô¨Åbers and synthetic Ô¨Åbers. Natural Ô¨Åbers are extracted from environmental sources, whereas synthetic Ô¨Åbers are manufactured through machinery and chemical compounds. Such instances of natural Ô¨Åbers are silk, wool, cotton, etc. whereas, nylon, polyester, rayons, etc. are the example of synthetic Ô¨Åbers. Raw Ô¨Åbers are used toarXiv:2101.05564v1  [cs.CV]  14 Jan 2021FabricNet: A Fiber Recognition Architecture Using Ensemble ConvNets A.Q. O HI ET AL . Figure 1: Different yarn cross sectional shapes of particular Ô¨Åbers: Tenasco (a,b,i), Nylon (e,g,j,l), Viscose (f,k) and Terylene. The image is adopted from the work of Hearle et al. [4]. assemble yarns. A single yarn is assembled using one or more types of raw Ô¨Åbers. The yarns are further utilized to construct fabrics and particular garments. Fiber recognition is the process of identifying raw Ô¨Åbers from fabrics, and it is widely used in different industrial applications. It is a wellapplied method in fabric reverse engineering [ 1]. Garment identiÔ¨Åcation is also possible using Ô¨Åber recognition systems since each garment type mostly requires Ô¨Åxed sets of Ô¨Åber elements [ 2]. The Ô¨Åber recognition system can also be implemented as fabric fault detection and garment inquiry systems [3]. Identifying raw textile Ô¨Åbers is considered difÔ¨Åcult due to the complicated weave structure and aging of Ô¨Åbers [5]. Moreover, present fabrics pass through complex printing procedures that may alter the yarn structure. Classical biological methods, such as soaking, cleaning, heating, etc. are considered less effective in raw textile Ô¨Åber identiÔ¨Åcation. However, microscopic observations are proven to be more accurate in identifying raw textile Ô¨Åbers. The textile Ô¨Åber recognition from manufactured fabrics is complicated since a single yarn (used to construct the fabric) can contain multiple textile Ô¨Åbers. In some cases, fabrics are mostly preprocessed. Moreover, microscopic observations may lead to false recognition, as numerous Ô¨Åbers can be used to generate a single textile yarn. Generally, most systems identify textile Ô¨Åbers through microscopic crosssection images [ 6] and spectroscopic features [7,8]. Fibers can be distinguished by microscopic crosssection images due to their unique geometrical properties [9]. Figure 1 illustrates different crosssection shapes of different types of yarns. Extricating crosssectional shots is nearly impossible for industrial usage of textile Ô¨Åber identiÔ¨Åcation systems, as it requires a careful preprocessing and microscopes. Using crosssectional images for recognizing textile Ô¨Åbers from fabrics is a critical approach in realtime industrial aspects. Hence, the crosssectional investigation requires laboratories and is timeconsuming. On the contrary, spectroscopybased methods can be used for industrial purposes, but it is limited to recognizing only a single textile Ô¨Åber from a fabric. Further, the method is not suitable for individual usage. 2FabricNet: A Fiber Recognition Architecture Using Ensemble ConvNets A.Q. O HI ET AL . Figure 2: The dataset contains fabric images in different light and orientations [ 2]. The Ô¨Årst row illustrates fabric made of artiÔ¨Åcial leather. The second row illustrates the fabric made of silk. The third row contains images of fabric which is made of polyester and viscose (rayon). The research endeavor‚Äôs sole purpose is to overcome the information gathering complexities of the Ô¨Åber recognition procedure. Smartphones and high deÔ¨Ånition cameras have made image capturing one of the most superÔ¨Åcial attempts for information gathering procedures. Therefore, we introduce a novel architecture that can recognize the textile Ô¨Åbers by a fabric surface image. Because of the availability of cameras, our proposed textile Ô¨Åber recognition process can be performed by individuals, and even by automated machines in a much more convenient and Ô¨Çexible way. Figure 2 represents some image samples that are used to conduct the training of our FabricNet architecture. The overall architecture performs CNN based image processing using an ensemble architecture. Since our proposed model recognizes textile Ô¨Åbers through fabric surface image, it can be widely applied for diverse industrial and individual applications for fault checking and authentication. Our process can further be used for textile fraud prevention and fault detection. The overall contribution of the research endeavor includes the following: ‚Ä¢We exploit the surface image of fabric in recognizing textile Ô¨Åbers, as it is one of the easiest ways for image collection. ‚Ä¢We outline different categories of ensemble methods, and introduce a classbased ensemble architecture that receives downsampled image data through a head CNN architecture. In classbased ensemble architecture, every single ensemble memorizes only one class. Therefore, the accuracy of FabricNet architecture increases. ‚Ä¢We experiment with seventeen different implementations of famous image classiÔ¨Åcation architectures, includ ing Inception, ResNet, VGG, MobileNet, DenseNet, Xception, and CUNet. Through the result analysis, we afÔ¨Årm that FabricNet architecture provides better accuracy. The remainder of this paper is outlined as follows. Section 2 demonstrates the procedures that are modeled to identify textile Ô¨Åbers. Section 3 presents the motivation and the architectural fundamentals of the FabricNet. Section 4 contains the experimental results that are performed to evaluate FabricNet architecture. Finally, Section 5 concludes the paper. 2 Related Works "
29,Model-agnostic Approaches to Handling Noisy Labels When Training Sound Event Classifiers.txt,"Label noise is emerging as a pressing issue in sound event classification.
This arises as we move towards larger datasets that are difficult to annotate
manually, but it is even more severe if datasets are collected automatically
from online repositories, where labels are inferred through automated
heuristics applied to the audio content or metadata. While learning from noisy
labels has been an active area of research in computer vision, it has received
little attention in sound event classification. Most recent computer vision
approaches against label noise are relatively complex, requiring complex
networks or extra data resources. In this work, we evaluate simple and
efficient model-agnostic approaches to handling noisy labels when training
sound event classifiers, namely label smoothing regularization, mixup and
noise-robust loss functions. The main advantage of these methods is that they
can be easily incorporated to existing deep learning pipelines without need for
network modifications or extra resources. We report results from experiments
conducted with the FSDnoisy18k dataset. We show that these simple methods can
be effective in mitigating the effect of label noise, providing up to 2.5\% of
accuracy boost when incorporated to two different CNNs, while requiring minimal
intervention and computational overhead.","Deep neural networks require large and varied data resources in or der to show their superior performance with respect to traditional machine learning methods, a fact that has become evident in Ô¨Åelds like computer vision. In the less explored Ô¨Åeld of sound event recog nition, we are currently moving from small and exhaustively labeled datasets of few hours of duration [1, 2, 3], towards larger datasets in the range of tens (e.g., FSDKaggle2018 [4] or FSDnoisy18k [5]) to thousands (e.g., AudioSet [6]) of hours of audio. The increasing size of datasets makes it hard to manually label the audio content reliably as it turns out to be difÔ¨Åcult and costly. This inevitably in curs in a certain amount of label noise either due to incomplete or incorrect annotations, even if produced by trained humans. Online repositories such as Freesound or Youtube host signiÔ¨Å cant volumes of audio content with associated metadata that can be used to create audio datasets. Labels can be inferred automatically by applying automated heuristics to the metadata, or pretrained classiÔ¨Åers on the audio content. While this way of collecting la beled data is much faster than the conventional dataset creation, the level of label noise generated can be much more severe. Hence, la bel noise is a problem in largescale sound event classiÔ¨Åcation that This work is partially supported by the European Union‚Äôs Horizon 2020 research and innovation programme under grant agreement No 688382 Au dioCommons and a Google Faculty Research Award 2018.can hinder the proper learning of classiÔ¨Åers, especially if they are based on deep neural networks [7, 8]. The topic of learning with noisy labels is an active area of re search in computer vision. The stateoftheart is based on selecting the clean data instances in the train set in order to train the network satisfactorily [9, 10, 11]. However, those methods can turn rela tively complex as they leverage two networks (sometimes trained simultaneously). Other methods rely on estimating the noise tran sition matrix, i.e., the probability of each true label being Ô¨Çipped into another [12, 13]. However, such estimation is not trivial, and it assumes that the only possible type of noise is Ô¨Çipping labels. Other approaches use noiserobust loss functions to mitigate the ef fect of label noise [14], or leverage an additional set of curated data, for example to train a label cleaning network in order to reduce the noise of a dataset [15]. Conversely, learning with noisy labels has received little attention in sound recognition, probably given the traditional paradigm of learning from relatively small and ex haustively labeled (hence clean ) datasets. In our previous work, the FSDnoisy18k dataset is presented along with an evaluation of noise robust loss functions [5]. In [16], two networks operating on differ ent views of the data coteach each other to learn from noisy labels. Recently, the topic of label noise was included for the Ô¨Årst time as one of the research problems in the DCASE2019 Challenge [17]. Most of the aforementioned approaches against label noise re quire complex networks (often more than one) or extra data re sources. Given the early stage of this Ô¨Åeld in sound event classiÔ¨Åca tion, we are interested in exploring simple and efÔ¨Åcient approaches, agnostic to network architecture, that can mitigate the effect of la bel noise. SpeciÔ¨Åcally, we seek approaches that can be plugged into existing learning settings composed by a noisy dataset and a deep network, without need for network modiÔ¨Åcations or extra resources. Our contribution is to provide insight on the modelagnostic ap proaches that can be incorporated to deep learning pipelines for sound event classiÔ¨Åcation in presence of noisy labels, as well as the performance boost that can be expected. In particular, we con sider regularization techniques external to the model, as well as noiserobust loss functions (Fig. 1). Regularization aims to prevent overÔ¨Åtting and improve generalization, which can also be beneÔ¨Å cial against label noise. Common regularization strategies include weight decay and dropout [18], which act on the weights or hid den units of the network; dropout has been shown useful in reduc ing label noise memorization [7]. In our attempt to regularize the model from the outside, we consider label smoothing regularization (LSR) and mixup . The former operates on the ground truth labels, while the latter operates on both ground truth labels and input data (Fig. 1). In addition, we explore two strategies to ignore poten tially noisy instances in the learning process through noiserobust loss functions. Section 2 describes the methods considered. Section 3 introduces the experimental setup. Section 4 describes the exper iments carried out and the results. Section 5 provides Ô¨Ånal remarks.arXiv:1910.12004v1  [cs.SD]  26 Oct 20192019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 2023, 2019, New Paltz, NY loss f(x) TF mixup deep network  predictions labels  LSRnoiserobust  loss f(x)  Figure 1: Sketch of the modelagnostic approaches against label noise considered (in red), indicating the component(s) of the learn ing pipeline where they operate. 2. METHODS "
334,Enabling Smart Data: Noise filtering in Big Data classification.txt,"In any knowledge discovery process the value of extracted knowledge is
directly related to the quality of the data used. Big Data problems, generated
by massive growth in the scale of data observed in recent years, also follow
the same dictate. A common problem affecting data quality is the presence of
noise, particularly in classification problems, where label noise refers to the
incorrect labeling of training instances, and is known to be a very disruptive
feature of data. However, in this Big Data era, the massive growth in the scale
of the data poses a challenge to traditional proposals created to tackle noise,
as they have difficulties coping with such a large amount of data. New
algorithms need to be proposed to treat the noise in Big Data problems,
providing high quality and clean data, also known as Smart Data. In this paper,
two Big Data preprocessing approaches to remove noisy examples are proposed: an
homogeneous ensemble and an heterogeneous ensemble filter, with special
emphasis in their scalability and performance traits. The obtained results show
that these proposals enable the practitioner to efficiently obtain a Smart
Dataset from any Big Data classification problem.","Vast amounts of information surround us today. Technologies such as the Internet generate data at an exponential rate thanks to the aordability and great development of storage and network resources. It is predicted that by 2020, the digital universe will be 10 times as big as it was in 2013, totaling an astonishing 44 zettabytes [22]. The current volume of data has exceeded the processing capabilities of classical data mining systems [50] and have created a need for new frameworks for storing and processing this data. It is widely Corresponding author Email addresses: djgarcia@decsai.ugr.es (Diego Garc aGil), julianlm@decsai.ugr.es (Juli an Luengo), salvagl@decsai.ugr.es (Salvador Garc a), herrera@decsai.ugr.es (Francisco Herrera) Preprint submitted to Journal of L ATEX Templates July 31, 2017arXiv:1704.01770v2  [cs.DB]  28 Jul 2017accepted that we have entered the Big Data era [31]. Big Data is the set of technologies that make processing such large amounts of data possible [7], while most of the classic knowledge extraction methods cannot work in a Big Data environment because they were not conceived for it. Big Data as concept is dened around ve aspects: data volume, data veloc ity, data variety, data veracity and data value [24]. While the volume, variety and velocity aspects refer to the data generation process and how to capture and store the data, veracity and value aspects deal with the quality and the useful ness of the data. These two last aspects become crucial in any Big Data process, where the extraction of useful and valuable knowledge is strongly in uenced by the quality of the used data. In Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases. The lack of ecient and aordable preprocessing tech niques implies that the problems in the data will aect the models extracted. Among all the problems that may appear in the data, the presence of noise in the dataset is one of the most frequent. Noise can be dened as the partial or complete alteration of the information gathered for a data item, caused by an exogenous factor not related to the distribution that generates the data. Learn ing from noisy data is an important topic in machine learning, data mining and pattern recognition, as real world data sets may suer from imperfections in data acquisition, transmission, storage, integration and categorization. Noise will lead to excessively complex models with deteriorated performance [49], re sulting in even larger computing times for less value. The impact of noise in Big Data, among other pernicious traits, has not been disregarded. Recently, Smart Data (focusing on veracity and value) has been in troduced, aiming to lter out the noise and to highlight the valuable data, which can be eectively used by companies and governments for planning, operation, monitoring, control, and intelligent decision making. Three key attributes are needed for data to be smart, it must be accurate, actionable and agile: Accurate: data must be what it says it is with enough precision to drive value. Data quality matters. Actionable: data must drive an immediate scalable action in a way that maximizes a business objective like media reach across platforms. Scalable action matters. Agile: data must be available in realtime and ready to adapt to the changing business environment. Flexibility matters. Advanced Big Data modeling and analytics are indispensable for discov ering the underlying structure from retrieved data in order to acquire Smart Data. In this paper we provide several preprocessing techniques for Big Data, transforming raw, corrupted datasets into Smart Data. We focus our interest on classication tasks, where two types of noise are distinguished: class noise , when it aects the class label of the instances, and attribute noise , when it aects 2the rest of attributes. The former is known to be the most disruptive [39, 54]. Consequently, many recent works, including this contribution, have been de voted to resolving this problem or at least to minimize its eects (see [15] for a comprehensive and updated survey). While some architectural designs are already proposed in the literature[52], there is no particular algorithm which deals with noise in Big Data classication, nor a comparison of its eect on model generalization abilities or computing times. Thereby we propose a framework for Big Data under Apache Spark for removing noisy examples composed of two algorithms based on ensembles of classiers. The rst one is an homogeneous ensemble, named Homogeneous Ensembe for Big Data (HMEBD), which uses a single base classier (Random Forest [4]) over a partitioning of the training set. The second ensemble is an heterogeneous ensemble, namely Heterogeneous Ensembe for BigData (HTE BD), that uses dierent classiers to identify noisy instances: Random Forest, Logistic Regression and KNearest Neighbors (KNN) as base classiers. For the sake of a more complete comparison, we have also considered a simple ltering approach based on similarities between instances, named Edited Nearest Neigh bor for Big Data (ENNBD). ENNBD examines the nearest neighbors of every example in the training set and eliminates those whose majority of neighbors belong to a dierent class. All these techniques have been implemented under the Apache Spark framework [20, 40] and can be downloaded from the Spark's community repository1. To show the performance of the three proposed algorithms, we have carried out an experimental evaluation with four large datasets, namely SUSY ,HIGGS , Epsilon and ECBDL14 . We have induced several levels of class noise to eval uate the eects of applying such framework and the improvements obtained in terms of classication accuracy for two classiers: a decision tree and the KNN technique. Decision trees with pruning are known to be tolerant to noise, while KNN is a noise sensitive algorithm when the number of selected neighbors is low. These dierences allow us to better compare the eect of the framework in classiers which behave dierently towards noise. We also show that, for the Big Data problems considered, the classiers also benet from applying the noise treatment even when no additional noise is induced, since Big Data problems contain implicit noise due to incidental homogeneity, spurious correlations and the accumulation of noisy examples [12]. The results obtained indicate that the framework proposed can successfully deal with noise. In particular, the ho mogeneous ensemble is a suitable technique for dealing with noise in Big Data problems, with low computing times and enabling the classier to achieve better accuracy. The remainder of this paper is organized as follows: Section 2 presents the concepts of noise, MapReduce and Smart Data. Section 3 explains the pro posed framework. Section 4 describes the experiments carried out to check the performance of the framework. Finally, Section 5 concludes the paper. 1https://sparkpackages.org/package/djgarcia/NoiseFramework 32. Related work "
387,CAGNN: Cluster-Aware Graph Neural Networks for Unsupervised Graph Representation Learning.txt,"Unsupervised graph representation learning aims to learn low-dimensional node
embeddings without supervision while preserving graph topological structures
and node attributive features. Previous graph neural networks (GNN) require a
large number of labeled nodes, which may not be accessible in real-world graph
data. In this paper, we present a novel cluster-aware graph neural network
(CAGNN) model for unsupervised graph representation learning using
self-supervised techniques. In CAGNN, we perform clustering on the node
embeddings and update the model parameters by predicting the cluster
assignments. Moreover, we observe that graphs often contain inter-class edges,
which mislead the GNN model to aggregate noisy information from neighborhood
nodes. We further refine the graph topology by strengthening intra-class edges
and reducing node connections between different classes based on cluster
labels, which better preserves cluster structures in the embedding space. We
conduct comprehensive experiments on two benchmark tasks using real-world
datasets. The results demonstrate the superior performance of the proposed
model over existing baseline methods. Notably, our model gains over 7%
improvements in terms of accuracy on node clustering over state-of-the-arts.","Unsupervised graph representation learning aims to learn lowdimensional node embeddings without supervision. The learned node embeddings preserve useful topological structures and node attributive features extracted from graphs. Traditional graph representation learning algorithms originate in the skipgram model for distributed language representation [ 17]. The pioneering work DeepWalk [ 21] constructs node sequences by performing random walks over the graph. Then, on top of these sequences, the node embeddings can be learned using the skipgram model. Following this line of development, various network embedding methods have been proposed, such as node2vec [9] and LINE [26]. Recently, the graph neural network (GNN), a generalized form of convolutional networks in the graph domain, has attracted a lot of attention. Compared with conventional graph embedding methods, GNN shows superior expressive power and has achieved promising performance in many tasks [ 15,29,35,37]. However, most existing GNN models are established on a semisupervised setting [ 15,28]. Training an accurate GNN model requires a number of highquality node labels, which might not be accessible. Then, a natural question is that can we leverage the expressive power of GNN models and produce node embeddings in an unsupervised manner ? In the real world, graphs can be derived from business data in quantity, which can help facilitate analytical tasks and provide valuable insights for business. Take an ecommerce website for example, structured data derived from everyday user purchases along with their relationship with items is produced in a million scale. Given the purchase data, we can better classify users and items if we further leverage the derived interaction graphs. Since obtaining labels is a laborintensive and timeconsuming process, if we can efficiently train a GNN model in an unsupervised manner, it would be greatly beneficial to facilitate downstream analytical tasks. There has been a surge of research interest in unsupervised visual representation learning to avoid expensive data annotations. As a subfield of unsupervised learning, selfsupervised methods have achieved great success [ 18‚Äì20]. Among them, a series of work performs clustering on embeddings and regards the clustering assignments as the pseudolabels to replace human annotations. Then, the classification objective can be used to train the model. The work DeepCluster [ 5] uses kMeans to compute pseudolabels from raw image data, enabling largescale visual representation learning in an unsupervised manner. Following this work, Asano et al . [2]further propose a selflabeling scheme to regularize the cluster size and avoid degenerate solutions, which has become the stateoftheart method on computer vision benchmarks. Although there is a proliferation of studies in selfsupervised visual representation learning, little attention is paid to graph representation learning using a selfsupervised manner. Hu et al . [12] propose a pretraining GNN model that is trained using several network measures such as betweenness and closeness. However, these statistical measures require domain knowledge and are sensitive to noise in graphs. On the contrary, as a natural characteristic of graph data, clusters group vertices that share similar functionalities in a graph and thus can be used as a good supervisory signal for training the GNN model. Moreover, we observe that graphs often contain noisy edges, which connect nodes belonging to different classes. Such edges may mislead GNN training and further confine the model from learning useful class information. In a graph with many interclass edges, when performing graph convolution through neighborhood aggregation, i.e. taking the average over neighbor nodes, the resulting node embeddings tend to be indistinguishable from different classes. Thus, we argue that a key to improving the quality of embeddings is to alleviate the impact of potentially noisy edges and strengthen edges between nodes of the same class, which will help preserve the cluster structures and obtain betterseparated node embeddings. In summary, considering the existence of noisy interclass edges, it is crucial to mitigate the impact of these ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: August 2019.CAGNN: ClusterAware Graph Neural Networks for Unsupervised Graph Representation Learning 111:3 Graph Neural Networks ClusterlabelsSelfsupervisorysignalsClusteringTopologyrefiningRefined adjacency matrix Fig. 1. The pipeline of the proposed CAGNN model. The CAGNN model alternates between node representa tion learning and clustering. We first obtain node embeddings using graph neural networks (GNN). Then, we perform clustering and use the cluster labels as the selfsupervisory signals. Following that, we use a novel clusteraware topology refining mechanism which reduces intercluster edges and strengthens intraclass connections to mitigate the impact of noisy edges. edges during training, which is usually neglected by previous work that merely leverages network measures as selfsupervision. Motivated by the aforementioned observations, we propose a novel clusteraware graph neu ral network model for selfsupervised graph representation learning in this paper. We term the model CAGNN for brevity. As illustrated in Fig. 1, our CAGNN model consists of three stages. At the first stage, we perform graph convolutions to obtain node embeddings. Then, the model conducts clustering on the node embeddings and updates the model parameters by predicting the corresponding cluster assignments. To avoid degenerate solutions, we use a balanced cluster strategy, which formulates the crossentropy minimization as an optimal transport problem. Finally, to alleviate the impact of noisy edges and better preserve cluster structures in the embedding space, we propose a novel graph topology refining scheme based on cluster assignments. The proposed refining process strengthens intraclass edges and weakens potentially noisy edges by isolating neighborhood nodes of different clusters. The core contribution of this paper is threefold. Firstly, we propose a novel selfsupervised graph neural network for unsupervised graph representation learning, which needs no supervision from labels. Secondly, unlike other GNN models, CAGNN further proposes a topology refining scheme which reduces intercluster connections of neighbor nodes to alleviate the impact of noisy edges. Thirdly, extensive experiments conducted on benchmark datasets demonstrate the superiority over existing baseline methods. It is worth mentioning that the proposed method gains over 7% performance improvement in terms of accuracy on node clustering over stateofthearts. The organization of the remaining of the paper is summarized below. We first review prior arts in relevant domains in Section 2. Then, in Section 3, we introduce our proposed clusteraware graph neural networks in detail. After that, we present empirical studies in Section 4. Finally, we conclude the paper and point out future research directions in Section 5. ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: August 2019.111:4 Zhu et al. 2 RELATED WORK "
100,AutoEnsemble: Automated Ensemble Search Framework for Semantic Segmentation Using Image Labels.txt,"A key bottleneck of employing state-of-the-art semantic segmentation networks
in the real world is the availability of training labels. Standard semantic
segmentation networks require massive pixel-wise annotated labels to reach
state-of-the-art prediction quality. Hence, several works focus on semantic
segmentation networks trained with only image-level annotations. However, when
scrutinizing the state-of-the-art results in more detail, we notice that
although they are very close to each other on average prediction quality,
different approaches perform better in different classes while providing low
quality in others. To address this problem, we propose a novel framework,
AutoEnsemble, which employs an ensemble of the ""pseudo-labels"" for a given set
of different segmentation techniques on a class-wise level. Pseudo-labels are
the pixel-wise predictions of the image-level semantic segmentation frameworks
used to train the final segmentation model. Our pseudo-labels seamlessly
combine the strong points of multiple segmentation techniques approaches to
reach superior prediction quality. We reach up to 2.4% improvement over
AutoEnsemble's components. An exhaustive analysis was performed to demonstrate
AutoEnsemble's effectiveness over state-of-the-art frameworks for image-level
semantic segmentation.","Generating highquality semantic segmentation predictions using only models trained on imagelevel annotated datasets would enable a new level of applicability. The progress of fully supervised semantic segmentation networks has already helped provide many useful tools and applications. For ex ample, in autonomous and selfdriving vehicles [1, 2], remote sensing [3, 4], facial recognition [5], agriculture [6, 7], and in the medical Ô¨Åeld [8, 9], etc. The downside of those fully supervised semantic segmentation networks (FSSS) is that (b) (c) (a) (e) (f) (d)Fig. 1 . Pseudo labels from (a) DRS, (b) PMM, (c) Puzzle CAM, (d) CLIMS, (e) AutoEnsemble (Ours), (f) Ground truth they require large amounts of pixelwise annotated images. Generating such a training set is very timeconsuming and tedious work. For instance, one frame of the Cityscapes dataset, which contains thousands of pixelwise frame anno tations of street scenes from cities, requires more than an hour of manual userdriven annotation [10]. Furthermore, medical imaging and molecular biology Ô¨Åelds require the knowledge of highly qualiÔ¨Åed and experienced individuals capable of interpreting and annotating the images. Therefore, to reduce the time and resources required for generating pixelwise masks, a wide range of research works focus on developing approaches that focus on weaker kinds of supervision. In this work, we will focus on weak supervision in the form of imagelevel labels. Imagelevel labels give the least amount of supervision for semantic segmentation but are the easiest to acquire. Several works already focus on imagelevel semantic seg mentation techniques, and they consistently reach better and better high scores. Most works are based on Class Activa tion Maps (CAMs) [11]. CAMs localize the object by trainarXiv:2303.07898v2  [cs.CV]  15 Mar 2023ing a DNN model with classiÔ¨Åcation loss and then reusing the learned weights to highlight the image areas responsible for its classiÔ¨Åcation decision. Most imagelevel segmentation approaches aim to improve the CAM baseline by adding addi tional regularizations to the classiÔ¨Åcation loss or reÔ¨Åning the CAM mask afterward. As more and more methods emerge for improving CAM quality, stateoftheart is usually compiled of combinations of regularizations and afterthefact reÔ¨Åne ment. However, when analyzing different imagelevel seg mentation tehniques on a classbyclass basis, we observed that the differences between those approaches vary signiÔ¨Å cantly on speciÔ¨Åc classes, although those methods generate predictions that reach comparable scores on average. Therefore, we are proposing our AutoEnsemble frame work. In our framework, we combine the pseudolabels of multiple imagelevel segmentation tehniques based on the re spective class scores to generate a superset of pseudolabels, combining the upsides of multiple different approaches. Fig. 1 visualizes the gains possible of AutoEnsemble com pared to its best component. We perform extensive experi ments on the PASCAL VOC2012 dataset [12] to prove the effectiveness of the proposed framework in various exper imental settings and compare them with a wide range of stateoftheart techniques to illustrate the beneÔ¨Åts of our approach. The key contribution of this work are: 1. Our novel AutoEnsemble framework improves the pre diction quality of the segmentation mask by combining stateoftheart pseudolabels and classbyclass base. 2. Our AutoEnsemble is not limited by the number or approach of any conventional imagelevel segmenta tion framework to combine their pseudolabels. Since the AutoEnsemble is only used for generating pseudo labels, it will not add more computations for inference predictions. 3. We have presented detailed ablation studies and anal ysis of the results comparing AutoEnsemble to state oftheart methods on the VOC2012 dataset to evaluate our method‚Äôs efÔ¨Åcacy and the improvements achieved using our framework. 2. RELATED WORK "
586,Selective-Supervised Contrastive Learning with Noisy Labels.txt,"Deep networks have strong capacities of embedding data into latent
representations and finishing following tasks. However, the capacities largely
come from high-quality annotated labels, which are expensive to collect. Noisy
labels are more affordable, but result in corrupted representations, leading to
poor generalization performance. To learn robust representations and handle
noisy labels, we propose selective-supervised contrastive learning (Sel-CL) in
this paper. Specifically, Sel-CL extend supervised contrastive learning
(Sup-CL), which is powerful in representation learning, but is degraded when
there are noisy labels. Sel-CL tackles the direct cause of the problem of
Sup-CL. That is, as Sup-CL works in a \textit{pair-wise} manner, noisy pairs
built by noisy labels mislead representation learning. To alleviate the issue,
we select confident pairs out of noisy ones for Sup-CL without knowing noise
rates. In the selection process, by measuring the agreement between learned
representations and given labels, we first identify confident examples that are
exploited to build confident pairs. Then, the representation similarity
distribution in the built confident pairs is exploited to identify more
confident pairs out of noisy pairs. All obtained confident pairs are finally
used for Sup-CL to enhance representations. Experiments on multiple noisy
datasets demonstrate the robustness of the learned representations by our
method, following the state-of-the-art performance. Source codes are available
at https://github.com/ShikunLi/Sel-CL","Deep networks are powerful in various tasks, e.g., im age recognition [19, 62], object detection [58], visual track ing [11] and text matching [3]. The power is largely at tributed to the collection of largescale datasets with high quality annotated labels. In supervised learning, with the *Shiming Ge is the corresponding author. Figure 1. Left: learning a classiÔ¨Åer with ideal representations in duced by clean labels; Right : learning a classiÔ¨Åer with corrupted representations caused by noisy labels. Circles represent the repre sentations of positive examples while triangles represent the rep resentations of negative examples. When the representations are corrupted by noisy labels, the decision boundary of the classiÔ¨Åer will be largely changed. Therefore, the learned classiÔ¨Åer in this case cannot generalize well on test examples. data ( i.e., the instance and label pairs) in such datasets, deep networks Ô¨Årst learn ideal latent representations of the in stances and then complete following tasks with the repre sentations [14, 57]. However, it is extremely expensive to obtain largescale highquality annotated labels. Alterna tively, we can collect labels based on web search and user tags [36, 55]. These labels are cheap but inevitably noisy. Noisy labels impair the generalization performance of deep networks [15, 60]. It is because, supervised by the datasets with noisy labels, the mislabeled data provide in correct signals when inducing latent representations for the instances. The corrupted representations then cause inac curate decisions for following tasks and hurt generaliza tion [29, 52]. For example, as shown in Fig. 1, the cor rupted representations result in an inprecise classiÔ¨Åcation boundary. Therefore, it is crucial to induce robust latent representations of instances for learning with noisy labels, which is also our focus in this paper. Recent works [7, 8, 13, 18, 65] show that, working in a pairwise manner, contrastive learning (CL) methods can bring good latent representations to help following tasks. Based on whether supervised information is provided, the 1arXiv:2203.04181v1  [cs.CV]  8 Mar 2022CL methods can be grouped into supervised contrastive learning (SupCL) [27] and unsupervised contrastive learn ing (UnsCL) [7, 8, 18]. It has been shown that SupCL can exploit the supervised information to learn better rep resentations than UnsCL, but relies on the quality of su pervised information [41]. If the supervised information is corrupted by noisy labels, built pairs by training exam ples are noisy, following corrupted representations learned by SupCL. Motivated by this phenomenon, prior meth ods use generalpurpose techniques in tackling noisy labels for robust representation learning with SupCL, e.g., intro ducing regularization [41] or generating pseudolabels [32]. Although these methods can work Ô¨Åne in some cases, the generalpurpose techniques fail to consider the remarkable pairwise characteristic of SupCL in strengthening repre sentation learning. The achieved performance by them is thus argued to be suboptimal . In this paper, we propose selectivesupervised con trastive learning (SelCL) to address the above issue. Sel CL can make use of the pairwise characteristic to learn ro bust latent representations. The core idea of SelCL is (1) select conÔ¨Ådent pairs out of noisy pairs; (2) employ the con Ô¨Ådent pairs to learn robust latent representations. Note that it is hard to identify conÔ¨Ådent pairs directly for representa tion learning. The main reason is that we always need to set a threshold with the noise rate for precise identiÔ¨Åcation, e.g., see [15, 16, 26]. Nevertheless, it is difÔ¨Åcult to estimate the noise rate of noisy pairs. To handle this problem, we propose to Ô¨Årst employ conÔ¨Ådent examples [16, 41], which is much easier to identiÔ¨Åed, to build a reliable set of conÔ¨Å dent pairs at each epoch. Then, based on the representation similarity distribution of conÔ¨Ådent pairs in this set, we set a dynamic threshold to selected more conÔ¨Ådent pairs out of all noisy pairs. By this pairwise selection, we can make better use of not only the pairs whose class labels are correct, but also the pairs whose class labels are incorrect, but the ex amples in them are misclassiÔ¨Åed to the same class. All se lected conÔ¨Ådent pairs are utilized to enhance representation learning with SupCL. As the selected conÔ¨Ådent pairs are less noisy, the learned representations with this selective supervised paradigm will be more robust, naturally follow ing promising generalization. The main contributions of this paper are summarized as three aspects: 1) We propose selectivesupervised con trastive learning with noisy labels, which can obtain robust pretrained representations by effectively selecting conÔ¨Å dent pairs for performing SupCL. 2) Without knowing the noise rate of pairs, our approach selects the pairs built by identiÔ¨Åed conÔ¨Ådent examples, and the pairs built by the ex amples with high representation similarities. It fulÔ¨Åls a pos itive cycle, where better conÔ¨Ådent pairs result in better rep resentations and better representations will identify better conÔ¨Ådent pairs. 3) We conduct experiments on syntheticand realworld noisy datasets, which clearly demonstrate our approach achieves better performance compared with the stateoftheart methods. Comprehensive ablation stud ies and discussions are also provided. 2. Related Works "
18,Temporal Localization of Fine-Grained Actions in Videos by Domain Transfer from Web Images.txt,"We address the problem of fine-grained action localization from temporally
untrimmed web videos. We assume that only weak video-level annotations are
available for training. The goal is to use these weak labels to identify
temporal segments corresponding to the actions, and learn models that
generalize to unconstrained web videos. We find that web images queried by
action names serve as well-localized highlights for many actions, but are
noisily labeled. To solve this problem, we propose a simple yet effective
method that takes weak video labels and noisy image labels as input, and
generates localized action frames as output. This is achieved by cross-domain
transfer between video frames and web images, using pre-trained deep
convolutional neural networks. We then use the localized action frames to train
action recognition models with long short-term memory networks. We collect a
fine-grained sports action data set FGA-240 of more than 130,000 YouTube
videos. It has 240 fine-grained actions under 85 sports activities. Convincing
results are shown on the FGA-240 data set, as well as the THUMOS 2014
localization data set with untrimmed training videos.","This paper addresses the problem of negrained action localization from unconstrained web videos. A negrained Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita tion on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a fee. Request permissions from Permissions@acm.org. MM‚Äô15, October 26‚Äì30, 2015, Brisbane, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 9781450334594/15/10 ...$15.00. DOI: http://dx.doi.org/10.1145/2733373.2806226 . Figure 1: Finegrained actions are usually present as a tiny fraction within videos (top). Our framework uses crossdomain transfer from possibly noisy im age search results (bottom) and identies the action related images for both domains (marked in green). action takes place in a higherlevel activity or event (e.g., jump shot andslam dunk inbasketball ,blow candle inbirth day party ). Its instances are usually temporally localized within the videos, and share similar context with other ne grained actions belonging to the same activity or event. Most existing work on action recognition focuses on action classication using presegmented short video clips [25, 14, 23], which assumes implicitly that the actions of interest are temporally segmented during both training and testing. The TRECVID Multimedia Event Recounting evaluation [17] as well as THUMOS 14 Challenge [10] both address action lo calization in untrimmed video, but the typical approach in volves training classiers on temporally segmented action clips and testing using sliding window on untrimmed video. This setting does not scale to large action vocabularies, when data is collected from consumer video websites. Videos here are unconstrained in length, format (home videos vs. pro fessional videos), and almost always only have video level annotations of actions. We assume that only videolevel annotations are available for the negrained action localization problem. The abil ity to localize ne grained actions in videos has important applications such as video highlighting, summarization, and automatic video transcription. It is also a challenging probarXiv:1504.00983v2  [cs.CV]  4 Aug 2015lem for several reasons: rst, negrained actions for any highlevel activity or event are inherently similar since they take place in similar scene context; second, occurrences of the negrained actions are usually short (a few seconds) in training videos, making it dicult to associate the video level labels to the occurrences. Our key observation is that one can exploit web images to help localize negrained actions in videos. As illustrated in Figure 1, by using action names ( basketball slam dunk ) as queries, many of the image search results oer well lo calized actions, though some of them are nonvideo like or irrelevant. Identifying action related frames from weakly su pervised videos and ltering irrelevant image tags is hard in either modality by itself; however, it is easier to tackle these two problems together. This is due to our observation that although most of the video frames and web images which correspond to actions are visually similar, the distributions of nonaction images from the video domain and the web image domain are usually very dierent. For example, in a video with a basketball slam dunk , non slam dunk frames in the video are mostly from a basketball game. The irrele vant results returned by image search are more likely to be product shots, or cartoons. This motivates us to formulate a domain transfer problem between web images and videos. To allow domain transfer, we rst treat the videos as a bag of frames, and use the feature activations from deep convolutional neural networks (CNN) [13] as the common representation for images and frames. Suppose we have selected a set of video frames and a set of web images for every action, the domain transfer framework goes in two directions: video frames to web im ages, and vice versa. For both directions, we use the selected images from the source domain to train action classiers by netuning the top layers of the CNN; we then apply the trained classiers to the target domain. Each image in the target domain is assigned a condence score given by its as sociated action classier from the source domain. By grad ually ltering out the images with low scores, the bidirec tional domain transfer can progress iteratively. In practice, we start from the video frames to web images direction, and randomly select the video frames for training. Since the nonaction related frames are not likely to occur in web im ages, the tuned CNN can be used to lter out the nonvideo like and irrelevant web images. The nal domain transfer from web images is used to localize action related frames in videos. We term these actionrelated frames as localized action frames (LAF). Videos are more than an unordered collection of frames. We choose long shortterm memory (LSTM) [8] networks as the temporal model. Compared with the traditional recur rent neural networks (RNN), LSTM has builtin input gates and forget gates to control its memory cells. These gates allow LSTM to either keep a long term memory or forget its history. The ability to learn from long sequences with unknown size of background is wellsuited for negrained action localization from unconstrained web videos. We treat every sampled video frame as a time step in LSTM. When we train LSTM models, we label all video frames by their videolevel annotation, but use the LAF scores generated by bidirectional domain transfer as weights on the loss for mis classication. By doing this, irrelevant frames are eectively downweighted in the training stage. The framework can be naturally extended to use video shots as time steps, fromwhich spatiotemporal features can be extracted to capture local motion information. Finegrained action localization from untrimmed web videos is a new task. The closest existing data set is THUMOS 2014 with 20 sports categories. It is designed for action lo calization using segmented videos as training, but has 1,010 untrimmed validation videos. To evaluate the framework in a large scale setting, we collected a new data set from YouTube. We chose 240 negrained actions belonging to 85 dierent sports activities, the total number of videos is over 130,000. Although the evaluated categories are sports ac tions, this method can be easily extended to other domains. For example, one can easily get cut cake, eat cake and blow candle images for a birthday party event with image search. Our work makes three major contributions: We show that learning temporally localized actions from videos becomes much easier if we combine weakly labeled video frames and noisily tagged web images. This is achieved by a simple yet eective domain trans fer algorithm. We propose a localization framework that uses LSTM network with the localized action frames to model the temporal evolution of actions. We introduce the problem of negrained action lo calization with untrimmed videos, and collect a large negrained sports action data set with over 130,000 videos in 240 categories. The data set is available on line.1 2. RELATED WORK "
67,Exploiting the relationship between visual and textual features in social networks for image classification with zero-shot deep learning.txt,"One of the main issues related to unsupervised machine learning is the cost
of processing and extracting useful information from large datasets. In this
work, we propose a classifier ensemble based on the transferable learning
capabilities of the CLIP neural network architecture in multimodal environments
(image and text) from social media. For this purpose, we used the InstaNY100K
dataset and proposed a validation approach based on sampling techniques. Our
experiments, based on image classification tasks according to the labels of the
Places dataset, are performed by first considering only the visual part, and
then adding the associated texts as support. The results obtained demonstrated
that trained neural networks such as CLIP can be successfully applied to image
classification with little fine-tuning, and considering the associated texts to
the images can help to improve the accuracy depending on the goal. The results
demonstrated what seems to be a promising research direction.","The high cost of processing and extracting useful information for its application in dierent areas of interest is one of the major issues related to unsupervised machine learning. Models based on neural networks need to be trained with sim ilar data to the ones they are going to predict or intend to model. Furthermore, depending on the type of data, its characteristics, dataset size, and other factors, one architecture could be more convenient than another. Usually, the cost of ad equate training consumes too much time and resources that are not available to most organizations. The release of new models based on neural networks, which have been trained with huge amounts of data, allows transferring their knowledge to dierent tasks and areas. In this paper, the goal is to classify images and text from social media to allow obtaining specic and useful information in dierent areas of knowledge of social sciences such as economics or sociology.arXiv:2107.03751v1  [cs.CV]  8 Jul 20212 Lucas et al. In this work, we will rst use CLIP (Contrastive LanguageImage PreTraining) [10] transformer to classify images of a dataset obtained from social networks containing picturetext pairs obtained with a pretrained neural network into 205 labels corresponding to places or scenes. We will then rene such classica tion with the associated texts to check and compare the obtained accuracy. For this purpose, we will use a samplingbased validation since the dataset used is unlabeled. The remainder of the paper is structured as follows: Section 2 shows related work; Section 3 outlines the pieces of the system that we will use in our exper iments; Section 4 describes the image classication work ow, rstly using only the images and secondly by rening the classication with the associated texts from the dataset; Section 5 shows the validation process and the results obtained; nally, in Section 6 we discuss our conclusions and suggest future work. 2 Related work "
615,ExpertNet: Adversarial Learning and Recovery Against Noisy Labels.txt,"Today's available datasets in the wild, e.g., from social media and open
platforms, present tremendous opportunities and challenges for deep learning,
as there is a significant portion of tagged images, but often with noisy, i.e.
erroneous, labels. Recent studies improve the robustness of deep models against
noisy labels without the knowledge of true labels. In this paper, we advocate
to derive a stronger classifier which proactively makes use of the noisy labels
in addition to the original images - turning noisy labels into learning
features. To such an end, we propose a novel framework, ExpertNet, composed of
Amateur and Expert, which iteratively learn from each other. Amateur is a
regular image classifier trained by the feedback of Expert, which imitates how
human experts would correct the predicted labels from Amateur using the noise
pattern learnt from the knowledge of both the noisy and ground truth labels.
The trained Amateur and Expert proactively leverage the images and their noisy
labels to infer image classes. Our empirical evaluations on noisy versions of
CIFAR-10, CIFAR-100 and real-world data of Clothing1M show that the proposed
model can achieve robust classification against a wide range of noise ratios
and with as little as 20-50% training data, compared to state-of-the-art deep
models that solely focus on distilling the impact of noisy labels.","The everincreasing selfgenerated contents on social media, e.g., Instagram images, power up the deep neural networks, but also aggravate the challenge of noisy data. Large portion of images accessible on the public domain come with labels which are unfortunately noisy due to careless annotations [1, 28] or even adversary strategies [4, 13, 22]. The learning capacity of deep neural networks is shown to be hindered by such noisy labels [30] due to the memo rization effect of networks. ClassiÔ¨Åcation accuracy on standard image benchmarks degrades drastically in the presence of dirty labels. For example, the accuracy of a trained AlexNet to classify CIFAR10 images drops from 77% to 10%, when trained on noisy labels [30]. c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2007.05305v2  [cs.LG]  13 Jul 20202 GHIASSI, BIRKE, CHEN: EXPERTNET Motivated by the signiÔ¨Åcant impact of noisy labels, the prior art [7] derives different robust deep networks with a central theme to distill the inÔ¨Çuence of noisy labels in the model training process without the knowledge of the label ground truth. As a result, the learned networks can robustly classify images in a standalone manner. D2L [26] estimates the Local Intrinsic Dimension (LID) at each epoch as a proxy to indicate the existence and impact of dirty labels. Coteaching [5] trains two networks simultaneously by exchanging the weights updated from possibly clean data. Forward [19] uses a noiseaware correction matrix to correct labels and train the network. Bootstrap [20] has a loss function which combines predicted and noisy labels. While prior art signiÔ¨Åcantly improves the robustness of deep networks, the preassumed scenarios overlook the opportunity of noisy labels. On the one hand, today‚Äôs image data are often bundled with labels of questionable quality and detrimental impact on the learning. On the other hand, such labels provide auxiliary information which can compliment the learnt knowledge of deep networks trained solely on image inputs. The core idea behind visual semantic models, e.g., DeVise [2], is to combine the learning capacities of labeled images and annotated data. CGAN [17] (conditional generative adversarial network) improves the quality of images synthesized by the generator network via additional label information and RCGAN [23] further addresses the challenge of dirty labels for CGAN. In this paper, we advocate to leverage the noisy labels as an additional feature to derive a stronger classiÔ¨Åer. We consider learning scenarios where at training time both the ground truth and noisy labels are available, and only noisy labels at inference time. In particular difÔ¨Åcult classiÔ¨Åcation problems, whose labels require a high degree of expertise, can Ô¨Åt this scenario well. One such example is cancer detection from medical images. This is a daunting task, and even trained experts are prone to make errors. Hence, these images are evaluated by multiple doctors of varying expertise. In such a setting, both noisy (Ô¨Årst evaluation by one expert) and true labels (e.g., stemming from a committee or subsequent indepth exams) are available at the same time. We derive a robust network, namely ExpertNet, composed of Amateur and Expert, where the former classiÔ¨Åes images based on the feedback from Expert and the latter learns how to correct the output of Amateur like human experts. Both models are trained simultaneously at each minibatch. Amateur learns to classify the input images to the corrected labels from Expert, and the softmax output of Amateur plus the given labels are inputs to train Expert to match the ground truth. Amateur can be seen as a regular image classiÔ¨Åer, which Expert helps it to be aware of the presence of noisy labels. Such trained Amateur and Expert can then classify images based on the image and corresponding noisy label. We empirically evaluate ExpertNet on synthetic noise injected into CIFAR10 and CIFAR 100, and noise drawn from real world contained in Clothing1M. For a fair comparison with stateoftheart robust deep models, we present the classiÔ¨Åcation accuracy in both Amateur only and complete ExpertNet model under different subsets of training inputs. ExpertNet consistently outperforms existing imageonly models, i.e., D2L, Coteaching, Forward and Boostrap, especially for CIFAR100. When using the same amount of training data, Ex pertNet can achieve absolute accuracy improvements of 5% up to 30%. ExpertNet reaches similar or higher accuracy than imageonly models even with just 20% of training data in the case of CIFAR100. Our contribution can be summarized as follows. First, we derive a novel network frame work, i.e., ExpertNet, that turns noisy labels into auxiliary learning advantages via imitating human experts. Secondly, we signiÔ¨Åcantly improve the robustness of deep networks against noisy labels compared to models based on images only.GHIASSI, BIRKE, CHEN: EXPERTNET 3 1.1 Problem statement                              Robust Deep Network Image   Noisy Image Label  Training  Image   Image Label Prediction  Inference  Robust Deep Network Image Label Prediction  Robust Deep Network Image   Noisy Image Label  Training Image Label Prediction  Image   Inference   Robust Deep Network  Noisy Image Label  Image Label Prediction  Ground Truth  Figure 1: Training and testing image classiÔ¨Åers with noisy labels. The problem considered here is as follows. Images collected in the public domain are tagged with preexisting noisy labels, whose true classes can be corrupted. We assume label noises follow random distribution. We illustrate in Fig. 1 (black elements only) the learning procedure that is commonly deployed by robust deep networks [5, 19, 20, 26]. The deep networks are trained by a set of images and labels, which are noisy, meaning with incorrect label classes. The objective of the training process is to minimize the loss function, which may be modiÔ¨Åed to be noise tolerant [26]. The network architecture may consist of different components, e.g., two networks that parallelly [5] or sequentially [19] train each other via stochastic gradient descent. In the inference phase, images are then fed into the trained network, and the prediction accuracy is computed based on true labels. The core idea behind such a learning process is to Ô¨Ålter out the negative impact of noisy labels during training and learn a model from clean information. In contrast to indirectly learn the label noise dynamics, our core idea is to leverage noisy labels as part of the training and inference input, as shown in Fig. 1 (including green el ements), to directly learn the noisy label dynamics and incorporate that as auxiliary input into the training process. To such an end, the ground truth of labels is assumed from human experts or oracles and provided as part of the training input. Essentially, the networks are trained by three inputs: images, their noisy labels, and the ground truth labels. Afterward, the trained network will be tested on images and their noisy labels. The classiÔ¨Åer can then classify images based on image inputs and limited label info. 2 Related Work "
598,Joint Binary Neural Network for Multi-label Learning with Applications to Emotion Classification.txt,"Recently the deep learning techniques have achieved success in multi-label
classification due to its automatic representation learning ability and the
end-to-end learning framework. Existing deep neural networks in multi-label
classification can be divided into two kinds: binary relevance neural network
(BRNN) and threshold dependent neural network (TDNN). However, the former needs
to train a set of isolate binary networks which ignore dependencies between
labels and have heavy computational load, while the latter needs an additional
threshold function mechanism to transform the multi-class probabilities to
multi-label outputs. In this paper, we propose a joint binary neural network
(JBNN), to address these shortcomings. In JBNN, the representation of the text
is fed to a set of logistic functions instead of a softmax function, and the
multiple binary classifications are carried out synchronously in one neural
network framework. Moreover, the relations between labels are captured via
training on a joint binary cross entropy (JBCE) loss. To better meet
multi-label emotion classification, we further proposed to incorporate the
prior label relations into the JBCE loss. The experimental results on the
benchmark dataset show that our model performs significantly better than the
state-of-the-art multi-label emotion classification methods, in both
classification performance and computational efficiency.","Multilabel emotion classiÔ¨Åcation, is a subtask of the text emotion classiÔ¨Åcation, which aims at identifying the coexisting emotions (such as joy, anger and anxiety, etc.) expressed in the text, has gained much attention due to its wide potential applications. Taking the following sentence Example 1: ‚Äú Feeling the warm of her hand and the attach ment she hold to me, I couldn‚Äôt afford to move even a little, fearing I may lost her hand ‚Äùfor instance, the coexisting emotions expressed in it contain joy,love, and anxiety . Traditional multilabel emotion classiÔ¨Åcation methods nor mally utilize a twostep strategy, which Ô¨Årst requires to de velop a set of handcrafted expert features (such as bagof words, linguistic features, emotion lexicons, etc.), and then makes use of multilabel learning algorithms [Xuet al. , 2012; Liet al. , 2015; Wang and Pal, 2015; Zhou et al. , 2016; Yan and Turtle, 2016 ]for multilabel classiÔ¨Åcation. How ever, the work of feature engineering is laborintensive and timeconsuming, and the system performance highly depend s on the quality of the manually designed feature set. In recent years, deep neural networks are of growing attention due to their capacity of automatically learn the internal representa tions of the raw data and integrating feature representation learning and classiÔ¨Åcation into one endtoend framework. Existing deep learning methods in multilabel classiÔ¨Åca tion can be roughly divided into two categories: ‚Ä¢Binary relevance neural network (BRNN), which con structs an independent binary neural network for each label, where multilabel classiÔ¨Åcation is considered as a set of isolate binary classiÔ¨Åcation tasks and the predic tion of the label set is composed of independent predic tions for individual labels. ‚Ä¢Threshold dependent neural network (TDNN), which normally constructs one neural network to yield the probabilities for all labels via a softmax function, where the probabilities sum up to one. Then, an additional threshold mechanism (e.g., the calibrated label ranking algorithm) is further needed to transform the multiclass probabilities to multilabel outputs. The structure of BRNN and TDNN are shown in Figure 1 (a) and (b), respectively. However, both kinds of methods have their shortcomings. The former one, BRNN, usually known in the literature as binary relevance (BR) transformation [Spyromitros et al. , 2008 ], not only ignores dependencies between labels, but al so consumes much more resources due to the need of training a unique classiÔ¨Åer and make prediction for each label. The latter one, TDNN, although has only one neural network, can only yield the category probabilities of all class labels. In stead, it needs an additional threshold function mechanism to transform the category probabilities to multilabel outputs. arXiv:1802.00891v1  [cs.LG]  3 Feb 2018InputLayerProbabilityOutputsMultilabelOutputs (a)	Binary	Relevance	NeuralNetwork(BRNN)(b)	Threshold	Dependent	 Neural	Network(TDNN)(c)	Our	Proposed	 Joint	 Binary	Neural	Network	 (JBNN)NeuralNetworkArgmaxArgmaxThresholdDecisionFigure 1: Different ways of constructing neural networks for multilabel classiÔ¨Åcation. However, building an effective threshold function is also full of challenges for multilabel learning [Zhang and Zhou, 2006; Read and PerezCruz, 2014; Nam et al. , 2014; Xu et al. , 2017; Lenc and Kr ¬¥al, 2017 ]. In this paper, we propose a simple joint binary neural net work (JBNN), to address these two problems. We display the structure of JBNN in Figure 1 (c). As can be seen, in JBNN, the bottom layers of the network are similar to that in TND D. SpeciÔ¨Åcally, we employ a Bidirectional Long ShortTerm Memory (BiLSTM) structure to model the sentence. The attention mechanism is also constructed to get the sentence representation. After that, instead of a softmax function used in TDNN, we feed the representation of a sentence to mul tiple logistic functions to yield a set of binary probabilities. That is, for each input sentence, we conduct multiple binary classiÔ¨Åcations synchronously in one neural network frame work. Different from BRNN, the word embedding, LSTMs, and the sentence representation are shared among the multi ple classiÔ¨Åcation components in the network. Moreover, the relations between labels are captured based on a joint binary learning loss. Finally, we convert the multivariate Bernoulli distributions into multilabel outputs, the same as BRNN. The JBNN model is trained based on a joint binary cross entropy (JBCE) loss. To better meet the multilabel emotion classiÔ¨Å cation task, we further proposed to incorporate the prior label relations into the JBCE loss. We evaluate our JBNN model on the widelyused multilabel emotion classiÔ¨Åcation dataset RenCECps [Quan and Ren, 2010 ]. We compare our mod el with both traditional methods and neural networks. The experimental results show that: ‚Ä¢Our JBNN model performs much better than the state oftheart traditional multilabel emotion classiÔ¨Åcation methods proposed in recent years; ‚Ä¢In comparison with the BRNN and TDNN systems, our JBNN model also shows the priority, in both classiÔ¨Åca tion performance and computational efÔ¨Åciency.2 Related Work "
551,Cross-dataset Person Re-Identification Using Similarity Preserved Generative Adversarial Networks.txt,"Person re-identification (Re-ID) aims to match the image frames which contain
the same person in the surveillance videos. Most of the Re-ID algorithms
conduct supervised training in some small labeled datasets, so directly
deploying these trained models to the real-world large camera networks may lead
to a poor performance due to underfitting. The significant difference between
the source training dataset and the target testing dataset makes it challenging
to incrementally optimize the model. To address this challenge, we propose a
novel solution by transforming the unlabeled images in the target domain to fit
the original classifier by using our proposed similarity preserved generative
adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the
generative adversarial networks with the cycle consistency constraint to
transform the unlabeled images in the target domain to the style of the source
domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is
measured by a siamese deep convolutional neural network, to preserve the
similarity of the transformed images of the same person. Comprehensive
experiments based on multiple real surveillance datasets are conducted, and the
results show that our algorithm is better than the state-of-the-art
cross-dataset unsupervised person Re-ID algorithms.","As one of the most important and challenging problems in the Ô¨Åeld of surveillance video analysis, person reidentiÔ¨Åcation (ReID) aims to match the image frames which contain the same person in the surveillance videos. How to extract the view invariant features from the images and design a robust visual classiÔ¨Åer to identify the persons is the core challenge of the ReID algorithms. Due to the privacy problem regarding the collection of surveillance videos and the expensive cost of data labeling, most of the proposed ReID algorithms [16] [12] con duct supervised learning on small labeled datasets. Directly deploying these trained models to the realworld largescale camera networks may lead to a poor performance, ?The work described in this paper was supported by the grants from NSFC (No. U1611461), Science and Technology Program of Guangdong Province, China (No. 2016A010101012), and CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, 100190, Beijing, China.(No.CASNDST201703).arXiv:1806.04533v2  [cs.CV]  21 Jun 20182 J. Lv et al. Market1501  GRID  CUHK03   Fig. 1: Samples in different datasets. because the images captured from different camera networks usually have totally differ ent backgrounds, noise distributions, brightness and resolution as shown in Fig.1. How to incrementally optimize the ReID algorithms based on the abundant unlabeled data collected from the target domain is a practical and extremely challenging problem. To address this problem, some unsupervised algorithms [7] [10] are proposed to extract view invariant features and measure the similarity of pedestrians based on the unlabeled dataset. Without the powerful supervision based on labels, this kind of pure unsupervised learning based algorithms working on one single dataset have a poor per formance in most cases. Recently, a crossdataset unsupervised transfer learning algo rithm, named UMDL [11] , is proposed to make use of data samples from both labeled source datasets and the unlabeled target dataset to learn the viewinvariant feature rep resentation and similarity measurement by the dictionary learning mechanism. UMDL gains much better performance than purely unsupervised algorithms, but is still much weaker than the stateoftheart supervised algorithms based on the labeled dataset. Most of above algorithms try to incrementally optimize the visual classiÔ¨Åer, which is pretrained in the source dataset, to Ô¨Åt the new data in the target domain. However, with out the labels in the target domain, it is hard to Ô¨Ånetune the classiÔ¨Åer to suit the source and target datasets simultaneously, which have diverse feature distributions. We address this crossdataset ReID challenge in a totally new direction in this pa per. Instead of optimizing the classiÔ¨Åer to Ô¨Åt the new data, we transform the unlabeled images in the target domain to Ô¨Åt the classiÔ¨Åer by using our proposed similarity pre served generative adversarial networks model, SimPGAN. As shown in Fig.5 regarding an example from the real datsets, after the transformation, the features of the images in the target datasets are projected into the features close to the distribution in the source dataset, and are easier to be processed by the visual classiÔ¨Åer trained in the source dataset. The main contributions of this paper are summarized as follows: ‚ÄìWe propose a novel efÔ¨Åcient solution, named SimPGAN, to improve the perfor mance of crossdataset person ReID by transforming the unlabeled images in theCrossdataset Person ReIdentiÔ¨Åcation 3 target domain into the style of the source domain to Ô¨Åt the visual classiÔ¨Åer trans ferred from the source domain. ‚ÄìSimPGAN adopts the generative adversarial networks with cycle consistency con straint to avoid sharp change of the images after transformation. Meanwhile, SimP GAN uses the similarity consistency loss, which is measured by a siamese deep convolutional neural network, to preserve the similarity of the transformed images of the same person. ‚ÄìWe conduct comprehensive experiments based on real datasets (Market1501[17], CUHK01[14] , GRID[1]), which show that SimPGAN is better than the stateof theart crossdataset unsupervised transfer learning algorithm[11] . The rest of this paper is organized as follows. Section 2 reviews the related work of ReID. Section 3 offers clear deÔ¨Ånitions of the problem about ReID in unlabeled dataset. Section 4 presents our proposed methods. Section 5 evaluates the performance of this system by conducting experiments on real datasets. We conclude the work in Section 6. 2 Related work "
185,Robust Data-Driven Discovery of Partial Differential Equations under Uncertainties.txt,"Robust physics (e.g., governing equations and laws) discovery is of great
interest for many engineering fields and explainable machine learning. A
critical challenge compared with general training is that the term and format
of governing equations is not known as a prior. In addition, significant
measurement noise and complex algorithm hyperparameter tuning usually reduces
the robustness of existing methods. A robust data-driven method is proposed in
this study for identifying the governing Partial Differential Equations (PDEs)
of a given system from noisy data. The proposed method is based on the concept
of Progressive Sparse Identification of PDEs (PSI-PDE or $\psi$-PDE). Special
focus is on the handling of data with huge uncertainties (e.g., 50$\%$ noise
level). Neural Network modeling and fast Fourier transform (FFT) are
implemented to reduce the influence of noise in sparse regression. Following
this, candidate terms from the prescribed library are progressively selected
and added to the learned PDEs, which automatically promotes parsimony with
respect to the number of terms in PDEs as well as their complexity. Next, the
significance of each learned terms is further evaluated and the coefficients of
PDE terms are optimized by minimizing the L2 residuals. Results of numerical
case studies indicate that the governing PDEs of many canonical dynamical
systems can be correctly identified using the proposed $\psi$-PDE method with
highly noisy data. One great benefit of proposed algorithm is that it avoids
complex algorithm modification and hyperparameter tuning in most existing
methods. Limitations of the proposed method and major findings are presented.","Despite that many dynamical systems can be well characterized by PDEs derived mathematically/physically from basic principles such as conservation laws, lots of other sys tems have unclear or elusive underlying mechanisms (e.g., ones in neuroscience, Ô¨Ånance, and Email addresses: zzhan506@asu.edu (Zhiming Zhang), corresponding author:yongming.liu@asu.edu (Yongming Liu) Preprint submitted to Elsevier February 15, 2021arXiv:2102.06504v1  [math.NA]  31 Jan 2021ecology). Thus, the governing equations are usually empirically formulated [1]. Datadriven physics discovery of dynamical systems gradually became possible in recent years due to the rapid development and extensive application of sensing technologies and computational power [2]. Over the past years, extensive eÔ¨Äorts have been devoted into discovering represen tative PDEs for complex dynamical systems of which limited prior knowledge are available [1‚Äì4]. Among all the methods investigated for PDE identiÔ¨Åcation [1‚Äì8], sparse regression gains the most attention in recent studies due to its inherent parsimonypromoting advan tage. Considering a nonlinear PDE of the general form ut=N(u;ux;uxx;:::;x ), in which the subscripts denote partial diÔ¨Äerentiation with respect to temporal or spatial coordinate(s), N()is an unknown expression on the right hand side of the PDE. It is usually a nonlinear function of the spatial coordinate x, the measured quantity u(x;t), and its spatial deriva tivesux,uxx, etc. Given time series measurements of uat certain spatial locations, the above equation can be approximated as Ut=(U), in which Utis the discretized form of ut, (U)is a library matrix with each column corresponding to a candidate term in N(). A key assumption in sparse identiÔ¨Åcation is that N()consists of only a few term for a real physical system, which requires the solution of regression (i.e., ) to be a sparse vector with only a limited number of nonzero elements. This assumption promotes a parsimonious form of the learned PDE instead of overÔ¨Åtting the measured data with a complex model containing redundant nonlinear higherorder terms. As pioneering researchers in sparse PDE learning, Rudy et al. [1, 9] modiÔ¨Åed the ridge regression method by imposing hard thresholding which recursively eliminates certain terms with coeÔ¨Écient values below a learned threshold. As pointed out in Limitations of [1, 9] (Section 4 in Supplementary Materials) and following studies [4, 10, 11], the identiÔ¨Åcation quality is very sensitive to data quantity and quality. For example, the terms of the reaction diÔ¨Äusion equation cannot be correctly identiÔ¨Åed using the data with only 0.5% random noise. Furthermore, as indicated in [12], the identiÔ¨Åcation results using this method are susceptible to the selection of hyperparameters of the algorithm, including the regularizer and the initial tolerance which is also the tolerance increment dtol. The hyperparameter tuning is especially critical for cases with noisy measurements. This limitation most probably comes from the hard thresholding in the modiÔ¨Åed algorithm (STRidge). A hard thresholding tends to suppress small coeÔ¨Écients that may not correspond to the most trivial terms of the intermediately learned PDEs. To overcome the challenge of numerical diÔ¨Äerentiation with scarce and noisy data in sparse regression methods, deep learning techniques were incorporated by generating a large quantity of metadata and adopting the automatic diÔ¨Äerentiation function in deep learning frameworks (TensorÔ¨Çow, PyTorch, etc.) [13, 14]. The intermediately learned PDE can be treated as a physics loss term in physicsinformed deep learning [15‚Äì17], and constrained neuralnetworksweredevelopedtoimprovetheperformanceofPDEidentiÔ¨Åcationrecursively [7, 10, 11]. Long et al. [2, 18] used a convolutional architecture and symbolic regression to replace the numerical diÔ¨Äerentiation and sparse regression procedures, respectively. A comprehensive review of the state of the art of PDE learning can be found in [10]. Despite improved performance of PDE identiÔ¨Åcation using these methods, the identiÔ¨Åcation results 2(both PDE forms and coeÔ¨Écients) are lacking robustness in most studies mentioned above. For example, approaches using constrained neural networks introduced more hyperparame ters into the algorithms in addition to those in the used STRidge algorithm, which further increases the challenge of identifying the correct PDE forms since PDE learning problems are sensitive to the hyperparameter tuning. This issue is ampliÔ¨Åed under noisy data, es pecially under high noise levels. Complete diÔ¨Äerent identiÔ¨Åcation results may be obtained under diÔ¨Äerent noise levels using same hyperparameter settings. Thus, a sound robust PDE learning needs to produce stable identiÔ¨Åcation results with respect to diÔ¨Äerent noise levels. Considering the gaps of existing studies in discovering PDEs from complex dynamical systems, a robust method for correctly identifying PDEs is needed to discover the underlying physics of the measured systems that lack prior knowledge of the governing principles. Thus, this study attempts to develop a robust method of PDE identiÔ¨Åcation within the framework of sparse regression. The key idea is to address both sparsity and accuracy of the learned PDE. Special focus is on the automatic and progressive selection of learned PDE forms without complex algorithms with hardtotune hyperparameters [3]. The proposed scheme automatically promotes sparsity in addition to simplicity of the learned model. Finally, the representativeness of each model will be further evaluated by solving its corresponding PDE with given/extracted initial and/or boundary conditions. The coeÔ¨Écients of each term are optimized by minimizing the error of model prediction with the measured data taken as the ground truth. In this way, the PDE that is most likely to represent the intrinsic mechanisms underlying the observed system will be determined. Since the proposed methodology progressively yields a sparse identiÔ¨Åcation of the governing PDE(s) of a given system, it is named the progressive and sparse identiÔ¨Åcation method of PDEs (PSIPDE or  PDE method). The remaining part of this paper is structured as follows. Section 2 establishes the framework of the  PDE method; section 3 presents and discusses the results of discovering govern equations for a variety of dynamical systems using the  PDE method; section 4 concludes this study with remarks and recommendations for future work. 2. Methodology: a robust PDE learning method "
49,Semi-Supervised Segmentation of Salt Bodies in Seismic Images using an Ensemble of Convolutional Neural Networks.txt,"Seismic image analysis plays a crucial role in a wide range of industrial
applications and has been receiving significant attention. One of the essential
challenges of seismic imaging is detecting subsurface salt structure which is
indispensable for identification of hydrocarbon reservoirs and drill path
planning. Unfortunately, exact identification of large salt deposits is
notoriously difficult and professional seismic imaging often requires expert
human interpretation of salt bodies. Convolutional neural networks (CNNs) have
been successfully applied in many fields, and several attempts have been made
in the field of seismic imaging. But the high cost of manual annotations by
geophysics experts and scarce publicly available labeled datasets hinder the
performance of the existing CNN-based methods. In this work, we propose a
semi-supervised method for segmentation (delineation) of salt bodies in seismic
images which utilizes unlabeled data for multi-round self-training. To reduce
error amplification during self-training we propose a scheme which uses an
ensemble of CNNs. We show that our approach outperforms state-of-the-art on the
TGS Salt Identification Challenge dataset and is ranked the first among the
3234 competing methods.","One of the major challenges of seismic imaging is localization and delineation of subsurface salt bodies. The precise location of salt deposits helps to identify reservoirs of hydrocarbons, such as crude oil or natural gas, which are trapped by overlying rocksalt formations due to the exceedingly small permeability of the latter. Modern seismic imaging techniques result in large amounts of unlabeled data which have to be interpreted. Unfortunately, the exact identication of large salt deposits is notoriously dicult [21] and often requires manual interpretation ofarXiv:1904.04445v3  [cs.CV]  5 Aug 20192 Yauhen Babakhin, Artsiom Sanakoyeu, Hirotoshi Kitamura Fig. 1. Progress of the validation loss (top) and the validation mAP score (bottom) during training our UResNet34 model on TGS Salt Identication Challenge dataset [22] forK= 3 rounds. Every next round the model converges faster and achieves better local minima. Loss spikes every 50 epochs correspond to the cycles of the cosine annealing learning rate schedule. seismic images by the domain experts. Despite being highly timeconsuming and expensive, manual interpretation induces a subjective human bias, which can lead to potentially dangerous situations for oil and gas company drillers. In recent years, a number of tools for automatic or semiautomatic seismic interpretation have been proposed [37,13,18,53,47,3,8,46] to speedup the inter pretation process and, to some extent, reduce the human bias. However, these methods do not generalize well for complex cases since they rely on handcrafted features. The advent of convolutional neural networks (CNNs) brought signicant ad vancements in dierent problems and several attempts have been made to apply CNNs in the eld of seismic imaging [43,11,45,52]. CNNs overcome the need for manual feature design and show superior performance on the tasks of the salt body delineation compared to the methods based on the handcrafted features. However, a low amount of publicly available annotated seismic images hinder the performance of the existing CNNbased methods since CNNs are notoriously hungry for data. To overcome the shortage of labeled data, we propose a semisupervised method for segmentation of salt bodies in seismic images which can make use of abundant unlabeled data. The unlabeled images are utilized for selftraining [42]. The proposed selftraining procedure (see Fig. 2) is an iterative process which extends the labeled dataset by alternating between training the model and pseudolabeling (i.e. imputing the labels on the unlabeled data). We do K rounds of retraining the model (see the straining in Fig. 1). At the rst round, we train model solely on the available labeled data and then predict labels on the unlabeled data. Every next round we use for training both original labeled data and the pseudolabels obtained at the previous round. The error amplication isSemiSupervised Segmentation of Salt Bodies in Seismic Images using CNNs 3 Fig. 2. The pipeline of the proposed selftraining procedure. We do Krounds of re training the model. Every round we train the model on the available labeled data and predicted condent pseudolabels for the unlabeled data. Allpseudolabels are recalculated at the end of every round. a wellknown problem in selftraining [29] when the error is accumulated during selftraining rounds and the models tend to generate less reliable predictions during the time. To mitigate it we propose to train an ensemble of CNNs and predict labels on the unlabeled data using the average voting of the models in the ensemble. Average voting scheme corrects examples which could be mislabeled by one of the models, hence facilitates more reliable pseudolabeling. Moreover, to further reduce the error amplication we retrain our models from scratch and predict labels for allunlabeled examples every round in similar spirit as [29]. We conduct experiments on the largest available to our knowledge dataset for salt body delineation { TGS Salt Identication Challenge dataset [22]. This dataset was collected by TGS, the world's leading geoscience data company, and was provided in the Kaggle competition. Our approach achieves stateoftheart performance on this dataset featuring the rst place in the global ranking among 3234 competitors. In summary, the contribution of this work is as follows: (i) we propose an iterative selftraining approach for semantic segmentation which benets from unlabeled data; (ii) we build a sophisticated network architecture which is tai lored for the task of salt body delineation (see Fig. 3); (iii) we evaluate our4 Yauhen Babakhin, Artsiom Sanakoyeu, Hirotoshi Kitamura Fig. 3. The outline of the UResNet34/UResNeXt50 architecture proposed. The dif ference between UResNet34 and UResNeXt50 is only in the structure of the encoder blocks (green). We insert scSE modules [39] after each encoder (green) and decoder (purple) blocks. Encoder blocks are connected with the corresponding decoder blocks using skipconnections. We use a Feature Pyramid Attention module (FPA) [15] after the last encoder block. All outputs of the decoder blocks are upsampled to have the same size as the output of the last decoder bock. Obtained feature maps are concate nated together into hypercolumns [16], which are used for prediction of the segmenta tion mask after applying two convolutional layers. approach on a realworld salt body delineation dataset { TGS Salt Identica tion Challenge [22], where the proposed method achieves the stateoftheart performance outperforming allother competing teams. 2 Related work "
431,Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing.txt,"In this paper, we introduce a new problem, named audio-visual video parsing,
which aims to parse a video into temporal event segments and label them as
either audible, visible, or both. Such a problem is essential for a complete
understanding of the scene depicted inside a video. To facilitate exploration,
we collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual
video parsing in a weakly-supervised manner. This task can be naturally
formulated as a Multimodal Multiple Instance Learning (MMIL) problem.
Concretely, we propose a novel hybrid attention network to explore unimodal and
cross-modal temporal contexts simultaneously. We develop an attentive MMIL
pooling method to adaptively explore useful audio and visual content from
different temporal extent and modalities. Furthermore, we discover and mitigate
modality bias and noisy label issues with an individual-guided learning
mechanism and label smoothing technique, respectively. Experimental results
show that the challenging audio-visual video parsing can be achieved even with
only video-level weak labels. Our proposed framework can effectively leverage
unimodal and cross-modal temporal contexts and alleviate modality bias and
noisy labels problems.","Human perception involves complex analyses of visual, auditory, tactile, gustatory, olfactory, and other sensory data. Numerous psychological and brain cognitive studies [ 3,20,46,51] show that combining dierent sensory data is crucial for human perception. However, the vast majority of work [ 9,26,48,64] in scene understanding, an essential perception task, focuses on visualonly methods ignoring other sensory modalities. They are inherently limited. For example, when the object of interest is outside of the eldofview (FoV), one would rely on audio cues for localization. While there is little data on tactile, gustatory, or olfactory signals, we do have an abundance of multimodal audiovisual data, e.g., YouTube videos. Utilizing and learning from both auditory and visual modalities is an emerg ing research topic. Recent years have seen progress in learning representa tions [ 1,2,19,23,37,38], separating visually indicated sounds [ 8,11,12,13,65,66,10,70],arXiv:2007.10558v1  [cs.CV]  21 Jul 20202 Y. Tian et al. A: Basketball5s10sDog 4s8sV: BasketballAV: Dog0s10s0s5s Speech2s3sBasketballSpeech2s3sBasketball5s10s0s5sSpeech2s3sBasketballDog2s3s2s 2s5s3s10s4s5s10s Fig. 1: Our audiovisual video parsing model aims to parse a video into dierent audio (audible), visual (visible), and audiovisual (audivisible) events with correct categories and boundaries. A dog in the video visually appears from 2nd second to 5th second and make barking sounds from 4th second to 8th second. So, we have audio event (4s8s), visual event (2s5s), and audiovisual event (4s5s) for theDog event category. spatially localizing visible sound sources [ 37,45,55], and temporally localizing audiovisual synchronized segments [ 27,55,63]. However, past approaches usually assume audio and visual data are always correlated or even temporally aligned. In practice, when we analyze the video scene, many videos have audible sounds, which originate outside of the FoV, leaving no visual correspondences, but still contribute to the overall understanding, such as outofscreen running cars and a narrating person. Such examples are ubiquitous, which leads us to some basic questions: what video events are audible, visible, and \audivisible,"" where and when are these events inside of a video, and how can we eectively detect them? To answer the above questions, we pose and try to tackle a fundamental problem: audiovisual video parsing that recognizes event categories bind to sensory modalities, and meanwhile, nds temporal boundaries of when such an event starts and ends (see Fig. 1). However, learning a fully supervised audiovisual video parsing model requires densely annotated event modality and category labels with corresponding event onsets and osets, which will make the labeling process extremely expensive and timeconsuming. To avoid tedious labeling, we explore weaklysupervised learning for the task, which only requires sparse labeling on the presence or absence of video events. The weak labels are easier to annotate and can be gathered in a large scale from web videos. We formulate the weaklysupervised audiovisual video parsing as a Multi modal Multiple Instance Learning (MMIL) problem and propose a new framework to solve it. Concretely, we use a new hybrid attention network (HAN) for lever aging unimodal and crossmodal temporal contexts simultaneously. We developWeaklySupervised AudioVisual Video Parsing 3 an attentive MMIL pooling method for adaptively aggregating useful audio and visual content from dierent temporal extent and modalities. Furthermore, we discover modality bias and noisy label issues and alleviate them with an individualguided learning mechanism and label smoothing [42], respectively. To facilitate our investigations, we collect a Look, listen, and Parse (LLP) dataset that has 11 ;849 YouTube video clips from 25 event categories. We label them with sparse videolevel event labels for training. For evaluation, we label a set of precise labels, including event modalities, event categories, and their temporal boundaries. Experimental results show that it is tractable to learn audio visual video parsing even with videolevel weak labels. Our proposed HAN model can eectively leverage multimodal temporal contexts. Furthermore, modality bias and noisy label problems can be addressed with the proposed individual learning strategy and label smoothing, respectively. Besides, we make a discussion on the potential applications enabled by audiovisual video parsing. The contributions of our work include: (1) a new audiovisual video parsing task towards a unied multisensory perception; (2) a novel hybrid attention network to leverage unimodal and crossmodal temporal contexts simultaneously; (3) an eective attentive MMIL pooling to aggregate multimodal information adaptively; (4) a new individual guided learning approach to mitigate the modality bias in the MMIL problem and label smoothing to alleviate noisy labels; and (5) a newly collected largescale video dataset, named LLP, for audiovisual video parsing. Dataset, code, and pretrained models are publicly available in https://github.com/YapengTian/AVVPECCV20 . 2 Related Work "
335,Transfer Learning for Named-Entity Recognition with Neural Networks.txt,"Recent approaches based on artificial neural networks (ANNs) have shown
promising results for named-entity recognition (NER). In order to achieve high
performances, ANNs need to be trained on a large labeled dataset. However,
labels might be difficult to obtain for the dataset on which the user wants to
perform NER: label scarcity is particularly pronounced for patient note
de-identification, which is an instance of NER. In this work, we analyze to
what extent transfer learning may address this issue. In particular, we
demonstrate that transferring an ANN model trained on a large labeled dataset
to another dataset with a limited number of labels improves upon the
state-of-the-art results on two different datasets for patient note
de-identification.","Electronic health records (EHRs) have been widely adopted in some countries such as the United States and represent gold mines of infor mation for medical research. The majority of EHR data exist in unstructured form such as patient notes (Murdoch and Detsky, 2013). Applying nat ural language processing on patient notes can im prove the phenotyping of patients (Ananthakrish nan et al., 2013; Pivovarov and Elhadad, 2015; Halpern et al., 2016), which has many down stream applications such as the understanding of diseases (Liao et al., 2015). However, before patient notes can be shared with medical investigators, some types of infor mation, referred to as protected health informa tion (PHI), must be removed in order to preserve These authors contributed equally to this work.patient conÔ¨Ådentiality. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) (OfÔ¨Åce for Civil Rights, 2002) de Ô¨Ånes 18 different types of PHI, ranging from pa tient names and ID numbers to addresses and phone numbers. The task of removing PHI from a patient note is referred to as deidentiÔ¨Åcation . The essence of deidentiÔ¨Åcation is recognizing PHI in patient notes, which is a form of namedentity recognition (NER). Existing deidentiÔ¨Åcation systems are often rulebased approaches or featurebased machine learning approaches. However, these techniques require additional lead time for developing and Ô¨Ånetuning the rules or features speciÔ¨Åc to each new dataset. Meanwhile, recent work using ANNs have yielded stateoftheart performances with out using any manual features (Dernoncourt et al., 2016). Compared to the previous systems, ANNs have a competitive advantage that the model can be Ô¨Ånetuned on a new dataset without the over head of manual feature development, as long as some labels for the dataset are available. However, it may still be inefÔ¨Åcient to mass de ploy ANNbased deidentiÔ¨Åcation system in prac tical settings, since creating annotations for pa tient notes is especially difÔ¨Åcult. This is due to the fact that only a restricted set of individuals is authorized to access original patient notes; the annotation task cannot be crowdsourced, mak ing it slow and expensive to obtain a large anno tated corpus. Medical professionals are therefore wary to explore patient notes because of this de identiÔ¨Åcation barrier, which considerably hampers medical research. In this paper, we analyze to what extent trans fer learning may improve deidentiÔ¨Åcation perfor mances on datasets with a limited number of la bels. By training an ANN model on a large dataset (MIMIC) and transferring it to smaller datasetsarXiv:1705.06273v1  [cs.CL]  17 May 2017(i2b2 2014 and i2b2 2016), we demonstrate that transfer learning allows to outperform the stateof theart results. 2 Related Work "
365,NoisyActions2M: A Multimedia Dataset for Video Understanding from Noisy Labels.txt,"Deep learning has shown remarkable progress in a wide range of problems.
However, efficient training of such models requires large-scale datasets, and
getting annotations for such datasets can be challenging and costly. In this
work, we explore the use of user-generated freely available labels from web
videos for video understanding. We create a benchmark dataset consisting of
around 2 million videos with associated user-generated annotations and other
meta information. We utilize the collected dataset for action classification
and demonstrate its usefulness with existing small-scale annotated datasets,
UCF101 and HMDB51. We study different loss functions and two pretraining
strategies, simple and self-supervised learning. We also show how a network
pretrained on the proposed dataset can help against video corruption and label
noise in downstream datasets. We present this as a benchmark dataset in noisy
learning for video understanding. The dataset, code, and trained models will be
publicly available for future research.","The ImageNet dataset [ 5] has been one of the catalysts behind the exponential growth in Deep Learning [ 19] and large scale machine learning research, along with transfer learning adoption to adapt large trained networks on problems with little data. This has led to many largescale datasets targeting various tasks such as classifica tion, detection, segmentation, etc., and a rising interest in training bigger networks to capture more variations and transfer well. Ima geNet [ 5], and Youtube8M [ 1] are enormous datasets in terms of size and annotations. Still, it is not always possible to construct such massive annotated datasets due to logistical and time constraints. Collecting data from the web is getting much popularity due to its availability on several social media platforms (e.g., Webvision [23], and Clothing1M [ 43]). Along with these datasets, many other works [ 7] [4] have shown how learning from web data dramatically increases performance in related domains, despite labels which are mostly inferred from surrounding meta data and not manually verified. Moreover, the meta data itself acts as a rich source of information about the data point for tasks like image captioning, video understanding, etc. As an active research area, there is a dire need to set a standard benchmark for efficient learning from noisy web data. With this objective in mind, we construct such a dataset with a primary focus on video modality. Our dataset consists of raw videos collected from Flickr, with surrounding meta data such as title, description, comments, etc. It has been collected using class labels from popu lar video classification benchmarks as search queries since these datasets have already established useful labels based on various criteria. We first look at various statistics of our dataset to set its importance and show some preliminary results for video action classification. A major challenge that one encounters while learning from web collected data is its heavily imbalanced multilabel nature. Similar works [ 9] randomly select one label from the list of given multi labels. We compare various multilabel learning strategies in literature while pretraining on our dataset and also look at the setting of simple pretraining or combining it with selfsupervised learning at various stages, hoping that this will set a benchmark for the research community. Finally, we also obtain some surprisingarXiv:2110.06827v1  [cs.MM]  13 Oct 2021Gold Coast ‚Äô21, December 01‚Äì03, 2021, Gold Coast, Australia Mohit Sharma, Raj Patra, Harshal Desai, Shruti Vyas, Yogesh Rawat, and Rajiv Ratn Shah results around how models pretrained on the proposed noisy dataset provide some robustness against label noise and video corruption with just simple finetuning and no modification to the training pipeline. We first talk about related work in Section 2. Next, in Section 3, we discuss our dataset construction and statistics. We describe our methodology in Section 4and our experimental setup in Section 5. We finally present our results and a discussion around them in Section 6. We end with Section 7, discussing how this work can be further improved and new research directions from our proposed dataset. 2 RELATED WORK "
476,Deep CNNs for Peripheral Blood Cell Classification.txt,"The application of machine learning techniques to the medical domain is
especially challenging due to the required level of precision and the
incurrence of huge risks of minute errors. Employing these techniques to a more
complex subdomain of hematological diagnosis seems quite promising, with
automatic identification of blood cell types, which can help in detection of
hematologic disorders. In this paper, we benchmark 27 popular deep
convolutional neural network architectures on the microscopic peripheral blood
cell images dataset. The dataset is publicly available, with large number of
normal peripheral blood cells acquired using the CellaVision DM96 analyzer and
identified by expert pathologists into eight different cell types. We fine-tune
the state-of-the-art image classification models pre-trained on the ImageNet
dataset for blood cell classification. We exploit data augmentation techniques
during training to avoid overfitting and achieve generalization. An ensemble of
the top performing models obtains significant improvements over past published
works, achieving the state-of-the-art results with a classification accuracy of
99.51%. Our work provides empirical baselines and benchmarks on standard
deep-learning architectures for microscopic peripheral blood cell recognition
task.","Blood carries oxygen and nutrients to living cells in dierent organs and tissues. It carries away the waste for detoxication. It transports hormones to the desired site of action to ght infections and regulates body temperature. The ability to classify blood constituents can be critical in assessing the patient's health. Plasma, which constitutes 55% of blood, is a colored liquid comprising mainly water (about 90%) and other essential substances such as proteins (albumin, clotting factors, antibodies, enzymes, and hormones), glucose, and fats. Rest 45% of blood is composed of white blood cells (WBCs/leukocytes), platelets (thrombocytes), and red blood cells (RBCs/erythrocytes), which  oat in the plasma(Fathima and Syeda, 2017; L., 2005). All these cells are associated with dierent functionalities. RBCs are responsible for transporting gases ( O2; CO 2) from lungs to tissues and maintaining systemic acid/base equilibria. Damage of red cell integrity, dened as hemolysis, has been shown to signicantly contribute to severe pathologies, including endothelial dysfunction(Kuhn et al., 2017). Based on the presence of visible granules in the microscopic view, WBCs can be clas sied into two broad categories: granulocytes and agranulocytes (nongranulocytes). Neu trophils, eosinophils, and basophils belong to the granulocytes category, while lymphocytes ‚àóEqual Contribution ¬©2021 E. Gavas & K. Olpadkar.arXiv:2110.09508v1  [cs.CV]  18 Oct 2021Gavas Olpadkar and monocytes belong to the agranulocytes category (Almezhghwi and Serte, 2020; Acevedo et al., 2019). Various types of WBCs play a role in immune response (L., 2005) and act as a defense mechanism in the body against illnesscausing agents. Immature granulocytes (IG) are underdeveloped WBCs released from the bone marrow into the blood. Except for blood from newborn children or pregnant women, the appearance of IG (promyelocytes, myelocytes, and metamyelocytes) (Acevedo et al., 2019) in the peripheral blood (PB) in dicates an earlystage response to infection, in ammation, or other stimuli of the bone marrow. Similarly, erythroblasts are nucleated immature RBCs or erythroid precursors not seen after the neonatal period. Their appearance in PB of children and adults can signify bone marrow damage, stress, malignant neoplasms, or other potentially serious diseases (Constantino and Cogionis, 2000). Platelets are anucleated cells in blood and get activated at the site of injury to form a blood clot. Besides, they play an important role in innate immunity and regulation of tumor growth and extravasations in the vessel(Holinstat, 2017). They make up less than 1% of blood volume. Usually, the typical percentages of neutrophils in the blood are 06%. Eosinophils constitute 1{3%, basophils 0{1%, lymphocytes 25{33%, and monocytes 3{10% of the leukocytes circulating in the blood(Acevedo et al., 2019). Recognition of various blood cell types can reveal anomalous blood cell populations like immature cells (IG or erythroblasts). On accurate identication, dierential blood cell count can suggest any possible abnormalities in the blood, or help diagnose an infection, in ammation, leukemia(Shaque and Tehsin, 2018; Mathur et al., 2013), or any immune system disorder. Analyzing the blood cell morphology is the outset for the diagnosis of 80% of hematological diseases(Acevedo et al., 2019). Quantitative morphological analysis can thus, help cytologists assess blood samples and conclude about the patient's blood conditions. However, the above processes are complex and timeconsuming, involving a specialist meticulously examining the blood smear under a microscope, subjecting it to human errors. For the past few years, several attempts have been made to automate these processes using image processing techniques and machine learning, making them time and costeective and substantially reducing the workload in laboratories. Convolutional neural networks (CNNs) are known to show excellent results on image recognition tasks, and hence, there has been an extensive research for applying them in the medical domain. In this paper, we explore various deep CNNs for blood cell classication task with peripheral blood cell (PBC) images dataset containing samples of eight dierent cell types: neutrophils, eosinophils, basophils, lymphocytes, monocytes, IG, erythroblasts, and platelets (Acevedo et al., 2020). Our work provides stateoftheart results on the PBC classication task without the need of manual feature extraction or designing complex and hybrid architectures. The main contributions of this paper are as follows: 1. Train and evaluate an endtoend deep learningbased classication system to recog nize eight dierent blood cell types in peripheral blood smear. 2. Explore and benchmark 27 standard deep CNN architectures for blood cell classica tion using transfer learning. 3. Exploit data augmentation and ensembling techniques to further improve the model performance. Our model achieves stateoftheart performance over previously pub lished works. 2Deep CNNs for Peripheral Blood Cell Classification 2. Related Work "
499,Ensemble Feature for Person Re-Identification.txt,"In person re-identification (re-ID), the key task is feature representation,
which is used to compute distance or similarity in prediction. Person re-ID
achieves great improvement when deep learning methods are introduced to tackle
this problem. The features extracted by convolutional neural networks (CNN) are
more effective and discriminative than the hand-crafted features. However, deep
feature extracted by a single CNN network is not robust enough in testing
stage. To improve the ability of feature representation, we propose a new
ensemble network (EnsembleNet) by dividing a single network into multiple
end-to-end branches. The ensemble feature is obtained by concatenating each of
the branch features to represent a person. EnsembleNet is designed based on
ResNet-50 and its backbone shares most of the parameters for saving computation
and memory cost. Experimental results show that our EnsembleNet achieves the
state-of-the-art performance on the public Market1501, DukeMTMC-reID and CUHK03
person re-ID benchmarks.","Person reidenti/f_ication (reID) is an important task in computer vision and a/t_tracts lots of a/t_tention for its application in intelligent video surveillance. It aims to match pedestrians across diÔ¨Äerent cameras. Due to the large variations in person appearance, pose, occlusion, illumination and so on, it is a very challenging problem. Fortunately, deep learning techniques have improved the perfor mance eÔ¨Äectively. However, there is a big generalization gap [ 1] between training and testing. /T_he main reason is that person reID Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. XXXX 2019, XXXX, XXXX ¬©2019 ACM. XXXXXXXXXXXXX/XX/XX... $15.00 DOI: XX.XXXX/XXXXXXX.XXXXXXXproblem is an uncloseset matching problem [ 2], where the testing identities are diÔ¨Äerent from the training ones. As we all known, classi/f_ication problem in testing is to predict the label of the sample, which still belongs to the training labels. DiÔ¨Äerent from classi/f_i cation, in person reID, the supervised labels of query person and gallery person both are not in the training set. As a result, it is diÔ¨Écult to learn eÔ¨Äective features for person reID. In a single classical CNN network, only one feature vector can be extracted for person reID, and it may have a limited feature rep resentation ability. So it is possible to fuse multiple feature vectors to promote the representation ability and reduce the generaliza tion gap. In practical, a simplest method is to just use multiple independent networks and concatenate their features to promote the performance. It‚Äôs naturally an ensemble idea. To verify this simplest idea, we train 8 independent ResNet50 networks on Mar ket1501 dataset, and concatenate their features for person reID. /T_he results are showed in Figure 1, where we can /f_ind that the mAP and Rank1 of 8 independent networks are similar and the average of them are showed in red dash line, while the ensemble features can easily achieve be/t_ter results as the number of ensemble net works increases. However, it is inconvenient to manage multiple independent networks for deployment and the timeconsuming increases linearly with the number of the networks. In this paper, we explore to propose a new ensemble model with an endtoend network with be/t_ter generalization ability. /T_he basic idea is create multiple branches to form multiple objectives. Each objective can be optimized to produce a solution for learning feature. Inspired by the partbased models [ 3], we use diÔ¨Äerent partbased model in each branch to make the features complementary. Finally, we evaluate the model and present a possible explanation. /T_he contributions of this work are as follows: An ensemble network (EnsembleNet) is proposed to learn the feature representation for person reID. It‚Äôs based on the ResNet50 and consists of multiple branches. /T_he features extracted from each branch are concatenated to form a feature for each image. It‚Äôs an endtoend architecture and has fewer parameters and computation than fusing multiple independent networks. To evaluate EnsembleNet, in experiments, we explore the eÔ¨Äect of stride size, branch numbers and adaptive average pooling. /T_he special se/t_ting can promote the performance eÔ¨Äectively. Experimental results show that our approach achieves the stateoftheart performance on the public Market1501, DukeMTMCreID and CUHK03 person reID benchmarks. To explain the eÔ¨Äectiveness of ensemble feature, inspiring by a visualization method of twodimensional loss land scape [ 4], we present the landscape of testing performancearXiv:1901.05798v1  [cs.CV]  17 Jan 2019XXXX 2019, XXX XX‚ÄìXX, 2019, XXXX, XXXX J. Wang et al. 1 2 3 4 5 6 7 8767880828486mAP (%) Average Ensemble Independent 1 2 3 4 5 6 7 8899091929394Rank1 (%) Average Ensemble Independent Figure 1: Ensemble multiple networks. /T_he networks are based on ResNet50 backbone, we add a 1 1 convolution to reduce the number of channels from 2048dims to 256dims, following so/f_tmax logloss for classi/f_ication. Note that the mAP and Rank1 of independent networks are presented in histogram, while the mAP and Rank1 of ensemble networks are showed in curve. with the ‚Äú/f_ilter normalization‚Äù. /T_he landscapes show that EnsembleNet has /f_latness of testing performance. 2 RELATED WORKS "
322,Pick up the PACE: Fast and Simple Domain Adaptation via Ensemble Pseudo-Labeling.txt,"Domain Adaptation (DA) has received widespread attention from deep learning
researchers in recent years because of its potential to improve test accuracy
with out-of-distribution labeled data. Most state-of-the-art DA algorithms
require an extensive amount of hyperparameter tuning and are computationally
intensive due to the large batch sizes required. In this work, we propose a
fast and simple DA method consisting of three stages: (1) domain alignment by
covariance matching, (2) pseudo-labeling, and (3) ensembling. We call this
method $\textbf{PACE}$, for $\textbf{P}$seudo-labels, $\textbf{A}$lignment of
$\textbf{C}$ovariances, and $\textbf{E}$nsembles. PACE is trained on top of
fixed features extracted from an ensemble of modern pretrained backbones. PACE
exceeds previous state-of-the-art by $\textbf{5 - 10 \%}$ on most benchmark
adaptation tasks without training a neural network. PACE reduces training time
and hyperparameter tuning time by $82\%$ and $97\%$, respectively, when
compared to state-of-the-art DA methods. Code is released here:
https://github.com/Chris210634/PACE-Domain-Adaptation","Deep learning is infamous for requiring a large amount of labeled data to achieve stateoftheart results, but in many applications, labeled data is expensive to obtain [ 1,2,3]. In the last few years, new subdisciplines of deep learning have emerged to address this overreliance on labeled data. Selfsupervised learning learns useful representations of data through handcrafted augmentations instead of labels [ 4,5].Semisupervised learning learns from a small amount of labeled data and a large amount of unlabeled data from the same distribution [ 6].Domain Adaptation (DA) is similar to semisupervised learning, but considers the case where the labeled and unlabeled data come from different distributions [ 7,8]. We call the labeled data the source domain and the unlabeled data the target domain. For example, consider the following scenario. A company wants to train a speech recognition system for deployment in a noisy car environment (target domain). A large amount of labeled recordings is available for indoor environments (source domain), but a limited amount of labeled target data is available. DA addresses this problem by minimizing the empirical risk on source data while encouraging features to be domain invariant. We consider both Semisupervised Domain Adaptation (SSDA) [9], where a small amount of labeled target data is available, and Unsupervised Domain Adaptation (UDA) [10], where no labeled target data is available. Many modern DA methods train a feature extractor and linear classiÔ¨Åer to minimize a weighted sum of three losses: (1) cross entropy loss on labeled source data [ 10,9,11,12], (2) divergence loss between source and target domain features [ 10,9,11,13] and (3) consistency loss between augmented views of unlabeled target data [ 14,15,16]. Generally, better performing methods require large batch sizes and a large number of hyperparameters [ 17,15,12,16,14]. For example, CDAC [15], a stateoftheart method in SSDA, requires a batch of labeled data, a batch of unlabeled data, and two augmented batches of unlabeled data. On a small feature extractor, such as ResNet34, Preprint. Under review.arXiv:2205.13508v1  [cs.LG]  26 May 2022Figure 1: Illustration of the PACE framework. A solid arrow indicates a forward pass. A dashed arrow indicates a backward pass. PACE consists of three steps: (1) the source features Xsare transformed such that their covariances match the target feature Xtucovariances, (2) a linear classiÔ¨Åer is trained with logistic loss on source data and pseudolabeled target data, and (3) the predictions from ensemble members are averaged to return an aggregate prediction. tuning and running this method is feasible. However, scaling this type of method to large modern feature extractors would require several GPUs working in parallel [ 18,19]. Computationally efÔ¨Åcient training is important for two reasons: (1) there is signiÔ¨Åcant interest in energyefÔ¨Åcient training on resourceconstrained edge devices [ 20,21], and (2) algorithms which achieve stateoftheart with low computational cost foster democratic research [ 22]. This raises the question: is it possible to leverage high quality features from large modern backbones, using only one GPU, and still achieve competitive DA results? The answer is yes! Present work: In this paper, we propose an efÔ¨Åcient DA method PACE (Pseudolabels, Alignment ofCovariances, and Ensembles), which uses predeep learning DA methods on top of Ô¨Åxed features extracted from an ensemble of ConvNeXt [ 19] and Swin [ 18] backbones pretrained on ImageNet [23]. Our method consists of three stages (see Figure 1): (1) We align the source and target feature distributions by matching their covariances with CORAL [ 24]. (2) We use selftraining with conÔ¨Ådence thresholding to align the classconditional feature distributions between source and target domains. (3) We run the Ô¨Årst two steps with different pretrained backbones and average the predictions. We hypothesize that the features extracted by the collection of pretrained backbones are not perfectly correlated because of differences in pretraining and architecture. Therefore, we should observe a boost in accuracy when combining predictions from the ensemble. Our contributions are: ‚Ä¢We revisit CORAL [ 24], a simple method for domain alignment that is not widely used by stateoftheart DA methods. We show that when combined with selftraining and ensembling, CORAL offers a boost in target accuracy without adding much complexity. ‚Ä¢We propose PACE, which reduces training time and hyperparameter tuning time by 82% and 97%, respectively (see Figure 2) while exceeding stateoftheart SSDA and UDA methods by 510% on most benchmark tasks (see Tables 2, 3 and 4). 2 Related Work "
596,Towards Label-free Scene Understanding by Vision Foundation Models.txt,"Vision foundation models such as Contrastive Vision-Language Pre-training
(CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot
performance on image classification and segmentation tasks. However, the
incorporation of CLIP and SAM for label-free scene understanding has yet to be
explored. In this paper, we investigate the potential of vision foundation
models in enabling networks to comprehend 2D and 3D worlds without labelled
data. The primary challenge lies in effectively supervising networks under
extremely noisy pseudo labels, which are generated by CLIP and further
exacerbated during the propagation from the 2D to the 3D domain. To tackle
these challenges, we propose a novel Cross-modality Noisy Supervision (CNS)
method that leverages the strengths of CLIP and SAM to supervise 2D and 3D
networks simultaneously. In particular, we introduce a prediction consistency
regularization to co-train 2D and 3D networks, then further impose the
networks' latent space consistency using the SAM's robust feature
representation. Experiments conducted on diverse indoor and outdoor datasets
demonstrate the superior performance of our method in understanding 2D and 3D
open environments. Our 2D and 3D network achieves label-free semantic
segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%,
respectively. And for nuScenes dataset, our performance is 26.8% with an
improvement of 6%. Code will be released
(https://github.com/runnanchen/Label-Free-Scene-Understanding).","Scene understanding aims to recognize the semantic information of objects within their contextual environment, which is a fundamental task for autonomous driving, robot navigation, digital city, etc. Existing methods have achieved remarkable advancements in 2D and 3D scene understanding [ 1‚Äì 10]. However, they heavily rely on extensive annotation efforts and often struggle to identify novel object categories that were not present in the training data. These limitations hinder their practical applicability in realworld scenarios where acquiring highquality labelled data can be expensive and novel objects may appear [ 11‚Äì17]. Consequently, labelfree scene understanding, which aims to perform semantic segmentation in realworld environments without requiring labelled data, emerges as a highly valuable yet relatively unexplored research topic. Vision foundation models, e.g., Contrastive VisionLanguage Pretraining (CLIP) [ 18] and Segment Anything (SAM) [ 19], have garnered significant attention due to their remarkable performance in addressing openworld vision tasks. CLIP, trained on a largescale collection of freely available image text pairs from websites, exhibits promising capabilities in openvocabulary image classification. On the other hand, SAM learns from an extensive dataset comprising 1 billion masks across 11 million 1https://github.com/runnanchen/LabelFreeSceneUnderstanding . Preprint. Under review.arXiv:2306.03899v1  [cs.CV]  6 Jun 2023CLIPSAM Noisy pseudolabels Clean object masksImagesLabelfree 3D scene understandingLabelfree 2D scene understandingTransfer  to 3D3D2D 3D ground truth2D ground truthFigure 1: We study how vision foundation models enable networks to comprehend 2D and 3D environments without relying on labelled data. To accomplish this, we introduce a novel framework called Crossmodality Noisy Supervision (CNS). By effectively harnessing the strengths of CLIP and SAM, our approach simultaneously trains 2D and 3D networks, yielding remarkable performance. images, achieving impressive zeroshot image segmentation performance. However, initially designed for image classification, CLIP falls short in segmentation performance. Conversely, SAM excels in zeroshot image segmentation but lacks object semantics (Fig. 1). Additionally, both models are trained exclusively on 2D images without exposure to any 3D modal data. Given these considerations, a natural question arises: Can the combination of CLIP and SAM imbue both 2D and 3D networks with the ability to achieve labelfree scene understanding in realworld open environments? Despite recent efforts [ 20] leverage CLIP for imagebased semantic segmentation, the pseudo labels generated by CLIP for individual pixels often exhibit significant noise, resulting in unsatisfactory performance. The uptodate work [ 17] has extended this method to encompass 3D labelfree scene understanding by transferring 2D knowledge to 3D space via projection. However, the pseudo labels assigned to 3D points are considerably noisier due to calibration errors, significantly limiting the accuracy of the networks. The primary challenge of utilizing vision foundation models for scene understanding is effectively supervising the networks using exceptionally noisy pseudo labels. Inspired by SAM‚Äôs impressive zeroshot segmentation capabilities, we propose a novel Crossmodality Noisy Supervision (CNS) framework incorporating CLIP and SAM to train 2D and 3D networks simultaneously. Specifically, we employ CLIP to densely pseudolabel 2D image pixels and transfer these labels to 3D points using the calibration matrix. Since pseudolabels are extremely noisy, leading to unsatisfactory performance, we refine pseudolabels using SAM‚Äôs masks, producing more reliable pseudolabels for supervision. To further mitigate error propagation and prevent the networks from overfitting the noisy labels, we consistently regularize the network predictions, i.e., cotraining the 2D and 3D networks using the randomly switched pseudo labels, where the labels are from the prediction of 2D, 3D, and CLIP networks. Moreover, considering that individual objects are well distinguished in SAM feature space, we use the robust SAM feature to consistently regularize the latent space of 2D and 3D networks, which aids networks in producing semantic predictions with more precise boundaries and further reduces label noise. Note that the SAM feature space is frozen during training, thus severed as the anchor metric space for aligning the 2D and 3D features. To verify the labelfree scene understanding capability of our method in 2D and 3D real open worlds, we conduct experiments on indoor and outdoor datasets, i.e., ScanNet and nuScenes, where 2D and 3D data are simultaneously collected. Extensive results show that our method significantly outperforms stateoftheart methods in understanding 2D and 3D scenes without training on any labelled data. Our 2D and 3D network achieves labelfree semantic segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%, respectively. And for the nuScenes dataset, the performance on 3D semantic segmentation is 26.8% mIoU with the improvement of 6%. Quantitative and qualitative ablation studies also demonstrate the effectiveness of each module in our method. The key contributions of our work are summarized as follows. ‚Ä¢We present the first work that incorporating CLIP and SAM for labelfree scene understand ing in 2D and 3D realworld open environments. 2‚Ä¢We propose a novel Crossmodality Noisy Supervision framework to effectively and syn chronously train 2D and 3D networks with severe label noise, including prediction consis tency and latent space consistency regularization schemes. ‚Ä¢Experiments conducted on indoor and outdoor datasets show that our method significantly outperforms stateoftheart methods on 2D and 3D semantic segmentation tasks. 2 Related Work "
290,Exploring Uncertainty in Deep Learning for Construction of Prediction Intervals.txt,"Deep learning has achieved impressive performance on many tasks in recent
years. However, it has been found that it is still not enough for deep neural
networks to provide only point estimates. For high-risk tasks, we need to
assess the reliability of the model predictions. This requires us to quantify
the uncertainty of model prediction and construct prediction intervals. In this
paper, We explore the uncertainty in deep learning to construct the prediction
intervals. In general, We comprehensively consider two categories of
uncertainties: aleatory uncertainty and epistemic uncertainty. We design a
special loss function, which enables us to learn uncertainty without
uncertainty label. We only need to supervise the learning of regression task.
We learn the aleatory uncertainty implicitly from the loss function. And that
epistemic uncertainty is accounted for in ensembled form. Our method correlates
the construction of prediction intervals with the uncertainty estimation.
Impressive results on some publicly available datasets show that the
performance of our method is competitive with other state-of-the-art methods.","WITH the rapid development of artiÔ¨Åcial intelligence, deep learning has attracted the interest of many researchers [1]. As a representative tool of deep learning, deep neural networks has achieved impressive performance in many tasks. At present, deep neural networks have important applications in computer vision [2], speech recog nition [3], natural language processing [4], bioinformatics [5] and other Ô¨Åelds. Although deep neural networks have achieved high accuracy for many tasks, they still perform poorly in quantifying the uncertainty of predictions and tend to be overconÔ¨Ådent. For many realworld applications, it is not enough for a model to be accurate in its pre dictions. It must also be able to quantify the uncertainty of each prediction. This is especially importance in tasks where wrong prediction has a great negative impact, such as: machine learning auxiliary medical diagnosis [6], au tomatic driving [7], Ô¨Åance and energy system, etc. These highrisk applications require not only point prediction, but also the precise quantiÔ¨Åcation of the uncertainty. So it can be said that in many application scenarios, uncertainty is very important, more important than precision. Moreover, uncertainty plays an important role in determining when to abandon the prediction of the model. Abandoning the inaccurate predictions of the model can handle exceptions and hand over highrisk decisions to humans [8]. As more and more autonomous systems based on deep learning are deployed in our real life that may cause physical or economic harm, we need to better understand when we can have conÔ¨Ådence in the prediction of deep neural networks and when we should not be so sure. The development of neural networks precise prediction intervals (PIs) [9] is a challenging work to explore the Y. Lai, Y. Shi and Y. Han are with College of Intelligence and Com puting, Tianjin University, Tianjin, China (email: fyuandulai, yucheng, yahong g@tju.edu.cn). Y. Shao, M. Qi and B. Li are with Huawei Noah‚Äôs Ark Lab, Huawei Technologies (email: fshaoyunfeng, qimeiyu, libingshuai g@huawei.com).uncertainty of prediction, which has just aroused the interest of researchers. PIs directly conveys uncertainty, providing a lower bound and an upper bound for prediction [10], but traditional deep neural networks can only provide a point estimation. PIs is of great use in practical applica tions, and can help people make better decisions. However, although there are some researches on uncertainty estima tion [11] [12] [13] [14] [15], there are few researches on PIs. In this paper, We will explore the uncertainty in deep learning to construct the prediction intervals. Uncertainty comes from different sources in vari ous forms, and most literature considers two sources of uncertainty: aleatory uncertainty and epistemic uncer tainty [16] [17] [18] [19] [20] [21]. Aleatory uncertainty describes the irreducible inherent noise of the observed data. This is due to the complexity of the data itself, which can be sensor noise, label noise, class overlap, etc. Aleatory uncertainty is also caused by hidden variables that are not observed or errors in the measurement process, and this type of uncertainty cannot be reduced by collecting more data. Aleatory uncertainty is also known in some literature as data uncertainty, random uncertainty [22], stochastic un certainty [23]. Epistemic uncertainty describes the model er rors due to lack of experience in some regions of the feature space [24]. In some feature spaces, if the training samples do not cover them, the uncertainty will increase in those places. Therefore, the epistemic uncertainty is inversely pro portional to the density of the training samples, which can be reduced by collecting more data in the lowdensity areas. In regions with highdensity training samples, epistemic uncertainty will decrease, and aleatory uncertainty caused by data noise will play a major role. Epistemic uncertainty is also known as model uncertainty [19], because it is caused by the limitations of the model. By analyzing the sources and categories of uncertainty, we can deal with and estimate each kind of uncertainty well in practice.arXiv:2104.12953v1  [cs.LG]  27 Apr 2021MANUSCRIPT SUBMITTED TO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 2 The precise prediction intervals of neural network is a difÔ¨Åcult task to solve, and its development is closely related to uncertainty estimation. The prediction interval directly reÔ¨Çects the uncertainty. Many literatures have proposed that the loss function can be modiÔ¨Åed to implicit learning prediction intervals [9] [10] [12] [25] [26]. These methods have obvious disadvantages, such as loss function can not be optimized by gradient descentbased algorithm, which is a modern machine learning technology. Or the accuracy of PIs is low. Although there are many conÔ¨Ådence interval construction methods with good performance in statistical literature, these methods can not be effectively combined with deep learning. In this paper, we propose a prediction interval construc tion method from the perspective of statistics which can be used effectively only by slight modiÔ¨Åcation of deep neural networks. We modify the last layer of the general deep neural network so that the network no longer outputs only a point estimation, but the upper bound and lower bound of the interval. And a loss function that can be used to train such a network is derived from the perspective of proba bility theory. Because most modern deep neural networks can use our loss function with only a few modiÔ¨Åcations, our method will have more practical use than other methods. The contributions of this work are summarized as fol lows: i) In this paper, the construction of optimal pre diction intervals is associated with uncertainty estimation. We design a novel loss function based on the theory of statistics. We think our work bridges the gap between the construction of prediction intervals and the uncertainty esti mation; ii) Unlike bayesian deep learning, our loss function can be seamlessly integrated with current deep learning frameworks such as TensorFlow andPyTorch . Existing deep learning networks can use our loss function with only minor modiÔ¨Åcations. And it can be optimized efÔ¨Åciently using standard optimization techniques. We believe that our method promotes the development of nonbayesian method to construct prediction interval. iii) Experimental results show that this method can construct compact opti mal prediction intervals. It can sensitively capture the epis temic uncertainty. In the lowdensity data distribution area, the uncertainty increases obviously. Experimental results on some publicly available regression datasets show the effectiveness of the method. The rest of the paper is organized as follows. In the section 2, we summarize the related work of uncertainty estimation and prediction interval. And we introduce the method of constructing prediction interval based on explor ing uncertainty in section 3. In the section 4, we organize some convincing experiments to show the effectiveness of our method. Finally, in the section 5, we make a brief summary of this paper. 2 RELATED WORK "
156,Semantic Identification Attacks on Web Browsing.txt,"We introduce a Semantic Identification Attack, in which an adversary uses
semantic signals about the pages visited in one browsing session to identify
other browsing sessions launched by the same user. This attack allows an adver-
sary to determine if two browsing sessions originate from the same user
regardless of any measures taken by the user to disguise their browser or
network. We use the MSNBC Anonymous Browsing data set, which contains a large
set of user visits (labeled by category) to implement such an attack and show
that even very coarse semantic information is enough to identify users. We
discuss potential counter- measures users can take to defend against this
attack.","Online privacy is becoming an increasingly important issue for users, policy makers, and academic researchers. In re cent years there has been signicant work highlighting the fragility of user privacy by exposing threats ranging from the deanonymization of public data sets to third party tracking in online browsing. In particular, an area of focus has been the ability for ad vertisers to identify users across browsing sessions. Many privacy experts advocate that users should regularly clear browsing cookies to prevent third party trackers from build ing detailed proles and linking their webpage visits. Other experts recommend private browsing modes (such as incog nito mode on Google Chrome) to prevent the browser from storing any persistent browsing data and to ensure that web cookies are never reused. There are even more powerful tools, such as Tor or Brave, a new open source browser fo cused explicitly on blocking advertisements and third party trackers. Unfortunately, these measures are not adequate. In this pa per, we introduce a semantic identication attack , a new attack that can identify users across multiple browsing ses sions. In this attack, an adversary can, in some cases, leverage semantic signals  features derived from the content viewed by a user (words on pages viewed, page titles, links in/out of page, etc)  to identify when two browsing sessions have been launched by the same user. If one of the sessions contains personally identifying information, the adversary now has a way of linking this information to other sessions launched by the user. A user is unlikely to visit the same urls in every session. However, the urls in a given user's browsing sessions are un likely to be drawn completely at random from all the urls on the web. If a user's web browsing is characterized by a set of tasks (checking certain news topics, reading specic blogs, etc), their browsing sessions will consistently contain visits to pages/websites relevant to these tasks. The distri bution of these visits can be viewed as a user specic session ngerprint. Though a user's browsing sessions are likely to deviate and contain other behavior as well, the presence of these pages ngerprints the user to the session. The ability to extract a browsing ngerprint and use it to link together a user's sessions has signicant implications for online privacy. An adversary that could identify the browsing ngerprint for a user could determine with high probability when a certain browsing session belongs to that user even when they use multiple devices, regularly clear cookies or obfuscate their identity through proxies. While there has been prior work in identifying users across browsing sessions, nearly all methods have relied on the browser or network metadata for identifying users. Network signals and browser metadata signatures can be spoofed or disguised through extensions and proxies. However, seman tic data about browsing sessions will still be available to websites and even to third parties such as advertisers. We demonstrate that this kind of data can be used to identify and track users. In this paper, we rst review past work and highlight the dierences between prior approaches and ours. We then ex plain the intuition behind the semantic identication attack and present several dierent strategies. Finally, we evaluate the attacks on a collection of user browsing data and discuss the countermeasures. 2. RELATED WORK "
442,Improved Regularization and Robustness for Fine-tuning in Neural Networks.txt,"A widely used algorithm for transfer learning is fine-tuning, where a
pre-trained model is fine-tuned on a target task with a small amount of labeled
data. When the capacity of the pre-trained model is much larger than the size
of the target data set, fine-tuning is prone to overfitting and ""memorizing""
the training labels. Hence, an important question is to regularize fine-tuning
and ensure its robustness to noise. To address this question, we begin by
analyzing the generalization properties of fine-tuning. We present a PAC-Bayes
generalization bound that depends on the distance traveled in each layer during
fine-tuning and the noise stability of the fine-tuned model. We empirically
measure these quantities. Based on the analysis, we propose regularized
self-labeling -- the interpolation between regularization and self-labeling
methods, including (i) layer-wise regularization to constrain the distance
traveled in each layer; (ii) self label-correction and label-reweighting to
correct mislabeled data points (that the model is confident) and reweight less
confident data points. We validate our approach on an extensive collection of
image and text data sets using multiple pre-trained model architectures. Our
approach improves baseline methods by 1.76% (on average) for seven image
classification tasks and 0.75% for a few-shot classification task. When the
target data set includes noisy labels, our approach outperforms baseline
methods by 3.56% on average in two noisy settings.","Learning from limited labeled data is a fundamental problem in many realworld applications (Ratner et al., 2016, 2017). A common approach to address this problem is Ô¨Ånetuning a large model that has been pretrained on publicly available labeled data (He et al., 2019). Since Ô¨Ånetuning is typically applied to a target task with limited labels, this algorithm is prone to overÔ¨Åtting or ‚Äúmemorization‚Äù issues (Tan et al., 2018). These issues worsen when the target task contains noisy labels (Zhang et al., 2016). In this paper, we analyze regularization methods for Ô¨Ånetuning from both theoretical and empirical perspectives. Based on the analysis, we propose a regularized selflabeling approach that improves the generalization and robustness properties of Ô¨Ånetuning. Previous works (Li et al., 2018a,b) have proposed regularization methods to constrain the distance between a Ô¨Ånetuned model and the pretrained model in the Euclidean norm. Li et al. (2020) provides extensive study to show that the performance of Ô¨Ånetuning and the beneÔ¨Åt of adding regularization depend on the hyperparameter choices. Salman et al. (2020) empirically Ô¨Ånd that performing adversarial training during the pretraining phase helps learn pretrained models that transfer better to downstream tasks. The work of Gouk et al. (2021) generalizes the above ideas to various norm choices and Ô¨Ånds that projected gradient descent methods perform well for implementing distancebased regularization. Additionally, they derive generalization bounds for Ô¨Ånetuning using Rademacher complexity. These works focus on settings where there is no label noise in the target data set. When 35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2111.04578v1  [cs.LG]  8 Nov 2021label noise is present, for example, due to applying weak supervision techniques (Ratner et al., 2016), an important question is to design methods that are robust to such noise. The problem of learning from noisy labels has a rich history of study in supervised learning (Natarajan et al., 2013). In contrast, little is known in the transfer learning setting. These considerations motivate us to analyze the generalization and robustness properties of Ô¨Ånetuning. In Section 4.1, we begin by conducting a PACBayesian analysis of regularized Ô¨Ånetuning. This is inspired by recent works that have found PACBayesian analysis correlates with empirical perfor mance better than Rademacher complexity (Jiang et al., 2020). We identify two critical measures for analyzing the generalization performance of Ô¨Ånetuning. The Ô¨Årst measure is the `2norm of the distance between the pretrained model (initialization) and the Ô¨Ånetuned model. The second measure is the perturbed loss of the Ô¨Ånetuned model, i.e. its loss after the model weights get perturbed by random noise. First, we observe that the Ô¨Ånetuned weights remain closed to the pretrained model. Moreover, the top layers travel much further away from the pretrained model than the bottom layers. Second, we Ô¨Ånd that Ô¨Ånetuning from a pretrained model implies better noise stability than training from a randomly initialized model. In Section 4.2, we evaluate regularized Ô¨Ånetuning for target tasks with noisy labels. We Ô¨Ånd that Ô¨Ånetuning is prone to ‚Äúmemorizing the noisy labels‚Äù, and regularization helps alleviate such memorization behavior. Moreover, we observe that the neural network has not yet overÔ¨Åtted to the noisy labels during the early phase of Ô¨Ånetuning. Thus, its prediction could be used to relabel the noisy labels. 0 10 20 30 Number of epochs0.00.20.40.6Prediction accuracyFinetuning: Training Finetuning: Test Regularized selflabeling: Training Regularized selflabeling: Test # Corrected training labels 1000130016001900 Number of data points Figure 1: Red: Layerwise regularization closes generalization gap. Magenta: Selflabeling rela bels noisy data points to their correct label.We propose an algorithm that incorporates layer wise regularization and selflabeling for im proved regularization and robustness based on our results. Figure 1 illustrates the two compo nents. First, we encode layerwise distance con straints to regularize the model weights at dif ferent levels. Compared to (vanilla) Ô¨Ånetuning, our algorithm reduces the gap between the train ing and test accuracy, thus alleviating overÔ¨Åtting. Second, we add a selflabeling mechanism that corrects and reweights ‚Äúnoisy labels‚Äù based on the neural network‚Äôs predictions. Figure 1 shows that our algorithm effectively hinders the model from learning the incorrect labels by relabeling them to correct ones. In Section 5, we evaluate our proposed algorithm for both transfer learning and fewshot classiÔ¨Åcation tasks with image and text data sets. First, using ResNet101 (He et al., 2016) as the pretrained model, our algorithm outperforms previous Ô¨Ånetuning methods on seven image classiÔ¨Åcation tasks by1:76% on average and 3:56% when their labels are noisy. Second, we Ô¨Ånd qualitatively similar results for applying our approach to medical image classiÔ¨Åcation tasks (ChestXray14 (Wang et al., 2017; Rajpurkar et al., 2017)) and vision transformers (Dosovitskiy et al., 2020). Finally, we extend our approach to fewshot learning and sentence classiÔ¨Åcation. For these related but different tasks and data modalities, we Ô¨Ånd an improvement of 0:75% and0:46% over previous methods, respectively. In summary, our contributions are threefold. First, we provide a PACBayesian analysis of regularized Ô¨Ånetuning. Our result implies empirical measures that explain the generalization performance of regularized Ô¨Ånetuning. Second, we present a regularized selflabeling approach to enhance the generalization and robustness properties of Ô¨Ånetuning. Third, we validate our approach on an extensive collection of classiÔ¨Åcation tasks and pretrained model architectures. 2 Related work "
277,LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification.txt,"Weakly supervised text classification methods typically train a deep neural
classifier based on pseudo-labels. The quality of pseudo-labels is crucial to
final performance but they are inevitably noisy due to their heuristic nature,
so selecting the correct ones has a huge potential for performance boost. One
straightforward solution is to select samples based on the softmax probability
scores in the neural classifier corresponding to their pseudo-labels. However,
we show through our experiments that such solutions are ineffective and
unstable due to the erroneously high-confidence predictions from poorly
calibrated models. Recent studies on the memorization effects of deep neural
models suggest that these models first memorize training samples with clean
labels and then those with noisy labels. Inspired by this observation, we
propose a novel pseudo-label selection method LOPS that takes learning order of
samples into consideration. We hypothesize that the learning order reflects the
probability of wrong annotation in terms of ranking, and therefore, propose to
select the samples that are learnt earlier. LOPS can be viewed as a strong
performance-boost plug-in to most of existing weakly-supervised text
classification methods, as confirmed in extensive experiments on four
real-world datasets.","Weakly supervised text classiÔ¨Åcation meth ods (Agichtein and Gravano, 2000; Riloff et al., 2003; Tao et al., 2015; Meng et al., 2018; Mekala and Shang, 2020; Mekala et al., 2020, 2021) typi cally start with generating pseudolabels, and train a deep neural classiÔ¨Åer to learn the mapping be tween documents and classes. There is no doubt that the quality of pseudolabels plays a fundamen tal role in the Ô¨Ånal classiÔ¨Åcation accuracy, how ever, they are inevitably noisy due to their heuristic ¬òJingbo Shang is the corresponding author. 0.10.20.30.40.50.60.70.80.91.0 Probability0.00.20.40.60.81.0Proportion of SamplesPrediction Probability Distribution Correct Wrong(a) 1 2 3 4 Never Epoch0.00.20.40.60.81.0Proportion of Samples learntProportion of Samples vs Learnt Epoch Correct Wrong (b) Figure 1: Distributions of correct and wrong instances using different pseudolabel selection strategies on the NYTCoarse dataset for its initial pseudolabels. The base classiÔ¨Åer is BERT. (a) is based on the softmax probability of samples‚Äô pseudolabels and (b) is based on the earliest epochs at which samples are learnt. nature. Pseudolabels are typically generated by some heuristic, for example, through string match ing between the documents and userprovided seed words (Mekala and Shang, 2020). Deep neural net works (DNNs) trained on such noisy labels have a high risk of making erroneous predictions. More importantly, when selftraining is employed, such error can be further ampliÔ¨Åed upon boostrapping. To address this problem, in this paper, we study the pseudolabel selection in weakly supervised text classiÔ¨Åcation, aiming to select a high quality subset of the pseudolabeled documents (in every iteration when using selftraining) that can poten tially achieve a higher classiÔ¨Åcation accuracy. A straightforward solution is to Ô¨Årst train a deep neural classiÔ¨Åer based on the pseudolabeled doc uments and then threshold the documents by the predicted probability scores corresponding to their pseudolabels. However, DNNs usually have a poor calibration and generate overconÔ¨Ådent predicted probability scores (Guo et al., 2017). For exam ple, on New York Times (NYT) coarsegrained dataset, as shown in Figure 1(a), 60% of wrong instances in the pseudolabeled documents have a predicted probability by BERT greater than 0:9for their wrong pseudolabels. Recent studies on the memorization effects of DNNs show that they memorize easy and clean inarXiv:2205.12528v2  [cs.CL]  25 Oct 2022Unlabeled Documents‚Ä¶TextLabel ‚ùì ‚ùì ‚ùì ‚ñ™Tampa bay won NFL championship‚ñ™He was banned by football federation..‚ñ™‚Äúcitizen kane‚Äù film music is composed by ‚Ä¶ Label Surface NamesSeed WordsWeak SupervisionMoviesCinematographerSongMusicSoccerGoalPenaltyFootballNFLMovieMusicSoccerFootballPseudolabeled Documents‚Ä¶Text‚ñ™Tampa bay won NFL championship‚ñ™He was banned by football federation..‚ñ™‚Äúcitizen kane‚Äù film music is composed by ‚Ä¶SoccerMusic ‚úÖ ‚ùå Football ‚úÖ LabelDeep Neural Classifier Learning Orderbased Label Selection123123123123MovieMusicSoccerFootball50%Selected Pseudolabeled Documents‚Ä¶Text‚ñ™Tampa bay won NFL championship‚ñ™He was banned by football federation..Soccer ‚úÖ Football ‚úÖ LabelProbing Classifier TrainInputSelftraining LOPS (Our proposed method)TrainBootstrap TrainFigure 2: An overview of our proposed LOPS and how it plugs into selftraining frameworks to replace the conven tional training step. Given pseudolabeled samples, LOPS trains a probing classiÔ¨Åer to obtain their learning order and we stop the training when at least %of samples corresponding to each class are learnt and select the learnt samples. The numbers shown are learnt epochs and the samples in the shaded part are selected. A text classiÔ¨Åer is trained on selected pseudolabeled documents that is further used for inference and bootstrapping. stances Ô¨Årst, and gradually learn hard instances and eventually memorize the wrong annotations (Arpit et al., 2017; Geifman et al., 2019; Zhang et al., 2021). We have conÔ¨Årmed this in our experiments for different classiÔ¨Åers. For example, as shown in Figure 1(b), BERT classiÔ¨Åer learns most of the clean instances in the Ô¨Årst epoch and learns wrong instances across all epochs. Although it also learns good number of wrong instances in the Ô¨Årst epoch, it is signiÔ¨Åcantly less than the probabilitybased selection in Figure 1(a). Therefore, we deÔ¨Åne the learning order of a pseudolabeled document as the epoch when it is learnt during training i.e. when the training model‚Äôs prediction is the same as its given pseudolabel. Since correct samples are learnt Ô¨Årst, we hypothesize that learning orderbased selection will be able to Ô¨Ålter out wrongly labeled samples. Inspired by our observation, we propose a novel learning order inspired pseudolabel selection method LOPS , as shown in Figure 2. SpeciÔ¨Å cally, LOPS involves training a probing classiÔ¨Åer on pseudolabeled data and tracking the learning order of samples. We deÔ¨Åne a sample is learnt if and only if the classiÔ¨Åer trained on pseudolabels gives the same argmax prediction as its pseudo label at the end of an epoch. We stop the training when at least %of samples corresponding to each class are learnt and select all the learnt samples. Then, we train a text classiÔ¨Åer on these selected pseudolabeled documents that is further used for inference. We empirically show that LOPS can boost the accuracy of various weakly supervisedtext classiÔ¨Åcation methods and it is much more effective and stable than probability scorebased selections. Our contributions are summarized as follows: ‚Ä¢We propose a novel pseudolabel selection method LOPS that takes learning order of sam ples into consideration. ‚Ä¢We show that selection based on learning order is much stable and effective than selection based on probability scores. ‚Ä¢Extensive experiments and case studies on real world datasets with different classiÔ¨Åers and weakly supervised text classiÔ¨Åcation methods demonstrate signiÔ¨Åcant performance gains upon using LOPS . It can be viewed as a solid performanceboost plugin for weak supervision. Reproducibility. We will release the code and datasets on Github1. 2 Related Work "
102,Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture.txt,"An accurate language identification tool is an absolute necessity for
building complex NLP systems to be used on code-mixed data. Lot of work has
been recently done on the same, but there's still room for improvement.
Inspired from the recent advancements in neural network architectures for
computer vision tasks, we have implemented multichannel neural networks
combining CNN and LSTM for word level language identification of code-mixed
data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of
93.28% and 93.32% is achieved on our two testing sets.","With the rise of social media, the amount of mine able data is rising rapidly. Countries where bilin gualism is popular, we see users often switch back and forth between two languages while typing, a phenomenon known as codemixing or code switching. For analyzing such data, language tag ging acts as a preliminary step and its accuracy and performance can impact the system results to a great extent. Though a lot of work has been done recently targeting this task, the problem of lan guage tagging in codemixed scenario is still far from being solved. Codemixing scenarios where one of the languages have been typed in its translit erated from possesses even more challenges, espe cially due to inconsistent phonetic typing. On such type of data, context capture is extremely hard as well. Proper context capture can help in solving problems like ambiguity, that is word forms which are common to both the languages, but for which, the correct tag can be easily understood by know ing the context. An additional issue is a lack of available codemixed data. Since most of the tasks require supervised models, the bottleneck of data crisis affects the performance quite a lot, mostly due to the problem of overÔ¨Åtting.In this article, we present a novel architecture, which captures information at both word level and context level to output the Ô¨Ånal tag. For word level, we have used a multichannel neural network (MNN) inspired from the recent works of computer vision. Such networks have also shown promising results in NLP tasks like sen tence classiÔ¨Åcation (Kim, 2014). For context cap ture, we used BiLSTMCRF. The context module was tested more rigorously as in quite a few of the previous work, this information has been sidelined or ignored. We have experimented on Bengali English (BnEn) and HindiEnglish (HiEn) code mixed data. Hindi and Bengali are the two most popular languages in India. Since none of them have Roman as their native script, both are writ ten in their phonetically transliterated from when codemixed with English. 2 Related Work "
585,SLADE: A Self-Training Framework For Distance Metric Learning.txt,"Most existing distance metric learning approaches use fully labeled data to
learn the sample similarities in an embedding space. We present a self-training
framework, SLADE, to improve retrieval performance by leveraging additional
unlabeled data. We first train a teacher model on the labeled data and use it
to generate pseudo labels for the unlabeled data. We then train a student model
on both labels and pseudo labels to generate final feature embeddings. We use
self-supervised representation learning to initialize the teacher model. To
better deal with noisy pseudo labels generated by the teacher network, we
design a new feature basis learning component for the student network, which
learns basis functions of feature representations for unlabeled data. The
learned basis vectors better measure the pairwise similarity and are used to
select high-confident samples for training the student network. We evaluate our
method on standard retrieval benchmarks: CUB-200, Cars-196 and In-shop.
Experimental results demonstrate that our approach significantly improves the
performance over the state-of-the-art methods.","Existing distance metric learning methods mainly learn sample similarities and image embeddings using labeled data [22, 17, 2, 29], which often require a large amount of data to perform well. A recent study [21] shows that most methods perform similarly when hyperparameters are properly tuned despite employing various forms of losses. The performance gains likely come from the choice of net work architecture. In this work, we explore another direc tion that uses unlabeled data to improve retrieval perfor mance. Recent methods in selfsupervised learning [14, 6, 5] and selftraining [32, 7] have shown promising results us ing unlabeled data. Selfsupervised learning leverages un *Work done during an internship at Amazon. Figure 1. A selftraining framework for retrieval. In the training phase, we train the teacher and student networks using both la beled and unlabeled data. In the testing phase, we use the learned student network to extract embeddings of query images for re trieval. labeled data to learn general features in a taskagnostic manner. These features can be transferred to downstream tasks by Ô¨Ånetuning. Recent models show that the features produced by selfsupervised learning achieve comparable performance to those produced by supervised learning for downstream tasks such as detection or classiÔ¨Åcation [5]. Selftraining methods [32, 7] improve the performance of fullysupervised approaches by utilizing a teacher/student paradigm. However, existing methods for selfsupervised learning or selftraining mainly focus on classiÔ¨Åcation but not retrieval. We present a SeLftrAining framework for Distance mEtric learning (SLADE) by leveraging unlabeled data. Figure 1 illustrates our method. We Ô¨Årst train a teacher model on the labeled dataset and use it to generate pseudo labels for the unlabeled data. We then train a student model on both labels and pseudo labels to generate a Ô¨Ånal featurearXiv:2011.10269v2  [cs.CV]  29 Mar 2021embedding. We utilize selfsupervised representation learning to ini tialize the teacher network. Most deep metric learning ap proaches use models pretrained on ImageNet ([17], [29], etc). Their extracted representations might overÔ¨Åt to the pretraining objective such as classiÔ¨Åcation and not gen eralize well to different downstream tasks including dis tance metric learning. In contrast, selfsupervised repre sentation learning [5, 6, 7, 14] learns taskneutral features and is closer to distance metric learning. For these reasons, we initialize our models using selfsupervised learning ap proaches. Our experimental results (Table 3) provide an em pirical justiÔ¨Åcation for this choice. Once the teacher model is pretrained and Ô¨Ånetuned, we use it to generate pseudo labels for unlabeled data. Ide ally, we would directly use these pseudo labels to generate positive and negative pairs and train the student network. However in practice, these pseudo labels are noisy, which affects the performance of the student model (cf. Table 4). Moreover, due to their different sources, it is likely that the labeled and unlabeled data include different sets of cate gories (see Section 4.1 for details about labeled and unla beled datasets). The features extracted from the embedding layer may not adequately represent samples from those un seen classes. To tackle these issues, we propose an addi tional representation layer after the embedding layer. This new layer is only used for unlabeled data and aims at learn ing basis functions for the feature representation of unla beled data. The learning objective is contrastive, i.e. im ages from the same class are mapped close while images from different classes are mapped farther apart. We use the learned basis vectors to compute the feature representation of each image and measure pairwise similarity for unlabeled data. This enables us to select highconÔ¨Ådent samples for training the student network. Once the student network is trained, we use it to extract embeddings of query images for retrieval. We evaluate our model on several standard retrieval benchmarks: CUB200, Cars196 and Inshop. As shown in the experimental section, our approach outperforms several stateoftheart methods on CUB200 and Cars196, and is competitive on Inshop. We also provide various ablation studies in the experimental section. The main technical contributions of our work are: ‚Ä¢ A selftraining framework for distance metric learning, which utilizes unlabeled data to improve retrieval per formance. ‚Ä¢ A feature basis learning approach for the student net work, which better deals with noisy pseudo labels gen erated by the teacher network on unlabeled data.2. Related work "
212,MarginDistillation: distillation for margin-based softmax.txt,"The usage of convolutional neural networks (CNNs) in conjunction with a
margin-based softmax approach demonstrates a state-of-the-art performance for
the face recognition problem. Recently, lightweight neural network models
trained with the margin-based softmax have been introduced for the face
identification task for edge devices. In this paper, we propose a novel
distillation method for lightweight neural network architectures that
outperforms other known methods for the face recognition task on LFW, AgeDB-30
and Megaface datasets. The idea of the proposed method is to use class centers
from the teacher network for the student network. Then the student network is
trained to get the same angles between the class centers and the face
embeddings, predicted by the teacher network.","The development of edge devices has sparked signicant interest in lightweight face recognition access systems. This type of solution is based on optimized neu ral network architectures for mobile devices. A typical example of such network is MobileFaceNet [2], designed specically for the face recognition on devices with low computing power. The usage of marginbase softmax approach [3,14,21] in the training procedure helps to obtain the stateoftheart performance for the face recognition tasks. Despite fast and compact mobile network architectures give lower face recog nition accuracy than the fullsize ones, in some applications, such as biometric access systems, it nevertheless plays a critical role. Distillation is a method that helps to achieve the highest accuracy for mobile neural network architectures, where the knowledge is transferred from a heavy teacher network to a small student network. In this article we propose a novel distillation method called MarginDistillation to reduce the gap between teacher and student networks dur ing distillation process. The idea of the proposed method is to copy class centers from a teacher networkarXiv:2003.02586v1  [cs.CV]  5 Mar 20202 Svitov David and Alyamkin Sergey to a student network and freeze class centers for the whole distillation proce dure, where the student network is trained to get angles between given class centers and face embeddings the same as in the teacher network. It allows the student network to better reproduce results of the teacher network trained with the marginbased loss function. The main contributions of our work: {We have proposed a novel method for the distillation of neural networks trained with a marginbased softmax. {The proposed method allows a gap reduction between teacher and student networks for face recognition problem. The accuracy of the mobile face recog nition neural network achieved with our method exceeds other known distil lation methods on dierent datasets: LFW [11], AgeDB30 [15] and Mage Face [13] dataset. {In presented work we made direct comparison of dierent distillation meth ods. Code for implemented methods and comparison experiments is available on the github. 2 Related Works "
294,De-identifying Australian Hospital Discharge Summaries: An End-to-End Framework using Ensemble of Deep Learning Models.txt,"Electronic Medical Records (EMRs) contain clinical narrative text that is of
great potential value to medical researchers. However, this information is
mixed with Personally Identifiable Information (PII) that presents risks to
patient and clinician confidentiality. This paper presents an end-to-end
deidentification framework to automatically remove PII from Australian hospital
discharge summaries. Our corpus included 600 hospital discharge summaries which
were extracted from the EMRs of two principal referral hospitals in Sydney,
Australia. Our end-to-end de-identification framework consists of three
components: 1) Annotation: labelling of PII in the 600 hospital discharge
summaries using five pre-defined categories: person, address, date of birth,
individual identification number, phone/fax number; 2) Modelling: training six
named entity recognition (NER) deep learning base-models on balanced and
imbalanced datasets; and evaluating ensembles that combine all six base-models,
the three base-models with the best F1 scores and the three base-models with
the best recall scores respectively, using token-level majority voting and
stacking methods; and 3) De-identification: removing PII from the hospital
discharge summaries. Our results showed that the ensemble model combined using
the stacking Support Vector Machine (SVM) method on the three base-models with
the best F1 scores achieved excellent results with a F1 score of 99.16% on the
test set of our corpus. We also evaluated the robustness of our modelling
component on the 2014 i2b2 de-identification dataset. Our ensemble model, which
uses the token-level majority voting method on all six basemodels, achieved the
highest F1 score of 96.24% at strict entity matching and the highest F1 score
of 98.64% at binary token-level matching compared to two state-of-the-art
methods.","There is currently intense interest in using narrative free text from Electronic Medical Records (EMRs)  to develop predictive algorithms for patient stratification and personalised health care. However,  sharing of and access to this clinical text for research purposes are severely constrained because it  includes information that may identify specific individuals, known as Personally Identifiable  Information (PII) [1]. Development of robust and scalable yet resourceefficient methods for de identifying clinical narrative text is a priority for making these valuable data more available to  researchers [2] to generate evidence that will deliver benefits to both health care providers and patients  [3].  In Australia, there is no clear definition of the term ‚Äúdeidentified health data‚Äù or agreed standards for  deidentification, even though a strong health privacy regulatory environment is in place [4]. In contrast,  the Health Insurance Portability and Accountability Act (HIPAA) which was passed in United States in  1996 provides two deidentification methods: 1) The ‚ÄúExpert Determination‚Äù method: qualified experts  analyse and determine the risk of reidentifying an individual who is a subject of the information; and  2) The ‚ÄúSafe Harbor‚Äù method: this method requires the removal of 18 types of identifiers (see Table  S1), including names, geographic subdivisions, telephone numbers, medical record numbers and so on  [5].   Manual deidentification of a large clinical corpus is a timeconsuming and challenging task [6, 7].  Some studies have explored the possibility of using Natural Language Processing (NLP) and deep  learning techniques for automatic deidentification relevant to the ‚ÄúSafe Harbor‚Äù method [810]. Three  deidentification challenges, organized by the Informatics for Integrating Biology and the Bedside (i2b2)  organization in 2006 [11] and 2014 [12], and Centres of Excellence in Genomic Science (CEGS) and  Neuropsychiatric GenomeScale and RDOC Individualized Domains (NGRID) in 2016 [13],  facilitated innovation in deidentification methods based on NLP and proved the efficiency of automatic  deidentification. However, these challenges involved building models and comparing results using a  specific annotated dataset. There is a need for endtoend deidentification solutions which can be easily  applied to raw clinical narrative documents, especially within the internet accessrestricted  environments of hospitals.  In this paper, we propose an endtoend deidentification framework with application to Australian  hospital discharge summaries. This paper describes the three components of the endtoend framework:  1) Annotation: a webbased annotation tool to help our human annotators tag the PII entities in the raw  hospital discharge summaries; 2) Modelling: training and evaluating ensembles of different named  entity recognition (NER) deep learning models to identify PII in hospital discharge summaries; 3) De identification: automatic tag and removal of PII entities from hospital discharge summaries.  2. Related work   "
186,Recalling Holistic Information for Semantic Segmentation.txt,"Semantic segmentation requires a detailed labeling of image pixels by object
category. Information derived from local image patches is necessary to describe
the detailed shape of individual objects. However, this information is
ambiguous and can result in noisy labels. Global inference of image content can
instead capture the general semantic concepts present. We advocate that
high-recall holistic inference of image concepts provides valuable information
for detailed pixel labeling. We build a two-stream neural network architecture
that facilitates information flow from holistic information to local pixels,
while keeping common image features shared among the low-level layers of both
the holistic analysis and segmentation branches. We empirically evaluate our
network on four standard semantic segmentation datasets. Our network obtains
state-of-the-art performance on PASCAL-Context and NYUDv2, and ablation studies
verify its effectiveness on ADE20K and SIFT-Flow.","Image analysis is a fundamental problem in computer vision. The task can be framed at different levels of gran ularity. At a Ô¨Åne scale, semantic segmentation labels each pixel to depict semantic elements by detailed shapes and contours. However, detailed semantic segmentation is challeng ing ‚Äì there exists signiÔ¨Åcant ambiguity in Ô¨Ånescale im age patches that can result in noisy semantic segmentation outputs. The main focus of this paper is utilizing holistic information derived from analyzing entire images to Ô¨Ålter equal contribution. Holistic	AnalysisSemanticSegmentationholisticInfo.HolisticFilteringpixellabelsbicyclemotorbikerefinedpixel	labelsbicyclemotorbikebicyclepersoncarmotorbikeFigure 1. An example showing the usage of holistic information in semantic segmentation. We obtain holistic information about the contents of the entire image, and leverage this to Ô¨Ålter out noisy pixel predictions to improve semantic segmentation. In our exam ple, the noisy pixel label ( i.e., bicycle) is removed after holistic Ô¨Åltering since it is not recalled as a likely label via the holistic analysis. noisy lowlevel semantic segmentation. Holistic informa tion about the content of an image is highly valuable for semantic segmentation on local pixels ‚Äì essentially, the ex istence, nonexistence and coexistence information on se mantic classes provides a strong cue in determining the lo cal pixel labels (see Figure 1). Stateoftheart methods for semantic segmentation leverage the successes of Convolutional Neural Networks (CNNs) [18]. CNNs have transformed the Ô¨Åeld of im age classiÔ¨Åcation, especially since the development of AlexNet [16]. There have been many followup CNN ar chitectures to further boost image classiÔ¨Åcation, including VGGNet [28], Google Inception [29], ResNet [11], etc. Se mantic segmentation utilizes these network structures, com bined with dense output structures to label image pixels by semantic categories. A representative work is the Fully Convolutional Network (FCN) [26] that leverages skip fea tures of CNNs to produce a detailed pixel labeling. Another 1arXiv:1611.08061v1  [cs.CV]  24 Nov 2016example is the DeepLab [3] framework, which augments FCN with dilated convolution [32], atrous spatial pyramid pooling and Conditional Random Fields (CRFs), and ob tains stateofart semantic segmentation performance. Common among these previous methods is a focus on (layers of) lowlevel pixel analysis leading to semantic seg mentation. Stateoftheart techniques combine this with sophisticated graphical modelstyle techniques (CRF) and pooling structures to obtain high accuracy. The role of these additional components is to smooth out the noisy pixel la belings that result from the direct CNN analysis. However, we believe that a simpler approach to incorporate this in formation is via a holistic analysis that globally suggests category labels that are likely to be present in the image. To make use of holistic information in semantic segmen tation, we leverage it to Ô¨Ålter out noisy pixel predictions ‚Äì if the holistic information suggests a semantic category is unlikely to be present, then pixels should be unlikely to be predicted as that label. Of course, holistic image analysis is imperfect. We conduct a detailed problem analysis in Sec tion 3 to study how and when the holistic information can help. We utilize these observations to propose a novel neu ral network architecture that uniÔ¨Åes a holistic branch and a detailed semantic segmentation branch. The two branches share common image features in lowlevel layers of convo lution and pooling. In the holistic branch, we then recall information about semantic categories by aggregating over a grid of image patches. In the segmentation branch, we pipe in holistic information to guide segmentation by Ô¨Ålter ing out noisy pixel predictions. Our network is endtoend trainable towards the goal of highquality semantic segmen tation. Contribution. We summarize our main contributions as: First, we advocate to leverage holistic image analysis to guide semantic segmentation, and provide empirical analysis to support our intuition. Second, we invent holistic Ô¨Åltering that enables us to Ô¨Ålter out noisy pixel predictions with the guidance of holistic information. Third, we propose a twostream neural network archi tecture for semantic segmentation. We implement a holistic branch and a segmentation branch, which fa cilitate the Ô¨Çow of global image information to the seg mentation branch. Our approach is general, and could be incorporated into a variety of CNNbased semantic segmentation architectures. Finally, we evaluate our proposed network on PASCALContext [23], ADE20K [35], NYUDv2 [27] and SIFTFlow [20]. Experimental results show that the proposed network improves upon stateoftheart semantic segmentation models such as FCN [26] and DeepLab [3].2. Related Work "
408,Anomaly Detection with Inexact Labels.txt,"We propose a supervised anomaly detection method for data with inexact
anomaly labels, where each label, which is assigned to a set of instances,
indicates that at least one instance in the set is anomalous. Although many
anomaly detection methods have been proposed, they cannot handle inexact
anomaly labels. To measure the performance with inexact anomaly labels, we
define the inexact AUC, which is our extension of the area under the ROC curve
(AUC) for inexact labels. The proposed method trains an anomaly score function
so that the smooth approximation of the inexact AUC increases while anomaly
scores for non-anomalous instances become low. We model the anomaly score
function by a neural network-based unsupervised anomaly detection method, e.g.,
autoencoders. The proposed method performs well even when only a small number
of inexact labels are available by incorporating an unsupervised anomaly
detection mechanism with inexact AUC maximization. Using various datasets, we
experimentally demonstrate that our proposed method improves the anomaly
detection performance with inexact anomaly labels, and outperforms existing
unsupervised and supervised anomaly detection and multiple instance learning
methods.","Anomaly detection is an important machine learning task, which is a task to nd the anomalous instances in a dataset. Anomaly detection has been used in a wide variety of applications (Chandola et al., 2009; Patcha and Park, 2007; Hodge and Austin, 2004), such as network intrusion detection for cybersecurity (Dokas et al., 2002; Yamanishi et al., 2004), fraud detection for credit cards (Aleskerov et al., 1997), defect detection in industrial machines (Fujimaki et al., 2005; Id e and Kashima, 2004) and disease outbreak detection (Wong et al., 2003). Many unsupervised anomaly detection methods have been proposed (Breunig et al., 2000; Sch olkopf et al., 2001; Liu et al., 2008; Sakurada and Yairi, 2014). When anomaly labels, which indicate whether each instance is anomalous, are given, the anomaly detection performance can be improved (Singh and Silakari, 2009; Mukkamala et al., 2005; Rapaka et al., 2003; Nadeem et al., 2016; Gao et al., 2006; Das et al., 2016, 2017). However, it is dicult to attach exact anomaly labels in some situations. Consider such example in server system failure detection. System operators often do not know the exact time of failures; they only know that a failure occurred within a certain period of time. In this case, anomaly labels can be attached to instances in a certain period of time, in which nonanomalous instances might be included. Another example is detecting anomalous IoT devices connected to a server. Even when the server displays unusual behavior, we sometimes cannot identify which IoT devices are anomalous. In these situations, only inexact anomaly labels are available. In this paper, we propose a supervised anomaly detection method for data with inexact anomaly labels. An inexact anomaly label is attached to a set of instances, indicating that at least one instance in the set is anomalous. We call this set an inexact anomaly set. First, we dene an extension of the area under the ROC curve (AUC) for performance measurement with inexact labels, which we call aninexact AUC . Then we develop an anomaly detection method that maximizes the inexact AUC. With the proposed method, a function, which outputs an anomaly score given an instance, is modeled by the reconstruction error with autoencoders, which are a successfully used neural networkbased 1arXiv:1909.04807v1  [stat.ML]  11 Sep 2019Figure 1: Example of anomalous (circle) and nonanomalous (triangle) instances, and inexact anomaly sets (red or blue) in an instance space. Instances with identical color (red or blue) are contained in the same inexact anomaly set. White circles are test anomalous instances. unsupervised anomaly detection method (Sakurada and Yairi, 2014; Sabokrou et al., 2016; Chong and Tay, 2017; Zhou and Paenroth, 2017). Note that the proposed method can use any unsupervised anomaly detection methods with learnable parameters instead of autoencoders, such as variational autoencoders (Kingma and Wellniga, 2014), energybased models (Zhai et al., 2016), and isolation forests (Liu et al., 2008). The parameters of the anomaly score function are trained so that the anomaly scores for nonanomalous instances become low while the smooth approximation of the inexact AUC becomes high. Since our objective function is dierentiable, the anomaly score function can be estimated eciently using stochastic gradientbased optimization methods. The proposed method performs well even when only a few inexact labels are given since it incor porates an unsupervised anomaly detection mechanism, which works without label information. In addition, the proposed method is robust to class imbalance since our proposed inexact AUC maximiza tion is related to AUC maximization, which achieved high performance on imbalanced data classication tasks (Cortes and Mohri, 2004). Class imbalance robustness is important for anomaly detection since anomalous instances occur more rarely than nonanomalous instances. Figure 1 shows an example of anomalous and nonanomalous instances, and inexact anomaly sets in a twodimensional instance space. For unsupervised methods, it is dicult to detect test anoma lous instance `A' since some instances are located around it. Unsupervised methods consider that an instance is anomalous when there are few instances around it. Since supervised methods can use label information, they can correctly detect test anomalous instance `A'. However, they might not detect test anomalous instance `B' since there are no labeled instances near it. In addition, with supervised methods that consider all the instances in inexact anomaly sets are anomaly, nonanomalous instances around training instances in the inexact anomaly sets (colored triangles in Figure 1) would be misclassi ed as anomaly. On the other hand, the proposed method detects `A' using label information and `B' by incorporating an unsupervised anomaly detection mechanism. Also, it would not detect nonanomalous instances as anomaly since it can handle inexact information. The remainder of the paper is organized as follows. In Section 2, we brie y review related work. In Section 3, we introduce AUC, which is the basis of the inexact AUC. In Section 4, we present the inexact AUC, dene our task, and propose our method for supervised anomaly detection using inexact labels. In Section 5, we experimentally demonstrate the eectiveness of our proposed method using various datasets by comparing with existing anomaly detection and multiple instance learning methods. Finally, we present concluding remarks and discuss future work in Section 6. 2 Related work "
605,Expert Sample Consensus Applied to Camera Re-Localization.txt,"Fitting model parameters to a set of noisy data points is a common problem in
computer vision. In this work, we fit the 6D camera pose to a set of noisy
correspondences between the 2D input image and a known 3D environment. We
estimate these correspondences from the image using a neural network. Since the
correspondences often contain outliers, we utilize a robust estimator such as
Random Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the
pose parameters. When the problem domain, e.g. the space of all 2D-3D
correspondences, is large or ambiguous, a single network does not cover the
domain well. Mixture of Experts (MoE) is a popular strategy to divide a problem
domain among an ensemble of specialized networks, so called experts, where a
gating network decides which expert is responsible for a given input. In this
work, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a
MoE. Our main technical contribution is an efficient method to train ESAC
jointly and end-to-end. We demonstrate experimentally that ESAC handles two
real-world problems better than competing methods, i.e. scalability and
ambiguity. We apply ESAC to fitting simple geometric models to synthetic
images, and to camera re-localization for difficult, real datasets.","In computer vision, we often have a model that explains an observation with a small set of parameters. For exam ple, our model is the 6D pose (translation and rotation) of a camera, and our observations are images of a known 3D environment. The task of camera relocalization is then to robustly and accurately predict the 6D camera pose given the camera image. However, inferring model parameters from an observation is difÔ¨Åcult because many effects are not explained by our model. People might move through the environment, and its appearance varies largely due to lighting effects such as day versus night. We usually map our observation to a representation from which we can in fer model parameters more easily. For example, in camera Gating Prediction: Environment: Query Image: Experts: ... Low Sample Consensus High Sample ConsensusExpert Predictions: Office 1 Office 2 Office 2 Office 1Figure 1. Camera ReLocalization Using ESAC. Given an envi ronment consisting of several ambiguous rooms (top) and a query image (middle), we estimate the 6D camera pose (bottom). A gat ing network (black) predicts a probability for each room. We dis tribute a budget of pose hypotheses to expert networks specialized to each room. We choose the pose hypothesis with maximum sam ple consensus (green), i.e. the maximum geometric consistency. We train all networks jointly and endtoend. relocalization we can train a neural network to predict cor respondences between the 2D input image and the 3D envi ronment. Inferring the camera pose from these correspon dences is much easier, and various geometric solvers for this problem exist [21, 16, 26]. Because some predictions of the network might be erroneous, i.e. we have outlier cor respondences, we utilize a robust estimator such as Random Sample Consensus (RANSAC) [14], resp. its differentiable counterpart Differentiable Sample Consensus (DSAC) [6], or other differentiable estimators [53, 35] for training. 1arXiv:1908.02484v1  [cs.CV]  7 Aug 2019For some tasks, the problem domain is large or ambigu ous. In camera relocalization, an environment could fea ture repeating structures that are unique locally but not glob ally, e.g. ofÔ¨Åce equipment, radiators or windows. A single feedforward network cannot predict a correct correspon dence for such objects because there are multiple valid solu tions. However, if we train an ensemble of networks where each network specializes in a local part of the environment, we can resolve such ambiguities. This strategy is known in machine learning as Mixture of Experts (MoE) [20]. Each expert is a network specialized to one part of the problem domain. An additional gating network decides which expert is responsible for a given observation. More speciÔ¨Åcally, the output of the gating network is a categorical distribution over experts, which either guides the selection of a single expert, or a weighted average of all expert outputs [30]. In this work, we extend Mixture of Experts for Ô¨Åtting parametric models. Each expert specializes to a part of all training observations, and predicts a representation to which we Ô¨Åt model parameters using DSAC. We argue that two realizations of a Mixture of Experts model are not op timal: i) letting the gating network select one expert only [19, 51, 3, 43]; ii) giving as output a weighted average of all experts [20, 1]. In the Ô¨Årst case, we ignore that the gat ing network might attribute substantial probability to more than one expert. We might choose the wrong expert, and get a poor result. In the second case, we calculate an average in model parameter space which can be instable in learn ing [6]. In our realization of a Mixture of Experts model, we integrate the gating network into the hypothesizeand verify framework of DSAC. To estimate model parameters, DSAC creates many model hypotheses by sampling small subsets of data points, and Ô¨Åtting model parameters to each subset. DSAC scores hypotheses according to their con sistency with all data points, i.e. their sample consensus. One hypothesis is selected as the Ô¨Ånal estimate according to this score. Hypothesis selection is probabilistic, and train ing aims at minimizing the expected task loss. Instead of letting the gating network pick one expert, and Ô¨Åt model parameters only to this expert‚Äôs prediction, we dis tribute model hypotheses among experts. Each expert re ceives a share of the total number of hypotheses according to the gating network. For the Ô¨Ånal selection, we score each hypothesis according to sample consensus, irrespective of what expert it came from, see Fig 1. Therefore, as long as the gating network attributes some probability to the cor rect expert, we can still get an accurate model parameter estimate. We call this framework Expert Sample Consensus (ESAC). We train the network ensemble jointly and endto end by minimizing the expected task loss. We deÔ¨Åne the expectation over both, hypotheses sharing according to the gating network, and hypothesis selection according to sam ple consensus.We demonstrate our method on a toy problem where the gating network has to decide which model to Ô¨Åt to syn thetic data  a line or a circle. Compared to naive expert selection, our method proves to be extremely robust regard ing the gating network‚Äôs ability to assign the correct expert. Our method also achieves stateoftheart results in camera relocalization where each expert specializes in a separate, small part of a larger indoor environment. We give the following main contributions : We present Expert Sample Consensus (ESAC), an en semble formulation of Differentiable Sample Consen sus (DSAC) which we derive from Mixture of Experts (MoE). A method to train ESAC jointly and endtoend. We demonstrate the properties of our algorithm on a toy problem of Ô¨Åtting simple parametric models to noisy, synthetic inputs. Our formulation improves on two realworld aspects of learningbased camera relocalization, scalability and ambiguity. We achieve stateoftheart results on difÔ¨Å cult, public datasets for indoor relocalization. 2. Related Work "
127,Modeling Surface Appearance from a Single Photograph using Self-augmented Convolutional Neural Networks.txt,"We present a convolutional neural network (CNN) based solution for modeling
physically plausible spatially varying surface reflectance functions (SVBRDF)
from a single photograph of a planar material sample under unknown natural
illumination. Gathering a sufficiently large set of labeled training pairs
consisting of photographs of SVBRDF samples and corresponding reflectance
parameters, is a difficult and arduous process. To reduce the amount of
required labeled training data, we propose to leverage the appearance
information embedded in unlabeled images of spatially varying materials to
self-augment the training process. Starting from an initial approximative
network obtained from a small set of labeled training pairs, we estimate
provisional model parameters for each unlabeled training exemplar. Given this
provisional reflectance estimate, we then synthesize a novel temporary labeled
training pair by rendering the exact corresponding image under a new lighting
condition. After refining the network using these additional training samples,
we re-estimate the provisional model parameters for the unlabeled data and
repeat the self-augmentation process until convergence. We demonstrate the
efficacy of the proposed network structure on spatially varying wood, metals,
and plastics, as well as thoroughly validate the effectiveness of the
self-augmentation training process.","Recovering the spatially varying bidirectional surface re/f_lectance distribution function (SVBRDF) from a single photograph under un known natural lighting is a challenging and illposed problem. Often a physically accurate estimate is not necessary, and for many appli cations, such as large scale content creation for virtual worlds and computer games, a physically plausible estimate would already be valuable. Currently, common practice, albeit very timeconsuming, is to rely on skilled artists to, given a single reference image, pro duce a plausible re/f_lectance decomposition. This manual process suggests that given suÔ¨Écient prior knowledge, it is possible to infer a plausible re/f_lectance estimate for a spatially varying material from a single photograph. Datadriven machine learning techniques have been successfully applied to a wide range of underconstrained computer graphics and computer vision problems. In this paper, we follow a similar route and design a Convolutional Neural Network (CNN) to estimate physically plausible SVBRDFs from a single near/f_ield observation of a planar sample of a spatially varying material under unknown natural illumination. However, recovering the SVBRDF from a single photograph is an inherently illconditioned problem, since it is unlikely that each pixel observes a signi/f_icant specular response, making it impossible to derive a full spatially varying specular component without enforcing spatial priors. We therefore estimate areduced SVBRDF de/f_ined by a spatially varying diÔ¨Äuse albedo, homogeneous specular albedo and roughness, and spatially varying surface normals. Training a CNN to estimate such a reduced SVBRDF from a sin gle photograph under unknown natural lighting requires a large set of ‚Äúlabeled‚Äù photographs, i.e., with corresponding re/f_lectance parameters. Gathering such a training dataset is often a tedious and arduous task. Currently, except for specialized materials, very few databases exist that densely cover all possible spatial variations of a material class. Unlabeled data (i.e., a photograph of a spatially ACM Transactions on Graphics, Vol. 36, No. 4, Article 45. Publication date: July 2017.arXiv:1809.00886v1  [cs.GR]  4 Sep 201845:2 ‚Ä¢X. Li et al. varying material) is typically much easier to obtain. Each unlabeled photograph contains an instance of the complex spatially varying re /f_lectance parameters, albeit it observed from a single view and under unknown lighting. This raises the question whether we can exploit this embedded knowledge of the spatially varying distributions to re/f_ine the desired SVBRDFestimation CNN. We propose, in addition to a CNNbased solution for SVBRDF esti mation from a single photograph, a novel training strategy to lever age a large collection of unlabeled data ‚Äìphotographs of spatially varying materials without corresponding re/f_lectance parameters‚Äì to augment the training of a CNN from a much smaller set of labeled training data. To ‚Äúupgrade‚Äù such unlabeled data for training, a pre diction of the unknown model parameters is needed. We propose to use the target CNN itself to generate a provisional estimate of the re/f_lectance properties in the unlabeled photographs. However, we cannot directly use the provisional estimates and the corresponding unlabeled photograph as a valid training pair, since the estimated parameters are likely biased by the errors in the CNN, and hence it misses the necessary information to correct these errors. Our key observation is that for SVBRDF estimation, the exact inverse of the desired CNN is actually known in the form of a physicallybased rendering algorithm, that given any lighting and view parameters, synthesizes a photograph of the estimated re/f_lectance parameters. Under the assumption that the initial CNN trained by the labeled data acts as a reasonable predictor, the resulting provisional re /f_lectance estimates represent reasonable SVBRDFs similar (but not identical) to the SVBRDFs in the unlabeled training photographs. Therefore, instead of directly using the provisional re/f_lectance esti mates and unlabeled photographs as training pairs, we synthesize a new training sample by rendering an image with the provisional re/f_lectance estimates under random lighting and view. After re/f_ining the CNN using this synthesized training data, we can update the provisional re/f_lectance estimates and corresponding synthetic visu alizations, and repeat the process. The proposed selfaugmentation training process progressively re/f_ines the CNN to be coherent with the known inverse process (i.e., rendering algorithm), thereby im proving the accuracy of the target CNN. We demonstrate the ef /f_icacy of our method by training a CNN for diÔ¨Äerent classes of spatially varying materials such as wood, plastics and metals, as well as perform a careful analysis and validation of the proposed selfaugmentation training strategy. 2 RELATED WORK "
338,Stochastic parameterization identification using ensemble Kalman filtering combined with expectation-maximization and Newton-Raphson maximum likelihood methods.txt,"For modelling geophysical systems, large-scale processes are described
through a set of coarse-grained dynamical equations while small-scale processes
are represented via parameterizations. This work proposes a method for
identifying the best possible stochastic parameterization from noisy data.
State-the-art sequential estimation methods such as Kalman and particle filters
do not achieve this goal succesfully because both suffer from the collapse of
the parameter posterior distribution. To overcome this intrinsic limitation, we
propose two statistical learning methods. They are based on the combination of
two methodologies: the maximization of the likelihood via
Expectation-Maximization (EM) and Newton-Raphson (NR) algorithms which are
mainly applied in the statistic and machine learning communities, and the
ensemble Kalman filter (EnKF). The methods are derived using a Bayesian
approach for a hidden Markov model. They are applied to infer deterministic and
stochastic physical parameters from noisy observations in coarse-grained
dynamical models. Numerical experiments are conducted using the Lorenz-96
dynamical system with one and two scales as a proof-of-concept. The imperfect
coarse-grained model is modelled through a one-scale Lorenz-96 system in which
a stochastic parameterization is incorpored to represent the small-scale
dynamics. The algorithms are able to identify an optimal stochastic
parameterization with a good accuracy under moderate observational noise. The
proposed EnKF-EM and EnKF-NR are promising statistical learning methods for
developing stochastic parameterizations in high-dimensional geophysical models.","The statistical combination of observations of a dynamical model with a priori information of physical laws allows the estimation of the full state of the model even when it is only ‚ãÜCorresponding author. email: pulido@unne.edu.arpartially observed. This is the main aim of data assimilatio n (Kalnay, 2002). One common challenge of evolving multisca le systems in applications ranging from meteorology, oceanog ra phy, hydrology and space physics to biochemistry and biolog  ical systems is the presence of parameters that do not rely on known physical constants so that their values are unknown an d unconstrained. Data assimilation techniques can also be fo r c/circlecopyrt0000 Tellus2 mulated to estimate these model parameters from observatio ns (Jazwinski, 1970; Wikle and Berliner, 2007). There are several multiscale physical systems which are modelled through coarsegrained equations. The most parad ig matic cases being climate models (Stensrud, 2009), largee ddy simulations of turbulent Ô¨Çows (Mason and Thomson, 1992), and electron Ô¨Çuxes in the radiation belts (Kondrashov et al. , 2011). These imperfect models need to include subgridscal e effects through physical parameterizations (Nicolis, 200 4). In the last years, stochastic physical parameterizations hav e been incorporated in weather forecast and climate models (Palme r, 2001; Shutts, 2015; Christensen et al., 2015). They are call ed stochastic parameterizations because they represent stoc hasti cally a process that is not explicitly resolved in the model, even when the unresolved process may not be itself stochastic. Th e forecast skill of ensemble forecast systems has been shown t o improve with these stochastic parameterizations (Ibid.). Deter ministic integrations with models that include these param eter izations have also been shown to improve climate features (s ee e.g. Lott et al. 2012). In general, stochastic parameteriza tions are expected to improve coarsegrained models of multisca le physical systems (Katsoulakis et al., 2003; Majda and Gersh  gorin, 2011). However, the functional form of the schemes an d their parameters, which represents smallscale effects, a re un known and must be inferred from observations. The develop ment of automatic statistical learning techniques to ident ify an optimal stochastic parameterization and estimate its para meters is, therefore, highly desirable. One standard methodology to estimate physical model pa rameters from observations in data assimilation technique s, such as the traditional Kalman Ô¨Ålter, is to augment the state space with the parameters (Jazwinski, 1970). This methodol  ogy has also been implemented in the ensemblebased Kalman Ô¨Ålter (see e.g. Anderson 2001). The parameters are constrai ned through their correlations with the observed variables. The collapse of the parameter posterior distribution found in both ensemble Kalman Ô¨Ålters (Delsole and Yang, 2010; Ruiz et al., 2013a;b; Santitissadeekorn and Jones, 2015) and par ti cle Ô¨Ålters (West and Liu, 2001) is a major contention point when one is interested in estimating stochastic parameters of nonlinear dynamical models. Hereinafter, we refer as stoch as tic parameters to those that deÔ¨Åne the covariance of a Gaussi an stochastic process (Delsole and Yang, 2010). In other words , the sequential Ô¨Ålters are, in principle, able to estimate deter ministic physical parameters, the mean of the parameter posterior di stribution, through the augmented statespace procedure, but t hey are unable to estimate stochastic parameters of the model, b e cause of the collapse of the corresponding posterior distri bution. Using the Kalman Ô¨Ålter with the augmentation method, Delsol e and Yang (2010) proved analytically the collapse of the para m eter covariance in a Ô¨Årstorder autoregressive model. They pro posed a generalized maximum likelihood estimation using an approximate sequential method to estimate stochastic para m eters. Carrassi and Vannitsem (2011) derived the evolution of the augmented error covariance in the extended Kalman Ô¨Ålter using a quadratic in time approximation that mitigates the c ol lapse of the parameter error covariance. Santitissadeekor n and Jones (2015) proposed a particle Ô¨Ålter blended with an ensem ble Kalman Ô¨Ålter and use a random walk model for the parameters. This technique was able to estimate stochastic parameters i n the Ô¨Årstorder autoregressive model, but a tunable parameter i n the random walk model needs to be introduced. The ExpectationMaximization (EM) algorithm (Dempster et al., 1977; Bishop, 2006) is a widely used methodology to maximize the likelihood function in a broad spectrum of ap plications. One of the advantages of the EM algorithm is that its implementation is rather straigthforward. Wu (1983) sh owed that if the likelihood is smooth and unimodal, the EM algorit hm converges to the unique maximum likelihood estimate. Accel  erations of the EM algorithm have been proposed for its use in machine learning (Neal and Hinton, 1999). Recently, it wa s used in an application with a highly nonlinear observation o p erator (Tandeo et al., 2015). The EM algorithm was able to es timate subgridscale parameters with good accuracy while s tan dard ensemble Kalman Ô¨Ålter techniques failed. It has also be en applied to the Lorenz63 system to estimate model error cova ri ance (Dreano et al., 2017). In this work, we combine for stochastic parameterization identiÔ¨Åcation these two independent methodologies: the en sem ble Kalman Ô¨Ålter (Evensen, 1994; 2003) for the stateestima te with maximum likelihood estimators, the EM (Dempster et al. , 1977; Bishop, 2006) and the NewtonRaphson (NR) algorithms (Capp¬¥ e et al., 2005). The derivation of the technique is ex plained in detail and simple terms so that readers that are not from those communities can understand the basis of the methodologies, how they can be combined, and hopefully fore  see potential applications in other geophysical systems. T he learning statistical techniques are suitable to infer the f unctional form and the parameter values of stochastic parameterizati ons in chaotic spatiotemporal dynamical systems. They are eva lSTOCHASTIC PARAMETERIZATION IDENTIFICATION USING MAXIMU M LIKELIHOOD METHODS 3 uated here on a twoscale spatially extended chaotic dynam ical system (Lorenz, 1996) to estimate deterministic physi cal parameters, together with additive and multiplicative sto chastic parameters. Pulido et al. (2016) evaluated methods based on the EnKF alone to estimate subgridscale parameters in a twosc ale system: they showed that an ofÔ¨Çine estimation method is able to recover the functional form of the subgridscale parameter iza tion, but none of the methods was able to estimate the stochas tic component of the subgridscale effects. In the present work , the results show that the NR and EM techniques are able to uncover the functional form of the subgridscale parameterization while succesfully determining the stochastic parameters of the r epre sentation of subgridscale effects. This work is organized as follows. Section 2 brieÔ¨Çy intro duces the EM algorithm and derives the marginal likelihood o f the data using a Bayesian perspective. The implementation o f the EM and NR likehood maximization algorithms in the con text of data assimilation using the ensemble Kalman Ô¨Ålter is also discussed. Section 3 describes the experiments which a re based on the one and twoscale Lorenz96 systems. The forme r includes simple deterministic and stochastic parameteriz ations to represent the effects of the smaller scale to mimic the two  scale Lorenz96 system. Section 4 focuses on the results: Se c tion 4.1 discusses the experiments for the estimation of mod el noise. Section 4.2 shows the results of the estimation of det er ministic and stochastic parameters in a perfectmodel scen ario. Section 4.3 shows the estimation experiments for an imperfe ct model. The conclusions are drawn in Section 5. 2. Methodology "
390,Releasing Graph Neural Networks with Differential Privacy Guarantees.txt,"With the increasing popularity of Graph Neural Networks (GNNs) in several
sensitive applications like healthcare and medicine, concerns have been raised
over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to
privacy attacks, such as membership inference attacks, even if only blackbox
access to the trained model is granted. To build defenses, differential privacy
has emerged as a mechanism to disguise the sensitive data in training datasets.
Following the strategy of Private Aggregation of Teacher Ensembles (PATE),
recent methods leverage a large ensemble of teacher models. These teachers are
trained on disjoint subsets of private data and are employed to transfer
knowledge to a student model, which is then released with privacy guarantees.
However, splitting graph data into many disjoint training sets may destroy the
structural information and adversely affect accuracy. We propose a new
graph-specific scheme of releasing a student GNN, which avoids splitting
private training data altogether. The student GNN is trained using public data,
partly labeled privately using the teacher GNN models trained exclusively for
each query node. We theoretically analyze our approach in the R\`{e}nyi
differential privacy framework and provide privacy guarantees. Besides, we show
the solid experimental performance of our method compared to several baselines,
including the PATE baseline adapted for graph-structured data. Our anonymized
code is available.","In the past few years, Graph Neural Networks (GNNs) have gained much attention due to their superior performance in a wide range of applications, such as social networks [ 8], biology [ 13], medicine [ 2], Woodstock ‚Äô22, June 03‚Äì05, 2022, Woodstock, NY ¬©2022 Association for Computing Machinery. This is the author‚Äôs version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Woodstock ‚Äô22: ACM Symposium on Neural Gaze Detection, June 03‚Äì05, 2022, Woodstock, NY , https://doi.org/10.1145/1122445.1122456.and molecular chemistry [ 15]. Specifically, GNNs achieved stateof theart results in various graphbased learning tasks, such as node classification, link prediction, and community detection. Realworld graphs, such as medical and economic networks, are associated with sensitive information about individuals and their activities and can not always be made public. Releasing pretrained models provides an opportunity for using the private knowledge beyond company boundaries [ 2]. However, recent works have shown that GNNs are vulnerable to membership inference attacks [ 9,18]. Specifically, membership inference attacks allow to identify which data points have been used for training the model. In general, GNNs are more vulnerable to such attacks as compared to traditional knowledge due to their encoding of the graph structure within the model itself [18]. In addition, the current legal data protection policies to pre serve user privacy highlights a compelling need to develop privacy preserving GNNs . In this work, we propose our framework PrivGnn , which builds on the rigid guarantees of differential privacy (DP), allowing us to protect the sensitive data while releasing the trained GNN model. Differential privacy [ 5] is one of the most popular approaches for releasing data statistics or the trained model while concealing the information about individuals present in the dataset. Roughly speak ing, the key idea of DP is that if we query a dataset containing ùëÅ individuals, the query‚Äôs result will be almost indistinguishable (in a probabilistic sense) from the result of querying a neighboring dataset with one less or one more individual. Hence, each indi vidual‚Äôs privacy is guaranteed with a specific probability. Such probabilistic indistinguishability is usually achieved by adding a sufficient amount of noise to the query result. The seminal work of Abadi et al . [1]proposed differential private stochastic gradient descent (DPSGD) algorithm to achieve differen tial privacy guarantees for deep learning models. Specifically, in each step of the training, DPSGD adds appropriate noise to the ‚Ñì2clipped gradients during the stochastic gradient descent opti mization. The incurred privacy budget ùúÄfor training is computed using the moment‚Äôs accountant technique that keeps track of the privacy loss across multiple invocations of the noise addition mech anism applied to random subsets of the input dataset [1]. Besides the slow training process of DPSGD, the injected noise is proportional to the number of training epochs, which further degrades performance. More importantly, the privacy guarantee for DPSGD does not trivially hold for graph data and GNN mod els [11]. While DPSGD is designed for independent and identically distributed data (i.i.d.), the nodes in graph data are related. GNNs use a messagepassing algorithm to exchange information among connected nodes [ 8]. Therefore, the privacy guarantee of DPSGD,arXiv:2109.08907v1  [cs.LG]  18 Sep 2021Woodstock ‚Äô22, June 03‚Äì05, 2022, Woodstock, NY Iyiola E. Olatunji, Thorben Funke, and Megha Khosla Private GNNPublic GNN+  NoiseRelease ModelQueryPrivate DataPoisson SamplingKNN from Poisson SubsampleTrain PseudolabelPublic Data TrainPRIV ATE PUBLIC PUBLIC Figure 1: Workflow of PrivGnn . We are given two corresponding datasets, labeled private data and unlabeled public data . PrivGnn starts by sampling the private data using Poisson sampling, Def. 7, to retrieve a subset of the private data. We then obtain the Knearest neighbor nodes based on the features of the public query node. The teacher GNN model is trained on the graph induced on Knearest neighbors. We obtain a pseudolabel for the query node by adding independent noise to the output posterior. The pseudolabel and data from the public graph are used in training the student model, which is then released. which requires a set of i.i.d. examples to form batches and lots, does not hold for GNNs and graph data [11]. To work around DPSGD‚Äôs dependency of the training proce dure, such as the number of epochs, Papernot et al . [19] proposed Private Aggregation of Teacher Ensembles (PATE). PATE leverages a large ensemble of teacher models trained on disjoint subsets of private data to transfer knowledge to a student model, which is then released with privacy guarantees. However, splitting graph data into many disjoint training sets destroys the structural information and adversely affects accuracy. Since existing DP methods are not directly applicable to GNNs, we propose a privacypreserving framework, PrivGnn , for releas ing GNN models with differential privacy guarantees. Similar to PATE‚Äôs assumptions, we are given two graphs: a labeled private graph and an unlabeled public graph. PrivGnn leverages the par adigm of knowledge distillation. The knowledge of the teacher model trained on the private graph is transferred to the student model trained only on the public graph in a differential privacy manner. PrivGnn achieves practical privacy guarantees by com bining the studentteacher training with two noise mechanisms: random subsampling using Poisson sampling andnoisy la beling mechanism to obtain pseudolabels for public nodes. In particular, we release the student GNN model, which is trained using a small number of public nodes labeled using the teacher GNN models developed exclusively for each public query node. We present a R√®nyi differential privacy (RDP) analysis of our approach and provide tight bounds on incurred privacy budget or privacy loss. Figure 1 shows an overview of our PrivGnn approach. To summarize, our key contributions are as follows. (1)We propose PrivGnn , a novel privacypreserving frame work for releasing GNN models via differential privacy. By leveraging the studentteacher training paradigm, PrivGnn is robust to attacks on GNN models, including MI attack and model stealing attacks. (2)We derive tight privacy guarantees employing the theoreti cal results of RDP for Poisson subsampled mechanisms and advanced composition theorem for RDP. Ours is the first work utilizing the RDP framework for analyzing the privacy of GNNs.(3)We experimentally show that PrivGnn achieves close to optimal accuracy of the nonprivate version with practical privacy guarantees (singledigit ùúÄ). 2 RELATED WORKS "
422,Adaptive Regularization of Labels.txt,"Recently, a variety of regularization techniques have been widely applied in
deep neural networks, such as dropout, batch normalization, data augmentation,
and so on. These methods mainly focus on the regularization of weight
parameters to prevent overfitting effectively. In addition, label
regularization techniques such as label smoothing and label disturbance have
also been proposed with the motivation of adding a stochastic perturbation to
labels. In this paper, we propose a novel adaptive label regularization method,
which enables the neural network to learn from the erroneous experience and
update the optimal label representation online. On the other hand, compared
with knowledge distillation, which learns the correlation of categories using
teacher network, our proposed method requires only a minuscule increase in
parameters without cumbersome teacher network. Furthermore, we evaluate our
method on CIFAR-10/CIFAR-100/ImageNet datasets for image recognition tasks and
AGNews/Yahoo/Yelp-Full datasets for text classification tasks. The empirical
results show significant improvement under all experimental settings.","In recent years, supervised neural networks have been widely used in a variety of deep learning tasks with backpropagation technology. It is wellknown that the cross entropy loss function shows remarkable high performances in various tasks in practice. In simple terms, the deÔ¨Ånition of the cross entropy loss function is the cross entropy between the predicted output of the neural network and the onehot encoded of labels. The onehot encoded label is a group of bits among which the legal combinations of values are only those with a single 1(represents groundtruth) and all the others 0. This means that the cross entropy loss based on onehot encoded labels only focuses on the correctness of the groundtruth category. Malcolm Forbes once said that failure is success if we learn from it. In our approach, we make neural networks learn from previous erroneous experience and beneÔ¨Åt future learning. SpeciÔ¨Åcally, our proposed adaptive label regularization enables the neural network to focus not only the correctness but also the incorrectness during the training phase. Not coincidentally, previous researches [ 7,20,22,31,30,15,17,1] have suggested that the cross entropy loss based on onehot encoded labels may not be optimal in the classiÔ¨Åcation task. [ 22,26] aimed to regularize the neural network by adding a stochastic perturbation to labels. In addition, [ 7] shows that softening onehot encoded labels could provide more knowledge of the relevance of labels, which refers to as dark knowledge. For example, there exist lots of similarities between the images labeled as ""cat"" and the images labeled as ""dog"". The images of both categories contain four legs and one head, while the images labeled as ""plane"" are signiÔ¨Åcantly different from them. Since onehot corresponding author. Preprint. Under review.arXiv:1908.05474v1  [cs.LG]  15 Aug 2019encoded labels are orthogonal each other, they can not indicate the relevance of the labels (""cat"" and ""dog"" are independent of each other). Therefore, the soft label was proposed to solve this defect. The essence of the soft label is to soften the onehot encoded labels to vectors of category distribution. But it is very challenging to obtain good soft labels since they are difÔ¨Åcult to accurately express using priori knowledge. And another challenge is how to embed soft labels into the training phase of neural networks after getting good soft labels. To address the above challenges, [ 7] indicated that the soft label, i.e.the category distribution, was distilled by a cumbersome teacher network using qi=exp(zi=T)PK j=1exp(zj=T), whereKis the number of categories and Tis a temperature, using a higher value for Tproduces a softer probability. By learning from soft labels, the student network can compress parameters and even improve performance. It is worth noting that the existing researches on knowledge distillation [ 7,31,30,15] just added the soft loss to the original loss after obtaining a good category distribution. SpeciÔ¨Åcally, they used the KL divergence (equivalent to the cross entropy) between the soft labels and the output of the neural network (soft loss) as part of the loss function in order to supplement the cross entropy with onehot encoded labels (hard loss). The form is typically as L= (1"
351,Self-Supervised Noisy Label Learning for Source-Free Unsupervised Domain Adaptation.txt,"It is a strong prerequisite to access source data freely in many existing
unsupervised domain adaptation approaches. However, source data is agnostic in
many practical scenarios due to the constraints of expensive data transmission
and data privacy protection. Usually, the given source domain pre-trained model
is expected to optimize with only unlabeled target data, which is termed as
source-free unsupervised domain adaptation. In this paper, we solve this
problem from the perspective of noisy label learning, since the given
pre-trained model can pre-generate noisy label for unlabeled target data via
directly network inference. Under this problem modeling, incorporating
self-supervised learning, we propose a novel Self-Supervised Noisy Label
Learning method, which can effectively fine-tune the pre-trained model with
pre-generated label as well as selfgenerated label on the fly. Extensive
experiments had been conducted to validate its effectiveness. Our method can
easily achieve state-of-the-art results and surpass other methods by a very
large margin. Code will be released.","In practical applications, a deep model trained on source domain is usually deployed in edge devices to test unlabeled images from unknown target domain. The data distribu tion of target domain is rather different from source domain due to agnostic domain shift, such as diverse illumination, complex weather, etc. This is the main factor of model performance degradation in realworld scenarios. Recently, there are more and more researchers delving into unsuper *Equal contribution1Zhejiang University, Hangzhou, China 2Hikvision Research Institute, Hangzhou, China3Fuzhou Uni versity, Fuzhou, China. Correspondence to: Yueting Zhuang <yzhuang@zju.edu.cn >. Copyright 2021 by the author(s). Figure 1. Sourcefree UDA can be viewed as pseudo label de noising. Here (Xs;Ys)denote the source data and the annotated label.Xtis the unlabeled target data. The pregenerated pseudo labelYtis denoised into a cleaner one Y0 tafter sourcefree UDA. vised domain adaptation (UDA) to address this problem. Most of previous UDA methods aim to align the labeled source data and unlabeled target data in a common represen tation space, so that the classiÔ¨Åer trained on source domain can be well generalized to target domain (Long et al., 2015; Sun & Saenko, 2016; Haeusser et al., 2017). These vanilla UDA methods always assume that source data is accessible and thus can be used with target data for domain transferring. However, it is illsuited in some practical applications, e:g:, source data is inaccessible and only the model pretrained on source domain is available due to the expensive data transmission and data privacy protection. Such situation is termed as sourcefree UDA, where only unlabeled tar get data is provided for model optimization (Li et al., 2020; Liang et al., 2020; Li et al., 2021). Note that we only discuss image classiÔ¨Åcation task in this paper. How to solve sourcefree UDA? Although the pretrained model performs not so well on target domain, it still contains informative cues of the task. Naturally, it can be exploited to pregenerate pseudo labels for target data via networkarXiv:2102.11614v1  [cs.CV]  23 Feb 2021SelfSupervised Noisy Label Learning for SourceFree Unsupervised Domain Adaptation inference. Inevitably, the pregenerated labels are not ex actly correct, where the falselabeled ones can be viewed as noisy labels. In this way, as shown in Fig.1, source free UDA can also be regarded as another form of noisy label learning. From this viewpoint, we propose a simple yet effective approach named SelfSupervised Noisy La bel Learning (SSNLL) to address this problem, which is strongly inspired by unsupervised image classiÔ¨Åcation and noisy label learning. First of all, we walk from Unsupervised Image ClassiÔ¨Åcation (UIC) method (Chen et al., 2020b), which is an unsuper vised technique to train the classiÔ¨Åcation network by self generating pseudo label. Similar to other selfsupervised learning methods (He et al., 2020; Chen et al., 2020a; Caron et al., 2018), a critical step is to avoid model collapse of classifying all images into one category. Besides, the clas siÔ¨Åcation results achieved in a totally unsupervised way cannot be directly used in downstream tasks. Hence, we consider to incorporate noisy label learning to eliminate these problems. Since the pregenerated noisy label contains informative cues of the task, it can regularize UIC towards a taskspeciÔ¨Åc optimization direction. Considering what if we can split tar get data into a truelabeled part and a falselabeled part? The truelabeled part trained with pregenerated labels can regu larize the falselabeled part trained with selfgenerated la bels. Only when the selfgenerated label in falselabeled part matches to the pregenerated pseudo label in truelabeled part with consistent semantic information can it achieve optimal solution. To achieve this objective, inspired by the smallloss trick used in noisy label learning (Jiang et al., 2018; Shu et al., 2019; Han et al., 2018), we split the target data Xtinto a cleaner partXclwith smaller loss and a noisier part Xnowith greater loss with respect to the pregenerated label. To avoid the aforementioned model collapse problem, we further de velop it into a labelwise dataset splitting method, which ensures no empty classes in the cleaner part Xcl. After that, we sample images from XclandXnouniformly to train the network with pregenerated label and selfgenerated label, respectively. As the training goes, the loss with respect to the Ô¨Åxed pregenerated label will get smaller in truelabeled samples and get larger in falselabeled samples. To fully exploit this positive feedback, the dataset splitting opera tion and network training operation are alternated epoch by epoch so as to progressively boost the performance. Actually, UIC and noisy label learning in our approach are promoted by each other. The former one can help reÔ¨Åne the pregenerated noisy label, whilst the latter one can regularize selfgenerated label and prevent UIC from model collapsing and class mismatching. Besides, in order to initially reduce the noise ratio, we also introduce two label denoising tricksduring the process of pregenerating pseudo labels, includ ing Adaptive Batch Normalization (AdaBN) (Li et al., 2016) and Deep Transfer Clustering (DTC) (Kai et al., 2019). Our method can wellsolve the sourcefree UDA problem. Extensive experiments had been carried out on several pop ular UDA benchmarks, which show our method can easily achieve stateoftheart results on these benchmarks. It sur passes other methods even the source databased ones by a very large margin. For instance, on VisDAC(Peng et al., 2017), one of the most challenging datasets in UDA, our method can achieve 85.8% accuracy and surpass the second place more than 3% accuracy. 2. Related Work "
189,Suppressing Mislabeled Data via Grouping and Self-Attention.txt,"Deep networks achieve excellent results on large-scale clean data but degrade
significantly when learning from noisy labels. To suppressing the impact of
mislabeled data, this paper proposes a conceptually simple yet efficient
training block, termed as Attentive Feature Mixup (AFM), which allows paying
more attention to clean samples and less to mislabeled ones via sample
interactions in small groups. Specifically, this plug-and-play AFM first
leverages a \textit{group-to-attend} module to construct groups and assign
attention weights for group-wise samples, and then uses a \textit{mixup} module
with the attention weights to interpolate massive noisy-suppressed samples. The
AFM has several appealing benefits for noise-robust deep learning. (i) It does
not rely on any assumptions and extra clean subset. (ii) With massive
interpolations, the ratio of useless samples is reduced dramatically compared
to the original noisy ratio. (iii) \pxj{It jointly optimizes the interpolation
weights with classifiers, suppressing the influence of mislabeled data via low
attention weights. (iv) It partially inherits the vicinal risk minimization of
mixup to alleviate over-fitting while improves it by sampling fewer
feature-target vectors around mislabeled data from the mixup vicinal
distribution.} Extensive experiments demonstrate that AFM yields
state-of-the-art results on two challenging real-world noisy datasets: Food101N
and Clothing1M. The code will be available at
https://github.com/kaiwang960112/AFM.","In recent years, deep neural networks (DNNs) have achieved great success in var ious tasks, particularly in supervised learning tasks on largescale image recog nition challenges, such as ImageNet [6] and COCO [21]. One key factor that ?Equallycontributed rst authors.yCorresponding author (yu.qiao@siat.ac.cn)arXiv:2010.15603v1  [cs.CV]  29 Oct 20202 X. Peng, K. Wang, Z. Zeng, Q. Li, J. Yang and Y. Qiao Fig. 1: Suppressing mislabeled samples by grouping and selfattention mixup. Dierent colors and shapes denote given labels and ground truths. Thick and thin lines denote high and low attention weights, respectively. A0;A10;B0, and B10are supposed to be mislabeled samples, and can be suppressed by assigning low interpolation weights in mixup operation. drives impressive results is the large amount of welllabeled images. However, highquality annotations are laborious and expensive, even not always available in some domains. To address this issue, an alternative solution is to crawl a large number of web images with tags or keywords as annotations [8,19]. These annotations provide weak supervision, which are noisy but easy to obtain. In general, noisy labeled examples hurt generalization because DNNs eas ily overt to noisy labels [7,30,2]. To address this problem, it is intuitive to develop noisecleaning methods which aim to correct the mislabeled samples either by joint optimization of classication and relabeling [31] or by iterative selflearning [11]. However, the noisecleaning methods often suer from the main diculty in distinguishing mislabeled samples from hard samples. Another solu tion is to develop noiserobust methods which aims to reduce the contributions of mislabeled samples for model optimization. Along this solution, some methods estimate a matrix for label noise modeling and use it to adapt output proba bilities and loss values [30,35,26]. Some others resort to curriculum learning [4] by either designing a stepwise easytohard strategy for training [10] or intro ducing an extra MentorNet [12] for sample weighting. However, these methods independently estimate the importance weights for individuals which ignore the comparisons among dierent samples while they have been proven to be the key of humans to perceive and learn novel concepts from noisy input images [29]. Some other solutions follow semisupervised conguration where they assume a small manuallyveried set can be used [20,32,15,17]. However, this assump tion may be not supported in realworld applications. With the Vicinal Risk Minimization(VRM) principle, mixup [36,33] exploits a vicinal distribution forSuppressing Mislabeled Data via Grouping and SelfAttention 3 sampling virtual sampletarget vectors, and proves its robustness for synthetic noisy data. But its eectiveness is limited in realworld noisy data [1]. In this paper, we propose a conceptually simple yet ecient training block, termed as Attentive Feature Mixup (AFM), to suppress mislabeled data thus to make training robust to noisy labels. The AFM is a plugandplay block for training any networks and is comprised of two crucial parts: 1) a Groupto Attend (GA) module that rst randomly groups a minibatch images into small subsets and then estimates sample weights within those subsets by an attention mechanism, and 2) a mixup module that interpolates new features and soft labels according to selfattention weights. Particularly, for the GA module, we evaluate three feature interactions to estimate groupwise attention weights, namely con catenation, summary, and elementwise multiplication. The interpolated samples and original samples are respectively fed into an interpolation classier and a normal classier. Figure 1 illustrates how AFM suppress mislabeled data. Gen erally, there exists two main types of mixup: intraclass mixup (Figure 1 (c)) and interclass mixup (Figure 1 (b)). For both types, the interpolations between mis labeled samples and clean samples may become useful for training with adaptive attention weights, i.e. low weights for the mislabeled samples and high weights for the clean samples.In other words, our AFM hallucinates numerous useful noisyreduced samples to guide deep networks learn better representations from noisy labels. Overall, as a noisyrobust training method, our AFM is promising in the following aspects. {It does not rely on any assumptions and extra clean subset. {With AFM, the ratio of harmful noisy interpolations ( i.e. between noisy samples) over all interpolations is largely less than the original noisy ratio. {It jointly optimizes the mixup interpolation weights and classier, suppress ing the in uence of mislabeled data via low attention weights. {It partially inherits the vicinal risk minimization of mixup to alleviate over tting while improves it by sampling less featuretarget vectors around mis labeled data from the mixup vicinal distribution. We validate our AFM on two popular realworld noisylabeled datasets: Food101N [15] and Clothing1M [35]. Experiments show that our AFM outperforms recent stateoftheart methods signicantly with accuracies of 87.23 % on Food101N and82.09 % on Clothing1M. 2 Related Work "
381,DeepPFCN: Deep Parallel Feature Consensus Network For Person Re-Identification.txt,"Person re-identification aims to associate images of the same person over
multiple non-overlapping camera views at different times. Depending on the
human operator, manual re-identification in large camera networks is highly
time consuming and erroneous. Automated person re-identification is required
due to the extensive quantity of visual data produced by rapid inflation of
large scale distributed multi-camera systems. The state-of-the-art works focus
on learning and factorize person appearance features into latent discriminative
factors at multiple semantic levels. We propose Deep Parallel Feature Consensus
Network (DeepPFCN), a novel network architecture that learns multi-scale person
appearance features using convolutional neural networks. This model factorizes
the visual appearance of a person into latent discriminative factors at
multiple semantic levels. Finally consensus is built. The feature
representations learned by DeepPFCN are more robust for the person
re-identification task, as we learn discriminative scale-specific features and
maximize multi-scale feature fusion selections in multi-scale image inputs. We
further exploit average and max pooling in separate scale for person-specific
task to discriminate features globally and locally. We demonstrate the
re-identification advantages of the proposed DeepPFCN model over the
state-of-the-art re-identification methods on three benchmark datasets:
Market1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%,
64.3%, and 52.6% respectively on these benchmark datasets.","Person reidentiÔ¨Åcation detects whether a person of interest has been observed in another place (time) by a different camera [ 1]. In many scenarios, people appearing in one camera do not necessarily appear in another camera and sometimes the camera view may include people that have never appeared in any other camera as well. Therefore, it is better to treat person reidentiÔ¨Åcation as a veriÔ¨Åcation problem [ 2]. Person reidentiÔ¨Åcation is a naturally challenging task because correctly matching two images of the same person is difÔ¨Åcult under extensive appearance changes, such as human pose, illumination, occlusion, background clutter, (non)uniform clothing, and camera viewangle [ 2]. It has applications in tracking a particular person across these cameras, tracking the trajectory of a person, real time surveillance, and forensic and security applications. In this paper, we propose Deep Parallel Feature Consensus Net (DeepPFCN) a novel network architecture that learns multiscale person appearance features using convolutional neural networks (CNN) [ 3] and factorizes the visual appearance of a person into latent discriminative factors at multiple semantic levels [ 4]. DeepPFCN is a combination of the above mentioned two architectures, which are orthogonal to each other as mentioned in [ 4]. DeepPFCN focuses on fusing both automated discovery of latent appearance factors and fusing image resolutions. DeepPFCN deploys a multiloss concurrent supervision mechanism. This allows enforcing and improving scalespeciÔ¨Åc feature learning.arXiv:1911.07776v1  [cs.CV]  18 Nov 2019APREPRINT  NOVEMBER 19, 2019 DeepPFCN is evaluated on three person reidentiÔ¨Åcation benchmark datasets  Market1501 [ 1], DukeMTMCreID [ 5], CUHK03 [ 6]. Extensive experiments and ablation study have been conducted on these datasets. In particular, we achieve the mAP scores of 75.8%, 64.3%, and 52.6% on the above mentioned benchmark datasets, which is observed to be better than the stateoftheart methods by 1.5%, 1.5% and 3.4%, respectively. This paper is organized as follows. In section 3, we describe the components of DeepPFCN architecture  (a) modiÔ¨Åed multilevel factor net as base model and (b) multiscale consensus learning with backpropagation. We combine these methods, which achieves the stateoftheart performance for person reidentiÔ¨Åcation. In section 4, we work with the following datasets: Market1501 [ 1], DukeMTMCreID [ 5] and CUHK03 [ 6] and explain the evaluation metrics, data augmentation, training and evaluation in detail. In section 5, we describe the experiments and present the ablation study conducted on these datasets. Finally, section 6 concludes the paper. 2 Related Work "
495,Diversity and Generalization in Neural Network Ensembles.txt,"Ensembles are widely used in machine learning and, usually, provide
state-of-the-art performance in many prediction tasks. From the very beginning,
the diversity of an ensemble has been identified as a key factor for the
superior performance of these models. But the exact role that diversity plays
in ensemble models is poorly understood, specially in the context of neural
networks. In this work, we combine and expand previously published results in a
theoretically sound framework that describes the relationship between diversity
and ensemble performance for a wide range of ensemble methods. More precisely,
we provide sound answers to the following questions: how to measure diversity,
how diversity relates to the generalization error of an ensemble, and how
diversity is promoted by neural network ensemble algorithms. This analysis
covers three widely used loss functions, namely, the squared loss, the
cross-entropy loss, and the 0-1 loss; and two widely used model combination
strategies, namely, model averaging and weighted majority vote. We empirically
validate this theoretical analysis with neural network ensembles.","Ensemble methods are one of the most widely used and studied techniques in machine learning (Hansen and Salomon, 1990; Breiman, 1996, 2001a). It has been successfully applied in many realworld problems (Gir shick et al., 2014; Wang et al., 2012; Zhou et al., 2014; Ykhlef and Bouchara, 2017) and is usually part ofthe winning strategies in many machine learning com petitions (e.g., Chen and Guestrin, 2016; Hoch, 2015; Puurula et al., 2014; Stallkamp et al., 2012). Recently, ensembles are also becoming very popular to improve uncertainty modeling in deep neural networks (Laksh minarayanan et al., 2017; Wen et al., 2019; Maddox et al., 2019; Wenzel et al., 2020). Ensembles are created by combining several individ ual predictors. It is widely accepted (Dietterich, 2000; Lu et al., 2010) that the prediction performance of an ensemble jointly depends of the individual perfor mance and the diversity of its individual members. In tuitively speaking, a set of predictors is diverse when their predictions do not coincide on all the samples. We know that when classiers are diverse, they tend to make independent errors, therefore when they are aggregated, their errors tend to cancel out (Berend and Kontorovich, 2016), which improves the ensemble prediction. For this reason, diversity has long been recognized as a key factor in ensemble performance (Kuncheva and Whitaker, 2003; Cunningham and Car ney, 2000; Brown et al., 2005). The same cancela tion of errors eect happens in the case of neural net work ensembles (Hansen and Salomon, 1990; Lee et al., 2016; Lakshminarayanan et al., 2017) where heuristic measures of diversity are usually analyzed to get in sights of the ensemble learning algorithms (Fort et al., 2019; Wen et al., 2019; Wenzel et al., 2020). Unfortunately, there is a lack of consensus surround ing the underlying theory that can explain the role of diversity in the generalization performance of ensem bles. The error rate of an ensemble and an individual predictor, for example, is well dened by the use of a loss function, but there is no wellestablished deni tion of diversity (Kuncheva and Whitaker, 2003). And it is not well known how exactly the diversity among ensemble members aects the generalization error of the ensemble. In this work, we introduce a novel theoretical frame work that explains the relationship between diversity and the generalization performance of an ensemble. This theoretical framework is derived from previouslyarXiv:2110.13786v2  [cs.LG]  16 Feb 2022Diversity and Generalization in Neural Network Ensembles published results with no direct connection among them (Krogh and Vedelsby, 1994; Masegosa, 2020; Masegosa et al., 2020). The main contribution of our work is to nd a theoretically sound way to combine these previous results in a single theoretical framework that explains the role of diversity in the generalization performance of a wide range of dierent ensembles. In our opinion, this general framework could poten tially help the machine learning community to have a better understanding of the underlying tradeos that have to be considered when designing novel ensemble learning algorithms, specially in the context of neural networks. The detailed contributions of this work are the followings: a general measure of ensemble diver sity; a theoretical analysis that shows how the corre lation among ensemble members aects diversity; the exact tradeo that exists between this diversity mea sure, the performance of the individual predictors and the generalization error of the ensemble; an analysis of the strategies used by most of current neural net work ensemble learning algorithms to promote diver sity; and, nally, an empirical evaluation of this theo retical framework. This analysis covers model averag ing and weighted majority vote ensembles under the crossentropy loss, square error and 01 loss. 2 Related Work "
599,End-to-end Semantic Role Labeling with Neural Transition-based Model.txt,"End-to-end semantic role labeling (SRL) has been received increasing
interest. It performs the two subtasks of SRL: predicate identification and
argument role labeling, jointly. Recent work is mostly focused on graph-based
neural models, while the transition-based framework with neural networks which
has been widely used in a number of closely-related tasks, has not been studied
for the joint task yet. In this paper, we present the first work of
transition-based neural models for end-to-end SRL. Our transition model
incrementally discovers all sentential predicates as well as their arguments by
a set of transition actions. The actions of the two subtasks are executed
mutually for full interactions. Besides, we suggest high-order compositions to
extract non-local features, which can enhance the proposed transition model
further. Experimental results on CoNLL09 and Universal Proposition Bank show
that our final model can produce state-of-the-art performance, and meanwhile
keeps highly efficient in decoding. We also conduct detailed experimental
analysis for a deep understanding of our proposed model.","Semantic role labeling (SRL), as one of the core tasks to identify the semantic predicates in text as well as their se mantic roles, has sparked much interest in natural language processing (NLP) community (Pradhan et al. 2005; Lei et al. 2015; Xia et al. 2019). SRL is a shallow semantic parsing, aiming to uncover the predicateargument structures, such as ‚Äòwho did what to whom, when and where ‚Äô, The task can be beneÔ¨Åcial for a range number of downstream tasks, such as information extraction (Christensen et al. 2011; Bastianelli et al. 2013), question answering (Shen and Lapata 2007; Be rant et al. 2013) and machine translation (Xiong, Zhang, and Li 2012; Shi et al. 2016). Traditionally, SRL is accomplished via two pipeline steps: predicate identiÔ¨Åcation (Scheible 2010) and argument role labeling (Pradhan et al. 2005). More recently, there is grow ing interest in endtoend SRL, which aims to achieve both two subtasks by a single model (He et al. 2018). Given a sentence, the goal is to recognize all possible predicates to gether with their arguments jointly. Figure 1 shows an exam ple of endtoend SRL. The endtoend joint architecture can *Corresponding author Copyright ¬© 2021, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved. A1ATMPA3 A1 When victims and witnesses stay silent , nothing changes (2)(1) stay.01 change.01 1                2             3                 4                5            6       7          8                   9Figure 1: An example of endtoend SRL, where two propo sitions are shown in one sentence. greatly alleviate the error propagation problem, and mean while simplify the overall decoding process, thus receives increasing attention. Graphbased models have been the mainstream methods to endtoend SRL, which are achieved by enumerating all the possible predicates and their argu ments exhaustively (He et al. 2018; Cai et al. 2018; Li et al. 2019). Their results show that endtoend modeling can ob tain better SRL performance. Alternatively, the transitionbased framework offers an other solution for endtoend modeling, which is totally or thogonal to the graphbased models. Transitionbased mod els have been widely exploited for endtoend sequence la beling (Zhang and Clark 2010; Lyu, Zhang, and Ji 2016; Zhang, Zhang, and Fu 2018), structural parsing (Zhou et al. 2015; Dyer et al. 2015; Yuan, Jiang, and Tu 2019) and rela tion extraction (Wang et al. 2018; Zhang et al. 2019), which are closely related to SRL. These models can also achieve very competitive performances for a range of tasks, and meanwhile maintain high efÔ¨Åciencies with lineartime de coding complexity. In this work, we present the Ô¨Årst work of exploiting the neural transitionbased architecture to endtoend SRL. The model handles SRL incrementally by predicting a sequence of transition actions step by step, which are used to detect all predicates as well as their semantic roles in a given sentence. The two subtasks of endtoend SRL, predicate identiÔ¨Åca tion and argument role labeling, are performed mutually in a single model to make full interactions of them. For argument role labeling, the recognition is conducted in a closeÔ¨Årst way, where the nearpredicate roles are processed Ô¨Årst. The partial outputs of the incremental processing are denoted as transition states. In addition, we suggest explicit highorder compositions to enhance our transitionbased model, leverarXiv:2101.00394v1  [cs.CL]  2 Jan 2021aging the precedent partiallyrecognized argumentpredicate structures for the current action classiÔ¨Åcation. Our neural transition system is built upon standard embeddingbased word representations, and then is im proved with dependencyaware representations by using re cursive TreeLSTM (Tai, Socher, and Manning 2015). Con cretely, we embed the surface words, characters, POS tags and dependency structures as input representations. Dur ing decoding, we represent the transition states by using standard BiLSTM (Hochreiter and Schmidhuber 1997) and StackLSTM (Dyer et al. 2015) to encode the elements in buffers and stacks, respectively. Finally, we predict transi tion actions incrementally based on the state representations. We conduct experiments on dependencybased SRL benchmarks, including CoNLL09 (Haji Àác et al. 2009) for the English language, and Universal Proposition Bank (Akbik et al. 2015; Akbik and Li 2016) for seven other languages. Our endtoend neural transition model wins the best re sults against the baselines, giving the stateoftheart perfor mances on both the predicate identiÔ¨Åcation and argument role labeling, meanwhile keeping efÔ¨Åcient on decoding. We also show that with recent contextualized word representa tions, e.g., ELMo (Devlin et al. 2019), BERT (Peters et al. 2018) or XLNet (Yang et al. 2019), the overall SRL perfor mances can be further improved. Indepth analysis is con ducted to uncover the important components of our Ô¨Ånal model, which can help comprehensive understanding of our model. Following we summarize our contributions: We Ô¨Åll the gap in the literature of employing neural transitionbased model for endtoend SRL. We also en hance the parsing procedure with a closeÔ¨Årst scheme. We compose the highorder features (i.e., with one more predicaterole attachments from multiple predicates) in our transition framework for endtoend SRL to model long term substructure information explicitly. Our transition framework wins new stateoftheart performances against all current graphbased methods on benchmark datasets, meanwhile being faster on decoding. 2 Related Work "
272,Deep 3D Face Identification.txt,"We propose a novel 3D face recognition algorithm using a deep convolutional
neural network (DCNN) and a 3D augmentation technique. The performance of 2D
face recognition algorithms has significantly increased by leveraging the
representational power of deep neural networks and the use of large-scale
labeled training data. As opposed to 2D face recognition, training
discriminative deep features for 3D face recognition is very difficult due to
the lack of large-scale 3D face datasets. In this paper, we show that transfer
learning from a CNN trained on 2D face images can effectively work for 3D face
recognition by fine-tuning the CNN with a relatively small number of 3D facial
scans. We also propose a 3D face augmentation technique which synthesizes a
number of different facial expressions from a single 3D face scan. Our proposed
method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC
datasets, without using hand-crafted features. The 3D identification using our
deep features also scales well for large databases.","Face recognition has been an active research topic for many years. It is a challenging problem because the facial appearance and surface of a person can be vary greatly due to changes in pose, illumination, makeup, expression or hard occlusions. Recently, the performance of 2D face recognition sys tems [24, 29, 34] was boosted signiÔ¨Åcantly with the pop ularization of deep convolutional neural networks (CNN). It turns out that recent methods using CNN feature extrac tors trained on a massive dataset outperform conventional methods using handcrafted feature extractors, such as Lo cal Binary Pattern [2] or Fisher vectors [30]. Deep learning approaches require a large dataset to learn a face represen tation which is invariant to different factors, such as expres sions or poses. Large scale datasets of 2D face images can be easily obtained from the web. FaceNet [29] uses about (a) 2D Face recognitions with deep learning Deep CNN Dataset # ID # Img  FaceNet 8M 200M VGG Face 2.7K 2.6M DeepFace 4K 4.4M Dataset # ID # Img  ND 2006 0.8K 13K Bosphorus 0.1K 4K  BU3DFE 0.1K 2.5K‚Ä¶2D Faces in VGG Face 2D Face dataset  3D Face dataset Augmented 3D faces  (b) An illustration of proposed 3D face recognition with deep  learning Deep CNN= + ‚Ä¶ Figure 1: Challenges in 3D face recognition with deep learning due to the absence of massive datasets. (a) Datasets for 2D are large enough (200M at most) to train a DCNN. Images in VGG Face contain rich variations of expression, pose, occlusion, and illumination. (b) The number of 3D images is very limited (13K at most). Therefore, it is im portant to augment 3D faces to add variations for leaning a robust representation. 200M face images of 8M independent people as training data. VGG Face [24] assembled a massive training dataset containing 2.6M face images over 2.7K identities. With 3D modalities, recent research [19, 21, 31] has fo cused on Ô¨Ånding robust feature points and descriptors based on geometric information of a 3D face in a handcrafted manner. Those methods achieve good recognition perfor mances but involve relatively complex algorithmic opera tions to detect key feature points and descriptors as com pared to endtoend deep learning models. While some of these methods can do veriÔ¨Åcation in realtime, they often do not scale well for identiÔ¨Åcation tasks where a probe scan needs to be matched with a largescale gallery set. Com pared to publicly available 2D face databases, 3D scans are hard to acquire, and the number of scans and subjects in 1arXiv:1703.10714v1  [cs.CV]  30 Mar 2017public 3D face databases is limited. According to the sur vey in [25], the biggest 3D dataset is ND 2006 [10] which contains 13,450 scans over 888 individuals. It is small com pared to publicly available labeled 2D faces, and may not be sufÔ¨Åcient to train a deep convolutional neural network from scratch. Figure 1 shows available face datasets for both 2D and 3D and exhibits the challenges of 3D face recognition with deep learning. As a result, coping with the limited amount of available 3D data is challenging. We propose to leverage existing networks, trained for 2D face recognition, and Ô¨Ånetune them with a small amount of 3D scans in order to perform 3D to 3D surface matching. Another challenge intrinsic to the recognition tasks comes from the need to minimize intraclass variances (e.g., differences in the same individual under expression varia tions) while maximizing interclass variances (differences between persons). For faces, variations in expression im pact the 3D structure and can degrade the performance of recognition systems [25]. To address this issue, we propose to augment our 3D face database with synthesized 3D face data considering facial expressions. To augment training data, we use multilinear 3D morphable models in which the shape comes from the Basel Face Model [26], and the expression comes from FaceWarehouse [6]. To pass our 3D data to the 2Dtrained CNN, we project the point clouds onto a 2D image plane with an orthographic projection. To make our system robust to small alignment error, each 3D shape is augmented by rigid transformations: 3D rotations and translations before the projection. Some random patches are also added to the 3D data to simulate random occlusions (e.g., facial hair, covering by hands or artifacts). We Ô¨Ånetune a deep CNN trained for 2D face recognition, VGGFace [24], with the augmented data. Fig ure 2 illustrates our proposed method. We report perfor mances on standard public 3D databases: Bosphorus [28], BU3DFE [39], and 3DTEC [36]. Our contributions are as follows: 1. To our knowledge, this work is the Ô¨Årst to use a deep convolutional neural network for 3D face recognition. We frontalize a 3D scan, generate a 2.5D depth map, extract deep features to represent the 3D surface, and match the feature vector to perform 3D face recogni tion. 2. We propose a 3D face augmentation method that gen erates a number of person speciÔ¨Åc 3D shapes with ex pression changes from a single raw 3D scan, which allows us to enlarge a limited 3D dataset and improve the performance of 3D face recognition in the presence of expression variations. 3. We have validated our approach on 3 standard datasets. Our method shows comparable results to the state ofthe art algorithms while enabling efÔ¨Åcient 3D match ing for largescale galleries. An overview of our framework is presented in Figure 2. The rest of the paper is organized as follows: Section 2 re views the related work. Section 3 describes our proposed method. Our augmentation and performances on the public 3D databases are evaluated in Section 4. Section 5 con cludes the paper. 2. Related work "
410,Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers.txt,"Fine-grained classification is a challenging task that involves identifying
subtle differences between objects within the same category. This task is
particularly challenging in scenarios where data is scarce. Visual transformers
(ViT) have recently emerged as a powerful tool for image classification, due to
their ability to learn highly expressive representations of visual data using
self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine
tuned using semi-supervised learning techniques, suitable for situations where
we have lack of annotated data. This is particularly common in e-commerce,
where images are readily available but labels are noisy, nonexistent, or
expensive to obtain. Our results demonstrate that Semi-ViT outperforms
traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned
with limited annotated data. These findings indicate that Semi-ViTs hold
significant promise for applications that require precise and fine-grained
classification of visual data.","In recent years, the development of deep neural networks has led to signiÔ¨Åcant advancements in the Ô¨Åeld of com puter vision [16]. One such architecture is the Visual Trans former (ViT) [5], which utilizes the selfattention mech anism to model longrange dependencies between image features. Unlike traditional convolutional neural networks (CNN) [7, 10, 26], which rely on handcrafted hierarchical feature extraction, visual transformers can learn global spa tial relationships among image features in a more efÔ¨Åcient and effective manner. This has enabled them to outper form stateoftheart methods on various visual recognition tasks [18]. However, in realworld scenarios labeled data can be scarce and expensive to obtain. Therefore, semi * Joint Ô¨Årst authors.supervised learning (SSL) [40] has emerged as a powerful technique for leveraging unlabeled data to improve the per formance of deep neural networks. CNN methods have sig niÔ¨Åcantly advanced the Ô¨Åeld [1, 3, 15, 27, 32] while ViT ar chitectures have only recently demonstrated promising re sults [2, 33] with SSL. In this paper, we investigate the effectiveness of SSL when used with ViT architectures. SpeciÔ¨Åcally, we utilize the SemiViT architecture [2] to conduct transfer learning for Ô¨Ånegrained classiÔ¨Åcation of ecommerce data. The use of ecommerce data presents a unique advantage for SSL as unlabeled images are readily available. However, the labels associated are often noisy or absent altogether. Tradition ally, this issue has been addressed through the use of manual curators, which can be costly and predominantly accessible for established marketplaces. In emerging markets, such as in Latin America, the scarcity of reliable labelled data poses an even greater challenge. We collect three datasets from ecommerce data contain ing labeled and unlabeled images. We perform Ô¨Ånegrained classiÔ¨Åcation on the neck style of a vest ( Vest Neck Style) , the pattern of a phone case ( Phone Case Pattern) , and the pattern of aprons and food bibs ( Apron Food Bib Pattern) . Each of the datasets contains 29K, 30K, and 33K labeled images, and 227K, 287K, 284K unlabeled images, respec tively. Labels were gathered using crowdsourced methods. We Ô¨Åne tune three different models, the wellknown ResNet architecture [10], a ViT, and a SemiViT architecture; all of them pretrained on ImageNet [4]. For the ViT and Semi ViT architectures, we additionally set different labeled data regimes where they are additionally Ô¨Ånetuned using 25%, 50%, and 75% of the labeled data for each of the datasets. In total, we train 9 different models for each task. 2. Related Work "
452,Confidence Adaptive Regularization for Deep Learning with Noisy Labels.txt,"Recent studies on the memorization effects of deep neural networks on noisy
labels show that the networks first fit the correctly-labeled training samples
before memorizing the mislabeled samples. Motivated by this early-learning
phenomenon, we propose a novel method to prevent memorization of the mislabeled
samples. Unlike the existing approaches which use the model output to identify
or ignore the mislabeled samples, we introduce an indicator branch to the
original model and enable the model to produce a confidence value for each
sample. The confidence values are incorporated in our loss function which is
learned to assign large confidence values to correctly-labeled samples and
small confidence values to mislabeled samples. We also propose an auxiliary
regularization term to further improve the robustness of the model. To improve
the performance, we gradually correct the noisy labels with a well-designed
target estimation strategy. We provide the theoretical analysis and conduct the
experiments on synthetic and real-world datasets, demonstrating that our
approach achieves comparable results to the state-of-the-art methods.","With the emergence of highlycurated datasets such as ImageNet [ 1] and CIFAR10 [ 2], deep neural networks have achieved remarkable performance on many classiÔ¨Åcation tasks [ 3‚Äì5]. However, it is extremely timeconsuming and expensive to label a new largescale dataset with highquality annotations. Alternatively, we may obtain the dataset with lower quality annotations efÔ¨Åciently through online keywords queries [ 6] or crowdsourcing [ 7], but noisy labels are inevitably introduced consequently. Previous studies [ 8,9] demonstrate that noisy labels are problematic for overparameter ized neural networks, resulting in overÔ¨Åtting and performance degradation. Therefore, it is essential to develop noiserobust algorithms for deep learning with noisy labels. The authors of [ 8‚Äì11] have observed that deep neural networks learn to correctly predict the true labels for all training samples during early learning stage, and begin to make incorrect predictions in memorization stage as it gradually memorizes the mislabeled samples (in Figure 1 (a) and (b)). In this paper, we introduce a novel regularization approach to prevent the memorization of mislabeled samples (in Figure 1 (c)). Our contributions are summarized as follows: Preprint. Under review.arXiv:2108.08212v2  [cs.LG]  5 Sep 2021(a) Cross Entropywith MultiStep learning rate scheduler(b) Cross Entropywith Cosine Annealing learning rate scheduler(c) ConÔ¨Ådence Adaptive Regularizationwith Cosine Annealing learning rate schedulerFigure 1: We conduct the experiments on the CIFAR10 dataset with 40% symmetric label noise using ResNet34 [ 5]. The top row shows the fraction of samples with clean labels that are predicted correctly (purple) and incorrectly (black). In contrast, the bottom row shows the fraction of samples with false labels that are predicted correctly (purple), memorized (i.e. the prediction equals the false label, shown in blue), and incorrectly predicted as neither the true nor the labeled class (black). For samples with clean labels, all three models predict them correctly with the increasing of epochs. However, for false labels in (a), the model trained with crossentropy loss Ô¨Årst predicts the true labels correctly, but eventually memorizes the false labels. With the cosine annealing learning rate scheduler [12] in (b), the model only slows down the speed of memorizing the false labels. However, our approach shown in (c) effectively prevents memorization, allowing the model to continue learning the correctlylabeled samples to attain high accuracy on samples with both clean and false labels. ‚Ä¢We introduce an indicator branch to estimate the conÔ¨Ådence of model prediction and propose a novel loss function called conÔ¨Ådence adaptive loss (CAL) to exploit the earlylearning phase. A high conÔ¨Ådence value is likely to be associated with a clean sample and a low conÔ¨Ådence value with a mislabeled sample. Then, we add an auxiliary regularization term forming a conÔ¨Ådence adaptive regularization (CAR) to further segregate the mislabeled samples from the clean samples. We also develop a strategy to estimate the target probability instead of using the noisy labels directly, allowing the proposed model to suppress the inÔ¨Çuence of the mislabeled samples successfully. ‚Ä¢We theoretically analyze the gradients of the proposed loss functions and compare them with cross entropy loss. We demonstrate that CAL and CAR have similar effects to existing regularization based approaches. Both neutralize the inÔ¨Çuence of the mislabeled samples on the gradient, and ensure the contribution from correctlylabeled samples to the gradient remains dominant. We also prove the robustness of the auxiliary regularization term to label noise. ‚Ä¢We show that the proposed approach achieves comparable and even better performance to the stateoftheart methods on four benchmarks with different types and levels of label noise. We also perform an ablation study to evaluate the inÔ¨Çuence of different components. 2 Related work "
339,Jo-SRC: A Contrastive Approach for Combating Noisy Labels.txt,"Due to the memorization effect in Deep Neural Networks (DNNs), training with
noisy labels usually results in inferior model performance. Existing
state-of-the-art methods primarily adopt a sample selection strategy, which
selects small-loss samples for subsequent training. However, prior literature
tends to perform sample selection within each mini-batch, neglecting the
imbalance of noise ratios in different mini-batches. Moreover, valuable
knowledge within high-loss samples is wasted. To this end, we propose a
noise-robust approach named Jo-SRC (Joint Sample Selection and Model
Regularization based on Consistency). Specifically, we train the network in a
contrastive learning manner. Predictions from two different views of each
sample are used to estimate its ""likelihood"" of being clean or
out-of-distribution. Furthermore, we propose a joint loss to advance the model
generalization performance by introducing consistency regularization. Extensive
experiments have validated the superiority of our approach over existing
state-of-the-art methods.","DNNs have recently lead to tremendous progress in var ious computer vision tasks [14, 28, 42, 25, 40, 21]. These successes largely attribute to largescale datasets with re liable annotations ( e.g., ImageNet [4]). However, col lecting wellannotated datasets is extremely laborintensive and timeconsuming, especially in domains where expert knowledge is required ( e.g., Ô¨Ånegrained categorization [37, 36]). The high cost of acquiring largescale well labeled data poses a bottleneck in employing DNNs in real world scenarios. As an alternative, employing web images to train DNNs has received increasing attention recently [20, 41, 43, 34, CleansamplesCleansamplesUncleansamples CleansamplesUncleansamples IDsamplesOODsamplesCleansamplesIDsamplesOODsamplesminibatchiminibatchjminibatchiminibatchjfixedrater‚Ä¶‚Ä¶‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶(a) (b) ‚Ä¶‚Ä¶Figure 1. Existing smallloss based sample selection methods (upper ) tend to regard a humandeÔ¨Åned proportion of samples within each minibatch as clean ones. They ignore the Ô¨Çuctu ation of noise ratios in different minibatches. On the contrary, our proposed method ( bottom ) selects clean samples in a global manner. Moreover, indistribution (ID) noisy samples and outof distribution (OOD) ones are also selected and leveraged for en hancing the model generalization performance. 46, 45, 52, 53, 32]. Unfortunately, whereas web images are cheaper and easier to obtain via image search engines [5, 29, 47, 44], they usually yield inevitable noisy labels due to the errorprone automatic tagging system or nonexpert annotations [23, 32, 46, 48]. A recent study has suggested that samples with noisy labels would be unavoidably over Ô¨Åtted by DNNs and consequently cause performance degra dation [15, 51]. To alleviate this issue, many methods have been pro posed for learning with noisy labels. Early approaches primarily attempt to correct losses during training. Some methods correct losses by introducing a noise transition ma trix [31, 24, 6, 11]. However, estimating the noise transition matrix is challenging, requiring either prior knowledge or a subset of welllabeled data. Some methods design noisearXiv:2103.13029v1  [cs.CV]  24 Mar 2021robust loss functions which correct losses according to pre dictions of DNNs [26, 55, 34]. However, these methods are prone to fail when the noise ratio is high. Another active research direction in mitigating the nega tive effect of noisy labels is training DNNs with selected or reweighted training samples [12, 27, 22, 8, 50, 38, 32]. The challenge is to design a proper criterion for identifying clean samples. It has been recently observed that DNNs have a memorization effect and tend to learn clean and simple pat terns before overÔ¨Åtting noisy labels [15, 51]. Thus, stateof theart methods ( e.g., Coteaching [50], Coteaching+ [50], and JoCoR [38]) propose to select a humandeÔ¨Åned propor tion of smallloss samples as clean ones. Although promis ing performance gains have been witnessed by employ ing the smallloss sample selection strategy, these meth ods tend to assume that noise ratios are identical among all minibatches. Hence, they perform sample selection within each minibatch based on an estimated noise rate. How ever, this assumption may not hold true in realworld cases, and the noise rate is also challenging to estimate accurately (e.g., Clothing1M [39]). Furthermore, existing literature mainly focuses on closedset scenarios, in which only in distribution (ID) noisy samples are considered. In openset cases ( i.e., realworld cases), both indistribution (ID) and outofdistribution (OOD) noisy samples exist. Highloss samples do not necessarily have noisy labels. In fact, hard samples, ID noisy ones, and OOD noisy ones all produce large loss values, but the former two are potentially beneÔ¨Å cial for making DNNs more robust [32]. Motivated by the selfsupervised contrastive learning [3, 7], we propose a simple yet effective approach named JoSRC ( Joint Sample Selection and Model Regularization based on Consistency) to address aforementioned issues. SpeciÔ¨Åcally, we Ô¨Årst feed two different views of an im age into a backbone network and predict two corresponding softmax probabilities accordingly. Then we divide samples based on two likelihood metrics. We measure the likelihood of a sample being clean using the JensenShannon diver gence between its predicted probability distribution and its label distribution. We measure the likelihood of a sample being OOD based on the prediction disagreement between its two views. Subsequently, clean samples are trained con ventionally to Ô¨Åt their given labels. ID and OOD noisy sam ples are relabeled by a meanteacher model before they are backpropagated for updating network parameters. Finally, we propose a joint loss, including a classiÔ¨Åcation term and a consistency regularization term, to further advance model performance. A comparison between JoSRC and existing sample selection methods is provided in Figure 1. The ma jor contributions of this work are: (1) We propose a simple yet effective contrastive ap proach named JoSRC to alleviate the negative effect of noisy labels. JoSRC trains the network with a joint loss,including a crossentropy term and a consistency term, to obtain higher classiÔ¨Åcation and generalization performance. (2) Our proposed JoSRC selects clean samples globally by adopting the JensenShannon divergence to measure the likelihood of each sample being clean. We also propose to distinguish ID noisy samples and OOD noisy ones based on the prediction consistency between samples‚Äô different views. ID and OOD noisy samples are relabeled by a mean teacher network before being used for network update. (3) By providing comprehensive experimental results, we show that JoSRC signiÔ¨Åcantly outperforms stateof theart methods on both synthetic and realworld noisy datasets. Furthermore, extensive ablation studies are con ducted to validate the effectiveness of our approach. 2. Related Works "
456,LA-HCN: Label-based Attention for Hierarchical Multi-label TextClassification Neural Network.txt,"Hierarchical multi-label text classification (HMTC) has been gaining
popularity in recent years thanks to its applicability to a plethora of
real-world applications. The existing HMTC algorithms largely focus on the
design of classifiers, such as the local, global, or a combination of them.
However, very few studies have focused on hierarchical feature extraction and
explore the association between the hierarchical labels and the text. In this
paper, we propose a Label-based Attention for Hierarchical Mutlti-label Text
Classification Neural Network (LA-HCN), where the novel label-based attention
module is designed to hierarchically extract important information from the
text based on the labels from different hierarchy levels. Besides, hierarchical
information is shared across levels while preserving the hierarchical
label-based information. Separate local and global document embeddings are
obtained and used to facilitate the respective local and global
classifications. In our experiments, LA-HCN outperforms other state-of-the-art
neural network-based HMTC algorithms on four public HMTC datasets. The ablation
study also demonstrates the effectiveness of the proposed label-based attention
module as well as the novel local and global embeddings and classifications. By
visualizing the learned attention (words), we find that LA-HCN is able to
extract meaningful information corresponding to the different labels which
provides explainability that may be helpful for the human analyst.","In recent years, there has been a growing interest in hierarchical multilabel classication (HMC) which can be applied in a wide range of applications such as International Patent Classication (Gomez & Moens, 2014), product annotation (Aly et al., 2019) and advertising recommendation (Agrawal et al., 2013). In the common  atclassication problem, each input sample is only associated with a single label from a set of disjoint labels. However, in HMC problem, the labels are organized in the form of a tree or a Directed Acyclic Graph(DAG) Silla & Freitas (2011) and each input sample is usually associated with multiple labels, which made it more challenging. The most straightforward approach in dealing with HMC problem is to convert it to a  at multilabel classication problem by simply ignoring the rel evance between labels (Li et al., 2018; Hu et al., 2018; Aly et al., 2019). The main disadvantage in doing so is the loss of the useful hierarchical information. Alternatively, the local approach (Koller & Sahami, 1997) is designed to perform multilabel classication, where the classications are carried out at each level of the label hierarchy (e.g., Local Classier per Parent Node, Local Classier per Level and Local Classier per Node). The overall classication results are then generated based on these local predictions. While hierarchical information can be better utilized in local approaches, misclassications are easily propagated to the next levels (Punera & Ghosh, 2008). Global approaches are proposed to learn a single global model for all labels to reduce the model size and consider the entire label hierarchy at once (Kiritchenko et al., 2005, 2006). These global classiers are typically built on  atclassiers with modications made to inte grate the hierarchical information of labels (Wang et al., 2009; Vens et al., 2008) into the model. Recently, more algorithms which combine the local and global 2approaches are proposed (Wehrmann et al., 2018; Mao et al., 2019). All algorithms introduced above only focus on the design of hierarchical clas sier while ignoring the hierarchical features which may be extracted and they are important in HMC as well. Huang et al. (2019) and Rojas et al. (2020) consider hierarchical feature extraction in their work. However, the extraction process is designed and fullled by applying the typical attention mechanism over the whole text. Since in HMC problem the text may be associated with mul tiple labels at each hierarchy level, the features extracted from typical attention may be diluted. We believe it is reasonable to hypothesize that a labelbased attention, where information extraction is performed based on dierent labels at dierent hierarchical levels, would allow the model to be more interpretable and have an overall better performance in accuracy. Given the above moti vations, we propose LAHCN | a HMTC model with a labelbased attention to facilitate labelbased hierarchical feature extraction, where we introduce the concept and mechanism of component which is an intermediate representation that helps bridge the latent association between the words and the labels for labelbased attention. Contribution. Main contributions of this work: ‚Ä¢We propose a novel HMTC model capable of learning disjoint features for each hierarchical level, while sharing the hierarchical information learned across levels. Besides, both local and global classiers with respective embeddings are applied to reduce the impact of errorpropagation across levels. ‚Ä¢We propose a novel mechanism to learn labelbased word attention at each level such that the important features of each document can be captured based on individual labels and prove that such mechanism can provide more interpretable results. ‚Ä¢We evaluate LAHCN against both classical and stateoftheart neural network HMTC baselines on four benchmark datasets and LAHCN out 3performs them on all the datasets. The ablation study shows the eec tiveness of the component mechanism and dierent classiers applied in LAHCN and demonstrates that the learned labelbased attention is able to give reasonable interpretation of the prediction results. 2. Related Work "
600,Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network.txt,"This paper proposes a neural network architecture and training scheme to
learn the start and end time of sound events (strong labels) in an audio
recording given just the list of sound events existing in the audio without
time information (weak labels). We achieve this by using a stacked
convolutional and recurrent neural network with two prediction layers in
sequence one for the strong followed by the weak label. The network is trained
using frame-wise log mel-band energy as the input audio feature, and weak
labels provided in the dataset as labels for the weak label prediction layer.
Strong labels are generated by replicating the weak labels as many number of
times as the frames in the input audio feature, and used for strong label layer
during training. We propose to control what the network learns from the weak
and strong labels by different weighting for the loss computed in the two
prediction layers. The proposed method is evaluated on a publicly available
dataset of 155 hours with 17 sound event classes. The method achieves the best
error rate of 0.84 for strong labels and F-score of 43.3% for weak labels on
the unseen test split.","Sound event detection (SED) is the task of recognizing sound events and its respective start and end timings in an audio recording. Rec ognizing such sound events and its temporal information can be use ful in different applications such as surveillance [1, 2], biodiversity monitoring [3, 4] and query based multimedia retrieval [5]. Tra ditionally, SED has been tackled with datasets that have temporal information for each of the sound event present [6, 7]. We refer to such temporal information of sound events as strong labels in this paper. The internet has a vast collection of audio data. Many collab orative and social websites like Freesound1and YouTube2allow users to upload multimedia with metadata like captions and tags. We can potentially automate the collection of audio data associated with a given tag from these online sources in considerably less time and manual effort. Recently, Gemekke et al. [8] carried out this with 632 sound event tags on YouTube and collected nearly two The research leading to these results has received funding from the Eu ropean Research Council under the European Unions H2020 Framework Programme through ERC Grant Agreement 637422 EVERYSOUND. The authors also wish to acknowledge CSCIT Center for Science, Finland, for computational resources. 1https://freesound.org/ 2https://www.youtube.com/million 10 second audio recordings. While these tags indicate that the sound event is present in the audio recording, the tags do not contain the information as to how many times they occur or at what time they occur. In this paper, we call such tags without any tempo ral information as weak labels. The task of identifying weak labels of an audio is also referred as audio tagging in literature [9, 10]. Collecting and annotating data with strong labels to train SED methods is a timeconsuming task involving a lot of manual labor. On the other hand, collecting weakly labeled data takes much less time to annotate manually, since the annotator has to mark only the active sound event classes and not its exact time boundaries. If we can build SED methods which can learn strong labels from such weakly labeled data, then the methods can learn on a large amount of data. In this paper, we propose to implement such a strong label learning SED method using weakly labeled training data. Similar research of using weakly labeled data to learn strong labels has been done in neighboring audio domains such as mu sic [11, 12], and bird classiÔ¨Åcation [13, 14]. Liu et al. [11] used a fully convolutional neural network (FCN) to recognize instru ments and tempo for each time frame of an audio clip given only the clip level information. They further extended this network to sound event detection [15] and experimented on publicly available datasets. The advantage of using an FCN is it can handle audio input of any length. On the other hand, the limitation is that the frame wise strong labels are obtained by an upscaling layer which repli cates segmentwise output to as many number of frames required. Similar FCN as [15] was proposed in [16] without the upscaling layer, thereby estimating labels for short segments of length 1.5 s instead of frame wise labels. The study compares the performance of this FCN with a VGGlike network [17] like network which out puts sound event labels in segments of 1.5 s. The FCN network is trained using the entire audio, and its respective weak label. On the other hand, the VGG network is trained on subsegments of the en tire audio, assuming that the recording level weak label annotation remains the same in all its subsegments. The study showed that using an FCN performs better SED than using the VGG method. Kumar et al. [18] proposed a multiple instance learning (MIL) ap proach [19] for this task, though the results were promising the ap proach was claimed to be not scalable to large datasets by the same authors in [16]. Sound events in real life most often overlap each other. A SED method which can recognize such overlapping sound events is re ferred as polyphonic SED method. The state of the art for poly phonic SED, trained using strong labels, was proposed recently in [20], where log melband energy feature was used along with a stacked convolutional and recurrent neural network and evaluated on multiple datasets. Similar stacked convolutional and recurrent neural network has also been shown to outperform state of the artarXiv:1710.02998v1  [cs.SD]  9 Oct 2017Detection and ClassiÔ¨Åcation of Acoustic Scenes and Events 2017 16 November 2017, Munich, Germany methods in audio tagging tasks [9, 10]. Motivated by the perfor mance of this method in SED and audio tagging, in this paper, we propose to extend the method to perform both SED and audio tag ging together, given only the audio and its respective weak labels. In particular, we use the log melband audio feature extracted from the audio and extend the stacked convolutional and recurrent neural network to predict two outputs sequentially, the strong followed by the weak labels. To train the proposed network we generate dummy strong labels by replicating the weak labels as many times as the number of frames in the audio input feature. We further propose to control the information that the network learns by separately scaling the loss calculated in the weak and strong prediction layers. Networks similar to the proposed stacked convolutional and neural network are the current state of the arts for audio tag ging [9, 10]. This shows that the architecture is capable of learn ing the relevant information in temporal domain and mapping it to active classes. In this paper, we show that the proposed training scheme can extract this temporal information that the network is learning in the intermediate layers and can be used as strong labels. In comparison to previous works [15, 16], the proposed method sup ports higher time resolution for strong labels by its inherent design. The feature extraction and the proposed network is described in Section 2. The dataset, metric and evaluation procedure is discussed in Section 3. Finally, the results and discussions of the evaluation performed are presented in Section 4. 2. METHOD "
146,Improving Neural Architecture Search Image Classifiers via Ensemble Learning.txt,"Finding the best neural network architecture requires significant time,
resources, and human expertise. These challenges are partially addressed by
neural architecture search (NAS) which is able to find the best convolutional
layer or cell that is then used as a building block for the network. However,
once a good building block is found, manual design is still required to
assemble the final architecture as a combination of multiple blocks under a
predefined parameter budget constraint. A common solution is to stack these
blocks into a single tower and adjust the width and depth to fill the parameter
budget. However, these single tower architectures may not be optimal. Instead,
in this paper we present the AdaNAS algorithm, that uses ensemble techniques to
compose a neural network as an ensemble of smaller networks automatically.
Additionally, we introduce a novel technique based on knowledge distillation to
iteratively train the smaller networks using the previous ensemble as a
teacher. Our experiments demonstrate that ensembles of networks improve
accuracy upon a single neural network while keeping the same number of
parameters. Our models achieve comparable results with the state-of-the-art on
CIFAR-10 and sets a new state-of-the-art on CIFAR-100.","Designing neural network (NN) architectures is often a demanding process. It often requires signiÔ¨Åcant time, re sources, and human expertise. These challenges are partially addressed by neural architecture search (NAS), which is able to Ô¨Ånd the best convolutional layer or cell that is then used as a building block for the network (Real et al., 2018; Zoph *Equal contribution1Google Research, New York, NY , USA 2Work done as a member of the Google AI Residency pro gram (g.co/brainresidency). Correspondence to: Vladimir Macko <vlejd@google.com >, Charles Weill <weill@google.com >.et al., 2017). However, once a good building block is found, it is still required to manually design the Ô¨Ånal architecture as a combination of multiple blocks. Moreover, there is usually a need to design multiple architectures with differ ent parameter budgets, as different applications might pose different hardware constraints on memory and computation. The critical question is how to upscale a small building block into a large architecture? A common solution is to stack those blocks into a single tower and adjust the width and depth to Ô¨Åll the parameter budget. The solution of having one tower is common also for architectures that are not the result of neural architecture search (Springenberg et al., 2014; Szegedy et al., 2015; He et al., 2016). Recently, it was proposed to construct the network as an ensemble of smaller networks trained in a special way (Dutt et al., 2018). Ensembles of neural networks are known to be much more robust and accurate than individual subnetworks. Ensembles perform well on a wide variety of tasks (Caruana et al., 2004) and are frequently used in the winning solu tions of machine learning competitions (e.g. Kaggle) often consist of ensembles of multiple models. Unlike a single large neural network, an ensemble‚Äôs size is not bounded by training, since each of the component subnetworks can be trained independently, and their outputs computed in paral lel. Ultimately the Ô¨Ånal ensemble‚Äôs size is bounded by its ability to Ô¨Åt on a serving hardware, and latency constraints. The main questions we tackle in this paper are the following: Can ensembles perform better than a single tower model with the same number of parameters? Can we beneÔ¨Åt from sequentially training the component subnetworks, and lever age information acquired from previously trained networks to improve the Ô¨Ånal ensemble performance? Is it possible to construct ensemble architectures automatically or with minimal human expertise? In this work, we present a new paradigm to automatically generate ensembles of subnetworks that achieve high accu racy given a Ô¨Åxed parameter budget. Our AdaNAS algo rithm works in an iterative manner, and it increases the size of each new subnetwork at each iteration until the ensemble hits the budget limit. As we iteratively learn the composition of the ensemble, wearXiv:1903.06236v1  [cs.LG]  14 Mar 2019Improving Neural Architecture Search Image ClassiÔ¨Åers via Ensemble Learning leverage information learned from subnetworks trained in previous iterations. We explore the effects of using ideas from Born Again Networks (BAN) (Furlanello et al., 2018) to the Ô¨Ånal ensemble performance. In addition, we introduce a novel technique called Adaptive Knowledge Distillation (AKD) that extends Born Again Networks to use the previ ous iteration‚Äôs ensemble as a teacher to assist in training the current iteration‚Äôs subnetworks. Resulting models are comprised of multiple separate tow ers that can be easily parallelized at inference time. Our presented technique requires minimal hyperparameter tun ing to achieve these results. Our experiments demonstrate that ensembles of subnetworks improve accuracy upon a single neural network with the same number of parameters. OnCIFAR10 our algorithm achieves error 2:26and on CIFAR100 it achieves error 14:58. To our knowledge, and as we will show in Section 5, our technique achieves a new stateoftheart on CIFAR100 compared to methods that do not use additional regularization or data augmenta tion (e.g., ShakeDrop (Yamada et al., 2018) or AutoAug ment (Cubuk et al., 2018)). This paper has been implemented as an extension of a frame work for the construction and search of boosted ensembles, AdaNet (Cortes et al., 2016; Weill et al., 2018). The code to reproduce our results is available in the AdaNet project repository1. Our implementation uses opensourced code provided by (Zoph et al., 2017) in the TensorFlow Models repository2. This paper is organized as follows. We review previous work in Section 2. In Section 3 we describe the ensembling algorithm. Experiment settings are outlined in Section 4 and Section 5 shows the Ô¨Ånal results. Finally, Section 6 discusses our proposed technique and our Ô¨Åndings. 2. Related work "
50,An Ensemble Noise-Robust K-fold Cross-Validation Selection Method for Noisy Labels.txt,"We consider the problem of training robust and accurate deep neural networks
(DNNs) when subject to various proportions of noisy labels. Large-scale
datasets tend to contain mislabeled samples that can be memorized by DNNs,
impeding the performance. With appropriate handling, this degradation can be
alleviated. There are two problems to consider: how to distinguish clean
samples and how to deal with noisy samples. In this paper, we present Ensemble
Noise-robust K-fold Cross-Validation Selection (E-NKCVS) to effectively select
clean samples from noisy data, solving the first problem. For the second
problem, we create a new pseudo label for any sample determined to have an
uncertain or likely corrupt label. E-NKCVS obtains multiple predicted labels
for each sample and the entropy of these labels is used to tune the weight
given to the pseudo label and the given label. Theoretical analysis and
extensive verification of the algorithms in the noisy label setting are
provided. We evaluate our approach on various image and text classification
tasks where the labels have been manually corrupted with different noise
ratios. Additionally, two large real-world noisy datasets are also used,
Clothing-1M and WebVision. E-NKCVS is empirically shown to be highly tolerant
to considerable proportions of label noise and has a consistent improvement
over state-of-the-art methods. Especially on more difficult datasets with
higher noise ratios, we can achieve a significant improvement over the
second-best model. Moreover, our proposed approach can easily be integrated
into existing DNN methods to improve their robustness against label noise.","Together with the resurgence and remarkable success of DNNs, largescale datasets have become increasingly com mon. For supervised learning tasks, modern DNNs gener ally require the datasets to be annotated with accurate la Equal contribution.bels to achieve high performance. However, to correctly label large amounts of data is very costly and errorprone, even highquality handlabeled benchmark dataset such as ImageNet [Deng et al. , 2009 ]contains mislabeled sam ples[Northcutt et al. , 2019 ]. There exist alternative, lowcost methods, including largescale annotation through crowd sourcing [Sheng et al. , 2008 ]and online web queries [Divvala et al. , 2014 ], but these inevitably yield a higher proportion of incorrect class labels. DNNs are prone to overÔ¨Åtting to corrupted data sam ples, which increases the generalization error of the net work [Zhang et al. , 2017a ]. To address this issue, numerous algorithms have been proposed to train DNNs in a way ro bust to label noise [Wang et al. , 2019; Xu et al. , 2019 ]. The capability of DNNs to Ô¨Åt noisy data has been further stud ied by Chen et al. [2019 ]. They showed that, for symmetric noise, the test accuracy is a quadratic function of the noise ratio, and claim that generalization occurs in the sense of dis tribution. In this paper, we relax their assumptions and give a theoretical analysis of the impact that an imperfect classiÔ¨Åer has. Our Ô¨Åndings demonstrate that, while the noise level has a signiÔ¨Åcant impact, the performance of the classiÔ¨Åer is key. Based on our analysis, we propose ENKCVS, a novel en semble method based on Kfold crossvalidation to increase the generalization performance. We empirically evaluate our solution and demonstrate that it outperforms the stateofthe art, proving the effectiveness of our method. In summary, our contributions are as follows. ‚Ä¢ We propose a novel method (ENKCVS) based on a combination of Kfold crossvalidation and ensemble learning. Samples are selected from the noisy data by keeping those where the predicted label matches the given (noisy) label. Any nonselected samples can then either be discarded or reweighted to have a lower im pact. Mixup [Zhang et al. , 2017b ]is applied during training to augment the data. ‚Ä¢ We further propose a label reweighting scheme for sam ples that are likely erroneous. For these uncertain sam ples, we consider both the given label and a generated pseudo label with the weight set using the entropy of the predicted labels given by ENKCVS. ‚Ä¢ We empirically show that the proposed solution out performs stateoftheart noiserobust methods on imarXiv:2107.02347v1  [cs.LG]  6 Jul 2021age recognition and text classiÔ¨Åcation tasks on multiple datasets. Moreover, our solution can easily be incorpo rated into existing network architectures to enhance their robustness to noisy labels. 2 Related Work "
413,On Unsupervised Uncertainty-Driven Speech Pseudo-Label Filtering and Model Calibration.txt,"Pseudo-label (PL) filtering forms a crucial part of Self-Training (ST)
methods for unsupervised domain adaptation. Dropout-based Uncertainty-driven
Self-Training (DUST) proceeds by first training a teacher model on source
domain labeled data. Then, the teacher model is used to provide PLs for the
unlabeled target domain data. Finally, we train a student on augmented labeled
and pseudo-labeled data. The process is iterative, where the student becomes
the teacher for the next DUST iteration. A crucial step that precedes the
student model training in each DUST iteration is filtering out noisy PLs that
could lead the student model astray. In DUST, we proposed a simple, effective,
and theoretically sound PL filtering strategy based on the teacher model's
uncertainty about its predictions on unlabeled speech utterances. We estimate
the model's uncertainty by computing disagreement amongst multiple samples
drawn from the teacher model during inference by injecting noise via dropout.
In this work, we show that DUST's PL filtering, as initially used, may fail
under severe source and target domain mismatch. We suggest several approaches
to eliminate or alleviate this issue. Further, we bring insights from the
research in neural network model calibration to DUST and show that a
well-calibrated model correlates strongly with a positive outcome of the DUST
PL filtering step.","In recent years, Automatic Speech Recognition (ASR) has improved dramatically due to the introduction of novel neural network ar chitectures, training frameworks, and large labeled and unlabeled datasets [1‚Äì5]. However, domain generalization remains an un solved problem; an ASR model‚Äôs performance drops signiÔ¨Åcantly when the training and testing (or inference) conditions do not match. Several previous works attempt to address the issue of domain adaptation, such as consistency regularization, domainadversarial training, multidomain selfsupervised pretraining, and the much simpler SelfTraining (ST) method [6‚Äì10]. ST is useful when the target domain data distribution differs signiÔ¨Åcantly from the source domain data distribution (e.g., read speech to YouTube speech). ST proceeds by training a teacher model on a labeled set in the source domain, which generates pseudolabels (PLs) for the unla beled set in the target domain. Then, a student model uses both labeled and pseudolabeled data for training. We can iterate over the Teacher/Student training such that the student from a previous iter ation acts as the teacher for the next ST iteration. Classical ST that This work is supported by DSTA. A part of this work was performed using HPC resources by GENCI‚ÄìIDRIS under allocation AD011012527.uses all PLs might be suboptimal as PLs might be noisy. To address this issue, we previously proposed Dropout Uncertaintydriven Self Training (DUST) [11, 12], which appends the original SelfTraining algorithm with PL Ô¨Åltering step to weed out noisy PLs. Intuitively, the PL Ô¨Åltering stage in DUST computes a proxy for the model‚Äôs conÔ¨Ådence in its predictions on an unlabeled speech ut terance. It is computed using the agreement between a reference prediction and Tsampled predictions. If the model‚Äôs conÔ¨Ådence is below a predeÔ¨Åned threshold, we discard that utterance. The DUST Ô¨Åltering stage assumes that the model‚Äôs conÔ¨Ådence is a reliable es timate of PL quality, i.e., high conÔ¨Ådence implies low Word Error Rate (WER) on accepted utterances. In this work, we found that the DUST algorithm fails when the domain mismatch is severe. Hence, the DUST Ô¨Åltering method could accept noisy PLs. This paper focuses on PseudoLabel (PL) Ô¨Åltering stage in DUST. The goal of this work is to stress test the DUST PL Ô¨Ål tering under severe domain mismatch, and suggest approaches to alleviate the DUST PL Ô¨Åltering issue. The following are the main contributions to this work, (i) We show that the DUST algorithm fails when the domain mismatch is severe. (ii) We propose several approaches to mitigate or eliminate this breakpoint. (iii) We study, for the time, the PL Ô¨Åltering in the context of model calibration for DUST. (iv) We show that there is a strong correlation between the model calibration errors and the quality of the Ô¨Åltered PLs. 2. METHODOLOGY "
270,PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification.txt,"We propose using an adversarial autoencoder (AAE) to replace generative
adversarial network (GAN) in the private aggregation of teacher ensembles
(PATE), a solution for ensuring differential privacy in speech applications.
The AAE architecture allows us to obtain good synthetic speech leveraging upon
a discriminative training of latent vectors. Such synthetic speech is used to
build a privacy-preserving classifier when non-sensitive data is not
sufficiently available in the public domain. This classifier follows the PATE
scheme that uses an ensemble of noisy outputs to label the synthetic samples
and guarantee $\varepsilon$-differential privacy (DP) on its derived
classifiers. Our proposed framework thus consists of an AAE-based generator and
a PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands
Dataset Version II, the proposed PATE-AAE improves the average classification
accuracy by +$2.11\%$ and +$6.60\%$, respectively, when compared with
alternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while
maintaining a strong level of privacy target at $\varepsilon$=0.01 with a fixed
$\delta$=10$^{-5}$.","The speech signal contains a rich set of information [1] that encompasses gender, accent, speaking environment, and other speaker characteristics; therefore, protecting data privacy be comes a raising concern when speech data is used to deploy commercial speech applications. In recent years, public regu lations, e.g., GDPR [2] and CCPA [3], have been proposed to establish new guidelines related to data privacy measurement and identity protection in enduser applications. Recent works on model inversion attacks [4, 5] indeed highlighted the impor tance of data privacy when the original data proÔ¨Åle (e.g., facial images [4]) could be recovered from a machine learning model by using queryfree optimization techniques. Differential privacy [6] (DP) is an effective mechanism for ensuring individual data protection, and it has been deployed in several industrial systems [7, 8]1to protect customer‚Äôs sensi tive information by exploiting a sophisticated noisy perturbation scheme. The ""DP mechanism [6] provides a way to quantify a privacy loss and set up a privacy budget (e.g., a minimum "" value) for a given dataset. However, ""differential private mod els [7] need to be reÔ¨Åned in order to improve a degraded pre diction accuracy [9] caused by the DP noise. The private aggre 1Apple has also applied differential privacy with a pri vacy budget ( ""=8) based on an ofÔ¨Åcial document in https: //www.apple.com/privacy/docs/Differential_ Privacy_Overview.pdf .gation of teacher ensembles [10] (PATE) is a recently proposed solution that aims to combat the accuracy loss of the machine learning models while ensuring privacy requirements. PATE follows a teacherstudent architecture [11], where the teacher is an ensemble model. The underpinning idea in PATE is to leverage upon noisy outputs of aggregated teacher models to (re)label nonsensitive public data with DP guarantees. The PATE method and its improved version [12] were proven useful in reducing the model accuracy drop through a voting process during the noisy ensemble. Nonetheless, the teacherstudent learning process highly depends on a hypothesis [10, 12, 13] that there exists a sufÔ¨Åcient amount of public (nonsensitive) data to train the model. PATEGAN [13] tries to overcome this issue by incorporating a generative block jointly trained with the PATE block; the goal is providing enough synthetic data to train deep models effectively. Unfortunately, PATEGAN does not work well for high dimensional data synthesis (e.g., images), as demonstrated in recent studies [14, 15]. Moreover, generating speech samples is a challenging task, as shown in recent studies about neural vocoders [16, 17]. Teacher  Models Teacher  Models (b) Teacher  Models (c) Student  Model Unlabeled Data  (NonSensitive) Sensitive Data  (e.g., Identity) (a) Generative  Model  Label with noisy (differentiallyprivate)  ensemble output Public Accessible  (queryfree)  Synthesis  Figure 1: Private aggregation of teachers ensemble (PATE) learning process [10, 12]: (a) the teacher prediction models training from sensitive data; (b) a joint generative model (e.g., adversarial autoencoder [18] for audio synthesis in this study); (c) student prediction model training from nonsensitive data. In this study, we introduce an adversarial autoencoder [18] (AAE) based model into PATE to improve the generative pro cess for privacypreserving speech classiÔ¨Åcation. PATEAAE Ô¨Årst adapts an autoencoder to minimize a reconstruction loss, training on sensitive data. As shown in Fig. 1(a), the gener ative model produces synthetic data as nonsensitive samples. Meanwhile, the training data are divided into Iisolated sub sets to train individual teacher classiÔ¨Åers. For instance, Iis equal to 3in Fig. 1(b). The teacher classiÔ¨Åers then undergo an output aggregation process to generate noisy labels, which ensures the""differentially private protection. Finally, a student classiÔ¨Åer uses the labeled synthetic samples (nonsensitive data) for training its model. The proposed PATEAAE framework isarXiv:2104.01271v2  [cs.SD]  15 Jun 2021assessed with the Google Speech Commands Dataset Version II [19]. Our experimental evidence demonstrates competitive results in terms of synthetic sample quality and classiÔ¨Åcation accuracy with a strong ""DP guarantee ( "" < 1) considering established privacypreserving learning (PPL) works [13, 20]. To the best of the authors‚Äô knowledge, this is the Ô¨Årst attempt to introduce the PATE architecture into a speech classiÔ¨Åcation task. Moreover, the proposed solution is beneÔ¨Åted from adver sarial autoencoder block, with advantages over existing GAN solutions [4, 13] of having a better testlikelihood estimation. 2. Related Work "
81,Selective Pseudo-label Clustering.txt,"Deep neural networks (DNNs) offer a means of addressing the challenging task
of clustering high-dimensional data. DNNs can extract useful features, and so
produce a lower dimensional representation, which is more amenable to
clustering techniques. As clustering is typically performed in a purely
unsupervised setting, where no training labels are available, the question then
arises as to how the DNN feature extractor can be trained. The most accurate
existing approaches combine the training of the DNN with the clustering
objective, so that information from the clustering process can be used to
update the DNN to produce better features for clustering. One problem with this
approach is that these ``pseudo-labels'' produced by the clustering algorithm
are noisy, and any errors that they contain will hurt the training of the DNN.
In this paper, we propose selective pseudo-label clustering, which uses only
the most confident pseudo-labels for training the~DNN. We formally prove the
performance gains under certain conditions. Applied to the task of image
clustering, the new approach achieves a state-of-the-art performance on three
popular image datasets. Code is available at
https://github.com/Lou1sM/clustering.","Clustering is the task of partitioning a dataset into clusters such that data points within the same cluster are similar to each other, and data points from diÔ¨Äerent clusters are diÔ¨Äerent to each other. It is applicable to any set of data for which there is a notion of similarity between data points. It requires no prior knowledge, neither the explicit labels of supervised learning nor the knowledge of expected symmetries and invariances leveraged in selfsupervised learning. The result of a successful clustering is a means of describing data in terms of the cluster that they belong to. This is a ubiquitous feature of human cognition. For example, we hear a sound and think of it as an utterance of the word ‚Äúwater‚Äù, or we see a video of a biomechanical motion and think of it as a jump. This can be further reÔ¨Åned among experts, so that a musician could describe a musical phrase as an English cadence in A major, or a dancer could describe a snippet of ballet as a rightleg fouette into arabesque. When clustering highdimensional data, the curse of dimensionality [ 2] means that many classic algorithms, such as kmeans [ 29] or expectation maximization [ 10], perform poorly. The Euclidean distance, which is the basis for the notion of similarity in the Euclidean space, becomes weaker in higher dimensions [ 51]. Several solutions to this problem have been proposed. In this paper, we consider those termed deep clustering.arXiv:2107.10692v1  [cs.LG]  22 Jul 20212 L. Mahon et al. Deep clustering is a set of techniques that use a DNN to encode the high dimensional data into a lowerdimensional feature space, and then perform clustering in this feature space. A major challenge is the training of the encoder. Much of the success of DNNs as image feature extractors (including [ 24,46]) has been in supervised settings, but if we already had labels for our data, then there would be no need to cluster in the Ô¨Årst place. There are two common approaches to training the encoder. The Ô¨Årst is to use the reconstruction loss from a corresponding decoder, i.e., to train it as an autoencoder [ 47]. The second is to design a clustering loss, so that the encoding and the clustering are optimized jointly. Both are discussed further in Section 2. Our model, selective pseudolabel clustering (SPC), combines reconstruction andclusteringloss.ItusesanensembletoselectdiÔ¨ÄerentlossfunctionsfordiÔ¨Äerent data points, depending on how conÔ¨Ådent we are in their predicted clusters. Ensemble learning is a function approximation where multiple approximating models are trained, and then the results are combined. Some variance across the ensemble is required. If all individual approximators were identical, there would be no gain in combining them. For ensembles composed of DNNs, variance is ensured by the random initializations of the weights and stochasticity of the training dynamics. In the simplest case, the output of the ensemble is the average of each individual output (mean for regression and mode for classiÔ¨Åcation) [36]. When applying an ensemble to clustering problems (referred to as consensus clustering; see [ 3] for a comprehensive discussion), the sets of cluster labels must be aligned across the ensemble. This can be performed eÔ¨Éciently using the Hungarian algorithm. SPC considers a clustered data point to be conÔ¨Ådent if it received the same cluster label (after alignment) in each member of the ensemble. The intuition is that, due to random initializations and stochasticity of training, there is some nonzero degree of independence between the diÔ¨Äerent sets of cluster labels, so the probability that all cluster labels are incorrect for a particular point is less than the probability that a single cluster label is incorrect. Our main contributions are brieÔ¨Çy summarized as follows. ‚ÄìWe describe a generally applicable deep clustering method (SPC), which treats cluster assignments as pseudolabels, and introduces a novel technique to increase the accuracy of the pseudolabels used for training. This produces a better feature extractor, and hence a more accurate clustering. ‚ÄìWeformallyprovetheadvantagesofSPC,givensomesimplifyingassumptions. SpeciÔ¨Åcally, we prove that our method does indeed increase the accuracy of the targets used for pseudolabel training, and this increase in accuracy does indeed lead to a better clustering performance. ‚ÄìWe implement SPC for image clustering, with a stateoftheart performance on three popular image clustering datasets, and we present ablation studies on its main components. The rest of this paper is organized as follows. Section 2 gives an overview of related work. Sections 3 and 4 give a detailed description of SPC and a proof of correctness, respectively. Section 5 presents and discusses our experimental results,Selective PseudoLabel Clustering 3 including a comparison to existing image clustering models and ablation studies on main components of SPC. Finally, Section 6 summarizes our results and gives an outlook on future work. Full proofs and further details are in the appendix. 2 Related Work "
87,Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective.txt,"Knowledge distillation is an effective approach to leverage a well-trained
network or an ensemble of them, named as the teacher, to guide the training of
a student network. The outputs from the teacher network are used as soft labels
for supervising the training of a new network. Recent studies
\citep{muller2019does,yuan2020revisiting} revealed an intriguing property of
the soft labels that making labels soft serves as a good regularization to the
student network. From the perspective of statistical learning, regularization
aims to reduce the variance, however how bias and variance change is not clear
for training with soft labels. In this paper, we investigate the bias-variance
tradeoff brought by distillation with soft labels. Specifically, we observe
that during training the bias-variance tradeoff varies sample-wisely. Further,
under the same distillation temperature setting, we observe that the
distillation performance is negatively associated with the number of some
specific samples, which are named as regularization samples since these samples
lead to bias increasing and variance decreasing. Nevertheless, we empirically
find that completely filtering out regularization samples also deteriorates
distillation performance. Our discoveries inspired us to propose the novel
weighted soft labels to help the network adaptively handle the sample-wise
bias-variance tradeoff. Experiments on standard evaluation benchmarks validate
the effectiveness of our method. Our code is available at
\url{https://github.com/bellymonster/Weighted-Soft-Label-Distillation}.","For deep neural networks (Goodfellow et al., 2016), knowledge distillation (KD) (Ba & Caruana, 2014; Hinton et al., 2015) refers to the technique that uses welltrained networks to guide the train ing of another network. Typically, the welltrained network is named as the teacher network while the network to be trained is named as the student network. For distillation, the predictions from the teacher network are leveraged and referred to as the soft labels (Balan et al., 2015; M ¬®uller et al., 2019). Soft labels generated by the teacher network have been proven effective in largescale em pirical studies (Liang et al., 2019; Tian et al., 2020; Zagoruyko & Komodakis, 2017; Romero et al., 2015) as well as recent theoretical studies (Phuong & Lampert, 2019). However, the reason why soft labels are beneÔ¨Åcial to the student network is still not well explained. Giving a clear theoretical explanation is challenging: The optimization details of a deep network with the common onehot labels are still not wellstudied (Nagarajan & Kolter, 2019), not to mention training with the soft labels. Nevertheless, two recent studies (M ¬®uller et al., 2019; Yuan et al., 2020) shed light on the intuitions about how the soft labels work. SpeciÔ¨Åcally, label smoothing, which is a special case of soft labels based training, is shown to regularize the activations of the penultimate layer to the network (M ¬®uller et al., 2019). The regularization property of soft labels is further explored in (Yuan et al., 2020). They hypothesize that in KD, one main reason why the soft labels work is the regularization introduced by soft labels. Based on the assumption, the authors These authors contributed equally to this work. yWork done while the author was a research intern at Horizon Robotics. 1arXiv:2102.00650v1  [cs.LG]  1 Feb 2021Published as a conference paper at ICLR 2021 design a teacherfree distillation method by turning the predictions of the student network into soft labels. Considering that soft labels are targets for distillation, the evidence of the regularization brought by soft labels drives us to rethink soft labels for KD: Soft labels are both supervisory signals and regularizers. Meanwhile, it is known that there is a tradeoff between Ô¨Åtting the data and imposing regularizations, i.e., the biasvariance dilemma (Kohavi & Wolpert, 1996; Bishop, 2006), but it is unclear how bias and variance change for distillation with soft labels. Since the biasvariance tradeoff is an important issue in statistical learning, we investigate whether the biasvariance tradeoff exists for soft labels and how the tradeoff affects distillation performance. We Ô¨Årst compare the bias and variance decomposition of direct training with that of distillation with soft labels, noticing that distillation results in a larger bias error and a smaller variance. Then, we rewrite distillation loss into the form of a regularization loss adding the direct training loss. Through inspecting the gradients of the two terms during training, we notice that for soft labels, the bias variance tradeoff varies samplewisely. Moreover, by looking into a conclusion from (M ¬®uller et al., 2019), we observe that under the same temperature setting, the distillation performance is nega tively associated with the number of some certain samples. These samples lead to bias increase and variance decrease and we name them as regularization samples. To investigate how regularization samples affect distillation, we Ô¨Årst examine if we can design ad hoc Ô¨Ålters for soft labels to avoid training with regularization samples. But completely Ô¨Åltering out regularization samples also de teriorates distillation performance, leading us to speculate that regularization samples are not well handled by standard KD. In the light of these Ô¨Åndings, we propose weighted soft labels for distil lation to handle the samplewise biasvariance tradeoff, by adaptively assigning a lower weight to regularization samples and a larger weight to the others. To sum up, our contributions are: ‚Ä¢ For knowledge distillation, we analyze how the soft labels work from a perspective of bias variance tradeoff. ‚Ä¢ We discover that the biasvariance tradeoff varies samplewisely. Also, we discover that if we Ô¨Åx the distillation temperature, the number of regularization samples is negatively associated with the distillation performance. ‚Ä¢ We design straightforward schemes to alleviate negative impacts from regularization sam ples and then propose the novel weighted soft labels for distillation. Experiments on large scale datasets validate the effectiveness of the proposed weighted soft labels. 2 R ELATED WORKS "
2,EEG-based video identification using graph signal modeling and graph convolutional neural network.txt,"This paper proposes a novel graph signal-based deep learning method for
electroencephalography (EEG) and its application to EEG-based video
identification. We present new methods to effectively represent EEG data as
signals on graphs, and learn them using graph convolutional neural networks.
Experimental results for video identification using EEG responses obtained
while watching videos show the effectiveness of the proposed approach in
comparison to existing methods. Effective schemes for graph signal
representation of EEG are also discussed.","The brain signal provides the most comprehensive informa tion regarding the mental state of a human subject. Many applications exploiting brain signals have been attempted, including neurological disease detection, emotion recogni tion, and behavioral modeling. There are several types of brain signals that can be used, such as electroencephalogra phy (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI). In particular, EEG has been considered as a promising solution for various real world applications thanks to the advances of portable EEG devices and signal processing techniques. Brain signals generated from different brain regions may have relationship, which can be exploited for analysis. One way to describe such relationship is based on the physical dis tances between different regions. Another way developed re cently is based on functional connectivity, which is deÔ¨Åned as similarity between signals from different regions, e.g., cross c 2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. This work was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Korea gov ernment (MSIT) (NRF2016R1E1A1A01943283).correlation, mutual information, phase synchronization, and imaginary coherence [1][2]. Recently, graph signal processing has been proposed to process irregularly structured signals effectively [3][4]. It is to extend traditional digital signal processing techniques to signals that are not sampled on regular domains (such as time and grid space) but reside on graphs composed of vertices and edges. Furthermore, deep learning on graph signals has been also studied, and neural network structures for graph signals were proposed [5][6][7]. Brain signals are good examples of graph signals, because graphs are suitable to represent physical or functional connec tivity across different brain regions. However, there exist lit tle work on applying graph signal processing techniques and graph signalbased deep learning methods to model brain sig nals, particularly EEG. This is probably due to the limited number of channels (i.e., electrodes) of EEG, which may not be sufÔ¨Åcient for rich graph representation. This paper proposes a method for deep learning on graph signals for EEG analysis and its application to EEGbased video identiÔ¨Åcation. To our best knowledge, this is the Ô¨Årst attempt to apply graph signalbased deep learning techniques to EEG. In particular, we present various ways to convert EEG signals into graph signals having appropriate graph structures and signal features, which can overcome the low dimension ality of EEG, and use the graph convolutional neural network (GCNN) to learn the graph signals. We deal with an EEG classiÔ¨Åcation problem where the visual stimulus watched by a human subject is identiÔ¨Åed through EEG. 2. RELATED WORK "
238,Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification.txt,"In this paper, we first introduce batch normalization to the edRVFL network.
This re-normalization method can help the network avoid divergence of the
hidden features. Then we propose novel variants of Ensemble Deep Random Vector
Functional Link (edRVFL). Weighted edRVFL (WedRVFL) uses weighting methods to
give training samples different weights in different layers according to how
the samples were classified confidently in the previous layer thereby
increasing the ensemble's diversity and accuracy. Furthermore, a pruning-based
edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based
on their importance for classification before generating the next hidden layer.
Through this method, we ensure that the randomly generated inferior features
will not propagate to deeper layers. Subsequently, the combination of weighting
and pruning, called Weighting and Pruning based Ensemble Deep Random Vector
Functional Link Network (WPedRVFL), is proposed. We compare their performances
with other state-of-the-art deep feedforward neural networks (FNNs) on 24
tabular UCI classification datasets. The experimental results illustrate the
superior performance of our proposed methods.","Deep learning has been extremely successful in recent years. Ranging from vision and video tasks to natural language processing, these deep neural networks have reached stateoftheart Email addresses: qiushi001@e.ntu.edu.sg (Qiushi Shi), epnsugan@ntu.edu.sg (Ponnuthurai Nagaratnam Suganthan), rakeshku001@e.ntu.edu.sg (Rakesh Katuwal) Preprint submitted to arXiv January 24, 2022arXiv:2201.05809v2  [cs.LG]  21 Jan 2022results in multiple domains [1, 2]. In conventional neural networks, backpropagation methods are used to train a large number of parameters in these models [3]. Although such a training method makes it possible to optimize the parameters, the timeconsuming training process has become a severe problem in recently designed complex neural networks. Also, a BPtrained neural network may fall into a local minimum and gives a suboptimal result [4, 5, 6]. By looking at the Kaggle competitions that have no relation with vision or sequence, we can easily nd that deep learning is not always the best solution for diverse tasks [7, 8]. At the same time, another kind of neural network based on randomization is attracting signif icant attention because of its superiority to overcome the shortcomings of the conventional models [9, 5, 10]. It has been successfully applied to a range of tasks from classication [11, 12, 13], regres sion [14, 15], visual tracking [16], to forecasting [17, 18]. Instead of using backpropagation to train, this randomizationbased neural network frequently uses a closedform solution to optimize param eters in the model [19]. Unlike the BPtrained neural networks which need multiple iterations, the randomizationbased neural networks only need to be trained once by feeding all the samples to the model together. Among these models, Random Vector Functional Link Network (RVFL) [20] is a typical representative with a single hidden layer. Its universal approximation ability has been proved in [21]. The weights and biases are randomly generated in this neural network. And its uniqueness lies in a direct link that connects the information from the input layer to the output layer. However, due to dierent random seeds and perturbations in the training set, this randomized neural network can perform quite dierently in each realization [22]. To increase the performance, stability, and robustness of this model, two improved structures named Deep Random Vector Functional Link Network (dRVFL) and Ensemble Deep Random Vector Functional Link Network (edRVFL) were proposed [23]. The dRVFL network is a deep version of RVFL network, which allows the existence of multiple hidden layers, while edRVFL network treats each hidden layer as a classier to compose an ensemble. However, with the edRVFL network goes deeper, the divergence of the randomized hidden features will become a serious problem. Therefore, using normalization methods to renormalize the hidden features is extremely important for improving the performance of the edRVFL network. In this paper, we employ the batch normalization scheme [24] to do the renormalization work. To the best of our knowledge, this is the rst time that batch normalization is introduced to the randomized neural network. After the renormalization process, the mean and the variance of the 2hidden features will become 0 and 1. Then, we scale and shift these values to increase the expression capacity of the neural network. Besides, there are still some drawbacks to the edRVFL network. Firstly, for every layer (or classier) in the edRVFL network, they share the same training samples. Meanwhile, these training samples have the same weights in the training process. Compared to ensemble methods that using diering training bags for each classier, these ensemble frameworks which utilize similar training sets usually perform worse [25, 26]. Moreover, the testing accuracy for the last few layers may slightly go down when the network becomes deeper. We believe that some inferior features can be generated since we randomly generate the weights for the hidden neurons. And these useless features will propagate to deeper layers inducing further inferior features to decrease the overall testing accuracy. Thus, for solving the rst problem, we introduce a weighting matrix. Each training sample will be allocated a particular weight when performing the closedform solution depending on its performance in the previous layer. Our approach diers from Weighted Extreme Learning Machine [27] which gives weights to each sample for addressing the problem of imbalance learning. The main purpose is to ensure that dierent classiers can have their preference for a particular portion of the training samples that were not classied with high condence in the previous layer. We have also tried to apply the sample weighting method of Adaboost [28]. However, most of the samples will be given weights near zero while only a few can be allocated reasonable weights. Therefore, we propose four dierent weighting methods in this paper, and this improved variant of edRVFL network is named Weighted Ensemble Deep Random Vector Functional Link Network (WedRVFL). Besides, pruning algorithms are widely used to reduce the heavy computational cost of deep neural networks in lowresource settings [29]. Dierent eective techniques have been proposed to cut o the redundant part of the neural network models [30, 31, 32, 33]. In our case, we perform it by selecting some inferior features in the hidden layer and prune them permanently. The selection process can help to prevent the propagation of inferior features and maintain the testing accuracy for deeper layers. We named this improved variant of edRVFL network as Pruningbased Ensemble Deep Random Vector Functional Link Network (PedRVFL). Although there was previous work that applying pruning strategy to the RVFL network in [34], we would like to highlight that our work is dierent from theirs at the following point: They do pruning after training to shrink the size of the neural network. However, we perform pruning during the generation step so that inferior features 3will not propagate to deeper layers. Additionally, we integrate the advantages of WedRVFL and PedRVFL to create a combined model called Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network (WPedRVFL). The key contributions of this paper are summarized as follows: ‚Ä¢We introduce the batch normalization to the edRVFL network for renormalizing the hidden features. ‚Ä¢We employ the weighting scheme to allocate dierent weights to dierent samples in the edRVFL network. We name it WedRVFL network. The weight matrix changes according to the samples' predictions in the previous layers. This method can make sure that each hidden layer in the network has dierent biases for each sample and increase the ensemble classication accuracy. ‚Ä¢We propose pruning based edRVFL network called PedRVFL network. Instead of pruning neurons after the training process, we cut o the inferior neurons according to their importance for classication when we are training the model. This method can prevent the propagation of detrimental features and increase the classication accuracy in deeper layers. ‚Ä¢The combination of weighting and pruning based edRVFL network named WPedRVFL net work is also presented in the paper. ‚Ä¢The empirical results show the superiority of our new methods over 11 stateoftheart methods on 24 UCI benchmark datasets. The rest of the paper is organized as follows: Section 2 outlines the basic concepts of RVFL network and illustrates the ensemble deep version of this structure. Section 3 introduces the re normalization method for the edRVFL network. Then Section 4 gives details about our new pro posed versions of edRVFL network. In Section 5, the performance of our methods, as well as other deep feedforward neural networks (FNNs) and RVFL variants are compared. Finally, conclusions and future research directions are presented in Section 6. 2. Related works "
165,Ex uno plures: Splitting One Model into an Ensemble of Subnetworks.txt,"Monte Carlo (MC) dropout is a simple and efficient ensembling method that can
improve the accuracy and confidence calibration of high-capacity deep neural
network models. However, MC dropout is not as effective as more
compute-intensive methods such as deep ensembles. This performance gap can be
attributed to the relatively poor quality of individual models in the MC
dropout ensemble and their lack of diversity. These issues can in turn be
traced back to the coupled training and substantial parameter sharing of the
dropout models. Motivated by this perspective, we propose a strategy to compute
an ensemble of subnetworks, each corresponding to a non-overlapping dropout
mask computed via a pruning strategy and trained independently. We show that
the proposed subnetwork ensembling method can perform as well as standard deep
ensembles in both accuracy and uncertainty estimates, yet with a computational
efficiency similar to MC dropout. Lastly, using several computer vision
datasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally
demonstrate that subnetwork ensembling also consistently outperforms recently
proposed approaches that efficiently ensemble neural networks.","An effective way to improve model accuracy and conÔ¨Ådence calibration in deep learning is ensembling. One efÔ¨Åcient technique that leverages this idea is ""Monte Carlo (MC) dropout"" [ 12] which extends the popular dropout technique used for regularization during training [ 42]. In MC Dropout, testtime inference involves multiple forward passes through the model, each executed with a different random dropout mask as in during the training phase. This yields an ensemble of predictions which are then averaged. It has been shown that MC dropout implements approximate Bayes averaging [ 12] and empirically yields enhanced uncertainty estimates and accuracy. While MC dropout can improve a baseline model, it is still inferior to explicit ensembles of neural networks trained independently with random initialization (called deep ensembles) [ 30]. Using the perspective of the errorambiguity decomposition [ 52], we can attribute this performance gap to the relatively poor performance of individual models and/or limited diversity in the MC dropout ensemble. We further hypothesize that these issues are largely due to the extensive parameter sharing among MC dropout models and the coupled training process. With this perspective in mind, we explore the idea of creating an ensemble of subnetworks in which a predetermined number of nonoverlapping dropout masks are used. We present an easytoimplement greedy optimization procedure that sequentially computes dropout masks via a recent dropoutmask optimization technique and trains each subnetwork independently. The resulting algorithm enables us to obtain a diverse ensemble of nonoverlapping subnetworks within one deep neural network. That is, we are able create many models out of one1. We experimentally demonstrate that subnetwork 1Hence our title: Ex uno plures. Preprint. Under review.arXiv:2106.04767v1  [cs.LG]  9 Jun 2021ensembling consistently outperforms MC dropout and several other recently proposed approaches that efÔ¨Åciently ensemble neural networks in terms of both accuracy and uncertainty estimates. We also show that our proposed approach achieves results on par with that of deep ensembles, yet with the much better testtime computational efÔ¨Åciency of MC dropout. Summary of Contributions. 1.We present the novel idea of ensembling nonoverlapping subnetworks within one standard neural network architecture. 2.We propose a simple sequential pruning based procedure to enhance the performance of subnetwork ensembling. 3.We demonstrate and discuss the regularization effect achieved by training pruned networks and using a randomized and frozen fully connected layer in the network. 4.Our experiments demonstrate that subnetwork ensembling outperforms MC dropout and several stateoftheart methods for efÔ¨Åcient ensembling. 2 Related Works "
523,Noise Mitigation for Neural Entity Typing and Relation Extraction.txt,"In this paper, we address two different types of noise in information
extraction models: noise from distant supervision and noise from pipeline input
features. Our target tasks are entity typing and relation extraction. For the
first noise type, we introduce multi-instance multi-label learning algorithms
using neural network models, and apply them to fine-grained entity typing for
the first time. This gives our models comparable performance with the
state-of-the-art supervised approach which uses global embeddings of entities.
For the second noise type, we propose ways to improve the integration of noisy
entity type predictions into relation extraction. Our experiments show that
probabilistic predictions are more robust than discrete predictions and that
joint training of the two tasks performs best.","Knowledge bases (KBs) are important resources for natural language processing tasks like ques tion answering and entity linking. However, KBs are far from complete (e.g., Socher et al. (2013)). Therefore, methods for automatic knowledge base completion (KBC) are beneÔ¨Åcial. Two subtasks of KBC are entity typing (ET) andrelation extraction (RE) . We address both tasks in this paper. As in other information extraction tasks, obtain ing labeled training data for ET and RE is chal lenging. The challenge grows as labels become more Ô¨Ånegrained. Therefore, distant supervision (Mintz et al., 2009) is widely used. It reduces the need for manually created resources. Distant su pervision assumes that if an entity has a type (resp. two entities have a relationship) in a KB, then all sentences mentioning that entity (resp. thosetwo entities) express that type (resp. that relation ship). However, that assumption is too strong and gives rise to many noisy labels. Different tech niques to deal with that problem have been in vestigated. The main technique is multiinstance (MI) learning (Riedel et al., 2010). It relaxes the distant supervision assumption to the assumption that at least one instance of a bag (collection of all sentences containing the given entity/entity pair) expresses the type/relationship given in the KB. Multiinstance multilabel (MIML) learning is a generalization of MI in which one bag can have several labels (Surdeanu et al., 2012). Most MI and MIML methods are based on hand crafted features. Recently, Zeng et al. (2015) in troduced an endtoend approach to MI learning based on neural networks. Their MI method takes the most conÔ¨Ådent instance as the prediction of the bag. Lin et al. (2016) further improved that method by taking other instances into account as well; they proposed MI learning based on selective attention as an alternative way of relaxing the im pact of noisy labels on RE. In selective attention, a weighted average of instance representations is calculated Ô¨Årst and then used to compute the pre diction of a bag. In this paper, we introduce two multilabel ver sions of MI. (i) MIMLMAX takes the maximum instance for each label. (ii) MIMLATT applies, for each label, selective attention to the instances. We apply MIMLMAX and MIMLATT to Ô¨Åne grained ET. In contrast to RE, the ET task we con sider contains a larger set of labels, with a variety of different granularities and hierarchical relation ships. We show that MIMLATT deals well with noise in corpuslevel ET and improves or matches the results of a supervised model based on global embeddings of entities. The second type of noise we address in this pa per inÔ¨Çuences the integration of ET into RE. It hasarXiv:1612.07495v2  [cs.CL]  10 Jan 2017been shown that adding entity types as features im proves RE models (cf. Ling and Weld (2012), Liu et al. (2014)). However, noisy training data and difÔ¨Åculties of classiÔ¨Åcation often cause wrong pre dictions of ET and, as a result, noisy inputs to RE. To address this, we propose a joint model of ET and RE and compare it with methods that integrate ET results in a strict pipeline. The joint model per forms best. Among the pipeline models, we show that using probabilities instead of binary decisions better deals with noise (i.e., possible ET errors). To sum up, our contributions are as follows. (i) We introduce new algorithms for MIML us ing neural networks. (ii) We apply MIML to Ô¨Åne grained entity typing for the Ô¨Årst time and show that it outperforms the stateoftheart supervised method based on entity embeddings. (iii) We show that a novel way of integrating noisy entity type predictions into a relation extraction model and joint training of the two tasks lead to large im provements of RE performance. We release code and data for future research.1 2 Related Work "
496,On the Effectiveness of Neural Ensembles for Image Classification with Small Datasets.txt,"Deep neural networks represent the gold standard for image classification.
However, they usually need large amounts of data to reach superior performance.
In this work, we focus on image classification problems with a few labeled
examples per class and improve data efficiency by using an ensemble of
relatively small networks. For the first time, our work broadly studies the
existing concept of neural ensembling in domains with small data, through
extensive validation using popular datasets and architectures. We compare
ensembles of networks to their deeper or wider single competitors given a total
fixed computational budget. We show that ensembling relatively shallow networks
is a simple yet effective technique that is generally better than current
state-of-the-art approaches for learning from small datasets. Finally, we
present our interpretation according to which neural ensembles are more sample
efficient because they learn simpler functions.","The advent of deep learning (DL) has revolutionized the computer vision Ô¨Åeld [43]. However, the cost to reach high recognition performances involves the collection and label ing of large quantities of images. This requirement can not always be fulÔ¨Ålled since it may happen that collecting im ages is extremely expensive or not possible at all. Differ ent approaches have been proposed by the research commu nity to mitigate the necessity of training data, tackling the problem from different perspectives. Among them of par ticular interest are deÔ¨Ånitely transfer and fewshot learning [4], [36] [37]. Still, these approaches rely on a large set of imageannotation pairs on which reusable representations can be learnt. In this work, we propose the use of neural ensembles composed of smaller networks to tackle the problem of learning from a small sample and show the superiority of such methodology. Similarly to what has been done in recent works, we benchmark the approaches by varying the number of data points in the training sample while keeping it low with respect to the current standards of computer vi sion datasets [1], [3]. Due to its great difÔ¨Åculty, this problem is still unsolved and hardly experimented despite its primary importance. It has been shown that large convolutional neural net works (CNNs) can handle overÔ¨Åtting and generalize well even if they are severely overparametrized [19], [25]. A recent study has also empirically shown that such behav ior might also be valid in the case of tiny datasets, making large nets a viable choice even when the training sample is limited [6]. On the other hand, a wellknown technique to reduce model variance is to average predictions from a set of weak learners (e.g. random forests [7]). An ensemble of lowbias decorrelated learners, combined with randomized inputs and prediction averaging, generally mitigates over Ô¨Åtting. Despite the high popularity of neural ensembles, our study is the Ô¨Årst one that systematically trains them from scratch on datasets with few samples per class and gives empirical evidence of their advantage over stateof theart approaches. Our strict experimental methodology compares ensembles with stateoftheart methods and sin gle networks keeping a fair comparison in terms of model resources. Therefore, we study ensembles of CNNs in smalldata tasks by a) Ô¨Åxing a computational budget and b) compar ing them to corresponding deeper or wider single variants. According to our empirical study, ensembles are preferable over wider networks that are in turn better than deeper ones. Moreover, obtained results make ensembles of smaller net works a strong baseline and an advantageous basic building block for future works that will tackle the problem of learn ing from small datasets. In summary, the contributions of our work are the following: i) we systematically study neural ensembles with small datasets and show that they generally outper form stateoftheart methods; ii) we make a structured study comparing ensembles of smallerscale networks and their computationally equivalent single competitors with in 9876arXiv:2111.14493v1  [cs.CV]  29 Nov 2021creased depth or width; iii) we explain the better perfor mance of ensembles by showing their bias towards learning less complex functions. 2. Related Work "
138,Local Boosting for Weakly-Supervised Learning.txt,"Boosting is a commonly used technique to enhance the performance of a set of
base models by combining them into a strong ensemble model. Though widely
adopted, boosting is typically used in supervised learning where the data is
labeled accurately. However, in weakly supervised learning, where most of the
data is labeled through weak and noisy sources, it remains nontrivial to design
effective boosting approaches. In this work, we show that the standard
implementation of the convex combination of base learners can hardly work due
to the presence of noisy labels. Instead, we propose $\textit{LocalBoost}$, a
novel framework for weakly-supervised boosting. LocalBoost iteratively boosts
the ensemble model from two dimensions, i.e., intra-source and inter-source.
The intra-source boosting introduces locality to the base learners and enables
each base learner to focus on a particular feature regime by training new base
learners on granularity-varying error regions. For the inter-source boosting,
we leverage a conditional function to indicate the weak source where the sample
is more likely to appear. To account for the weak labels, we further design an
estimate-then-modify approach to compute the model weights. Experiments on
seven datasets show that our method significantly outperforms vanilla boosting
methods and other weakly-supervised methods.","Weaklysupervised learning (WSL) has gained significant attention as a solution to the challenge of label scarcity in machine learning. WSL leverages weak supervision signals, such as labeling functions or other models, to generate a large amount of weakly labeled data, which is easier to obtain than complete annotations. Despite achieving promising results in various tasks including text classifi cation [ 1], sequence tagging [ 2], and ecommerce [ 3], an empirical study [ 4] reveals that even stateoftheart WSL methods still un derperform fullysupervised methods by significant margins, where the average performance discrepancy is 18.84%, measured by accu racy or F1 score. On the other hand, boosting algorithm is one of the most com monly used approaches to enhance the performance of machine learning models by combining multiple base models [ 5‚Äì9]. For ex ample, AdaBoost [ 5] dynamically adjusts the importance weight of each training example to learn multiple base models and uses a weighted combination to aggregate these base models‚Äô predictions. XGBoost [ 8] iteratively computes the gradients and hessians de fined on a clean training set to fit base learners and combines their predictions via weighted summation. Despite the encouring perfor mance, these boosting algorithms usually assume the availability of a clean labeled dataset. In WSL, however, the imperfect supervision signals interfere with the training data importance reweightingarXiv:2306.02859v1  [cs.LG]  5 Jun 2023KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA Rongzhi Zhang et al. which further prevents us from computing an accurate weight of each base learner. If we naively apply these supervised boosting methods using a weakly labeled dataset, we observe a phenome non called ‚Äú weight domination ‚Äù where the assigned weight of the initial base model is too large and dominates the ensemble model prediction, as shown in Figure 1. A key challenge of adapting boosting methods to the WSL set ting is to accurately compute the importance of each example in the weakly labeled training data for each base learner. Previously, when a clean dataset is provided, the goal of data importance reweighing process is to prioritize instances with large errors for subsequent base learner training. This effectively localizes the base learner to the error region in the label space. However, in WSL, the noisy labels hinder the accurate identification of error instances and thus we need to shift our focus from the label space to the training data space. One potential approach is to partition the weaklylabeled training data into subsets and constructs a mixture of expert mod els (MoE) [ 10] where each expert is localized for one training data subset. Along this line, Tsai et al. [11] propose partitioning the unlabeled dataset into latent semantics subsets and using multiple expert models to discriminate instances. However, this approach assumes the input data naturally reside in a homogeneous feature space and requires a hyperparameter search to appropriately lo calize the expert models. Additionally, the offtheshelf clusters do not adapt during the learning process, which conflicts with the philosophy of boosting methods. We investigate the problem of boosting in the context of weakly supervised learning, where most of the data are labeled by weak sources and only a limited number of data points have accurate labels. To address the difficulties posed by this setting, we introduce LocalBoost , a novel iterative and adaptive framework for WSL boosting. LocalBoost retains the essential concepts of the tradi tional boosting approach while incorporating adaptations specifi cally designed for the WSL scenario, described as follows: ‚Ä¢Base Learner Locality . Motivated by the challenges posed by the data reweighting approach in AdaBoost for WSL and the limi tations of hard clustering in MoE methods, we propose a new approach to base learner localization. In AdaBoost, largeerror instances are assigned with larger weights for model training, however, this approach does not account for the fact that error instances exist in multiple feature regimes that are difficult to capture with weak labels. Additionally, the rigid clusters and fixed expert models in MoE cannot adapt in the iterative learn ing process, hindering the framework‚Äôs ability to dynamically target weak feature regimes and build upon preceding models. To address these issues, our proposed framework LocalBoost assigns base learners to adaptively updated local regions in the embedding space, thereby introducing locality to the base learners. ‚Ä¢Twodimension Boosting . Effective aggregation of localized base learners in WSL goes beyond the simple convex combination in supervised settings (as shown in Sec. 4.2). To account for po tential label noises from weak sources, we aim to learn multiple complementary base learners in LocalBoost . To fulfill this goal, we introduce a weighting function to compute the conditional probability of weak sources that are more likely to annotatea given data instance. We further design a twodimensional boosting framework in LocalBoost , where intersource boost ing and intrasource boosting are performed alternately. The former improves the base learners within a given weak source, while the latter complements the base learners with additional models trained from other weak sources. ‚Ä¢Interactions between Weak and Clean Labels . We incorporate the interactions between weak and clean labels into Local Boost framework in two steps: (1) We compute a mapping between the small clean dataset and the large weakly labeled dataset to localize base learners in the data embedding space. We first identify the errors made by the current model ensemble, and then sample corresponding clusters from the large weak dataset to form the training set for the next base learner. (2) We propose a novel estimatethenmodify approach for computing base learner weights. Initially, the weights are estimated on the large weakly labeled dataset. Then, we refine these estimates by generating multiple perturbations of the model weights and selecting the one that results in the lowest error rate on the small clean dataset as the modified weights. We evaluate LocalBoost on seven datasets including senti ment analysis, topic classification and relation classification from WRENCH [ 4], the standard benchmark for weakly supervised learn ing. The results indicate that LocalBoost achieves superior per formance compared with other stateoftheart methods. Moreover, our analysis further confirms the effectiveness of boosting in two dimensions and incorporating interactions between weak and clean labels. We summarize our key contributions as follows: (1)We present LocalBoost1, a novel weaklysupervised boosting framework that implements progressive intersource and intra source boosting. (2)We incorporate explicit locality into the base learners of the boosting framework, allowing them to specialize in finergrained data regions and perform well in specific feature regimes. (3)We leverage the interactions between weak and clean labels for effective base learner localization and weight estimation. (4)We conduct extensive experiments on seven benchmark datasets and demonstrate the superiority of LocalBoost over WSL and ensemble baselines. 2 RELATED WORK "
45,UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning.txt,"Supervised deep learning methods require a large repository of annotated
data; hence, label noise is inevitable. Training with such noisy data
negatively impacts the generalization performance of deep neural networks. To
combat label noise, recent state-of-the-art methods employ some sort of sample
selection mechanism to select a possibly clean subset of data. Next, an
off-the-shelf semi-supervised learning method is used for training where
rejected samples are treated as unlabeled data. Our comprehensive analysis
shows that current selection methods disproportionately select samples from
easy (fast learnable) classes while rejecting those from relatively harder
ones. This creates class imbalance in the selected clean set and in turn,
deteriorates performance under high label noise. In this work, we propose
UNICON, a simple yet effective sample selection method which is robust to high
label noise. To address the disproportionate selection of easy and hard
samples, we introduce a Jensen-Shannon divergence based uniform selection
mechanism which does not require any probabilistic modeling and hyperparameter
tuning. We complement our selection method with contrastive learning to further
combat the memorization of noisy labels. Extensive experimentation on multiple
benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%
improvement over the current state-of-the-art on CIFAR100 dataset with a 90%
noise rate. Our code is publicly available","Deep neural networks (DNNs) have proven to be highly effective in solving various computer vision tasks [9,18,22, 36, 43, 49, 50, 56, 65]. Most stateoftheart (SOTA) meth ods require supervised training with a large pool of anno tated data [4, 8, 27, 28, 60]. Collecting and manually an 1https : / / github . com / nazmul  karim170 / UNICON  NoisyLabel SSL Training (Network 1)Semi Supervised LossContrastive LossFeature ExtractorCls. LayerTraining SetClean SetUniform Selection Noisy SetTraining SetClean SetUniform Selection Noisy SetClean LabelNoisy LabelProj. HeadSSL Training (Network 2)Semi Supervised LossContrastive LossFeature ExtractorCls. LayerProj. HeadFigure 1. U NICONtraining overview: At each iteration, we em ploy a uniform selection technique to partition the training set into clean and noisy sets. Upon separation, we perform SSLtraining with an additional contrastive loss function. The uniform selection and subsequent SSLtraining of two networks (with same architec ture) is repeated until convergence. Noise Rate (%) 90% 92% 95% 98% DMix [25] 76.08 57.62 51.28 17.18 UNICON(Ours) 90.81 87.61 80.82 50.63 Table 1. Classification performance (%) of the proposed method on CIFAR10 under severe label noise. notating such data is challenging and oftentimes very ex pensive. Most largescale data collection techniques rely on opensource web data that can be automatically anno tated using search engine queries and user tags [33, 54]. This annotation scheme inevitably introduces label noise [27, 60]. Training with such noisy labels is challenging since DNNs can effectively memorize arbitrary (noisy) la bels over the course of training [2]. Combating label noise is one of the fundamental problems in deep learn ing [15, 24, 38, 47, 57, 57, 61, 63, 64, 68], and is the focus of this study. Training with noisy label data has been the subject ofmany recent studies [12, 16, 31, 42, 46, 73]. Existing tech niques can be categorized into two dominant groups: i) la bel correction, [11,40] and ii) sample separation [12,25,69]. The former approach requires the estimation of noise tran sition matrix, which is hard to estimate for high number of classes and in high noise scenarios. The latter approach tries to filter out the noisy samples from the clean ones based on the smallloss criterion [25], where the lowloss samples are assumed to have clean labels. Next, an offtheshelf semi supervised learning (SSL) technique [3, 44, 48, 53] is used for training where the selected noisy samples are treated as unlabeled data. However, the selection process is usually biased towards easy classes as clean samples from the hard classes (e.g. cats and dogs can be considered as hard classes in CIFAR10 [21]) may produce highloss values. This is more prominent at the early stage of training and can intro duce classdisparity among the selected clean samples. Se vere classimbalance may lead to poor precision of sample selection, hence, subpar classification performance. In this work, we revamp the selection process from a more fundamental perspective. Our goal is to simplify the selection process by introducing an effective and scalable JensenShannon divergence based sample separation mech anism. To address the disproportionate selection of easy and hard samples, we enforce a classbalance prior by se lecting an equal number of clean samples from each class. Such a prior improves the overall quality of pseudolabels, and hence, significantly boosts the performance of subse quent semi supervised learningbased training. In addition, we opt to employ unsupervised contrastive learning (CL) because of its inherent resistance (as labels are not required for training) to label noise memorization. We empirically show that unsupervised feature learning lowers memoriza tion risk and improves the sample separation performance; especially under severe noise levels. We call this combined technique of U NIform selection and C ONtrastive learning UNICON(shown in Fig. 1), which is found to be effective even in the presence of very high label noise (see Table 1). Our contributions are summarized as follows: ‚Ä¢ We propose a simple yet effective uniform selection mechanism that ensures classbalancing among the selected clean samples. Through empirical analy sis, we observe that classuniformity helps in gener ating higher quality pseudolabels for samples from all classes irrespective of their difficulty level. ‚Ä¢ We further minimize the risk of label noise memoriza tion by performing unsupervised feature learning using contrastive loss. This in turn boosts the sample separa tion performance. ‚Ä¢ Our extensive experimentation demonstrates that U NI CONachieves significant performance improvement over stateoftheart methods, especially on datasets with severe label noise.2. Related Work "
75,Normalized Loss Functions for Deep Learning with Noisy Labels.txt,"Robust loss functions are essential for training accurate deep neural
networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown
that the commonly used Cross Entropy (CE) loss is not robust to noisy labels.
Whilst new loss functions have been designed, they are only partially robust.
In this paper, we theoretically show by applying a simple normalization that:
any loss can be made robust to noisy labels. However, in practice, simply being
robust is not sufficient for a loss function to train accurate DNNs. By
investigating several robust loss functions, we find that they suffer from a
problem of underfitting. To address this, we propose a framework to build
robust loss functions called Active Passive Loss (APL). APL combines two robust
loss functions that mutually boost each other. Experiments on benchmark
datasets demonstrate that the family of new loss functions created by our APL
framework can consistently outperform state-of-the-art methods by large
margins, especially under large noise rates such as 60% or 80% incorrect
labels.","Training accurate deep neural networks (DNNs) in the pres ence of noisy (incorrect) labels is of great practical impor tance. Different approaches have been proposed for robust learning with noisy labels. This includes 1) label correction methods that aim to identify and correct wrong labels (Xiao et al., 2015; Vahdat, 2017; Veit et al., 2017; Li et al., 2017b); 2) loss correction methods that correct the loss function based on an estimated noise transition matrix (Sukhbaatar et al., 2014; Reed et al., 2014; Patrini et al., 2017; Han et al., 2018a); 3) reÔ¨Åned training strategies that modify the train ing procedure to be more adaptive to incorrect labels (Jiang et al., 2018; Wang et al., 2018; Tanaka et al., 2018; Ma et al., *Equal contribution1The University of Melbourne, Australia 2Shanghai Jiao Tong University, China. Correspondence to: Yisen Wang<eewangyisen@gmail.com >. Proceedings of the 37thInternational Conference on Machine Learning , Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).2018; Han et al., 2018b); and 4) robust loss functions that are inherently tolerant to noisy labels (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c). Compared to the Ô¨Årst three approaches that may suffer from inaccurate noise estimation or involve sophisticated training procedure modiÔ¨Åcations, robust loss functions provide a simpler solu tion, which is also the main focus of this paper. It has been theoretically shown that some loss functions such as Mean Absolute Error (MAE) are robust to label noise, while others are not, which unfortunately includes the commonly used Cross Entropy (CE) loss. This has mo tivated a body of work to design new loss functions that are inherently robust to noisy labels. For example, Gen eralized Cross Entropy (GCE) (Zhang & Sabuncu, 2018) was proposed to improve the robustness of CE against noisy labels. GCE can be seen as a generalized mixture of CE and MAE, and is only robust when reduced to the MAE loss. Recently, a Symmetric Cross Entropy (SCE) (Wang et al., 2019c) loss was suggested as a robustly boosted version of CE. SCE combines the CE loss with a Reverse Cross En tropy (RCE) loss, and only the RCE term is robust. Whilst these loss functions have demonstrated improved robustness, theoretically, they are only partially robust to noisy labels. Different from previous works, in this paper, we theoreti cally show that any loss can be made robust to noisy labels, and all is needed is a simple normalization. However, in practice, simply being robust is not enough for a loss func tion to train accurate DNNs. By investigating several robust loss functions, we Ô¨Ånd that they all suffer from an underÔ¨Åt ting problem. Inspired by recent developments in this Ô¨Åeld, we propose to characterize existing loss functions into two types: 1) ‚ÄúActive‚Äù loss, which only explicitly maximizes the probability of being in the labeled class, and 2) ‚ÄúPassive‚Äù loss, which also explicitly minimizes the probabilities of being in other classes. Based on this characterization, we further propose a novel framework to build a new set of robust loss functions called Active Passive Losses (APLs). We show that under this framework, existing loss functions can be reworked to achieve the stateoftheart for training DNNs with noisy labels. Our key contributions are: We provide new theoretical insights into robust loss func tions demonstrating that a simple normalization can make any loss function robust to noisy labels.arXiv:2006.13554v1  [cs.LG]  24 Jun 2020Normalized Loss Functions for Deep Learning with Noisy Labels We identify that existing robust loss functions suffer from an underÔ¨Åtting problem. To address this, we propose a generic framework Active Passive Loss (APL) to build new loss functions with theoretically guaranteed robust ness and sufÔ¨Åcient learning properties. We empirically demonstrate that the family of new loss functions created following our APL framework can out perform the stateoftheart methods by considerable mar gins, especially under large noise rates of 60% or 80%. 2. Related Work "
122,Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee.txt,"Over-parameterized deep neural networks trained by simple first-order methods
are known to be able to fit any labeling of data. Such over-fitting ability
hinders generalization when mislabeled training examples are present. On the
other hand, simple regularization methods like early-stopping can often achieve
highly nontrivial performance on clean test data in these scenarios, a
phenomenon not theoretically understood. This paper proposes and analyzes two
simple and intuitive regularization methods: (i) regularization by the distance
between the network parameters to initialization, and (ii) adding a trainable
auxiliary variable to the network output for each training example.
Theoretically, we prove that gradient descent training with either of these two
methods leads to a generalization guarantee on the clean data distribution
despite being trained using noisy labels. Our generalization analysis relies on
the connection between wide neural network and neural tangent kernel (NTK). The
generalization bound is independent of the network size, and is comparable to
the bound one can get when there is no label noise. Experimental results verify
the effectiveness of these methods on noisily labeled datasets.","Modern deep neural networks are trained in a highly overparameterized regime, with many more trainable parameters than training examples. It is wellknown that these networks trained with simple Ô¨Årstorder methods can Ô¨Åt any labels, even completely random ones (Zhang et al., 2017). Although training on properly labeled data usually leads to good generalization performance, the ability to overÔ¨Åt the entire training dataset is undesirable for generalization when noisy labels are present. Therefore preventing overÔ¨Åtting is crucial for robust performance since mislabeled data are ubiquitous in very large datasets (Krishna et al., 2016). In order to prevent overÔ¨Åtting to mislabeled data, some form of regularization is necessary. A simple such example is early stopping , which has been observed to be surprisingly effective for this purpose (Rolnick et al., 2017; Guan et al., 2018; Li et al., 2019). For instance, training ResNet34 with early stopping can achieve 84% test accuracy on CIFAR10 even when 60% of the training labels are corrupted (Table 1). This is nontrivial since the test error is much smaller than the error rate in training data. How to explain such generalization phenomenon is an intriguing theoretical question. As a step towards a theoretical understanding of the generalization phenomenon for over parameterized neural networks when noisy labels are present, this paper proposes and analyzes two simple regularization methods as alternatives of early stopping: 1.Regularization by distance to initialization . Denote bythe network parameters and by p0qits random initialization. This method adds a regularizer }p0q}2to the training objective. 2.Adding an auxiliary variable for each training example. Let xibe theith training example andfp;qrepresent the neural net. This method adds a trainable variable biand tries to Ô¨Åt theith label using fp;xiq"
529,Population-Based Evolutionary Gaming for Unsupervised Person Re-identification.txt,"Unsupervised person re-identification has achieved great success through the
self-improvement of individual neural networks. However, limited by the lack of
diversity of discriminant information, a single network has difficulty learning
sufficient discrimination ability by itself under unsupervised conditions. To
address this limit, we develop a population-based evolutionary gaming (PEG)
framework in which a population of diverse neural networks is trained
concurrently through selection, reproduction, mutation, and population mutual
learning iteratively. Specifically, the selection of networks to preserve is
modeled as a cooperative game and solved by the best-response dynamics, then
the reproduction and mutation are implemented by cloning and fluctuating
hyper-parameters of networks to learn more diversity, and population mutual
learning improves the discrimination of networks by knowledge distillation from
each other within the population. In addition, we propose a cross-reference
scatter (CRS) to approximately evaluate re-ID models without labeled samples
and adopt it as the criterion of network selection in PEG. CRS measures a
model's performance by indirectly estimating the accuracy of its predicted
pseudo-labels according to the cohesion and separation of the feature space.
Extensive experiments demonstrate that (1) CRS approximately measures the
performance of models without labeled samples; (2) and PEG produces new
state-of-the-art accuracy for person re-identification, indicating the great
potential of population-based network cooperative training for unsupervised
learning.","Person reidentification (reID) aims to match persons in an image gallery collected from nonoverlapping cam era networks, which has attracted increasing interest thanks to its wide applications in security and surveilarXiv:2306.05236v1  [cs.CV]  8 Jun 20232 Yunpeng Zhai et al. ResNet 50 DenseNet 169 (a) Before training (b) Single model training (c) Multi model training (Ours ) Fig. 1: Feature distribution of the same samples with different methods where each color denotes a person identity. Single model training(b) uses the selflearning mechanism only to enhance the discrimination ability it already has before training(a) and still suffers from inaccurate pseudolabels. However, multimodel training(c) explores and exploits the complementary information among different models (marked by corresponding colored boxes) and achieves more discrimination. lance. Though supervised reID methods (Yang et al., 2020) (Zheng et al., 2016) have achieved very decent results, they are largely dependent on sufficient data with expensive manual annotation, which also require substantial personal identity information and entail privacy issues. By contrast, unsupervised reID not only reduces the cost of labeling but also protects personal privacy without checking images manually. Commonly, unsupervised reID can be divided into two categories: unsupervised domain adaptation (UDA) (Zhai et al., 2020a) (Zhong et al., 2020) and fully unsupervised reID (FU) (Chen et al., 2021a) (Lin et al., 2019) depending on whether using extra labeled data. In this study, we will mainly focus on the fully unsupervised setting which learns directly from unlabeled images and allows for more scalability in realworld deployments. To address the challenges of unsupervised reID, recent efforts concentrate on training individual neural networks by means of a selfimprovement strategy (Song et al., 2018) (Ge et al., 2020b). They attempt to learn better representations based on selfpredictedpseudolabels via clustering algorithms (Caron et al., 2018) or graph neural networks (Ye et al., 2017). However, a single model can use such a selflearning mechanism only to enhance the discrimination ability it already has and cannot tackle the incorrectly predicted pseudolabels, which prevents it from maximizing its discrimination. Due to the lack of diversity of single models, incorrect pseudolabels are likely to remain the same after unsupervised training such as the false positive samples where images of different persons are clustered into the same group or the false negative samples where the images of the same person are clustered into different groups, as shown in Fig. 1. Importantly, since models learn diverse discrimination with different architectures, the incorrect pseudolabels predicted by a model may be predicted correctly by another model, marked by boxes in Fig. 1(b). In this paper, we attempt to address unsupervised reID by multiple model training, in which the complementary information of different models can be integrated and utilized effectively to explore the various latent knowlPopulationBased Evolutionary Gaming for Unsupervised Person Reidentification 3 edge contained in unlabeled data (the quantitative analysis is shown in Sec. 4.4.1). However, multiple model training still faces two challenging issues: (1) How to learn diverse discrim ination with multiple different models? (2) How to select a set of better models from many diverse models for training? To tackle these issues, we propose a populationbased evolutionary gaming (PEG), which selects and trains discriminative models by exploration and exploitation of their diversity. PEG trains a pop ulation of models concurrently by iterative selection, reproduction, mutation, and population mutual learn ing of neural networks, as shown in Fig. 2. Specifically, selection adapts the whole population to the unlabeled data by selecting and preserving the optimal subset of networks with complementary discrimination ability while abandoning other networks out of the subset. This combinatorial optimization of networks in selection is modeled as a multiagent cooperative game and solved by the best response dynamics, in which each agent attempts to learn the best response to the other agents‚Äô action and thus leads to Nash equilibrium. Then, reproduction and mutation are performed on the selected population to increase its diversity by making multiple copies of each network and applying a stochastic disturbance to their hyperparameters. Selection and reproduction jointly maintain the size of the population. Afterward, population mutual learning is conducted among networks to assemble and further explore the discrimination capacity via knowledge dis tillation within populations. Each network learns rep resentations from both populationshared pseudolabels and softlabels predicted by other individual networks. Utilizing periodically performing selection, reproduc tion and mutation, population mutual learning, the evolutionary gaming process enables favorable traits and knowledge of neural networks to be transmitted through successive generations. In the evolution gaming, a core issue is to define the utility function of the game, that is, the criterion of network selection in the evolution. However, the evaluation of CNN models without labeled datasets has not been well studied. Here, we propose crossreference scatter (CRS), which can approximately evaluate the quality of networks using unlabeled samples. Generally, the pseudolabels predicted by better networks are more accurate; however, their accuracy cannot be directly evaluated when the ground truth is unavailable. More over, models trained by more accurate pseudolabels tend to achieve larger intracluster cohesion and inter cluster separation in the feature space because incorrect labels will enforce models to separate samples of the same class or aggregate samples of different classes.Motivated by this phenomenon, we indirectly evaluate a network according to the feature cohesion and sepa ration of a reference model that is trained by pseudo labels of the evaluated network. Hence, the CRS is defined by the ratio of the intercluster and intracluster variance of features to measure both separation and cohesion. We demonstrate that the CRS approximately reflects the discrimination capacity of models without ground truth data and thus promotes the evolution gaming to learn better representations. A preliminary version of this work has been partially published (Zhai et al., 2020c), which has demonstrated the effectiveness of mutual learning among multiple networks in unsupervised conditions. Based on that version, this manuscript has made great improvements, including: 1) We propose a novel populationbased evolutionary gaming (PEG) framework (Sec. 3.1). The previous algorithm works passively only on given net works, and cannot adaptively select the most suitable models from the model base. Based on the mutual learning, PEG additionally contains an iterative selec tion of networks via a multiagent cooperative game preventing the weak networks to distract the overall discrimination capability (Sec. 3.1.1). 2) We propose a new crossreference scatter (CRS) to approximately measure reID models without labeled data. To evaluate the model discrimination, the previous version intro duced inter/intracluster scatter to roughly modulate the weights of models during mutual learning. However, it cannot be considered as the utility function of the cooperative game in PEG due to the lack of capability to accurately evaluate models. This paper improves inter/intracluster scatter to crossreference scatter by adding a crossreference evaluation (CR) scheme (Sec. 3.1.1). 3) More qualitative and quantitative experi ments are conducted to evaluate the effectiveness of the method, including but not limited to the validation and analysis of CRS, the cooperative game, and PEG. In summary, our contribution is as follow: ‚ÄìIt proposes a novel populationbased evolutionary gaming framework for unsupervised person reID which trains a diverse population of neural networks by iterative selection, reproduction, mutation and mutual learning. ‚ÄìIt introduces a multiagent cooperative game for the selection of networks in the PEG, which aims to find and preserve an optimal subset of the population on unlabeled data. ‚ÄìIt investigates the evaluation of reID models us ing unlabeled data and proposes a crossreference scatter which approximately measures a model‚Äôs discrimination capability by indirectly estimating4 Yunpeng Zhai et al. its predicted pseudolabels according to the cohesion and separation of feature space. ‚ÄìExperiments show that PEG outperforms stateof theart methods on largescale datasets, indicating the great potential of populationbased multiple model training. 2 Related Works "
430,Identification of Rhetorical Roles of Sentences in Indian Legal Judgments.txt,"Automatically understanding the rhetorical roles of sentences in a legal case
judgement is an important problem to solve, since it can help in several
downstream tasks like summarization of legal judgments, legal search, and so
on. The task is challenging since legal case documents are usually not
well-structured, and these rhetorical roles may be subjective (as evident from
variation of opinions between legal experts). In this paper, we address this
task for judgments from the Supreme Court of India. We label sentences in 50
documents using multiple human annotators, and perform an extensive analysis of
the human-assigned labels. We also attempt automatic identification of the
rhetorical roles of sentences. While prior approaches towards this task used
Conditional Random Fields over manually handcrafted features, we explore the
use of deep neural models which do not require hand-crafting of features.
Experiments show that neural models perform much better in this task than
baseline methods which use handcrafted features.","Rhetorical role labelling of sentences in a legal document r efers to understanding what semantic function a sentence is associated with, such as fac ts of the case, arguments of the parties, the Ô¨Ånal judgement of the court, and so on. Ident ifying the rhetorical roles of sentences in a legal case document can help in a variety of d ownstream tasks like se mantic search [1], summarization [2,3], case law analysis [ 4], and so on. However, legal case documents are usually not well structured [5,6], and va rious themes often interleave with each other. For instance, the reason behind the judgmen t (Ratio of the decision) of ten interleaves with Precedents and Statutes. Hence it some times becomes difÔ¨Åcult even for human experts to understand the intricate differences b etween the rhetorical roles. Hence, automating the identiÔ¨Åcation of these rhetorical roles is a challengin g task. For supervised machine learning of the roles, it is importan t to develop a high qual ity gold standard corpus, capturing the rhetorical roles of sentences as accurately as pos sible. Different approaches for the task have constructed t heir own set of annotated doc 1Equal contribution by the Ô¨Årst and second authors. 2Corresponding Author: Paheli Bhattacharya; Email: paheli .cse.iitkgp@gmail.comuments [1, 2, 4], but do not report an extensive analysis on th e annotation process. Apart from InterAnnotator Agreement (IAA) scores, it is useful t o understand issues such as the amount of subjectivity associated to the labels. In this paper, we perform a system atic annotation study and an extensive interannotator stu dy. We show that even legal ex perts Ô¨Ånd it difÔ¨Åcult to distinguish some speciÔ¨Åc pairs of la bels, thus showing that some subjectivity is inherent in these labels. Prior attempts to automate the identiÔ¨Åcation of rhetorical roles of sentences in legal documents [2‚Äì4] rely on handcrafted features (see Section 2 for details) such as lin guistic cue phrases indicative of a particular rhetorical r ole [2, 3, 7], the sequential ar rangement of labels [2], and so on. Some of these features, e. g., indicator cue phrases, arelargely dependent on legalexpert knowledge which is expensive to obtain. Also, the handcrafted features developed in the prior works are ofte n speciÔ¨Åc to one or a few do mains/categories (e.g., Cyber crime and Trade secrets in [4 ]). It has not been explored whether one can devise a set of features that works for docume nts across domains. Recently developed deep learning, neural network models do not require hand engineering features, but are able to automatically learn t he features, given sufÔ¨Åcient amounts of training data. Additionally, such models perfor m better in tasks like classiÔ¨Å cation than methods using handcrafted features. In this paper, we explore two neural network models to automa tically identify the rhetorical roles of sentences in legal documents ‚Äì (i) a Hier archical BiLSTM model, and (ii) a Hierarchical BiLSTMCRF model. Similar models have b een used in the medical domain [8], but to our knowledge, this work is the Ô¨Årst to use t hem in the legal domain. We use these models for supervised classiÔ¨Åcation across seven rhetorical labels (classes) and over documents from Ô¨Åve different legal domains . The Hierarchical BiLSTMCRF model achieves a very good performance (Macro Fscore in the range[0.8‚àí0.9]), out performing baseline methods that use handcrafted feature s. We also analyse the rhetor ical roles predicted by our model, and Ô¨Ånd that the subjectiv ity between certain pairs of labels (e.g., Ratio vs. Precedent) that is present among t he human annotators is also reÔ¨Çected in the predictions by the algorithm. This is the Ô¨Årst paper on identifying rhetorical roles of sen tences in legal documents that brings together (i) an extensive annotation study, and (ii) deep learning models for automating the task.3 2. Related Work "
498,Single Model Ensemble using Pseudo-Tags and Distinct Vectors.txt,"Model ensemble techniques often increase task performance in neural networks;
however, they require increased time, memory, and management effort. In this
study, we propose a novel method that replicates the effects of a model
ensemble with a single model. Our approach creates K-virtual models within a
single parameter space using K-distinct pseudo-tags and K-distinct vectors.
Experiments on text classification and sequence labeling tasks on several
datasets demonstrate that our method emulates or outperforms a traditional
model ensemble with 1/K-times fewer parameters.","A model ensemble is a promising technique for increasing the performance of neural network mod els (Lars. and Peter., 1990; Anders and Jesper, 1994). This method combines the outputs of multi ple models that are individually trained using the same training data. Recent submissions to natural language processing(NLP) competitions are primar ily composed of neural network ensembles (Bojar et al., 2018; Barrault et al., 2019). Despite its ef fectiveness, a model ensemble is costly. Because it handles multiple models, it requires increased time for training and inference, increased memory, and greater management effort. Therefore, the model ensemble technique cannot always be applied to real systems, as many systems, such as edge de vices, must work with limited computational re sources. In this study, we propose a novel method that replicates the effects of the ensemble technique with a single model. Following the principle that aggregating multiple models improves per formance, we create multiple virtual models in a shared space. Our method virtually inÔ¨Çates the training data Ktimes withKdistinct pseudotags [Tag 1] I watched this ..[Tag 2] I watched this ..[Tag 3] I watched this ..ùíêùüë ùíÜùüé:ùëªùüë'ùíÜùüé:ùëªùüëùú±(ùë¨ùëµùë™('ùíÜùüé:ùëªùüè))ùú±(ùë¨ùëµùë™('ùíÜùüé:ùëªùüê))ùú±(ùë¨ùëµùë™('ùíÜùüé:ùëªùüë))Aggregateùíêùüè ùíÜùüé:ùëªùüè'ùíÜùüé:ùëªùüèùíêùüê ùíÜùüé:ùëªùüê'ùíÜùüé:ùëªùüêFigure 1: Overview of our proposed method. A single model processes the same input with distinct pseudo tags. Each pseudotag deÔ¨Ånes the kth virtual model, and the corresponding vector okis added to the em bedding. Thus, the model function of a singe model (ENC())generates different outputs. appended to all input data. It also incorporates K distinct vectors, which correspond to pseudotags. Each pseudotag k2f1;:::;Kgis attached to the beginning of the input sentence, and the kth vector is added to the embedding vectors for all tokens in the input sentence. Fig. 1 presents a brief overview of our proposed method. Intuitively, this opera tion allows the model to shift the embedding of the same data to the kth designated subspace and can be interpreted as explicitly creating Kvirtual mod els in a shared space. We thus expect to obtain the same (or similar) effects as the ensemble technique composed of Kmodels with our Kvirtual models generated from a single model. Experiments in text classiÔ¨Åcation and sequence labeling tasks reveal that our method outperforms single models in all settings with the same param eter size. Moreover, our technique emulates or surpasses the normal ensemble with 1=Ktimes fewer parameters on several datasets. 2 Related Work "
539,Data efficient surrogate modeling for engineering design: Ensemble-free batch mode deep active learning for regression.txt,"In a computer-aided engineering design optimization problem that involves
notoriously complex and time-consuming simulator, the prevalent approach is to
replace these simulations with a data-driven surrogate that approximates the
simulator's behavior at a much cheaper cost. The main challenge in creating an
inexpensive data-driven surrogate is the generation of a sheer number of data
using these computationally expensive numerical simulations. In such cases,
Active Learning (AL) methods have been used that attempt to learn an
input--output behavior while labeling the fewest samples possible. The current
trend in AL for a regression problem is dominated by the Bayesian framework
that needs training an ensemble of learning models that makes surrogate
training computationally tedious if the underlying learning model is Deep
Neural Networks (DNNs). However, DNNs have an excellent capability to learn
highly nonlinear and complex relationships even for a very high dimensional
problem. To leverage the excellent learning capability of deep networks along
with avoiding the computational complexity of the Bayesian paradigm, in this
work we propose a simple and scalable approach for active learning that works
in a student-teacher manner to train a surrogate model. By using this proposed
approach, we are able to achieve the same level of surrogate accuracy as the
other baselines like DBAL and Monte Carlo sampling with up to 40 % fewer
samples. We empirically evaluated this method on multiple use cases including
three different engineering design domains:finite element analysis,
computational fluid dynamics, and propeller design.","In the engineering and scientic community, surrogate modeling is a prevalent approach in the design process that involves computationally costly, complex simulators. Surrogate is a datadriven approximation for a physicsbased simulation typically involving interpolation or regression on a set of data generated from the original simulator[1]. Learning these highly complex nonlinear hyperplanes using learning models can assist human designers to nd good designs much faster than traditional methods. The expected benet of creating a surrogate is two folds: rst, leverage the generalization capability of learning models to get a good approximate prediction on other designs in the design space, and second, parallel faster evaluation of design points, which will speed up the whole design decision process. Creating a surrogate for a reasonably sized design space for a complex simulation process has two main issues: rst, since these simulation models and the simulation process have an iterative subroutine and consequently long simulation time, without any strategic sampling approach, the trivial randomized sampling approach to generate data becomes unavailing in high dimension design space because the number of data points needed to give reasonably uniform coverage rises exponentially this phenomenon is infamously called the curse of dimensionality[2]. The second issue is the stepsizing of the sampling i.e., without any prior information about the involved nonlinearity in the solution space manifold, setting an arbitrary step size for sample selection consequently aects the quality of the collected data and trained surrogate. Corresponding author Email address: harsh.vardhan@vanderbilt.edu (Harsh Vardhan) Preprint submitted to Engineering applications of Articial IntelligencearXiv:2211.10360v1  [cs.LG]  16 Nov 2022The utopian solution would be a dynamically adaptive step sizing to select samples with maximum informational value to learn these hyperplanes that produce an accurate surrogate in a few strategic samples. Traditional surrogate modeling approaches use learning models like kriging[1], Gaussian Process[3], etc., to do strategic adaptive sampling and step sizing but these learning models are not scalable for high dimensions or big design space. The recent advances in Machine Learning (ML) models, especially in Deep Learning (DL) have shown it as a promising paradigm in this scenario. Deep learning models have powerful learning capabilities, can learn in highdimension space and automatically extract salient features, and have been used in various engineering surrogate problems[4, 5, 6]. However, for using a deep learning model for surrogate modeling in a meaningful design space, we need to address the issue of strategic sampling and adaptive step sizing. In most learning problem settings, we rely on theories for data modeling, which often assume that the data is provided by a source that we do not control, but we can sample from the data source. However, for a practical design problem, the data labeling /evaluation is left to the designer and which is relatively computationally expensive or slow in some complex engineering domains. In this scenario, the designer needs to be frugal and should select samples with the goal to learn as much as possible. To this end, a framework or algorithm is required that can objectively estimate the utility of candidate data points. The AL framework directly addresses this by estimating the utility of candidate data points. By combining DL with AL (this subeld of ML is called DeepAL[7]), it is possible to retain the strong high dimensional learning capability of DL while also beneting from strategic samples selected by AL. Compared with the multiple research work on Deep active learning for classication, there are only a few approaches for Deep active learning for a regression problem. For a classication problem, since the output is the probability of a class, that can be easily used with other statistical measures (like Shannon's Entropy[8] function) to create a query strategy. Many research work has leveraged it to create one or another  avor of query strategy. However, most scientic surrogate modeling problem involved in the design process are regression problems and their output is a scalar value or a vector of scalar values. In such cases, direct statistical measurement to give information about the quality of the sample cannot be deployed. To counter this, researchers took the route of the Bayesian framework to measure variance in estimation by an ensemble of predictors that can estimate uncertainty directly such as the Bayesian Neural network[9], or to estimate uncertainty indirectly by ensembling models[10]. Bayesian methods become expensive when the number of samples is in thousands, or the input dimension exceeds 10. Ensemble methods require independent training of several models, which is also timeconsuming. Another challenge in applying AL is that most AL framework works on a single sample per iteration. In contrast, retraining a neural network after each single data point is not computationally feasible. In such a scenario, although AL relatedresearch on query strategy is quite rich, its direct application in the DL framework for a regression problems is complex. To solve adaptive sampling and stepsizing for training a deep learning model for regression problem, in this work we propose a scalable feedbackbased learning architecture called `studentteacher' architecture. It is an iterative learning process similar to AL paradigm in which the teacher is an another neural network with the goal to nd the regions where the student network is failing and can guide sampling and data generation in the next iteration. In the context of feedbackbased learning, an analogy can be found in actorcritic based learning[11] in Reinforcement learning[12] literature. Empirical evaluation of the proposed approach shows better accuracy of the trained surrogate model compared to other baseline methods in dierent engineering domains. The major contribution of this work is summarized below: ‚Ä¢Development of a noble approach of deep active learning for regression problems by selecting strategic samples during surrogate training to learn the inputoutput behavior of a function/simulator with fewer evaluations. ‚Ä¢Development of a scalable and easytotrain batch mode DeepAL approach for surrogate training that is tractable as it needs to train only one main network called `student/learner' and one small network called `teacher/guide'. ‚Ä¢Empirical evaluation of the proposed method on dierent realworld engineering design domains and comparison with other baseline methods to establish the performance gains from this novel approach in terms of sample eciency or better accuracy. 2‚Ä¢ To our knowledge, all active learning{based adaptive sampling approach for regression problem uses the Bayesian approach, which is not scalable or dicult to train for high dimensional design space. Our method of active sampling is scalable since it uses only one main network (student) and one small (in comparison to the student network) teacher network. It selects a batch of the most informative samples and automatically handles explorationexploitation dilemmas during design exploration. This computationally scalable approach can make learning in a big design space possible, which was not possible with current approaches. Accordingly, surrogate training and trained model can make a big leap in the design automation process. We also observed that in high dimensional design space, on a given number of labeled samples the gap between the accuracy of our method and other baselines is wider in comparison to low dimensional design space. This is intuitive because spacelling sampling used in baseline methods needs combinatorial exponential samples while our approach is very strategic in labeling. The code and data used for our method is available publicly at https://github.com/vardhah/epsilon_weighted_Hybrid_Query_Strategy . The remainder of this paper is organized as follows: Section 2 talks about the related work in this eld. In Section 3, we formalize the problem and discuss our methodology and baselines in detail. In Section 4, we present our experiments and evaluation of results. We conclude in Section 5 with a brief discussion of future work. 2. Related Work "
460,Unified Robust Training for Graph NeuralNetworks against Label Noise.txt,"Graph neural networks (GNNs) have achieved state-of-the-art performance for
node classification on graphs. The vast majority of existing works assume that
genuine node labels are always provided for training. However, there has been
very little research effort on how to improve the robustness of GNNs in the
presence of label noise. Learning with label noise has been primarily studied
in the context of image classification, but these techniques cannot be directly
applied to graph-structured data, due to two major challenges -- label sparsity
and label dependency -- faced by learning on graphs. In this paper, we propose
a new framework, UnionNET, for learning with noisy labels on graphs under a
semi-supervised setting. Our approach provides a unified solution for robustly
training GNNs and performing label correction simultaneously. The key idea is
to perform label aggregation to estimate node-level class probability
distributions, which are used to guide sample reweighting and label correction.
Compared with existing works, UnionNET has two appealing advantages. First, it
requires no extra clean supervision, or explicit estimation of the noise
transition matrix. Second, a unified learning framework is proposed to robustly
train GNNs in an end-to-end manner. Experimental results show that our proposed
approach: (1) is effective in improving model robustness against different
types and levels of label noise; (2) yields significant improvements over
state-of-the-art baselines.","Nowadays, graphstructured data is being generated across many highimpact applications, ranging from nancial fraud detection in transaction networks to gene interaction analysis, from cyber security in computer networks to social network analysis. To ingest rich information on graph data, it is of paramount importance to learn eective node representations that encode both node at tributes and graph topology. To this end, graph neural networks (GNNs) havearXiv:2103.03414v1  [cs.LG]  5 Mar 20212 Yayong Li, Jie Yin, and Ling Chen been proposed, built upon the success of deep neural networks (DNNs) on grid structured data (e.g., images, etc.). GNNs have abilities to integrate both node attributes and graph topology by recursively aggregating node features across the graph. GNNs have achieved stateoftheart performance on many graph related tasks, such as node classication or link prediction. The core of GNNs is to learn neural network primitives that generate node representations by passing, transforming, and aggregating node features from local neighborhoods [3]. As such, nearby nodes would have similar node rep resentations [20]. By generalizing convolutional neural networks to graph data, graph convolutional networks (GCNs) [10] dene the convolution operation via a neighborhood aggregation function in the Fourier domain. The convolution of GCNs is a special form of Laplacian smoothing on graphs [11], which mixes the features of a node and its nearby neighbors. However, this smoothing operation can be disrupted when the training data is corrupted with label noise. As the training proceeds, GCNs would completely t noisy labels, resulting in degraded performance and poor generalization. Hence, one key challenge is how to improve the robustness of GNNs against label noise. Learning with noisy labels has been extensively studied on image classi cation. Label noise naturally stems from interobserver variability, human an notator's error, and errors in crowdsourced annotations [9]. Existing methods attempt to correct the loss function by directly estimating a noise transition matrix [15,19], or by adding extra layers to model the noise transition ma trix [17,4]. However, it is dicult to accurately estimate the noise transition matrix particularly with a large number of classes. Alternative methods such as MentorNet [8] and Coteaching [6] seek to separate clean samples from noisy samples, and use only the most likely clean samples to update model training. Other methods [2,16] reweight each sample in the gradient update of the loss function, according to model's predicted probabilities. However, they require a large number of labeled samples or an extra clean set for training. Otherwise, reweighting would be unreliable and result in poor performance. The aforementioned learning techniques, however, cannot be directly applied to tackle label noise on graphs. This is attributed to two signicant challenges. (1) Label sparsity : graphs with interconnected nodes are arguably harder to label than individual images. Very often, graphs are sparsely labeled, with only a small set of labeled nodes provided for training. Hence, we cannot simply drop \bad nodes"" with corrupted labels like previous methods using \smallloss trick"" [6,8]. (2)Label dependency : graph nodes exhibit strong label dependency, so nodes with high structural proximity (directly or indirectly connected) tend to have a similar label. This presses a strong need to fully exploit graph topology and sparse node labels when training a robust model against label noise. To tackle these challenges, we propose a novel approach for robustly learning GNN models against noisy labels under semisupervised settings. Our approach provides a uni ed ro bust train ing framework for graph neural net works (Union NET) that performs sample reweighting and label correction simulatenously. The core idea is twofold: (1) leverage random walks to perform label aggregationUnied Robust Training for Graph Neural Networks against Label Noise 3 among nodes with structural proximity. (2) estimate nodelevel class distribu tion to guide sample reweighting and label correction. Intuitively, noisy labels could cause disordered predictions around context nodes, thus its derived node class distribution could in turn re ect the reliability of given labels. This pro vides an eective way to assess the reliability of given labels, guided by which sample reweighting and label correction are expected to weaken unreliable su pervision and encourage label smoothing around context nodes. We verify the eectiveness of our proposed approach through experiments and ablation studies on realworld networks, demonstrating its superiority over competitive baselines. 2 Related Work "
96,Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks.txt,"The quantization of deep neural networks (QDNNs) has been actively studied
for deployment in edge devices. Recent studies employ the knowledge
distillation (KD) method to improve the performance of quantized networks. In
this study, we propose stochastic precision ensemble training for QDNNs (SPEQ).
SPEQ is a knowledge distillation training scheme; however, the teacher is
formed by sharing the model parameters of the student network. We obtain the
soft labels of the teacher by changing the bit precision of the activation
stochastically at each layer of the forward-pass computation. The student model
is trained with these soft labels to reduce the activation quantization noise.
The cosine similarity loss is employed, instead of the KL-divergence, for KD
training. As the teacher model changes continuously by random bit-precision
assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms
the existing quantization training methods in various tasks, such as image
classification, question-answering, and transfer learning without the need for
cumbersome teacher networks.","Deep neural networks (DNNs) have achieved remarkable accuracy for tasks in a wide range of applications, includ ing image processing (He et al. 2016a), machine transla tion (Gehring et al. 2017), and speech recognition (Zhang et al. 2017). These stateoftheart neural networks use very deep models, consuming hundreds of ExaOps of computa tion during training and GBytes of storage for model and data. This complexity poses a tremendous challenge for widespread deployment, especially in resourceconstrained edge environments, leading to a plethora of explorations in model compression that minimize memory footprint and computational complexity while attempting to preserve the performance of the model. Among them, research on quan tized DNNs (QDNNs) focuses on quantizing key data struc tures, namely weights and activations, into lowprecision. Hence, we can save memory access overhead and simplify the arithmetic unit to perform reducedprecision computa tion. There have been extensive studies on QDNNs (Fengfu, Bo, and Bin 2016; Courbariaux, Bengio, and David 2015; Copyright c 2021, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.Choi et al. 2018; Hou and Kwok 2018), but most of them suffer from accuracy loss due to quantization. To enhance the performance of lowcapacity models, knowledge distillation (Hinton, Vinyals, and Dean 2015; Bucilu, Caruana, and NiculescuMizil 2006) (KD) has been widely adopted. KD employs a more accurate model as a teacher network to guide the training of a student model. For the same input, the teacher network provides its pre diction as a soft label, which can be further considered in the loss function to guide the training of the student net work. In the case of QDNNs, the quantized student network can compensate for its accuracy loss via supervision of the teacher model (Mishra and Marr 2018; Polino, Pascanu, and Alistarh 2018; Shin, Boo, and Sung 2019; Kim et al. 2019). However, the need for large and highperformance teacher models introduces signiÔ¨Åcant overhead when applying KD. In particular, KD has not been successfully employed in the emerging study of ondevice training for model adaptation and transfer learning, since the memoryintensive teacher models may not be available once the quantized models are deployed. In this work, we propose a new practical approach to KD for QDNNs, called stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is motivated by an inspiring ob servation about activation quantization. Table 1 shows that the accuracy of the WFA2 (Ô¨Çoat weight and 2bit activation) model improves as the activation precision increases. How ever, the W2AF (2bit weight and Ô¨Çoat activation) model shows the opposite characteristic. The accuracy drops as the weight precision increases for inference. This simple exper iment reveals interesting insights: the activation quantiza tion mostly adds noise to the decision boundary (Boo, Shin, and Sung 2020). Therefore, inference with various activation precision results in selective removal of such noise, leading to diverse guidance that can be exploited for self knowledge distillation. In SPEQ, we form a teacher network that shares the quan tized weights with the student but employs different bit pre cision for activation. The clipping levels of activation are also shared. In fact, the activation precision for the teacher is randomly selected between the low and high precision, such as 2 and 8bit. Since the teacher stochastically applies the target lowbit activation quantization for its soft label com putation, it can experience the impact of quantization for thearXiv:2009.14502v1  [cs.LG]  30 Sep 2020Table 1: CIFAR100 test accuracy (%) in higher precision on the quantized model. ResNet20 is trained with 2bit weight / Ô¨Çoat activation and Ô¨Çoat weight / 2bit activation. (Details in Appendix A.) Trained precision Test accuracy (%) / Inference precision 2bit W, Ô¨Çoat A (W2AF) 65.74 / W2AF 58.01 / W4AF 55.85 / W8AF 54.70 / WFAF Float W, 2bit A (WFA2) 66.93 / WFA2 68.48 / WFA4 68.77 / WFA8 68.71 / WFAF guidance. Furthermore, we reveal that the cosine similarity loss is essential for distilling the knowledge of the teacher of stochastic quantization to the lowprecision student. Although this form of guidance resembles KD, there is a signiÔ¨Åcant difference in that the same model is shared and any other auxiliary models, such as large teacher net works, are unnecessary. The forwardpass computation of the teacher and student in SPEQ can be performed eco nomically as the same weight parameters can be loaded only once. Therefore, the SPEQ can improve the perfor mance much without the overhead of teachermodel search or hyperparameter tuning needed for conventional KD. Fur thermore, since the stochastic precision ensemble provides distinctive knowledge, SPEQ can be combined with the con ventional KD method to further improve the performance of the target QDNNs. We demonstrate the superior performance and efÔ¨Å ciency of our SPEQ on various applications, including CI FAR10/CIFAR100/ImageNet image classiÔ¨Åcation and also transfer learning scenarios such as BERTbased question answering and Ô¨Çower classiÔ¨Åcation. The contributions of our work are summarized as follows: We propose a new practical KD method called SPEQ that can enhance the accuracy of QDNNs QDNNs employing lowprecision bitwidths for weights and activation sig nals. This method can yield better results compared to conventional KDbased QDNN optimization that utilizes large teacher models. We suggest cosine similarity as an essential loss function to effectively distill the knowledge of activation quantiza tion in SPEQ training. We demonstrate that the proposed method outperforms the existing KD methods for training QDNNs with lower training overhead. We conÔ¨Årm this on various models and tasks including image classiÔ¨Åcations, question answering, and transfer learning. We show that the proposed method can be combined to the conventional KD method with a large teacher to fur ther improve the performance of the target model. 2 Related Works "
230,Reinforcement Learning to Rank with Coarse-grained Labels.txt,"Ranking lies at the core of many Information Retrieval (IR) tasks. While
existing research on Learning to Rank (LTR) using Deep Neural Network (DNN) has
achieved great success, it is somewhat limited because of its dependence on
fine-grained labels. In practice, fine-grained labels are often expensive to
acquire, i.e. explicit relevance judgements, or suffer from biases, i.e. click
logs. Compared to fine-grained labels, coarse-grained labels are easier and
cheaper to collect. Some recent works propose utilizing only coarse-grained
labels for LTR tasks. A most representative line of work introduces
Reinforcement Learning (RL) algorithms. RL can help train the LTR model with
little reliance on fine-grained labels compared to Supervised Learning. To
study the effectiveness of the RL-based LTR algorithm on coarse-grained labels,
in this paper, we implement four different RL paradigms and conduct extensive
experiments on two well-established LTR datasets. The results on simulated
coarse-grained labeled dataset show that while using coarse-grained labels to
train an RL model for LTR tasks still can not outperform traditional approaches
using fine-grained labels, it still achieve somewhat promising results and is
potentially helpful for future research in LTR. Our code implementations will
be released after this work is accepted.","Ranking lies at the core of many Information Retrieval (IR) tasks including web search [ 13,35] and recommender system [ 49,50, 55,56]. Learning to rank (LTR) typically applies machine learning techniques for ranking [ 12,24,31,34,37,43,53‚Äì55,57]. One of the popular approaches is to use Supervised Learning [ 7,9,12] with documentlevel relevance annotation data [ 31] to optimize ranking metrics [ 20,42] such as normalized Discounted Cumulative Gain (nDCG), or Expected Reciprocal Rank (ERR). While this method has been proven effective, one of the main drawbacks is that in order to construct the loss function based on these metrics, the model requires finegrained labels, i.e. the explicit relevance judgements of each querydocument pair. On the one hand, such labels can often be expensive to attain (e.g. the high cost of human annotations) or suffer from different biases, e.g. trust bias and qualityofcontext bias for click logs [ 22,23]; on the other hand, myopically optimizing ranking metrics handcrafted from those finegrained labels may not always serve the ultimate goal of the ranking systems (e.g. user satisfaction and engagements) directly. Compared to finegrained labels, coarsegrained labels, such as query reformation, second search result page examination, user scroll patterns, are abundant and can be easily collected from search logs to generate largescale training data [ 15,22,23,29]. Some of recent works study the usage of RL algorithms in LTR task [31,37,43,46,47,60]. While the Supervised Learning approach requires finegrained labels to compute the evaluation metrics and construct the loss function, certain RLalgorithms, such as policy gradient, can directly use rewards from the environment to update the model [ 40]. As a result, we can leverage this feature and train the RLbased model with coarsegrained labels as rewards. Existing research in Reinforcement Learning to Rank can be generally categorized into two learning methods: stepwise learning and SERPlevel1learning. In stepwise learning, the ranking of documents is a sequence of actions in which the model selects the appropriate document for the position in the ranklist [ 43,47]. Thus, a ranklist containing Ndocuments results from Ndiscrete ranking time steps. While this approach utilizes the discreteness of classical RL (i.e., only picking one action per time step), it also requires finegrained documentlevel reward. Here, we pay more focus on situations where such reward is unavailable. SERPlevel learning, on the other hand, returns a ranklist corresponding to a query in each time step [ 31,37]. To be more specific, in contrast 1short for Search Engine Result PagearXiv:2208.07563v1  [cs.IR]  16 Aug 2022Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Zhichao Xu, Anh Tran, Tao Yang, and Qingyao Ai to the stepwise approach, it constructs a loss function based on SERPlevel reward collected from the ranklist constructed by the model. The ranking model can be trained with only coarsegrained SERPlevel rewards with this approach. Because of this nature, we believe this SERPlevel learning approach is better suited for our purpose of training a model on data that lacks finegrained labels. While existing works of RL in LTR mainly focus on improving the performance of the model, we are more interested in seeing whether training RL model without finegrained data can also deliver good performance in the LTR task. In this work, we implement four different RL models, namely, Policy Gradient Rank (PGRank), TopK Offpolicy Correction for Reinforcement Learning (REINFORCE), Deep Deterministic Pol icy Gradient (DDPG), Batch Constrained Deep QLearning (BCQ). Because of the lack of realworld search logs, we first simulate coarsegrained labels from explicit finegrained labels, then use them to train RL algorithms and report the performance. We com pare these results to the classical DNN algorithms trained with finegrained labels. We use two variants of DNN with different loss functions, i.e. CrossEntropy loss [ 2,6] and LambdaRank loss [ 6,42]. We conduct the experiments on two wellestablished public LTR datasets, Yahoo! LETOR [ 10] and MSLR10K [ 32]. Based on the result, we find that the RLbased algorithms are still less effective than the classical approach for ranking tasks. Nevertheless, with more research, it could act as a good alternative for scenarios where the finegrained labels are not available. The rest parts of this paper are organized as follows: we first discuss the related works in ¬ß2; then cover our methods (¬ß3) includ ing the problem formulation (¬ß3.1) and details of RL algorithms we adopt (¬ß3.2). We cover the experimental details in ¬ß4 and analyze the results in ¬ß5. Finally, we conclude this work in ¬ß6. 2 Related Work "
53,Using Deep Networks and Transfer Learning to Address Disinformation.txt,"We apply an ensemble pipeline composed of a character-level convolutional
neural network (CNN) and a long short-term memory (LSTM) as a general tool for
addressing a range of disinformation problems. We also demonstrate the ability
to use this architecture to transfer knowledge from labeled data in one domain
to related (supervised and unsupervised) tasks. Character-level neural networks
and transfer learning are particularly valuable tools in the disinformation
space because of the messy nature of social media, lack of labeled data, and
the multi-channel tactics of influence campaigns. We demonstrate their
effectiveness in several tasks relevant for detecting disinformation: spam
emails, review bombing, political sentiment, and conversation clustering.","Electronic communication is more embedded and essential to human life than ever before. This communication in creasingly relies on the distributed, selfpublishing model of social media platforms. The increasing ease of dis tributed communication does not come without drawbacks: disinformation‚Äîdeceptive information spread deliberately to change behavior, inÔ¨Çuence public opinion, or obscure the truth‚Äîhas inÔ¨Åltrated the online information ecosystem, with damaging consequences (Carvalho et al., 2011; Mo canu et al., 2015). Malicious electronic communication takes many forms. It spans from lowlevel social engineering attacks (i.e., phish ing) to more sophisticated, distributed efforts to disseminate state propaganda (Inkster, 2016; Okoro & Nwafor, 2013; Woolley, 2016). We propose that the language characteris 1New Knowledge, Austin, Texas, USA2Algorine, Inc., Austin, Texas, USA3Watson School of Engineering and Applied Science and the Department of Psychology: Cognitive and Brain Sciences, Binghamton University (SUNY), Binghamton, New York, USA. Correspondence to: Numa Dhamani <numa@newknowledge.io >. Appearing at the International Conference on Machine Learning AI for Social Good Workshop , Long Beach, United States, 2019.tics of known and identiÔ¨Åed sources of malicious electronic communication can be used as a signal for the detection and mitigation of these efforts across the diverse (and fractured) electronic communication ecosystem. The motivation of this work is to demonstrate how semantic classiÔ¨Åcation of natural language can be used as a tool for the detection of inÔ¨Çammatory, inauthentic, or otherwise nefarious communication. Characterlevel convolutional neural networks (CNNs) are particularly wellsuited for this task‚Äîas opposed to a wordlevel model‚Äîbecause they allow for nonvernacular discourse, misspelling, and other social media features (e.g., emoticons) to be learned without the constraint of Ô¨Åxed vocabularies (Zhang et al., 2015). We implement an adaptation of a neural network architecture recently demonstrated to be effective for text classiÔ¨Åcation (Zhang et al., 2015; J ¬¥ozefowicz et al., 2016). The method is purely contentbased and does not require any additional metadata beyond the text. To show the effectiveness of this method in relation to malicious communication and disinformation, we present a series of experimental results on semantic classiÔ¨Åcation for spam emails, review bombing, political sentiment, and conversation clustering. 2. Related Work "
101,Semi-supervised Stance Detection of Tweets Via Distant Network Supervision.txt,"Detecting and labeling stance in social media text is strongly motivated by
hate speech detection, poll prediction, engagement forecasting, and concerted
propaganda detection. Today's best neural stance detectors need large volumes
of training data, which is difficult to curate given the fast-changing
landscape of social media text and issues on which users opine. Homophily
properties over the social network provide strong signal of coarse-grained
user-level stance. But semi-supervised approaches for tweet-level stance
detection fail to properly leverage homophily. In light of this, We present
SANDS, a new semi-supervised stance detector. SANDS starts from very few
labeled tweets. It builds multiple deep feature views of tweets. It also uses a
distant supervision signal from the social network to provide a surrogate loss
signal to the component learners. We prepare two new tweet datasets comprising
over 236,000 politically tinted tweets from two demographics (US and India)
posted by over 87,000 users, their follower-followee graph, and over 8,000
tweets annotated by linguists. SANDS achieves a macro-F1 score of 0.55 (0.49)
on US (India)-based datasets, outperforming 17 baselines (including variants of
SANDS) substantially, particularly for minority stance labels and noisy text.
Numerous ablation experiments on SANDS disentangle the dynamics of textual and
network-propagated stance signals.","Social media is regarded as a barometer of modern society‚Äôs emo tional state. Billions of social media users express their stances toward events, social issues, and political parties in their tweets, Facebook articles, or blogs. The ‚Äòtarget entity‚Äô may not be explic itly mentioned in the text expressing the stance. Automatic stance detection is a strongly motivated mining operation on social media and networks [ 24]. Some applications include hate speech detection [14], poll prediction [ 13], and rumor veracity detection [ 10]. The im portance of political stance analysis over Twitterlike platforms has increased dramatically in recent times owing to several phenomena ‚Äî sharp increase in partisanship among users, malicious efforts of organized groups to distort popular opinion at a largescale, etc. Leveraging homophily in stance detection. A large number of approaches have been proposed for textbased stance detection [1,24,32]. Relatively few approaches recognize and exploit the fact that text in social networks is accompanied by rich graph structured metadata, e.g., friends, followers/followees, retweetsand replies, hashtags, textauthor associations, etc. [ 7,23]. It is wellknown that homophily (friends have similar taste) and social balance (enemy of an enemy is a friend, etc.) are pervasive in social networks. Therefore, these signals have the potential to improve the accuracy of stance prediction. Consider the tweets shown in Figure 1. The stance labels in this example include proRepublican, antiRepublican, proDemocrat, antiDemocrat, and neutral. Tweets from users 2‚Äì5 clearly express an antiDemocrat stance, while user 6‚Äôs tweet is proRepublican. It may be nontrivial to correctly label the tweet of user 1 (because ‚Äòshe‚Äô is not identified, and the only han dle @realDonaldTrump is uninformative) unless we see whom user 1 follows and what tweets they write. However, previous works connecting network dynamics for stance classification mostly deal with userlevel stance analysis. An overall stance of the user often does not reflect in the tweet. For example, a Democrat supporter can tweet something prodemocrat or antirepublican. Volatile users may often switch their stances depending on the issue. For exam ple, among the tweets we collected and annotated for the analysis presented in this work, about 8%show stance switch. On the other hand, supplementing local, tweetlevel features with homophily driven features might add bias towards the majority stance of the neighbor nodes. Instead, we seek to use homophily as a navigator of distant supervision. The end results are simple, tweetlevel stance classifiers that rely on networklevel information only at training time. Scarcity of labeled data for stance classification. A key hur dle in our setting is the paucity of humanlabeled data. Neural text processors are among the best for stance detection; however, they need large volumes of training data. The rapid pace of information generation and consumption over social media leads to the emer gence of completely new entities and concepts (persons, events, issues, etc.), too fast for the curation of humanlabeled highquality data for each scenario. Semisupervised and active learning are common coping mechanisms. Starting from a small set of instances manually labeled with groundtruth (‚Äògold instances‚Äô), they expose the learner to progressively larger sets of instances that are automat ically and manually labeled, respectively. Label sampling decisions are not usually informed by an overlying network. Proposed method: SANDS Our central research question is the manner in which network signals like homophily can improve semi supervised stance detection. Our investigation results in a system, called SANDS (Stance Analysis via Network Distant Supervision) that starts from a few highquality seed instances, obtains noisy labeling guidance from homophily (between a user and her follow ers), uses multiple views of tweet content with customized feature extraction models, and iteratively train the component learners. In more detail, SANDS uses three kinds of textual feature extrac tors, designed to fit their semantics in the context of Twitter ‚Äî 1arXiv:2201.00614v2  [cs.SI]  5 Jan 2022WSDM ‚Äô22, February 21‚Äì25, 2022, Tempe, AZ, USA1Subhabrata Dutta,2Samiya Caur,3Soumen Chakrabarti,2Tanmoy Chakraborty 123 4 5 6 Figure 1: Stance homophily in Twitter. User 1follows users 2,3,4,5, and 6. All these users carry similar opinion, with user 6‚Äôs tweet showing support to the Republicans while the rest are antiDemocrat. While the tweet posted by user 1does not link any entity related to Republican or Democrats directly to some polarity words (thereby making the stance classification difficult), a classification framework with the knowledge of the rest of the tweets can break the ambiguity. (i) A simple symmetric set aggregated encoding is used for hash tags. (ii) Shortrange contextual text representation is captured through a convolutional network. (iii) Longer range textual con text is captured through a biLSTM. These are combined into two main learners/predictors with somewhat different capabilities and strengths. Rather than throwing these into a standard cotraining or disagreementbased learning scheme, we also make use of network information along with a homophily assumption that followees of a user tend to have the same stance as the user . Therefore, the pre dictors are also applied to recent tweets by followees of the user who posted the given tweet, and their majority vote turned into a suitable loss for the other learner. This novel combination of networkdriven distant supervision and synthetic feature view separation makes SANDS ‚Äôs predictive accuracy considerably superior to many recent and competitive baselines as well as ablations. SANDS is particularly effective at improving the accuracy of minority class labels. Two new datasets and the superiority of SANDS .As part of this work, we offer two large collections of politically tinted tweets from the two most vociferous social media with political content: 59,684and176,619tweets from US and Indiabased users, respec tively, along with the corresponding interuser follow relations; 3,822and4,185tweets respectively among these collections are manually annotated with corresponding stance labels. While using only 1,500samples as labeled data, SANDS achieves 0.55and0.47 macroF1 scores on US and Indiabased datasets, surpassing the best baseline by (absolute) 4% and 5%, respectively. Summary of our major contributions: ‚Ä¢We propose a novel stance classification framework for tweets, SANDS , which employs distant supervision using follow network information to learn tweet stances with frugal labeled data.‚Ä¢Using separate neural feature extractors to focus on local and global contextual features from tweet texts separately, we formu late a learning strategy facilitated by distant network supervision that iteratively trains multiple models with twophased view sep arations: local vs. global textual features within the tweet and candidate tweet vs. followee tweets over the network. ‚Ä¢We present two large collections of tweets from US and India based users, along with manually annotated labels for political stance on subsets of each. ‚Ä¢We perform extensive experiments with multiple supervised and semisupervised methods along with ablation variants to analyze signal importance. Reproducibility: We detail the preprocessing and parameters ofSANDS necessary to reproduce the results in the supplementary material. Also, the source codes are available on this anonymous repository: https://github.com/LCS2IIITD/SANDS. 2 RELATED WORK "
341,Label Aware Speech Representation Learning For Language Identification.txt,"Speech representation learning approaches for non-semantic tasks such as
language recognition have either explored supervised embedding extraction
methods using a classifier model or self-supervised representation learning
approaches using raw data. In this paper, we propose a novel framework of
combining self-supervised representation learning with the language label
information for the pre-training task. This framework, termed as Label Aware
Speech Representation (LASR) learning, uses a triplet based objective function
to incorporate language labels along with the self-supervised loss function.
The speech representations are further fine-tuned for the downstream task. The
language recognition experiments are performed on two public datasets - FLEURS
and Dhwani. In these experiments, we illustrate that the proposed LASR
framework improves over the state-of-the-art systems on language
identification. We also report an analysis of the robustness of LASR approach
to noisy/missing labels as well as its application to multi-lingual speech
recognition tasks.","The conventional approach for deriving speech representations for nonsemantic speech tasks, such as speaker and language recognition, involved the use of training deep neural models with a statistics pooling layer. Some of the popular methods in this direction include dvectors [1] and xvectors [2], where a deep neural model is trained to classify the speaker/language labels on a large corpus of supervised data. However, recent trends in speech processing has seen a paradigm shift towards selfsupervision based representation learning, mirroring the ef forts in computer vision [3] and natural language processing [4]. Some popular examples of such approaches include contrastive predictive coding (CPC) [5], wav2vec family of models [6, 7], and hidden unit BERT (HuBERT) [8]. These methods primar ily rely on learning speech representations at the framelevel with its impact reported on semantic tasks such as lowresource speech recognition [8, 9] or zero resource spoken language modeling [10]. These representations have also been investi gated for speaker and language recognition tasks [11] through various benchmarks such as SUPERB [12] and NOSS [13]. In many learning paradigms, it is plausible to have portions of pretraining data along with the corresponding metadata. In the broad spectrum of representation learning, where supervised and selfsupervised frameworks constitute the twoends of the spectrum, we hypothesize that a combination of supervision and selfsupervision based methods may be more optimal than ei ther of the two frameworks in isolation, for scenarios whereparts of the pretraining have additional metadata in the form of labels. In this paper, we propose a framework for Label Aware Speech Representation learning (LASR) for such scenarios. To the best of our knowledge, this is the first attempt to combine label information with a selfsupervision loss for nonsemantic speech tasks. The contributions from this work are as follows. 1. We propose LASR, a framework for incorporating label in formation in selfsupervised speech representation learning. 2. We demonstrate the effectiveness of LASR for language identification task and establish its efficacy even with miss ing and noisy labels. 3. Our findings demonstrate that inclusion of language infor mation in the pretraining phase results in stateofartresults on the FLEURS dataset [14]. 2. Related Work "
126,Noisy Machines: Understanding Noisy Neural Networks and Enhancing Robustness to Analog Hardware Errors Using Distillation.txt,"The success of deep learning has brought forth a wave of interest in computer
hardware design to better meet the high demands of neural network inference. In
particular, analog computing hardware has been heavily motivated specifically
for accelerating neural networks, based on either electronic, optical or
photonic devices, which may well achieve lower power consumption than
conventional digital electronics. However, these proposed analog accelerators
suffer from the intrinsic noise generated by their physical components, which
makes it challenging to achieve high accuracy on deep neural networks. Hence,
for successful deployment on analog accelerators, it is essential to be able to
train deep neural networks to be robust to random continuous noise in the
network weights, which is a somewhat new challenge in machine learning. In this
paper, we advance the understanding of noisy neural networks. We outline how a
noisy neural network has reduced learning capacity as a result of loss of
mutual information between its input and output. To combat this, we propose
using knowledge distillation combined with noise injection during training to
achieve more noise robust networks, which is demonstrated experimentally across
different networks and datasets, including ImageNet. Our method achieves models
with as much as two times greater noise tolerance compared with the previous
best attempts, which is a significant step towards making analog hardware
practical for deep learning.","Deep neural networks (DNNs) have achieved unprecedented performance over a wide variety of tasks such as computer vision, speech recognition, and natural language processing. However, DNN inference is typically very demanding in terms of compute and memory resources Li et al. (2019). Consequently, larger models are often not well suited for largescale deployment on edge devices, which typically have meagre performance and power budgets, especially battery powered mobile and IoT devices. To address these issues, the design of specialized hardware for DNN inference has drawn great interest, and is an extremely active area of research (Whatmough et al., 2019). To date, a plethora of techniques have been proposed for designing efÔ¨Åcient neural network hardware (Sze et al., 2017; Whatmough et al., 2019). In contrast to the current status quo of predominantly digital hardware, there is signiÔ¨Åcant research interest in analog hardware for DNN inference. In this approach, digital values are represented by analog quantities such as electrical voltages or light pulses, and the computation itself (e.g., Work performed during Prad Kadambi‚Äôs internship with Arm Research 1arXiv:2001.04974v1  [cs.LG]  14 Jan 2020Preprint multiplication and addition) proceeds in the analog domain, before eventually being converted back to digital. Analog accelerators take advantage of particular efÔ¨Åciencies of analog computation in exchange for losing the bitexact precision of digital. In other words, analog compute is cheap but somewhat imprecise. Analog computation has been demonstrated in the context of DNN inference in both electronic (Binas et al., 2016), photonic (Shen et al., 2017) and optical (Lin et al., 2018) systems. Analog accelerators promise to deliver at least two orders of magnitude better performance over a conventional digital processor for deep learning workloads in both speed (Shen et al., 2017) and energy efÔ¨Åciency (Ni et al., 2017). Electronic analog DNN accelerators are arguably the most mature technology and hence will be our focus in this work. The most common approach to electronic analog DNN accelerator is inmemory computing , which typically uses nonvolatile memory (NVM) crossbar arrays to encode the network weights as analog values. The NVM itself can be implemented with memristive devices, such as metaloxide resis tive randomaccess memory (ReRAM) (Hu et al., 2018) or phasechange memory (PCM) (Le Gallo et al., 2018; Boybat et al., 2018; Ambrogio et al., 2018). The matrixvector operations computed during inference are then performed in parallel inside the crossbar array, operating on analog quan tities for weights and activations. For example, addition of two quantities encoded as electrical currents can be achieved by simply connecting the two wires together, whereby the currents will add linearly according to Kirchhoff‚Äôs current law. In this case, there is almost zero latency or energy dissipation for this operation. Similarly, multiplication with a weight can be achieved by programming the NVM cell conduc tance to the weight value, which is then used to convert an input activation encoded as a voltage into a scaled current, following Ohm‚Äôs law. Therefore, the analog approach promises signiÔ¨Åcantly improved throughput and energy efÔ¨Åciency. However, the analog nature of the weights makes the compute noisy, which can limit inference accuracy. For example, a simple twolayer fullyconnected network with a baseline accuracy of 91:7%on digital hardware, achieves only 76:7%when imple mented on an analog photonic array (Shen et al., 2017). This kind of accuracy degradation is not acceptable for most deep learning applications. Therefore, the challenge of imprecise analog hard ware motivates us to study and understand noisy neural networks , in order to maintain inference accuracy under noisy analog computation. The question of how to effectively learn and compute with a noisy machine is a longstanding prob lem of interest in machine learning and computer science (Stevenson et al., 1990; V on Neumann, 1956). In this paper, we study noisy neural networks to understand their inference performance. We also demonstrate how to train a neural network with distillation and noise injection to make it more resilient to computation noise, enabling higher inference accuracy for models deployed on analog hardware. We present empirical results that demonstrate stateoftheart noise tolerance on multiple datasets, including ImageNet. The remainder of the paper is organized as follows. Section 2 gives an overview of related work. Section 3 outlines the problem statement. Section 4 presents a more formal analysis of noisy neural networks. Section 5 gives a distillation methodology for training noisy neural networks, with exper imental results. Finally, Section 6 provides a brief discussion and Section 7 closes with concluding remarks. 2 R ELATED WORK "
454,Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes.txt,"How should one combine noisy information from diverse sources to make an
inference about an objective ground truth? This frequently recurring, normative
question lies at the core of statistics, machine learning, policy-making, and
everyday life. It has been called ""combining forecasts"", ""meta-analysis"",
""ensembling"", and the ""MLE approach to voting"", among other names. Past studies
typically assume that noisy votes are identically and independently distributed
(i.i.d.), but this assumption is often unrealistic. Instead, we assume that
votes are independent but not necessarily identically distributed and that our
ensembling algorithm has access to certain auxiliary information related to the
underlying model governing the noise in each vote. In our present work, we: (1)
define our problem and argue that it reflects common and socially relevant real
world scenarios, (2) propose a multi-arm bandit noise model and count-based
auxiliary information set, (3) derive maximum likelihood aggregation rules for
ranked and cardinal votes under our noise model, (4) propose, alternatively, to
learn an aggregation rule using an order-invariant neural network, and (5)
empirically compare our rules to common voting rules and naive
experience-weighted modifications. We find that our rules successfully use
auxiliary information to outperform the naive baselines.","Many collective decision making processes aggregate noisy good faith opinions in order to make an inference about some underlying ground truth. In cooperative policy making, for example, each party advocates for the policy they believe is objectively best. Similarly, in academic peer review, a metareviewer combines good faith reviewer opinions about a submitted paper. Other examples are easy to come by. We refer to this setting as objective social choice, to contrast it with the typical subjective social choice setting [ 35], where the optimal choice is defined in terms of the voter utilities *Code available at https://github.com/spitis/objective_social_choice Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020), B. An, N. YorkeSmith, A. El Fallah Seghrouchni, G. Sukthankar (eds.), May 9‚Äì13, 2020, Auckland, New Zealand .¬©2020 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.rather than a ground truth. Whereas subjective social choice can be viewed as collective compromise , objective social choice can be viewed as collective estimation . Unlike the subjective setting, where it is natural to consider each source or voter equally‚Äîan axiom known as ‚Äúanonymity‚Äù [ 32]‚Äî objective analysis suggests otherwise: diverse and more informed opinions should be valued more. Many sensible, realworld set tings involve asymmetric (nonanonymous) voting, making this a relevant line of analysis. Academic review is one. Another is corporate governance, where different stakeholder classes have varying voting powers, depending on the issue. In such cases, vary ing voter weights are natural, and one can evaluate the quality of social choices via other avenues (e.g., direct evaluation [ 27] or ex post analysis [ 19]). In other settings, such as national elections, the objective approach raises ethical concerns of fairness, and the objective approach may be inappropriate. Although objective social choice has been the subject of numer ous studies in social choice [ 6,10,12,44], forecasting [ 3,9,13], statistics [ 16,18] and machine learning [ 15,37] (Section 2), to our knowledge, no prior work has dealt with the case of noni.i.d. ordi nal feedback (i.e., ranked preferences). Yet this is the case in many practical applications. During peer review, for instance, two of three reviewers might share primary areas of expertise, but being human, cannot share comparable cardinal estimates. Or consider a robot that must aggregate feedback from human principals. Once again, the different principals will draw upon diverse background to form their opinions, which can only be shared as ordinal preferences. In each case, how should the noni.i.d. feedback be aggregated? Our work is intended as a first step toward answering this ques tion. To narrow the scope of our inquiry, we make several modeling assumptions (Section 3), which we hope can be relaxed in future work. In particular, we assume that (1) the underlying ground truth and noise generating process is modeled as a karmed bandit prob lem, where the different arms represent different alternatives, (2) different voters see different pulls of the arms, and (3) the social decision rule sees how many pulls each voter saw (but not their outcomes). We solve for the maximum likelihood social choice in a series of cases (Section 4). As our derived rules rest on strong assumptions about the noise generation process, we also propose to learn a more flexible aggregation rule using an orderinvariant neural network (Section 4.2). We empirically compare our derived and learned rules to classical voting rules (Section 5). Our results confirm the intuition that objective estimation can be improved by upweighing opinions from diverse and more informed sources.arXiv:2001.10092v1  [cs.MA]  27 Jan 2020AAMAS‚Äô20, May 9‚Äì13, 2020, Auckland, New Zealand Silviu Pitis and Michael R. Zhang 2 RELATED WORK "
393,Learn to Propagate Reliably on Noisy Affinity Graphs.txt,"Recent works have shown that exploiting unlabeled data through label
propagation can substantially reduce the labeling cost, which has been a
critical issue in developing visual recognition models. Yet, how to propagate
labels reliably, especially on a dataset with unknown outliers, remains an open
question. Conventional methods such as linear diffusion lack the capability of
handling complex graph structures and may perform poorly when the seeds are
sparse. Latest methods based on graph neural networks would face difficulties
on performance drop as they scale out to noisy graphs. To overcome these
difficulties, we propose a new framework that allows labels to be propagated
reliably on large-scale real-world data. This framework incorporates (1) a
local graph neural network to predict accurately on varying local structures
while maintaining high scalability, and (2) a confidence-based path scheduler
that identifies outliers and moves forward the propagation frontier in a
prudent way. Experiments on both ImageNet and Ms-Celeb-1M show that our
confidence guided framework can significantly improve the overall accuracies of
the propagated labels, especially when the graph is very noisy.","The remarkable advances in visual recognition [6,34,33,11,12,46,7,41,13,17,15,42,29,41,16,28,40] are built on top of largescale annotated training data. However, the ever increas ing demand on annotated data has resulted in prohibitive annotation cost. Trans ductive learning, which aims to propagate labeled information to unlabeled sam ples, is a promising way to tackle this issue. Recent studies [50,26,21,38,25,18] show that transductive methods with an appropriate design can infer unknown labels accurately while dramatically reducing the annotation eorts. Many transductive methods adopt graphbased propagation [49,26,21,38] as a core component. Generally, these methods construct a graph among all samples, propagating labels or other relevant information from labeled sam ples to unlabeled ones. Early methods [49,47,1] often resort to a linear diusion paradigm, where the class probabilities for each unlabeled sample are predictedarXiv:2007.08802v1  [cs.CV]  17 Jul 20202 L. Yang and Q. Huang and H. Huang and L. Xu and D. Lin Geodesic distance to labeled verticesAccuracy0.0 1.00.80.60.40.2Most confident set ‚Ä¶ Most unconfident set ‚Ä¶ConfNet Calibrated confidenceInitial confidence Confidence Fig. 1: In this paper, we propose a framework for transductive learning on noisy graphs, which contain a large number of outliers, e.g.outofclass samples. The framework con sists of a local predictor and a condencebased path scheduler. The predictor updates local patches sequentially following a path driven by the estimated condences. The path scheduler leverages both the condent and uncondent samples from the predic tor to further calibrate the estimated condence. The uncondent samples are usually images with low quality( e.g.the leftmost image is a clock with only top part), hard examples( e.g.the middle image is a spoon mixed with the background) or outofclass samples( e.g.the rightmost image is a lamp but none of the labeled samples belong to this class). The lower left gure experimentally shows that the proposed method improves the reliability of propagation. When the distance from unlabeled samples to labeled ones increases, our method surpasses stateoftheart by a signicant margin as a linear combination of those for its neighbors. Relying on simplistic assump tions restricts their capability of dealing with complicated graph structures in realworld datasets. Recently, graph convolutional networks [21,38,39] have re vealed its strong expressive power to process complex graph structures. Despite obtaining encouraging results, these GCNbased methods remain limited in an important aspect, namely the capability of coping with outliers in the graph. In realworld applications, unlabeled samples do not necessarily share the same classes with the labeled ones, leading to a large portion of outofclass samples, which becomes the main source of outliers. Existing methods ignore the fact that the condences of predictions on dierent samples can vary signicantly, which may adversely in uence the reliability of the predictions. In this paper, we aim to explore a new framework that can propagate labels over noisy unlabeled data reliably . This framework is designed based on three principles: 1) Local update: each updating step can be carried out within a local part of the graph, such that the algorithm can be easily scaled out to a largescale graph with millions of vertices. 2) Learnable: the graph structures over a real world dataset are complex, and thus it is dicult to prescribe a rule that works well for all cases, especially for various unknown outliers. Hence, it is desirable toLearn to Propagate Reliably on Noisy Anity Graphs 3 have a core operator with strong expressive power that can be learned from real data. 3) Reliable path: graphbased propagation is sensitive to noises { a noisy prediction can mislead other predictions downstream. To propagate reliably, it is crucial to choose a path such that most inferences are based on reliable sources. Specically, we propose a framework comprised of two learnable components, namely, a local predictor and a path scheduler. The local predictor is a light weight graph neural network operating on local subgraphs, which we refer to asgraph patches , to predict the labels of unknown vertices. The path scheduler is driven by condence estimates, ensuring that labels are gradually propagated from highly condent parts to the rest. The key challenge in designing the path scheduler is how to estimate the condences eectively. We tackle this problem via a twostage design. First, we adopt a multiview strategy by exploiting the fact that a vertex is usually covered by multiple graph patches , where each patch may project a dierent prediction on it. Then the condence can be evaluated on how consistent and certain the predictions are. Second, with the estimated condence, we construct a candidate set by selecting the most condent samples and the most uncondent ones. As illustrated in Fig. 1, we devise a ConfNet to learn from the candidate set and calibrate the condence estimated from the rst stage. Highly condent samples are assumed to be labeled and used in later propagation, while highly uncondent samples are assumed to be outliers and excluded in later propagation. Both components work closely together to drive the propagation process. On one hand, the local predictor follows the scheduled path to update predictions; on the other hand, the path scheduler estimates condences based on local predictions. Note that the training algorithm also follows the same coupled procedure, where the parameters of the local predictor and condence estimator are learned endtoend. Our main contributions lie in three aspects: (1) A learnable framework that involves a local predictor and a path scheduler to drive propagation reliably on noisy largescale graphs. (2) A novel scheme of exploiting both condent and uncondent samples for condence estimation. (3) Experiments on ImageNet [6] and MsCeleb1M [9] show that our proposed approach outperforms previous algorithms, especially when the graphs are noisy and the initial seeds are sparse. 2 Related Work "
130,How Universal is Genre in Universal Dependencies?.txt,"This work provides the first in-depth analysis of genre in Universal
Dependencies (UD). In contrast to prior work on genre identification which uses
small sets of well-defined labels in mono-/bilingual setups, UD contains 18
genres with varying degrees of specificity spread across 114 languages. As most
treebanks are labeled with multiple genres while lacking annotations about
which instances belong to which genre, we propose four methods for predicting
instance-level genre using weak supervision from treebank metadata. The
proposed methods recover instance-level genre better than competitive baselines
as measured on a subset of UD with labeled instances and adhere better to the
global expected distribution. Our analysis sheds light on prior work using UD
genre metadata for treebank selection, finding that metadata alone are a noisy
signal and must be disentangled within treebanks before it can be universally
applied.","Identifying document genre automatically has long been of interest to the NLP community due to its immediate applications both in document grouping (Petrenz, 2012) as well as taskspeciÔ¨Åc data selec tion (Ruder and Plank, 2017; Sato et al., 2017). Crosslingual genre identiÔ¨Åcation has however remained a challenge, mainly due to the lack of stable crosslingual representations (Petrenz, 2012). Recent work has shown that pretrained masked language models (MLMs) capture monolingual genre (Aharoni and Goldberg, 2020). Do such distinctions man ifest in highly multilingual spaces as well? In this work, we investigate whether this property holds for the genre distribution in the 114 language Universal Dependencies corpus (UD version 2.8; Zeman et al., 2021) using the multilingual mBERT MLM (Devlin et al., 2019). In absence of an exact deÔ¨Ånition of textual genre (Kessler et al., 1997; Webber, 2009; Plank, 2016), this work will focus on the information speciÔ¨Åcally denoted by the genres metadata tag in UD. We hope that an indepth, crosslingual analysis of what this label represents will enable practitioners to better control for the effects of domain shift in their experiments. Previous work using these UD metadata for proxy training data selection have produced mixed results (Stymne, 2020). We investigate possible reasons and identify inconsistencies in genre annotation. The fact that genre labels are only available at the level of treebanks makes it difÔ¨Åcult to gather a clear picture of the sentencelevel genre distribution ‚Äî especially with some treebanks having up to 10 genre labels. We therefore investigate the degree to which instancelevel genre is recoverable using only the treebanklevel metadata as weak supervision. Our contributions entail the, to our knowledge, Ô¨Årst detailed deÔ¨Ånition of all UD metadata genre labels (Section 3), four weakly supervised methods for extracting instancelevel genre across 114 languages (Section 4) as well as genre identiÔ¨Åcation experiments which show that our proposed twostep procedure allows for effective genre recovery in multilingual setups where language relatedness typically outweighs genre similarities (Section 5).1 This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ . 1Code available at https://personads.me/x/syntaxfest2021code .arXiv:2112.04971v1  [cs.CL]  9 Dec 20212 Related Work "
234,Iterative Learning with Open-set Noisy Labels.txt,"Large-scale datasets possessing clean label annotations are crucial for
training Convolutional Neural Networks (CNNs). However, labeling large-scale
data can be very costly and error-prone, and even high-quality datasets are
likely to contain noisy (incorrect) labels. Existing works usually employ a
closed-set assumption, whereby the samples associated with noisy labels possess
a true class contained within the set of known classes in the training data.
However, such an assumption is too restrictive for many applications, since
samples associated with noisy labels might in fact possess a true class that is
not present in the training data. We refer to this more complex scenario as the
\textbf{open-set noisy label} problem and show that it is nontrivial in order
to make accurate predictions. To address this problem, we propose a novel
iterative learning framework for training CNNs on datasets with open-set noisy
labels. Our approach detects noisy labels and learns deep discriminative
features in an iterative fashion. To benefit from the noisy label detection, we
design a Siamese network to encourage clean labels and noisy labels to be
dissimilar. A reweighting module is also applied to simultaneously emphasize
the learning from clean labels and reduce the effect caused by noisy labels.
Experiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets
demonstrate that our proposed model can robustly train CNNs in the presence of
a high proportion of open-set as well as closed-set noisy labels.","The success of Convolutional Neural Networks (CNNs) [20] is highly tied to the availability of largescale anno tated datasets, e.g., ImageNet [10]. However, largescale datasets with highquality label annotations are not always available for a new domain, due to the signiÔ¨Åcant time and effort it takes for human experts. There exist several cheap but imperfect surrogates for collecting labeled data, such as crowdsourcing from nonexperts or annotations from the web, especially for images ( e.g., extracting tags from the surrounding text or query keywords from search engines). These approaches provide the possibility to scale the acqui sition of training labels, but invariably result in the intro Figure 1. An illustration of closedset vs openset noisy labels. Figure 2. An overview of our framework that iteratively learns dis criminative representations on a ‚Äújasminecat‚Äù dataset with open set noisy labels. It not only learns a proper decision boundary (the black line separating jasmine and cat) but also pulls away noisy samples (green and purple) from clean samples (blue and red). duction of some noisy (incorrect) labels. Moreover, even highquality datasets are likely to have noisy labels, as data labeling can be subjective and errorprone. The presence of noisy labels for training samples may adversely affect rep resentation learning and deteriorate prediction performance [27]. Training accurate CNNs against noisy labels is there fore of great practical importance. We will refer to samples whose classes are misla beled/incorrectly annotated as noisy samples and denote their labels as noisy labels . Such noisy labels can fall into two types, closedset andopenset . More speciÔ¨Åcally, a closedset noisy label occurs when a noisy sample possesses a true class that is contained within the set of known classes in the training data. While, an openset noisy label occurs when a noisy sample possesses a true class that is not con tained within the set of known classes in the training data. The former scenario has been studied in previous work, but the latter one is a new direction we explore in this paper. 1arXiv:1804.00092v1  [cs.CV]  31 Mar 2018Table 1. Types of labels for a ‚Äújasminecat‚Äù dataset. labeled as ‚Äújasmine‚Äù labeled as ‚Äúcat‚Äù true ‚Äújasmine‚Äù clean closedset true ‚Äúcat‚Äù closedset clean other class images openset openset Figure 1 provides a pictorial illustration of noisy labels, where we have an image dataset with two classes, jasmine (the plant) and cat (the animal). The closedset noisy labels occur when cat and jasmine are mislabeled from one cate gory to the other, but the true labels of these images are still cat or jasmine. The openset noisy labels occur for those images labeled as cat or jasmine, but their true labels are neither cat nor jasmine, e.g., the zoo map and the cartoon character. Table 1 demonstrates all the possible cases on how different samples are labeled in this problem. The left most column speciÔ¨Åes the true class and the other columns specify the type of label in the dataset. Previous work has addressed the noisy label problem explicitly or implicitly in a closedset setting, via either loss correction or noise model based clean label inferring [22, 29, 37, 38]. However, these methods are vulnerable in the more generic openset scenario, as loss or label correc tion may be inaccurate since the true class may not exist in the dataset. Openset noisy labels are likely to occur for sce narios where data are harvested rapidly, or use approximate labels ( e.g., using a search engine query to retrieve images and then labeling the images according to the query key word that was used). To the best of our knowledge, how to address the openset noisy label problem is a new challenge. In this paper, we propose an iterative learning framework that can robustly train CNNs on datasets with openset noisy labels. Our model works iteratively with: (1) a noisy label detector to iteratively identify noisy labels; (2) a Siamese network for discriminative feature learning, which imposes a representation constraint via contrastive loss to pull away noisy samples from clean samples in the deep representa tion space; and (3) a reweighting module on the softmax loss to express a relative conÔ¨Ådence of clean and noisy la bels on the representation learning. A simpliÔ¨Åed illustration of the proposed framework is presented in Figure 2. Our main contributions can be summarized as follows: (1) We identify the openset noisy label problem as a new challenge for representation learning and prediction. (2) We propose an iterative learning framework to ro bustly train CNNs in the presence of openset noisy labels. Our model is not dependent on any assumption of noise. (3) We empirically demonstrate that our model sig niÔ¨Åcantly outperforms stateoftheart noisy label learning models for the openset setting, and has a comparable or even better performance under the closedset setting. 2. Related work "
475,An Entropic Optimal Transport Loss for Learning Deep Neural Networks under Label Noise in Remote Sensing Images.txt,"Deep neural networks have established as a powerful tool for large scale
supervised classification tasks. The state-of-the-art performances of deep
neural networks are conditioned to the availability of large number of
accurately labeled samples. In practice, collecting large scale accurately
labeled datasets is a challenging and tedious task in most scenarios of remote
sensing image analysis, thus cheap surrogate procedures are employed to label
the dataset. Training deep neural networks on such datasets with inaccurate
labels easily overfits to the noisy training labels and degrades the
performance of the classification tasks drastically. To mitigate this effect,
we propose an original solution with entropic optimal transportation. It allows
to learn in an end-to-end fashion deep neural networks that are, to some
extent, robust to inaccurately labeled samples. We empirically demonstrate on
several remote sensing datasets, where both scene and pixel-based hyperspectral
images are considered for classification. Our method proves to be highly
tolerant to significant amounts of label noise and achieves favorable results
against state-of-the-art methods.","Deep learning has been applied with tremendous success on a variety of tasks in remote sensing image analysis. For instance, achievement of stateoftheart performance in scene classiÔ¨Åcation (Cheng et al., 2018; Anwer et al., 2018), pixelwise labeling of both multispectral (Huang et al., 2018; Audebert et al., 2018; Maggiori et al., 2017) and hyperspectral datasets (Zhong et al., 2018; Wang et al., 2017), object detection (Kellenberger et al., 2018) and image retrieval (Zhou et al., 2018; Ye et al., 2017; Li et al., 2018), highlights the recent success of deep learning models in remote sensing. But these phenomenal performances is highly dependant on the availability of large collection of datasets with accurate annotations (labels). If either the size of the dataset or the accuracy of the labels is not sufÔ¨Åcient (i.e, small scale datasets or inaccurate labels), the performance of the deep learning methods could suffer drastically. The former one can be addressed to some Under consideration at Computer Vision and Image Understanding. Preprint. Work in progress.arXiv:1810.01163v1  [cs.CV]  2 Oct 2018degree by data augmentation strategies, however solving the later case of inaccurate labeling is more difÔ¨Åcult. To address the large scale data requirements of deep learning methods, new datasets have been proposed recently in the remote sensing community (Zhou et al., 2018; Huang et al., 2018; Cheng et al., 2017; Kemker et al., 2017; Wang et al., 2016; Xia et al., 2017). This trend will grow con tinuously in the coming years, due for instance to the large constellation of the Earth observation satellites. One of the major challenge in collecting this new large scale data is accurate labeling of the samples. Manual expert labeling of such large collection of samples is often not feasible and not costeffective. Thus, labeling is usually performed by nonexperts through crowd sourcing (Snow et al., 2008; Haklay, 2010), keyword query through search engine in the case of images, open street maps, and outdated classiÔ¨Åcation maps (Kaiser et al., 2017). These cheap surrogate procedures allows scaling the size of labeled datasets, but at the cost of introducing label noise (i.e. inaccurately labeled samples). Even when manual experts are involved in labeling the data samples, they must be provided with sufÔ¨Åcient information; otherwise inaccurate labeling may still occur (for instance, during the Ô¨Åeld survey) (Hickey, 1996). Note that in the some applications, labeling is a subjective task (Smyth et al., 1995) that can again introduce label noise. Furthermore, the label noise could occur due to the misregistration of satellite images. Hence in general, large scale datasets might mostly contain inaccurately labeled samples or affected by label noise. In this case, when deep learning methods are employed with conventional loss functions (for instance, categorical cross en tropy, mean square error), they will not be robust to label noise, and as a result the classiÔ¨Åcation accuracy decreases signiÔ¨Åcantly (Zhang et al., 2017). This calls for robust approaches to mitigate the impact of label noise on the deep learning methods. Recently, it was shown that while training deeper neural networks, models tend to memorize the training data, and this phenomena is more severe when the dataset is affected by the label noise (Zhang et al., 2017). The impact of the label noise in the deep learning models can be partly cir cumvented by regularization techniques such as drop out layers, and weight regularization. These standard procedures make neural networks robust to some extend, but they are still prone to mem orize noisy labels for mediumtolarge noise levels. The problem of learning with noisy labels has been long studied in machine learning (Frenay and Verleysen, 2014; Brooks; Zhu and Wu, 2004; S¬¥aez et al., 2014; Hickey, 1996; Smyth et al., 1995; Natarajan et al., 2013), but still only few works have focused on neural networks. Recently, new approaches have been proposed in the computer vision and machine learning Ô¨Åelds to tackle the label noise by cleaning the noisy labels or designing robust loss functions within the deep learning framework (Jiang et al., 2018; Vahdat, 2017; Patrini et al., 2017). To mitigate the impact of label noise, one category of method relies on estimating the noise transition probability that describes the probability of ithclass label being mislabeled to the jthclass label, and use it to be robust to label noise (Vahdat, 2017; Natarajan et al., 2013; Patrini et al., 2017). Among those, some of them require a small set of clean labels to estimate the noise transition probability (Vahdat, 2017). The other category of methods proposes to use loss functions which are inherently tolerant to the label noise (Natarajan et al., 2013; van Rooyen et al., 2015; MasnadiShirazi, Hamed and Vasconcelos, 2008; Ghosh et al., 2015; Aritra et al., 2017). Though these methods provided satisfactory results, none of them consider the implicit local geometric structure of the underlying data. The primary objective of this paper is to develop a robust approach to tackle the label noise for remote sensing image analysis. The sensitiveness of deep neural networks to label noise has not been well studied in remote sensing image analysis so far as per our knowledge. Hence the Ô¨Årst contribution of this article lies in studying the robustness of deep neural networks to label noise, and also to analyse the efÔ¨Åciency of existing robust loss functions for remote sensing classiÔ¨Åcation tasks. The second contribution of this paper is to propose a novel robust solution to tackle the label noise based on optimal transportation theory (Villani, 2009). Indeed we propose to learn a deep learning model which is robust to label noise by Ô¨Åtting the model to the labelfeatures joint distribution of the dataset with respect to the entropyregularized optimal transport distance. We coin this method as CLEOT for ClassiÔ¨Åcation Loss with Entropic Optimal Transport. One major advantage of our approach compared to existing methods is that our method inherently exploits the geometric structure of the underlying data. A stochastic approximation schemes is proposed to solve the learning problem, and allows the use of our approach within deep learning frameworks. Experiments are conducted on several remote sensing aerial and hyperspectral benchmark datasets, 2and the results demonstrate that our approach is more robust (tolerant) to high level label noise than current stateoftheart methods. The remaining of the paper is organized as follows. Section 2 discusses related works, section 3 deÔ¨Ånes the label noise and describes the problem formulation, and section 4 introduces optimal transport. The proposed method is then presented in section 4.2 while experimental datasets and results are explained in section 5. We Ô¨Ånally draw some conclusions in section 6. 2 Related works "
524,Refining Pseudo Labels with Clustering Consensus over Generations for Unsupervised Object Re-identification.txt,"Unsupervised object re-identification targets at learning discriminative
representations for object retrieval without any annotations. Clustering-based
methods conduct training with the generated pseudo labels and currently
dominate this research direction. However, they still suffer from the issue of
pseudo label noise. To tackle the challenge, we propose to properly estimate
pseudo label similarities between consecutive training generations with
clustering consensus and refine pseudo labels with temporally propagated and
ensembled pseudo labels. To the best of our knowledge, this is the first
attempt to leverage the spirit of temporal ensembling to improve classification
with dynamically changing classes over generations. The proposed pseudo label
refinery strategy is simple yet effective and can be seamlessly integrated into
existing clustering-based unsupervised re-identification methods. With our
proposed approach, state-of-the-art method can be further boosted with up to
8.8% mAP improvements on the challenging MSMT17 dataset.","Recent years witnessed the remarkable progresses of employing unsupervised representation learning in various downstream visual recognition tasks, such as image classi Ô¨Åcation [1, 13, 14, 20], object detection [23, 22, 17, 33], and object reidentiÔ¨Åcation (reID) [27, 28, 37, 46, 10]. Ob ject reID aims at retrieving objects of interest in largescale gallery images given an object‚Äôs query images. The task of unsupervised object reID further requires learning dis criminative representations to properly model inter/intra identity afÔ¨Ånities without any annotations, which is a more *The Ô¨Årst two authors contribute equally. +hIkG]Y<DIYh<j+hIkG]Y<DIYh<j ""Ijq]gX ""Ijq]gX 0g<Q[Q[Oh<ZdYIh /kdIgpQhIGDsdhIkG]Y<DIYh+hIkG]¬üY<DIYE][NQGI[EIh/kdIgpQhIGDsgINQ[IGdhIkG]Y<DIYh+hIkG]Y<DIYdg]d<O<jQ][ YkhjIgQ[O ][hI[hkhFigure 1: Illustration of the proposed ReÔ¨Åning pseudo La bel with Clustering Consensus (RLCC) framework. Hard pseudo labels or soft pseudolabel conÔ¨Ådences from the pre vious generation t"
181,Training Data Subset Search with Ensemble Active Learning.txt,"Deep Neural Networks (DNNs) often rely on very large datasets for training.
Given the large size of such datasets, it is conceivable that they contain
certain samples that either do not contribute or negatively impact the DNN's
optimization. Modifying the training distribution in a way that excludes such
samples could provide an effective solution to both improve performance and
reduce training time. In this paper, we propose to scale up ensemble Active
Learning (AL) methods to perform acquisition at a large scale (10k to 500k
samples at a time). We do this with ensembles of hundreds of models, obtained
at a minimal computational cost by reusing intermediate training checkpoints.
This allows us to automatically and efficiently perform a training data subset
search for large labeled datasets. We observe that our approach obtains
favorable subsets of training data, which can be used to train more accurate
DNNs than training with the entire dataset. We perform an extensive
experimental study of this phenomenon on three image classification benchmarks
(CIFAR-10, CIFAR-100 and ImageNet), as well as an internal object detection
benchmark for prototyping perception models for autonomous driving. Unlike
existing studies, our experiments on object detection are at the scale required
for production-ready autonomous driving systems. We provide insights on the
impact of different initialization schemes, acquisition functions and ensemble
configurations at this scale. Our results provide strong empirical evidence
that optimizing the training data distribution can provide significant benefits
on large scale vision tasks.","Deep Neural Networks (DNNs) have become the domi nant approach for addressing supervised learning tasks in volving highdimensional inputs. There is signiÔ¨Åcant inter est in automating the endtoend process of applying DNNs to realworld problems such as training perception systems for autonomous driving [1], [2], [3]. While there has been a considerable effort towards methods and frameworks that automate DNN architecture search [4], [5], [6], [7], [8] and training hyperparameter search [9], [10], [11]; the process of searching for the right training data distribution (also called dataset curation) is still performed by experts, requiring several heuristics and signiÔ¨Åcant manual effort. With the rapid growth in the availability of labeled data for perception tasks, to the order of billions of samples [12], [13], automating the training data subset search would make the application of DNNs much easier for nonexperts, and potentially lead to datasets and models that outperform those that were curated by hand. In this paper, we present a simple yet effective method to perform a training data subset search by using ensemble Active Learning (AL). The typical goal of AL is to select, from a large unlabeled dataset, the smallest possible train ing set to label in order to solve a speciÔ¨Åc task [14]. We instead propose to use AL to build data subsets of a large labeled training dataset that give more accurate DNNs in less training time. We demonstrate that this approach can automatically curate large datasets. We study the impact of K. Chitta is with the Max Planck Institute for Intelligent Systems, T¬® ubingen and University of T¬® ubingen. Work completed during an in ternship at NVIDIA. Email: kashyap.chitta@tue.mpg.de J. M. ¬¥Alvarez, E. Haussmann and C. Farabet are with NVIDIA.key design choices for AL, and the robustness of the selected subsets to changes in model architectures. We tackle two key issues that have not been addressed so far by stateoftheart AL methods. The Ô¨Årst is the dif Ô¨Åculty in scaling the number of models for the popular ensemble AL technique. While it seems intuitive that more ensembles can improve performance, existing studies show no gains in AL performance beyond 10 models, and even recommend the use of only 5 models [15], [16]. In this study, we propose the use of implicit ensembles with hundreds of training checkpoints from different experimental runs, and empirically demonstrate the effectiveness of this approach. Second, we switch to a largescale experimental setting compared to what is typically used for AL experiments. For example, Beluch et al. [17] and Sinha et al. [18] never use more than 30% of the ImageNet dataset, and do not compete with the full dataset performance. In contrast, for ResNet18 training with 80% of ImageNet, we improve the top1 accuracy by 0.5% over a model trained with the entire ImageNet dataset. For object detection, existing studies are limited to acquisition at the order of 10k samples, beyond which improvements become marginal [19], [20], [21], [22]. In this paper, we scale the process to the acquisition of 200k images in each iteration, where the Ô¨Ånal selected subset outperforms training with all the available data. Our exper iments are the Ô¨Årst to provide insights regarding automatic dataset curation at the scale required for productionready autonomous driving systems. To summarize, our contributions are as follows: (1) we propose a simple approach to scale up ensemble AL meth ods to hundreds of models with a negligible computational overhead at train time, and (2) we conduct a detailed empirical study on how to effectively reduce the size ofarXiv:1905.12737v3  [cs.LG]  7 Nov 20202 existing datasets, containing millions of samples, without human curation. Our study provides practical insights by covering several large datasets for object detection and image classiÔ¨Åcation. 2 R ELATED WORK "
409,Kinship Identification through Joint Learning Using Kinship Verification Ensembles.txt,"Kinship verification is a well-explored task: identifying whether or not two
persons are kin. In contrast, kinship identification has been largely ignored
so far. Kinship identification aims to further identify the particular type of
kinship. An extension to kinship verification run short to properly obtain
identification, because existing verification networks are individually trained
on specific kinships and do not consider the context between different kinship
types. Also, existing kinship verification datasets have biased
positive-negative distributions which are different than real-world
distributions. To this end, we propose a novel kinship identification approach
based on joint training of kinship verification ensembles and classification
modules. We propose to rebalance the training dataset to become more realistic.
Large scale experiments demonstrate the appealing performance on kinship
identification. The experiments further show significant performance
improvement of kinship verification when trained on the same dataset with more
realistic distributions.","Kinship is the relationship between people who are biologically related with over lapping genes [17,18], such as parentchildren, siblingsibling, and grandparent grandchildren [1,20,21,28]. Imagebased kinship identication is used in a variety of applications including missing children searching [28], family album organiza tion, forensic investigation [21], automatic image annotation [17], social media analysis [34,6,3], social behavior analysis [14,35,19,11], historical and genealogi cal research [15,6], and crime scene investigation [16]. While kinship verication is a wellexplored task, identifying whether or not persons are kin, kinship identication, which is the task to further identify the particular type of kinship, has been largely ignored so far. Existing kinship veri cation methods usually train and test each type of kinship model independently [24,20,28] and hence do not fully exploit the complementary information amongarXiv:2004.06382v4  [cs.CV]  24 Aug 20202 W. Wang et al. Is	Kin/not	Kin	? Which	type	? father  daughterfather  sonmother  daughtermother  son YesYesYesYes ?father daughter (a)	Outputs	from	individual verification	models	of	attention Network(Y an2019)Is	Kin/not	Kin	? Which	type	?kinship veriÔ¨Åcation model with Attention Network structure (Y an2019) binary output of each veriÔ¨Åcation model without softmax multiclassiÔ¨Åcation result of joint learning approach (b)	Output	from	proposed	joint learning	approach(JLNet) MINBasicfeature extraction Module father  daughterfather  sonmother  daughtermother  son+ binary output of each veriÔ¨Åcation model with softmaxkinship veriÔ¨Åcation model with Attention Network structure (Y an2019) Fig. 1: Identication of kinship relationships using verication ensembles. (a) Ex isting verication networks are trained independently resulting in contradictory outputs. (b) The output of our proposed joint training dierent kin types. Moreover, existing datasets have unrealistic positivenegative sample distributions. This leads to signicant limitations in real world applica tions. When conducting kinship identication, since there is no prior knowledge of the distribution of images, all independently trained models are used to de termine the kinship type of a specic image pair. Fig. 1 shows an example of providing an image pair to four individually trained verication networks based on a recent stateoftheart method by Yan et al. [33]. The network generates contradictory outputs showing that the test subjects are simultaneously father daughter, fatherson, motherson and motherdaughter. In this paper, a new identication method is proposed to learn the identica tion and verication labels jointly i.e. combining the kinship identication and verication tasks. Specically, all kinshiptype verication models are ensembled by combining the binary output of each verication model to form a multiclass output while training. The binary and multiclass models are leveraged in a multitasklearning way during the training process to enhance generalization capabilities. Also, we propose a baseline multiclassication neural network for comparison. We test our proposed kinship identication method on the KinfaceWI and KinfaceWII datasets and demonstrate stateoftheart performance for kinship identication. We also show that the proposed method signicantly improves the performance of kinship verication when trained on the same unbiased dataset. To summarize, the contributions of our work are: {We propose a theoretical analysis in metric space of relationships between kinship identication and kinship verication. {We propose a joint learnt network that simultaneously optimizes the perfor mance of kinship verication and kinship identication. {The proposed method outperforms existing methods for both kinship iden tication and unbiased kinship verication.Kinship Identication through Joint Learning 3 2 Related Work "
213,Weakly Supervised Vessel Segmentation in X-ray Angiograms by Self-Paced Learning from Noisy Labels with Suggestive Annotation.txt,"The segmentation of coronary arteries in X-ray angiograms by convolutional
neural networks (CNNs) is promising yet limited by the requirement of precisely
annotating all pixels in a large number of training images, which is extremely
labor-intensive especially for complex coronary trees. To alleviate the burden
on the annotator, we propose a novel weakly supervised training framework that
learns from noisy pseudo labels generated from automatic vessel enhancement,
rather than accurate labels obtained by fully manual annotation. A typical
self-paced learning scheme is used to make the training process robust against
label noise while challenged by the systematic biases in pseudo labels, thus
leading to the decreased performance of CNNs at test time. To solve this
problem, we propose an annotation-refining self-paced learning framework
(AR-SPL) to correct the potential errors using suggestive annotation. An
elaborate model-vesselness uncertainty estimation is also proposed to enable
the minimal annotation cost for suggestive annotation, based on not only the
CNNs in training but also the geometric features of coronary arteries derived
directly from raw data. Experiments show that our proposed framework achieves
1) comparable accuracy to fully supervised learning, which also significantly
outperforms other weakly supervised learning frameworks; 2) largely reduced
annotation cost, i.e., 75.18% of annotation time is saved, and only 3.46% of
image regions are required to be annotated; and 3) an efficient intervention
process, leading to superior performance with even fewer manual interactions.","Coronary artery disease (CAD) is one of the leading causes of death globally [1]. It is primarily caused by ob structive atherosclerotic plaque [2], which narrows the in ner wall of coronary artery and decreases normal myocar dial perfusion, leading to symptoms such as angina and even myocardial infarction [3]. Percutaneous coronary in tervention (PCI) is a minimally invasive surgery to eec tively treat CAD in clinical practice. In such a procedure, a cardiologist delivers a catheter with a premounted stent through coronary arteries to the stenosis lesion. Once the lesion is reached, the stent is deployed against the nar row coronary wall by in ating the delivery balloon. Since target vessels are not directly visible, PCI is performed under image guidance by using Xray angiography to vi sualize coronary arteries for the injection of radiopaque contrast agent. The accurate segmentation of vessels in Corresponding author Email addresses: xiehongzhi@medmail.com.cn (Hongzhi Xie), gulixu@sjtu.edu.cn (Lixu Gu)Xray angiograms (XAs) enables the quantitative analy sis of coronary trees [4] and is fundamental for the safe navigation of intervention devices for PCI surgery. Deep learning with convolutional neural networks (CNNs) has achieved the stateoftheart performance for medical image segmentation [5, 6, 7], including vessel seg mentation in XA [8, 9]. Following the fully supervised learning framework, its success relies heavily on a large amount of precise annotations for all pixels in training im ages to improve generalization capability for unseen test ing images. However, precisely annotating coronary ar teries is costly and requires special expertise, especially for thin branches with tubular appearance and low con trast in XA. To alleviate such heavy annotation burden on the annotator, reducing the amount of precise manual annotations is highly demanded in clinical practice [10]. In contrast, obtaining noisy pseudo labels appears to be less expensive. Specically, vessel enhancement [11] au tomatically extracts vascular structures based on hand craft priors [12], providing a feasible method for generat ing pseudo labels for training CNNs without any manual interaction. This can largely reduce the manual annota Preprint submitted to Elsevier May 28, 2020arXiv:2005.13366v1  [cs.CV]  27 May 2020Xray  Angiogram Pseudo  Label Figure 1: Noisy pseudo labels generated from vessel enhancement, where the systematic errors are highlighted by yellow arrows. tions required for model training, while leading to noise with systematic biases in pseudo labels for structures, such as bifurcation points and thin vessels with small scales, as shown in Fig. 1. These noisy pseudo labels challenge the learning process and cause performance degradation of CNNs at test time [13]. It is desirable to develop a robust training framework against systematic label noise and facilitate segmentation performance close to the fully supervised learning framework. Aimed at robustly learning from noisy labels, some pre vious weakly supervised training frameworks model label noise explicitly as an additional network layer [14, 15, 16] or implicitly using prior knowledge [17, 18]. Among them, researchers have shown that the selfpaced learning paradigm can be substantially eective and scalable [19], owing to its predened selfpaced regularizer [20]. This learning paradigm typically assumes a plain distribution of label noise without systematic biases to specic seg mentation regions and semantic categories. An iterative optimization process is used to facilitate noise robustness of the model. In each iteration, the selfpaced regular izer progressively selects only easy pixels while excluding dicult pixels with potential label noise from model train ing. Noisy labels are modied automatically by updating the segmentation results of training images based on the current model. They are expected to contain fewer er rors than those in previous iterations, providing improved supervision for the next iteration. Unfortunately, this self paced learning paradigm may make the model overt on easy pixels, leading to a poor generalization performance at test time. Moreover, the noise in pseudo labels often contains specic biases due to the inherent limitations of vessel enhancementbased generation process. Using this naive selfpaced learning paradigm alone has only the lim ited ability to correct the erroneous pseudo labels. Manually detecting and correcting potentially erroneous pseudo labels is a practical way to avoid the selfpaced learning being corrupted by systematic errors, while it is still laborintensive and timeconsuming. Suggestive anno tation [21] has been shown to be a more ecient method for interactive renement by intelligently selecting a small number of the most valuable pixels and then querying their labels. It suggests the annotator accurately label only the most uncertain pixels with potentially incorrect labels [22],commonly based on the widely used model uncertainty [23], i.e., the entropy of CNNs. The required annotation cost can be successfully reduced owing to the eective ex ploration of potential errors. However, model uncertainty fails to exploit geometric features derived directly from training images, resulting in redundancy among queries [24, 25] and a low eciency for manual interaction. In contrast, considering the vesselness of pixels is expected to lead to more contextaware uncertainty estimation as it takes advantage of vascular geometric features. Since the model uncertainty and vesselness uncertainty are comple mentary, we believe that their combination would provide more reliable uncertainty estimation that eciently guides user interaction in suggestive annotation. To solve these problems, this paper develops a novel weakly supervised vessel segmentation framework, which learns from costfree but noisy pseudo labels generated from automatic vessel enhancement. Specically, to over come noisy pseudo labels with systematic biases, we pro pose to progressively guide the naive selfpaced learn ing with auxiliary sparse manual annotations, which is called annotationrening selfpaced learning (ARSPL). ARSPL not only exploits the available knowledge from noisy pseudo labels, but also corrects potential errors us ing their corresponding manual annotations. These man ual annotations, even when sparse in training images, play an important role in hedging the risk of learning from noisy pseudo labels. Furthermore, to enable a minimal set of an notations, we propose a modelvesselness uncertainty esti mation for suggestive annotation, which dynamically and compactly takes into account the trained CNN and the geometric features of coronary arteries in XAs. 1.1. Contributions The contributions of this work are threefold. ‚Ä¢First, we propose a novel weakly supervised learning framework in the context of vessel segmentation, aim ing to safely learn from noisy pseudo labels generated by vessel enhancement without performance deterio ration at test time. ‚Ä¢Second, to deal with the biased label noise, we develop online guidance for the naive selfpaced learning based on sparse manual annotations, which is crucial for a signicant segmentation performance boost. ‚Ä¢Third, towards minimal manual intervention, we pro pose a customized vesselness uncertainty based on vascular geometric feature, and then couple it with the widely used model uncertainty by a dynamic tradeo for more ecient suggestive annotation. Experiments demonstrate the eectiveness and e ciency of the proposed framework, where only a very small set of manual annotations can lead to an accurate segmen tation result that is comparable to the fully supervised learning. 21.2. Related Works "
510,Semi-Supervised Confidence Network aided Gated Attention based Recurrent Neural Network for Clickbait Detection.txt,"Clickbaits are catchy headlines that are frequently used by social media
outlets in order to allure its viewers into clicking them and thus leading them
to dubious content. Such venal schemes thrive on exploiting the curiosity of
naive social media users, directing traffic to web pages that won't be visited
otherwise. In this paper, we propose a novel, semi-supervised classification
based approach, that employs attentions sampled from a Gumbel-Softmax
distribution to distill contexts that are fairly important in clickbait
detection. An additional loss over the attention weights is used to encode
prior knowledge. Furthermore, we propose a confidence network that enables
learning over weak labels and improves robustness to noisy labels. We show that
with merely 30% of strongly labeled samples we can achieve over 97% of the
accuracy, of current state of the art methods in clickbait detection.","With the number of social media users increasing by the day, one of the prime objectives of online news media agencies is to lead social media users onto bogus pages through the display of luscious text/images (Loewenstein, 1994). In most cases the content on the landing page is disparate to the headline the user clicked on. Source veriÔ¨Åcation is no longer warranted as news agencies aren‚Äôt held accountable for the content they post online. As (M. Potthast and Hagen, 2016) suggests, at least 15 of the most prominent content creators use clickbaits in some form or the other to honeytrap users. Impetus for such schemes can range fromdirecting trafÔ¨Åc to web sites that force users to pur chase a product, to shaping popular opinion espe cially during elections. Some clickbaits claim to accomplish inconceivable tasks while others rely on a viewer‚Äôs inducement to grapevines. ‚ÄúYou will never believe what this celebrity did at the awards ceremony.‚Äù ‚Äú10 things that will get you fairer in 5 days.‚Äù ‚ÄúMillionaires want to conceal this scheme. It can make you rich in a week.‚Äù Earlier approaches on tackling clickbaits mainly focused on cheap and easy to implement solu tions. Blacklisting URLs has been, to some ex tent, effective in regulating an average user‚Äôs expo sure to clickbaits. (Gianotto, 2014) assumed that most clickbaits are based on a few key phrases, and a naive way yet effective strategy would be to simply look for such phrases. Such an as sumption holds no ground today. As the problem grew to be more pervasive, social media compa nies modeled the probability for a content to be a clickbait based on factors like clicktoshare ra tio, time spent by user on the target page, and other such quantiÔ¨Åers. Recent research focuses on salvaging sentence structures, ngrams & embed dings among other features, in classiÔ¨Åers like Ran dom Forests (RF), Gradient Boosted Trees (GBT), Support Vector Machines (SVM) or the vanilla Logistic Regression (LR). With the advent of on line news agencies, there exists a plethora of such sources, but labeling each of the headlines from them would be a staggering task. This vindicates the need for an unsupervised / semisupervised ap proach. Contributions of this paper include: 1. A novel loss component on the attention weights that en codes prior information from a weak source of laarXiv:1811.01355v1  [cs.CL]  4 Nov 2018bels, which eventually improves the generalizabil ity of the deep learning model that has been trained on a small representative dataset. 2. A joint archi tecture that incorporates into the clickbait classiÔ¨Å cation framework a conÔ¨Ådence network that tack les label noise. 3. Using GumbelSoftmax for gated attentions, thus enforcing peaky attentions over word contexts. 4. Empirically proving the performance of the proposed approach on popu lar clickbait datasets with only a small portion of labeled samples. 2 Related Work "
587,Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning.txt,"Distantly supervised named entity recognition (DS-NER) efficiently reduces
labor costs but meanwhile intrinsically suffers from the label noise due to the
strong assumption of distant supervision. Typically, the wrongly labeled
instances comprise numbers of incomplete and inaccurate annotation noise, while
most prior denoising works are only concerned with one kind of noise and fail
to fully explore useful information in the whole training set. To address this
issue, we propose a robust learning paradigm named Self-Collaborative Denoising
Learning (SCDL), which jointly trains two teacher-student networks in a
mutually-beneficial manner to iteratively perform noisy label refinery. Each
network is designed to exploit reliable labels via self denoising, and two
networks communicate with each other to explore unreliable annotations by
collaborative denoising. Extensive experimental results on five real-world
datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising
methods.","Named Entity Recognition (NER) is the task of detecting entity spans and then classifying them into predeÔ¨Åned categories, such as person, location and organization. Due to the capability of extract ing entity information and beneÔ¨Åting many NLP applications (e.g., relation extraction (Lin et al., 2017), question answering (Li et al., 2019)), NER appeals to many researchers. Traditional super vised methods for NER require a large amount of highquality corpus for model training, which is extremely expensive and timeconsuming as NER requires tokenlevel labels. Therefore, in recent years, distantly supervised named entity recognition (DSNER) has been pro posed to automatically generate labeled training set Corresponding author 1The source code and data can be found at https:// github.com/AIRobotZhang/SCDL . Jack  Lucas  was  born  in  the  Amazon  region  . BPER  I PER    O       O OO BLOC O      O O         O O O OO BORG O      OGolden Labels : Noisy Labels :Figure 1: A noisy sample generated by distantly supervised methods, where Jack Lucas is the incom plete annotation and Amazon is inaccurate. by aligning entities in knowledge bases (e.g., Free base) or gazetteers to corresponding entity men tions in sentences. This labeling procedure is based on a strong assumption that each entity mention in a sentence is a positive instance of the correspond ing type according to the extra resources. How ever, this assumption is far from reality. Due to the limited coverage of existing resources, many entity mentions in the text cannot be matched and are wrongly annotated as nonentity, resulting in incomplete annotations. Moreover, two entity men tions with the same surface name can belong to different entity types, thus simple matching rules may fall into the dilemma of labeling ambiguity and produce inaccurate annotations. As illustrated in Figure 1, the entity mention ‚Äú Jack Lucas ‚Äù is not recognized due to the limited coverage of extra resources and ‚Äú Amazon ‚Äù is wrongly labeled with organization type owing to the labeling ambiguity. Recently, many denoising methods (Shang et al., 2018b; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019; Li et al., 2021) have been developed to handle noisy labels in DSNER. For example, Shang et al. (2018b) obtained highquality phrases through AutoPhrase (Shang et al., 2018a) and de signed AutoNER to model these phrases that may be potential entities. Peng et al. (2019) proposed a positiveunlabeled learning algorithm to unbiasedly and consistently estimate the NER task loss, and Li et al. (2021) used negative sampling to elimi nate the misguidance brought by unlabeled entities. Though achieving good performance, most studies mainly focus on solving incomplete annotationsarXiv:2110.04429v2  [cs.CL]  15 Feb 2023with a strong assumption of no inaccurate ones existing in DSNER. Meanwhile, these methods aim to reduce the negative effect of noisy labels by weakening or abandoning the wrongly labeled instances. Hence, they can at most alleviate the noisy supervision and fail to fully mine useful in formation from the mislabeled data. Intuitively, if we can rectify those unreliable annotations into positive instances for model training, a higher data utilization and better performance will be achieved. We argue that an ideal DSNER denoising system should be capable of solving two kinds of label noise (i.e., incomplete and inaccurate annotations) and making full use of the whole training set. In this work, we strive to reconcile this gap and propose a robust learning framework named SCDL (SelfCollaborative Denoising Learning). SCDL cotrains two teacherstudent networks to form in ner and outer loops for coping with label noise without any assumption, as well as making full exploration of mislabeled data. The inner loop in side each teacherstudent network is a self denois ing scheme to select reliable annotations from two kinds of noisy labels, and the outer loop between two networks is a collaborative denoising proce dure to rectify unreliable instances into useful ones. SpeciÔ¨Åcally, in the inner loop, each teacherstudent network selects consistent and highconÔ¨Ådence la beled tokens generated by the teacher to train the student, and then updates the teacher gradually via exponential moving average (EMA)2based on the retrained student. And as for the outer loop, the highquality pseudo labels generated by one net work‚Äôs teacher are used to update the noisy labels of the other network thanks to the stability of EMA and different noise sensitivities between two net works. Moreover, the inner and outer loop proce dures will be performed alternately. Obviously, a successful self denoising process (inner loop) can generate highquality pseudo labels which beneÔ¨Åt the collaborative learning procedure (outer loop) a lot and a promising outer loop will promote the inner loop by reÔ¨Åning noisy labels, thus handling the label noise in DSNER effectively. We evaluate our method on Ô¨Åve DSNER datasets. Experimental results indicate that SCDL consistently achieves superior performance over previous competing approaches. Extensive valida 2A momentum technique that has been explored in several studies, e.g., Adam (Kingma and Ba, 2015), semisupervised (Tarvainen and Valpola, 2017) and self supervised (Grill et al., 2020) learning.tion studies demonstrate the rationality and robust ness of our selfcollaborative denoising framework. 2 Related Work "
407,UnibucKernel: Geolocating Swiss German Jodels Using Ensemble Learning.txt,"In this work, we describe our approach addressing the Social Media Variety
Geolocation task featured in the 2021 VarDial Evaluation Campaign. We focus on
the second subtask, which is based on a data set formed of approximately 30
thousand Swiss German Jodels. The dialect identification task is about
accurately predicting the latitude and longitude of test samples. We frame the
task as a double regression problem, employing an XGBoost meta-learner with the
combined power of a variety of machine learning approaches to predict both
latitude and longitude. The models included in our ensemble range from simple
regression techniques, such as Support Vector Regression, to deep neural
models, such as a hybrid neural network and a neural transformer. To minimize
the prediction error, we approach the problem from a few different perspectives
and consider various types of features, from low-level character n-grams to
high-level BERT embeddings. The XGBoost ensemble resulted from combining the
power of the aforementioned methods achieves a median distance of 23.6 km on
the test data, which places us on the third place in the ranking, at a
difference of 6.05 km and 2.9 km from the submissions on the first and second
places, respectively.","The Social Media Variety Geolocation (SMG) task was proposed, for the second year consecutively, in the 2021 edition of the VarDial Evaluation Cam paign (Chakravarthi et al., 2021). This task is aimed at geolocation prediction based on short text mes sages exchanged by the users of social media plat forms such as Twitter or Jodel. The location from where a short text was posted on a certain social media platform is expressed by two components: the latitude and the longitude. Naturally, the ge olocation task is formulated as a double regressionproblem. Twitter and Jodel are the platforms used for data collection, and similar to the previous spin of SMG at VarDial 2020 (G Àòaman et al., 2020), the task is divided into three subtasks, by language area, namely: ‚Ä¢Standard German Jodels (DEAT)  which tar gets conversations initiated in Germany and Austria in regional dialectal forms (Hovy and Purschke, 2018). ‚Ä¢Swiss German Jodels (CH)  containing a smaller number of Jodel conversations from the German speaking half of Switzerland (Hovy and Purschke, 2018). ‚Ä¢BCMS Tweets  from the area of Bosnia and Herzegovina, Croatia, Montenegro and Ser bia where the macrolanguage is BCMS, with both similarities and a fair share of variation among the component languages (Ljube Àási¬¥c et al., 2016). The focus of our work falls only on the second subtask, SMGCH, tackled via a variety of hand crafted and deep learning models. We propose a single ensemble model joining the power of sev eral individual models through metalearning based on Extreme Gradient Boosting (XGBoost) (Chen and Guestrin, 2016). We trained two independent ensemble models, each predicting one of the com ponents that form the geographical coordinates (lat itude and longitude). The Ô¨Årst model plugged into our metalearner is a Support Vector Regression (SVR) model (Chang and Lin, 2002) based on string kernels. Previous usage in dialect identiÔ¨Åcation has proved the ef Ô¨Åciency of this technique in the task of interest (Butnaru and Ionescu, 2018b; G Àòaman and Ionescu, 2020; Ionescu and Butnaru, 2017; Ionescu and Popescu, 2016).arXiv:2102.09379v3  [cs.CL]  26 Feb 2021The second model included in the ensemble is a hybrid convolutional neural network (CNN) (Liang et al., 2017) that combines, in the same architecture, characterlevel (Zhang et al., 2015) and wordlevel representations. The ability of capturing morpho logical relationships at the character level and using them as features for CNNs is also known to give promising results in dialect identiÔ¨Åcation (Butnaru and Ionescu, 2019; Tudoreanu, 2019). Different from works using solely characterlevel CNNs for dialect identiÔ¨Åcation (Butnaru and Ionescu, 2019; Tudoreanu, 2019), we believe that the addition of words might bring the beneÔ¨Åt of learning dialect speciÔ¨Åc multiword expressions that are hard to capture at the character level (Dhingra et al., 2016; Ling et al., 2015). Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is a top performing technique used in recent years for solv ing mainstream NLP problems. Thus, it seems Ô¨Åt to also include the outputs of a Ô¨Ånetuned German version of BERT in our XGBoost metalearner. We conducted experiments on the development set provided by the shared task organizers (Hovy and Purschke, 2018) in order to decide which model to choose as our submission for the SMG CH subtask. Our results indicate that the ensemble model attains the best performance. With median distances that are 56km higher, all the other mod els, tested individually on the development set, pro vide slightly worse predictions. The remainder of this paper is organized as fol lows. We present related work on dialect identiÔ¨Å cation and geolocation of short texts in Section 2. Our approach is described in detail in Section 3. We present the experiments and empirical results in Section 4. Finally, our conclusions are drawn in Section 5. 2 Related Work "
549,Student Beats the Teacher: Deep Neural Networks for Lateral Ventricles Segmentation in Brain MR.txt,"Ventricular volume and its progression are known to be linked to several
brain diseases such as dementia and schizophrenia. Therefore accurate
measurement of ventricle volume is vital for longitudinal studies on these
disorders, making automated ventricle segmentation algorithms desirable. In the
past few years, deep neural networks have shown to outperform the classical
models in many imaging domains. However, the success of deep networks is
dependent on manually labeled data sets, which are expensive to acquire
especially for higher dimensional data in the medical domain. In this work, we
show that deep neural networks can be trained on much-cheaper-to-acquire
pseudo-labels (e.g., generated by other automated less accurate methods) and
still produce more accurate segmentations compared to the quality of the
labels. To show this, we use noisy segmentation labels generated by a
conventional region growing algorithm to train a deep network for lateral
ventricle segmentation. Then on a large manually annotated test set, we show
that the network significantly outperforms the conventional region growing
algorithm which was used to produce the training labels for the network. Our
experiments report a Dice Similarity Coefficient (DSC) of $0.874$ for the
trained network compared to $0.754$ for the conventional region growing
algorithm ($p < 0.001$).","Lateral ventricles are anatomical parts of the ventricular system in the brain, where the cerebrospinal  uid is produced. Ventricular volume and its progression are associated with several brain diseases. In certain forms of dementia, the increase of lateral ventricular volume has been associated to decline in cognitive function.1 Some psychiatric illnesses such as schizophrenia have also been linked to enlargement in ventricular volume.2 Additionally, asymmetrical shapes between the left and the right lateral ventricles together with the size of the ventricles can be indicative of abnormalities in the brain.3 Even though a rough estimation of the ventricular volume such as the number of slices that the ventricles appear in, might be sucient for some applications, more accurate quantitative measurements are necessary to longitudinally study subtle dierences. It has also been shown that leveraging spatial information using ventricles as landmarks are benecial for the detection of a number of pathologies in the brain including white matter hyperintensities4and lacunes.5Though manual annotation of lateral ventricles might be an option on Send correspondence to Jonas Teuwen: jonas.teuwen@radboudumc.nl.arXiv:1801.05040v2  [cs.CV]  3 Mar 2018smaller datasets and crosssectional studies, this would not be feasible otherwise as the task is timeconsuming, laborious and subjective. Therefore an accurate, objective and independent segmentation of the left and right ventricles is desirable in clinical practice. With the success of deep neural networks6,7in visual pattern recognition, many studies have been successfully conducted in the medical image analysis domain during the past few years,8,9that have resulted in intelligent systems that reach or surpass the level of medical experts on dierent tasks and domains.10{12 Since the recent deep learning approaches follow a datadriven strategy to learn the optimal representations for the specic tasks at hand, these methods often require large sets of annotated data to train on. Several recent studies have shown strong implications of training dataset size on the quality of trained networks. For instance, it has been shown that even with gigantic datasets, the performance of the trained network linearly scales with logarithm of the size of the training data.13 Given the reasoning above, the computer vision community has created enormous labeled datasets using crowd sourcing methods, for instance using Amazon mechanical turk. However this solution is not feasible for medical datasets, as the labeling process requires specic expertise that is only possible with medical experts available. Therefore, the high costs of gathering large medical datasets have still hindered feasibility of gigantic datasets that fully leverage the high capacity of the deep neural networks on various medical image analysis domains. Another strategy to provide large labeled datasets is to use (not necessarily very accurate) available methods for the task in order to provide pseudolabels. Using this, one can provide arbitrarily large datasets as far as unlabeled data is available. This however, arises a few interesting questions to be answered: 1) Considering an imposed tradeo between the dataset size and its relative label accuracy, would that make sense to train neural networks with noisy but large datasets rather than smaller ones with more accurate labels, and 2) In case we opt for the latter, is the low accuracy of the provided pseudolabels necessarily an upperbound for the accuracy of a trained network? In this study, we aim to answer the aforementioned rather important questions by reporting a deep neural network that achieves high accuracy in segmenting the left and right ventricles separately, being trained on noisy pseudolabels. We also show that, though desirable, accurate manual labels are not mandatory to produce good results, given a large set of (unbiased) noisylabeled images. 2. METHODS "
304,Spatial-context-aware deep neural network for multi-class image classification.txt,"Multi-label image classification is a fundamental but challenging task in
computer vision. Over the past few decades, solutions exploring relationships
between semantic labels have made great progress. However, the underlying
spatial-contextual information of labels is under-exploited. To tackle this
problem, a spatial-context-aware deep neural network is proposed to predict
labels taking into account both semantic and spatial information. This proposed
framework is evaluated on Microsoft COCO and PASCAL VOC, two widely used
benchmark datasets for image multi-labelling. The results show that the
proposed approach is superior to the state-of-the-art solutions on dealing with
the multi-label image classification problem.","With the rapid development of technologies, abundant visual information is constantly received. One of the fundamental but challenging problems for image understanding is to label the objects, locations or attributes in the images, possibly with more than one label. Multilabel image classiÔ¨Åcation problem has attracted the attention of researchers in the past few years. However, the rich semantic information and higherorder la bel cooccurrence are challenging to model [1, 2]. Recently, many deep convolutional neural networks (DC NNs) were developed for singlelabel image classiÔ¨Åcation problem [3‚Äì5], and transforming the multilabel classiÔ¨Åca tion problem into multiple binary classiÔ¨Åcation tasks is one of the common strategies [6] to solve the multilabel image classiÔ¨Åcation problem. However, this kind of method ignores the interdependencies among labels, which have been proved useful [7, 8]. To tackle this problem, researchers developed various DCNNs [7, 9‚Äì14] that can consider all the labels for a given image concurrently. For example, Wang et al. designed a sequentially predict model and used a recurrent This work was supported in part by the National Natural Science Foun dation of China under Grant 72071116, and in part by the Ningbo Municipal Bureau of Science and Technology under Grant 2019B10026. Fig. 1 . An illustrative example. Previous methods cannot detect the tennis racket. By exploiting the label dependencies, spatial and context information, the proposed approach could more accurately detect it. neural network (RNN) to determine label dependencies [7], while Chen et al. and Wang et al. employed graph convolu tional networks to capture global label dependencies [13, 15]. By exploiting labelcorrelation information, these approaches made great progress on image multilabelling. Nevertheless, object spatial information and image context information are not fully exploited in these approaches. Wei et al. [16] and Yang et al. [17] addressed this problem by devising a 2stage pipeline for multilabelling in which the model generates image patches Ô¨Årst and then labels them. However, these methods overemphasize the generated patches, thereby neglecting surrounding context infomation and label dependencies. The idea of object local ization is similar to the attention mechanism that has been successfully applied in many vision tasks [10, 11, 18‚Äì20]. Fig. 1 illustrates the importance of label dependencies, spa tial and context information. Additionally, context has been demonstrated useful in various visual processing tasks, such as recognition and detection [21‚Äì23]. To make good use of these dependencies and information, a twobranch spatialcontextaware DCNN is designed, where one branch is designed to extract the spatial information of the objects and the other aims at capturing the image con text information. The network‚Äôs label predictor follows thearXiv:2111.12296v2  [cs.CV]  20 Feb 2022principle of multilabel image classiÔ¨Åcation and utilizes the dependencies among labels. Moreover, with more contextual information, the proposed model performs well in labelling small objects that other models may not be able to capture. Different from existing spatialcontextaware models ex ploiting the context information between objects [24] or con sidering the receptive Ô¨Åeld of the shallow layers as the context information to the pixels on the deep layers of the network [25], the proposed method exploits the spatial context infor mation directly on the feature maps and utilizes the feature maps around the object as the background context. The exper imental results on two large benchmark datasets, MSCOCO and PASCAL VOC, demonstrate the effectiveness of the pro posed approach compared with stateoftheart approaches. The contributions of this paper are summarized as fol low: 1) To make use of the spatial and context information to the object, a twobranch spatialcontextaware deep neu ral network is proposed for multilabel image classiÔ¨Åcation problem. 2) The proposed imagecontextaware branch could well exploit both spatial and semantic information of objects. 3) The proposed approach signiÔ¨Åcantly outperforms the state oftheart approaches [5, 7, 10, 11, 13‚Äì15, 18, 19, 26] on the MSCOCO dataset [27] and PASCAL VOC [28] dataset. 2. METHODOLOGY "
303,Amended Cross Entropy Cost: Framework For Explicit Diversity Encouragement.txt,"Cross Entropy (CE) has an important role in machine learning and, in
particular, in neural networks. It is commonly used in neural networks as the
cost between the known distribution of the label and the Softmax/Sigmoid
output. In this paper we present a new cost function called the Amended Cross
Entropy (ACE). Its novelty lies in its affording the capability to train
multiple classifiers while explicitly controlling the diversity between them.
We derived the new cost by mathematical analysis and ""reverse engineering"" of
the way we wish the gradients to behave, and produced a tailor-made, elegant
and intuitive cost function to achieve the desired result. This process is
similar to the way that CE cost is picked as a cost function for the
Softmax/Sigmoid classifiers for obtaining linear derivatives. By choosing the
optimal diversity factor we produce an ensemble which yields better results
than the vanilla one. We demonstrate two potential usages of this outcome, and
present empirical results. Our method works for classification problems
analogously to Negative Correlation Learning (NCL) for regression problems.","It has been shown in several studies, both theoretically and empirically, that training an ensemble of models, i.e. aggregating predictions from multiple models, is superior to training a single model[ 1‚Äì 10]. Many works point out that one of the keys for an ensemble to perform well is to encourage diversity among the models [5, 10‚Äì15]. This property is the main motivation our work. Sigmoid and Softmax are both well known functions which are used for classiÔ¨Åcation (the former for binary and the second for multi label classiÔ¨Åcations). Both are used to generate distribution vectors qY(x) =fq1(x);::;qL(x)gover the labels Y=f1;::;Lg, wherexis a given input. For Deep Neural Networks (DNNs) the framework of applying a Sigmoid/Softmax on top of the network is very popular, where the goal is to estimate the real distribution pY(x) =fp1(x);::;pL(x)g, which might be a 1hot vector for a hard label. Henceforth, we omit xunless it is crucial for some deÔ¨Ånition or proof. We denote p=pY(x);q=qY(x). We optimize qby minimizing the CE cost function H(p;q) =Ep["
104,QActor: On-line Active Learning for Noisy Labeled Stream Data.txt,"Noisy labeled data is more a norm than a rarity for self-generated content
that is continuously published on the web and social media. Due to privacy
concerns and governmental regulations, such a data stream can only be stored
and used for learning purposes in a limited duration. To overcome the noise in
this on-line scenario we propose QActor which novel combines: the selection of
supposedly clean samples via quality models and actively querying an oracle for
the most informative true labels. While the former can suffer from low data
volumes of on-line scenarios, the latter is constrained by the availability and
costs of human experts. QActor swiftly combines the merits of quality models
for data filtering and oracle queries for cleaning the most informative data.
The objective of QActor is to leverage the stringent oracle budget to robustly
maximize the learning accuracy. QActor explores various strategies combining
different query allocations and uncertainty measures. A central feature of
QActor is to dynamically adjust the query limit according to the learning loss
for each data batch. We extensively evaluate different image datasets fed into
the classifier that can be standard machine learning (ML) models or deep neural
networks (DNN) with noise label ratios ranging between 30% and 80%. Our results
show that QActor can nearly match the optimal accuracy achieved using only
clean data at the cost of at most an additional 6% of ground truth data from
the oracle.","We are in the era of big data, which are continuously generated on dif ferent web platforms, e.g., social media, and disseminated via search engines often in a casual and unstructured way. Consequently, such a big data experience suffers from diversiÔ¨Åed quality issues, e.g., images tagged with incorrect labels, so called noisy labels. Today‚Äôs easy access to this large amount of data further exasperates the pres ence of extremely noisy data. According to [ 15], noisy data costs the US industry more than $3 trillion per year to cleanse or to mitigate the impact of derived incorrect analyses. While the learning models conveniently leverage such a free source of data, its quality greatly un dermines the learning efÔ¨Åciencies and their associate utilities [ 9]. For example [ 27], using the image classiÔ¨Åer trained from data with highly noisy labels can signiÔ¨Åcantly degrade the classiÔ¨Åcation accuracy and hinder its applicability on different application domains. Another challenge brought by big data is the stream of data genera tion and continuous data curation. On the one hand, this invalidates the assumptions of offline learning scenarios and drastically increases the 1TU Delft, The Netherlands, email: t.younesian@tudelft.nl 2Universit√© Grenoble Alpes, France, email: zilong.zhao@gipsalab.fr 3TU Delft, The Netherlands, email: s.ghiassi@tudelft.nl 4ABB Future Labs, Switzerland. email: robert.birke@ch.abb.com 5TU Delft, The Netherlands, email: lydiaychen@ieee.orgstorage overhead. On the other hand, due to the privacy concern and government regulation, e.g., European GDPR, data shall be closely managed, imposing a limit on using curated data from the public domains. As such, today‚Äôs machine learning models, e.g., classifying images, in reality often encounter such data that arrives in a stream of high velocity and can only be kept for a limited time. It is no mean feat to derive learning models which can cater to such a multifaced challenge, i.e., noisy stream data. Noisy label issue has been a long standing challenge [ 13], from standard machine learning (ML) models to the recent deep neural networks (DNN), whose large learning capacities can have detrimental memorization effects on dirty labels [ 27]. The central theme here is to Ô¨Ålter out the suspicious data which might have corrupted labels via quality estimates. Although such approaches show promising results in combating noisy labels, the applicability to noisy stream data is unfortunately limited, due to their assumption of offline and complete data availability. The other drawback of Ô¨Åltering approaches is the risk of dropping informative data points which can be inÔ¨Çuential for the underlying learning models. For instance, images with corrupted labels can be exactly on the class boundaries. It might be worthwhile to actively cleanse such data due to its high potential in improving the learning tasks, even at a certain expense. Active learning techniques [ 21] are designed to query extra infor mation from the oracle for the data whose (true) labels are not readily available. Such an oracle is assumed to know (uncorrected) labels. Due to the high oracle query cost, only the informative/uncertain data is queried within a certain query budget. The majority of active learn ing approaches focus on the offline scenario and constant budget, except [29] that explores the dynamic budget based on the classiÔ¨Åca tion errors on one by one drifting streaming data. Motivated by its power of cleansing data, [ 2] applies active learning techniques on support vector machines which encounter moderate noise ratios, i.e., roughly 30%. The efÔ¨Åcacy of active learning relies on the identifying the most informative instances based on uncertainty measurements of learning tasks, e.g., class probability [ 18] or entropy value [ 8]. While the related work shows promising results of active learning on noisy labels, it is not clear how active query approach can be adopted when encountering noisy data streams that can be learnt only for a short period of time. In this paper, we focus on a challenging multiclass learning prob lem whose data is streamed and its labels are extremely noisy, i.e., more than half of the given labels of streaming data are wrong. Due to the privacy concerns and limited storage capacity, the data can only be stored for a limited amount of epochs, drastically shortening the data validity for training classiÔ¨Åers. In other words, only a small fraction of data is available for learning the model at any point in time, compared to the ofÔ¨Çine scenario. Our objective is to enhancearXiv:2001.10399v1  [cs.LG]  28 Jan 2020the noiseresiliency of the underlying classiÔ¨Åer by selectively learning from good data as well as noisy labels that are critical to train the classiÔ¨Åer. In order to turn the noisy labels into a learning advantage, we resort to the oracle for recovering their label ground truth under a given query budget. Ultimately, we aim to optimize classiÔ¨Åcation accuracy with a minimum number of oracle queries, in combating stream of noisy labels. To such an end, we design an online active learning algorithm, termed Qualitydriven Active Learning (QActor ), which combines the merits of quality models and typical active learning algorithms. Upon receiving new data instances, QActor Ô¨Årst Ô¨Ålters it via the quality model into ‚Äúclean‚Äù and ‚Äúnoisy‚Äù categories. Second, QAc tor queries from the oracle the true labels of highly uncertain and informative noisy instances. Another unique feature of QActor is that the overall query budget is Ô¨Åxed but the number of queries per batch is dynamically adjusted based on the current training loss value. QActor uses more queries when the loss value increases to avoid incorrectly including noisy labels, and reduced otherwise. We exten sively evaluate QActor on an extensive set of scenarios, i.e., noise ratios, multiclass classiÔ¨Åer models, uncertain measures, and more importantly different data sets. Our results show that in the presence of very large label noise, i.e., up to 80% corrupted labels, QActor can achieve remarkable accuracy, i.e., almost match the optimal accuracy obtained excluding all noisy labels, at the cost of just a small fraction of oracle information, i.e., up to 6% oracle queried labels. Our contributions are three fold. We design a novel and efÔ¨Åcient learning framework, termed QActor , whose core is the combination of quality model and online active learning. Secondly, we propose a dynamic learning strategy that can achieve similar results as the static one. Thirdly, we extensively evaluate the proposed QActor on an extensive set of scenarios and datasets, strongly arguing for the combination of human and artiÔ¨Åcial intelligence. 2 Related Work "
361,3D Point Cloud Denoising via Deep Neural Network based Local Surface Estimation.txt,"We present a neural-network-based architecture for 3D point cloud denoising
called neural projection denoising (NPD). In our previous work, we proposed a
two-stage denoising algorithm, which first estimates reference planes and
follows by projecting noisy points to estimated reference planes. Since the
estimated reference planes are inevitably noisy, multi-projection is applied to
stabilize the denoising performance. NPD algorithm uses a neural network to
estimate reference planes for points in noisy point clouds. With more accurate
estimations of reference planes, we are able to achieve better denoising
performances with only one-time projection. To the best of our knowledge, NPD
is the first work to denoise 3D point clouds with deep learning techniques. To
conduct the experiments, we sample 40000 point clouds from the 3D data in
ShapeNet to train a network and sample 350 point clouds from the 3D data in
ModelNet10 to test. Experimental results show that our algorithm can estimate
normal vectors of points in noisy point clouds. Comparing to five competitive
methods, the proposed algorithm achieves better denoising performance and
produces much smaller variances.","The rapid development of 3D sensing techniques and the emerging Ô¨Åeld of 2D imagebased 3D reconstruction make it possible to sample or generate millions of 3D points from surfaces of objects [1‚Äì3]. 3D point clouds are discrete rep resentations of continuous surfaces and are widely used in robotics, virtual reality, and computeraided shape design. 3D point clouds sampled by 3D scanners are generally noisy due to measurement noise, especially around edges and cor ners [4]. 3D point clouds reconstructed from multiview images contain noise since the reconstructed algorithms fail to manage matching ambiguities [5, 6]. The inevitable noise in 3D point clouds undermines the performance of surfacereconstruction algorithms and impairs further geometry pro cessing tasks since the Ô¨Åne details are lost and the underlying manifold structures are prone to be deformed [7]. However, 3D point clouds denoising or processing is chal lenging because 3D point clouds are permutation invariant and the neighboring points representing the local topology interact without any explicit connecting information. To de noise 3D point clouds, we aim to estimate the continuous sur face localized around each 3D point and remove noise by pro jecting noisy points to the corresponding local surfaces. The intuition is that noiseless points are sampled from surfaces. To estimate local surfaces, we parameterize them by 2D ref erence planes. By projecting noisy points to the estimated reference planes, we ensure that all the denoised points come from the underlying surfaces. Deep neural networks have shown groundbreaking per formances in various domains, such as speech processing and image processing [8]. Recently, several deeplearning archi tectures have been proposed to deal with 3D point clouds in tasks such as classiÔ¨Åcation, segmentation, and upsam pling [9‚Äì11]. In this work, we learn reference planes from noisy point clouds and further reduce noise with deep learn ing; we name the proposed algorithm as neural projection de noising (NPD). Estimated reference planes are represented by normal vectors and interceptions. The reason for using deep learning is that previous algorithms are not robust enough to noise intensity, sampling density, and curvature variations. These algorithms need to deÔ¨Åne neighboring points to capture local structures. However, it is difÔ¨Åcult to choose the neigh boring points adaptively according to the sampling density or curvature variation. In our experiments, the point clouds used for training are sampled from the 3D dataset ShapeNet and the point clouds used for testing are sampled from the 3D dataset Mod elNet10 [12, 13]. The experimental results show that NPD outperforms four of Ô¨Åve other denoising algorithms in all seven categories and achieves the lowest variance in both evaluation metrics. Contributions. 1) To the best of our knowledge, NPD is the Ô¨Årst to directly deal with 3D point clouds for denoisarXiv:1904.04427v1  [cs.CV]  9 Apr 2019ing tasks with deep learning techniques; 2) NPD can estimate normal vector for each point with both local and global infor mation and is less affected by noise intensity and curvature variation; 3) NPD can denoise noisy point clouds without deÔ¨Åning neighboring points for noisy point clouds or calcu lating the eigendecomposition to estimate local geometries; 4) NPD provides the possibility of 3D point cloud parameter ization with the combination of local and global information. 2. RELATED WORK "
461,Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images.txt,"In the last few years, image denoising has benefited a lot from the fast
development of neural networks. However, the requirement of large amounts of
noisy-clean image pairs for supervision limits the wide use of these models.
Although there have been a few attempts in training an image denoising model
with only single noisy images, existing self-supervised denoising approaches
suffer from inefficient network training, loss of useful information, or
dependence on noise modeling. In this paper, we present a very simple yet
effective method named Neighbor2Neighbor to train an effective image denoising
model with only noisy images. Firstly, a random neighbor sub-sampler is
proposed for the generation of training image pairs. In detail, input and
target used to train a network are images sub-sampled from the same noisy
image, satisfying the requirement that paired pixels of paired images are
neighbors and have very similar appearance with each other. Secondly, a
denoising network is trained on sub-sampled training pairs generated in the
first stage, with a proposed regularizer as additional loss for better
performance. The proposed Neighbor2Neighbor framework is able to enjoy the
progress of state-of-the-art supervised denoising networks in network
architecture design. Moreover, it avoids heavy dependence on the assumption of
the noise distribution. We explain our approach from a theoretical perspective
and further validate it through extensive experiments, including synthetic
experiments with different noise distributions in sRGB space and real-world
experiments on a denoising benchmark dataset in raw-RGB space.","Image denoising is a lowlevel vision task that is funda mental in computer vision, since noise contamination de grades the visual quality of collected images and may ad versely affect subsequent image analysis and processing tasks, such as classiÔ¨Åcation and semantic segmentation [18]. *The work was done in Noah‚Äôs Ark Lab, Huawei Technologies. ‚Ä†Corresponding authorTraditional image denoising methods such as BM3D [7], NLM [4], and WNNM [11], use local or nonlocal struc tures of an input noisy image. These methods are non learningbased without the need for groundtruth images. Recently, convolutional neural networks (CNNs) provide us with powerful tools for image denoising. Numerous CNN based image denoisers, e.g., DnCNN [34], UNet [26], RED [20], MemNet [28], and SGN [10], have superior perfor mance over traditional denoisers. However, CNNbased de noisers depend heavily on a large number of noisyclean image pairs for training. Unfortunately, collecting large amounts of aligned pairwise noisyclean training data is extremely challenging and expensive in realworld photog raphy. Additionally, models trained with synthetic noisy clean image pairs degrade greatly due to the domain gap between synthetic and real noise. To mitigate this problem, a series of unsupervised and selfsupervised methods that do not require any clean im ages for training are proposed. These methods require 1) training the network with multiple independent noisy ob servations per scene [17], 2) designing speciÔ¨Åc blindspot network structures to learn selfsupervised models on only single noisy images [13, 15, 30], and making further im provements by using noise models, e.g., GaussianPoisson models [15, 30], or 3) training the network with noisier noisy pairs, where the noisier image is derived from the noisy one with synthetic noise added [31, 22]. However, these requirements are not practical in realworld denois ing scenarios. Firstly, capturing multiple noisy observa tions per scene remains very challenging, especially for motion scenarios or medical imaging. Secondly, the rel atively low accuracy and heavy computational burden of blindspot networks greatly limit the application. Moreover, selfsupervised methods with noise model assumptions may work well in synthetic experiments when the noise distribu tion is known as a prior. However, these methods degrade sharply when dealing with realworld noisy images where the noise distribution remains unknown. In this work, we propose Neighbor2Neighbor, a novel selfsupervised image denoising framework that overcomesarXiv:2101.02824v3  [eess.IV]  31 Mar 2021the limitations above. Our approach consists of a train ing image pairs generation strategy based on subsampling and a selfsupervised training scheme with a regulariza tion term. SpeciÔ¨Åcally, training input and target are gen erated by random neighbor subsamplers, where two sub sampled paired images are extracted from a single noisy image with each element on the same position of the two images being neighbors in the original noisy image. In this way, if we assume that noise with each pixel is indepen dent conditioned on its pixel value and there is no corre lation between noise in different positions, then these two subsampled paired noisy images are independent given the groundtruth of the original noisy image. Accordingly, in spired by Noise2Noise [17], we use the above training pairs to train a denoising network. Besides, we develop a reg ularization term to address the essential difference of pixel groundtruth values between neighbors on the original noisy image. The proposed selfsupervised framework aims at training denoising networks with only single images avail able, without any modiÔ¨Åcations to the network structure. Any network that performs well in supervised image de noising tasks can be used in our framework. Moreover, our method does not depend on any noise models either. To evaluate the proposed Neighbor2Neighbor, a series of experiments on both synthetic and realworld noisy images are conducted. The extensive experiments show that our Neighbor2Neighbor outperforms traditional denoisers and existing selfsupervised denoising methods learned from only single noisy images. The results demonstrate the ef fectiveness and superiority of the proposed method. The main contributions of our paper are as follows: 1. We propose a novel selfsupervised framework for im age denoising, in which any existing denoising net works can be trained without any clean targets, net work modiÔ¨Åcations, or noise model assumptions. 2. From the theoretical perspective, we provide a sound motivation for the proposed framework. 3. Our method performs very favorably against stateof theart selfsupervised denoising methods especially on realworld datasets, which shows its potential ap plications in realworld scenarios. 2. Related Work "
196,Improving Training on Noisy Stuctured Labels.txt,"Fine-grained annotations---e.g. dense image labels, image segmentation and
text tagging---are useful in many ML applications but they are labor-intensive
to generate. Moreover there are often systematic, structured errors in these
fine-grained annotations. For example, a car might be entirely unannotated in
the image, or the boundary between a car and street might only be coarsely
annotated. Standard ML training on data with such structured errors produces
models with biases and poor performance. In this work, we propose a novel
framework of Error-Correcting Networks (ECN) to address the challenge of
learning in the presence structured error in fine-grained annotations. Given a
large noisy dataset with commonly occurring structured errors, and a much
smaller dataset with more accurate annotations, ECN is able to substantially
improve the prediction of fine-grained annotations compared to standard
approaches for training on noisy data. It does so by learning to leverage the
structures in the annotations and in the noisy labels. Systematic experiments
on image segmentation and text tagging demonstrate the strong performance of
ECN in improving training on noisy structured labels.","The quality of labeled data plays a signiÔ¨Åcant role in the performance of supervised machine learning methods trained on the data ([Nettleton et al., 2010, Hendrycks and Dietterich, 2019]). However, in many settings, it may be difÔ¨Åcult to obtain highquality data, such as due to lim ited time, budget, or expertise dedicated to the annotation process. This is particularly the case for Ô¨Ånegrained an notations ([Heller et al., 2018]), which are labels that are Corresponding author: jamesz@stanford.edu.applied to individual elements of each input data and they often follow certain structures (we will use the terms label andannotation interchangeably in this paper). For examples, in computer vision, semantic segmentation models are trained on image data in which each pixel is la beled for a class (e.g. car, street) [Long et al., 2015]. Such pixellevel labels are not independent, and systematic er rors may be present in the training set and thereby learned by a supervised machine learning algorithm. In natural lan guage processing, the analogous task of nameentity recog nition can be seen as operating on Ô¨Ånegrained structured data, in which word or token is labeled with the entity that it represents [Lample et al., 2016]. Because of the level of precision needed to Ô¨Ånely annotate such structured datasets, it is very common in practice to have datasets with substantial annotation mistakes. This is the case both in widely used public datasets, but even more so in private datasets that are collected and annotated us ing customized processes. In Fig. 1, we provide image and text examples of how various types of complex errors may appear in Ô¨Ånegrain labels. Certain elements could be mis labeled to be a wrong class; entire elements (e.g. a car) could be missing an annotation; often times, the bound aries between different annotated classes (e.g. where does the street begin and sidewalk end) are imprecise. It is im portant to note that the errors in the labels have many struc tures and are not independent. For example, errors tend to locally cluster‚Äîif a whole car is missed by the labeler, then all of its pixels are misannotated. These structured er rors are challenging for standard ML training. Common approaches for training on noisy data are typically devel oped in settings where there is a simple label per data, as is the case in standard classiÔ¨Åcation and regression. They are not wellsuited for Ô¨Ånegrained predictions with structured errors within the label of each data. On the other hand, structures in the label enables us to more easily learn to correct the errors. We leverage this idea in developing the new approach of ErrorCorrecting Networks (ECN). Human label errors are difÔ¨Åcult to avoid because it is ex tremely labor intensive (and tedious) to precisely annotatearXiv:2003.03862v1  [cs.LG]  8 Mar 2020Figure 1: Typical labeling errors in structured data. Here, we show examples of errors that commonly occur in two ma chine learning tasks that operate with structured data: image segmentation (top row) and nameentity recognition (bottom row). Generating precise labels for each pixel or word is very labor intensive. In many images/text, parts of the data is misannotated to be the wrong class (left), or annotations are missing (center), or the borders of the annotations are coarse, e.g. the annotated street doesn‚Äôt reach all the way to the sidewalk (right). Errorcorrection Network (ECN) is a general framework to address these structured mistakes in the labels. and segment all of the individual elements in an image or text. In such settings, it is usually the case that a small amount of samples are known to be highquality data (ei ther by manual quality assurance checks or by dedicating additional annotation resources). In this paper, we pro pose the ECN method to leverage a small amount of the highquality datasets, which we refer to as gold data , to im prove the quality of the entire training dataset. Our method is simple to implement and intuitive, and we demonstrate that, even with a relatively small amount of gold data, we can obtain qualitative and quantitative improvements in both computer vision and natural language applications. Our contributions Structured errors in Ô¨Ånegrained la bels is a prevalent challenge. However, previous work on label noise has focused on the setting where the label is simple (e.g. a class or a value). We propose a novel and intuitive algorithm of Errorcorrection Networks (ECN), which can Ô¨Çexibly correct structured errors across diverse domains. Our experiments demonstrate that ECN can sub stantially improve performance in Ô¨Ånegrained image seg mentation/annotation and in text tagging, which are two important and widely used settings. ECN is computation ally as well as data efÔ¨Åcient‚Äîit works well even when there is only a small number of gold standard labeled samples. To the best of knowledge ECN is the Ô¨Årst Ô¨Çexible method that can correct diverse types of structured label errors.2 RELATED WORKS "
451,NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection.txt,"The labeling cost of large number of bounding boxes is one of the main
challenges for training modern object detectors. To reduce the dependence on
expensive bounding box annotations, we propose a new semi-supervised object
detection formulation, in which a few seed box level annotations and a large
scale of image level annotations are used to train the detector. We adopt a
training-mining framework, which is widely used in weakly supervised object
detection tasks. However, the mining process inherently introduces various
kinds of labelling noises: false negatives, false positives and inaccurate
boundaries, which can be harmful for training the standard object detectors
(e.g. Faster RCNN). We propose a novel NOise Tolerant Ensemble RCNN (NOTE-RCNN)
object detector to handle such noisy labels. Comparing to standard Faster RCNN,
it contains three highlights: an ensemble of two classification heads and a
distillation head to avoid overfitting on noisy labels and improve the mining
precision, masking the negative sample loss in box predictor to avoid the harm
of false negative labels, and training box regression head only on seed
annotations to eliminate the harm from inaccurate boundaries of mined bounding
boxes. We evaluate the methods on ILSVRC 2013 and MSCOCO 2017 dataset; we
observe that the detection accuracy consistently improves as we iterate between
mining and training steps, and state-of-the-art performance is achieved.","With the recent advances in deep learning, modern ob ject detectors, such as Faster RCNN [22], YOLO [21], SSD [20] and RetinaNet [18], are reliable in predicting both ob ject classes and their bounding boxes. However, the appli cation of deep learningbased detectors is still limited by the efforts of collecting bounding box training data. These detectors are trained with huge amount of manually labelled bounding boxes. In real world, each application may require Source detector Boxes | Labels Training Localization  Target detector Box predictions Mined box labels Image labels  Predict  InitUpdate Seed box labels Figure 1. Iterative trainingmining pipeline. us to detect a unique set of the categories. It‚Äôs expensive and timeconsuming to label tens of thousands of object bound ing boxes for each application. To reduce the effort of labelling bounding boxes, re searchers worked on learning object detectors with only imagelevel labels, which are substantially cheaper to an notate, or even free with image search engines; this task is called weakly supervised object detection [4, 26, 30]. Mul tiple Instance Learning (MIL) [6] based trainingmining pipeline [4, 26, 28] is widely used for this task; however, the resulting detectors perform considerably worse than the fully supervised counterparts. We believe the reasons are twofold: First, a detector learned with only imagelevel la bels often performs poorly in localization, it may focus on the object part, but not the whole object ( e.g., in Figure 2, ‚Äúcat‚Äù detector detects cat head); second, without an accurate detector, object instances cannot be mined correctly, espe cially when the scene is complicated. To address the aforementioned problems in weakly su pervised object detection, we propose a semisupervised object detection setting: learning an object detector with a limited amount of labelled bounding boxes (e.g. 10 to 20 images with fully labeled bounding boxes) as well as a large amount of imagelevel labels. SpeciÔ¨Åcally, we want to train an object detector for a set of target categories. For target categories, a small amount of seed bounding box an notations and a large amount of imagelevel annotations are 1arXiv:1812.00124v1  [cs.CV]  1 Dec 2018available for training. We also assume that a pretrained ob ject detector for source categories is available. The source and target categories do not overlap with each other. Given the wide availability of large scale object detection datasets, such as MSCOCO [19] and ILSVRC [23], this assumption is not hard to satisfy in practice. This assumption is not es sential for the formulation either. Note that our formulation is different from previous semisupervised object detection [11, 27], in which the seed bounding box annotations are not considered. The standard trainingmining pipeline [4, 26] in weakly supervised object detection iterates between the following steps: 1. Train object detector with the mined bounding boxes (the initial detector is trained with the whole images and the labels); 2. Mine the bounding boxes with the current object detector. A straightforward way to incorporate the seed bounding boxes is that we use them to train the initial object detector, mine bounding boxes with the initial ob ject detector, train a new detector with both seed and mined bounding boxes, and iterate between mining and training steps. The mining process inherently introduces various types of noise. First, mining process inevitably misses some ob jects, which are treated as negative ( i.e. background) sam ples in training phase; such false negatives are harmful for training the classiÔ¨Åcation head of object detector. Second, the boundaries of the mined bounding boxes are not precise, which is harmful for learning the box regression head of the detector. Third, the class labels of the mined boxes cannot be 100% accurate, leading to some false positives. Some vi sualization examples for the mined labels from the baseline method are shown in Figure 2. Because of these issues, we observe that the detection accuracy usually decreases as we iterate between training and mining steps if standard object detector architecture ( e.g. Faster RCNN) is employed. We propose a novel NOise Tolerant Ensemble RCNN (NOTERCNN) architecture. The NOTERCNN incorpo rates an ensemble of classiÔ¨Åcation heads for both box pre dictor ( i.e. second stage) and region proposal predictor ( i.e. Ô¨Årst stage) to increase the precision of the mined bound ing boxes, i.e., reduce false positives. SpeciÔ¨Åcally, one classiÔ¨Åcation head is only trained with seed bounding box annotations; the other head is trained with both seed and mined box annotations. The consensus of the both heads is employed to determine the conÔ¨Ådence of the classiÔ¨Åca tion. This is similar to recent work that uses ensemble for robust estimation of prediction conÔ¨Ådence [3, 2]. We also utilize the knowledge of the pretrained detector on source categories as weak teachers . SpeciÔ¨Åcally, another classiÔ¨Å cation head is added to distill knowledge [10] from a weak teacher; the distillation process acts as a regularizer to pre vent the network from overÔ¨Åtting on the noisy annotations. The NOTERCNN architecture is also designed to be ro Figure 2. Top: examples of weakly supervised object detection failure cases: poor localization; objects can‚Äôt be discovered in complicated scenes. Bottom: examples of the mined box noises using a standard faster RCNN: 1) false negatives, 2) false pos itives, 3) inaccurate box boundaries; groundtruth boxes are in black, mined boxes are in other colors. bust to false negative labels. For the classiÔ¨Åcation head in the box predictor that uses mined bounding boxes for train ing, we remove the loss of predicting negatives ( i.e. back ground) from its training loss, thus the training is not af fected by the false negatives. Finally, the regression head is only trained with the seed bounding boxes, which avoids it being affected by the inaccurate boundaries of the mined bounding boxes. We evaluated the proposed architecture on MSCOCO [19] and ILSVRC [23] datasets. The experimental results show that the proposed framework increases the precision of mined box annotations and can bring up to 40% improve ment on detection performance by iterative training. Com pared with weakly supervised detection, training with seed annotations using NOTERCNN improves the stateofthe art performance from 36.9% to 43.7%, while using standard Faster RCNN only achieves 38.7%. We also perform a large scale experiment which employs MSCOCO as seed annota tions and Open Image Dataset as imagelevel annotations. We observe the proposed method also leads to consistent performance improvement during the trainingmining pro cess. In summary, our contributions are threefold: Ô¨Årst, we propose a practical semisupervised object detection prob lem, with a limited amount of labelled bounding boxes as well as a large amount of imagelevel labels; second, we identiÔ¨Åed three detrimental types of noise that inherently exists in trainingmining framework ; third, we propose a novel NOTERCNN architecture that is robust to such noise, and achieves stateoftheart performance on bench mark datasets. 2. Related Work "
296,DivideMix: Learning with Noisy Labels as Semi-supervised Learning.txt,"Deep neural networks are known to be annotation-hungry. Numerous efforts have
been devoted to reducing the annotation cost when learning with deep networks.
Two prominent directions include learning with noisy labels and semi-supervised
learning by exploiting unlabeled data. In this work, we propose DivideMix, a
novel framework for learning with noisy labels by leveraging semi-supervised
learning techniques. In particular, DivideMix models the per-sample loss
distribution with a mixture model to dynamically divide the training data into
a labeled set with clean samples and an unlabeled set with noisy samples, and
trains the model on both the labeled and unlabeled data in a semi-supervised
manner. To avoid confirmation bias, we simultaneously train two diverged
networks where each network uses the dataset division from the other network.
During the semi-supervised training phase, we improve the MixMatch strategy by
performing label co-refinement and label co-guessing on labeled and unlabeled
samples, respectively. Experiments on multiple benchmark datasets demonstrate
substantial improvements over state-of-the-art methods. Code is available at
https://github.com/LiJunnan1992/DivideMix .","The remarkable success in training deep neural networks (DNNs) is largely attributed to the collection of large datasets with human annotated labels. However, it is extremely expensive and timeconsuming to label extensive data with highquality annotations. On the other hand, there exist alternative and inexpensive methods for mining largescale data with labels, such as querying commercial search engines (Li et al., 2017a), downloading social media images with tags (Mahajan et al., 2018), leveraging machinegenerated labels (Kuznetsova et al., 2018), or using a single annotator to label each sample (Tanno et al., 2019). These alternative methods inevitably yield samples with noisy labels . A recent study (Zhang et al., 2017) shows that DNNs can easily overÔ¨Åt to noisy labels and results in poor generalization performance. Existing methods on learning with noisy labels (LNL) primarily take a loss correction approach. Some methods estimate the noise transition matrix and use it to correct the loss function (Patrini et al., 2017; Goldberger & BenReuven, 2017). However, correctly estimating the noise transition matrix is challenging. Some methods leverage the predictions from DNNs to correct labels and modify the loss accordingly (Reed et al., 2015; Tanaka et al., 2018). These methods do not perform well under high noise ratio as the predictions from DNNs would dominate training and cause overÔ¨Åtting. To overcome this, Arazo et al. (2019) adopt MixUp (Zhang et al., 2018) augmentation. Another approach selects or reweights samples so that noisy samples contribute less to the loss (Jiang et al., 2018; Ren et al., 2018). A challenging issue is to design a reliable criteria to select clean samples. It has been shown that DNNs tend to learn simple patterns Ô¨Årst before Ô¨Åtting label noise (Arpit et al., 2017). Therefore, many methods treat samples with small loss as clean ones (Jiang et al., 2018; Arazo et al., 2019). Among those methods, Coteaching (Han et al., 2018) and Coteaching +(Yu et al., 2019) train two networks where each network selects smallloss samples in a minibatch to train the other. Another active area of research that also aims to reduce annotation cost is semisupervised learning (SSL). In SSL, the training data consists of unlabeled samples in addition to the labeled samples. SigniÔ¨Åcant progress has been made in leveraging unlabeled samples by enforcing the model to produce 1arXiv:2002.07394v1  [cs.CV]  18 Feb 2020Published as a conference paper at ICLR 2020 low entropy predictions on unlabeled data (Grandvalet & Bengio, 2004) or consistent predictions on perturbed input (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Miyato et al., 2019). Recently, Berthelot et al. (2019) propose MixMatch , which uniÔ¨Åes several dominant SSL approaches in one framework and achieves stateoftheart performance. Despite the individual advances in LNL and SSL, their connection has been underexplored. In this work, we propose DivideMix , which addresses learning with label noise in a semisupervised manner. Different from most existing LNL approaches, DivideMix discards the sample labels that are highly likely to be noisy, and leverages the noisy samples as unlabeled data to regularize the model from overÔ¨Åtting and improve generalization performance. The key contributions of this work are: We propose codivide, which trains two networks simultaneously. For each network, we dynamically Ô¨Åt a Gaussian Mixture Model (GMM) on its persample loss distribution to divide the training samples into a labeled set and an unlabeled set. The divided data is then used to train the other network. Codivide keeps the two networks diverged, so that they can Ô¨Ålter different types of error and avoid conÔ¨Årmation bias in selftraining. During SSL phase, we improve MixMatch with label coreÔ¨Ånement and coguessing to account for label noise. For labeled samples, we reÔ¨Åne their groundtruth labels using the network‚Äôs predictions guided by the GMM for the other network. For unlabeled samples, we use the ensemble of both networks to make reliable guesses for their labels. We experimentally show that DivideMix signiÔ¨Åcantly advances stateoftheart results on multiple benchmarks with different types and levels of label noise. We also provide extensive ablation study and qualitative results to examine the effect of different components. 2 R ELATED WORK "
288,In Defense of the Triplet Loss Again: Learning Robust Person Re-Identification with Fast Approximated Triplet Loss and Label Distillation.txt,"The comparative losses (typically, triplet loss) are appealing choices for
learning person re-identification (ReID) features. However, the triplet loss is
computationally much more expensive than the (practically more popular)
classification loss, limiting their wider usage in massive datasets. Moreover,
the abundance of label noise and outliers in ReID datasets may also put the
margin-based loss in jeopardy. This work addresses the above two shortcomings
of triplet loss, extending its effectiveness to large-scale ReID datasets with
potentially noisy labels. We propose a fast-approximated triplet (FAT) loss,
which provably converts the point-wise triplet loss into its upper bound form,
consisting of a point-to-set loss term plus cluster compactness regularization.
It preserves the effectiveness of triplet loss, while leading to linear
complexity to the training set size. A label distillation strategy is further
designed to learn refined soft-labels in place of the potentially noisy labels,
from only an identified subset of confident examples, through teacher-student
networks. We conduct extensive experiments on three most popular ReID
benchmarks (Market-1501, DukeMTMC-reID, and MSMT17), and demonstrate that FAT
loss with distilled labels lead to ReID features with remarkable accuracy,
efficiency, robustness, and direct transferability to unseen datasets.","Person reidentiÔ¨Åcation (ReID) has attracted tremendous attention owing to its vast applications in video surveillance, public safety, and so on. Given a person image spotted by one camera, ReID aims to accurately match that probe im age against a large amount of gallery images, taken by other cameras and timestamps. The dramatic visual appearance variations of the same person, as caused by different poses, view angles, illuminations, and backgrounds, constitute se rious challenges for learning robust identity representations. Most existing ReID algorithms use a classiÔ¨Åcation loss to train their feature learning backbones [48, 41, 42, 19, 3, 45]. (a) Triplet Loss (b) FAT Loss  Figure 1: Illustrative comparison of standard triplet loss and FAT loss. The former compares pointtopoint distances, while the lat ter compares pointtoset distances while regularizing all cluster sets to be compact. The solid arrows depict the ‚Äúpush and pull‚Äù ef fect of triplet loss and the pointtoset term of FAT loss. The dash arrows represents the compactness regularization of FAT loss. See details in Section 3. However, ReID is essentially an ‚Äúopenended‚Äù retrieval problem rather than closedset classiÔ¨Åcation, e.g., the train ing and testing sets usually have no overlapped identity classes. The learned feature extractor should be able to gen eralize to matching unseen identities. The testing perfor mance is evaluated by the precision and recall of the match ing instances, rather than classiÔ¨Åcation accuracy. Therefore, the classiÔ¨Åcationdriven learning could be misaligned with the end goal. Instead, the comparative losses [31, 7, 25, 49], which compares the distances between two sample pairs, are naturally better choices, as empirically validated by a handful of works [21, 19, 4, 40, 5]. Among many, the triplet loss [13], which maximizes the margin between the intra class distance and the interclass distance, has been mostly used in ReID, in order to explicitly embed the relative ordersarXiv:1912.07863v2  [cs.CV]  19 Dec 2019between right and wrong matches ( i.e., the correct matches should always be closer to the query than the wrong ones). However, an important downside of triplet loss lies in itscomputational expensiveness , which prohibits its wide usage in the largescale ReID applications. A naive triplet loss that compares every possible pair of training samples will incur cubic complexity w.r.t. the training set size [13]. Also, triplet loss relatively quickly learns to correctly map most trivial triplets, rendering a large fraction of all triplets uninformative. Applying triplet loss with randomly selected triplets can accelerate training but quickly stagnates, or be comes difÔ¨Åcult to converge. Hard sample mining [43, 46] has recently become the standard practice in using triplet loss, to select only ‚Äúinformative‚Äù (a.k.a. hard) pairs rather than all pairs to enforce the loss. However, it runs the risk of causing sample bias [43], and often appears fragile to outliers. The vanilla triplet loss needs to calculate over allPK(K"
376,FMT:Fusing Multi-task Convolutional Neural Network for Person Search.txt,"Person search is to detect all persons and identify the query persons from
detected persons in the image without proposals and bounding boxes, which is
different from person re-identification. In this paper, we propose a fusing
multi-task convolutional neural network(FMT-CNN) to tackle the correlation and
heterogeneity of detection and re-identification with a single convolutional
neural network. We focus on how the interplay of person detection and person
re-identification affects the overall performance. We employ person labels in
region proposal network to produce features for person re-identification and
person detection network, which can improve the accuracy of detection and
re-identification simultaneously. We also use a multiple loss to train our
re-identification network. Experiment results on CUHK-SYSU Person Search
dataset show that the performance of our proposed method is superior to
state-of-the-art approaches in both mAP and top-1.","Person search combines person detection and person reidentiÔ¨Åc ation, which detects all candidate persons in an image and then compares all pos sible pairs of the query persons to identify the target persons, which is diÔ¨Äer ent from per son reidentiÔ¨Åcation. It is a challenging and fastgrowing Ô¨Åeld. It ha s many im portant applications in video surveillance and multimedia, such as pede strian retrieval[1] and crosscamera visual tracking[2]. The recent work [3] proposed an endtoend person search model based on a single convolutiona l neural net work,which adoptsproposedOnline Instance Matching(OIM) lossf unction to 1School of Mathematical Sciences, Anhui University, Hefei 2 30601, China 2School of Computer Science and Technology, Anhui Universit y, Hefei 230601, China Email: sulanzhai@gmail.com21Sulan Zhai et al. Fig. 1: Process of person search. For each query person, we det ect possible query persons in the gallery images, and then compares all possible p airs of the query to identify the target person. train reidentiÔ¨Åcation networks, and built a largescale benchmark dataset for person search (CUHKSYSU). Person search is generally based on twostage search strategy. Firstly, we detect all candidate persons in an ga llery images. Secondly, we reidentify the query persons from the candiate per sons by mak ing a comparison between the query person and all the candidates. See Fig. 1 for demonstration. In general, a single task needs a single convolutional neural networ k to extract the speciÔ¨Åc features. Multitask learning,where a single ne twork is trained to tackle several related tasks, may learn more robust pr esentation and reduce complexity and training time. it has been successfully use d to perform the tasks, such as person reidentiÔ¨Åcation, face alignme nt. The recent work[3] introduced the multitask learning into person search by co mbining the person detection and person reidentiÔ¨Åcation. However, it on ly considers reidentiÔ¨Åcation as expansion of detection task. Detection and re identiÔ¨Åcation are not only interrelated, but also heterogeneous. They are two d iÔ¨Äerent task. Soweshouldconsidertheheterogeneityofdetectionandreident iÔ¨Åcationwhen designing the multitask CNN for person search. In this paper, we propose a fusing multitask convolutional neural network (FMTCNN) to tackle heterogeneity of detection and reidentiÔ¨Åca tion and fo cus on how the interplay of pedestrian detection and person reide ntiÔ¨Åcation aÔ¨Äects the overall performance. Person label is one kind of import ant informa tion that identiÔ¨Åes person which will help to improve the accuracy of p erson reidentiÔ¨Åcation. In a single convolutional neural network, we add person la bels into the RPN and produce more suitable features for reidentiÔ¨Å cation task while keeping the accuracy of detection unchanged. To improv e the per formance of reidentiÔ¨Åcation network, we use the multiple loss and t he more suitablefeaturestotrainreidentiÔ¨Åcationnetwork.Finally,ourpr oposedFMT CNN achieves 77.15% mAP, 79.83% top1 accuracy and 90.90% top5 a ccu racy on CUHKSYSU[3], which shows that FMTCNN can outperform s tate ofthearts in both mAP and top1 evaluation protocols. Our work s can be summarized as the following three aspects.(1) We present an eÔ¨Écien t fusing multitask convolution neural network(FMTCNN) for person sea rch. (2) We add person labels into region proposal network to tackle heteroge neity of deFMT:Fusing Multitask Convolutional Neural Network for Pe rson Search 3 tection and reidentiÔ¨Åcation. (3) We adopt the multiple loss to bette r train reidentiÔ¨Åcation network. 2 Related Works "
609,GAN-based Vertical Federated Learning for Label Protection in Binary Classification.txt,"Split learning (splitNN) has emerged as a popular strategy for addressing the
high computational costs and low modeling efficiency in Vertical Federated
Learning (VFL). However, despite its popularity, vanilla splitNN lacks
encryption protection, leaving it vulnerable to privacy leakage issues,
especially Label Leakage from Gradients (LLG). Motivated by the LLG issue
resulting from the use of labels during training, we propose the Generative
Adversarial Federated Model (GAFM), a novel method designed specifically to
enhance label privacy protection by integrating splitNN with Generative
Adversarial Networks (GANs). GAFM leverages GANs to indirectly utilize label
information by learning the label distribution rather than relying on explicit
labels, thereby mitigating LLG. GAFM also employs an additional cross-entropy
loss based on the noisy labels to further improve the prediction accuracy. Our
ablation experiment demonstrates that the combination of GAN and the
cross-entropy loss component is necessary to enable GAFM to mitigate LLG
without significantly compromising the model utility. Empirical results on
various datasets show that GAFM achieves a better and more robust trade-off
between model utility and privacy compared to all baselines across multiple
random runs. In addition, we provide experimental justification to substantiate
GAFM's superiority over splitNN, demonstrating that it offers enhanced label
protection through gradient perturbation relative to splitNN.","Federated learning trains algorithms across multiple decentralized remote devices or siloed data centers without sharing sensitive data. There are three types of federated learning depending on the data partitioning methods used: horizontal federated learning (HFL), vertical federated learning (VFL), and federated transfer learning [ 36]. VFL partitions the data vertically, where local participants have datasets with the same sample IDs but different features [ 17]. With stricter data privacy regulations like CCPA1 [ 24] and GDPR3 [ 31], VFL is a viable solution for enterpriselevel data collaborations, as it facilitates collaborative training and privacy protection. However, VFL faces challenges in terms of high memory costs and processing time overheads, attributed to the complex cryptographic operations employed to provide strong privacy guarantees [ 38], including additive homomorphic encryption [ 17] and secure multiparty computation [ 22], which are computationally intensive. To address these issues, split learning [ 14,30,2] has emerged as an efÔ¨Åcient solution, allowing multiple participants to jointly train federated models without encrypting intermediate results, thus reducing computational costs. SplitNN [ 6], which applies the concept of split learning to neural networks, has been used successfully in the analysis of medical data [26, 15]. Split learning, while reducing computational costs, poses substantial privacy risks due to the absence of encryption protection for model privacy. One speciÔ¨Åc privacy risk is Label Leakage from Gradients (LLG) [ 33], in which gradients Ô¨Çowing from the label party to the nonlabel (only data) party can expose the label information [ 10,40,32]. LLG is susceptible to exploitation for stealing label information in binary classiÔ¨Åcation and has limited proposed solutions to address it. Recent work by Li et al. [ 21] indicates that in binary classiÔ¨Åcation, the gradient norm of positive instancesarXiv:2302.02245v2  [cs.LG]  17 May 2023APREPRINT  M AY18, 2023 is generally larger than negative ones , which could potentially enable attackers to easily infer sample labels from intermediate gradients in splitNN. Despite the fact that binary classiÔ¨Åcation is widely used in various federated scenarios, such as healthcare, Ô¨Ånance, credit risk, and smart cities [ 9,5,8,39], and is vulnerable to LLG, limited research has been conducted on addressing the LLG issue in binary classiÔ¨Åcation. Previous studies [ 6,10,29,25] have focused mainly on securing the data information of nonlabel parties, while ignoring the risk of leaking highly sensitive label information of the label party. Therefore, it is critical to address how splitNN can resist LLG in binary classiÔ¨Åcation tasks. In this work, in order to prevent inferring sample labels from the gradient calculation, we introduce a novel Generative Adversarial Federated Model (GAFM), which synergistically combines the vanilla splitNN architecture with Generative Adversarial Networks (GANs) to indirectly incorporate labels into the model training process. SpeciÔ¨Åcally, the GAN discriminator within GAFM allows federated participants to learn a prediction distribution that closely aligns with the label distribution, effectively circumventing the direct use of labels inherent in the vanilla SplitNN approach. Moreover, to counteract the potential degradation of model utility induced by GANs, we enhance our method by incorporating additional label information via an additional crossentropy loss, which encourages the intermediate results generated by the nonlabel party and the labels with added noise provided by the label party to perform similarly. The entire framework of the proposed GAFM and the training procedures is displayed in Figure 1. Our contributions are highlighted as follows. ‚Ä¢We propose a novel GANbased approach, called GAFM, which combines vanilla splitNN with GAN to mitigate LLG in binary classiÔ¨Åcation (section 3.2). Our analysis in section 3.4 demonstrates that GAFM protects label privacy by generating more mixed intermediate gradients through the mutual gradient perturbation of both the GAN and crossentropy components. ‚Ä¢We enrich the existing gradientbased label stealing attacks by identifying two additional simple yet practical attack methods, namely mean attack and median attack in section 3.5. The experimental results in section 4.2.1 demonstrate that our new attacks are more effective in inferring labels than the existing ones. ‚Ä¢We evaluate the effectiveness of GAFM on various datasets. Empirical results in section 4 show that GAFM mitigates LLG without signiÔ¨Åcant model utility degradation and the performance of GAFM across different random seeds is more stable compared to baselines. We also provide additional insights based on the ablation experiment (section 4.2.2) to demonstrate the necessity of combining both the GAN and crossentropy components in GAFM. Compared to the stable balance between utility and privacy that can be achieved by using both components, GAFM with only the GAN component provides enhanced privacy protection at the cost of reduced utility, while GAFM with only the crossentropy component delivers superior utility but offers limited privacy protection. 2 Related Work "
84,Neighbour Consistency Guided Pseudo-Label Refinement for Unsupervised Person Re-Identification.txt,"Unsupervised person re-identification (ReID) aims at learning discriminative
identity features for person retrieval without any annotations. Recent advances
accomplish this task by leveraging clustering-based pseudo labels, but these
pseudo labels are inevitably noisy which deteriorate model performance. In this
paper, we propose a Neighbour Consistency guided Pseudo Label Refinement
(NCPLR) framework, which can be regarded as a transductive form of label
propagation under the assumption that the prediction of each example should be
similar to its nearest neighbours'. Specifically, the refined label for each
training instance can be obtained by the original clustering result and a
weighted ensemble of its neighbours' predictions, with weights determined
according to their similarities in the feature space. In addition, we consider
the clustering-based unsupervised person ReID as a label-noise learning
problem. Then, we proposed an explicit neighbour consistency regularization to
reduce model susceptibility to over-fitting while improving the training
stability. The NCPLR method is simple yet effective, and can be seamlessly
integrated into existing clustering-based unsupervised algorithms. Extensive
experimental results on five ReID datasets demonstrate the effectiveness of the
proposed method, and showing superior performance to state-of-the-art methods
by a large margin.","Person reidentiÔ¨Åcation (ReID) aims to train a deep model capable of retrieving a person of interest across mul tiple cameras. This task has attracted increasing atten tion, due to its great application in video surveillance sys tem. Although supervised methods have achieved impres sive performances, they require to annotate large amount of crosscamera labels of the surveillance data, which is labor intensive, costly, and eventually leads to limited practical *Equal contribution ‚Ä†Corresponding author.application in realworld scenarios. Therefore, developing effective unsupervised methods for person retrieval from unlabeled data is very appealing and important, not only in academic sector but also for the industrial Ô¨Åelds. Existing stateoftheart unsupervised learning (USL) ReID methods leverage the pseudolabels obtained from unsupervised clustering [9, 15] or knearest neighbor search [28,40] to train the deep model. The training scheme of these methods usually alternates between the following two steps: 1) Generating pseudolabels for the training ex amples through some clusteringbased methods, e.g., DB SCAN [12]; 2) Optimizing the deep neural network with these pseudo labels in a supervised manner by some met ric learning objectives, such as triplet loss [4], InfoNCE loss [16]. Although these pseudolabelbased methods have achieved remarkable performances, there still contains a large performance gap between the purely USL methods and supervised learning methods. The rationale behind this is that the generated pseudo labels inevitably contain a portion of noise, which could signiÔ¨Åcantly deteriorate the model performance due to the model memorization/over Ô¨Åtting to the noisy labels [5, 8]. Therefore, how to mitigate the sideeffects of the inaccurate clustering results automat ically becomes the key issue for these clusteringbased un supervised methods. To address this problem, we propose a neighbour con sistency guided pseudolabel reÔ¨Ånement method (NCPLR) for USL person ReID. Although recent advances in pseudo label reÔ¨Ånement could reduce the label noise to a certain ex tent, these methods usually adopt multiple predictions from auxiliary backbone networks for mutual conÔ¨Årmation of the estimated pseudolabels [14,52], or employ some additional information (e.g., partbased reÔ¨Ånement [8]) to improve the quality of the pseudo labels, resulting in high computation costs during model training. Our proposed NCPLR algo rithm is simple yet effective, just through assembling the predictions of its nearest neighbours to reÔ¨Åne the pseudo la bel. It can be seamlessly integrated into existing clustering based USL methods. Even for learning with noisy labels, although neighbours have often been used to identify mis labeled examples [20], few works have considered the usearXiv:2211.16847v1  [cs.CV]  30 Nov 2022of neighbours to generate or reÔ¨Åne the pseudo labels. The proposed NCPLR method is inspired by the label propagation algorithms [19, 23, 58], which try to transfer labels from the supervised instances to their neighbouring unsupervised examples based on the similarities in the fea ture space. It seeks to transfer labels from its neighbour ing instances, and encourage each example to have similar predictions to its neighbours‚Äô. As we know, existing meth ods for label propagation represent the transductive learn ing, where they could produce labels for the given exam ples during model training. NCPLR can also be regarded as an transductive form of label propagation for pseudolabel reÔ¨Ånement. SpeciÔ¨Åcally, the reÔ¨Åned label for each training example can be obtained by the original clustering result and a weighted combination of its neighbours‚Äô predictions, with weights determined according to their similarities in the feature space. The motivation behind this is that the NCPLR is to enable the incorrect labels among the gener ated pseudo labels to be improved or at least attenuated by the labels of their neighbours [20], relying on the moderate assumption that the prediction of each example should be similar to its nearest neighbours‚Äô. In addition, we consider the USL person ReID as part of the labelnoise learning problem. For labelnoise learning, the key challenge is that the deep model could easily mem orize and overÔ¨Åt to the noisy labels during model train ing, which leads to severe performance degradation. There fore, effective measures should also be taken to address the overÔ¨Åtting problem. In term of this issue, we further lever age the explicit neighbour consistency regularization to en courage each example to have similar predictions as their neighbours‚Äô, and penalize the divergence of each example‚Äôs prediction from a weighted combination of its neighbours‚Äô predictions. Since the similarity graph is computed in the feature space instead of directly using their predictions, the neighbour consistency regularization can be seen as boot strapping the learned feature representations. This could be more helpful to reduce the susceptibility to overÔ¨Åtting and improve the training stability, as the last fully connected classiÔ¨Åcation layers are more prone to memorize/overÔ¨Åt to the noisy labels [29]. The main contributions of this paper are as follows, ‚Ä¢ We propose a neighbour consistency guided pseudo label reÔ¨Ånement algorithm for USL person ReID, which reÔ¨Åne the pseudo label through a weighted com bination of its neighbours‚Äô predictions, with weights determined by their similarities in the feature space. ‚Ä¢ We consider the clusteringbased USL person ReID task as the labelnoise learning problem, and we fur ther leverage the explicit neighbour consistency regu larization to reduce the model susceptibility to over Ô¨Åtting and improve the training stability.‚Ä¢ The proposed NCPLR algorithm is simple yet effec tive, and can be seamlessly integrated into existing clusteringbased unsupervised methods. Besides, ex tensive experimental results demonstrate the effective ness of the proposed method, and the performances are superior to stateoftheart methods by a large margin. 2. Related Work "
347,No Regret Sample Selection with Noisy Labels.txt,"Deep neural networks (DNNs) suffer from noisy-labeled data because of the
risk of overfitting. To avoid the risk, in this paper, we propose a novel DNN
training method with sample selection based on adaptive k-set selection, which
selects k (< n) clean sample candidates from the whole n noisy training samples
at each epoch. It has a strong advantage of guaranteeing the performance of the
selection theoretically. Roughly speaking, a regret, which is defined by the
difference between the actual selection and the best selection, of the proposed
method is theoretically bounded, even though the best selection is unknown
until the end of all epochs. The experimental results on multiple noisy-labeled
datasets demonstrate that our sample selection strategy works effectively in
the DNN training; in fact, the proposed method achieved the best or the
second-best performance among state-of-the-art methods, while requiring a
significantly lower computational cost. The code is available at
https://github.com/songheony/TAkS.","Deep neural networks (DNNs) require a large number of ‚Äúcorrectlylabeled‚Äù samples to achieve highperformance classiÔ¨Åcation [43]. However, it is practically difÔ¨Åcult for many CV/PRrelated datasets to guarantee the correctness of the attached labels [6, 29]. For example, datasets an notated via crowdsourcing [37, 42] often become noisy labeled samples, which contain a certain number of sam ples with incorrect labels. Beyer et al. [2] point out that Im ageNet contains several incorrectly labeled samples, even though each sample in ImageNet is labeled by a careful majority voting scheme among at least 10 workers [5]. Datasets created by automatic data collection and annota tion, such as Clothing1M [36], will also contain a large number of incorrectly labeled samples. A possible remedy for noisylabeled samples is sample selection , which is a method to select a clean subset of train ing samples (i.e., correctly labeled training samples) from the whole sample set. A typical strategy is cotraining, which compares the results from two DNNs to detect theincorrectly labeled samples, as shown in Fig. 1 (a). This strategy has achieved stateoftheart performance so far but requires extra computations due to its coupledDNN struc ture. Another drawback is that it has no theoretical guaran tee on its performance. We can consider another sample selection strategy by using the idea of kset, which is a subset of ksamples. Fig. 1 (b) shows the most naive ksetbased sample selec tion strategy; after generating all"
174,Learning advisor networks for noisy image classification.txt,"In this paper, we introduced the novel concept of advisor network to address
the problem of noisy labels in image classification. Deep neural networks (DNN)
are prone to performance reduction and overfitting problems on training data
with noisy annotations. Weighting loss methods aim to mitigate the influence of
noisy labels during the training, completely removing their contribution. This
discarding process prevents DNNs from learning wrong associations between
images and their correct labels but reduces the amount of data used, especially
when most of the samples have noisy labels. Differently, our method weighs the
feature extracted directly from the classifier without altering the loss value
of each data. The advisor helps to focus only on some part of the information
present in mislabeled examples, allowing the classifier to leverage that data
as well. We trained it with a meta-learning strategy so that it can adapt
throughout the training of the main model. We tested our method on CIFAR10 and
CIFAR100 with synthetic noise, and on Clothing1M which contains real-world
noise, reporting state-of-the-art results.","Modern image classication systems are based on using deep neural network models that are trained on a huge number of labeled images [11]. Due to the extreme cost of labeling such an amount of images and diculty in covering many concepts, researchers recently have looked into methods that generate labels automatically. One signicant line of research exploits available labeled images from nonexperts (e.g. from social networks, online stores) that can be easily retrieved in large quantities but may have been mislabeled [1]. Deep neural networks typically consist of a large number of parameters that are highly shared among feature dimensions and states, enabling  exibility in learning dierent tasks and classes. This  exibility has the advantage to lead to strong discriminative models unless data annotations are corrupted by noise, leading to performance reduction and overtting problems [9]. Recent methods tried to address the problem by using curriculum learning [4], directly estimatingarXiv:2211.04177v1  [cs.CV]  8 Nov 20222 S. Ricci et al. the labels noise in the set [8], or measuring the condence of the network during training [12], also using another cotrained network [7]. The idea was usually to understand mislabeled samples out of distribution and reduce their in uence on the learning by dampening their loss or decreasing their impact directly from the training set. In this paper, we proposed a metalearning approach to address the problem of noisy labels in image classication based on an advisor network, developed to help the classier. While a standard image classication model is trained, the advisor network observes the main network activations and adjusts features at training time when noisy label images are identied as input. This allows the classier model to get information even from mislabeled samples where some noise structure is present. We only retained the main model as the nal classi er, while the advisor was discarded. Unlike the teacherstudent paradigm, the advisor network was not trained to solve the image classication task, but only to help the learning process of the classier model by its altering activations. In summary, our contribution is: {We propose the use of an advisor network, i.e. the use of an additional net work at training time, learned by metalearning, that can adjust activations and gradient of the main network that is being trained. {We develop such concept for the task of image classication, allowing the training of an image classication network in presence of articial label noise. {We test our approach in presence of articial label noise and on a popular noisy dataset, obtaining stateoftheart performance. 2 Related works "
428,Random Vector Functional Link Neural Network based Ensemble Deep Learning.txt,"In this paper, we propose a deep learning framework based on randomized
neural network. In particular, inspired by the principles of Random Vector
Functional Link (RVFL) network, we present a deep RVFL network (dRVFL) with
stacked layers. The parameters of the hidden layers of the dRVFL are randomly
generated within a suitable range and kept fixed while the output weights are
computed using the closed form solution as in a standard RVFL network. We also
propose an ensemble deep network (edRVFL) that can be regarded as a marriage of
ensemble learning with deep learning. Unlike traditional ensembling approaches
that require training several models independently from scratch, edRVFL is
obtained by training a single dRVFL network once. Both dRVFL and edRVFL
frameworks are generic and can be used with any RVFL variant. To illustrate
this, we integrate the deep learning networks with a recently proposed
sparse-pretrained RVFL (SP-RVFL). Extensive experiments on benchmark datasets
from diverse domains show the superior performance of our proposed deep RVFL
networks.","Deep Learning, also known as representational learning, has sparked a surg ing interest in neural networks amongst the machine learning enthusiasts with the stateoftheart results in diverse applications ranging from image/video classication to segmentation, action recognition and many others. The supe riority of a deep learning model emanates from its potential ability to extract meaningful representations at dierent levels of the hierarchical model while disentangling a complex task into several simpler ones [1]. Deep neural networks typically consist of multiple hidden layers stacked together. Each hidden layer builds an internal representation of the data with the hidden layers closer to the input layer learning simple features such as edges and layers above them learning sophisticated (complex) features [1, 2]. With such stacked layers, deep learning models typically have thousands of model parameters that need to be optimized during the training phase. These networks are typically trained using backpropagation (BP) technique so as to minimize the loss function (crossentropy or mean square error or others depending on the particular task). In addition to be timeconsuming, such models may fail to converge to a global minimum, thus, giving suboptimal performance or lower generalization [3]. Also, such deep learning models require large amount of training data. While the usual image and speech datasets that are commonly used with deep learning models have abundant data, there are datasets from a wide variety of domains, such as agriculture, credit scoring, health outcomes, ecology and others, with very limited data size. The performance of the state oftheart deep learning models on such datasets are far from superior [4]. Apart from the conventional BPtrained neural networks, there has also been a growing interest in the class of randomization based neural networks [5, 6, 7]. Randomization based neural networks with closed form solution avoid the pit falls of conventional BPtrained neural networks [3, 8, 9]. They are faster to train and have demonstrated good learning performance [10, 11]. Among the random ization based methods, Random Vector Functional Link (RVFL) [12] network 2has rapidly gained signicant traction because of its superior performance in several diverse domains ranging from visual tracking [13], classication [14, 15], regression [16], to forecasting [17, 18]. RVFL is a single layer feedforward neu ral network (SLFN) in which the weights and biases of the hidden neurons are randomly generated within a suitable range and kept xed while the output weights are computed via a simple closed form solution [12, 19]. Randomization based neural networks greatly benet from the presence of direct links from the input layer to the output layer as in RVFL network [16, 18, 20]. The original features are reused or propagated to the output layer via the direct links. The direct links act as a regularization for the randomization [21, 22]. It also helps to keep the model complexity low with the RVFL network being thinner and simpler compared to its other counterparts. With the Occam's Razor principle and PAC learning theory [23] advocating for simpler and less complex mod els, this makes the RVFL network attractive to use compared to other similar randomized neural networks. Ensembles of neural networks are known to be much more robust and ac curate than individual networks [20, 24, 25, 26]. Because of the existence of several randomization operations in their training procedure, neural networks are regarded as unstable algorithms whose performance greatly vary even when there is a small perturbation in training set or random seed. It is therefore not surprising that two neural networks with identical architectures optimized with dierent initialization or slightly perturbed training data will converge to dif ferent solutions. This diversity can be exploited through ensembling, in which multiple neural networks are trained with slightly dierent training set or pa rameters and then combined with majority voting or averaging. Ensembling often leads to drastic reductions in error rates. However, this comes with an ob vious trade o: computational cost. While ensembling shallow neural networks doesn't incur great computational cost, the same is not true for the ensembling of deep networks. With the current trend of building deep networks, there have also been sev eral attempts in the literature to build deep or multilayer networks based on 3randomized neural networks [27, 28, 29]. Even though there exist several deep learning models with randomized neural networks, there are limited works in the context of RVFL network. In this paper, we investigate the performance of deep learning and ensemble deep learning models based on RVFL networks. To the best of our knowledge, [28] is one of the pioneering paper to propose multilayer RVFL network. However, the performance of the multilayer RVFL network compared to a shallow RVFL network (with 1 hidden layer) is suboptimal and nonpersuasive. A deep model enriched with complex feature learning capa bilities should achieve good generalization. Thus, in this paper, we propose deep neural networks based on RVFL while maintaining its advantages of lower complexity, training eciency and good generalization. We also propose an en semble of such deep networks without incurring any signicant training costs. Specically, we propose an ensemble deep RVFL network which can be regarded as a marriage of ensemble and deep learning that is simple and straightforward to implement. The key contributions of this paper are summarized as follows: ‚Ä¢We propose a deep RVFL network (dRVFL), an extension of RVFL for representational learning. The dRVFL network consists of several hidden layers stacked on top of each other. The parameters of the hidden lay ers are randomly generated and kept xed while only the output weights need to be computed. Thus, the deep RVFL network emanates from the standard RVFL network. ‚Ä¢We also propose an implicit ensembling approach called ensemble deep RVFL framework (edRVFL), a marriage of ensembling learning with deep learning. Instead of training Lneural networks independently from scratch as in traditional ensembling method, we only train a single deep RVFL network. The ensemble consists of Lmodels equivalent to the number of hidden layers in the single deep RVFL network. The ensemble is trained in such a way that the higher models (equivalent to higher layers in deep RVFL network) utilize both the original features (from direct links as in standard RVFL network) and nonlinearly transformed features from the 4preceding layers. Thus, the framework is consistent with the tenets of both ensemble learning and deep learning at the same time. The training cost of edRVFL is slightly higher than that of a single dRVFL network while it is signicantly lower than that of traditional ensembles. ‚Ä¢The deep learning models proposed in this paper (dRVFL and edRVFL) are generic and are applicable with any RVFL variant. We create deep learning models using both standard RVFL and recently proposed sparse pretrained RVFL (SPRVFL) [30]. ‚Ä¢With extensive experiments on several realworld classication datasets, we show that our proposed deep RVFL models (dRVFL and edRVFL) have superior performance compared to other relevant neural networks. The rest of this paper is structured as follows. Section 2 gives a brief overview of related works on shallow randomized neural networks followed by random ization based multilayer neural network. Section 3 details our proposed deep RVFL method followed by its ensemble. In Section 4, we compare the perfor mance of our proposed methods with other relevant neural networks. Finally, the conclusion is presented in Section 5. 2. Related Works "
123,Improving Label Quality by Jointly Modeling Items and Annotators.txt,"We propose a fully Bayesian framework for learning ground truth labels from
noisy annotators.
  Our framework ensures scalability by factoring a generative, Bayesian soft
clustering model over label distributions into the classic David and Skene
joint annotator-data model. Earlier research along these lines has neither
fully incorporated label distributions nor explored clustering by annotators
only or data only. Our framework incorporates all of these properties as:
  (1) a graphical model designed to provide better ground truth estimates of
annotator responses as input to \emph{any} black box supervised learning
algorithm, and
  (2) a standalone neural model whose internal structure captures many of the
properties of the graphical model.
  We conduct supervised learning experiments using both models and compare them
to the performance of one baseline and a state-of-the-art model.","The recent interest in few and zeroshot learning and the reemergence of weakly supervised learning speaks to the reality that ground truth labels are a limited resource and that, in many common situations, obtaining them remains a major challenge. Multiple sources estimate the the global costs of human annotators (only one of many sources of labels) to be approaching $1‚Äì3 billion by 2026 and growing [ Met19 ,Res20 ]. Among the costdriving challenges is the noise associated with many of the most common processes for obtaining labels. In this paper, we explore novel graphical and neural models that tie together two rather successful approaches, itemannotators tableaus [ DS79 ], and label distribution learning (LDL) [ Gen16 ], based on converging studies in later research [ VGK+14,LVBH19 ] on the use of clustering to boost the signal of noisy data. We adopt a theoretical framework motivated by the anthropologist Malinowski [ Mal67 ] and Ô¨Årst used by Aroyo and Welty [ AW14 ] in the context of machine learning to characterize meaning as a function of three components: 1) an act (represented by the learning task), 2) the symbols (the labels), and, 3) the referent (the annotators). Human labeling is a special challenge not only due to its great expense but also due to the fact that humans often disagree over the labels that they provide. In fact, it is precisely the problems where disagreement is most common that human input is hardest to replace through automation or sensing. This paper speciÔ¨Åcally addresses the following research questions: RQ1: Do predictive graphical models for LDL that cluster on both item AND annotator distributions outperform those that do not? RQ2: Do predictive neural models for LDL outperform graphical models?arXiv:2106.10600v1  [cs.AI]  20 Jun 2021Preprint, Work in Progress RQ3: Do predictive neural models for LDL that cluster on both item AND annotator distributions outperform those that do not? To help us answer these questions, we contribute two new models. The Ô¨Årst is a generative graphical model that boosts conventional label distribution learning by clustering label distributions jointly in item and annotator label distribution spaces. Previous approaches have studied clustering in one space or the other. This is, to our knowledge, the Ô¨Årst time that clustering has been applied simultaneously to both. Our second model is a neuralbased adaptation of the graphical model. While the graphical model has a sound theoretical foundation, it is somewhat unwieldy from a computational perspective. The neural model sacriÔ¨Åces some rigor for more Ô¨Çexibility and algorithmic efÔ¨Åciency. 2 Related Work "
343,Robust Inference via Generative Classifiers for Handling Noisy Labels.txt,"Large-scale datasets may contain significant proportions of noisy (incorrect)
class labels, and it is well-known that modern deep neural networks (DNNs)
poorly generalize from such noisy training datasets. To mitigate the issue, we
propose a novel inference method, termed Robust Generative classifier (RoG),
applicable to any discriminative (e.g., softmax) neural classifier pre-trained
on noisy datasets. In particular, we induce a generative classifier on top of
hidden feature spaces of the pre-trained DNNs, for obtaining a more robust
decision boundary. By estimating the parameters of generative classifier using
the minimum covariance determinant estimator, we significantly improve the
classification accuracy with neither re-training of the deep model nor changing
its architectures. With the assumption of Gaussian distribution for features,
we prove that RoG generalizes better than baselines under noisy labels.
Finally, we propose the ensemble version of RoG to improve its performance by
investigating the layer-wise characteristics of DNNs. Our extensive
experimental results demonstrate the superiority of RoG given different
learning models optimized by several training techniques to handle diverse
scenarios of noisy labels.","Deep neural networks (DNNs) tend to generalize well when they are trained on largescale datasets with groundtruth label annotations. For example, DNNs have achieved state oftheart performance on many classiÔ¨Åcation tasks, e.g., image classiÔ¨Åcation (He et al., 2016), object detection (Gir shick, 2015), and speech recognition (Amodei et al., 2016). However, as the scale of training dataset increases, it be comes infeasible to obtain all groundtruth class labels from domain experts. A common practice is collecting the class labels from data mining on social media (Mahajan et al., 1KAIST2University of Michingan Ann Arbor3Google Brain 4University of Illinois at Urbana Champaign5AItrics. Correspon dence to: Kimin Lee <kiminlee@kaist.ac.kr >. Proceedings of the 36thInternational Conference on Machine Learning , Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).2018) or web data (Krause et al., 2016). Machinegenerated labels are often used; e.g., the Open Images Dataset V4 con tains such 70 million labels for training images (Kuznetsova et al., 2018). However, they may contain incorrect labels, and recent studies have shown that modern deep architec tures may generalize poorly from the noisy datasets (Zhang et al., 2017) (e.g., see the black line of Figure 1(a)). To address the poor generalization issue of DNNs with noisy labels, many training strategies have been investigated (Reed et al., 2014; Patrini et al., 2017; Ma et al., 2018; Han et al., 2018b; Hendrycks et al., 2018; Goldberger & BenReuven, 2017; Jiang et al., 2018; Ren et al., 2018; Zhang & Sabuncu, 2018; Malach & ShalevShwartz, 2017; Han et al., 2018a). However, using such training methods may incurs expensive backandforth costs (e.g., additional time and hyperparam eter tuning) and suffer from the reproducibility issue. This motivates our approach of developing a more plausible in ference method applicable to any pretrained deep model. Hence, our direction is complementary to the prior works: one can combine ours and a prior training method for the best performance (see Tables 3, 4, & 5 in Section 4). The key contribution of our work is to develop such an infer ence method, Robust Generative classiÔ¨Åer (RoG) , which is applicable to any discriminative (e.g., softmax) neural classi Ô¨Åer pretrained on noisy datasets (without retraining). Our main idea is inducing a better posterior distribution from the pretrained (noisy, though) feature representation by uti lizing a robust generative classiÔ¨Åer. Here, our belief is that the softmax DNNs can learn meaningful feature patterns shared by multiple training examples even under datasets with noisy labels, e.g., see (Arpit et al., 2017). To motivate our approach, we Ô¨Årst observe that training samples with noisy labels (red circles) are distributed like outliers when their hidden features are projected in a 2 dimensional space using tSNE (Maaten & Hinton, 2008) (see Figure 1(b)). In other words, this phenomena implies that DNN representations even when trained with noisy la bels may still exhibit clustering properties (i.e., the DNN learns embedding that tend to group clean examples of the same class into the clusters while pushing away the exam ples with corrupt labels outside these clusters). The obser vation inspires us to induce a generative classiÔ¨Åer on the pretrained hidden features since it can model joint data distributions P(x;y)for inputxand its label yfor outlierarXiv:1901.11300v2  [stat.ML]  13 May 2019Robust Inference via Generative ClassiÔ¨Åers for Handling Noisy Labels Softmax Generative (sample mean on noisy labels) Generative (MCD on noisy labels) Generative (MCD on noisy labels) + ensemble Generative (sample mean on clean labels) [Oracle]Test set accuracy (%)405060708090100 Noise fraction0 0.2 0.4 0.6 (a) Test set accuracy comparison Samples with clean labels Samples with noisy labels (b) Penultimate features by tSNE MCD estimatorStandard sample estimator (c) An illustration of the MCD estimator Figure 1. Experimental results under DenseNet100 and CIFAR10 with uniform noise, i.e., the labels of a given proportion of training samples are Ô¨Çipped to other labels uniformly at random. (a) Test set accuracy of softmax and generative classiÔ¨Åers with various parameter estimations. (b) Visualization of features on the penultimate layer using tSNE from training samples when the noise fraction is 20%. (c) An illustration of the MCD estimator: it is more robust against outliers by Ô¨Ånding a subset with minimum covariance determinant. detection and thus produce robust posterior P(yjx)for pre diction. Here, one may suggest to train a deep generative classiÔ¨Åer from scratch. However, such a fully generative approach is expensive and has been not popular for recent stateofart classiÔ¨Åcation. We instead postprocess a light generative classiÔ¨Åer only for inference. In particular, we propose to induce the generative classiÔ¨Åer under linear discriminant analysis (LDA) assumption and choose its parameters by the minimum covariance determi nant (MCD) (Rousseeuw, 1984) estimator which calculates more robust parameters. We provide a theoretical support on the generalization property (Durrant & Kab ¬¥an, 2010) of RoG based on MCD: it has the smaller errors on parame ter estimations provably under some Gaussian assumptions. To improve RoG further, we observe that RoG built from lowlevel features can be often more effective since DNNs tend to have similar hidden features, regardless of whether they are trained with clean or noisy labels at early layers (Arpit et al., 2017; Morcos et al., 2018). Under the obser vations, we Ô¨Ånally propose an ensemble version of RoG to incorporate all effects of low and high layers. We demonstrate the effectiveness of RoG using modern neural architectures on image classiÔ¨Åcation and natural lan guage processing tasks. In all tested cases, our methods (e.g., see green and blue lines in Figure 1(a)) signiÔ¨Åcantly outperform the softmax classiÔ¨Åer, although they use the same feature representations trained by the noisy dataset. In particular, we show that RoG can be used to further improve various prior training methods (Reed et al., 2014; Patrini et al., 2017; Ma et al., 2018; Han et al., 2018b; Hendrycks et al., 2018) which are specialized to handle the noisy en vironment. For example, we improve the test accuracy of the stateoftheart training method (Han et al., 2018b) on CIFAR100 dataset with 45% noisy labels from 33.34% to 43.02%. Finally, RoG is shown to be working properly against more semantic noisy labels (generated from a ma chine labeler) and openset noisy labels (Wang et al., 2018).2. Related work "
336,Iterative fully convolutional neural networks for automatic vertebra segmentation and identification.txt,"Precise segmentation and anatomical identification of the vertebrae provides
the basis for automatic analysis of the spine, such as detection of vertebral
compression fractures or other abnormalities. Most dedicated spine CT and MR
scans as well as scans of the chest, abdomen or neck cover only part of the
spine. Segmentation and identification should therefore not rely on the
visibility of certain vertebrae or a certain number of vertebrae. We propose an
iterative instance segmentation approach that uses a fully convolutional neural
network to segment and label vertebrae one after the other, independently of
the number of visible vertebrae. This instance-by-instance segmentation is
enabled by combining the network with a memory component that retains
information about already segmented vertebrae. The network iteratively analyzes
image patches, using information from both image and memory to search for the
next vertebra. To efficiently traverse the image, we include the prior
knowledge that the vertebrae are always located next to each other, which is
used to follow the vertebral column. This method was evaluated with five
diverse datasets, including multiple modalities (CT and MR), various fields of
view and coverages of different sections of the spine, and a particularly
challenging set of low-dose chest CT scans. The proposed iterative segmentation
method compares favorably with state-of-the-art methods and is fast, flexible
and generalizable.","Segmentation and identiÔ¨Åcation of the vertebrae is often a prerequisite for automatic analysis of the spine, such as detection of vertebral fractures (Yao et al., 2012), assessment of spinal deformities (Forsberg et al., 2013), or computerassisted surgical interventions (Knez et al., 2016). Automatic spine analysis can be performed with a large variety of tomographic scans, includ ing dedicated spine scans but also scans of the neck, chest or abdomen that incidentally cover part of the spine. A generic vertebra segmentation algorithm therefore needs to be robust with respect to di erent image resolutions and di erent cover ages of the spine. This especially means that no assumptions should be made about the number of visible vertebrae and their anatomical identity, i.e., to which section of the spine they be long. Vertebra segmentation is therefore essentially an instance segmentation problem with an a priori unknown number of in stances ( i.e.vertebrae). However, in contrast to generic instance segmentation the individual instances are not independent of each other. The instances are known to be located in close prox imity to each other in the image, forming together the vertebral column. We propose to approach vertebra segmentation with aninstance segmentation algorithm that explicitly incorporates this prior knowledge to locate instances, but that makes no further assumptions. Approaching vertebra segmentation as an instance segmen tation problem entails treating all vertebrae as instances of the same class of objects. However, an anatomical identiÔ¨Åcation of the segmented vertebrae is often also needed, for instance, for further analysis steps or for reporting purposes. Especially in images originally not intended for spine imaging, anatomical labeling of the vertebrae can be challenging due to variations in the Ô¨Åeld of view. These variations lead to variable coverage of the spine and also of structures that provide anatomical cues for identiÔ¨Åcation of the vertebrae, such as the ribs or the sacrum. Additionally, neighboring vertebrae often have similar shape and appearance so that independent labeling of each vertebra may result in mistakes. Vertebra identiÔ¨Åcation therefore requires a global rather than a perinstance approach to ensure an overall plausible, anatomically correct labeling. Another challenge inherent to an instance segmentation ap proach is the identiÔ¨Åcation of partially visible instances. While occlusion is a typical problem in twodimensional but not in threedimensional images, some vertebrae may be only partially Preprint submitted to Medical Image Analysis January 19, 2019arXiv:1804.04383v3  [cs.CV]  11 Feb 2019visible due to the limited Ô¨Åeld of view of the scan. If these in completely visible vertebrae are included in subsequent analyses that are based on the obtained vertebra segmentations, such as measurement of vertebral heights for detection and classiÔ¨Åca tion of vertebral compression fractures (Grigoryan et al., 2003), their results may be unreliable. Therefore, incompletely visi ble instances need to be either ignored or explicitly identiÔ¨Åed as incomplete so that they can be excluded from subsequent analyses. In this paper, we propose an iterative instancebyinstance segmentation approach for vertebra segmentation based on a fully convolutional neural network. This network performs vertebra detection, segmentation, anatomical identiÔ¨Åcation and classiÔ¨Åcation of their completeness concurrently and therefore presents an entirely supervised approach that can be trained end toend. While we propose to attempt a perinstance identiÔ¨Åcation of the individual vertebrae together with the segmentation, the labeling is subsequently adjusted taking all segmented vertebrae into account. In contrast to previous approaches, the presented method can be used for any imaging modality, any Ô¨Åeld of view and any number and type (cervical, thoracic, lumbar) of visi ble vertebrae because it avoids explicit modeling of shape and appearance of the vertebrae and the vertebral column. We evalu ate these claims using a diverse selection of datasets, including scans from di erent modalities (CT and MR), various Ô¨Åelds of view, cases with severe compression fractures and a particularly challenging set of lowdose chest CT. 2. Related work "
491,Structured learning of metric ensembles with application to person re-identification.txt,"Matching individuals across non-overlapping camera networks, known as person
re-identification, is a fundamentally challenging problem due to the large
visual appearance changes caused by variations of viewpoints, lighting, and
occlusion. Approaches in literature can be categoried into two streams: The
first stream is to develop reliable features against realistic conditions by
combining several visual features in a pre-defined way; the second stream is to
learn a metric from training data to ensure strong inter-class differences and
intra-class similarities. However, seeking an optimal combination of visual
features which is generic yet adaptive to different benchmarks is a unsoved
problem, and metric learning models easily get over-fitted due to the scarcity
of training data in person re-identification. In this paper, we propose two
effective structured learning based approaches which explore the adaptive
effects of visual features in recognizing persons in different benchmark data
sets. Our framework is built on the basis of multiple low-level visual features
with an optimal ensemble of their metrics. We formulate two optimization
algorithms, CMCtriplet and CMCstruct, which directly optimize evaluation
measures commonly used in person re-identification, also known as the
Cumulative Matching Characteristic (CMC) curve.","The task of person reidentiÔ¨Åcation (reid) is to match pedes trian images observed from different and disjoint camera views. Despite extensive research efforts in reid [9, 36, 48, 51, 44, 47, 33, 20, 46], the problem itself is still a very challenging task due to (a) large variation in visual appearance (person‚Äôs appearance often undergoes large variations across different camera views); (b) signiÔ¨Åcant changes in human poses at the time the image was captured; (c) large amount of illumination changes, back ground clutter and occlusions; d) relatively low resolution and the different placement of the cameras. Moreover, the problem becomes increasingly difÔ¨Åcult when there are high variations in pose, camera viewpoints, and illumination, etc. To address these challenges, existing research has concen trated on the development of sophisticated and robust features to describe visual appearance under signiÔ¨Åcant changes. Most of them use appearancebased features that are viewpoint in variant such as color and texture descriptors [7, 9, 11, 39, 23]. However, the system that relies heavily on one speciÔ¨Åc type of visual cues, e.g., color, texture or shape, would not be practical Corresponding author. Email address: chhshen@gmail.com (Chunhua Shen )and powerful enough to discriminate individuals with similar visual appearance. Some studies have tried to address the above problem by seeking a combination of robust and distinctive fea ture representation of person‚Äôs appearance, ranging from color histogram [11], spatial cooccurrence representation [39], LBP [44], to color SIFT [47]. The basic idea of exploiting multiple visual features is to build an ensemble of metrics (distance func tions), in which each distance function is learned using a sin gle feature and the Ô¨Ånal distance is calculated from a weighted sum of these distance functions [7, 44, 47]. These works often predeÔ¨Åne distance weights, which need to be reestimated be forehand for different data sets. However, such a predeÔ¨Åned principle has some drawbacks. ‚Ä¢ Different realworld reid scenarios can have very dif ferent characteristics, e.g., variation in view angle, light ing and occlusion. Simply combining multiple distance functions using predetermined weights may be undesir able as highly discriminative features in one environment might become irrelevant in another environment. ‚Ä¢ The effectiveness of distance learning heavily relies on the quality of the feature selected, and such selection re quires some domain knowledge and expertise. ‚Ä¢ Given that certain features are determined to be more re Preprint submitted to Elsevier 13102018arXiv:1511.08531v2  [cs.CV]  24 May 2016liable than others under a certain condition, applying a standard distance measure for each individual match is undesirable as it treats all features equally without differ entiation on features. In these ends, it necessarily demands a principled approach that is able to automatically select and learn weights for diverse met rics, meanwhile generic yet adaptive to different scenarios. Person reidentiÔ¨Åcation problem can also be cast as a learn ing problem in which either metrics or discriminative models are learned [4, 5, 16, 17, 42, 43, 44, 20, 41, 27, 49], which typically learn a distance measure by minimizing intraclass distance and maximizing interclass distance simultaneously. Thereby, they require sufÔ¨Åcient labeled training data from each class1and most of them also require new training data when camera settings change. Nonetheless, in person reid bench mark, available training data is relatively scarce, and thus in herently undersampled for building a representative class dis tribution. This intrinsic characteristic of person reid problem makes metric learning pipelines easily overÔ¨Åtted and unable to be applicable in small image sets. To combat above difÔ¨Åculties simultaneously, in this paper, we introduce two structured learning based approaches to per son reid by learning weights of distance functions for lowlevel features. The Ô¨Årst approach, CMCtriplet, optimizes the relative distance using the triplet units, each of which contains three person images, i.e., one person with a matched reference and a mismatched reference. Treating these triplet units as input, we formulate a large margin framework with triplet loss where the relative distance between the matched pair and the mismatched pair tends to be maximized. An illustration of CMCtripletis shown in Fig. 1. This triplet based model is more natural for person reid mainly because the intraclass and interclass varia tion may vary signiÔ¨Åcantly for different classes, making it inap propriate to require the distance between a matched/mismatched pair to fall within an absolute range [49]. Also, training images in person reid are relatively scarce, whereas the tripletbased training model is to make comparison between any two data points rather than comparison between any data distribution boundaries or among clusters of data. This thus alleviates the overÔ¨Åtting problem in person reid given undersampled data. The second approach, CMCtop, is developed to maximize the average rank krecognition rate, in which kis chosen to be small, e.g., k<10. Setting the value of kto be small is cru cial for many realworld applications since most surveillance operators typically inspect only the Ô¨Årst ten or twenty items retrieved. Thus, we directly optimize the testing performance measure commonly used in CMC curve, i.e., the recognition rate at rank kby using structured learning. The main contributions of this paper are threefold: ‚Ä¢ We propose two principled approaches, CMCtripletand CMCtop, to build an ensemble of person reid metrics. The standard approach CMCtripletis developed based on triplet information, which is more tolerant to large intra 1Images of each person in a training set form a class.and interclass variations, and alleviate the overÔ¨Åtting problem. The second approach of CMCtopdirectly op timizes an objective closer to the testing criteria by maxi mizing the correctness among top kmatches using struc tured learning, which is empirically demonstrated to be more beneÔ¨Åcial to improving recognition rates. ‚Ä¢ We perform feature quantiÔ¨Åcation by exploring the ef fects of diverse feature descriptors in recognizing per sons in different benchmarks. An ensemble of metrics is formulated into a late fusion paradigm where a set of weights corresponding to visual features are automati cally learned. This late fusion scheme is empirically stud ied to be superior to various early fusions on visual fea tures. ‚Ä¢ Extensive experiments are carried out to demonstrate that by building an ensemble of person reid metrics learned from different visual features, notable improvement on rank 1recognition rate can be obtained. In addition, our ensemble approaches are highly Ô¨Çexible and can be com bined with linear and nonlinear metrics. For nonlinear base metrics, we extend our approaches to be tractable and suitable to largescale benchmark data sets by ap proximating the kernel learning. 2. Related Work "
139,Recurrent Collective Classification.txt,"We propose a new method for training iterative collective classifiers for
labeling nodes in network data. The iterative classification algorithm (ICA) is
a canonical method for incorporating relational information into
classification. Yet, existing methods for training ICA models rely on the
assumption that relational features reflect the true labels of the nodes. This
unrealistic assumption introduces a bias that is inconsistent with the actual
prediction algorithm. In this paper, we introduce recurrent collective
classification (RCC), a variant of ICA analogous to recurrent neural network
prediction. RCC accommodates any differentiable local classifier and relational
feature functions. We provide gradient-based strategies for optimizing over
model parameters to more directly minimize the loss function. In our
experiments, this direct loss minimization translates to improved accuracy and
robustness on real network data. We demonstrate the robustness of RCC in
settings where local classification is very noisy, settings that are
particularly challenging for ICA.","Data science tasks often require reasoning about net works of connected entities, such as social and informa tion networks. In classication tasks, the connections among network nodes can have important eects on nodelabeling patterns, so models that perform classi cation in networks should consider network structure to fully represent the underlying phenomena. For ex ample, when classifying individuals by their personality traits in a social network, a common pattern is that individuals will communicate with likemined individu als, suggesting that predicted labels should also tendto be uniform among connected nodes. Collective clas sication methods aim to make predictions based on this insight. In this paper, we introduce a collective classication framework that will enable an algorithm to more directly optimize the performance of trained collective classiers. Iterative classication is a framework that enables a variety of supervised learning methods to incorporate information from networks. The base machine learn ing method can be any standard classier that labels examples based on input features. The iterative clas sication algorithm (ICA) operates by using previous predictions about neighboring nodes as inputs to the current predictor. This pipeline creates a feedback loop that allows models to pass information through the net work and capture the eect of structure on classication. In spite of the feedback loop being the most important aspect of ICA, existing approaches train models in a manner that ignores the feedbackloop structure. In this paper, we introduce recurrent collective classica tion (RCC), which corrects this discrepancy between the learning and prediction algorithms, incorporating principles used in deep learning and recurrent neural networks into the training process. Existing learning algorithms for iterative classication resort to an approximation based on the unrealistic assumption that the predicted labels of neighbors are their true classes (Neville and Jensen, 2000; London and Getoor, 2013). This assumption is overly opti mistic. If it were true, iteration would be unnecessary. Because the assumption is overly optimistic, it causes the learned models to cascade and amplify errors when the assumption is broken in early stages of prediction. In contrast, ICA uses predicted neighbor labels as feed back for each subsequent prediction, which means that if the model was trained expecting these predicted la bels to be perfect, it will not be robust to situations where predictions are noisy or inaccurate. In this paper, we correct this faulty assumption and develop an ap proach that trains models for iterative classication by treating the intermediate predictions as latent variables. We compute gradients to the classication loss function using backpropagation through iterative classication.arXiv:1703.06514v1  [cs.LG]  19 Mar 2017Recurrent Collective Classication To compute gradients for ICA, we break down the ICA process into dierentiable (or subdierentiable) opera tions. In many cases, the base classier is dierentiable with respect to its parameters. For example, if it is a logistic regression, it has a wellstudied gradient. ICA also computes dynamic relational features using the predictions of network neighbors. These relational fea tures are also functions through which gradients can be propagated. Finally, because the same baseclassier parameters should be used at all iterations of ICA, we can use methods for recurrent neural networks such as backpropagation through time (BPTT) (Werbos, 1990) to compute the combined gradient. In contrast with existing strategies for training ICA, the resulting training optimization more closely mimics the actual procedure that ICA uses for prediction. The RCC framework accommodates a variety of base classiers and relational feature functions. The only re striction is that they must be dierentiable. Therefore, RCC is nearly as general as ICA, and its prediction procedure is practically identical to ICA. The key dif ference is that the view of the algorithm as nested, dierentiable functions enables a training procedure that is better aligned with RCC and ICA prediction. We evaluate RCC on data where collective classication has previously been shown to be helpful. We demon strate that RCC trains classiers that are robust to situations where local predictions are inaccurate. 2 Related Work "
223,Tripartite: Tackle Noisy Labels by a More Precise Partition.txt,"Samples in large-scale datasets may be mislabeled due to various reasons, and
Deep Neural Networks can easily over-fit to the noisy label data. To tackle
this problem, the key point is to alleviate the harm of these noisy labels.
Many existing methods try to divide training data into clean and noisy subsets
in terms of loss values, and then process the noisy label data varied. One of
the reasons hindering a better performance is the hard samples. As hard samples
always have relatively large losses whether their labels are clean or noisy,
these methods could not divide them precisely. Instead, we propose a Tripartite
solution to partition training data more precisely into three subsets: hard,
noisy, and clean. The partition criteria are based on the inconsistent
predictions of two networks, and the inconsistency between the prediction of a
network and the given label. To minimize the harm of noisy labels but maximize
the value of noisy label data, we apply a low-weight learning on hard data and
a self-supervised learning on noisy label data without using the given labels.
Extensive experiments demonstrate that Tripartite can filter out noisy label
data more precisely, and outperforms most state-of-the-art methods on five
benchmark datasets, especially on real-world datasets.","Thanks to the largescale datasets with human precisely annotated labels, DNNs achieve a great success. How ever, collecting highquality and extensive data is consid erably costly and timeconsuming. To alleviate this is sue, some cheaper alternatives are often employed, such as webcrawling [23], online queries [4, 20, 34], crowdsourc ing [41, 45] and so on. Unfortunately, they inevitably intro duce some noisy labels. Many studies [3,19,33,46] have re ported that DNNs could easily overÔ¨Åt to the noises, which signiÔ¨Åcantly degrades their generalization performance. There have been many efforts to tackle noisy labels. Many of them reach a consensus that the key is to allevi ate the impact of noisy labels for network training. Loss correction based methods [9, 27, 28, 32] aim to rectify thelosses of noisy labeled data in the training stage, but may mistakenly rectify some clean data. Sampleselection based methods [10, 18, 37, 44] tackle noisy labels by partitioning the training data into clean and noisy subsets, then using them for network training in different ways. The main stream partition criteria are two types: 1) Smallloss cri terion [10] assumes that the losses of noisy labeled data are signiÔ¨Åcantly higher than those of clean data during training. Therefore, they try to Ô¨Ånd a threshold, T, and select the samples, whose losses < T , as clean data. The others are treated as noisy labeled data. 2) Gaussian Mixture Model (GMM) criterion [18] believes that the statistical distribu tion of noisy data losses is different from that of clean data losses. It aims to Ô¨Ånd the probability of a sample being noisy or clean by Ô¨Åtting a mixture model. However, we observe that many realworld noisy labels are introduced between similar categories, especially hap pen among hard samples. For instances, a dolphin is mis labeled as a whale, vice versa, as shown in Fig. 1. In this paper, we deÔ¨Åne ‚Äúhard samples‚Äù as data that distribute close to the decision boundary and are difÔ¨Åcult to be dis tinguished. Our investigation shows, regardless of clean or noisy, the training losses of hard samples are neither small nor signiÔ¨Åcantly different. So, existing sampleselection based methods have two Ô¨Çaws: 1) Low quality of training data partition. They are likely to mistake the hard noisy la beled samples as clean ones solely based on losses, and vice versa. This downgrades the performance of a network be cause the network will learn certain noisy labels and discard some clean data. 2) Ineffective usage of noisy labeled data. Smallloss criterion often drops the noisy samples which is a waste of valuable information. GMM criterion applies the semisupervised learning to reuse noisy labeled samples by assigning pseudolabels to them. But it heavily relies on the discriminative ability of networks. The incorrect relabeling will cause a severe harm to networks as well. To tackle above problems, we propose a novel method: Tripartite that mainly addresses hard samples in realworld datasets. Since hard samples distribute around the decision boundary, predictions of varied networks are often inconsis tent at the early training stage. Meanwhile, the prediction 1arXiv:2202.09579v2  [cs.CV]  19 Mar 2022Data distribution Network Partition criterion Partition result ùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL : keyboard GL :  keyboardùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL: woman GL:  keyboardEasy samplesùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL : whale GL :  dolphin ùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL :  dolphin GL :  dolphin ùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL : dolphin GL : womanHard samplesHard Noisy Clean xSmall loss Loss Loss:  <R% Loss:  >R% Loss:  <R% Loss:  <R%  Loss: >R% P1 P2 x xTripartite  GL dolphin Ôºö ùêèùüè:  whale ùêèùüê:  dolphin GL:  keyboard ùêèùüè:  woman ùêèùüê:  woman GL dolphin ùêèùüè:  whale ùêèùüê:  dolphin GL:  woman ùêèùüè:  dolphin ùêèùüê:  dolphin GL :  keyboard ùêèùüè:  keyboard ùêèùüê:  keyboard Divided by Loss distributionLoss xGMM ùêèùêßùê®ùê¢ùê¨ùêû : 0.55 ùêèùêúùê•ùêûùêöùêß: 0.45 ùêèùêúùê•ùêûùêöùêß: 0. 55 ùêèùêßùê®ùê¢ùê¨ùêû : 0.45 ùêèùêúùê•ùêûùêöùêß: 0.90 ùêèùêßùê®ùê¢ùê¨ùêû : 0.10 ùêèùêßùê®ùê¢ùê¨ùêû : 0.85 ùêèùêúùê•ùêûùêöùêß: 0.15 ùêèùêßùê®ùê¢ùê¨ùêû : 0.51 ùêèùêúùê•ùêûùêöùêß: 0.49 Divided by Tripartition RuleP1=GL‚ãÇP2=GL P1=GL‚ãÇP2‚â†GL ‚à™ P1‚â†GL‚ãÇP2=GL P1‚â†GL‚ãÇP2‚â†GLClean Hard NoisyT1 T2Divided by Loss ValueSmall LargeR% 1R%T Clean Noisywoman dolphinwhalekeyboardFigure 1. Comparison with existing partition criteria. From top to bottom are Smallloss criterion, GMM criterion and the proposed Tripartition criterion. TL denotes the true label, GL denotes the given label. P1andP2are the predicted labels of inputs. PTLandPGL denote the predicted probabilities of true and given labels, respectively. Pclean andPnoise denote the predicted probabilities of a sample to be clean and noisy, respectively. In the partition result, the check mark and cross mark denote that the sample is partitioned correctly and incorrectly, respectively. Smallloss criterion sorts losses and selects R% samples with small losses as clean data for training. GMM criterion Ô¨Ånds the probability of a sample being clean or noisy by Ô¨Åtting a mixture model. Neither of them can Ô¨Ålter out hard samples. By contrast, Tripartite divides the training set into hard, noisy, and clean subsets according to the relations between P1,P2and GL. of a network of an easy noisy sample is usually inconsis tent with the given label. Based on this observation, Tri partite can divide training data into hard, noisy and clean subsets. It has the advantage of improving the quality of clean and noisy subsets. To effectively use data, we apply a lowweight training strategy for samples in the hard subset, and employ a selfsupervised training strategy for samples in the noisy subset without using the given labels. Hence, Tripartite is designed toward minimizing the harm of noisy labels and maximizing the value of noisy labeled data. The extensive experiments on Ô¨Åve benchmark datasets demon strate the superior performance of Tripartite compared with the stateoftheart (SOTA) methods. Especially, it shows robustness at a wide range of realworld datasets. The key contributions of our work are threefold. ‚Ä¢ We propose a novel partition criterion, which divides training data into three subsets: hard, noisy, and clean. It alleviates the hard sample selection problem of other cri teria, and largely improves the quality of clean and noisy subsets. ‚Ä¢ We design a lowweight training strategy for hard data and a selfsupervised training strategy for noisy labeleddata, which aim at minimizing the harm of noisy labels and maximizing the value of noisy labeled data. ‚Ä¢ To mimic the noisy label of hard sample in realworld datasets, we create a synthetic classdependent label noise on CIFAR datasets, called realistic noise. It Ô¨Çips labels of samples, which are from two different classes, at con trolled ratios according to their similarity. The details are shown in Supplementary. 2. Related work "
521,Efficacy of Bayesian Neural Networks in Active Learning.txt,"Obtaining labeled data for machine learning tasks can be prohibitively
expensive. Active learning mitigates this issue by exploring the unlabeled data
space and prioritizing the selection of data that can best improve the model
performance. A common approach to active learning is to pick a small sample of
data for which the model is most uncertain. In this paper, we explore the
efficacy of Bayesian neural networks for active learning, which naturally
models uncertainty by learning distribution over the weights of neural
networks. By performing a comprehensive set of experiments, we show that
Bayesian neural networks are more efficient than ensemble based techniques in
capturing uncertainty. Our findings also reveal some key drawbacks of the
ensemble techniques, which was recently shown to be more effective than Monte
Carlo dropouts.","Although machine learning techniques have achieved a major breakthrough in recent years, their performance comes at a cost of acquiring large volumes of training data. This is especially true for supervised deep learning mod els that demand a substantial amount of labeled data to achieve a reasonable performance. For applications that require expert knowledge such as medical and biological images, labels are extremely hard and expensive to ob tain. Active learning (AL) aims to mitigate this problem by smartly selecting data points to label (from an expert) from a large pool of unlabeled data to improve model per formance. This sampling is typically based on some ac quisition function (AF) which provides a score for each un labeled data that signiÔ¨Åes its level of importance. While there are many approaches to implementing AF [1, 2], uncertaintybased based approaches are shown to be the most effective [3, 4, 5, 6]. Bayesian neural network (BNN) naturally models uncer tainty by learning a probability distribution over the neural network weights. Therefore, for a given input, as we takemultiple realizations of the network, the variance captured by the weights is reÔ¨Çected as the variation in the output, which inturn models uncertainty. BNNs learn by applying a prior distribution over weights and performing variational inference to approximate the posterior distribution. In [7], the authors proved that applying dropout to neural networks is equivalent to a BNN. This theory was further leveraged by proposing Monte Carlo Dropout (MCD) for uncertainty estimation in AL [5]. In a recent work, [6] showed that ensemble of neural networks (EN) outperform MCD when it comes to uncertainty estimation; thus, proving to be the choice for active learning. Consequentially, it is natural to assume EN to perform better than BNN since MCD is equivalent to BNN. However, dropout neural networks form a special class of BNN where the posterior distribution is a special case of spikeandslab distribution. Contrary to this, BNNs allow for a broader class of prior and posterior distributions on weights . In this paper, we reestablish the efÔ¨Åcacy of BNNs in active learning over ensembles and MCD by using a more general scaled normal prior based BNN proposed in [8]. The scaled normal prior is a continuous relaxation of thespikeandslab distribution and subsumes Dropout as a special case. Through extensive experiments on multiple datasets namely, MNIST, Fashion MNIST, CIFAR10 and CIFAR100 and a regression dataset on housing price predic tion we show that the scaled normal prior based BNN pro vides more robust and efÔ¨Åcient active learning over EN and MCD. We perform several experiments to demonstrate the pros and cons of BNN over EN and MCD. For each round of active learning, the models are trained using two different settings: (1) reuse the trained state of the model from pre vious round and retrain on the newly appended datapoints (termed as continual training ) and (2) reset the model pa rameters and retrain from scratch. Our results show that BNN performs signiÔ¨Åcantly better than EN in terms of clas siÔ¨Åcation accuracy when it comes to continual training. In fact, the performance of EN is worse than MCD which can be attributed to overÔ¨Åtting and catastrophic forgetting. That being said, when retrained from scratch, BNN and EN per 4321arXiv:2104.00896v2  [cs.LG]  19 Apr 2021form on a similar level, which is still an advantage for BNN since estimating uncertainty using ensembles is a costly process. We found that EN requires about Ô¨Åve ensembles in order to achieve good active learning performance. This implies, training of Ô¨Åve different i.i.d networks and storing the trained state of every single network instance. BNN on other hand, achieves similar yet a more robust performance with a tradeoff of just doubling parameter size of conven tional neural network. Besides illustrating the overall effectiveness of BNN for active learning, we answer the following questions: (1) do acquisition functions behave the same for Bayesian, ensem ble and MC dropouts? (2) how does model capacity af fect the outcome, do BNNs with lower model capacity per form worse than EN (or MCD)? (3) are BNNs better than EN when predicting challenging class labels? Inspired by the performance of BNNs, we also propose a computation ally efÔ¨Åcient uncertainty estimation method for fully con nected dense layers with ReLU nonlinearity. Since AL in volves repeated uncertainty estimation over large unlabeled dataset, efÔ¨Åcient uncertainty estimation is of huge practi cal importance. In the proposed method, instead of taking multiple instantiations of neural networks to estimate the uncertainty, we perform just one forward pass. In this for ward pass, at each neuron, we approximate the probability distribution parametrically. We show that this algorithm is capable of achieving performances that is onpar with the traditional uncertainty estimation in BNN. To the best of our knowledge, we are the Ô¨Årst to per form comprehensive empirical analysis to demonstrate the efÔ¨Åcacy of BNNs for active learning. While most existing research limit themselves to experiments on small architec tures and dataset, ours does not have such constraints. 2. Related Work "
504,Blind Knowledge Distillation for Robust Image Classification.txt,"Optimizing neural networks with noisy labels is a challenging task,
especially if the label set contains real-world noise. Networks tend to
generalize to reasonable patterns in the early training stages and overfit to
specific details of noisy samples in the latter ones. We introduce Blind
Knowledge Distillation - a novel teacher-student approach for learning with
noisy labels by masking the ground truth related teacher output to filter out
potentially corrupted knowledge and to estimate the tipping point from
generalizing to overfitting. Based on this, we enable the estimation of noise
in the training data with Otsus algorithm. With this estimation, we train the
network with a modified weighted cross-entropy loss function. We show in our
experiments that Blind Knowledge Distillation detects overfitting effectively
during training and improves the detection of clean and noisy labels on the
recently published CIFAR-N dataset. Code is available at GitHub.","Learning with noisy labels is a challenging task in image classiÔ¨Åcation. It is well known that label noise leads to heavy performance drops with standard classiÔ¨Åcation meth ods[Song et al. , 2022 ]. The goal of learning with noisy la bels is therefore to train a classiÔ¨Åcation model with labelled training images and achieve high classiÔ¨Åcation performance on unseen test images, even if the labels for training are noisy and corrupted. Labels are noisy because humans are natu rally unable to classify images perfectly due to ambiguous images, individual human bias, pressure of time, or various other reasons. Many modern methods [Liuet al. , 2022b; Rawat and Wang, 2017 ]are trained on large and potentially noisy datasets and thus it is an interest of the community to make classiÔ¨Åcation robust against noisy labels. To evaluate the robustness of methods for learning with noisy labels, clean image datasets like CIFAR [Krizhevsky and Hinton, 2009 ],Clothing1M [Xiao et al. , 2015 ], orWe bVision [Liet al. , 2017 ]are synthetically corrupted by ran 1https://github.com/TimoK93/blind knowledge distillation0 0:25 0:5 0:75 100:10:20:30:4 1 s 2 P(y=yjx)nNoisy Labels Clean Labels Otsu Distributions Figure 1: Distribution of ground truth label related probabilities PA(y=yjx)at beginning of overÔ¨Åtting (tipping point) and the resulting gaussian distributions after Otsu‚Äôs algorithm for the dataset Worst . Red bars show the normalized distribution of noisy labels and green bars of clean labels, respectively. Note that the gaussian distributions (blue) are scaled for visualization purposes. Our presented Blind Knowledge Distillation enables an adaptive noise estimation via the thresholds 1,s, and2and a robust learning with noisy labels. domly Ô¨Çipping label annotations either symmetrically with out constraints or asymmetrically with predeÔ¨Åned rules to mimic realistic label noise. However, Wei et al. [2022 ]shows that synthetic label noise has different behaviour compared to realworld label noise and is thus not an ideal choice to eval uate robust learning. To close this gap, Wei et al. have made great efforts and presented CIFARN with multiple newly an notated ground truth labels for CIFAR with humaninduced label noise. With these new annotations, robust learning can be evaluated more realistically. In this paper, we introduce a novel method to detect the be ginning of overÔ¨Åtting on sample details during training, that is usually roughly estimated as in [Liet al. , 2020 ], and present a simple but effective method to detect most likely corrupted laarXiv:2211.11355v1  [cs.CV]  21 Nov 2022bels. Our method is inspired by Knowledge Distillation [Hin tonet al. , 2015 ]for neural networks which extracts ‚Äòknowl edge‚Äô from a teacher network to train a student network. Differently than usual, our student network is just trained with a subset of the teachers ‚Äòknowledge‚Äô. SpeciÔ¨Åcally, it does not ‚Äòsee‚Äô the ‚Äòknowledge‚Äô about the given and poten tially corrupted ground truth labels by utilizing the teachers ground truth complementary logits. Therefore we call it Blind Knowledge Distillation . Based on the detected noisy labels, we propose a simple but effective losscorrection method to train the teacher model robustly with label noise. We perform extensive experiments on CIFAR10N and the results show thatBlind Knowledge Distillation ‚Ä¢ successfully estimates the tipping point from Ô¨Åtting to general patterns to (over)Ô¨Åtting to sample details, ‚Ä¢ is an effective method to estimate the likelihood of labels being noisy, ‚Ä¢ and improves the classiÔ¨Åcation accuracy while training with high noise levels. 2 Related Work "
