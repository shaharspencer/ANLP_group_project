,titles,abstract,introduction
0,Regularization in network optimization via trimmed stochastic gradient descent with noisy label.txt,"Regularization is essential for avoiding over-fitting to training data in
network optimization, leading to better generalization of the trained networks.
The label noise provides a strong implicit regularization by replacing the
target ground truth labels of training examples by uniform random labels.
However, it can cause undesirable misleading gradients due to the large loss
associated with incorrect labels. We propose a first-order optimization method
(Label-Noised Trim-SGD) that uses the label noise with the example trimming in
order to remove the outliers based on the loss. The proposed algorithm is
simple yet enables us to impose a large label-noise and obtain a better
regularization effect than the original methods. The quantitative analysis is
performed by comparing the behavior of the label noise, the example trimming,
and the proposed algorithm. We also present empirical results that demonstrate
the effectiveness of our algorithm using the major benchmarks and the
fundamental networks, where our method has successfully outperformed the
state-of-the-art optimization methods.","The neural networks learning is a large scale problem that is characterized by large size dataset with large model. The neural network model con sists of a number of ‚àóCorresponding author: ByungWoo Hong Preprint submitted to Journal of Neurocomputing May 4, 2022layers that is known to approximate linear and nonlinear fu nctions. Due to its high degree of freedom, however, the network network model is alw ays at the risk of over Ô¨Åtting to the training examples that degenerates the genera lization, or the estimation performance for unknown data Thus, regularization is requi red in the training process of the neural networks for better generalization. The neural network model is trained using the stochastic gra dient descent (SGD) and its variants in combination with explicit and implicit r egularization methods. The explicit regularization restricts the model parameters wi th a prior knowledge, e.g., weightdecay adds the regularization term into the object f unction, assuming the model parameters should follow a L2ball. Dropout may assume that the model is an ensem ble of sparse networks. In contrast, the implicit methods of fer regularization effect independent to the model structure. SGD is actually an impli cit regularization that updates the model using a subset of the training examples in a n iterative manner that imposes the stochastic noise into the optimization process . Early stopping is also an implicit regularization. The label noise [1, 2] is an implic it method that replaces the target label of randomlyselected examples by random unifo rm labels. The label noise is simple and computationally efÔ¨Åcient, yet it provides a st rong regularization effect [2] in the classiÔ¨Åcation problems. However, we Ô¨Ånd that the labe l noise also cause outliers with high loss values that can degenerate the model training . We propose a Ô¨Årstorder optimization algorithm, called Lab elNoised TrimSGD, that intendedly uses the label noise with the example trimmi ng in order to obtain an implicit regularization effect. Our algorithm imposes the label noise and then removes data with low and high loss values using an exampletrimming in order to remove outlier examples. This enables us to apply a large amount of l abel noise than the naive labelnoise method, resulting in an improvement of ge neralization of the network model. Different with the data trimming algorithms, we inte ntionally use the label noise in order to improve generalization of model. We relate our method to prior works in Section 2 and present th e naive algorithms of the label noise and the example trimming in Section 3, foll owed by our proposed algorithm in Section 4. The effectiveness of our algorithm i s demonstrated by experi mental results in Section 5 and we conclude in Section 6. 22. Related works "
1,Multi-Objective Interpolation Training for Robustness to Label Noise.txt,"Deep neural networks trained with standard cross-entropy loss memorize noisy
labels, which degrades their performance. Most research to mitigate this
memorization proposes new robust classification loss functions. Conversely, we
propose a Multi-Objective Interpolation Training (MOIT) approach that jointly
exploits contrastive learning and classification to mutually help each other
and boost performance against label noise. We show that standard supervised
contrastive learning degrades in the presence of label noise and propose an
interpolation training strategy to mitigate this behavior. We further propose a
novel label noise detection method that exploits the robust feature
representations learned via contrastive learning to estimate per-sample
soft-labels whose disagreements with the original labels accurately identify
noisy samples. This detection allows treating noisy samples as unlabeled and
training a classifier in a semi-supervised manner to prevent noise memorization
and improve representation learning. We further propose MOIT+, a refinement of
MOIT by fine-tuning on detected clean samples. Hyperparameter and ablation
studies verify the key components of our method. Experiments on synthetic and
real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves
state-of-the-art results. Code is available at https://git.io/JI40X.","Building a new dataset usually involves manually la beling every sample for the particular task at hand. This process is cumbersome and limits the creation of large datasets, which are usually necessary for training deep neu ral networks (DNNs) in order to achieve the required per formance. Conversely, automatic data annotation based on web search and user tags [ 29,22] leverages the use of larger data collections at the expense of introducing some incorrect labels. This label noise degrades DNN perfor mance [ 3,52] and this poses an interesting challenge that has recently gained a lot of interest in the research commu nity [45, 41, 23, 50, 12, 1, 28, 55, 13, 31].In image classiÔ¨Åcation problems, label noise usually in volves different noise distributions [ 22,55]. Indistribution noise types consist of samples with incorrect labels, but whose image content belongs to the dataset classes. When indistribution noise is synthetically introduced, it usually follows either an asymmetric or symmetric random distribu tion. The former involves label Ô¨Çips to classes with some semantic meaning, e.g., a cat is Ô¨Çipped to a tiger, while the latter does not. Furthermore, web label noise types are usually dominated by outofdistribution samples where the image content does not belong to the dataset classes. Recent studies show that all label noise types impact DNN perfor mance, although performance degrades less with web noise [22, 34]. Robustness to label noise is usually pursued by identify ing noisy samples to: reduce their contribution in the loss [23,11], correct their label [ 1,28], or abstain their classiÔ¨Åca tion [ 42]. Other methods exploit interpolation training [ 53], regularizing label noise information in DNN weights [ 13], or small sets of correctly labeled data [ 18,55]. However, most previous methods rely exclusively on classiÔ¨Åcation losses and little effort has being directed towards incorporating sim ilarity learning frameworks [ 32], i.e. directly learning image representations rather than a class mapping [45]. Similarity learning frameworks are very popular in com puter vision for a variety of applications including face recog nition [ 44], Ô¨Ånegrained retrieval [ 37], or visual search [ 35]. These methods learn representations for samples of the same class (positive samples) that lie closer in the feature space than those of samples from different classes (negative sam ples). Many traditional methods are based on sampling pairs or triplets to measure similarities [ 7,19]. However, super vised and unsupervised contrastive learning approaches that consider a high number of negatives have recently received signiÔ¨Åcant attention due to their success in unsupervised learning [ 5,14,27]. In the context of label noise, there are some attempts at training with simple similarity learning losses [ 45], but there are, to the best of our knowledge, no works exploring more recent contrastive learning losses [ 24]. This paper proposes MultiObjective Interpolation Train ing (MOIT), a framework to robustly learn in the presencearXiv:2012.04462v2  [cs.CV]  18 Mar 2021of label noise by jointly exploiting synergies between con trastive and semisupervised learning. The former intro duces a regularization of the contrastive loss in [ 24] to learn noiserobust representations that are key for accurately de tecting noisy samples and, ultimately, for semisupervised learning. The latter performs robust image classiÔ¨Åcation and boosts performance. Our MOIT+ reÔ¨Ånement further demonstrates that Ô¨Ånetuning on the detected clean data can boost performance. MOIT/MOIT+ achieves stateofthe art results across a variety of datasets (CIFAR10/100 [ 26], miniImageNet [ 22], and miniWebVision [ 29]) with both synthetic and realworld web label noise. Our main contri butions are as follows: 1.A multiobjective interpolation training (MOIT) frame work where supervised contrastive learning and semi supervised learning help each other to robustly learn in the presence of both synthetic and web label noise under a single hyperparameter conÔ¨Åguration. 2.An interpolated contrastive learning (ICL) loss that imposes linear relations both on the input and the con trastive loss to mitigate the performance degradation observed for the supervised contrastive learning loss in [24] when training with label noise. 3.A novel label noise detection strategy that exploits the noiserobust feature representations provided by ICL to enable semisupervised learning. This detection strat egy performs a knearest neighbor search to infer per sample label distributions whose agreements with the original labels identify correctly labeled samples. 4.A Ô¨Ånetuning strategy over detected clean data (MOIT+) that further boosts performance based on sim ple noise robust losses from the literature. 2. Related work "
2,EEG-based video identification using graph signal modeling and graph convolutional neural network.txt,"This paper proposes a novel graph signal-based deep learning method for
electroencephalography (EEG) and its application to EEG-based video
identification. We present new methods to effectively represent EEG data as
signals on graphs, and learn them using graph convolutional neural networks.
Experimental results for video identification using EEG responses obtained
while watching videos show the effectiveness of the proposed approach in
comparison to existing methods. Effective schemes for graph signal
representation of EEG are also discussed.","The brain signal provides the most comprehensive informa tion regarding the mental state of a human subject. Many applications exploiting brain signals have been attempted, including neurological disease detection, emotion recogni tion, and behavioral modeling. There are several types of brain signals that can be used, such as electroencephalogra phy (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI). In particular, EEG has been considered as a promising solution for various real world applications thanks to the advances of portable EEG devices and signal processing techniques. Brain signals generated from different brain regions may have relationship, which can be exploited for analysis. One way to describe such relationship is based on the physical dis tances between different regions. Another way developed re cently is based on functional connectivity, which is deÔ¨Åned as similarity between signals from different regions, e.g., cross c 2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. This work was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Korea gov ernment (MSIT) (NRF2016R1E1A1A01943283).correlation, mutual information, phase synchronization, and imaginary coherence [1][2]. Recently, graph signal processing has been proposed to process irregularly structured signals effectively [3][4]. It is to extend traditional digital signal processing techniques to signals that are not sampled on regular domains (such as time and grid space) but reside on graphs composed of vertices and edges. Furthermore, deep learning on graph signals has been also studied, and neural network structures for graph signals were proposed [5][6][7]. Brain signals are good examples of graph signals, because graphs are suitable to represent physical or functional connec tivity across different brain regions. However, there exist lit tle work on applying graph signal processing techniques and graph signalbased deep learning methods to model brain sig nals, particularly EEG. This is probably due to the limited number of channels (i.e., electrodes) of EEG, which may not be sufÔ¨Åcient for rich graph representation. This paper proposes a method for deep learning on graph signals for EEG analysis and its application to EEGbased video identiÔ¨Åcation. To our best knowledge, this is the Ô¨Årst attempt to apply graph signalbased deep learning techniques to EEG. In particular, we present various ways to convert EEG signals into graph signals having appropriate graph structures and signal features, which can overcome the low dimension ality of EEG, and use the graph convolutional neural network (GCNN) to learn the graph signals. We deal with an EEG classiÔ¨Åcation problem where the visual stimulus watched by a human subject is identiÔ¨Åed through EEG. 2. RELATED WORK "
3,Towards Early Prediction of Human iPSC Reprogramming Success.txt,"This paper presents advancements in automated early-stage prediction of the
success of reprogramming human induced pluripotent stem cells (iPSCs) as a
potential source for regenerative cell therapies.The minuscule success rate of
iPSC-reprogramming of around $ 0.01% $ to $ 0.1% $ makes it labor-intensive,
time-consuming, and exorbitantly expensive to generate a stable iPSC line.
Since that requires culturing of millions of cells and intense biological
scrutiny of multiple clones to identify a single optimal clone. The ability to
reliably predict which cells are likely to establish as an optimal iPSC line at
an early stage of pluripotency would therefore be ground-breaking in rendering
this a practical and cost-effective approach to personalized medicine. Temporal
information about changes in cellular appearance over time is crucial for
predicting its future growth outcomes. In order to generate this data, we first
performed continuous time-lapse imaging of iPSCs in culture using an ultra-high
resolution microscope. We then annotated the locations and identities of cells
in late-stage images where reliable manual identification is possible. Next, we
propagated these labels backwards in time using a semi-automated tracking
system to obtain labels for early stages of growth. Finally, we used this data
to train deep neural networks to perform automatic cell segmentation and
classification. Our code and data are available at
https://github.com/abhineet123/ipsc_prediction.","1.1 Motivation The goal of this work is to apply machine learning to automate the identification of human iPSCs that show promise for clinical cell therapies in regenerative medicine. IPSCs are ¬©2023 Singh,Jasra,Mouhammed,Dadheech,Ray and Shapiro. License: CCBY 4.0arXiv:2305.14575v1  [cs.CV]  23 May 2023Singh,Jasra,Mouhammed,Dadheech,Ray and Shapiro generated by reprogramming a patient‚Äôs own cells back in time to make more malleable cells with differentiation potential for generating any cells or tissues of interest. This tech nology has shown great potential for transforming regenerative cell therapies, drug and dis ease modelling, tissue repair and regeneration, and personalized genecorrected products. However, the pipeline for iPSC generation, characterization and cell banking is a highly laborintensive, timeconsuming and costly one. The monetary cost of researchgrade iPSC line generation is estimated at USD 10,00025,000 while that of clinicalgrade iPSC line is approximately USD 800,000 based on published reports (Huang et al., 2019). The entire process of optimal iPSC line generation and selection can take up to 35 days and requires a further 3 months to produce large scale iPSCs for therapeutic application in patients. Additionally, quality control techniques for growing iPSCs to limit inter or intrapatient iPSC line variability, which is currently assessed manually, remain imperfect in largescale biomanufacturing. The current solution relies on the judgement of an expert cell biolo gist, who determines precise iPSC induction, confirms pluripotency based on morphological changes and assesses molecular characterization for multiple clones  all tasks that remain highly effortintensive and subjectively biased. Manual cell quality control therefore cannot be used to scale up the production of iPSCs and derived products for therapeutic appli cations. An automated method enabling highthroughput surveillance and validation of cell identity, growth kinetics, and morphological features is desirable throughout the entire manufacturing process. The screening is multifold and needed to not only select optimal cells which have been fully converted to iPSCs during reprogramming stage but also to exclude unstable and pseudo iPSC contaminants during the expansion stage. Automating this process using machine learning would therefore be groundbreaking in improving iPSC bioprocess efficiency and yield, thereby drastically reducing the time and cost involved in the generation of iPSCbased products for therapeutic applications. This paper presents some early but promising steps in this direction. 1.2 Background 1.2.1 iPSC Reprogramming Takahashi and Yamanaka (2006) demonstrated that mouse embryonic or adult fibroblasts can be reprogrammed into pluripotent stem cells by introducing four genes encoding tran scription factors, namely Oct3/4, Sox2, Klf4, and cMYC (Takahashi et al., 2007; Ye et al., 2013). Generated stem cells showed similar morphological or functional behavior as embry onic pluripotent stem cells and were thus termed iPSCs. Soon thereafter, Takahashi et al. (2007) reported directed conversion of human fibroblasts into pluripotent stem cells, termed as human iPSCs. With the discovery of Yamanaka‚Äôs human iPSC technology, patient derived stem cells have huge potential in regenerative medicine (Takahashi and Yamanaka, 2013). Human iPSCs show merit not only in delivering any desired cell types for treating degenerative diseases, tissue repairing, disease modeling, and drug screening (Amabile and Meissner, 2009; Yamanaka, 2009), but they also solve two major problems associated with other pluripotent stem cells such as embryonic stem cells (Ye et al., 2013), namely immune tolrenace after transplantation and ethical concerns. However, there still exist technical and biomedical challenges including the risk of teratoma formation and the uncertainty of efficient nuclear reprogramming completeness due to variability and inconsistencies in the 2Towards Early Prediction of Human iPSC Reprogramming Success selection of optimal cells (Ye et al., 2013). There are two major problems to be solved be fore human iPSCs can be applied as a standardized technique, Firstly, manually monitoring the quality of growing iPSC colonies that is currently practiced does not scale. Secondly, only colonies that satisfy clinical good manufacturing practice (GMP) standards need to be identified for use in downstream applications. Hence, there is an urgent need for automated quality control, thereby also lending it an element of objectivity and standardization. 1.2.2 Machine learning in iPSC Recognition Though many applications of machine learning for iPSC recognition in images have been presented in the literature (Kusumoto et al., 2018; Waisman et al., 2019; Zhang et al., 2019a; Hirose et al., 2021; Coronnello and Francipane, 2021; Kusumoto et al., 2022; Lan et al., 2022), there are none that include both detection and classification or use time lapse imaging, which is the object of this study. To the best of our knowledge, Zhang et al. (2019b) presented the method that comes closest to this work though that too differs in several key respects. It utilizes fluorescence imaging and the commercial closedsource IMARIS software to segment cells and it captures 3D shape information that is the basis for extracting morphological features to train the classifier it uses. Our aim is to make open source cell segmentation possible without fluorescence and with only the 2D pixel data in standard phasecontrast microscpy images. 1.2.3 Deep Learning in Visual Recognition In the past decade, deep learning (Alzubaidi et al., 2021) has been applied extensively in computer vision (Chai et al., 2021), especially for recognition tasks like image classification (Byerly et al., 2022), object detection (Liu et al., 2020), instance segmentation (Gu et al., 2022), semantic segmentation (Mo et al., 2022), and object tracking (MarvastiZadeh et al., 2021; Jiao et al., 2021; Pal et al., 2021). It has likewise seen broad application in medical image analysis (Suganyadevi et al., 2021; Cai et al., 2020) including segmentation in general (Liu et al., 2021a) and cell segmentation in particular (Wen et al., 2022). The latter is the task most relevant to this work, though cell tracking (BenHaim and RiklinRaviv, 2022; Chen et al., 2021; Wang et al., 2020; Lugagne et al., 2020; Ulman et al., 2017) is also important here. More recently, the advent of transformers (Vaswani et al., 2017) has led to significant performance improvements (Liu et al., 2021b) over the convolutional neural network (CNN) based architectures that had been prominent earlier. Liu et al. (2021c) proposed the Swin transformer to improve the original vision transformer (Dosovitskiy et al., 2021) further using shifted windows. This currently appears to be the backbone of choice in most state of the art models, though that might change with the recent introduction of ConvNext (Liu et al., 2022) as a competitive CNNbased alternative. For this project, we needed instance segmentation models, preferably ones that could benefit from the temporal information in timelapse images. Therefore, we selected topperforming static and video segmentation models (sec. 2.1.4) with publicly available code from a popular leaderboard (Xiang, 2023). We also searched through the leaderboards on a couple of cell tracking and segmentation benchmark challenges (Ulman et al., 2017; Anjum and Gurari, 2020) but failed to find any models with publicly available code. 3Singh,Jasra,Mouhammed,Dadheech,Ray and Shapiro 2. Methodology "
4,A Simple yet Effective Baseline for Robust Deep Learning with Noisy Labels.txt,"Recently deep neural networks have shown their capacity to memorize training
data, even with noisy labels, which hurts generalization performance. To
mitigate this issue, we provide a simple but effective baseline method that is
robust to noisy labels, even with severe noise. Our objective involves a
variance regularization term that implicitly penalizes the Jacobian norm of the
neural network on the whole training set (including the noisy-labeled data),
which encourages generalization and prevents overfitting to the corrupted
labels. Experiments on both synthetically generated incorrect labels and
realistic large-scale noisy datasets demonstrate that our approach achieves
state-of-the-art performance with a high tolerance to severe noise.","Recently deep neural networks (DNNs) have achieved re markable performance on many tasks, such as speech recog nition [ 1], image classiÔ¨Åcation [ 8], object detection [ 25]. However, DNNs usually need a largescale training dataset to generalize well. Such largescale datasets can be collected by crowdsourcing, web crawling and machine generation with a relative low price, but the labeling may contain er rors. Recent studies [ 34,2] reveal that mislabeled exam ples hurt generalization. Even worse, DNNs can memorize the training data with completely randomlyÔ¨Çipped labels, which indicates that DNNs are prone to overÔ¨Åt noisy training data. Therefore, it is crucial to develop algorithms robust to various amounts of label noise that still obtain good general ization. To address the degraded generalization of training with noisy labels, one direct approach is to reweigh training exam ples [ 24,12,7,17], which is related to curriculum learning. The general idea is to assign important weights to examples with a high chance of being correct. However, there are two major limitations of existing methods. First, imagine an ideal weighting mechanism. It will only focus on the Work done during an internship in Google Cloud AI.selected clean examples. For those incorrectly labeled data samples, the weights should be near zero. If a dataset is under 80% noise corruption, an ideal weighting mechanism assigns nonzero weights to only 20% examples and aban dons the information in a large amount of 80% examples. This leads to an insufÔ¨Åcient usage of training data. Second, previous methods usually need some prior knowledge on the noise ratio or the availability of an additional clean unbiased validation dataset. But it is usually impractical to get this extra information in real applications. Another approach is correctionbased, i.e. estimating the noisy corruption ma trix and correcting the labels [ 22,23,6]. But it is often difÔ¨Åcult to estimate the underlying noise corruption matrix when the number of classes is large. Further, there may not be an underlying ground truth corruption process but an open set of noisy labels in the real world. Although many complex approaches [ 12,24,7] have been proposed to deal with label noise, we Ô¨Ånd that a simple yet effective baseline can achieve surprisingly good performance compared to the strong competing methods. In this paper, we propose to minimize the predictive vari ance, which is an unbiased estimator of Jacobian norm. A model with simpler hypothesis and smoother decision bound aries is assumed to generalize better. Our method is simple yet effective which can take advantage of the whole dataset including the noisy examples to improve the generalization. Our main contributions are: We propose a new strong baseline method for robust ness to noisy labels, which greatly mitigates overÔ¨Åtting and should not be omitted in the label noise community. A thorough empirical evaluation on various datasets (e.g., CIFAR10, CIFAR100, ImageNet) is conducted and demonstrates signiÔ¨Åcant improvements over previ ous competing methods. We also apply our method to a largescale realworld noisy dataset, Webvision [ 15], and establish the new stateoftheart results. We show that the variancebased regularizer is an un biased estimator of Jacobian norm and analyze the re liability of this estimator. Its good performance is due to that Jacobian norm correlates with generalization. 1arXiv:1909.09338v2  [cs.LG]  27 Sep 2019The method can be applied to any neural network archi tecture. Additional knowledge on the clean validation dataset is not required. Empirically we Ô¨Ånd that our method learns a model with lower subspace dimensionality and lower complexity, which are the indicators of better generalization. 2. Related work "
5,Large-Scale Pre-training for Person Re-identification with Noisy Labels.txt,"This paper aims to address the problem of pre-training for person
re-identification (Re-ID) with noisy labels. To setup the pre-training task, we
apply a simple online multi-object tracking system on raw videos of an existing
unlabeled Re-ID dataset ""LUPerson"" nd build the Noisy Labeled variant called
""LUPerson-NL"". Since theses ID labels automatically derived from tracklets
inevitably contain noises, we develop a large-scale Pre-training framework
utilizing Noisy Labels (PNL), which consists of three learning modules:
supervised Re-ID learning, prototype-based contrastive learning, and
label-guided contrastive learning. In principle, joint learning of these three
modules not only clusters similar examples to one prototype, but also rectifies
noisy labels based on the prototype assignment. We demonstrate that learning
directly from raw videos is a promising alternative for pre-training, which
utilizes spatial and temporal correlations as weak supervision. This simple
pre-training task provides a scalable way to learn SOTA Re-ID representations
from scratch on ""LUPerson-NL"" without bells and whistles. For example, by
applying on the same supervised Re-ID method MGN, our pre-trained model
improves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%,
2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or
few-shot setting, the performance gain is even more significant, suggesting a
better transferability of the learned representation. Code is available at
https://github.com/DengpanFu/LUPerson-NL","A large highquality labeled dataset for person re identification (ReID) is labor intensive and costly to cre ate. Existing fully labeled datasets [25, 52, 58, 61] for per son ReID are all of limited scale and diversity compared to other vision tasks. Therefore, model pretraining be *Corresponding author. (a) Market1501 with MGN  (b) Market1501 with IDE (c) DukeMTMC with MGN  (d) DukeMTMC with IDE Figure 1. Comparing person ReID performances of three pre trained models on two methods (IDE [59] and MGN [51]). Re sults are reported on Market1501 and DukeMTC, with different scales under the smallscale setting. IN.sup. refers to the model supervised pretrained on ImageNet, LUP .unsup. is the model un supervised pretrained on LUPserson, and LUPnl.pnl. is the model pretrained on our LUPersonNL dataset using our proposed PNL. comes a crucial approach to achieve good ReID perfor mance. However, due to the lack of largescale ReID dataset, most previous methods simply use the models pre trained on the crowdlabeled ImageNet dataset, resulting in a limited improvement because of the big domain gap be tween generic images in ImageNet and personfocused im ages desired by the ReID task. To mitigate this problem, the recent work [12] has demonstrated that unsupervised pretraining on a webscale unlabeled ReID image dataset ‚ÄúLUPerson‚Äù (subsampled from massive streeview videos) surpasses that of pretraining on ImageNet. In this paper, our hypothesis is that scalable ReID pre training methods that learn directly from raw videos cangenerate better representations . To verify it, we propose the noisy labels guided person ReID pretraining , which lever ages the spatial and temporal correlations in videos as weak supervision. This supervision is nearly costfree, and can be achieved by the tracklets of a person over time derived from any multiobject tracking algorithm, such as [56]. In par ticular, we track each person in consecutive video frames, and automatically assign the tracked persons in the same tracklet to the same ReID label and vice versa. Enabled by the large amounts of raw videos in LUPerson [12], publicly available data of this form on the internet, we create a new variant named ‚ÄúLUPerson NL‚Äù with derived pseudo ReID labels from tracklets for pretraining with noisy labels. This variant totally consists of 10Mperson images from 21K scenes with noisy labels of about 430Kidentities. We demonstrate that contrastive pretraining of ReID is an effective method of learning from this weak supervision at large scale. This new Pretraining framework utilizing Noisy Labels ( PNL ) composes three learning modules: (1) a simple supervised learning module directly learns from ReID labels through classification; (2) a prototypebased contrastive learning module helps cluster instances to the prototype which is dynamically updated by moving aver aging the centroids of instance features, and progressively rectify the noisy labels based on the prototype assignment. and (3) a labelguided contrastive learning module utilizes the rectified labels subsequently as the guidance. In contrast to the vanilla momentum contrastive learning [7,12,19] that treats only features from the same instance as positive sam ples, our labelguided contrastive learning uses the rectified labels to distinguish positive and negative samples accord ingly, leading to a better performance. In principle, joint learning of these three modules make the consistency be tween the prototype assignment from instances and the high confident (rectified) labels, as possible as it can. The experiments show that our PNL model achieves re markable improvements on various person ReID bench marks. Figure 1 indicates that the performance gain from our pretrained models is consistent on different scales of training data. For example, upon the strong MGN [51] baseline, our pretrained model improves the mAP by 4.4%,4.9%on Market1501 and DukeMTMC over the Im ageNet supervised one, and 0.9%,2.2%over the unsuper vised pretraining baseline [12]. Moreover, the gains are even larger under the smallscale and fewshot settings, where the labeled ReID data are extremely limited. To the best of our knowledge, we are the first to show that large scale noisy label guided pretraining can significantly ben efit person ReID task. Our key contributions can be summarized as follows: ‚Ä¢ We propose noisy label guided pretraining for person Re ID, which incorporates supervised learning, prototype based contrastive learning, labelguided contrastive learning and noisy label rectification to a unified framework. ‚Ä¢ We construct a largescale noisy labeled person ReID dataset ‚ÄúLUPersonNL‚Äù as a new variant of ‚ÄúLUPerson‚Äù. It is by far the largest noisy labeled person ReID dataset without any human labeling effort. ‚Ä¢ Our models pretrained on LUPersonNL push the state oftheart results on various public benchmarks to a new limit without bells and whistles. 2. Related Work "
6,Empirical Error Modeling Improves Robustness of Noisy Neural Sequence Labeling.txt,"Despite recent advances, standard sequence labeling systems often fail when
processing noisy user-generated text or consuming the output of an Optical
Character Recognition (OCR) process. In this paper, we improve the noise-aware
training method by proposing an empirical error generation approach that
employs a sequence-to-sequence model trained to perform translation from
error-free to erroneous text. Using an OCR engine, we generated a large
parallel text corpus for training and produced several real-world noisy
sequence labeling benchmarks for evaluation. Moreover, to overcome the data
sparsity problem that exacerbates in the case of imperfect textual input, we
learned noisy language model-based embeddings. Our approach outperformed the
baseline noise generation and error correction techniques on the erroneous
sequence labeling data sets. To facilitate future research on robustness, we
make our code, embeddings, and data conversion scripts publicly available.","Deep learning models have already surpassed humanlevel performance in many Natural Lan guage Processing (NLP) tasks1. Sequence labeling systems have also reached extremely high accu racy (Akbik et al., 2019; Heinzerling and Strube, 2019). Still, NLP models often fail in scenarios, where nonstandard text is given as input (Heigold et al., 2018; Belinkov and Bisk, 2018). NLP algorithms are predominantly trained on errorfree textual data but are also employed to pro cess usergenerated text (Baldwin et al., 2013; Der czynski et al., 2013) or consume the output of prior Optical Character Recognition (OCR) or Auto matic Speech Recognition (ASR) processes (Miller et al., 2000). Errors that occur in any upstream 1GLUE benchmark (Wang et al., 2018a): https:// gluebenchmark.com/leaderboard Training Loss Sailing is a passion. Sailing 1s o passion. Seq2Seq ModelFigure 1: Our modiÔ¨Åcation of the NAT approach (green boxes). We propose a learnable seq2seqbased error generator and retrain FLAIR embeddings using noisy text to improve the accuracy of noisy neural sequence labeling.  is a process that induces noise to the input xproducing erroneous ~x.E(x)is an embedding matrix. F(x)is a sequence labeling model. e(x)ande(~x)are the embeddings of xand~x, respectively. y(x)andy(~x) are the outputs of the model for xand~x, respectively. component of an NLP system deteriorate the accu racy of the target downstream task (Alex and Burns, 2014). In this paper, we focus on the problem of per forming sequence labeling on the text produced by an OCR engine. Moreover, we study the trans ferability of the methods learned to model OCR noise to the distribution of the humangenerated errors. Both misrecognized and mistyped text pose a challenge for the standard models trained using errorfree data (Namysl et al., 2020). We make the following contributions (Figure 1): ‚Ä¢We propose a noise generation method for OCR that employs a sequencetosequence (seq2seq) model trained to translate from errorfree to erroneous text ( ¬ß4.1). Our ap proach improves the accuracy of noisy neu ral sequence labeling compared to prior work (¬ß6.1). ‚Ä¢We present an unsupervised parallel training data generation method that utilizes an OCR engine ( ¬ß4.2). Similarly, realistic noisy ver sions of popular sequence labeling data sets can be synthesized for evaluation (¬ß5.5).arXiv:2105.11872v1  [cs.CL]  25 May 2021‚Ä¢We exploit erroneous text to perform Noisy Language Modeling (NLM; ¬ß4.5). Our NLM embeddings further improve the accuracy of noisy neural sequence labeling ( ¬ß6.3), also in the case of the humangenerated errors ( ¬ß6.4). ‚Ä¢To facilitate future research on robustness, we integrate our methods into the NoiseAware Training (NAT) framework (Namysl et al., 2020) and make our code, embeddings, and data conversion scripts publicly available.2 2 Related Work "
7,Improving patch-based scene text script identification with ensembles of conjoined networks.txt,"This paper focuses on the problem of script identification in scene text
images. Facing this problem with state of the art CNN classifiers is not
straightforward, as they fail to address a key characteristic of scene text
instances: their extremely variable aspect ratio. Instead of resizing input
images to a fixed aspect ratio as in the typical use of holistic CNN
classifiers, we propose here a patch-based classification framework in order to
preserve discriminative parts of the image that are characteristic of its
class. We describe a novel method based on the use of ensembles of conjoined
networks to jointly learn discriminative stroke-parts representations and their
relative importance in a patch-based classification scheme. Our experiments
with this learning procedure demonstrate state-of-the-art results in two public
script identification datasets. In addition, we propose a new public benchmark
dataset for the evaluation of multi-lingual scene text end-to-end reading
systems. Experiments done in this dataset demonstrate the key role of script
identification in a complete end-to-end system that combines our script
identification method with a previously published text detector and an
off-the-shelf OCR engine.","Script and language identication are important steps in modern OCR systems designed for multilanguage environments. Since text recognition al gorithms are languagedependent, detecting the script and language at hand allows selecting the correct language model to employ [1]. While script iden tication has been widely studied in document analysis [2, 3], it remains an almost unexplored problem for scene text. In contrast to document im ages, scene text presents a set of specic challenges, stemming from the high variability in terms of perspective distortion, physical appearance, variable illumination and typeface design. At the same time, scene text comprises typically a few words, contrary to longer text passages available in document images. Current endtoend systems for scene text reading [4, 5, 6] assume single script and language inputs given beforehand, i.e. provided by the user, or inferred from available metadata. The unconstrained text understanding problem for large collections of images from unknown sources has not been considered up to very recently [7, 8, 9, 10, 11]. While there exists some previ ous research in script identication of text over complex backgrounds [12, 13], such methods have been so far limited to video overlaidtext, which presents in general dierent challenges than scene text. This paper addresses the problem of script identication in natural scene images, paving the road towards true multilingual endtoend scene text 2Figure 1: Collections of images from unknown sources may contain textual information in dierent scripts. understanding. Multiscript text exhibits high intraclass variability (words written in the same script vary a lot) and high interclass similarity (certain scripts resemble each other). Examining text samples from dierent scripts, it is clear that some strokeparts are quite discriminative, whereas others can be trivially ignored as they occur in multiple scripts. The ability to distinguish these relevant strokeparts can be leveraged for recognising the corresponding script. Figure 2 shows an example of this idea. Figure 2: (best viewed in color) Certain strokeparts (in green) are discrimi native for the identication of a particular script (left), while others (in red) can be trivially ignored because are frequent in other classes (right). The use of state of the art CNN classiers for script identication is not straightforward, as they fail to address a key characteristic of scene text instances: their extremely variable aspect ratio. As can be seen in Figure 3, scene text images may span from single characters to long text sentences, and thus resizing images to a xed aspect ratio, as in the typical use of holistic CNN classiers, will deteriorate discriminative parts of the image that are characteristic of its class. The key intuition behind the proposed method is 3that in order to retain the discriminative power of stroke parts we must rely in powerful local feature representations and use them within a patchbased classier. In other words, while holistic CNNs have superseded patchbased methods for image classication, we claim that patchbased classiers can still be essential in tasks where image shrinkage is not feasible. Figure 3: Scene text images with the larger/smaller aspect ratio available in three dierent datasets: MLe2e(left), SIW13(center), and CVSI(right). In previously published work [10] we have presented a method combining convolutional features, extracted by sliding a window with a single layer Convolutional Neural Network (CNN) [14], and the NaiveBayes Nearest Neighbour (NBNN) classier [15] with promising results. In this paper we demonstrate far superior performance by extending our previous work in two dierent ways: First, we use deep CNN architectures in order to learn more discriminative representations for the individual image patches; Second, we propose a novel learning methodology to jointly learn the patch representa tions and their importance (contribution) in a global image to class proba bilistic measure. For this, we train our CNN using an Ensemble of Conjoined Networks and a loss function that takes into account the global classication error for a group of Npatches instead of looking only into a single image patch. Thus, at training time our network is presented with a group of N patches sharing the same class label and produces a single probability distri bution over the classes for all them. This way we model the goal for which the network is trained, not only to learn good local patch representations, 4but also to learn their relative importance in the global image classication task. Experiments performed over two public datasets for scene text classica tion demonstrate stateoftheart results. In particular we are able to reduce classication error by 5 percentage points in the SIW13 dataset. We also introduce a new benchmark dataset, namely the MLe2e dataset, for the eval uation of scene text endtoend reading systems and all intermediate stages such as text detection, script identication and text recognition. The dataset contains a total of 711 scene images, and 1821 text line instances, covering four dierent scripts (Latin, Chinese, Kannada, and Hangul) and a large variability of scene text samples. 2. Related Work "
8,Learning from Noisy Labels via Dynamic Loss Thresholding.txt,"Numerous researches have proved that deep neural networks (DNNs) can fit
everything in the end even given data with noisy labels, and result in poor
generalization performance. However, recent studies suggest that DNNs tend to
gradually memorize the data, moving from correct data to mislabeled data.
Inspired by this finding, we propose a novel method named Dynamic Loss
Thresholding (DLT). During the training process, DLT records the loss value of
each sample and calculates dynamic loss thresholds. Specifically, DLT compares
the loss value of each sample with the current loss threshold. Samples with
smaller losses can be considered as clean samples with higher probability and
vice versa. Then, DLT discards the potentially corrupted labels and further
leverages supervised learning techniques. Experiments on CIFAR-10/100 and
Clothing1M demonstrate substantial improvements over recent state-of-the-art
methods.
  In addition, we investigate two real-world problems for the first time.
Firstly, we propose a novel approach to estimate the noise rates of datasets
based on the loss difference between the early and late training stages of
DNNs. Secondly, we explore the effect of hard samples (which are difficult to
be distinguished) on the process of learning from noisy labels.","Although deep neural networks (DNNs) have achieved great success for image classiÔ¨Åcation tasks [10, 16], their excellent performance mainly relies on largescale datasets with clean label annotations. However, it is extremely ex pensive and timeconsuming to label highquality datasets, *Equal Contribution. This work was done at Huawei Technologies. ‚Ä†Correspondence to: MinLing Zhang (zhangml@seu.edu.cn) and Xin Geng (xgeng@seu.edu.cn). 50 100 150 200 250 300 Epoch0.00.51.01.52.02.53.0LossClean Noisy(a) 50 100 150 200 250 300 Epoch0.02.55.07.510.012.515.017.520.022.5LossClean Noisy (b) Figure 1: Crossentropy loss on CIFAR10 under 50% sym metric label noise. (a) Training with crossentropy loss re sults in Ô¨Åtting the noisy labels. (b) Using DLT avoids Ô¨Åtting label noise. The bold lines represent the means of losses and the shaded areas are the ranges of all samples. thus deep models are usually trained on data with lots of corrupted labels. As a result, dealing with label noise is a common adverse scenario which requires attention and has been extensively studied these years [1, 9, 13, 19]. A recent study on the generalization capabilities of deep networks [39] demonstrates that DNNs can easily overÔ¨Åt to noisy labels and result in poor generalization performance. However, even though deep networks can Ô¨Åt everything in the end, they learn patterns Ô¨Årst [3], and this suggests that DNNs gradually memorize the data, moving from correct data to mislabeled data. As shown in Figure 1(a), DNNs Ô¨Åt the correctly labeled samples (clean samples) before Ô¨Åtting noisy samples, resulting in notably larger loss values for noisy samples in the early training stage. Existing methods for learning from noisy labels can be grouped into three main categories. The Ô¨Årst one is based on label correction which aims to correct noisy labels to the groundtruth ones [20, 32, 33]. For example, the recent proposed PENCIL [37] utilizes backpropagation to correct image labels and update the network parameters simultane ously in an endtoend manner. The second one is based on the robust loss function [21, 35, 41]. Ghoshet al. [6] proves that the loss functions which satisfy the symmetricarXiv:2104.02570v1  [cs.LG]  1 Apr 2021condition, such as Mean Absolute Error (MAE), would be inherently tolerant to both uniform and class conditional la bel noise. The third one is based on sample selection which involves selecting correctly labeled samples from a noisy training dataset [1, 11, 19]. Coteaching [9, 38] is a repre sentative framework on this line which trains two networks where each network selects smallloss samples to teach an other one. In light of these recent advances, we propose dynamic loss thresholding (DLT) to avoid Ô¨Åtting noisy labels when training deep models, as shown in Figure 1(b). SpeciÔ¨Åcally, DLT records the loss value of each sample during training and calculates loss thresholds dynamically. By comparing the loss value of each training sample with the current loss threshold, we expect to distinguish the potentially clean and noisy samples during training. Samples with smaller losses can be considered as correctly labeled samples and vice versa. Then, DLT discards these potentially corrupted la bels and treats the corresponding samples as unlabeled data, thus we can leverage semisupervised learning techniques likeMixup [40] to improve the performance. Experiments on various benchmarks demonstrate substantial improve ments over recent stateoftheart methods. Furthermore, DLT is also of excellent generalization and Ô¨Çexibility and we empirically demonstrate that our noisy label detection method is still effective when combining with other meth ods. In addition, the noise rates of datasets are always un known but crucial to many realworld situations due to the fact that numbers of existing methods need the noise rate as their prior knowledge. In this paper, we further propose a novel method to estimate the noise rate based on the pat terns of losses. In particular, our method calculates the loss difference between the early and late deep network train ing stages and dynamically Ô¨Åts a Gaussian Mixture Model (GMM) on persample loss difference to divide the training samples into a clean set and a noisy set. Accordingly, noise rate can be obtained by calculating the proportion of clean samples to total samples. In realworld situations, there exists a common class of samples which are always quite close to the decision bound ary and hence difÔ¨Åcult to be distinguished by DNNs. Intu itively, we call these samples hard samples . Hard samples can be deÔ¨Åned as a subset of clean samples but easy to be mistaken for other classes. However, nearly no research dis cussed hard samples in the Ô¨Åeld of learning from noisy la bels to our knowledge. By means of our proposed method, we investigate the effect of hard samples during deep net work training for the Ô¨Årst time. In summary, our main con tributions are as follows: ‚Ä¢ We propose a novel noisy label detection method DLT, which is based on dynamic loss thresholds . Combined with semisupervised learning techniques, our wholeframework achieves stateoftheart performance. We experimentally show that DLT is of excellent general ization and Ô¨Çexibility. ‚Ä¢ We provide a method to estimate the noise rates of datasets based on loss difference . This is a key com plement to eliminate the dependence on using noise rate as common prior knowledge. ‚Ä¢ We provide insights into the effect of hard samples on the training of DNNs. We do this for not only clari fying the effectiveness of our method, but also Ô¨Ånding out the patterns of hard samples when learning from noisy labels. 2. Related Work "
9,Addressing Ambiguity of Emotion Labels Through Meta-Learning.txt,"Emotion labels in emotion recognition corpora are highly noisy and ambiguous,
due to the annotators' subjective perception of emotions. Such ambiguity may
introduce errors in automatic classification and affect the overall
performance. We therefore propose a dynamic label correction and sample
contribution weight estimation model. Our model is based on a standard BLSTM
model with attention with two extra parameters. The first learns a new
corrected label distribution, and is aimed to fix the inaccurate labels from
the dataset. The other instead estimates the contribution of each sample to the
training process, and is aimed to ignore the ambiguous and noisy samples while
giving higher weight to the clear ones. We train our model through an
alternating optimization method, where in the first epoch we update the neural
network parameters, and in the second we keep them fixed to update the label
correction and sample importance parameters. When training and evaluating our
model on the IEMOCAP dataset, we obtained a weighted accuracy (WA) and
unweighted accuracy (UA) of respectively 65.9% and 61.4%. This yielded an
absolute improvement of 2.5%, 2.7% respectively compared to a BLSTM with
attention baseline, trained on the corpus gold labels.","Automatic recognition of affect and emotion is important to enable a more natural and engaging communication between humans an d machines. In this work we concentrate on emotion recognitio n from speech, which is the task to estimate the emotional content o f a spo ken utterance. In the past, emotion recognition was performed by extractin g a set of lowlevel features from each frame of an audio sample. These features were then aggregated through various statistical aggrega tion function (mean, standard deviation, min, max, etc.) to a global utterancelevel vector representation [1], to be Ô¨Ånally fe d through a shallow classiÔ¨Åer such as Support Vector Machines (SVM) [2, 3]. However, in recent years, the accuracy of speech emotion rec ogni tion has dramatically improved with the introduction of Dee p Neural Networks (DNN). Initial DNNbased models [4] were still bas ed on the same utterancelevel feature extraction. However, in s ubsequent approaches, speech features extracted from each frame were used as inputs of more complex neural network architectures such as Con volutional Neural Networks (CNN) and Recurrent Neural Netw orks (RNN), and the accuracy was further improved [5, 6, 7]. Recen t years saw the application of novel methods developed from ot her AI Ô¨Åelds, such as selfattention models [8], Connectionist Temporal ClassiÔ¨Åcation (CTC) [9] and Dilated Residual Network (DRN) [10]. Even higher performance was achieved by employing multimo dal information, such as audio and image together with speech [1 1].While most of the effort concentrated on the development of more accurate classiÔ¨Åcation models, there were other aspec ts of emotion classiÔ¨Åcation regarding the data itself that were m ostly ignored, but that could help improving the performance. In m any datasets, the emotional labels are annotated based on human anno tators‚Äô perception and sensibility to emotion. Emotion per ception is highly subjective [12], therefore the labels often contain some noise due to humans‚Äô decision ambiguity. For instance, an annotat or may assign the label neutral not when the sample is actually neutral, but when he is unsure about the most correct emotion class. Likew ise, he may mistakenly recognize some loud enthusiastic speech a s an gry, while instead it is happy. Training a model on such noisy labels is likely the cause of some performance degradation, becaus e the model may become confused and may not clearly distinguish on e emotion from another. Another important issue is that, in many emotion recognitio n datasets, the numbers of utterances for each emotional cate gory are imbalanced. Generally, in the classiÔ¨Åcation task using the se category imbalanced dataset, accuracy of the small class is decrease d [13, 14], which in turn affects the overall accuracy. To overcome thes e prob lems, some methods were proposed to employ soft target appro aches to correct the annotation ambiguities [15], or to augment th e dataset with synthetic data to reduce the effect of data imbalance [1 6]. How ever, the former method only performs a static label contrib ution estimation based on the original annotation data, while the latter method is complex and the generated data might still be affec ted by the original labeling noise. In other domains, such as image recogni tion, similar issues were tackled by performing the label up date, not a priori but during training, by gradually tuning the estima tion [17]. Inspired by the achievements in image recognition [17], we propose a method to automatically tune the contribution of e ach data sample during training. We do this by alternately updat ing the parameters of a DNN emotion classiÔ¨Åcation model, and then us e the neural network prediction to correct the relative contr ibution and the target labels of each sample, in order to reduce the ov erall loss. The main purpose is to correct or ignore altogether the am biguously labeled utterances, while giving higher importa nce to the clear and unambiguous ones. Results obtained in the Interac tive Emotional Dyadic Motion Capture (IEMOCAP) dataset [18] sho w that our proposed method is effective in removing the annota tion noise. It achieves an improvement of 2.5% for weighted accur acy, and of 2.7% for unweighted accuracy compared to a stateoft heart BLSTM model trained on the original labels only [7]. 2. METHODOLOGY "
10,Adversarial Partial Multi-Label Learning.txt,"Partial multi-label learning (PML), which tackles the problem of learning
multi-label prediction models from instances with overcomplete noisy
annotations, has recently started gaining attention from the research
community. In this paper, we propose a novel adversarial learning model,
PML-GAN, under a generalized encoder-decoder framework for partial multi-label
learning. The PML-GAN model uses a disambiguation network to identify noisy
labels and uses a multi-label prediction network to map the training instances
to the disambiguated label vectors, while deploying a generative adversarial
network as an inverse mapping from label vectors to data samples in the input
feature space. The learning of the overall model corresponds to a minimax
adversarial game, which enhances the correspondence of input features with the
output labels in a bi-directional mapping. Extensive experiments are conducted
on multiple datasets, while the proposed model demonstrates the
state-of-the-art performance for partial multi-label learning.","In partial multilabel learning (PML), each training instance is assigned multiple candidate labels which are only partially relevant; that is, some irrelevant noise labels are assigned together with the groundtruth labels. As it is typically difÔ¨Åcult and costly to precisely annotate instances for multilabel data [ 22], the task of PML naturally arises in many realworld scenarios with crowdsource annotations. In such a scenario, in order to collect the complete set of positive labels for each data instance, one can gather all labels provided by multiple annotators to form the candidate label set, which is usually overcomplete and contains additional noisy labels beyond all the true labels, leading to the PML problem. Figure 1 presents such an example of overcompletely annotated training image for object recognition, where the candidate labels provided by crowdsource annotators cover all the ground truth labels (in black color) and some irrelevant noise labels (in red color). PML is much more challenging than standard multilabel learning as the true labels are hidden among irrelevant labels and the number of true labels is unknown. The goal of PML is to learn a good multilabel prediction model from such a partial label training set, and hence reduce the annotation cost. An intuitive strategy of PML is to treat all candidate labels as relevant ground truth, thus any offthe shelf multilabel classiÔ¨Åcation method can be adapted to induce an expected multilabel predictor [29]. This strategy, though simple, cannot work well since taking the noise labels as part of the true labels will mislead the multilabel training and induce inferior prediction models. The PML work in [ 22] assumes that each candidate label has a conÔ¨Ådence score of being a true label, and learns the conÔ¨Ådence scores and the classiÔ¨Åer in an alternative manner by minimizing a conÔ¨Ådence weighted ranking loss. Although this work yields some reasonable results, the estimation of label conÔ¨Ådence scores is errorprone, especially when noise labels dominate, which can seriously impair the classiÔ¨Åer‚Äôs performance. The recent work in [ 23] proposes to perform groundtruth label recovery and noise label identiÔ¨Åcation simultaneously by exploring the label correlations and the relationships between the noise labels and feature representations. Another recent work in [ 5] presents a two Preprint. Under review.arXiv:1909.06717v2  [cs.LG]  5 Jun 2020Figure 1: An annotated im age under the partial multilabel learning (PML) setting. Figure 2: The proposed PMLGAN model. It has four com ponent networks: generator G, disambiguator eD, predictor F, and discriminator D. stage PML method. It estimates the conÔ¨Ådence values of the candidate labels using iterative label propagation and then chooses the highly conÔ¨Ådent candidate labels as credible labels to induce a multi label prediction model. This work however suffers from the cumulative errors induced in propagation, which can impact the label conÔ¨Ådence estimation and consequently impair the prediction. In this paper, we propose a novel adversarial learning model, PMLGAN, under a generalized encoderdecoder framework to tackle the partial multilabel learning problem. The PMLGAN model comprises four component networks: a disambiguation network that predicts the probability of each candidate label being an additive noise for a training instance; a prediction network that predicts the disambiguated true labels of each instance from its input features; a generation network that generates samples in the feature space given latent vectors in the label space; and a discrimination network that separates the generated samples from the real data. The prediction network and disambiguation network together form an encoder that maps data samples in the input feature space to the disambiguated label vectors, while the generation network and discrimination network form a generative adversarial network (GAN) as an inverse decoding mapping from vectors in the multilabel space to samples in the input feature space. The learning of the overall model corresponds to a minimax adversarial game, which enhances the correspondence of input features with the output labels through the bidirectional encoderdecoder mapping mechanism, and consequently boosts multilabel prediction performance. To the best of our knowledge, this is the Ô¨Årst work that exploits a generative adversarial model based bidirectional mapping mechanism for PML. We conduct extensive experiments on multiple multilabel datasets under partial multilabel learning setting. The empirical results show the proposed PMLGAN yields the stateoftheart PML performance. 2 Related Work "
11,Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels.txt,"Noisy training set usually leads to the degradation of generalization and
robustness of neural networks. In this paper, we propose using a theoretically
guaranteed noisy label detection framework to detect and remove noisy data for
Learning with Noisy Labels (LNL). Specifically, we design a penalized
regression to model the linear relation between network features and one-hot
labels, where the noisy data are identified by the non-zero mean shift
parameters solved in the regression model. To make the framework scalable to
datasets that contain a large number of categories and training data, we
propose a split algorithm to divide the whole training set into small pieces
that can be solved by the penalized regression in parallel, leading to the
Scalable Penalized Regression (SPR) framework. We provide the non-asymptotic
probabilistic condition for SPR to correctly identify the noisy data. While SPR
can be regarded as a sample selection module for standard supervised training
pipeline, we further combine it with semi-supervised algorithm to further
exploit the support of noisy data as unlabeled data. Experimental results on
several benchmark datasets and real-world noisy datasets show the effectiveness
of our framework. Our code and pretrained models are released at
https://github.com/Yikai-Wang/SPR-LNL.","Deep learning has achieved remarkable success on many topics of supervised learning with millions of labeled training data. The performance heavily relies on the quality of label annotation since neural networks are susceptible to noisy labels and even can easily memorize randomly labeled annotations [63], leading to the degradation of generalization and robustness. In many realworld scenarios, it is expensive and difÔ¨Åcult to obtain precise labels, exposing a realistic challenge for supervised deep models to learn with noisy data. *Corresponding author.There is a large literature for this challenge from various perspectives, including modifying the network architectures [6, 12, 13, 59] or loss functions [11, 27, 53, 65], or dynamically selecting clean data during training [5, 14, 17, 27,34,40,44,61]. Particularly, the dynamic sample selection methods adopt the spirit of providing only clean data for the training. Such a spirit can form a ‚Äòvirtuous‚Äô cycle between the noisy data elimination and network training: the elimination of noisy data can help the network training; and on the other hand, the improved network is empowered with a better ability in picking up clean data. As this virtuous cycle evolves, the performance can be improved. Typical principles to identify outliers include large loss [14], inconsistent prediction [67], and irregular feature representation [57]. The former two principles focus on the label space, while the last one focuses on the feature space of the same class. In this paper, we unify the label and feature space and assume linear relationship between the featurelabel pair (denoted as (xi;yi)) of dataiby yi=x> i+""; (1) wherexi2Rpis the feature vector, and yi2Rcis the onehot label vector; 2Rpcis the Ô¨Åxed (unknown) coefÔ¨Åcient matrix and ""2Rcis random noise. This linear relation is approximately established as the networks are trained to minimize the divergence between a (softmax) linear projection of the feature and onehot label vector. For a welltrained network, the output prediction of clean data is expected to be as similar to a onehot vector as possible, while for noisy data the output is dense. Intuitively, when the linear relation is wellapproximated without softmax operation, the corresponding data is likely to be clean data. The simplest way to identify the suspected outliers in the linear model is checking the predict error, or residual, ri=yi x> i^, where ^is the estimate of . The largerkrkindicates more possibility for the instance ito be outlier/noisy data. The classical statistical method to test whether the instance riis nonzero is using the leaveone 1arXiv:2203.07788v2  [cs.LG]  19 Mar 2022out approach [38] to test the externally studentized residual ti=yi x> i^ i ^ i 1 +x> i  X>  iX i 1xi1=2; (2) where ^is the scale estimate and the subscript  iindicates estimates based on the n 1observations, leaving out theith data where we are testing. Equivalently, the linear regression model can be reformulated into explicitly representing the residual by the meanshift parameter  as in [39], Y=X+ +""; ""i;jN(0;2); (3) where we have the feature X2Rnp, and labelY2Rnc paired and stacked by rows; and each row of  2Rnc,  i, represents the predict residual of the corresponding data. This formulation has been widely studied in different research topics, including economics [4, 18, 32, 33], robust regression [8, 39], statistical ranking [9], face recognition [56], semisupervised fewshot learning [54, 55], and Bayesian preference learning [43], to name a few. The focused formulation is different depending on the speciÔ¨Åc research tasks. For example, for the robust regression problem, the target is to get a robust estimate ^ against the inÔ¨Çuence of  . Here for solving the problem of learning with noisy labels, we instead aim to amplify the impact of such that nonzero values can represent the noisy label that existed in the training set. To this end, from the statistical perspective, this paper starts from Eq. (3) to build up a sample selection framework, dubbed Scalable Penalized Regression (SPR), which has theoretical guarantees of consistently identifying noisy data, and thus can efÔ¨Åciently learn with noisy labels. Naturally, we expect  in Eq. (3) to be sparse and only a small number of  iare nonzeros, indicating that those data are noisy or outlying. Thus a sparse penalty is utilized on  ito encourage that the nonzero solution is restricted in a small portion. We thus optimize the induced penalized regression problem to solve  and identify the instances with nonzero  ias noisy data. Theoretically, in terms of the model selection consistency theory [51, 66], there is some nice statistical property and theoretical insight in our SPR framework, as we can guarantee that, by meeting certain conditions, our SPR should at least in principle, successfully identify all the noisy data. To incorporate Eq. (3) into the endtoend training pipeline of deep architecture, the simplest way is to solve Eq. (3) for each training minibatch to detect and remove noisy data. However, when we train large model with small batch size, the information of current minibatch may not be identiÔ¨Åable enough to distinguish true pattern from noise.On the other hand, use SPR on the whole training data after training an epoch leads to an unacceptable computation cost due to the quadratically increased complexity of solving Eq. (3) with the training data. To design a proper optimization environment for solving Eq. (3) that is data efÔ¨Åcient and identiÔ¨Åable, we utilize the whole training set and propose a split algorithm to divide it into small pieces that are class balance with proper data size such that the noisy pattern is identiÔ¨Åable and can be solved efÔ¨Åciently in parallel, making SPR scalable to large datasets. Inspired by [69], to further encourage the linear relation between features and labels, we propose using a sparse penalty on the fullyconnected output before it is soft maxed. Moreover, we utilize SPR to train the network in a semisupervised manner using CutMix [62], regarding the detected noisy data as unlabeled data to fully utilize the feature information. We conduct extensive experiments to validate the effectiveness of our framework on several benchmark datasets and realworld noisy datasets. Contributions. Our contributions are as follows: ‚Ä¢ We present a statistical approach, SPR, to identify noisy data under a general scenario with theoretical guarantees. ‚Ä¢ A split algorithm is proposed to make SPR scalable to large datasets. ‚Ä¢ A sparse penalty is proposed to encourage the linear relation, and a full training framework that combines SPR with semisupervised methods is designed. ‚Ä¢ Experiments on benchmark datasets and realworld noisy datasets validate the effectiveness of SPR. 2. Related Work "
12,Deep metric learning for multi-labelled radiographs.txt,"Many radiological studies can reveal the presence of several co-existing
abnormalities, each one represented by a distinct visual pattern. In this
article we address the problem of learning a distance metric for plain
radiographs that captures a notion of ""radiological similarity"": two chest
radiographs are considered to be similar if they share similar abnormalities.
Deep convolutional neural networks (DCNs) are used to learn a low-dimensional
embedding for the radiographs that is equipped with the desired metric. Two
loss functions are proposed to deal with multi-labelled images and potentially
noisy labels. We report on a large-scale study involving over 745,000 chest
radiographs whose labels were automatically extracted from free-text
radiological reports through a natural language processing system. Using 4,500
validated exams, we demonstrate that the methodology performs satisfactorily on
clustering and image retrieval tasks. Remarkably, the learned metric separates
normal exams from those having radiological abnormalities.","Chest radiographs are performed to diagnose and monitor a wide range of conditions aecting lungs, heart, bones, and Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation on the Ô¨Årst page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). SAC 2018April 9‚Äì13, 2018, Pau, France Copyright held by the owner/author(s). ACM 9781450351911/18/04. DOI: https://doi.org/10.1145/3167132.3167379soft tissues. Despite being commonly performed, their read ing is challenging and interpretation discrepancies can occur. There is a need to develop machine learning algorithms that can assist the reporting radiologist. In this work we address the problem of learning a distance metric for chest radio graphs using a very large repository of historical exams that have already been reported. An ideal metric should be able to cluster together radiographs presenting similar radiologi cal abnormalities and place them far away from exams with normal radiological appearance. Learning a suitable met ric would enable a variety of applications, from automated retrieval of radiologically similar exams, for teaching and training, to their automated prioritization based on visual patterns. The problem we discuss here is challenging for several rea sons. First, the number of potential abnormalities that can be observed in a chest radiograph can be quite large. Visual patterns detected in radiographs are important cues used by the clinicians when making a diagnosis. Often, during the reporting time, the clinician will describe the visual pattern using descriptors (e.g. \enlarged heart"") or stating the exact medical pathology associated with the visual pattern (e.g. \consolidation in the right lower lobe""). A metric learning algorithm should be able to deal with any such labels and their potential overlaps. Second, the labels may not always be accurate or comprehensive due to the fact that not all the abnormalities are always reported in an image, e.g. due to omissions or when deemed unimportant by the radiologist. When these labels are automatically obtained from freetext reports, as we do in this work, mislabelling errors may also occur. Third, certain abnormalities are less frequently ob served than others, and may not even exist in the training dataset. To support this study, we have prepared a large repository consisting of over 745 ;000 chest radiograph examinations extracted from the PACS (Picture Archiving and Commu nication System) of a large teaching hospital in London. To our knowledge, this is the largest chest radiograph reposi tory to ever be deployed in a machine learning study. Due to the large sample size, manual annotation of all the exams is unfeasible. All the historical freetext reports have been parsed using a Natural Language Processing (NLP) system, which has identied and classied any mention of radiologi cal abnormalities. As a result of this process, each lm has been automatically assigned to one or multiple labels. Our contributions are the following. First, we discuss the probarXiv:1712.07682v1  [stat.ML]  11 Dec 2017A1  The lungs and pleural spaces are  clear. No pneumothorax. The heart is  not enlarged.  Wrong report refers to another xray!  A2  Large leftsided pleural effusion  with almost complete collapse of left  lower lobe. Rightsided thoracostomy  tube.  B1  The heart size is at the upper limits  of normal, the lungs are clear.  B2  The heart is enlarged.  No active  lung lesion.  Figure 1: Examples of pairs of images that are placed close to each other in the learned embedding space shown in Fig. 3. A1 was incorrectly reported, but a second reading shows the presence of pleural eusion and a medical device, which justies its proximity to A2. B1 was labelled as \normal"", but a second reading reveals some degree of cardiomegaly and, as such, the scan is placed close to B2. An extract from the original reports can be found under each image. Fig. 3 contains the legend for the labels. lem of deep metric learning with multilabelled images and propose two versions of a loss function specically designed to deal with overlapping and potentially noisy labels. At the core of the architecture, a DCN is used to learn compact im age representations capturing the visual patterns described by the labels. Second, we report on a largescale evaluation of the proposed methodology using a manually curated sub set of over 4 ;500 exams. Each historical radiological report was reviewed by two independent clinicians who extracted all the labels associated to the lms. We report on compar ative results for two tasks, clustering and image retrieval, and provide evidence that the learned metric can be used to cluster radiographs with a normal appearance as well as clusters of abnormal exams with cooccurring abnormalities. 2. RELATED WORK "
13,Bayesian graph convolutional neural networks for semi-supervised classification.txt,"Recently, techniques for applying convolutional neural networks to
graph-structured data have emerged. Graph convolutional neural networks (GCNNs)
have been used to address node and graph classification and matrix completion.
Although the performance has been impressive, the current implementations have
limited capability to incorporate uncertainty in the graph structure. Almost
all GCNNs process a graph as though it is a ground-truth depiction of the
relationship between nodes, but often the graphs employed in applications are
themselves derived from noisy data or modelling assumptions. Spurious edges may
be included; other edges may be missing between nodes that have very strong
relationships. In this paper we adopt a Bayesian approach, viewing the observed
graph as a realization from a parametric family of random graphs. We then
target inference of the joint posterior of the random graph parameters and the
node (or graph) labels. We present the Bayesian GCNN framework and develop an
iterative learning procedure for the case of assortative mixed-membership
stochastic block models. We present the results of experiments that demonstrate
that the Bayesian formulation can provide better performance when there are
very few labels available during the training process.","Novel approaches for applying convolutional neural net works to graphstructured data have emerged in recent years. Commencing with the work in (Bruna et al. 2013; Henaff, Bruna, and LeCun 2015), there have been numer ous developments and improvements. Although these graph convolutional neural networks (GCNNs) are promising, the current implementations have limited capability to handle uncertainty in the graph structure, and treat the graph topol ogy as groundtruth information. This in turn leads to an in ability to adequately characterize the uncertainty in the pre dictions made by the neural network. In contrast to this past work, we employ a Bayesian framework and view the observed graph as a realization from a parametric random graph family. The observed ad jacency matrix is then used in conjunction with features and labels to perform joint inference. The results reported in this These authors contributed equally to this work. Copyright c 2019, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.paper suggest that this formulation, although computation ally more demanding, can lead to an ability to learn more from less data, a better capacity to represent uncertainty, and better robustness and resilience to noise or adversarial attacks. In this paper, we present the novel Bayesian GCNN framework and discuss how inference can be performed. To provide a concrete example of the approach, we focus on a speciÔ¨Åc random graph model, the assortative mixed mem bership block model. We address the task of semisupervised classiÔ¨Åcation of nodes and examine the resilience of the derived architecture to random perturbations of the graph topology. 2 Related work "
14,TextileNet: A Material Taxonomy-based Fashion Textile Dataset.txt,"The rise of Machine Learning (ML) is gradually digitalizing and reshaping the
fashion industry. Recent years have witnessed a number of fashion AI
applications, for example, virtual try-ons. Textile material identification and
categorization play a crucial role in the fashion textile sector, including
fashion design, retails, and recycling. At the same time, Net Zero is a global
goal and the fashion industry is undergoing a significant change so that
textile materials can be reused, repaired and recycled in a sustainable manner.
There is still a challenge in identifying textile materials automatically for
garments, as we lack a low-cost and effective technique for identifying them.
In light of this, we build the first fashion textile dataset, TextileNet, based
on textile material taxonomies - a fibre taxonomy and a fabric taxonomy
generated in collaboration with material scientists. TextileNet can be used to
train and evaluate the state-of-the-art Deep Learning models for textile
materials. We hope to standardize textile related datasets through the use of
taxonomies. TextileNet contains 33 fibres labels and 27 fabrics labels, and has
in total 760,949 images. We use standard Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs) to establish baselines for this dataset. Future
applications for this dataset range from textile classification to optimization
of the textile supply chain and interactive design for consumers. We envision
that this can contribute to the development of a new AI-based fashion platform.","Clothing and textiles are ubiquitous in our daily lives. Online shopping elevates individuals to a new level of purchasing experience  customised shopping with tons of options from a global market, simple checkout procedures, convenient delivery and returns. According to market estimates, the global fashion emarket is worth $752.5 billion in 2020 [ 9]. This enormous economic value indicates an increased demand for ecommerce services by individuals. These rising demands in the fashion industry motivate the use of Machine Learning (ML) techniques to facilitate lowlevel pixel recognition, midlevel fashion comprehension and highlevel fashion applications [61]. Higherlevel applications, such as outÔ¨Åt recommendations and virtual tryons [ 23], are supported by lowerlevel fashion tasks, e.g.parsing (segmentation) [ 18,32], landmark detection [ 43],etc.A number of works have then developed apparel related datasets for all levels of fashion tasks, including landmark annotations [ 43,32,18], category classiÔ¨Åcation [21, 73, 67], attributes labelling [32, 26], recognitionbased retrieval [31, 23, 22], etc. Despite the development of ML techniques in the fashion industry, the textile industry still faces challenges in its chase of a more sustainable model to reduce the enormous volumes of textile wastes and to meet the global Net Zero goal [ 16,4]. Textile materials play a critical role in garments due to the fact that it is selected based on their particular properties, which may include the level of comfort they provide and the degree to which they can be recycled, etc.[40]. Millions of tons of garments end up in landÔ¨Åll every year [ 48]. Textiles circularity, a novel conceptual model of the circular economy, demonstrates an option for the fashion industry to reduce its carbon footprint and costs, maximise the life of textiles, and minimise waste. Textiles can bearXiv:2301.06160v1  [cs.DL]  15 Jan 2023GARMENTS RAW   MATERIAL FIBRE FABRIC TEXTILES ManufacturePlant Animal Synthetic RegeneratedFibre types cotton ... wool ... polyeste ... viscose ...FIBRE FABRICWoven Knitted NonwovenTwill ...Fabric types Jersey ... Felt ...Outerwear Top Skirt ... Wool Aramids Flax Milk Casein Denim Tweed Velvet TaffetaPlant / Cellulose Animal / Protein Petroleum WasteFigure 1: The general production Ô¨Çow of textiles. Textilesis an umbrella term and it includes raw material, Ô¨Åbre, fabric and garments etc(indicated in dashed box). The dataset labels are generated from two textile material taxonomies: a Ô¨Åbre taxonomy (Figure 2) and a fabric taxonomy (in Supplementary material). Fibres have four macrotypes and we show several Ô¨Åbre examples (Ô¨Çax, wool, aramids, and milk casein) in the Ô¨Ågure. Fabrics have three macrotypes of production methods, we also show several fabric examples (denim, tweed, velvet and taÔ¨Äeta) . reused at many levels, being regenerated into new Ô¨Åbres or utilised textile wastes as energy fuel etc., thereby reducing the carbon footprint [ 45]. It is a recommended practice to ensure that textiles can be traced back to their original source so that the recycling process can be guaranteed. Yet, nowadays, textiles are mostly sorted manually [ 51], despite recent research raise using near infrared spectroscopy (NIR spectroscopy) to recognize textiles for automated garments sorting line [ 10].A lowcost, higheÔ¨Éciency technique for the automatic identiÔ¨Åcation of textile materials in garments is missing, so that the digitised fashion sector would be able to retrieve the materials they are composed of. This would help reduce a large amount of textile wastes and carbon emissions [15]. Given the signiÔ¨Åcance of textile material identiÔ¨Åcation in clothing, it is worth noticing that this identiÔ¨Åcation process can be complicated because Ô¨Åbre and fabric refer to diÔ¨Äerent textile materials. Fibre is the material to make fabric, however, most existing fashion datasets in ML mixed them in the same class. No dataset presently contains organized textile material labels; they do not provide a systematic picture of materialrelated labels [43,21]. They contain partial textile material attributes; their annotation scheme lacks a rationale and is not systematically reviewed by material scientists. Here we propose TextileNet, a material taxonomybased fashion textile dataset to close the gap in current research on textile material identiÔ¨Åcation in clothing. We developed the TextileNet dataset (illustrated in Figure 1 and the detailed illustration is in Figure 2) based on Ô¨Åbre and fabric labels. We achieved this through carefully designing two taxonomies, one for Ô¨Åbres and another for fabrics in close collaboration with material domain experts. We discuss the design details in Section 2.3. The goal of this TextileNet dataset is to contribute to textile material identiÔ¨Åcation in the fashion industry and imagebased textile material retrieval, at the same time, standardize the digitized textile material labelling . TextileNet can be deployed in various domains, including material science, fashion design, retails and the textile supply chain, etc. Our contributions are: ‚Ä¢We present a Ô¨Åbre taxonomy and a fabric taxonomy created in collaboration with material domain experts; thesetaxonomiescontainmacrotypesoftextilesandareextendableforfuturenewÔ¨Åbre/fabric types; ‚Ä¢Using the labels from these taxonomies, we collect and build material taxonomybased fashion datasets for Ô¨Åbre and fabric. The built datasets, named TextileNet, TextileNetÔ¨Åbre contains 33 Ô¨Åbre labels, 27 fabric labels in TextileNetfabric, and have 760,949 images; 2‚Ä¢We present and report two baseline models (CNNs and Vision Transformers) for Ô¨Åbre and fabric classiÔ¨Åcation, both models achieve >80%top5 accuracy on our datasets. 2 Related work "
15,Co-matching: Combating Noisy Labels by Augmentation Anchoring.txt,"Deep learning with noisy labels is challenging as deep neural networks have
the high capacity to memorize the noisy labels. In this paper, we propose a
learning algorithm called Co-matching, which balances the consistency and
divergence between two networks by augmentation anchoring. Specifically, we
have one network generate anchoring label from its prediction on a
weakly-augmented image. Meanwhile, we force its peer network, taking the
strongly-augmented version of the same image as input, to generate prediction
close to the anchoring label. We then update two networks simultaneously by
selecting small-loss instances to minimize both unsupervised matching loss
(i.e., measure the consistency of the two networks) and supervised
classification loss (i.e. measure the classification performance). Besides, the
unsupervised matching loss makes our method not heavily rely on noisy labels,
which prevents memorization of noisy labels. Experiments on three benchmark
datasets demonstrate that Co-matching achieves results comparable to the
state-of-the-art methods.","Deep Neural Networks (DNNs) have shown remarkable performance in a variety of applications [19, 24, 38]. How ever, the superior performance comes with the cost of re quiring a correctly annotated dataset, which is extremely timeconsuming and expensive to obtain in most realworld scenarios. Alternatively, we may obtain the training data with annotations efÔ¨Åciently and inexpensively through ei ther online key search engine [22] or crowdsourcing [49], but noisy labels are likely to be introduced consequently. Previous studies [2, 50] demonstrate that fully memorizing noisy labels affects accuracy of DNNs signiÔ¨Åcantly, hence it is desirable to develop effective algorithms for learning with noisy labels. To handle noisy labels, most approaches focus on esti mating the noise transition matrix [12, 30, 37, 44] or cor recting the label according to model prediction [26, 31, 39, 46]. However, it is challenging to estimate the noise transition matrix especially when the number of classes is large. Another promising direction of study proposes to train two networks on smallloss instances [7, 14, 43, 47], wherein Decoupling [27] and Coteaching+ [47] introduce the ‚ÄúDis agreement‚Äù strategy to keep the two networks diverged to achieve better ensemble effects. However, the instances selected by ‚ÄúDisagreement‚Äù strategy are not guaranteed to have correct labels [14, 43], resulting in only a small por tion of clean instances being utilized in the training process. Coteaching [14] and JoCoR [43] aim to reduce the diver gence between two different networks so that the number of clean labels utilized in each minibatch increases. In the be ginning, two networks with different learning abilities Ô¨Ålter out different types of error. However, with the increase of training epochs, two networks gradually converge to a con sensus and even make the wrong predictions consistently. To address the above concerns, it is essential to keep a balance between divergence and consistency of the two net works. Inspired by augmentation anchoring [5, 35] from semisupervised learning, we propose a method using weak (e.g. using only cropandÔ¨Çip) and strong (e.g. using Ran dAugment [9]) augmentations for two networks respec tively to address the consensus issue. SpeciÔ¨Åcally, one network produces the anchoring labels based on weakly augmented images. The anchoring labels are used as tar gets when the peer network is fed the stronglyaugmented version of the same images. Their difference is captured by an unsupervised matching loss. Stronger augmentation results in disparate predictions, which guarantees the diver gence between two networks, unless they have learned ro bust generalization ability. As earlylearning phenomenon shows that the networks Ô¨Åt training data with clean labels before memorizing the samples with noisy labels [2]. Co matching trains two networks with a loss calculated by in terpolating between two loss terms: 1) A supervised clas siÔ¨Åcation loss encourages learning from clean labels during the earlylearning phase. 2) An unsupervised matching loss limits the divergence of two networks and prevents mem orization of noisy labels after the earlylearning phase. In each training step, we use the smallloss trick to select the most likely clean samples, thus ensuring the error Ô¨Çow fromarXiv:2103.12814v1  [cs.CV]  23 Mar 2021the biased selection would not be accumulated. To show that Comatching improves the robustness of deep learning on noisy labels, we conduct extensive exper iments on both synthetic and realworld noisy datasets, in cluding CIFAR10, CIFAR100 and Clothing1M datasets. Experiments show that Comatching signiÔ¨Åcantly advances stateoftheart results with different types and levels of la bel noise. Besides, we study the impact of data augmen tation and provide ablation study to examine the effect of different components in Comatching. 2. Related work "
16,Ensembling Neural Networks for Improved Prediction and Privacy in Early Diagnosis of Sepsis.txt,"Ensembling neural networks is a long-standing technique for improving the
generalization error of neural networks by combining networks with orthogonal
properties via a committee decision. We show that this technique is an ideal
fit for machine learning on medical data: First, ensembles are amenable to
parallel and asynchronous learning, thus enabling efficient training of
patient-specific component neural networks. Second, building on the idea of
minimizing generalization error by selecting uncorrelated patient-specific
networks, we show that one can build an ensemble of a few selected
patient-specific models that outperforms a single model trained on much larger
pooled datasets. Third, the non-iterative ensemble combination step is an
optimal low-dimensional entry point to apply output perturbation to guarantee
the privacy of the patient-specific networks. We exemplify our framework of
differentially private ensembles on the task of early prediction of sepsis,
using real-life intensive care unit data labeled by clinical experts.","Ensembling describes a family of algorithms that train mult iple learners to solve the same problem, and exploit their heterogeneous properties to per form a committeebased predic tion that achieves higher accuracy than any single componen t learner. These techniques are welltried in machine learning practice and have led to t heoretically wellfounded algo rithms such as stacking ( Wolpert,1992), boosting ( Freund and Schapire ,1995), or bagging (Breiman,1996). Research on ensembling has very early tackled the problem of reducing variance of neural networks while keeping bias low at the sam e time. In the wide spectrum of approaches, ranging from sophisticated techniques to jo intly train component networks (Liu and Yao ,1999;Buschj¬® ager et al. ,2020) to building ensembles from model parameters of a single training trajectory ( Huang et al. ,2017;Izmailov et al. ,2018), we are speciÔ¨Å cally interested in approaches where component models are t rained independently and then smartly combined. A key insight in this area, Ô¨Årst formulated in Perrone and Cooper (1992), is that the generalization error of theweighted average of prediction sof individualcomponent networks can be formalized as the weighted correlation between the co mponent neural networks participating in the ensemble. This formulation opens seve ral possibilities for eÔ¨Écient and ¬©2022 S. Schamoni, M. Hagmann & S. Riezler.Ensembling Neural Networks for Improved Prediction and Pri vacy eÔ¨Äective machine learning: First, the bulk of the machine lea rning cost, namely the cost of training individual component networks, can be triviall y parallelized or even be done asynchronously, thus providing an eÔ¨Écient way of enhancing the representational power of the ensemble by training multiple classiÔ¨Åers at once. Sec ond, optimizing combination weights to minimize the weighted correlation between compo nent networks provides a direct avenue to minimize the generalization error of the ensemble , or to build a sparse ensemble from the optimal subset of component networks with small err or and small correlation with other component networks. A further advantage of weightedaveraging ensembles that h as been investigated much less than their generalization performance is the possibil ity to seamlessly integrate privacy protection into machine learning. In the case of machine lea rning models trained on medical data, the privacy to be protected might concern the membersh ip of patientspeciÔ¨Åc data in the training data for a particular disease. As argued by Dinur and Nissim (2003), removal of ‚Äúidentifying‚Äù attributes such as patients‚Äô names is not e nough, but instead random per turbations have to be applied to the outputs in order to prote ct privacy even in the simplest case of ‚Äústatistical‚Äù queries such as averages over databas es. The framework of diÔ¨Äerential privacy ( Dwork and Roth ,2014) allows giving strong guarantees on the information deriv able from private training data when querying a machine lear ning algorithm. We show that weightedaveraging ensembles do possess small sensitivit y by tightly bounded output ranges and do not accumulate privacy budget via iterative training , thus they are ideally suited for privacy protection at small noise scales. Furthermore, we prove that uniform weights are optimal to protect privacy in a weightedaveraging ense mble. Specifying guarantees on privacy protection is of increasi ng importance for medical re search. National laws and regulations such as the US HIPAA Pr ivacy rule1requiremeasures to protect the privacy of health information. On the hospita l level, protecting a patient‚Äôs privacy is crucial especially when information is shared ac ross institutions. Our method demonstrates the beneÔ¨Åt of output sharing where hospitals keep their inhouse model in a secured area and only share the output with other institutio ns, thus avoiding the challenges and diÔ¨Éculties of model sharing techniques such as federated learning ( Rieke et al. ,2020). On the patient level, a recent survey has shown that more than 30% of the participants are comfortable with sharing their electronic health data f or personalized healthcare, while less than 5% are very uncomfortable with sharing ( Garett and Young ,2022). This means more than 60% do not have a strong opinion on this topic, thus w e hope that an increasing number of people will share their data if stronger privacy gu arantees can be given. Generalizable Insights about Machine Learning in the Context of Healthcare Expert labels and neural networks are a powerful combinatio n for early sepsis prediction. However, patient data for this task is scarce as expert label s are diÔ¨Écult to obtain, while the protection of privacy is crucial to encourage patients t o contribute with their personal private data. We show how to train individual personalized m odels and how to combine a small number of patient models in an ensemble that has more de sirable properties in the Ô¨Åeld of medical data analysis than a standard full model, i.e ., a single model that is trained on all available patient data. 1.www.hhs.gov/hipaa/forprofessionals/privacy/ (accessed 07/06/2022) 2Ensembling Neural Networks for Improved Prediction and Pri vacy ‚Ä¢We present theoretical results that an ensemble of models wh ich was trained on a fraction of the available data can be better than a full model , and we verify this empirically. ‚Ä¢Our training method not only exposes fewer patients in the pr edictor than a full model, but also protects the privacy better: we apply a stron g membership attack and show that the ensemble successfully prevents privacy le akage. ‚Ä¢We show that an ensemble of several models is favorable to a si ngle model due to its reduced sensitivity in theory, and we experimentally ve rify that our ensemble maintains its accuracy at privacy budgets almost two orders of magnitude smaller than a full model. Furthermore, our ensemble can be easily updated by modelgr owing without the need of retraining the whole system when new patient‚Äôs data becomes available. 2. Related Work "
17,Are Labels Necessary for Neural Architecture Search?.txt,"Existing neural network architectures in computer vision -- whether designed
by humans or by machines -- were typically found using both images and their
associated labels. In this paper, we ask the question: can we find high-quality
neural architectures using only images, but no human-annotated labels? To
answer this question, we first define a new setup called Unsupervised Neural
Architecture Search (UnNAS). We then conduct two sets of experiments. In
sample-based experiments, we train a large number (500) of diverse
architectures with either supervised or unsupervised objectives, and find that
the architecture rankings produced with and without labels are highly
correlated. In search-based experiments, we run a well-established NAS
algorithm (DARTS) using various unsupervised objectives, and report that the
architectures searched without labels can be competitive to their counterparts
searched with labels. Together, these results reveal the potentially surprising
finding that labels are not necessary, and the image statistics alone may be
sufficient to identify good neural architectures.","Neural architecture search (NAS) has emerged as a research problem of searching for architectures that perform well on target data and tasks. A key mystery sur rounding NAS is what factors contribute to the success of the search. Intuitively, using the target data and tasks during the search will result in the least domain gap, and this is indeed the strategy adopted in early NAS attempts [36,27]. Later, researchers [37] started to utilize the transferability of architectures, which en abled the search to be performed on dierent data and labels ( e.g., CIFAR10) than the target ( e.g., ImageNet). However, what has not changed is that both the images and the (semantic) labels provided in the dataset need to be used in order to search for an architecture. In other words, existing NAS approaches perform search in the supervised learning regime. In this paper, we take a step towards understanding what role supervision plays in the success of NAS. We ask the question: How indispensable are labels in 3Code release: https://github.com/facebookresearch/unnasarXiv:2003.12056v2  [cs.CV]  3 Aug 20202 C. Liu et al. neural architecture search? Is it possible to nd highquality architectures using images only? This corresponds to the important yet underexplored unsupervised setup of neural architecture search, which we formalize in Section 3. With the absence of labels, the quality of the architecture needs to be esti mated in an unsupervised fashion during the search phase. In the present work, we conduct two sets of experiments using three unsupervised training methods [12,35,22] from the recent selfsupervised learning literature.4These two sets of experiments approach the question from complementary perspectives. In sample based experiments , we randomly sample 500 architectures from a search space, train and evaluate them using supervised vs. selfsupervised objectives, and then examine the rank correlation (when sorting models by accuracy) between the two training methodologies. In searchbased experiments , we take a wellestablished NAS algorithm, replace the supervised search objective with a selfsupervised one, and examine the quality of the searched architecture on tasks such as Ima geNet classication and Cityscapes semantic segmentation. Our ndings include: {The architecture rankings produced by supervised and selfsupervised pre text tasks are highly correlated . This nding is consistent across two datasets, two search spaces, and three pretext tasks. {The architectures searched without human annotations are comparable in performance to their supervised counterparts. This result is consistent across three pretext tasks, three pretext datasets, and two target tasks. There are even cases where unsupervised search outperforms supervised search. {Existing NAS approaches typically use labeled images from a smaller dataset to learn transferable architectures. We present evidence that using unlabeled images from a large dataset may be a more promising approach. We conclude that labels are not necessary for neural architecture search, and the deciding factor for architecture quality may hide within the image pixels. 2 Related Work "
18,Temporal Localization of Fine-Grained Actions in Videos by Domain Transfer from Web Images.txt,"We address the problem of fine-grained action localization from temporally
untrimmed web videos. We assume that only weak video-level annotations are
available for training. The goal is to use these weak labels to identify
temporal segments corresponding to the actions, and learn models that
generalize to unconstrained web videos. We find that web images queried by
action names serve as well-localized highlights for many actions, but are
noisily labeled. To solve this problem, we propose a simple yet effective
method that takes weak video labels and noisy image labels as input, and
generates localized action frames as output. This is achieved by cross-domain
transfer between video frames and web images, using pre-trained deep
convolutional neural networks. We then use the localized action frames to train
action recognition models with long short-term memory networks. We collect a
fine-grained sports action data set FGA-240 of more than 130,000 YouTube
videos. It has 240 fine-grained actions under 85 sports activities. Convincing
results are shown on the FGA-240 data set, as well as the THUMOS 2014
localization data set with untrimmed training videos.","This paper addresses the problem of negrained action localization from unconstrained web videos. A negrained Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita tion on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a fee. Request permissions from Permissions@acm.org. MM‚Äô15, October 26‚Äì30, 2015, Brisbane, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 9781450334594/15/10 ...$15.00. DOI: http://dx.doi.org/10.1145/2733373.2806226 . Figure 1: Finegrained actions are usually present as a tiny fraction within videos (top). Our framework uses crossdomain transfer from possibly noisy im age search results (bottom) and identies the action related images for both domains (marked in green). action takes place in a higherlevel activity or event (e.g., jump shot andslam dunk inbasketball ,blow candle inbirth day party ). Its instances are usually temporally localized within the videos, and share similar context with other ne grained actions belonging to the same activity or event. Most existing work on action recognition focuses on action classication using presegmented short video clips [25, 14, 23], which assumes implicitly that the actions of interest are temporally segmented during both training and testing. The TRECVID Multimedia Event Recounting evaluation [17] as well as THUMOS 14 Challenge [10] both address action lo calization in untrimmed video, but the typical approach in volves training classiers on temporally segmented action clips and testing using sliding window on untrimmed video. This setting does not scale to large action vocabularies, when data is collected from consumer video websites. Videos here are unconstrained in length, format (home videos vs. pro fessional videos), and almost always only have video level annotations of actions. We assume that only videolevel annotations are available for the negrained action localization problem. The abil ity to localize ne grained actions in videos has important applications such as video highlighting, summarization, and automatic video transcription. It is also a challenging probarXiv:1504.00983v2  [cs.CV]  4 Aug 2015lem for several reasons: rst, negrained actions for any highlevel activity or event are inherently similar since they take place in similar scene context; second, occurrences of the negrained actions are usually short (a few seconds) in training videos, making it dicult to associate the video level labels to the occurrences. Our key observation is that one can exploit web images to help localize negrained actions in videos. As illustrated in Figure 1, by using action names ( basketball slam dunk ) as queries, many of the image search results oer well lo calized actions, though some of them are nonvideo like or irrelevant. Identifying action related frames from weakly su pervised videos and ltering irrelevant image tags is hard in either modality by itself; however, it is easier to tackle these two problems together. This is due to our observation that although most of the video frames and web images which correspond to actions are visually similar, the distributions of nonaction images from the video domain and the web image domain are usually very dierent. For example, in a video with a basketball slam dunk , non slam dunk frames in the video are mostly from a basketball game. The irrele vant results returned by image search are more likely to be product shots, or cartoons. This motivates us to formulate a domain transfer problem between web images and videos. To allow domain transfer, we rst treat the videos as a bag of frames, and use the feature activations from deep convolutional neural networks (CNN) [13] as the common representation for images and frames. Suppose we have selected a set of video frames and a set of web images for every action, the domain transfer framework goes in two directions: video frames to web im ages, and vice versa. For both directions, we use the selected images from the source domain to train action classiers by netuning the top layers of the CNN; we then apply the trained classiers to the target domain. Each image in the target domain is assigned a condence score given by its as sociated action classier from the source domain. By grad ually ltering out the images with low scores, the bidirec tional domain transfer can progress iteratively. In practice, we start from the video frames to web images direction, and randomly select the video frames for training. Since the nonaction related frames are not likely to occur in web im ages, the tuned CNN can be used to lter out the nonvideo like and irrelevant web images. The nal domain transfer from web images is used to localize action related frames in videos. We term these actionrelated frames as localized action frames (LAF). Videos are more than an unordered collection of frames. We choose long shortterm memory (LSTM) [8] networks as the temporal model. Compared with the traditional recur rent neural networks (RNN), LSTM has builtin input gates and forget gates to control its memory cells. These gates allow LSTM to either keep a long term memory or forget its history. The ability to learn from long sequences with unknown size of background is wellsuited for negrained action localization from unconstrained web videos. We treat every sampled video frame as a time step in LSTM. When we train LSTM models, we label all video frames by their videolevel annotation, but use the LAF scores generated by bidirectional domain transfer as weights on the loss for mis classication. By doing this, irrelevant frames are eectively downweighted in the training stage. The framework can be naturally extended to use video shots as time steps, fromwhich spatiotemporal features can be extracted to capture local motion information. Finegrained action localization from untrimmed web videos is a new task. The closest existing data set is THUMOS 2014 with 20 sports categories. It is designed for action lo calization using segmented videos as training, but has 1,010 untrimmed validation videos. To evaluate the framework in a large scale setting, we collected a new data set from YouTube. We chose 240 negrained actions belonging to 85 dierent sports activities, the total number of videos is over 130,000. Although the evaluated categories are sports ac tions, this method can be easily extended to other domains. For example, one can easily get cut cake, eat cake and blow candle images for a birthday party event with image search. Our work makes three major contributions: We show that learning temporally localized actions from videos becomes much easier if we combine weakly labeled video frames and noisily tagged web images. This is achieved by a simple yet eective domain trans fer algorithm. We propose a localization framework that uses LSTM network with the localized action frames to model the temporal evolution of actions. We introduce the problem of negrained action lo calization with untrimmed videos, and collect a large negrained sports action data set with over 130,000 videos in 240 categories. The data set is available on line.1 2. RELATED WORK "
19,Two Routes to Scalable Credit Assignment without Weight Symmetry.txt,"The neural plausibility of backpropagation has long been disputed, primarily
for its use of non-local weight transport $-$ the biologically dubious
requirement that one neuron instantaneously measure the synaptic weights of
another. Until recently, attempts to create local learning rules that avoid
weight transport have typically failed in the large-scale learning scenarios
where backpropagation shines, e.g. ImageNet categorization with deep
convolutional networks. Here, we investigate a recently proposed local learning
rule that yields competitive performance with backpropagation and find that it
is highly sensitive to metaparameter choices, requiring laborious tuning that
does not transfer across network architecture. Our analysis indicates the
underlying mathematical reason for this instability, allowing us to identify a
more robust local learning rule that better transfers without metaparameter
tuning. Nonetheless, we find a performance and stability gap between this local
rule and backpropagation that widens with increasing model depth. We then
investigate several non-local learning rules that relax the need for
instantaneous weight transport into a more biologically-plausible ""weight
estimation"" process, showing that these rules match state-of-the-art
performance on deep networks and operate effectively in the presence of noisy
updates. Taken together, our results suggest two routes towards the discovery
of neural implementations for credit assignment without weight symmetry:
further improvement of local rules so that they perform consistently across
architectures and the identification of biological implementations for
non-local learning mechanisms.","Backpropagation is the workhorse of modern deep learning and the only known learning algorithm that allows multi layer networks to train on largescale tasks. However, any exact implementation of backpropagation is inherently non local, requiring instantaneous weight transport in which backward errorpropagating weights are the transpose of the forward inference weights. This violation of locality is biologically suspect because there are no known neural mechanisms for instantaneously coupling distant synaptic weights. Recent approaches such as feedback alignment (Lillicrap et al., 2016) and weight mirror (Akrout et al., 2019) have identiÔ¨Åed circuit mechanisms that seek to ap proximate backpropagation while circumventing the weight transport problem. However, these mechanisms either fail to operate at largescale (Bartunov et al., 2018) or, as we demonstrate, require complex and fragile metaparameter scheduling during learning. Here we present a unifying framework spanning a space of learning rules that allows for the systematic identiÔ¨Åcation of robust and scalable alter natives to backpropagation. To motivate these rules, we replace tied weights in back propagation with a regularization loss on untied forward and backward weights. The forward weights parametrize the global cost function, the backward weights specify a descent direction, and the regularization constrains the relationship between forward and backward weights. As the system iterates, forward and backward weights dynamically align, giving rise to a pseudogradient. Different regularization terms are possible within this framework. Critically, these regularization terms decompose into geometrically natural primitives, which can be parametrically recombined to con struct a diverse space of credit assignment strategies. This space encompasses existing approaches (including feedback alignment and weight mirror), but also elucidates novel learning rules. We show that several of these new strategiesarXiv:2003.01513v2  [qbio.NC]  25 Jun 2020Two Routes to Scalable Credit Assignment without Weight Symmetry are competitive with backpropagation on realworld tasks (unlike feedback alignment), without the need for complex metaparameter tuning (unlike weight mirror). These learn ing rules can thus be easily deployed across a variety of neural architectures and tasks. Our results demonstrate how highdimensional errordriven learning can be robustly per formed in a biologically motivated manner. 2. Related Work "
20,MGH: Metadata Guided Hypergraph Modeling for Unsupervised Person Re-identification.txt,"As a challenging task, unsupervised person ReID aims to match the same
identity with query images which does not require any labeled information. In
general, most existing approaches focus on the visual cues only, leaving
potentially valuable auxiliary metadata information (e.g., spatio-temporal
context) unexplored. In the real world, such metadata is normally available
alongside captured images, and thus plays an important role in separating
several hard ReID matches. With this motivation in mind, we
propose~\textbf{MGH}, a novel unsupervised person ReID approach that uses meta
information to construct a hypergraph for feature learning and label
refinement. In principle, the hypergraph is composed of camera-topology-aware
hyperedges, which can model the heterogeneous data correlations across cameras.
Taking advantage of label propagation on the hypergraph, the proposed approach
is able to effectively refine the ReID results, such as correcting the wrong
labels or smoothing the noisy labels. Given the refined results, We further
present a memory-based listwise loss to directly optimize the average precision
in an approximate manner. Extensive experiments on three benchmarks demonstrate
the effectiveness of the proposed approach against the state-of-the-art.","Given a person of interest for query, person reidentification(ReID) aims to search the same identities against gallery, it has been widely used in many realworld applications, such as video surveillance system, robotics, humancomputer interaction, etc. Due to the ex pensive annotation cost, recent researches focus on the unsuper vised person ReID, which requires no manual annotations. More over, ReID is generally carried out in a distributed camera network, where a wealth of auxiliary metadata (e.g. camera index, timestamp, etc.) is attached with the captured images. As shown in Figure 1, the metadata could provide auxiliary guidance to unsupervised person ReID [ 23,24,31,33,40,43,49,51,66,68], but how to adequatelyarXiv:2110.05886v1  [cs.CV]  12 Oct 2021GlobalDBSCANIntracameraDBSCANGlobalKNN(K=3)ùëí!ùëí""ùëí#ùëí$ùëí%ùëí&ùëí'HypergraphFigure 2: Illustration of hypergraph construction. We inves tigate different hyperedge construction strategies, i.e. KNN, clustering, cameraaware clustering. Global Clustering: per form global clustering and group the instances in a cluster as a hyperedge. Intracamera Clustering: camera informa tion is combined with clustering to perform intracamera Clustering. Global KNN: given a vertex (i.e., centroid), a hy peredge connects itself and its nearest neighbors. utilize such heterogeneous structure in unsupervised ReID is still an open problem. Recently, unsupervised person ReID approaches [ 9,12,60] follow clusteringandfinetune pipeline, which iteratively assigns pseudo labels for data then trains the feature extractor with the pseudo labels. In practice, since the ReID image data is generated from a distributed camera network, the correlation among images is usually diverse, heterogeneous, and complicated. Therefore, the relations are various, such as visual connections and camera con nections, which causes the unreliable pseudo labels in unsupervised person ReID. To solve this issue, a hypergraph representation [ 16] is an appropriate tool to model the multimodal and heterogeneous data correlation by means of a variety of hyperedges [ 10]. Due to the heterogeneous information in the metadata, the relation ship among instances are varied from the different viewpoints. To address this issuse, we propose three kinds of hypergraph construc tion techniques: ‚ÄúGlobal Clustering‚Äù, ‚ÄúIntracamera Clustering‚Äù, and ‚ÄúGlobal KNN‚Äù. As illustrated in Figure 2, ‚ÄúGlobal Clustering‚Äù means performing DBSCAN on all data and then grouping the in stances in a cluster as a hyperedge, this strategy aims to capture the global densitybased mode in the data. ‚ÄúIntracamera Clustering‚Äù considers the camera information and performs DBSCAN under an individual camera, this strategy concentrates on capture the cameraconditioned marginal density mode. ‚ÄúGlobal KNN‚Äù stands for building a hyperedge by connecting one vertex with its ùêænearest neighbors, this strategy focuses on local neighborhood to capture the smoothness locally. By means of merging several hyperedges created by different approaches, the hypergraph is able to perceivethe camera topology and model the heterogeneous data correla tion across cameras. With this hypergraph, we can generate more reliable pseudo labels in learning process. Given the generated pseudo labels, the ReID model could be finetuned iteratively. Several approaches related to mutual learn ing [ 3,11,44,58] and memorybased contrastive loss [ 43,68] are proposed to learn the robust feature representation with noisy labels. Although these proposed classification losses are verified to be effective, the end task such as average precision is not di rectly optimized in these approaches. To help address this issue, we propose a listwise loss combined with an instancelevel memory, which provides a finegrained supervision by directly optimize av erage precision in an approximate manner. Besides, we combine the proposed listwise loss and cameraaware contrastive loss as coarsetofine supervision for model updating. Based on the motivation above, we propose a novel method termed Metadata Guided Hypergraph (MGH) to mutually guide the process of label refinement and feature learning. Specifically, we construct a heterogeneous hypergraph based on the metadata to generate pseudo labels, and utilize coarsetofine memorybased supervision to update the model. To achieve this, we first generate the noisy pseudo labels by clustering the visual feature represen tations of all unlabeled images. Then utilize hypergraph for label refinement. Specifically, we construct the hyperedges based on a joint similarity matrix [ 40] by considering visual information and spatiotemporal context simultaneously. Hyperedges are grouped to generate a hypergraph, which models the complicated highorder relationships among the data. Then, we perform label propaga tion on the hypergraph structure, which rectifies the label errors caused by the previous clustering algorithm. For model updating, the instancelevel memory and cameraaware prototype memory are constructed to simultaneously capture local and global distribu tion. We propose to use a listwise loss based on the instancelevel memory, which is combined with a cameraaware contrastive loss to provide coarsetofine supervision. In summary, our main contributions are threefold as follows: ‚Ä¢We propose a heterogeneous hypergraph to model the com plicated data correlation among the metadata, which facil itates the unsupervised person ReID in the realworld sce nario. ‚Ä¢We propose a novel unsupervised person ReID model named MGH , which consists of label generation with hypergraph and model updating through memorybased coarsetofine supervision. ‚Ä¢On three public person ReID benchmarks with readily avail able metadata, i.e. camera index and timestamp, our proposed method outperforms the stateoftheart approaches. 2 RELATED WORK "
21,Person Re-identification with Deep Similarity-Guided Graph Neural Network.txt,"The person re-identification task requires to robustly estimate visual
similarities between person images. However, existing person re-identification
models mostly estimate the similarities of different image pairs of probe and
gallery images independently while ignores the relationship information between
different probe-gallery pairs. As a result, the similarity estimation of some
hard samples might not be accurate. In this paper, we propose a novel deep
learning framework, named Similarity-Guided Graph Neural Network (SGGNN) to
overcome such limitations. Given a probe image and several gallery images,
SGGNN creates a graph to represent the pairwise relationships between
probe-gallery pairs (nodes) and utilizes such relationships to update the
probe-gallery relation features in an end-to-end manner. Accurate similarity
estimation can be achieved by using such updated probe-gallery relation
features for prediction. The input features for nodes on the graph are the
relation features of different probe-gallery image pairs. The probe-gallery
relation feature updating is then performed by the messages passing in SGGNN,
which takes other nodes' information into account for similarity estimation.
Different from conventional GNN approaches, SGGNN learns the edge weights with
rich labels of gallery instance pairs directly, which provides relation fusion
more precise information. The effectiveness of our proposed method is validated
on three public person re-identification datasets.","Person reidentication is a challenging problem, which aims at nding the per son images of interest in a set of images across dierent cameras. It plays a signicant role in the intelligent surveillance systems. ?Hongsheng Li is the corresponding author.arXiv:1807.09975v1  [cs.CV]  26 Jul 20182 Y. Shen, H. Li, S. Yi, D. Chen and X. Wang ProbeProbeGallery 1ProbeGallery 2ProbeGallery 3ProbeGallery 4Gallery 1Gallery 2Gallery 4Gallery 3 SiameseCNN		""#		""$		""%		""&Similarity Estimator		'#		'$		'%		'& (a) Conventional Approach. ProbeGallery 1ProbeGallery 1Gallery 2Gallery 4Gallery 3 SiameseCNN		""#	""$		""%		""&Similarity Estimator		'#		'$		'%		'&ProbeGallery 2ProbeGallery 3ProbeGallery 4Graph edge (b) Our proposed SGGNN. Fig. 1. Illustration of our Proposed SGGNN method and conventional person re identication approach. (a) The pipeline of conventional person reidentication ap proach, the pairwise relationships between dierent probegallery pairs are ignored. The similarity score of each probegallery pair di(i= 1;2;3;4) is estimated individ ually. (b) Our proposed SGGNN approach, pairwise relationships between dierent probegallery pairs are involved with deeply learned message passing on a graph for more accurate similarity estimation. To enhance the reidentication performance, most existing approaches at tempt to learn discriminative features or design various metric distances for better measuring the similarities between person image pairs. In recent years, witness the success of deep learning based approaches for various tasks of com puter vision [25,17,51,62,59,12,39,63,67,31,20], a large number of deep learning methods were proposed for person reidentication [37,81,64,40]. Most of these deep learning based approaches utilized Convolutional Neural Network (CNN) to learn robust and discriminative features. In the mean time, metric learning methods were also proposed [4,3,72] to generate relatively small feature distances between images of same identity and large feature distances between those of dierent identities. However, most of these approaches only consider the pairwise similarity while ignore the internal similarities among the images of the whole set. For instance, when we attempt to estimate the similarity score between a probe image and a gallery image, most feature learning and metric learning approaches only con sider the pairwise relationship between this single probegallery image pair in both training and testing stages. Other relations among dierent pairs of images are ignored. As a result, some hard positive or hard negative pairs are dicult to obtain proper similarity scores since only limited relationship information among samples is utilized for similarity estimation.Person ReID with Deep SimilarityGuided Graph Neural Network 3 To overcome such limitation, we need to discover the valuable internal simi larities among the image set, especially for the similarities among the gallery set. One possible solution is utilizing manifold learning [2,42], which considers the similarities of each pair of images in the set. It maps images into a manifold with more smooth local geometry. Beyond the manifold learning methods, reranking approaches [78,16,70] were also utilized for rening the ranking result by inte grating similarities between topranked gallery images. However, both manifold learning and reranking approaches have two major limitations: (1) most mani fold learning and reranking approaches are unsupervised, which could not fully exploit the provided training data label into the learning process. (2) These two kinds of approaches could not benet feature learning since they are not involved in training process. Recently, Graph Neural Network (GNN) [6,18,23,45] draws increasing at tention due to its ability of generalizing neural networks for data with graph structures. The GNN propagates messages on a graph structure. After mes sage traversal on the graph, node's nal representations are obtained from its own as well as other node's information, and are then utilized for node classi cation. GNN has achieved huge success in many research elds, such as text classication [13], image classication [6,46], and human action recognition [66]. Compared with manifold learning and reranking, GNN incorporates graph com putation into the neural networks learning, which makes the training endtoend and benets learning the feature representation. In this paper, we propose a novel deep learning framework for person re identication, named SimilarityGuided Graph Neural Network (SGGNN). SG GNN incorporates graph computation in both training and testing stages of deep networks for obtaining robust similarity estimations and discriminative feature representations. Given a minibatch consisting of several probe images and gallery images, SGGNN will rst learn initial visual features for each image (e.g., global average pooled features from ResNet50 [17].) with the pairwise re lation supervisions. After that, each pair of probegallery images will be treated as a node on the graph, which is responsible for generating similarity score of this pair. To fully utilize pairwise relations between other pairs (nodes) of im ages, deeply learned messages are propagated among nodes to update and rene the pairwise relation features associated with each node. Unlike most previous GNNs' designs, in SGGNN, the weights for feature fusion are determined by sim ilarity scores by gallery image pairs, which are directly supervised by training labels. With these similarity guided feature fusion weights, SGGNN will fully exploit the valuable label information to generate discriminative person image features and obtain robust similarity estimations for probegallery image pairs. The main contribution of this paper is twofold. (1) We propose a novel Similarity Guided Graph Neural Network (SGGNN) for person reidentication, which could be trained endtoend. Unlike most existing methods, which uti lize intergalleryimage relations between samples in the postprocessing stage, SGGNN incorporates the intergalleryimage relations in the training stage to enhance feature learning process. As a result, more discriminative and accurate4 Y. Shen, H. Li, S. Yi, D. Chen and X. Wang person image feature representations could be learned. (2) Dierent from most Graph Neural Network (GNN) approaches, SGGNN exploits the training label supervision for learning more accurate feature fusion weights for updating the nodes' features. This similarity guided manner ensures the feature fusion weights to be more precise and conduct more reasonable feature fusion. The eective ness of our proposed method is veried by extensive experiments on three large person reidentication datasets. 2 Related Work "
22,Deep Attributes Driven Multi-Camera Person Re-identification.txt,"The visual appearance of a person is easily affected by many factors like
pose variations, viewpoint changes and camera parameter differences. This makes
person Re-Identification (ReID) among multiple cameras a very challenging task.
This work is motivated to learn mid-level human attributes which are robust to
such visual appearance variations. And we propose a semi-supervised attribute
learning framework which progressively boosts the accuracy of attributes only
using a limited number of labeled data. Specifically, this framework involves a
three-stage training. A deep Convolutional Neural Network (dCNN) is first
trained on an independent dataset labeled with attributes. Then it is
fine-tuned on another dataset only labeled with person IDs using our defined
triplet loss. Finally, the updated dCNN predicts attribute labels for the
target dataset, which is combined with the independent dataset for the final
round of fine-tuning. The predicted attributes, namely \emph{deep attributes}
exhibit superior generalization ability across different datasets. By directly
using the deep attributes with simple Cosine distance, we have obtained
surprisingly good accuracy on four person ReID datasets. Experiments also show
that a simple metric learning modular further boosts our method, making it
significantly outperform many recent works.","Person ReIdentication (ReID) targets to identify the same person from dif ferent cameras, datasets, or time stamps. As illustrated in Fig. 1, factors like viewpoint variations, illumination conditions, camera parameter dierences, as well as body pose changes make person ReID a very challenging task. Due to its important applications in public security, e.g., cross camera pedestrian searching, tracking, and event detection, person ReID has attracted lots of attention from both the academic and industrial communities. Currently, research on this topic mainly focus on two aspects: a) extracting and coding local invariant features to arXiv:1605.03259v2  [cs.CV]  9 Aug 20162 authors running upperBodyLongSleeve upperBodyBlacklowerBodyJeanspersonalFemalehairBlackpersonalFemale upperBodyRedfootwearBlacklowerBodyBlackupperBodyOtherhairShort upperBodyWhitelowerBodyGreypersonalMalelowerBodyTrousers (a) (b) (c) Fig. 1. Example images of the same person taken by two cameras from three datasets: (a)VIPeR [26], (b) PRID [27], and (c) GRID [28]. This gure also shows ve of our predicted attributes shared by these two images. represent the visual appearance of a person [1,2,3,4,5,6,7] and b) learning a dis criminative distance metric hence the distance of features from the same person can be smaller [8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]. Although signicant progress has been made from previous studies, person ReID methods are still not mature enough for real applications. Local features mostly describe the lowlevel visual appearance, hence are not robust to variances of viewpoints, body poses, etc. On the other side, distance metric learning suers from the poor generalization ability and the quadratic computational complexity, e.g., dierent datasets present dierent visual characteristics corresponding to dierent metrics. Compared with lowlevel visual feature, human attributes like long hair, blue shirt, etc., represent midlevel semantics of a person. As illustrated in Fig. 1, attributes are more consistent for the same person and are more robust to the above mentioned variances. Some recent works hence have started to use attributes for person ReID [29,30,31,32,33,34]. Because human attributes are expensive for manual annotation, it is dicult to acquire enough training data for a large set of attributes. This limits the performance of current attribute features. Consequently, lowlevel visual features still play a key role and attributes are mostly used as auxiliary features [31,32,33,34]. Recently, deep learning has exhibited promising performance and general ization ability in various visual tasks. For example in [35], an eightlayer deep Convolutional Neural Network (dCNN) is trained with largescale images for visual classication. The modied versions of this network also perform impres sively in object detection [36] and segmentation [37]. Motivated by the issues of low level visual features and the success of dCNN, our work targets to learn a dCNN to detect a large set of human attributes discriminative enough for person ReID. Due to the diversity and complexity of human attributes, it is a laborious task to manually label enough of attributes for dCNN training. The key issuestitle running 3 ‚Ä¶ ‚Ä¶ ‚Ä¶ Attributes Triplet  Loss ‚Ä¶‚Ä¶ ‚Ä¶dCNN dCNN Sigmoid Cross Entropy  LossFc7:  4096 nodes Fc8:  Knodes Fc7:  4096 nodesFc8: K nodes ‚Ä¶ ‚Ä¶Anchor Positive Negative ‚Ä¶ 111‚Ä¶ 101‚Ä¶ 110‚Ä¶ 001‚Ä¶ 101‚Ä¶ 011‚Ä¶Stage 1: Fullysupervised dCNN training Independent dataset  with attribute labels Dataset with person ID labels Predicted attributesStage 2: Fine tuning using attributes triplet loss Stage 3:Final fine tuning on the combined dataset ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶‚Ä¶dCNN Sigmoid Cross Entropy  LossFc7:  4096 nodes Fc8:  Knodes ‚Ä¶ ‚Ä¶ ‚Ä¶101‚Ä¶ 110‚Ä¶ 110‚Ä¶ 001‚Ä¶ 101‚Ä¶ 101‚Ä¶Independent dataset Dataset with refined attributesAnchor Positive Negative Fig. 2. Illustration of Semisupervised Deep Attribute Learning (SSDAL). are hence how to train this dCNN from a partiallylabeled dataset and ensure its discriminative power and generalization ability in the person ReID tasks. To address these issues, we propose a Semisupervised Deep Attribute Learn ing (SSDAL) algorithm. As illustrated in Fig. 2, this algorithm involves three stages. The rst stage uses an independent dataset with attribute labels to per form fullysupervised dCNN training. The resulting dCNN produces initial at tribute labels for the target dataset. To improve the discriminative power of these attributes for ReID task, we start the second stage of training, i.e., ne tuning the network using the person ID labels and our dened attributes triplet loss. The training data for netuning can be easily collected because the person ID labels are readily accessible in many person tracking datasets. The attributes triplet loss updates the network to enforce that the same person has more similar attributes and vice versa. This netuned dCNN hence predicts initial attribute labels for target datasets. Finally in the third stage, the initially labeled target dataset plus the original independent dataset are combined for the nal stage of netuning. The attributes predicted by the nal dCNN model are named as deep attributes . In this manner, the dCNN is rstly trained with the indepen dent dataset, then is rened to acquire more discriminative power for person ReID task. Because this procedure involves one dataset with attribute labels and another without attribute labels, we call it a semisupervised learning. To validate the performance of deep attributes, we test them on four popular person ReID datasets without combining with the local visual features. The4 authors running experimental results show that deep attributes perform impressively, e.g., they signicantly outperform many recent works combining both attributes and local features [31,32,33,34]. Note that, predicting and matching deep attributes make person ReID system signicantly faster, because it no longer needs to extract and code local features, compute distance metric, and fuse with other features. Our contributions can be summarized as follows: 1) we propose a three stage semisupervised deep attribute learning algorithm, which makes learning a large set of human attributes from a limited number of labeled attribute data possible, 2) deep attributes achieve promising performance and generalization ability on four person ReID datasets, and 3) deep attributes release the previous dependencies on local features, thus make the person ReID system more robust and ecient. To the best of our knowledge, this is an original work predicting human attributes using dCNN for person ReID tasks. The promising results of this work guarantees further investigation in this direction. 2 Related Work "
23,Adversarial Training with Stochastic Weight Average.txt,"Adversarial training deep neural networks often experience serious
overfitting problem. Recently, it is explained that the overfitting happens
because the sample complexity of training data is insufficient to generalize
robustness. In traditional machine learning, one way to relieve overfitting
from the lack of data is to use ensemble methods. However, adversarial training
multiple networks is extremely expensive. Moreover, we found that there is a
dilemma on choosing target model to generate adversarial examples. Optimizing
attack to the members of ensemble will be suboptimal attack to the ensemble and
incurs covariate shift, while attack to ensemble will weaken the members and
lose the benefit from ensembling. In this paper, we propose adversarial
training with Stochastic weight average (SWA); while performing adversarial
training, we aggregate the temporal weight states in the trajectory of
training. By adopting SWA, the benefit of ensemble can be gained without
tremendous computational increment and without facing the dilemma. Moreover, we
further improved SWA to be adequate to adversarial training. The empirical
results on CIFAR-10, CIFAR-100 and SVHN show that our method can improve the
robustness of models.","Although DNN shows great performance and generaliza tion ability, it has been found that convolutional neural net works (CNNs) are susceptible to designed adversarial attack, even if the attack is imperceptibly small. As the modern computer vision technology heavily relies on CNN, the vul nerability becomes a great threat. To mitigate such threat, many algorithms have been proposed to make network to be robust to adversarial attack (Xie et al. 2018; Song et al. 2018; Papernot et al. 2016; Madry et al. 2017). However, it has been found that many of these methods are not robust indeed; many proposed defense algorithms rely on obfus cated gradient which gives robustness against particular at tacks only, and can be circumvented by adaptively designed attack (Athalye, Carlini, and Wagner 2018; Tram `er et al. 2017). Among the methods compared in the paper, the only defense method that is believed to provide true robustness is training network with strong adversarial examples such as projected gradient descent (PGD) (Madry et al. 2017). Copyright c 2021, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.Hence, adversarial training is considered as the most reli able defense paradigm so far. However, adversarial training is prone to overÔ¨Åt ting (Schmidt et al. 2018). In robust training, adversarial loss on test set increases substantially after a certain point while training error decrease continuously. This overÔ¨Åtting phenomenon brings large generalization gap on adversarial accuracy and limits robustness. To explain such susceptibil ity, Schmidt (Schmidt et al. 2018) provide theoretical analy sis that concludes the overÔ¨Åtting is due to the lack of train ing data. According to the analysis, the sample complexity required to generalize robustness is signiÔ¨Åcantly lager than that for standard generalization. In traditional machine learning, one way to relieve overÔ¨Åt ting is adopting ensemble methods (Dietterich 2000). When the hypothesis space is too large to explore for limited train ing data, there may be several different hypotheses giving similar accuracy on the training data, and combining the hy potheses can reduce the risk to choose wrong hypotheses. Therefore, to alleviate the overÔ¨Åtting, one can easily con sider naive ensemble method which combines the results of adversarial trained model (Grefenstette et al. 2018). However, adopting naive ensemble method faces two problems. First and trivial problem is computation cost in crements. Adversarial training is already expensive due to multiple gradient computation required in generating ad versarial examples. Training multiple networks to ensemble with adversarial examples will multiply the expense accord ingly and the overall training becomes overburden. The other problem not yet discussed much is dilemma on choosing models to generate adversarial examples with respect to. The adversarial characteristic of an example is deÔ¨Åned by the corresponding network model. In naive en semble method, there can be two options on which model the adversarial example should be generated with respect to; individual members or whole ensemble system. If adversar ial examples are generated with respect to each member, the whole system is trained with suboptimal attack. If adversar ial examples are generated with respect to the whole ensem ble system, the members of ensembles are trained weakly and are likely to become less diverse. We illustrate the idea in Ô¨Ågure 1 and name it as dilemma of decoupling. Further elaboration is dealt in proposed method section in detail. The main purpose of this paper is to introduce the enarXiv:2009.10526v1  [cs.LG]  21 Sep 2020Adv Generating Stage Training Stage ùíéùüè Ensemble ùë¨ Ensemble ùë¨ ùíôùíÇùíÖùíó=ùêöùê´ùê†ùê¶ùêöùê± ùíô‚ààùìëùìõ(ùíô,ùíö,ùíéùüè) ùíôùíÇùíÖùíó=ùêöùê´ùê†ùê¶ùêöùê± ùíô‚ààùìëùìõ(ùíô,ùíö,ùíéùüê) ùíôùíÇùíÖùíó=ùêöùê´ùê†ùê¶ùêöùê± ùíô‚ààùìëùìõ(ùíô,ùíö,ùë¨) nat image ùíô ùíéùüêùíôùíÇùíÖùíó‚â†ùêöùê´ùê†ùê¶ùêöùê± ùíô‚ààùìëùìõ(ùíô,ùíö,ùë¨) ùíôùíÇùíÖùíó‚â†ùêöùê´ùê†ùê¶ùêöùê± ùíô‚ààùìëùìõ(ùíô,ùíö,ùë¨) nat image ùíô ùíôùíÇùíÖùíó‚â†ùêöùê´ùê†ùê¶ùêöùê± ùíô‚ààùìëùìõ(ùíô,ùíö,ùíéùüê)ùíôùíÇùíÖùíó‚â†ùêöùê´ùê†ùê¶ùêöùê± ùíô‚ààùìëùìõ(ùíô,ùíö,ùíéùüè)ùíéùüè ùíéùüê(a) (b)Figure 1: Dilemma of decoupling. Adversarial training with naive ensemble faces dilemma on choosing target model for generating adversarial examples. (a) Adversarial examples generated with respect to each member are suboptimal attack to whole ensemble and less useful to minimize empirical risk. (b) Adversarial examples generated with respect to whole ensemble are suboptimal to members and make them weak and less diverse, which in consequence loses beneÔ¨Åt of ensemble. semble method which can improve the adversarial training further. In this paper, we propose stochastic weight averag ing method(SWA)(Izmailov et al. 2018) based adversarial training(SWAAT) that defense whitebox attack effectively. During adversarial training, weight states on training trajec tory are aggregated to the average and the aggregated weight state is updated periodically. This makes it possible to have an ensemble effect while it does not suffer from the down side of ensemble we discussed previously. The adversarial examples generated during the proposed method is optimal to the members and nearly optimal to overall ensemble. This is a special beneÔ¨Åt of adopting SWA in adversarial training which does not belong to SWA in standard training. More over, due to the original characteristic of SWA, additional computation cost can be remained negligible. As a result, SWA enables one to train models with stronger robustness efÔ¨Åciently. The contributions of this paper we claim are as follows: We point out a dilemma of decoupling that arises when using ensemble method in adversarial training. The dilemma is concerning about which model to choose when generating adversarial examples to train. We propose a SWAAT without the drawbacks of conven tional ensembling methods. The proposed method pro vides not only effective ensembling strategy, but also fast batch normalization method in weight averaging pro cess of SWA and diversiÔ¨Åcation of member models by memberspeciÔ¨Åc data selection stage. Extensive experiments which are conducted using CIFAR10 and CIFAR100 datasets show that the proposed method signiÔ¨Åcantly improves performance in de fending designed adversarial attacks. 2 Related work "
24,Contextual Pyramid Attention Network for Building Segmentation in Aerial Imagery.txt,"Building extraction from aerial images has several applications in problems
such as urban planning, change detection, and disaster management. With the
increasing availability of data, Convolutional Neural Networks (CNNs) for
semantic segmentation of remote sensing imagery has improved significantly in
recent years. However, convolutions operate in local neighborhoods and fail to
capture non-local features that are essential in semantic understanding of
aerial images. In this work, we propose to improve building segmentation of
different sizes by capturing long-range dependencies using contextual pyramid
attention (CPA). The pathways process the input at multiple scales efficiently
and combine them in a weighted manner, similar to an ensemble model. The
proposed method obtains state-of-the-art performance on the Inria Aerial Image
Labelling Dataset with minimal computation costs. Our method improves 1.8
points over current state-of-the-art methods and 12.6 points higher than
existing baselines on the Intersection over Union (IoU) metric without any
post-processing. Code and models will be made publicly available.","The developments in the systematic collection and organiza tion of remote sensing imagery have resulted in several high resolution aerial imagery datasets. Information from aerial imagery plays a key role in urban planning, disaster aver sion, and change detection. Building detection is a crucial aspect for the aforementioned applications. Depending on the geographical region and conditions, building structures have different shapes and sizes. This challenge is particularly ad dressed by Maggiori et al. [1]. They created a dataset of la beled aerial imagery from different locations for this problem, such that a model trained from a variety of sources general izes to the task of segmentation. Semantic segmentation in aerial imagery is challenging due to variable lighting condi tions, shapes/sizes, and large intraclass variations. In this re search, we address the problem of improving the building seg mentation by utilizing attentive multiscale pathways. Each of the paths exploits nonlocal neighborhoods that account for buildings of varying sizes. This allows our network to Fig. 1 . Comparison of building segmentation on Inria Aerial Image Labeling Dataset. RGB, GT and outputs (bottom row) ResNet101FPN without (left) and with our module (right). learn longrange dependencies at various scales with minimal computation costs. In addition to attentive multiscale path ways, we also incorporate a channelwise attention module to model interdependencies across channels. In summary, our contributions are as follows. We introduce a selfattention based contextual pyramid attention (CPA) module that accounts for various build ing sizes to segment buildings in aerial images. The proposed module outperforms current stateoftheart methods by about 2% on the IoU metric and 12.6% over FCN baselines. Through experiments, we also show that our base model offers competitive performance to current state oftheart methods, while having much lower inference costs. We also provide ablation studies on the impact of our proposed module and other comparisons.arXiv:2004.07018v1  [cs.CV]  15 Apr 20202. RELATED WORK "
25,Field Extraction from Forms with Unlabeled Data.txt,"We propose a novel framework to conduct field extraction from forms with
unlabeled data. To bootstrap the training process, we develop a rule-based
method for mining noisy pseudo-labels from unlabeled forms. Using the
supervisory signal from the pseudo-labels, we extract a discriminative token
representation from a transformer-based model by modeling the interaction
between text in the form. To prevent the model from overfitting to label noise,
we introduce a refinement module based on a progressive pseudo-label ensemble.
Experimental results demonstrate the effectiveness of our framework.","Formlike documents, such as invoices, paystubs and patient referral forms, are very common in daily business workÔ¨Çows. A large amount of hu man effort is required to extract information from forms every day. In form processing, a worker is usually given a list of expected form Ô¨Åelds (e.g., purchase_order ,invoice_number andtotal_amount in Figure 1), and the goal is to extract their corre sponding values based on the understanding of the form, where keys are generally the most important features for value localization. A Ô¨Åeld extraction system aims to automatically extract Ô¨Åeld values from redundant information in forms, which is cru cial for improving processing efÔ¨Åciency and reduc ing human labor. Field extraction from forms is a challenging task. Document layouts and text representations can be very different even for the same form type, if they are from different vendors. For example, invoices from different companies may have signiÔ¨Åcantly different designs (see Figure 3). Paystubs from different systems (e.g., ADP and Workday) have different representations for similar information. Recent methods formulate this problem as Ô¨Åeld value pairing or Ô¨Åeld tagging. Majumder et al. INVOICE #: 1234PO Number:Company LOGOField (purchase_order): [PO Number, PO #]Total:   100.00value00000001localized keyFigure 1: Field extraction from forms is to ex tract the value for each Ô¨Åeld, e.g., invoice_number , purchase_order andtotal_amount , in a given list. A key, e.g., INVOICE #, PO Number and Total, refers to a concrete text representation of a Ô¨Åeld in a form and it is an important indicator for value localization. (2020) propose a representation learning method that takes Ô¨Åeld and value candidates as inputs and utilizes metric learning techniques to enforce high pairing score for positive Ô¨Åeldvalue pairs and low score for negative ones. LayoutLM (Xu et al., 2020) is a pretrained transformer that takes both text and their locations as inputs. It can be used as a Ô¨Åeldtagger which predicts Ô¨Åeld tags for in put texts. These methods show promising results, but they require large amount of Ô¨Åeldlevel annota tions for training. Acquiring Ô¨Åeldlevel annotations of forms is challenging and sometimes even im possible since (1) forms usually contain sensitive information, so there is limited public data avail able; (2) working with external annotators is also infeasible, due to the risk of exposing private in formation and (3) annotating Ô¨Åeldlevel labels is timeconsuming and hard to scale. Motivated by these reasons, we propose a Ô¨Åeld extraction system that does not require Ô¨Åeldlevel annotations for training (see Figure 2). First, we bootstrap the training process by mining pseudo labels from unlabeled forms using simple rules. Then, a transformerbased architecture is used toarXiv:2110.04282v2  [cs.CV]  11 Apr 2022model interactions between text tokens in the form and predict a Ô¨Åeld tag for each token accordingly. The pseudolabels are used to supervise the trans former training. Since the pseudolabels are noisy, we propose a reÔ¨Ånement module to improve the learning process. SpeciÔ¨Åcally, the reÔ¨Ånement mod ule contains a sequence of branches, each of which conducts Ô¨Åeld tagging and generates reÔ¨Åned labels. At each stage, a branch is optimized by the labels ensembled from all previous branches to reduce label noise. Our method shows strong performance on real invoice datasets. Each designed module is validated via comprehensive ablation experiments. Our contribution is summarized as follows: (1) to the best of our knowledge, this is the Ô¨Årst work that addresses the problem of Ô¨Åeld extraction from forms without using Ô¨Åeldlevel labels; (2) we pro pose a novel training framework where simple rules are Ô¨Årst used to bootstrap the training process and a transformerbased model is used to improve per formance; (3) our proposed reÔ¨Ånement module is demonstrated as effective to improve model per formance when trained with noisy labels and (4) to facilitate future research, we introduce the INV CDIP dataset as a public benchmark. The dataset is available at https://github.com/salesforce/invcdip. 2 Related Work "
26,Building a Noisy Audio Dataset to Evaluate Machine Learning Approaches for Automatic Speech Recognition Systems.txt,"Automatic speech recognition systems are part of people's daily lives,
embedded in personal assistants and mobile phones, helping as a facilitator for
human-machine interaction while allowing access to information in a practically
intuitive way. Such systems are usually implemented using machine learning
techniques, especially with deep neural networks. Even with its high
performance in the task of transcribing text from speech, few works address the
issue of its recognition in noisy environments and, usually, the datasets used
do not contain noisy audio examples, while only mitigating this issue using
data augmentation techniques. This work aims to present the process of building
a dataset of noisy audios, in a specific case of degenerated audios due to
interference, commonly present in radio transmissions. Additionally, we present
initial results of a classifier that uses such data for evaluation, indicating
the benefits of using this dataset in the recognizer's training process. Such
recognizer achieves an average result of 0.4116 in terms of character error
rate in the noisy set (SNR = 30).",2 Related Work 2 
27,Hydra: an Ensemble of Convolutional Neural Networks for Geospatial Land Classification.txt,"We describe in this paper Hydra, an ensemble of convolutional neural networks
(CNN) for geospatial land classification. The idea behind Hydra is to create an
initial CNN that is coarsely optimized but provides a good starting pointing
for further optimization, which will serve as the Hydra's body. Then, the
obtained weights are fine-tuned multiple times with different augmentation
techniques, crop styles, and classes weights to form an ensemble of CNNs that
represent the Hydra's heads. By doing so, we prompt convergence to different
endpoints, which is a desirable aspect for ensembles. With this framework, we
were able to reduce the training time while maintaining the classification
performance of the ensemble. We created ensembles for our experiments using two
state-of-the-art CNN architectures, ResNet and DenseNet. We have demonstrated
the application of our Hydra framework in two datasets, FMOW and NWPU-RESISC45,
achieving results comparable to the state-of-the-art for the former and the
best reported performance so far for the latter. Code and CNN models are
available at https://github.com/maups/hydra-fmow","Land use is a critical piece of information for a wide range of applications, from humanitarian to military pur poses. For this reason, automatic land use classiÔ¨Åcation from satellite images has been drawing increasing attention from academia, industry and government agencies [1]. This research problem consists in classifying a target location in a satellite image as one of the classes of interest or as none of them. It is also common to have metadata associated with these images. Figure 1 illustrates a typical input data used for land use classiÔ¨Åcation. There are many factors that make the land classiÔ¨Åcation problem very challenging: Clutter: satellite images cover a large piece of land and may include a variety of elements ( e.g., objects, R. Minetto is with Universidade Tecnol¬¥ ogica Federal do Paran¬¥ a (UTFPR), Brazil. Email: rodrigo.minetto@gmail.com M. P . Segundo is with Universidade Federal da Bahia (UFBA), Brazil. Email: mauriciops@ufba.br S. Sarkar is with Department of Computer Science and Engineering, Uni versity of South Florida (USF), Tampa, FL, USA. Email: sarkar@usf.edu The research in this paper was conducted while the authors were at the Computer Vision and Pattern Recognition Group, USF.Spectral: 4/8 band multispectral images . . .  Temporal SpatialSpatialTextual metadata gsd: 0.44421135.. cloud cover: 25 date: 20130825 time 18:47:39 utm: 11S country: USA ... Textual metadata gsd: 0.57625807.. cloud cover: 0 date: 20170302 time 18:21:45 utm: 11S country: USA ... Fig. 1. Typical data associated with aerial images for land use classiÔ¨Å cation. It provides spatial, temporal, spectral and metadata information together with the annotation of target locations (example taken from the FMOW dataset [1]). buildings, and vegetation), making it hard to classify a target place. Viewpoint: aerial images can be acquired from dif ferent angles depending on the satellite location, which could considerably change the appearance of the imaged content. Occlusion: some parts of a target place may not be visible in aerial images due to viewpoint variations, cloud cover or shadows. Time: variations over time affect the appearance in different ways. Shortterm temporal variations include illumination changes and movable objects, such as vehicles and temporary facilities, while long term variations consist of major topographic changesarXiv:1802.03518v2  [cs.CV]  20 Mar 2019TO APPEAR IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, 2019 2 caused by construction, weather, among others. Scale: there is a huge variation in size between differ ent target types ( e.g., airport versus swimming pool) and sometimes even between multiple instances of the same target type ( e.g., a parking lot), which is not easy to process with a single approach. Functionality: sometimes targets with similar ap pearance may have completely different functions. For instance, a similar building structure could be used as an ofÔ¨Åce, a police station or an educational facility. Stateoftheart methods tackle these difÔ¨Åculties by train ing a single Convolutional Neural Network (CNN) classiÔ¨Åer over a large dataset of satellite images seeking to learn a generalizable model [1], [2], [3], [4], which were shown to outperform earlier works based on handcrafted features. Even though it is well known that ensembles of classi Ô¨Åers improve the performance of their individual counter parts [5], this solution was not exploited by these works, probably due to its high training cost for large datasets. However, ensembles tackle one of the most common prob lems in multiclass classiÔ¨Åcation, which is the existence of several critical points in the search space (saddle points and plateaus) that prioritize some classes over others and the eventual absence of a global minimum within the classiÔ¨Åer search space. An ensemble ends up expanding this space by combining multiple classiÔ¨Åers that converged to different endpoints and reaches a better global approximation. As our main contribution, we present a faster way of creating ensembles of CNNs for land use classiÔ¨Åcation in satellite images, which we called Hydra for its similarity in shape to the namesake mythical creature (see Figure 2). The idea behind Hydra is to create an initial CNN that is coarsely optimized but provides a good starting pointing for further optimization, which will serve as the Hydra‚Äôs body. Then, the obtained weights are Ô¨Ånetuned multiple times to form an ensemble of CNNs that represent the Hydra‚Äôs heads. We take certain precautions to avoid affecting the diversity of the classiÔ¨Åers, which is a key factor in ensemble performance. By doing so, we were able to cut the training time approximately by half while maintaining the classiÔ¨Å cation performance of the ensemble. Figure 2 illustrates this process, in which a black line represents the optimization of the body and the red lines represent the heads reaching different endpoints. To stimulate convergence to different endpoints during training and thus preserve diversity, we exploit different strategies, such as online data augmenta tion, variations in the size of the target region, and changing image spectra. 2 R ELATED WORK "
28,Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition.txt,"Noisy label Facial Expression Recognition (FER) is more challenging than
traditional noisy label classification tasks due to the inter-class similarity
and the annotation ambiguity. Recent works mainly tackle this problem by
filtering out large-loss samples. In this paper, we explore dealing with noisy
labels from a new feature-learning perspective. We find that FER models
remember noisy samples by focusing on a part of the features that can be
considered related to the noisy labels instead of learning from the whole
features that lead to the latent truth. Inspired by that, we propose a novel
Erasing Attention Consistency (EAC) method to suppress the noisy samples during
the training process automatically. Specifically, we first utilize the flip
semantic consistency of facial images to design an imbalanced framework. We
then randomly erase input images and use flip attention consistency to prevent
the model from focusing on a part of the features. EAC significantly
outperforms state-of-the-art noisy label FER methods and generalizes well to
other tasks with a large number of classes like CIFAR100 and Tiny-ImageNet. The
code is available at
https://github.com/zyh-uaiaaaa/Erasing-Attention-Consistency.","Facial Expression Recognition (FER) has wide applications in the real world, such as driver fragile detection, service robots, and humancomputer interac tion [35]. The most common paradigm for FER is the endtoend supervised manner, whose performance largely relies on the massive highquality annotated data. However, collecting largescale datasets with fully precise annotations is usuallyexpensiveandtimeconsuming,sometimesevenimpossible.Furthermore, facialexpressionimageshaveinherentinterclasssimilarity(allclassesarehuman faces) and annotation ambiguity (some expression images are quite confusing), making noisy label FER more challenging than traditional noisy label classifica tion tasks. On the other hand, it is wellknown that deep neural networks have enough capacity to memorize largescale data with even completely random la bels, leading to poor performance in generalization [2,19,48]. Therefore, robustarXiv:2207.10299v2  [cs.CV]  20 Sep 20222 Y. Zhang et al. FER with noisy labels has become an essential and challenging task in computer vision [4,7,9,18,35,38,47,49,50]. Mainstream noisy label FER methods can be mainly classified into two cat egories, sample selection and label ensembling. SCN [38] and RUL [50] can be viewed as sample selection methods, which learn more from clean samples and then relabel the noisy samples. SCN [38] uses a fullyconnected layer to learn an importance weight for each sample and suppresses uncertain samples during the trainingphase.RUL[50]learnsuncertaintyweightsthroughcomparisonbetween different samples. IPA2LT [35] and DMUE [35] are label ensembling methods, which provide several labels for a single sample to better mine the latent truth. IPA2LT [35] assigns each sample more than one labels with human annotations or model predictions while DMUE [35] uses a multibranch model to better mine the latent distribution in the label space. All the aforementioned methods get good performances under noisy label FER while they still have defects. Specif ically, sample selection methods are based on the smallloss assumption [2,48], which might confuse hard samples and noisy samples as both of them have large loss values during the training process. Sample selection methods also need the noise rate, which is nontrivial in largescale realworld datasets. Label ensem bling methods provide different views of the same sample using several networks, similar to crowdsourcing in real FER applications. However, the extra informa tiongaintheybringmightbenoisy.Labelensemblingmethodsmightbringgreat computation overhead, making them less preferable in real applications. Thus, the noisy label FER problem demands better methods that do not need to know the noise rate or train several models to perform well. In this paper, instead of following the traditional path to detect noisy sam ples according to their loss values and then suppress them, we view noisy label learning from a new featurelearning perspective and propose a novel framework to deal with all the aforementioned defects. We find that the FER model remem bers noisy samples by focusing on a part of the features that can be considered related to the noisy labels, shown in Figure 1. The image in the first column is labeled as sad, while its latent truth is surprise. SCN [38] remembers this noisy sample by focusing on the frown feature which can be considered related to the noisy label of the sad expression. However, it neglects the open mouth feature, which is vital for the correct classification as an open mouth combined with a frown leads to the latent truth surprise instead of the noisy label sad. From the attention regions of the noisy samples, we conclude that the FER model only observes a part of the features that can be considered related to the noisy la bels to remember noisy samples. It is intuitive as remembering noisy samples by focusing on a part of the features that can be considered related to the noisy labels does not contradict the other learned features from the clean samples. Inspired by this finding, we propose to deal with noisy label FER from a new featurelearning perspective. If the model can not focus on a part of the features and always learns from the whole features, then it cannot remember the noisy samples. Learning from the whole features from all training samples also meansLearn From All 3 Fig.1: (a) shows the attention regions of the noisy samples learned by SCN and EAC (Ours). NL represents the noisy label, LT represents the latent truth. The prediction results are shown under the images. SCN only focuses on a part of the features that can be considered related to the noisy labels to remember the noisysamples.(b)showsSCNpredictsdifferentlyontheflippedimage.OurEAC forces the model to focus on similar parts before and after the flip to prevent the model from remembering noisy labels. the model does not need to filter out largeloss samples like traditional methods which might confuse useful hard samples with noisy samples. In this paper, we use Attention Consistency to implement the consistency regularization. Attention Consistency [11] assumes that the learned attention maps should follow the same transformation as the input images to achieve better multilabel classification performance. The attention maps denote the features that the model based on to make the predictions. We find that the flip semantic consistency of facial expression images can help to detect noisy labels. Flip semantic consistency means the original image and its flipped counterpart should be classified into the same category. However, if we train a FER model with a noisy sample, the model might remember the noisy sample while it still predicts the latent truth on its flipped counterpart, shown as the images in the first row of Figure 1. Inspired by that, we propose an imbalanced framework to prevent the model from remembering noisy samples. Specifically, we onlycompute classification loss on the original images and com pute consistency loss between the attention maps extracted from the original images and their flipped counterparts. We utilize the consistency loss to prevent the model from remembering a part of the features of the original images. Such an imbalanced framework cannot help the model totally get rid of the noisy labels as the model can still gradually overfit the attention maps of the flipped images to keep the consistency loss small, which degrades the regularization ef fect. We further propose Erasing Attention Consistency (EAC) to increase the performance of the imbalanced framework. Before flipping, we first randomly erase the input images during the whole training phase. During the training4 Y. Zhang et al. phase, the dynamic changing of the erased area ensures that the model can not simply remember the attention maps before and after the flip to get small con sistency loss values. When the model starts to overfit the noisy original samples by focusing on a part of the features related to the noisy labels, the attention maps of the original images will deviate largely from the attention maps of their flipped counterparts, which will lead to large consistency loss values. We set the weight of the consistency loss larger enough to ensure the model first optimizes the consistency loss. Thus, to get small consistency loss values, the model will automatically quit overfitting the noisy samples. The main contributions of our work are as follows: 1. Instead of using traditional methods which deal with noisy labels from high level smallloss selection, we cope with noisy labels from middlelevel feature learning, which does not require the noise rate to perform well. 2. We propose a novel method named Erasing Attention Consistency (EAC) which automatically prevents the model from memorizing noisy samples. 3. We experimentally show that EAC significantly advances stateoftheart results on multiple FER benchmarks with different levels of label noise. EAC also generalizes well to image classification tasks with a large number of classes. 2 Related Work "
29,Model-agnostic Approaches to Handling Noisy Labels When Training Sound Event Classifiers.txt,"Label noise is emerging as a pressing issue in sound event classification.
This arises as we move towards larger datasets that are difficult to annotate
manually, but it is even more severe if datasets are collected automatically
from online repositories, where labels are inferred through automated
heuristics applied to the audio content or metadata. While learning from noisy
labels has been an active area of research in computer vision, it has received
little attention in sound event classification. Most recent computer vision
approaches against label noise are relatively complex, requiring complex
networks or extra data resources. In this work, we evaluate simple and
efficient model-agnostic approaches to handling noisy labels when training
sound event classifiers, namely label smoothing regularization, mixup and
noise-robust loss functions. The main advantage of these methods is that they
can be easily incorporated to existing deep learning pipelines without need for
network modifications or extra resources. We report results from experiments
conducted with the FSDnoisy18k dataset. We show that these simple methods can
be effective in mitigating the effect of label noise, providing up to 2.5\% of
accuracy boost when incorporated to two different CNNs, while requiring minimal
intervention and computational overhead.","Deep neural networks require large and varied data resources in or der to show their superior performance with respect to traditional machine learning methods, a fact that has become evident in Ô¨Åelds like computer vision. In the less explored Ô¨Åeld of sound event recog nition, we are currently moving from small and exhaustively labeled datasets of few hours of duration [1, 2, 3], towards larger datasets in the range of tens (e.g., FSDKaggle2018 [4] or FSDnoisy18k [5]) to thousands (e.g., AudioSet [6]) of hours of audio. The increasing size of datasets makes it hard to manually label the audio content reliably as it turns out to be difÔ¨Åcult and costly. This inevitably in curs in a certain amount of label noise either due to incomplete or incorrect annotations, even if produced by trained humans. Online repositories such as Freesound or Youtube host signiÔ¨Å cant volumes of audio content with associated metadata that can be used to create audio datasets. Labels can be inferred automatically by applying automated heuristics to the metadata, or pretrained classiÔ¨Åers on the audio content. While this way of collecting la beled data is much faster than the conventional dataset creation, the level of label noise generated can be much more severe. Hence, la bel noise is a problem in largescale sound event classiÔ¨Åcation that This work is partially supported by the European Union‚Äôs Horizon 2020 research and innovation programme under grant agreement No 688382 Au dioCommons and a Google Faculty Research Award 2018.can hinder the proper learning of classiÔ¨Åers, especially if they are based on deep neural networks [7, 8]. The topic of learning with noisy labels is an active area of re search in computer vision. The stateoftheart is based on selecting the clean data instances in the train set in order to train the network satisfactorily [9, 10, 11]. However, those methods can turn rela tively complex as they leverage two networks (sometimes trained simultaneously). Other methods rely on estimating the noise tran sition matrix, i.e., the probability of each true label being Ô¨Çipped into another [12, 13]. However, such estimation is not trivial, and it assumes that the only possible type of noise is Ô¨Çipping labels. Other approaches use noiserobust loss functions to mitigate the ef fect of label noise [14], or leverage an additional set of curated data, for example to train a label cleaning network in order to reduce the noise of a dataset [15]. Conversely, learning with noisy labels has received little attention in sound recognition, probably given the traditional paradigm of learning from relatively small and ex haustively labeled (hence clean ) datasets. In our previous work, the FSDnoisy18k dataset is presented along with an evaluation of noise robust loss functions [5]. In [16], two networks operating on differ ent views of the data coteach each other to learn from noisy labels. Recently, the topic of label noise was included for the Ô¨Årst time as one of the research problems in the DCASE2019 Challenge [17]. Most of the aforementioned approaches against label noise re quire complex networks (often more than one) or extra data re sources. Given the early stage of this Ô¨Åeld in sound event classiÔ¨Åca tion, we are interested in exploring simple and efÔ¨Åcient approaches, agnostic to network architecture, that can mitigate the effect of la bel noise. SpeciÔ¨Åcally, we seek approaches that can be plugged into existing learning settings composed by a noisy dataset and a deep network, without need for network modiÔ¨Åcations or extra resources. Our contribution is to provide insight on the modelagnostic ap proaches that can be incorporated to deep learning pipelines for sound event classiÔ¨Åcation in presence of noisy labels, as well as the performance boost that can be expected. In particular, we con sider regularization techniques external to the model, as well as noiserobust loss functions (Fig. 1). Regularization aims to prevent overÔ¨Åtting and improve generalization, which can also be beneÔ¨Å cial against label noise. Common regularization strategies include weight decay and dropout [18], which act on the weights or hid den units of the network; dropout has been shown useful in reduc ing label noise memorization [7]. In our attempt to regularize the model from the outside, we consider label smoothing regularization (LSR) and mixup . The former operates on the ground truth labels, while the latter operates on both ground truth labels and input data (Fig. 1). In addition, we explore two strategies to ignore poten tially noisy instances in the learning process through noiserobust loss functions. Section 2 describes the methods considered. Section 3 introduces the experimental setup. Section 4 describes the exper iments carried out and the results. Section 5 provides Ô¨Ånal remarks.arXiv:1910.12004v1  [cs.SD]  26 Oct 20192019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 2023, 2019, New Paltz, NY loss f(x) TF mixup deep network  predictions labels  LSRnoiserobust  loss f(x)  Figure 1: Sketch of the modelagnostic approaches against label noise considered (in red), indicating the component(s) of the learn ing pipeline where they operate. 2. METHODS "
30,Adversarially Optimized Mixup for Robust Classification.txt,"Mixup is a procedure for data augmentation that trains networks to make
smoothly interpolated predictions between datapoints. Adversarial training is a
strong form of data augmentation that optimizes for worst-case predictions in a
compact space around each data-point, resulting in neural networks that make
much more robust predictions. In this paper, we bring these ideas together by
adversarially probing the space between datapoints, using projected gradient
descent (PGD). The fundamental approach in this work is to leverage
backpropagation through the mixup interpolation during training to optimize for
places where the network makes unsmooth and incongruous predictions.
Additionally, we also explore several modifications and nuances, like
optimization of the mixup ratio and geometrical label assignment, and discuss
their impact on enhancing network robustness. Through these ideas, we have been
able to train networks that robustly generalize better; experiments on CIFAR-10
and CIFAR-100 demonstrate consistent improvements in accuracy against strong
adversaries, including the recent strong ensemble attack AutoAttack. Our source
code would be released for reproducibility.","The vulnerability of neural networks to adversarial at tack has been plaguing machine learning researchers ever since the discovery by Szegedy et al. [18]. In the years since, many research efforts have been geared towards mak ing neural networks robust to adversarial perturbations, but many defense strategies have failed to stand the test of time. One of the strongest baselines for adversarial ro bustness that has repeatedly stood up to rigorous scrutiny is adversarial training [6], in particular adversarial training based on the Projected Gradient Descent (PGD) attack strat egy [14]. PGD adversarial training can be mathematically represented as a constrained inner minmax optimization, whose solution gives us a minimal perturbation that max imizes classiÔ¨Åcation loss. This minmax optimization isa characteristic of many approaches to adversarial robust training, including in other related formulations of the loss, like TRADES [27]. Such networks have repeatedly withstood numerous evaluations, as shown in works like [3]. One concern is that the robust accuracy of such networks on test sets leaves much to be desired. For instance, state of the art neural net works typically have less than 50% accuracyunderattack for the CIFAR10 dataset. This low robust accuracy occurs despite the fact that the network is able to memorize the ro bust (`1bounded boxed) training distribution with nearly 100% accuracy against its own attack model, as a result of PGD adversarial training. This points to a familiar prob lem for machine learning practitioners ‚Äì overÔ¨Åtting to the training set. As recent work has pointed out [22], robust adversarial training is just as susceptible to ‚ÄúoverÔ¨Åtting‚Äù as standard neural network training, because the robust train ing distribution is an incomplete subsampling or imperfect match to the robust test distribution due to the Ô¨Ånite size of the dataset. Recent developments have also shown that adversarial vulnerability is a problem of networks severely overÔ¨Åtting to features that are imperceptibly small yet useful for clas siÔ¨Åcation [11]. With this view, adversarial training is an advanced form of worstcase data augmentation. This view has been shown in [23] to be able to usefully improve ac curacy under various corruptions including ‚Äúnatural adver sarial examples‚Äù [10] with certain domain adaptation tools. Hence, improvements in adversarial training are useful not just for security purposes, but for a wide audience who de sires robust classiÔ¨Åcation to be more understandably as well as consistently generalizeable. Our approach focuses on the poor and overÔ¨Åtting accuracyunderattack of robust training. We frame together the effective mixup augmentation [26] introduced for typi cal (nonadversarial) training, and robust PGD adversarial optimization. We use adversarial optimization to pinpoint the locations in the interpolation space between datapoints, where the classiÔ¨Åcation decisions of the neural networks are the least smooth, i.e. that adversarially maximize the 1arXiv:2103.11589v1  [cs.LG]  22 Mar 2021KL divergence between the network‚Äôs predictions and the smoothed label interpolation between datapoints. Robust adversarial learning is susceptible to overÔ¨Åtting, and we show that data augmentation insights from standard training transfer well to adversarial optimization. Our con tributions include: ‚Ä¢ We show through intuitive geometry and empirical re sults that previous works integrating mixup and ad versarial optimization were limited in their ability to probe the vicinal distribution to Ô¨Ånd worstcase points. It is important for the adversarial optimization to be able to fully Ô¨Ånd the worstcase points to learn from. ‚Ä¢ With ablation experiments we break down the opti mization components that led our results to surpass the baselines. ‚Ä¢ Our approach demonstrates signiÔ¨Åcant improvements in robust accuracyunderattack against strong, state oftheart adversaries. We evaluate against an ensem ble of stateoftheart adversaries including a strong gradientfree blackbox attack, demonstrating that our approach provides real improvements that do not in troduce or rely on any gradient obfuscation. The rest of this paper is organized as follows. In Sec tion 2, we summarize related work exploring mixup for data augmentation or adversarial robustness. We present the background of this work and the details of our approach in Section 3. Section 4 presents the experimental results of our work, which are then comprehensively discussed in Section 5. Finally, we conclude this paper and discuss future av enues of research in Section 6. 2. Related Work "
31,Decoupling Representation and Classifier for Noisy Label Learning.txt,"Since convolutional neural networks (ConvNets) can easily memorize noisy
labels, which are ubiquitous in visual classification tasks, it has been a
great challenge to train ConvNets against them robustly. Various solutions,
e.g., sample selection, label correction, and robustifying loss functions, have
been proposed for this challenge, and most of them stick to the end-to-end
training of the representation (feature extractor) and classifier. In this
paper, by a deep rethinking and careful re-examining on learning behaviors of
the representation and classifier, we discover that the representation is much
more fragile in the presence of noisy labels than the classifier. Thus, we are
motivated to design a new method, i.e., REED, to leverage above discoveries to
learn from noisy labels robustly. The proposed method contains three stages,
i.e., obtaining the representation by self-supervised learning without any
labels, transferring the noisy label learning problem into a semisupervised one
by the classifier directly and reliably trained with noisy labels, and joint
semi-supervised retraining of both the representation and classifier. Extensive
experiments are performed on both synthetic and real benchmark datasets.
Results demonstrate that the proposed method can beat the state-of-the-art ones
by a large margin, especially under high noise level.","Convolutional neural networks (ConvNets) [17, 33] have achieved remarkable success in many computer vision tasks, e.g., image classiÔ¨Åcation [26, 53] and object detec tion [16, 25], because of their ability to model complex patterns. To fully exploit the learning ability of ConvNets, largescale and wellannotated datasets, e.g., ImageNet [50] and COCO [38], are needed. However, noisy labels are ubiquitous and inevitable, since such large and accurate datasets are expensive and timeconsuming to acquire. Be sides, modern ConvNets can easily overÔ¨Åt and memorize these noisy labels due to overparameterization, which sub sequently leads to very poor generalization [2, 43, 68]. Thus, how to train ConvNets robustly against noisy labelsTable 1. A comparison between the proposed REED with stateof theart methods. Manner: how a method learn from noisy labels; Semi: can semisupervised learn from samples with wrong labels; HN: can deal with high noise level; NCln: no need for extra clean training data. MethodLearning Properties manner Semi HN NCln CoTeaching [21] end2end 7 7 X Fcorrection [44] end2end 7 7 X Ren et al. [49] end2end 7 7 7 PENCIL [66] end2end 7 7 X Mcorrection [1] end2end 7 7 X NLNL [32] end2end X 7 X Zhang et al. [72] end2end 7 X 7 DivideMix [35] end2end X 7 X REED (ours) decoupled X X X becomes a problem of great importance. Recently, many approaches have been proposed to ro bustly learn from noisy labels [20], and they generally fol low three directions, i.e., sample selection [21, 29, 35, 51, 60, 67], label correction [22, 54, 56, 62, 66], and robustify ing loss functions [8, 14, 32, 39, 42, 55, 71]. SpeciÔ¨Åcally, sample selection methods construct some criterion attempt ing to pick up samples with clean labels for training. These criterion, e.g., smallloss [21, 29] and disagreement [41, 67], usually rely on the memorization effect [2, 68] of deep networks. Label correction methods attempt to directly cor rect the possibly noisy labels. They achieve this purpose by, e.g., pseudo labeling technologies [34], using class proto type [22], or even treating label as learnable and latent vari ables [54, 66]. Finally, since the existing loss functions for classiÔ¨Åcation, e.g., categorical cross entropy (CE) [14] and focal loss [37], can be skewed by noisy labels [71], more robust loss functions are proposed. Examples are general ized CE loss [71] and curriculum loss [39]. They are less biased on noisy labels and can be learned together with rep 1arXiv:2011.08145v1  [cs.CV]  16 Nov 2020resentation. Indeed, due to the superior performance resulting from the endtoend training of deep networks [17], most afore mentioned methods also jointly learn the representation and classiÔ¨Åer in an endtoend manner. However, such a joint learning scheme neglects an important issue  is there any difference in the learning behaviors between the represen tation and classiÔ¨Åer with noisy labels? Intuitively, when the representation is good enough,1the decision boundary of the classiÔ¨Åer can be easy to Ô¨Ånd even there is strong noise [23, 58]. Thus, we decouple the training scheme with noisy labels into representation and classiÔ¨Åer learning, and then look inside their learning behaviors. Interestingly, we discover that noisy labels will damage the representation learning much more signiÔ¨Åcantly than classiÔ¨Åer learning, and the classiÔ¨Åer itself indeed can exhibit strong robustness w.r.t. noisy labels with a good representation. These dis coveries are not noticed previously and new to the literature of noisy label learning. Thus, instead of the classical endtoend training method, which can lead to suboptimal performance, we are motivated to decouple the representation and classiÔ¨Åer in noisy label learning. The proposed method, named REED, contains three stages and can take good care of both repre sentation and classiÔ¨Åer by leveraging the above discoveries (see Table 1). SpeciÔ¨Åcally, in the Ô¨Årst stage, inspired by the recent advances of selfsupervised representation learning technologies [10, 24], we learn the representation through a contrastive pretext task. Then, in the second stage, we uti lize the intrinsic robustness in classiÔ¨Åer learning to obtain a reliable classiÔ¨Åer with noisy labels, which helps to trans fer the noisy label learning into a semisupervised learning problem. Finally, to fully explore the information in the transferred labels, we construct a classbalanced sampler and graphstructured regularizer to jointly Ô¨Ånetune the rep resentation and classiÔ¨Åer in the third stage. Contributions of this paper are as follows: ‚Ä¢ We decouple the classical endtoend training procedures into representation learning and classiÔ¨Åer learning and systematically explore their robustness in the presence of noisy labels respectively. We discover that representation matters much more than the classiÔ¨Åer, since the represen tation is very fragile while classiÔ¨Åer can exhibit strong robustness in the presence of noisy labels. ‚Ä¢ We propose an effective and threestage learning man ner, i.e., REED, which leverages selfsupervised repre sentation learning to solve the fragility of representation learning and make full use of the robustness of the clas siÔ¨Åer. Also by assigning credibility to samples and two improvements for semisupervised learning (i.e., graph structured regularization and classbalanced sampler), we 1We offer a detailed example in Appendix ??.further improve the ability of classiÔ¨Åcation. ‚Ä¢ We perform extensive experiments on both synthetic, i.e., noisy CIFAR10 and CIFAR100 datasets, and real benchmark, i.e., Clothing1M dataset. Results demon strate that the proposed method can beat the stateofthe art ones by a large margin, especially under high noise level. Effectiveness of each stage is also elaborated in ablation studies. Notations. Letxidenote the image, yibe the clean label, andyibe the noisy version of yi. A ConvNet typically con tains two parts, one is the representation learning part, i.e., zi=h(xi;), whereziis the representation for xiandh is implemented by the feature extractor (i.e., deep stack of convolutional and pooling layers) with parameter ; another part is the classiÔ¨Åer, i.e., f=g(zi;W), whereWis the pa rameters of a multilayer perceptron (MLP). Its prediction probability (or conÔ¨Ådence) for the ith class is given by a softmax function as pi=efi=PC k=1efk, whereCis the number of classes. Crossentropy loss is used for training, i.e.,Lce(p;y) = PC k=1yklogpk, and the class with the highest conÔ¨Ådence, i.e., y0= arg max i(p), is taken as the prediction for the sample. 2. Related work "
32,A Multi-cascaded Model with Data Augmentation for Enhanced Paraphrase Detection in Short Texts.txt,"Paraphrase detection is an important task in text analytics with numerous
applications such as plagiarism detection, duplicate question identification,
and enhanced customer support helpdesks. Deep models have been proposed for
representing and classifying paraphrases. These models, however, require large
quantities of human-labeled data, which is expensive to obtain. In this work,
we present a data augmentation strategy and a multi-cascaded model for improved
paraphrase detection in short texts. Our data augmentation strategy considers
the notions of paraphrases and non-paraphrases as binary relations over the set
of texts. Subsequently, it uses graph theoretic concepts to efficiently
generate additional paraphrase and non-paraphrase pairs in a sound manner. Our
multi-cascaded model employs three supervised feature learners (cascades) based
on CNN and LSTM networks with and without soft-attention. The learned features,
together with hand-crafted linguistic features, are then forwarded to a
discriminator network for final classification. Our model is both wide and deep
and provides greater robustness across clean and noisy short texts. We evaluate
our approach on three benchmark datasets and show that it produces a comparable
or state-of-the-art performance on all three.","In recent years, short text in the form of posts on microblogs, question answer forums, news headlines, and tweets is being generated in abundance [1]. Performing NLP tasks is relatively easier in longer documents (e.g. news articles) than in short texts (e.g. headlines) because, in longer documents, greater context is available for semantic understanding [2]. Moreover, in many cases, short texts (e.g. tweets) tend to use informal language (spelling variations, improper grammar, slang) compared to longer documents (e.g. blogs). Thus, the techniques tailored for formal and clean text do not perform well on informal one [3], which call for a need to develop an approach that can work in both settings (i.e., clean and noisy informal text) [4]. A paraphrase of a document is another document that can be dierent in syntax, but that expresses the same meaning in the same language. Automatically detecting paraphrases among a set of documents has many signicant applications in natural language processing (NLP) and information retrieval (IR) such as plagiarism detection [5], query ranking [6], duplicate question detection [7, 8], web searching [9], and automatic question answering [10]. Paraphrase detection is a binary classication problem in which pairs of texts are labeled as either positive (paraphrase) or negative (nonparaphrase). In this setting, pairs of texts are mapped into a xed dimensional featurespace, where a standard classier is learned. Feature maps based on lexical, syntactic Muhammad Haroon Shakeel Email addresses: 15030040@lums.edu.pk (Muhammad Haroon Shakeel), akarim@lums.edu.pk (Asim Karim), imdad.khan@lums.edu.pk (Imdadullah Khan) Preprint submitted to Journal December 30, 2019arXiv:1912.12068v1  [cs.CL]  27 Dec 2019and semantic similarities in conjunction with SVM are proposed in [3, 11]. More recently, it has been demonstrated that for short text, deep learningbased pairs representations and classication yield better accuracy [4]. Many deep learningbased schemes employ one or two Convolutional Neural Network (CNN) or Long Short Term Memory (LSTM) based models to learn features and make predictions on clean texts [7, 12, 13], while a recent model also incorporates linguistic features to detect paraphrases in both clean and noisy short texts [4]. For many NLP tasks involving short texts, it has been shown that developing wider models can yield signicant gains [14]. While deep models produce richer representations, they require large amounts of training data for a robust paraphrase detection system [3]. Thus, for small datasets, such as Microsoft Research Paraphrase (MSRP) corpus and SemEval2015 Twitter paraphrase dataset (SemEval), handcrafted features and SVM classier have been widely used [3, 15]. Labeling pairs of documents in a humanbased computation setting (e.g. crowdsourcing) is costly [16]. Therefore, [4] and [13] add to the training set each labeled pair also in the reversed order. However, this simple data augmentation strategy can be extended in a systematic manner by relying upon set and graph theory. For instance, consider four documents: ( a)How can I lose weight quickly? (b)How can I lose weight fast? (c)What are the ways to lose weight as soon as possible? (d)Will Trump win US elections? . If in the annotated corpus, documents aandband documents band care marked as paraphrases, then by transitive extension, documents aandccan also be considered as paraphrases. Similarly, if documents aandbare labeled as paraphrase, while documents banddare labeled as nonparaphrase, then a new nonparaphrase pair based on document aanddcan be inferred reliably. Such a strategy can be used to generate additional annotations in a sound and costeective manner, and potentially enhance the performance of deep learning models for paraphrase detection. In this paper, we propose a data augmentation strategy for generating additional paraphrase and non paraphrase annotations reliably from existing annotations. We consider notions of paraphrases and non paraphrases as binary relations over the set of documents. Representing the binary relation induced by the paraphrase labels as an undirected graph and performing transitive closure on this graph, we include addi tional paraphrase annotation in the training set. Similarly, by comparing paraphrase and nonparaphrase annotations we infer additional nonparaphrase annotations for inclusion in the training corpus. Our strat egy involves several steps and a parameter through which the data augmentation can be tuned for enhanced paraphrase detection. We also present a robust multicascaded deep learning model for paraphrase detection in short texts. Our model utilizes three independent CNN and LSTM (with and without soft attention) cascades for feature learning in a supervised manner. We also employ a number of additional linguistic features after corpus specic text preprocessing. All these features are fed into a discriminator network for nal classication. To show eectiveness of our approach we evaluate the data augmentation and deep model on three benchmark short text datasets (MSRP and Quora (clean), and SemEval (noisy)). We also perform extensive comparisons with the stateoftheart methods. We make the following key contributions in this work: We present an ecient strategy for augmenting existing paraphrase and nonparaphrase annotations in a consistent manner. This strategy generates additional annotations and enhances the performance of the datahungry deep learning models. We develop a multicascaded learning model for robust paraphrase detection in both clean and noisy texts. This model incorporates multiple learned and linguistic features in a wide and deep discriminator network for paraphrase detection. We address both clean and noisy texts in our presentation and show that the proposed model matches current best performances on benchmark datasets of both types. We analyze the impact of various data augmentation steps and dierent components of the multi cascaded model on paraphrase detection performance. The rest of the paper is organized as follows: We discuss the related work in paraphrase detection and data augmentation in Section 2. We present our data augmentation strategy in Section 3. Our multi 2cascaded model for paraphrase detection is presented in Section 4. Section 5 outlines our experimental evaluation setup including discussion of data augmentation. We present and discuss the results of our approach in Section 6. Finally, we present our concluding remarks in Section 7. 2. Related Work "
33,An Efficient Method of Training Small Models for Regression Problems with Knowledge Distillation.txt,"Compressing deep neural network (DNN) models becomes a very important and
necessary technique for real-world applications, such as deploying those models
on mobile devices. Knowledge distillation is one of the most popular methods
for model compression, and many studies have been made on developing this
technique. However, those studies mainly focused on classification problems,
and very few attempts have been made on regression problems, although there are
many application of DNNs on regression problems. In this paper, we propose a
new formalism of knowledge distillation for regression problems. First, we
propose a new loss function, teacher outlier rejection loss, which rejects
outliers in training samples using teacher model predictions. Second, we
consider a multi-task network with two outputs: one estimates training labels
which is in general contaminated by noisy labels; And the other estimates
teacher model's output which is expected to modify the noise labels following
the memorization effects. By considering the multi-task network, training of
the feature extraction of student models becomes more effective, and it allows
us to obtain a better student model than one trained from scratch. We performed
comprehensive evaluation with one simple toy model: sinusoidal function, and
two open datasets: MPIIGaze, and Multi-PIE. Our results show consistent
improvement in accuracy regardless of the annotation error level in the
datasets.","Recent development of deep neural network (DNN) re search allows us to solve many kinds of problems with veryhigh accuracy, such as classiÔ¨Åcation, regression, and obje ct detection. To enhance DNNs‚Äô accuracy, a straightforward method is just increasing depth and channel of the network, and a considerable amount of studies have been conducted on Ô¨Ånding a method to train deeper networks effectively. Their large memory and numerical costs, however, prohibits us to apply them to realworld solutions. To alleviate this problem, many techniques have been proposed, e.g., Ô¨Ånding efÔ¨Åcient network structures [16, 27], channel pruning [14] , and quantization of network weights [17]. Knowledge dis tillation is one of the most popular methods for this purpose which tries to mimic the behavior of deeper and larger mod els (teacher) by a smaller or compressed model (student) [6, 3, 15]. Although seminal work have followed to improve the technique, those work mainly focused on the classiÔ¨Åca tion problem. On the other hand, little attention has been given on regression problems which also have many appli cations, for example, estimating age [11], gaze angle [26], and so on. Though some techniques of the above work can also be applied to regression problems, it has another in herent difÔ¨Åculty, that is, the uncertainty of giving annota  tion. In general, regression problems treat continuous var i ables as annotation, and it is unavoidable to accept a certai n amount of annotation error which originates from human errors and limitations of measurement. Since information from teacher models can also be an origin of those errors, it is necessary to consider the treatment of these errors when developing knowledge distillation for regression problem s, which no existing work tackled as far as we know. To address the above problems, we propose a method to train a fast and accurate student networks for regression problems using a newly developed knowledge distillation method. Our contributions are as follows: ‚Ä¢We propose a new formulation for knowledge distillation for regression problems which solves regression problems using a multitask network (Section 3). ‚Ä¢We propose a new loss, socalled Teacher Outlier Re jection (TOR) Loss which allows the student models to avoid suffering from outliers with the help of teacher models (Section 4.1). ‚Ä¢We present insights into the nature of regression prob lems with noisy data which was obtained from com prehensive numerical experiments (Section 5). 2 Related Work "
34,Combating noisy labels by agreement: A joint training method with co-regularization.txt,"Deep Learning with noisy labels is a practically challenging problem in
weakly supervised learning. The state-of-the-art approaches ""Decoupling"" and
""Co-teaching+"" claim that the ""disagreement"" strategy is crucial for
alleviating the problem of learning with noisy labels. In this paper, we start
from a different perspective and propose a robust learning paradigm called
JoCoR, which aims to reduce the diversity of two networks during training.
Specifically, we first use two networks to make predictions on the same
mini-batch data and calculate a joint loss with Co-Regularization for each
training example. Then we select small-loss examples to update the parameters
of both two networks simultaneously. Trained by the joint loss, these two
networks would be more and more similar due to the effect of Co-Regularization.
Extensive experimental results on corrupted data from benchmark datasets
including MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is
superior to many state-of-the-art approaches for learning with noisy labels.","Deep Neural Networks (DNNs) achieve remarkable suc cess on various tasks, and most of them are trained in a su pervised manner, which heavily relies on a large number of training instances with accurate labels [14]. However, col lecting largescale datasets with fully precise annotations is expensive and timeconsuming. To alleviate this problem, data annotation companies choose some alternating meth ods such as crowdsourcing [39, 43] and online queries [3] to improve labelling efÔ¨Åciency. Unfortunately, these methods usually suffer from unavoidable noisy labels, which have been proven to lead to noticeable decrease in performance of DNNs [1, 44]. As this problem has severely limited the expansion of neural network applications, a large number of algorithms Corresponding author.have been developed for learning with noisy labels, which belongs to the family of weakly supervised learning frame works [2, 5, 6, 7, 8, 9, 11]. Some of them focus on improv ing the methods to estimate the latent noisy transition ma trix [21, 24, 32]. However, it is challenging to estimate the noise transition matrix accurately. An alternative approach is training on selected or weighted samples, e.g., Men tornet [16], gradientbased reweight [30] and Coteaching [12]. Furthermore, the stateoftheart methods including Coteaching+ [41] and Decoupling [23] have shown excel lent performance in learning with noisy labels by introduc ing the ‚ÄúDisagreement"" strategy, where ‚Äúwhen to update"" depends on a disagreement between two different networks. However, there are only a part of training examples that can be selected by the ‚ÄúDisagreement"" strategy, and these exam ples cannot be guaranteed to have groundtruth labels [12]. Therefore, there arises a question to be answered: Is ‚ÄúDis agreement"" necessary for training two networks to deal with noisy labels? Motivated by Cotraining for multiview learning and semisupervised learning that aims to maximize the agree ment on multiple distinct views [4, 19, 34, 45], a straight forward method for handling noisy labels is to apply the regularization from peer networks when training each sin gle network. However, although the regularization may im prove the generalization ability of networks by encourag ing agreement between them, it still suffers from memoriza tion effects on noisy labels [44]. To address this problem, we propose a novel approach named JoCoR ( Joint Train ing with CoR egularization). SpeciÔ¨Åcally, we train two net works with a joint loss, including the conventional super vised loss and the CoRegularization loss. Furthermore, we use the joint loss to select smallloss examples, thereby en suring the error Ô¨Çow from the biased selection would not be accumulated in a single network. To show that JoCoR signiÔ¨Åcantly improves the robust ness of deep learning on noisy labels, we conduct exten sive experiments on both simulated and realworld noisy datasets, including MNIST, CIFAR10, CIFAR100 andarXiv:2003.02752v3  [cs.CV]  22 Apr 2020Clothing1M datasets. Empirical results demonstrate that the robustness of deep models trained by our proposed ap proach is superior to many stateoftheart approaches. Fur thermore, the ablation studies clearly demonstrate the effec tiveness of CoRegularization and Joint Training. 2. Related work "
35,Identification of 1H-NMR Spectra of Xyloglucan Oligosaccharides: A Comparative Study of Artificial Neural Networks and Bayesian Classification Using Nonparametric Density Estimation.txt,"Proton nuclear magnetic resonance (1H-NMR) is a widely used tool for chemical
structural analysis. However, 1H-NMR spectra suffer from natural aberrations
that render computer-assisted automated identification of these spectra
difficult, and at times impossible. Previous efforts have successfully
implemented instrument dependent or conditional identification of these
spectra. In this paper, we report the first instrument independent
computer-assisted automated identification system for a group of complex
carbohydrates known as the xyloglucan oligosaccharides. The developed system is
also implemented on the world wide web (http://www.ccrc.uga.edu) as part of an
identification package called the CCRC-Net and is intended to recognize any
submitted 1H-NMR spectrum of these structures with reasonable signal-to-noise
ratio, recorded on any 500 MHz NMR instrument. The system uses Artificial
Neural Networks (ANNs) technology and is insensitive to the instrument and
environment-dependent variations in 1H-NMR spectroscopy. In this paper,
comparative results of the ANN engine versus a multidimensional Bayes'
classifier is also presented.","1HNMR  spectroscopy.  NMR  (nuclear magnetic  resonance) spectroscopy  is  a widely used  tool  for chemical analysis.  It is  used  to identify  materials,  determine  the  chemical structure of organic compounds, and can be usedto  quantify  chemical  substituents  or  the components of chemical mixtures.  The proton (1H)  is  the  nuclide  that  is  most  frequently observed by NMR.  When a sample is placed in a strong magnetic field, the magnetically active nuclei become aligned.   The resulting  sample magnetization can be manipulated by applying a very brief magnetic field pulse that oscillates at radio frequency (RF).  Such RF pulses perturb the  sample  magnetization,  which  can  be observed  via  its  induction  of  a  current  in  a detector coil as the magnetization relaxes back to  its  equilibrium  state.   The  resulting  ""free induction  decay""  (FID)  contains  information regarding  the  chemical  environment  of  nuclei within  the  chemical  sample,  and  thus  can  be used  to  identify  and  quantitate  individual chemical components of the sample.  The FID consists of a mixture of sinusoidal oscillations in the timedomain with decaying amplitudes. The timedomain signal is normally transformed (usually  using  Fourier  transform)  into  the frequency domain.  Figure 1 (located at the end of  this  article),  illustrates  two  examples  of  a frequency  domain  signal  (spectrum)  of  a xyloglucan  oligosaccharide.  1HNMR spectra, in general, suffer from environmental, instrumental, and other types of variations that manifest themselves in a variety of aberrations.  Low signaltonoise ratio [1, 2, 4], baseline drifts [3, 4, 7], frequency shifts dueto temperature variations,  line broadening and negative  peaks  due  to  phasing  problems,  and malformed peaks (or overlapped peaks) due to inaccurate  shimming,  are  among  the  most prominent  and  common  aberrations.   For example, Figure 1, shows two 1HNMR spectra of  a  complex  carbohydrate.   The  spectrum labeled (B) in this figure suffer from a variety of the  above  mentioned  aberrations,  and contamination by lactate, frequently introduced by  touching  laboratory  glassware  with  bare hands.   It  is  important  to  realize  that  this spectrum by no means represents a worst case scenario, and it does not represent the level of complexity present in the problem of instrument independent identification of 1HNMR spectra of xyloglucan oligosaccharides.  Spectrum (B) is merely  a  demonstration  of  some  types  of possible aberrations. For  the  purpose  of  automated identification of these spectra, elimination of the above mentioned aberrations becomes essential, as they can lead to erroneous identification [1 7].  A variety of signal processing techniques have  been  applied  to  ""clean  up""  1HNMR spectra.  For instance, signal averaging1 [4] and apodization2 [4] have become standard ways of improving the signaltonoise ratio.  To correct baseline problems, a number of techniques have been used such as parametric modeling using a priori  knowledge  [3,  5],  optimal  associative memory  (OAM)  [5],  and  spectral  derivatives [6].  Other mathematical techniques have also 1 In signal averaging a spectrum is recorded several times.  Each recorded signal is referred to as a ‚Äútransient.‚Äù  The final spectrum is the arithmetic average of all the transients.  The hope is that by doing  so the  zero  mean  components  of  the  noise present in the signal will be averaged out. 2 Apodization is a type of low (high) pass filtering  performed  in  the  timedomain. Apodization  is  performed  by  speeding  up,  or slowing  down  the  rate  of  decay  of  timedomain exponential  functions.   This  is  accomplished  by multiplying  the  timedomain  signal  by  another function.  This technique allows the improvement of signaltonoise ratio in exchange for the reduction of signal resolution (or visa versa).been introduced to address each specific type of aberration encountered in 1HNMR spectra.  Although many of these signal processing techniques  have  enjoyed  success  in  specific applications,  they remain solutions  to specific types  of  aberrations.   In  order to  produce an overall  ‚Äúclean‚Äù  spectrum,  one  needs  to  use several  of  these  methods  to  eliminate  the aberrations  present  in  a  real  spectrum. Furthermore, most of these techniques produce side effects that are magnified when improperly processed  by  a  second  signal  processing algorithm.  Furthermore, after the initial signal processing steps have been taken, the task of identifying  the  processed  spectrum  remains. This  is  not  a  trivial  task  as  many  times  the quality of the processed spectrum remains poor, requiring a sophisticated identification system. In  this  paper  we  show  that  instead  of eliminating  all  the  present  aberrations  by  a signal processing procedure as a preprocessor, it is  possible  to  eliminate some  of  them  in  the processing  step,  and  some  in  the  actual identification  step.   Here,  we  show  that  an adaptive  identification  system  can  learn  to effectively ignore some forms of aberrations. Xyloglucan  Oligosaccharides.   Complex carbohydrates  are important  biomolecules  that play a role in many biological functions such as providing physical strength (connective tissue in animals and woody tissue in plants) and as a source of energy reserves (glycogen in animals and starch in plants).  These molecules are also known  to  be  directly  and  widely  involved  in biological recognition and regulatory processes in normal growth and development as well as in disease processes.  The recent discovery of the role  of  complex  carbohydrates  in  disease processes,  and  therefore  drug  development, among others has triggered a large number of studies in order to better understand the role of abnormal  (structurally  altered)  complex carbohydrates in disease development.  For this reason,  an  automated  identification  system  of complex carbohydrates can eliminate the many manhours  wasted  in  duplicated  efforts  in structural  characterization  of  known carbohydrates.A specific group of these molecules from plant  cell  wall  are  called  xyloglucan oligosaccharides.  The 1HNMR spectra of these molecules resemble each other to a great degree, and the experiments in developing an automated identification system for these spectra is a good indicator for the success of such future projects for automated identification of other molecules. 2. Method: "
36,CTRL: Clustering Training Losses for Label Error Detection.txt,"In supervised machine learning, use of correct labels is extremely important
to ensure high accuracy. Unfortunately, most datasets contain corrupted labels.
Machine learning models trained on such datasets do not generalize well. Thus,
detecting their label errors can significantly increase their efficacy. We
propose a novel framework, called CTRL (Clustering TRaining Losses for label
error detection), to detect label errors in multi-class datasets. It detects
label errors in two steps based on the observation that models learn clean and
noisy labels in different ways. First, we train a neural network using the
noisy training dataset and obtain the loss curve for each sample. Then, we
apply clustering algorithms to the training losses to group samples into two
categories: cleanly-labeled and noisily-labeled. After label error detection,
we remove samples with noisy labels and retrain the model. Our experimental
results demonstrate state-of-the-art error detection accuracy on both image
(CIFAR-10 and CIFAR-100) and tabular datasets under simulated noise. We also
use a theoretical analysis to provide insights into why CTRL performs so well.","Neural networks (NNs) have demonstrated success in nu merous classiÔ¨Åcation applications. Large and highquality datasets are essential for the success of NN training. In gen eral, it takes a lot of effort to label a large dataset manu ally. This process is also errorprone. Sometimes, it is not even feasible. Even wellknown humanannotated datasets have been found to have signiÔ¨Åcant labeling errors, e.g., Im ageNet (Deng et al. 2009) is known to have close to 6% labeling error in its validation set (Northcutt, Athalye, and Mueller 2021). Labels that are different from their true class are said to be noisy, else clean. In practice, each data in stance belongs to one or multiple hidden true classes and we are only provided with observed labels that may poten tially be erroneous. In fact, many popular datasets are not very clean (Northcutt, Athalye, and Mueller 2021). It is also known that deep NNs can easily Ô¨Åt random labels. Models overÔ¨Åtted on bad training data have poor predictive power on clean test sets (Zhang et al. 2021; Arpit et al. 2017). Deep learning models also display memorization ef fects. They Ô¨Årst memorize samples with clean labels (also 1CTRL is opensource: https://github.com/changyue/ctrlcalled ‚Äúearly learning‚Äù) and then start adapting to samples with noisy labels after sufÔ¨Åcient epochs of training. Large capacity models can eventually memorize all samples. This phenomenon is independent of the optimizations used dur ing training or the NN architectures employed (Zhang et al. 2021). Before overÔ¨Åtting, when all samples have close to zero losses, clean and noisy labels result in different loss curves due to the difference in how learning progresses for each type. This is exploited in many label error detection methods. MentorNet (Jiang et al. 2018) monitors the learn ing process and provides a curriculum to reweight sam ples. O2UNet (Huang et al. 2019) Ô¨Ånds label errors by sorting the average training loss of all samples. Wang et al. (Wang et al. 2022) detect label errors by classifying the loss curves. Arazo et al. (Arazo et al. 2019) model loss dis tribution in every epoch to infer a sample‚Äôs probability of being wrongly annotated. Xia et al. (Xia et al. 2020) prevent NNs from overÔ¨Åtting bad samples through early stopping. Early learning is theoretically analyzed in (Liu et al. 2020; Li, Soltanolkotabi, and Oymak 2020). Some methods demonstrate success against noisy labels by training NNs with selected samples. ConÔ¨Ådent learning (CL) (Northcutt, Jiang, and Chuang 2021) and O2UNet (Huang et al. 2019) train models over two rounds: Ô¨Årst, they train models on the noisy dataset to detect errors, then detect and remove samples with wrong labels, and Ô¨Ånally retrain the model on the clean data. Coteaching (Han et al. 2018) and Coteaching+ (Yu et al. 2019) perform dynamic sample selection during training. In this article, we present an effective framework, called CTRL, to detect noisy labels. It relies on the observation that training progresses differently for clean and noisy la bels. CTRL uses the Kmeans algorithm to classify labels as clean or noisy by clustering their training loss curves. The main contributions of this work are as follows. ‚Ä¢ We introduce a label error detection method that Ô¨Ånds noisy labels of samples by clustering their training loss trajectories. ‚Ä¢ After label error detection, we present label cleaning methods to enable retraining of the model on the cleaned dataset. ‚Ä¢ We verify the proposed method on popular benchmark datasets under simulated noise; our method achievesarXiv:2208.08464v1  [cs.LG]  17 Aug 2022stateoftheart label error detection accuracy and com parable model classiÔ¨Åcation accuracy to prior stateof theart. ‚Ä¢ To enable better understanding of the mechanism behind the method, we theoretically analyze a binary classiÔ¨Åca tion problem to demonstrate that the presence of a train ing loss gap between clean and noisy labels is highly probable. The article is organized as follows. We present related work in Section 2. We describe the methodology in Section 3. We present experimental results in Section 4. We conclude in Section 5, mentioning future directions. 2 Related Work "
37,Semi-supervised Skin Lesion Segmentation via Transformation Consistent Self-ensembling Model.txt,"Automatic skin lesion segmentation on dermoscopic images is an essential
component in computer-aided diagnosis of melanoma. Recently, many fully
supervised deep learning based methods have been proposed for automatic skin
lesion segmentation. However, these approaches require massive pixel-wise
annotation from experienced dermatologists, which is very costly and
time-consuming. In this paper, we present a novel semi-supervised method for
skin lesion segmentation by leveraging both labeled and unlabeled data. The
network is optimized by the weighted combination of a common supervised loss
for labeled inputs only and a regularization loss for both labeled and
unlabeled data. In this paper, we present a novel semi-supervised method for
skin lesion segmentation, where the network is optimized by the weighted
combination of a common supervised loss for labeled inputs only and a
regularization loss for both labeled and unlabeled data. Our method encourages
a consistent prediction for unlabeled images using the outputs of the
network-in-training under different regularizations, so that it can utilize the
unlabeled data. To utilize the unlabeled data, our method encourages the
consistent predictions of the network-in-training for the same input under
different regularizations. Aiming for the semi-supervised segmentation problem,
we enhance the effect of regularization for pixel-level predictions by
introducing a transformation, including rotation and flipping, consistent
scheme in our self-ensembling model. With only 300 labeled training samples,
our method sets a new record on the benchmark of the International Skin Imaging
Collaboration (ISIC) 2017 skin lesion segmentation challenge. Such a result
clearly surpasses fully-supervised state-of-the-arts that are trained with 2000
labeled data.","Skin cancer is currently one of the fastest growing cancers worldwide, and melanoma is the most deadly form of skin cancer, leading to an estimated 9,730 deaths in the United States in 2017 [24]. To improve the diagnostic performance of melanoma, dermoscopy has been proposed as a noninvasive imaging technique to enhance the visual effect of pigmented skin c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:1808.03887v1  [cs.CV]  12 Aug 20182 LI ET AL.: SEMISUPERVISED SKIN LESION SEGMENTATION Figure 1: Skin lesion cases with artifacts (the left two); examples of ambiguous (the middle two) and clearcut (the right two) labels. lesions. However, recognizing malignant melanoma by visual interpretation alone is time consuming and errorprone to inter and intraobserver variabilities. To assist dermatologists in the diagnosis, an automatic melanoma segmentation method is highly demanded in the clinical practice. Automatic melanoma segmentation is a very challenging task due to large variations in lesion size, location, shape and color over different patients and the presence of artifacts such as hairs and veins; see Figure 1. Traditional segmentation methods are mainly based on clustering, intensity thresholding, region growing, and deformable models. These methods, however, rely on handcrafted features, and have limited feature representation capability. Recently, convolutional neural networks (CNNs) have been widely used and achieved re markable success in a variety of vision recognition tasks. Many researchers advanced the skin lesion segmentation and showed decent results [2, 6, 17, 28]. For example, Yuan et al.[28] proposed a deep convolutional neural network (DCNN), trained it with multiple color spaces, and achieved the best performance in the ISIC 2017 skin lesion segmentation challenge. All the above methods, however, are based on fully supervised learning, which requires a large amount of annotated images to train the network for accuracy and robustness. Such pixellevel annotation is laborious and difÔ¨Åcult to obtain, especially for melanoma in the der moscopic images, which rely heavily on experienced dermatologists. Moreover, the limited amount of labeled data with pixelwise annotations also restricts the performance of deep networks. Lastly, there exists some cases that display ambiguous melanocytic or borderline features of melanoma. These cases are inherently difÔ¨Åcult to have an accurate annotation from the dermoscopic diagnosis [22]; see again Figure 1. Previous supervised learning based methods do not have speciÔ¨Åc schemes to deal with these ambiguous annotations, which may degrade the performance on those dermoscopic images with clearcut lesions. To alleviate the above issues, we address the skin lesion segmentation problem via semisupervised learn ing, which leverages both a limited amount of labeled and an arbitrary amount of unlabeled data. As a byproduct, our semisupervised method is robust and has a potential to be tol erant to ambiguous labels; see experiments in Section 4.2. There are some semisupervised approaches for dermoscopy images and other medical image processing [1, 10, 14, 18]. However, they either suffer from limited representation capacity of handcrafted features or may easily get into local minimum. In this paper, we present a novel semisupervised learning method for skin lesion seg mentation. The whole framework is trained with a weighted combination of the supervised loss and the unsupervised loss. To utilize the unlabeled data, our selfensembling method encourages the consistent prediction of the network for the same input data under differ ent regularizations ( e.g., randomized Gaussian noise, network dropout and randomized data transformation). In particular, we design our method to account for the challenging semi supervised segmentation task, in which pixellevel classiÔ¨Åcation is required to be predicted.LI ET AL.: SEMISUPERVISED SKIN LESION SEGMENTATION 3 We observe that in the segmentation problem, if one transforms ( e.g., rotate) the input image, the expected prediction should be transformed in the same manner. Actually, when the inputs of CNNs are rotated, the corresponding network predictions would not rotated in the same way [25]. In this regard, we take advantages of this property by introducing a transforma tion ( i.e., rotation, Ô¨Çipping) consistent scheme at the input and output space of our network. SpeciÔ¨Åcally, we design the unsupervised/regularization loss by minimizing the differences between the network predictions under different transformations of the same input. In summary, our work has the following achievements: We present a novel semisupervised learning method for the practical biomedical im age segmentation problem by taking advantage of a large amount of unlabeled data, which largely reduces annotation efforts for the dermatologists. To better utilize the unlabeled data for segmentation tasks, we propose a transforma tion consistent scheme in selfensembling model and demonstrate the effectiveness for semisupervised learning. We establish a new record with only 300 labeled data on the benchmark of ISIC 2017 skin lesion segmentation challenge, which excels the stateofthearts that are based on fully supervised learning with 2000 labeled data. 2 Related Work "
38,RankNEAT: Outperforming Stochastic Gradient Search in Preference Learning Tasks.txt,"Stochastic gradient descent (SGD) is a premium optimization method for
training neural networks, especially for learning objectively defined labels
such as image objects and events. When a neural network is instead faced with
subjectively defined labels--such as human demonstrations or annotations--SGD
may struggle to explore the deceptive and noisy loss landscapes caused by the
inherent bias and subjectivity of humans. While neural networks are often
trained via preference learning algorithms in an effort to eliminate such data
noise, the de facto training methods rely on gradient descent. Motivated by the
lack of empirical studies on the impact of evolutionary search to the training
of preference learners, we introduce the RankNEAT algorithm which learns to
rank through neuroevolution of augmenting topologies. We test the hypothesis
that RankNEAT outperforms traditional gradient-based preference learning within
the affective computing domain, in particular predicting annotated player
arousal from the game footage of three dissimilar games. RankNEAT yields
superior performances compared to the gradient-based preference learner
(RankNet) in the majority of experiments since its architecture optimization
capacity acts as an efficient feature selection mechanism, thereby, eliminating
overfitting. Results suggest that RankNEAT is a viable and highly efficient
evolutionary alternative to preference learning.","Forms of gradient descent are the natural choice of optimization method for training deep neural networks to predict objectively defined labels in tasks such as image and speech recognition, fraud detection, and event prediction. Over the last few years, we have witnessed a rapidly growing interest in the use of neural networks that are able to classify subjectively defined labels. This family of learningtorank or preference learning algorithms [ 9] that train neural networks‚Äîsuch as RankNet [ 2], DeepRank [ 29] and Lamb daMART [ 3]‚Äîyield good performance by relying primarily on gra dient descent methods. Subjectively defined labels, however, includ ing human demonstrations (e.g. creative tasks, navigation traces and paths) or human annotations (e.g. of emotion or aesthetics) yield highly complex, deceptive and noisy loss landscapes for a neural network to learn. Assuming that the plasticity of neuroevo lutionary processes would be beneficial for such loss landscapes, in this paper we test the hypothesis that evolutionary search would be a better optimizer for neural network training in preference learning (PL) tasks compared to stochastic gradient descent (SGD). To test our hypothesis, this paper explores the efficacy of neu roevolutionary search in PL tasks by building on the efficient and popular RankNet [ 2] architecture and enhancing its search capacity through neuroevolution. In particular, we introduce a novel algo rithm named RankNEAT that relies on the Siamese neural network architecture of RankNet and learns to rank via NeuroEvolution of Augmenting Topologies (NEAT) [ 36]. Unlike traditional gradient based PL methods, RankNEAT resembles the process of plastic ity [7], which induces changes in both the coupling strength and the spatial organization of synapses in biological neural networks. RankNEAT learns to rank subjectively defined labels with high degrees of accuracy through its ability to optimize the synaptic pa rameters such as the network‚Äôs weights and the edge architecture simultaneously. We test RankNEAT (neuroevolution) and compare it against the vanilla RankNet (stochastic gradient decent) in the task of player affect modeling across three games, using the AGAINarXiv:2204.06901v1  [cs.NE]  14 Apr 2022GECCO ‚Äô22, July 9‚Äì13, 2022, Boston, MA, USA Pinitas et al. [23] dataset of arousalannotated gameplay videos. Player modeling [46] is an important subfield in game research since it promotes the development of reliable human computer interaction systems and consequently improves the users‚Äô experience. Our current approach feeds images of gameplay to a pretrained vision transformer, while the last fullyconnected layer of the network is then trained to pre dict ordinal values of arousal, using RankNet or RankNEAT. Results indicate that RankNEAT is superior to SGD (RankNet) in training PL models of arousal in the majority of experiments performed. Our key findings suggest that RankNEAT is a viable PL paradigm which achieves comparable or significantly higher performances to RankNet. In this first experiment, RankNEAT optimizes the edge topology of the networks‚Äô last layer, resembling an evolutionary feature selection strategy that eliminates unnecessary features from the observed input space. Additional studies should explore how RankNEAT performs in other subjectively defined tasks and hyper parameter setups, such as increasing the topological complexity. This paper is novel in many ways. First, to the best of our knowl edge, this is the first time a NEATbased preference learner is in troduced, combining a traditional learningtorank neural network architecture with neuroevolution. Second, RankNEAT is tested broadly across three dissimilar games from the same genre show casing the robustness of the method for affect modeling. Third, the proposed approach is compared thoroughly against SGD (RankNet) across different games and hyperparameters. Finally, RankNEAT is combined with vision transformers (pretrained on ImageNet) enabling us to offer generalpurpose representations for solving tasks with subjectively defined labels. 2 RELATED WORK "
39,Towards Quantifying Intrinsic Generalization of Deep ReLU Networks.txt,"Understanding the underlying mechanisms that enable the empirical successes
of deep neural networks is essential for further improving their performance
and explaining such networks. Towards this goal, a specific question is how to
explain the ""surprising"" behavior of the same over-parametrized deep neural
networks that can generalize well on real datasets and at the same time
""memorize"" training samples when the labels are randomized. In this paper, we
demonstrate that deep ReLU networks generalize from training samples to new
points via piece-wise linear interpolation. We provide a quantified analysis on
the generalization ability of a deep ReLU network: Given a fixed point
$\mathbf{x}$ and a fixed direction in the input space $\mathcal{S}$, there is
always a segment such that any point on the segment will be classified the same
as the fixed point $\mathbf{x}$. We call this segment the $generalization \
interval$. We show that the generalization intervals of a ReLU network behave
similarly along pairwise directions between samples of the same label in both
real and random cases on the MNIST and CIFAR-10 datasets. This result suggests
that the same interpolation mechanism is used in both cases. Additionally, for
datasets using real labels, such networks provide a good approximation of the
underlying manifold in the data, where the changes are much smaller along
tangent directions than along normal directions. On the other hand, however,
for datasets with random labels, generalization intervals along mid-lines of
triangles with the same label are much smaller than those on the datasets with
real labels, suggesting different behaviors along other directions. Our
systematic experiments demonstrate for the first time that such deep neural
networks generalize through the same interpolation and explain the differences
between their performance on datasets with real and random labels.","In recent years, deep neural networks have improved the state of the art performance substantially in computer vision [He et al., 2016a; Krizhevsky et al. , 2012; Salman and Liu, 2019], machine translation [Sutskever et al. , 2014], speech recog nition [Graves et al. , 2013], healthcare [Miotto et al. , 2017; Salman et al. , 2019] and game playing [Silver et al. , 2017] among other applications. However, the underlying mecha nisms that enable them to perform well are still not well un derstood. Even though they typically have more parameters than the training samples and exhibit very large capacities, they generalize well on real datasets trained via stochastic gradient descent or its variants. In an insightful paper, Zhang et al. [2016] have identiÔ¨Åed a number of intriguing phenom ena of such networks. In particular, they demonstrate that overparametrized neural networks can achieve 100% accu racy trained on datasets with the original labels and general ize well. At the same time, the exact same neural network architectures can also achieve 100% accuracy on the datasets with random labels, and therefore ‚Äúmemorize‚Äù the training samples. Clearly, this is not consistent with statistical learn ing theory [Vapnik, 1998] and biasvariance tradeoff [Geman et al. , 1992], where models should match the (unknown) ca pacity of the underlying processes in order to generalize well. Understanding and explaining this typical behavior of deep neural networks has attracted a lot of attention recently with the hope of revealing the underlying mechanisms of how deep neural networks generalize. Fundamentally, while training, deep neural networks iter atively minimize a loss function deÔ¨Åned as the sum of the loss on the training samples. The parameters in the trained network depend on the initial parameter values, the optimiza tion process, and training data. As 100% accuracy on the training samples can be achieved even with random labels, Ô¨Ånding good solutions for such networks that minimize the loss is therefore not a key issue. While regularization tech niques can affect the parameters of trained networks, Zhang et al. [2016] have demonstrated that their effects are typi cally small, suggesting that they are not a key component. Therefore, the generalization performance of a trained over parametrized network should depend on the training data and network architecture. In this paper, we focus on deep ReLU networks. We show that such networks generalize consis tently and reliably by interpolating among the training points.arXiv:1910.08581v1  [cs.LG]  18 Oct 2019Using generalization intervals deÔ¨Åned as the range of the data that have the same classiÔ¨Åcation along a direction, we dis cover that pairwise generalization intervals on datasets with real and random labels are almost identical for high dimen sional inputs (e.g., MNIST and CIFAR10 samples). Further more, we show that pairwise interpolations approximate the underlying manifold in the data well, enabling the networks to generalize well. We show that the properties are remark ably consistent among networks with different architectures and on different datasets. The properties enable us to char acterize the generalization performance of neural networks based on their behaviors on the training sets only, which we call intrinsic generalization. This notion of generalization is very different from the typical deÔ¨Ånition of the performance gap on the training set and test set. While intrinsic generaliza tion of a network on a training set can be quantiÔ¨Åed through generalization intervals, the gapbased generalization perfor mance can not be studied without having a validation set or test set. Furthermore, the gapbased deÔ¨Ånition is extrinsic as it can vary when a different validation set is used. In other words, for the Ô¨Årst time, we demonstrate the underlying mechanisms that enable overparametrized networks to gen eralize well when all the training samples are classiÔ¨Åed cor rectly. The systematic results demonstrate the effectiveness of the proposed method and therefore validate the proposed solution. The rest of the paper is organized as follows. In the next section, we review recent works that are closely related to our study. After that, we present the theoretical foundation of the generalization mechanism via interpolation for deep ReLU networks. We introduce a novel notation called generalization interval (GI) to quantify the generalization of such networks. Then, we illustrate our proposed ideas on intrinsic generaliza tion behavior of deep ReLU networks with systematic experi ments on representative datasets such as MNIST and CIFAR 10 along with a twodimensional synthetic dataset. Finally, we discuss correlations between generalization intervals on training sets and validation accuracy and whether there ex ists a mechanism in deep neural networks that supports deep memorization. We conclude the paper with a brief summary and plan for future work. 2 Related Work "
40,Early-Learning Regularization Prevents Memorization of Noisy Labels.txt,"We propose a novel framework to perform classification via deep learning in
the presence of noisy annotations. When trained on noisy labels, deep neural
networks have been observed to first fit the training data with clean labels
during an ""early learning"" phase, before eventually memorizing the examples
with false labels. We prove that early learning and memorization are
fundamental phenomena in high-dimensional classification tasks, even in simple
linear models, and give a theoretical explanation in this setting. Motivated by
these findings, we develop a new technique for noisy classification tasks,
which exploits the progress of the early learning phase. In contrast with
existing approaches, which use the model output during early learning to detect
the examples with clean labels, and either ignore or attempt to correct the
false labels, we take a different route and instead capitalize on early
learning via regularization. There are two key elements to our approach. First,
we leverage semi-supervised learning techniques to produce target probabilities
based on the model outputs. Second, we design a regularization term that steers
the model towards these targets, implicitly preventing memorization of the
false labels. The resulting framework is shown to provide robustness to noisy
annotations on several standard benchmarks and real-world datasets, where it
achieves results comparable to the state of the art.","Deep neural networks have become an essential tool for classiÔ¨Åcation tasks [ 19,15,11]. These models tend to be trained on large curated datasets such as CIFAR10 [ 18] or ImageNet [ 9], where the vast majority of labels have been manually veriÔ¨Åed. Unfortunately, in many applications such datasets are not available, due to the cost or difÔ¨Åculty of manual labeling (e.g. [ 13,32,25,1]). However, datasets with lower quality annotations, obtained for instance from online queries [ 5] or crowdsourcing [ 49,53], may be available. Such annotations inevitably contain numerous mistakes or label noise . It is therefore of great importance to develop methodology that is robust to the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to Ô¨Årst Ô¨Åt the training data with clean labels during an early learning phase, before eventually memorizing the examples with 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.arXiv:2007.00151v2  [cs.LG]  22 Oct 2020Clean labels Wrong labels Cross Entropy Earlylearning Regularization Figure 1: Results of training a ResNet34 [ 15] neural network with a traditional cross entropy loss (top row) and our proposed method (bottom row) to perform classiÔ¨Åcation on the CIFAR10 dataset where 40% of the labels are Ô¨Çipped at random. The left column shows the fraction of examples with clean labels that are predicted correctly (green) and incorrectly (blue). The right column shows the fraction of examples with wrong labels that are predicted correctly (green), memorized (the prediction equals the wrong label, shown in red), and incorrectly predicted as neither the true nor the labeled class (blue). The model trained with cross entropy begins by learning to predict the true labels, even for many of the examples with wrong label, but eventually memorizes the wrong labels. Our proposed method based on earlylearning regularization prevents memorization, allowing the model to continue learning on the examples with clean labels to attain high accuracy on examples with both clean and wrong labels. false labels [ 3,54]. In this work we study this phenomenon and introduce a novel framework that exploits it to achieve robustness to noisy labels. Our main contributions are the following: ‚Ä¢In Section 3 we establish that early learning and memorization are fundamental phenomena in high dimensions, proving that they occur even for simple linear generative models. ‚Ä¢In Section 4 we propose a technique that utilizes the earlylearning phenomenon to counteract the inÔ¨Çuence of the noisy labels on the gradient of the cross entropy loss. This is achieved through a regularization term that incorporates target probabilities estimated from the model outputs using several semisupervised learning techniques. ‚Ä¢In Section 6 we show that the proposed methodology achieves results comparable to the state of the art on several standard benchmarks and realworld datasets. We also perform a systematic ablation study to evaluate the different alternatives to compute the target probabilities, and the effect of incorporating mixup data augmentation [55]. 2 Related Work "
41,QFCNN: Quantum Fourier Convolutional Neural Network.txt,"The neural network and quantum computing are both significant and appealing
fields, with their interactive disciplines promising for large-scale computing
tasks that are untackled by conventional computers. However, both developments
are restricted by the scope of the hardware development. Nevertheless, many
neural network algorithms had been proposed before GPUs become powerful enough
for running very deep models. Similarly, quantum algorithms can also be
proposed as knowledge reserves before real quantum computers are easily
accessible. Specifically, taking advantage of both the neural networks and
quantum computation and designing quantum deep neural networks (QDNNs) for
acceleration on Noisy Intermediate-Scale Quantum (NISQ) processors is also an
important research problem. As one of the most widely used neural network
architectures, convolutional neural network (CNN) remains to be accelerated by
quantum mechanisms, with only a few attempts have been demonstrated. In this
paper, we propose a new hybrid quantum-classical circuit, namely Quantum
Fourier Convolutional Network (QFCN). Our model achieves exponential speed-up
compared with classical CNN theoretically and improves over the existing best
result of quantum CNN. We demonstrate the potential of this architecture by
applying it to different deep learning tasks, including traffic prediction and
image classification.","Existing works [ 1,2] have attempted to incorporate the classical machine learning algorithms and quantum computation techniques, which yields quantum machine learning frameworks. It is also promising for deep convolutional neural network (CNN) to be leveraged with quantum methods towards achieving quantum convolutional models, thanks to the following reasons. First, CNNs manipulate data in the manner of performing matrix operations in high dimensional vector space, while quantum computation also conducts matrix operations in high dimensional vector space [ 1]. Second, most of the deep CNN models require large datasets for parameter optimization, and often need heavy computation for both the training and testing. Meanwhile, based on special quantum information properties named entanglement and superposition, quantum computers can make exponential number of calculations in parallel. This mechanism, known as quantum speedup, can accelerate some learning algorithms to surpass their classical counterparts based on mathematical proofs [3, 4, 5, 6]. Two decades ago, scientists began to implement quantum algorithms on real quantum machines. With the continuous development of quantum hardware, scientists suggest that human beings will live in Noisy IntermediateScale Quantum (NISQ) era in the future, and everyone is expected to have a quantum computer with 50100 qubits, which could be used to perform some special tasks that surpass the capabilities of today‚Äôs classical computers [7, 8]. Recently, deep neural networks become quite popular and have been widely applied in computer vision, natural language processing, and many other areas, thanks to the advancement of deep learning techniques [ 9,10,11]. As one of the most important deep learning techniques, the convolution module has been used in many network architectures, and many stateofart models for handling different types of tasks contain convolutional layers [12, 13, 14, 15].arXiv:2106.10421v1  [quantph]  19 Jun 2021APREPRINT  JUNE 22, 2021 Given the aforementioned background of quantum computation and Ô¨Çourish CNN Ô¨Åelds, it becomes natural that we get interested in integrating the potentials and advantages of these two kinds of techniques, i.e., quantum computing and CNNs, to handle speciÔ¨Åc convolutionbased learning tasks that need continuously increasing computation capabilities. However, the direct convolution of quantum state has been shown to be inaccessible by previous studies [ 16]. In this paper, we propose a hybrid circuit with quantum Fourier transform to achieve deep convolutional models with quantum. The major blocks in CNNs are the convolutional operations, i.e., the product of the kernel and the input data that can have various dimensions. This process often accounts for a large proportion of the computation cost and time consumption in the whole network. Another approach that can be used to efÔ¨Åciently implement the convolutional operation is transforming the kernel and data to Fourier basis and then performing inverse transform based on their product. Some of the classical networks like graph convolutional networks (GCNs) have used the approximation of this method to implement convolution [ 17,18,19]. Inspired by this, to achieve quantum convolution, here we propose a novel quantum Fourier convolutional network (QFCN) to speedup the CNN with quantum Fourier transform (QFT), which could be exponentially faster than the classical Fourier transform for convolutions. We circumvent the inaccessibility issue [ 16] by adopting part quantum circuit of windowed Fourier transform [ 20]. We also propose a hybrid quantumclassical circuit to avoid some of the quantum noise and also to take advantage of classical computers that are able to perform rapid iterations for model optimization. We demonstrate the potential of our network on the trafÔ¨Åc prediction task with spatiotemporal graph convolutional models and the image classiÔ¨Åcation task with 2D convolutional networks. Motivations and contributions. We all know the importance of convolution to the deep learning Ô¨Åeld. If we want to speed up the training of nowadays‚Äô networks which mostly build upon a bunch of convolutional layers, the quantum convolutional layer and its optimization will be inevitable problems. There are only two architectures of convolutional layer being proposed[ 21,22] . However, due to the horriÔ¨Åc time assumption of quantum stimulation, the results of these two papers are based on toy experiments and their network only contain two quantum convolutional layers. Unfortunately, with the increase of the layer number, the gradient of these two structures may lead to Barren plateaus [ 23] in training. So, it is essential to proposed another quantum convolution method rather than simply transferring conventional multiply accumulation (MAC) to quantum mechanism like previous works. The contributions of this paper are summarized below. To the best of our knowledge, we are the Ô¨Årst to propose a hybrid quantumclassical circuit to speed up CNN with a parametric quantum circuit (PQC), and exploit quantum Fourier transform for the trainable convolution models. SpeciÔ¨Åcally, our PQC can be inserted into different deep networks to replace the convolutional layers. Besides, we also introduce the quantum optimization algorithm (backpropagation) for our designed network. 2 Related Work "
42,CorrDetector: A Framework for Structural Corrosion Detection from Drone Images using Ensemble Deep Learning.txt,"In this paper, we propose a new technique that applies automated image
analysis in the area of structural corrosion monitoring and demonstrate
improved efficacy compared to existing approaches. Structural corrosion
monitoring is the initial step of the risk-based maintenance philosophy and
depends on an engineer's assessment regarding the risk of building failure
balanced against the fiscal cost of maintenance. This introduces the
opportunity for human error which is further complicated when restricted to
assessment using drone captured images for those areas not reachable by humans
due to many background noises. The importance of this problem has promoted an
active research community aiming to support the engineer through the use of
artificial intelligence (AI) image analysis for corrosion detection. In this
paper, we advance this area of research with the development of a framework,
CorrDetector. CorrDetector uses a novel ensemble deep learning approach
underpinned by convolutional neural networks (CNNs) for structural
identification and corrosion feature extraction. We provide an empirical
evaluation using real-world images of a complicated structure (e.g.
telecommunication tower) captured by drones, a typical scenario for engineers.
Our study demonstrates that the ensemble approach of \model significantly
outperforms the state-of-the-art in terms of classification accuracy.","Inspecting faults (e.g. corrosion) is a major problem in industrial structures such as building roofs, pipes, poles, bridges, and telecommunication towers [1]. This is a vital service for several industrial sectors, especially manufacturing, where structures (assets) that are subject to corrosion due to their exposure to the weather are used to deliver critical products or services. The problem of corrosion may cost Australia up to $32 billion annually, which is greater than $1500 for every Australian each year [2]. Corrosion is not simply a nancial cost if left unattended; the endangerment of lives may also be a real risk. Without adopting to the latest in AIdriven solutions, businesses are losing millions in time and money to identify corrosion using methods that have changed little with heavy reliance on human judgement [3]. The timely and accurate detection of corrosion is a key way to improve the eciency of economy by instigating appropriately managed maintenance processes that will also safe lives. A fast and reliable inspection process for corrosion can ensure industrial assets are maintained in time to prevent regulatory breaches, outages or catas trophic disasters. In most cases, inspections of such assets are conducted man ually which can be slow, hazardous, expensive and inaccurate. Recently, drones have proven to be a viable and safer solution to perform such inspections in many adverse conditions by  ying upclose to the structures and take a very large number of highresolution images from multiple angles [4]. The images acquired through such process are stored and then subsequently reviewed man ually by expert engineers who decide about further actions. However, this causes a problem of plenty for highly qualied engineers to manually identify corrosion from the images which further leads to a high level of human error, inconsisten cies, high lead time and high costs in terms of manhours. Existing approaches for identifying structural corrosion from images are ei ther based on Computer Vision (CV) [5] or Deep Learning (DL) techniques [6, 7]. In recent CVbased techniques [8], nontrivial prior knowledge and ex tensive human eorts are required in designing high quality corrosion features from images. In addition, one cannot hope much on the performance (or the accuracy of corrosion detection) in the case that the corrosion features are some what incorrectly identied. Compared with computer vision/image processing [9] and vanilla machine learning approaches [10], DLbased methods, in partic ular Convolutional Neural Networks (CNNs) [11, 12] have shown the ability to automatically learn important features, outperforming stateoftheart vision based approaches [6, 7, 13] and achieving humanlevel accuracy. In this paper, we present a Deep Learning (DL)based framework named CorrDetector , for detecting corrosion from high resolution images captured by 2drones. As the key innovation, we propose and develop an ensemble of CNN models [14] which is capable of detecting corrosion in target structure (i.e. ob ject) from such high resolution images at signicantly higher accuracy than the current stateoftheart CNN models. More specically, the proposed framework is capable of providing i) industrial structure recognition  detect the industrial structure (i.e. object of interest) in the image captured by the drone (since the drone image is captured in a realworld environment that is lled with back ground noise) and; iii) localised detection of corrosion  detect which areas in the industrial structure contains corrosion. Most DLbased solutions for corro sion detection use image samples captured by DSLR (digital singlelens re ex), digital or mobile cameras with human involvement in taking pictures [7, 15] in more controlled environment. Such image samples are much lower in resolution than drone images. Moreover, these samples can be biased as they are captured specically to be utilised for experimental purposes at certain distances and an gles. Therefore, such images comprised of human judgements to focus in specic type of corrosion area within the image which can be easily isolated and distin guishable even in visual inspection [7]. Moreover, previous studies have mostly focused on corrosion identication only in metallic surfaces [4, 6, 7, 10]. To the best of our knowledge, this work is the rst attempt that utilises realworld highresolution unaltered images captured by drones in industrial and realworld settings to identify corrosion in industrial structure such as telecommunication tower. More specically, this paper makes the following contributions: ‚Ä¢Present a novel framework, CorrDetector with a 4layer architecture to detect industrial object and identify regions of corrosion in highresolution images of industrial assets captured by drones in a realworld setting from various positions, angles and distances. ‚Ä¢Present an innovative ensemble approach that combines two deep learning models; a deep learning model for recognising and separating targeted industrial structure from the background and a deep learning model to identify corrosion in specic regions of the industrial structure (localised). ‚Ä¢Present a systematic methodology for training our ensemble model us ing highresolution drone images that includes two types of annotation techniques namely gridbased and objectbased. ‚Ä¢A comprehensive evaluation using a realworld dataset (high resolution drone images of telecommunication towers) and comparison with current stateoftheart deep learning models for corrosion detection to demon strate the ecacy of the proposed CorrDetector . The rest of the paper is organised as follows. Section 2 provides a discussion of current stateoftheart in corrosion detection from images. Section 3 presents the systematic methodology for developing an ensemble of CNN models for corrosion detection. Section 4 presents the experimental domain, the empirical evaluation and a comprehensive analysis of our proposed approach against the currentstateoftheart and nally Section 5 concludes the paper. 32. Related Works "
43,Towards Understanding Deep Learning from Noisy Labels with Small-Loss Criterion.txt,"Deep neural networks need large amounts of labeled data to achieve good
performance. In real-world applications, labels are usually collected from
non-experts such as crowdsourcing to save cost and thus are noisy. In the past
few years, deep learning methods for dealing with noisy labels have been
developed, many of which are based on the small-loss criterion. However, there
are few theoretical analyses to explain why these methods could learn well from
noisy labels. In this paper, we theoretically explain why the widely-used
small-loss criterion works. Based on the explanation, we reformalize the
vanilla small-loss criterion to better tackle noisy labels. The experimental
results verify our theoretical explanation and also demonstrate the
effectiveness of the reformalization.","Deep neural networks (DNNs) have achieved great success in many realworld applications, but rely on largescale data with accurate labels [Deng et al. , 2009 ]. Obtaining large scale accurate labels is expensive while the alternative meth ods such as crowdsourcing [Raykar et al. , 2010 ]and web queries [Jiang et al. , 2020 ]can easily provide extensive la beled data, but unavoidably incur noisy labels. The perfor mance of deep neural networks may be severely hurt if these noisy labels are blindly used [Zhang et al. , 2017a ], and thus how to learn with noisy labels has become a hot topic. In the past few years, many deep learning methods for tack ling noisy labels have been developed. Some methods try to exploit noiserobust loss functions, e.g., MAE loss [Ghosh et al., 2017 ], TruncatedLqloss[Zhang and Sabuncu, 2018 ]and the informationtheoretic loss [Xuet al. , 2019 ]. These meth ods do not consider the speciÔ¨Åc information about label noise, and thus usually have limited utility in realworld applica tions. Some methods use the transition matrix to model label noise and construct an unbiased loss term to alleviate the in Ô¨Çuence of noisy labels [Sukhbaatar et al. , 2014; Patrini et al. , 2017; Goldberger and BenReuven, 2017; Han et al. , 2018a; Hendrycks et al. , 2018 ]. However, the performance of these Corresponding author.methods is usually suboptimal due to the difÔ¨Åculty of ac curately estimating the noise transition matrix. Some other methods try to correct the noisy labels [Maet al. , 2018; Arazo et al. , 2019; Yi and Wu, 2019 ], but may suffer from the false correction. Sometimes, although correcting the noisy labels might be challenging especially for the classiÔ¨Åcation task with a large number of classes, the detection of noisy labels is relatively easy. Along this direction, the sample selection strategy with the widelyused smallloss criterion has been proposed, i.e., treating the examples with small loss as the clean data and using them in the training pro cess. Although many methods based on the smallloss crite rion have achieved prominent performance in practice [Han et al. , 2018b; Yu et al. , 2019; Shen and Sanghavi, 2019; Song et al. , 2019; Wei et al. , 2020 ], the theoretical expla nation about when and why it works is rarely studied. When there are noisy labels in the data, it is somehow overly optimistic to expect that deep neural networks could achieve good performance without any assumption on la bel noise. Thus, most of previous studies potentially make assumptions on label noise, e.g., the condition that correct labels are not overwhelmed by the false ones [Sukhbaatar et al. , 2014; Han et al. , 2018b ]. Some methods focus on the classconditional noise setting [Natarajan et al. , 2013; Patrini et al. , 2017 ],i.e., the label noise classconditionally depends only on the latent true class, but not on the fea ture. This assumption is an approximation of realworld label noise and can encode the similarity information be tween classes. Based on this, three representative types of label noise have been considered, i.e., uniform label noise [Hendrycks et al. , 2018 ], pairwise label noise [Han et al., 2018b ]and structured label noise [Patrini et al. , 2017; Zhang and Sabuncu, 2018 ]. For these types of label noise, it is usually assumed that the diagonallydominant condition holds, and many methods could achieve good performance with this condition [Rolnick et al. , 2017; Wei et al. , 2020 ]. Unfortunately, there are few theoretical analyses to explain why this diagonallydominant condition is necessary for good performance. In this work, we Ô¨Årst reveal the theoretical con dition under which learning methods could achieve good per formance with noisy labels, which exactly matches the con dition assumed in previous methods, and then theoretically explain when and why the smallloss criterion works. Based on the explanation, we reformalize the vanilla smallloss criarXiv:2106.09291v1  [cs.LG]  17 Jun 2021terion to better tackle noisy labels. The experimental results on synthetic and realworld datasets verify our theoretical re sults and demonstrate the effectiveness of the reformalization of smallloss criterion. 2 Related Work "
44,Ordered or Orderless: A Revisit for Video based Person Re-Identification.txt,"Is recurrent network really necessary for learning a good visual
representation for video based person re-identification (VPRe-id)? In this
paper, we first show that the common practice of employing recurrent neural
networks (RNNs) to aggregate temporal spatial features may not be optimal.
Specifically, with a diagnostic analysis, we show that the recurrent structure
may not be effective to learn temporal dependencies than what we expected and
implicitly yields an orderless representation. Based on this observation, we
then present a simple yet surprisingly powerful approach for VPRe-id, where we
treat VPRe-id as an efficient orderless ensemble of image based person
re-identification problem. More specifically, we divide videos into individual
images and re-identify person with ensemble of image based rankers. Under the
i.i.d. assumption, we provide an error bound that sheds light upon how could we
improve VPRe-id. Our work also presents a promising way to bridge the gap
between video and image based person re-identification. Comprehensive
experimental evaluations demonstrate that the proposed solution achieves
state-of-the-art performances on multiple widely used datasets (iLIDS-VID, PRID
2011, and MARS).","PERSON reidentiÔ¨Åcation (Reid) addresses the problem of reassociation persons across disjoint camera views. In this paper, we consider more practical scenarios of video based person reidentiÔ¨Åcation ( VPReid ), in which a video of a person, as seen in one camera, must be matched against a gallery of videos captured by a different non overlapping camera. VPReid is an active research topic in computer vision due to its wideranging applications in problems, including visual surveillance and forensics. Since the pioneering work [1], several visual features [2], [3], and learning methods [4]‚Äì[6] have consistently improved the matching performance, leading the research community to address more challenging scenarios in complex datasets [4], [7], [8]. However, signiÔ¨Åcant hurdles due to variations in appearance, viewpoint, illumination, and occlusion come in the way of solving the problem. Recently, deep convolutional neural networks (Con vNets) stand at the forefront of several vision tasks, includ ing image classiÔ¨Åcation [9]‚Äì[12], segmentation [13], pose es timation [14], face recognition [15], crowd counting [16], and image based person reidentiÔ¨Åcation [17], just to mention a few. Deep learning for VPReid , however, is also witnessing L. Zhang and Z. Zeng are with the Institute for Infocomm Research, the Agency for Science, Technology and Research (A*STAR), Singapore. J. T. Zhou is with the Institute of High Performance Computing, the Agency for Science, Technology and Research (A*STAR), Singapore. Z. Shi is with the University of Amsterdam, Netherlands. J. W. Bian and C. Shen is with the School of Computer Science, The University of Adelaide, Australia. M. M. Cheng and Y Liu are with the TKLNDST, College of Computer Science, Nankai University, China. The Ô¨Årst two authors are the joint Ô¨Årst author, and Joey Tianyi Zhou is the corresponding author (Email: joey.tianyi.zhou@gmail.com). Manuscript received April 19, 2005; revised August 26, 2015. t1 t2 t3 t4 t5 t6 t7 t1 t3 t2 t5 t4 t7 t6 t2 t3 t4 t5 t6 t7 t1 True ÔºüFig. 1. Motivation. First two rows: existing methods adopts RNNs to model temporal dependencies for VPReid . Last two rows: human be ings can easily performs this on image sequences with a random order. such vast popularity. Typically, the prior methods [6], [18]‚Äì [23] process videos by using RNNs to temporally aggregate spatial information extracted from ConvNets. However, unlike other sequential modelling tasks [24], it is littleknown about the beneÔ¨Åts of using the RNN for mod elling temporally extracted spatial information for VPRe id. Although theoretically fascinating, we show a typical recurrent structure may be less effective to capture temporal dependencies as they assumed. To simplify our analysis, we Ô¨Ånd that the pioneering work [18] of using RNN and its followups for VPReid leads to an orderless representation. This motivates us to reponder over the task of VPReid as illustrated in Fig. 1. According to the separable visual pathway hypothesis [25], the human visual cortex contains the ventral stream and the dorsal stream, which recognize objects and how they move, respectively. Existing methodsarXiv:1912.11236v1  [cs.CV]  24 Dec 2019IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2 [18] employing RNNs overemphasize the temporal depen dencies related to person‚Äôs motion. Unfortunately, these behavioral biometrics suffer from large intra/interclass variations, making existing RNNs difÔ¨Åcult to generalize. In this work we postulate that the appearance information from a single still image plays a more important role in re associating persons from different cameras. Considering an example in Fig. 1, human beings can easily classify whether the images in two cameras come from the same identity, even the image frames in the second camera has been manually shufÔ¨Çed to remove temporal dependencies. Declaring that orderless encoding‚Äôs beneÔ¨Åts in video analytic is noncontroversial, and indeed we are certainly not the Ô¨Årst to inject orderless encoding into video based tasks. Even in other tasks such as human action recognition where the temporal dependency between each video frame is considered to be vital, orderless representations [26]‚Äì[28] can still demonstrate their superiority over recurrent struc tures [29]. While we take inspiration from these works and the recent success of image based person reidentiÔ¨Åcation, we are the Ô¨Årst to dive deep into the effect of modelling temporal information for VPReid . More speciÔ¨Åcally, we present a simple yet surprisingly powerful approach by dividing videos into individual images and perform VPReid with an orderless ensemble of shared image based rankers. We show that, under the i.i.d. assumption, the ensemble of rankers are consistent and the error bound of VPReid decreases exponentially with the number of image frames, when each base ranker is better than a random guess. Our work opens a path towards putting more emphasis on ap pearance information for VPReid . It not only achieves state oftheart performance on multiple benchmarks (iLIDSVID, PRID 2011, and MARS) but also bridges the gap between video and image based person reidentiÔ¨Åcation. 2 R ELATED WORK "
45,UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning.txt,"Supervised deep learning methods require a large repository of annotated
data; hence, label noise is inevitable. Training with such noisy data
negatively impacts the generalization performance of deep neural networks. To
combat label noise, recent state-of-the-art methods employ some sort of sample
selection mechanism to select a possibly clean subset of data. Next, an
off-the-shelf semi-supervised learning method is used for training where
rejected samples are treated as unlabeled data. Our comprehensive analysis
shows that current selection methods disproportionately select samples from
easy (fast learnable) classes while rejecting those from relatively harder
ones. This creates class imbalance in the selected clean set and in turn,
deteriorates performance under high label noise. In this work, we propose
UNICON, a simple yet effective sample selection method which is robust to high
label noise. To address the disproportionate selection of easy and hard
samples, we introduce a Jensen-Shannon divergence based uniform selection
mechanism which does not require any probabilistic modeling and hyperparameter
tuning. We complement our selection method with contrastive learning to further
combat the memorization of noisy labels. Extensive experimentation on multiple
benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%
improvement over the current state-of-the-art on CIFAR100 dataset with a 90%
noise rate. Our code is publicly available","Deep neural networks (DNNs) have proven to be highly effective in solving various computer vision tasks [9,18,22, 36, 43, 49, 50, 56, 65]. Most stateoftheart (SOTA) meth ods require supervised training with a large pool of anno tated data [4, 8, 27, 28, 60]. Collecting and manually an 1https : / / github . com / nazmul  karim170 / UNICON  NoisyLabel SSL Training (Network 1)Semi Supervised LossContrastive LossFeature ExtractorCls. LayerTraining SetClean SetUniform Selection Noisy SetTraining SetClean SetUniform Selection Noisy SetClean LabelNoisy LabelProj. HeadSSL Training (Network 2)Semi Supervised LossContrastive LossFeature ExtractorCls. LayerProj. HeadFigure 1. U NICONtraining overview: At each iteration, we em ploy a uniform selection technique to partition the training set into clean and noisy sets. Upon separation, we perform SSLtraining with an additional contrastive loss function. The uniform selection and subsequent SSLtraining of two networks (with same architec ture) is repeated until convergence. Noise Rate (%) 90% 92% 95% 98% DMix [25] 76.08 57.62 51.28 17.18 UNICON(Ours) 90.81 87.61 80.82 50.63 Table 1. Classification performance (%) of the proposed method on CIFAR10 under severe label noise. notating such data is challenging and oftentimes very ex pensive. Most largescale data collection techniques rely on opensource web data that can be automatically anno tated using search engine queries and user tags [33, 54]. This annotation scheme inevitably introduces label noise [27, 60]. Training with such noisy labels is challenging since DNNs can effectively memorize arbitrary (noisy) la bels over the course of training [2]. Combating label noise is one of the fundamental problems in deep learn ing [15, 24, 38, 47, 57, 57, 61, 63, 64, 68], and is the focus of this study. Training with noisy label data has been the subject ofmany recent studies [12, 16, 31, 42, 46, 73]. Existing tech niques can be categorized into two dominant groups: i) la bel correction, [11,40] and ii) sample separation [12,25,69]. The former approach requires the estimation of noise tran sition matrix, which is hard to estimate for high number of classes and in high noise scenarios. The latter approach tries to filter out the noisy samples from the clean ones based on the smallloss criterion [25], where the lowloss samples are assumed to have clean labels. Next, an offtheshelf semi supervised learning (SSL) technique [3, 44, 48, 53] is used for training where the selected noisy samples are treated as unlabeled data. However, the selection process is usually biased towards easy classes as clean samples from the hard classes (e.g. cats and dogs can be considered as hard classes in CIFAR10 [21]) may produce highloss values. This is more prominent at the early stage of training and can intro duce classdisparity among the selected clean samples. Se vere classimbalance may lead to poor precision of sample selection, hence, subpar classification performance. In this work, we revamp the selection process from a more fundamental perspective. Our goal is to simplify the selection process by introducing an effective and scalable JensenShannon divergence based sample separation mech anism. To address the disproportionate selection of easy and hard samples, we enforce a classbalance prior by se lecting an equal number of clean samples from each class. Such a prior improves the overall quality of pseudolabels, and hence, significantly boosts the performance of subse quent semi supervised learningbased training. In addition, we opt to employ unsupervised contrastive learning (CL) because of its inherent resistance (as labels are not required for training) to label noise memorization. We empirically show that unsupervised feature learning lowers memoriza tion risk and improves the sample separation performance; especially under severe noise levels. We call this combined technique of U NIform selection and C ONtrastive learning UNICON(shown in Fig. 1), which is found to be effective even in the presence of very high label noise (see Table 1). Our contributions are summarized as follows: ‚Ä¢ We propose a simple yet effective uniform selection mechanism that ensures classbalancing among the selected clean samples. Through empirical analy sis, we observe that classuniformity helps in gener ating higher quality pseudolabels for samples from all classes irrespective of their difficulty level. ‚Ä¢ We further minimize the risk of label noise memoriza tion by performing unsupervised feature learning using contrastive loss. This in turn boosts the sample separa tion performance. ‚Ä¢ Our extensive experimentation demonstrates that U NI CONachieves significant performance improvement over stateoftheart methods, especially on datasets with severe label noise.2. Related Work "
46,Weakly Supervised Training of Speaker Identification Models.txt,"We propose an approach for training speaker identification models in a weakly
supervised manner. We concentrate on the setting where the training data
consists of a set of audio recordings and the speaker annotation is provided
only at the recording level. The method uses speaker diarization to find unique
speakers in each recording, and i-vectors to project the speech of each speaker
to a fixed-dimensional vector. A neural network is then trained to map
i-vectors to speakers, using a special objective function that allows to
optimize the model using recording-level speaker labels. We report experiments
on two different real-world datasets. On the VoxCeleb dataset, the method
provides 94.6% accuracy on a closed set speaker identification task, surpassing
the baseline performance by a large margin. On an Estonian broadcast news
dataset, the method provides 66% time-weighted speaker identification recall at
93% precision.","Conventional speaker identiÔ¨Åcation models are usually trained on data where the speech segments corresponding to the target speakers are handannotated. However, the process of hand labelling speech data is expensive and doesn‚Äôt scale well, espe cially if a large set of speakers needs to be covered. This makes such models difÔ¨Åcult to manage and to deploy. For example, this problem is evident in the domain of media monitoring or indexing of large speech corpora where one might need to iden tify the speech segments of a wide and growing range of public Ô¨Ågures, such as politicians, scientists and celebrities. On the other hand, very large collections of audio and video documents are available online. The documents often come with metadata which may provide information about the per sons speaking in the document. For example, many items in the BBC radio archive1have a ‚ÄòContributors‚Äô section which lists the names of the speakers appearing in the programme, often together with names of other crew, such as producers and di rectors. Often, the names of the speakers appearing in the au dio/video document are given in free form text. For example, a video on YouTube that contains an interview with Elon Musk has probably the name ‚ÄòElon Musk‚Äô in the title and may further elaborate on the contents in the video description. However, the metadata rarely provides information about ‚Äòwho spoke when‚Äô, i.e., it doesn‚Äôt provide the necessary information for training speaker identiÔ¨Åcation models in a supervised way. This paper presents a method for training speaker identiÔ¨Å cation models in a weakly supervised manner, relying only on the possibly incomplete set of speakers occurring in each audio document in the training set. That is, the method does not need training data annotation at the speech segment level but rather 1http://www.bbc.co.uk/archive/at the recording level. The method relies on speaker diarization, ivectors [1] and deep neural networks. First, each recording in the training corpus is processed by a speaker diarization mod ule that partitions the recording into homogeneous segments, applies speech/nonspeech detection and clusters the result ing speech segments according to likely (anonymous) speaker turns. Second, ivectors are extracted from each segment and averaged across the speaker turns. Finally, a deep neural net work (DNN) is trained to perform speaker identiÔ¨Åcation, us ing ivectors as inputs and true speaker identities as outputs. Since we don‚Äôt know the true mapping between ivectors and speakers, we cannot use supervised learning using the cross entropy criterion to train the DNN, as it is usually done when training classiÔ¨Åcation models. Instead, we use a technique in spired by a method called expectation regularization [2]: rather than providing the true speaker label for each ivector, we pro vide a speaker distribution over allowed speaker labels for the ivectors of each recording and the training procedure is encour aged to Ô¨Ånd model parameters that predict a similar speaker dis tribution for each recording. That is, the loss function is deÔ¨Åned at the recording level, not at the speaker level. If different speak ers occur intermixed in different recordings in the training data, then the optimal solution to this objective function is a model that predicts the true label for each ivector. The method is sur prisingly robust and easy to implement: it requires deÔ¨Åning a loss function for the DNN that can be implemented in just a few lines of code in any modern deep learning framework. The evaluation of this technique is performed on two very different datasets: the V oxCeleb dataset of YouTube videos [3] and an Estonian broadcast news dataset. V oxCeleb con tains automatically retrieved videos corresponding to over 1000 different US celebrities, with around 18 videos per person. The videos do not have any metadata, other than the name of celebrity that was used for the corresponding YouTube search. The Estonian broadcast news dataset consists of over 6000 recordings of the main evening news program of Estonian na tional radio. Each recording is accompanied with metadata that lists all speakers (both news reporters and interviewees) speak ing in that programme. The two datasets are very different: while the V oxCeleb dataset is relatively heterogeneous, with unconstrained audio recording conditions, it contains roughly equal amount of speech for each speaker identity. The Estonian broadcast news database is much more homogeneous but the amount of speech from different speakers varies greatly: some news reporters appear in thousands of recordings while most speakers occur only once. We show that the proposed method works well for both datasets. The remainder of the paper is organized as follows. Section 2 presents related work. Section 3 describes the main idea of the training algorithm. Experimental results are shown in section 4. Section 5 concludes the paper.arXiv:1806.08621v1  [cs.SD]  22 Jun 20182. Related work "
47,PARS: Pseudo-Label Aware Robust Sample Selection for Learning with Noisy Labels.txt,"Acquiring accurate labels on large-scale datasets is both time consuming and
expensive. To reduce the dependency of deep learning models on learning from
clean labeled data, several recent research efforts are focused on learning
with noisy labels. These methods typically fall into three design categories to
learn a noise robust model: sample selection approaches, noise robust loss
functions, or label correction methods. In this paper, we propose PARS:
Pseudo-Label Aware Robust Sample Selection, a hybrid approach that combines the
best from all three worlds in a joint-training framework to achieve robustness
to noisy labels. Specifically, PARS exploits all training samples using both
the raw/noisy labels and estimated/refurbished pseudo-labels via self-training,
divides samples into an ambiguous and a noisy subset via loss analysis, and
designs label-dependent noise-aware loss functions for both sets of filtered
labels. Results show that PARS significantly outperforms the state of the art
on extensive studies on the noisy CIFAR-10 and CIFAR-100 datasets, particularly
on challenging high-noise and low-resource settings. In particular, PARS
achieved an absolute 12% improvement in test accuracy on the CIFAR-100 dataset
with 90% symmetric label noise, and an absolute 27% improvement in test
accuracy when only 1/5 of the noisy labels are available during training as an
additional restriction. On a real-world noisy dataset, Clothing1M, PARS
achieves competitive results to the state of the art.","Deep neural networks rely on largescale training data with human annotated labels for achieving good performance (Deng et al., 2009; Everingham et al., 2010). Collecting millions or billions of labeled training data instances is very expensive, requires signiÔ¨Åcant human time and effort, and can also compromise user privacy (Zheng et al., 2020; Bonawitz et al., 2017). Hence, there has been a paradigm shift in the interests of the research community from largescale supervised learning (Krizhevsky et al., 2017; He et al., 2016a; Huang et al., 2017) to Learning with Noisy Labels (LNL) (Natarajan et al., 2013; Goldberger & BenReuven, 2016; Patrini et al., 2017; Tanno et al., 2019) and/or unlabeled data (Berthelot et al., 2019b;a; Sohn et al., 2020). This is largely due to the abundance of raw unlabeled data with weak user tags (Plummer et al., 2015; Xiao et al., 2015) or caption descriptions (Lin et al., 2014). However, it is not trivial to build models that are robust to these noisy labels as the deep convolutional neural networks (CNNs) trained with crossentropy loss can quickly overÔ¨Åt to the noise in the dataset, harming generalization (Zhang et al., 2016). Most of the existing approaches on LNL can be divided into three main categories. First, several noise robust loss functions (Ghosh et al., 2017; Wang et al., 2019a; Zhang & Sabuncu, 2018) were proposed that are inherently tolerant to label noise. Second, sample selection methods (also referred to as loss correction in some literature) (Han et al., 2018; Yu et al., 2019; Arazo et al., 2019) are a popular technique that analyzes the persample loss distribution and separates the clean and noisy samples. The identiÔ¨Åed noisy samples are then reweighted so that they contribute less in the loss computation. A challenge in this direction is to design a reliable criterion for separation and hence prevent overÔ¨Åtting to highly conÔ¨Ådent noisy samples, a behavior known as selfconÔ¨Årmation bias . Third, label correction methods attempt to correct the noisy labels using classprototypes (Han et al., Work done during internship at Amazon, Alexa Shopping 1arXiv:2201.10836v1  [cs.CV]  26 Jan 20222019) or pseudolabeling techniques (Tanaka et al., 2018; Yi & Wu, 2019). However, in order to correct noisy labels, we typically need an extra (usually small) set of correctly labeled validation labels. In particular, these methods can fail when the noise ratio is high and estimating correct labels or highquality pseudolabels is nontrivial. More recently, the success of several stateoftheart LNL methods is attributed to leveraging Semi Supervised Learning (SSL) based approaches (Li et al., 2020; Kim et al., 2019). Typically, a sample selection technique is applied to separate clean and noisy labels in the training data, then the noisy labels are deemed unreliable and hence treated as unlabeled in a SSL setting. Following the recent SSL literature (Lee et al., 2013; Arazo et al., 2020), estimated pseudolabels are usually used to replace the Ô¨Åltered noisy labels during training. These approaches have shown to be highly tolerant to labelnoise. However, the noisy labels are always discarded in favor of pseudolabels in all the existing literature, but they may still contain useful information for training. Pseudolabeling is in turn only applied to the Ô¨Åltered noisy subset while the rest of the raw labels are typically used as is, which makes it sensitive to the quality of the Ô¨Åltering algorithm. In particular, motivated by a simple principle of making the most of the signal contained in the noisy training data, we design PARS, short for PseudoLabel Aware Robust Sample Selection. Our contributions are as follows: 1.PARS proposes a novel, principled training framework for LNL. It trains on both the original labels and pseudolabels. Unlike previous works, instead of Ô¨Åltering and then discarding the lowconÔ¨Ådent noisy labels, PARS uses the entire set of original labels, and applies selftraining with pseudolabeling and data augmentation for the entire dataset (rather than the Ô¨Åltered noisy data only). 2.PARS is able to learn useful information from all the available data samples through labeldependent noiseaware loss functions. SpeciÔ¨Åcally, in order to prevent overÔ¨Åtting to inaccurate original labels (or inaccurate pseudolabels), PARS performs a simple conÔ¨Ådence based Ô¨Åltering technique by setting a high threshold on their predicted conÔ¨Ådence, and applies robust/negative learning (or positive/negative learning) accordingly. 3.We perform extensive experiments on multiple benchmark datasets i.e.noisy CIFAR10, noisy CIFAR100 and Clothing1M. Results demonstrate that PARS outperforms previous stateoftheart methods by a signiÔ¨Åcant margin, in particular when high level of noise is present in the training data. We also conduct sufÔ¨Åcient ablation studies to validate the importance of our contributions. 4.We design a novel lowresource semisupervised LNL setting where only a small subset of data is weakly labeled (Section 4.3). We show signiÔ¨Åcant gains over stateoftheart approaches using PARS. This setting is particularly interesting when it is hard to obtain largescale noisy labeled data. In particular, we Ô¨Ånd that surprisingly none of the existing LNL methods outperform a baseline SSL model (FixMatch) (Sohn et al., 2020) that is not even designed to handle label noise, and yet PARS can achieve up to an absolute 27% improvement in test accuracy in a controlled highnoise lowresource setting. 2 R ELATED WORK "
48,Attention Disturbance and Dual-Path Constraint Network for Occluded Person Re-Identification.txt,"Occluded person re-identification (Re-ID) aims to address the potential
occlusion problem when matching occluded or holistic pedestrians from different
camera views. Many methods use the background as artificial occlusion and rely
on attention networks to exclude noisy interference. However, the significant
discrepancy between simple background occlusion and realistic occlusion can
negatively impact the generalization of the network.To address this issue, we
propose a novel transformer-based Attention Disturbance and Dual-Path
Constraint Network (ADP) to enhance the generalization of attention networks.
Firstly, to imitate real-world obstacles, we introduce an Attention Disturbance
Mask (ADM) module that generates an offensive noise, which can distract
attention like a realistic occluder, as a more complex form of
occlusion.Secondly, to fully exploit these complex occluded images, we develop
a Dual-Path Constraint Module (DPC) that can obtain preferable supervision
information from holistic images through dual-path interaction. With our
proposed method, the network can effectively circumvent a wide variety of
occlusions using the basic ViT baseline. Comprehensive experimental evaluations
conducted on person re-ID benchmarks demonstrate the superiority of ADP over
state-of-the-art methods.","Person reidentiÔ¨Åcation (ReID) refers to the process of matching pedestrian images captured by nonoverlapping cameras. This technique has gained popularity in recent years as surveillance systems have become more advanced and widespread. With the rapid development of deep learn *Corresponding Author. Significant discrepancy Functional SimilarTrain on Vanilla occlusionTest on Real occlusionTrain on ADM occlusionTest on Vanilla occlusionTest on Real occlusion (a) Baseline (b) ADP Figure 1. Visualization of attention to baseline and proposed ADP. (a) The baseline trained with the assistance of background occlu sion failed to avoid the realistic occlusion in the testing set. (b) The ADP trained by the proposed realitysimilar occlusion ADM performs well on both artiÔ¨Åcial and real occlusion. ing technology [8, 33, 5], ReID has also achieved re markable performance [20, 42, 45, 6, 2, 38] by meriting from its powerful feature extraction capabilities. How ever, most existing methods assume that the pedestrians in retrieved images are unobstructed, ignoring the possi ble occlusion problems that can occur in realworld sce narios. Consequently, these methods signiÔ¨Åcantly degrade when dealing with occluded images. While recent endeav ors have facilitated person ReID under occlusion condi tions [39, 36, 31, 18, 28, 16], two main problems associated with occlusions still need to be addressed. Firstly, the pres ence of obstacles will vanish some parts of the human body, missing and misaligned extracted features. Traditional Re ID methods cannot perform valid retrievals when some dis criminative parts are obscured. Secondly, occlusions intro duce noise into extracted features, polluting the Ô¨Ånal featurearXiv:2303.10976v1  [cs.CV]  20 Mar 2023representation of each image. When dealing with these pol luted features, different identities may have high similarities due to the same obstacle, resulting in incorrect matches. To address the aforementioned problems, some meth ods [34, 7, 23, 24] use additional trained networks, such as human parsing and keypoint estimation, to align differ ent human parts. With the aid of these extra networks, the occluded parts can be repaired by disseminating informa tion from the visible parts. However, these approaches are severely limited due to the domain gap between the pre trained network and the ReID dataset. Recently, with the exploration of attention mechanisms for various vision tasks, it has also been adopted for oc cluded person ReID to eliminate the interference of noisy information [44, 29, 12]. During the process of attention learning, many data augmentation strategies [1, 36, 51] gen erate artiÔ¨Åcial occlusion, which directs the attention to per son and forces it to avoid occluded regions. Currently, the most widely used artiÔ¨Åcial occlusion methods are ran dom erasing [48] or using the background as occlusion [1]. Nevertheless, pretrained attention networks are inherently more likely to focus on the semantically rich foreground than the background. Therefore, network will inevitably tend to ignore the occlusion constituted by the background, which will result in a lack of generalization. To illustrate this point, we utilized background for artiÔ¨Åcial occlusion based on the ViT baseline in TransReID [12] and visual ize the attention for both the training and testing sets in Fig.1(a). The results demonstrate that while the baseline can avoid artiÔ¨Åcial occlusions well in the training set, atten tion is still disturbed in the face of actual occlusions in the testing set due to the signiÔ¨Åcant discrepancy between artiÔ¨Å cial occlusion and actual occlusion. In this paper, we propose a solution to the challenges mentioned above by introducing an Attention Disturbance Mask (ADM) module that simulates realworld occlusions with greater Ô¨Ådelity. The primary way in which occlusions disrupt models is by impeding attention. However, obtain ing enough occluded data to enable the model to avoid such disruptions is difÔ¨Åcult. To surmount this problem, we utilize an attackoriented methodology that produces noise masks with the capacity to simulate the interference effects of ac tual obstructions at the feature level. This enables us to con struct occlusions that mirror the effects of those encountered in realworld scenarios. As illustrated in Figure 1(b), the proposed Attention Disturbance Module (ADM) performs a similar role to realworld occlusions by introducing dis ruptions to the neural network‚Äôs attention. This Ô¨Ånding di rectly veriÔ¨Åes the capability of our designed ADM in faith fully emulating occlusions at the feature level. By training the network on such occlusions that closely resemble those encountered in realworld scenarios, we can effectively en hance its robustness against occlusions during testing.However, handling complex occlusions directly can pose optimization challenges for the network. To address this is sue, we propose the DualPath Constraint Module (DPC) to handle both holistic and occluded images simultaneously, thus using holistic features as an extra supervisor to guide attention more towards the target pedestrian. Notably, the network parameters in the proposed DPC are shared by both paths, while the individual classiÔ¨Åers learn information about holistic and occluded images separately. The classi Ô¨Åer of the holistic path can be considered a prototype for passing holistic information to the occluded path as an extra supervisor, and global metric learning is also used to inter act with the information of each path and align the holistic and occluded features. The main contributions of our method can be summa rized as below: ‚Ä¢ We Ô¨Årst introduce a novel attackbased augmenta tion strategy called the Attention Disturbance Mask (ADM), which simulates real occlusion at the feature level and effectively diverts attention away from actual occlusions during testing. ‚Ä¢ We propose a DualPath Constraint module (DPC) that utilizes dualpath interactions to encourage the net work to learn a more generalized attention mechanism. DPC is compatible with existing occlusionbased data augmentation methods and can provide signiÔ¨Åcant per formance improvements. ‚Ä¢ The two proposed methods are both used to assist in the training of the baseline, and can be discarded in the inference stage, making them easy to be compatible with many existing methods, indicating the efÔ¨Åciency and wide applicability of our method. ‚Ä¢ Trained with our proposed ADP, the transformer base line can achieve new stateoftheart performance on multiple benchmark datasets e.g.,74:5%on Rank1 on OccludedDuke dataset. 2. Related Work "
49,Semi-Supervised Segmentation of Salt Bodies in Seismic Images using an Ensemble of Convolutional Neural Networks.txt,"Seismic image analysis plays a crucial role in a wide range of industrial
applications and has been receiving significant attention. One of the essential
challenges of seismic imaging is detecting subsurface salt structure which is
indispensable for identification of hydrocarbon reservoirs and drill path
planning. Unfortunately, exact identification of large salt deposits is
notoriously difficult and professional seismic imaging often requires expert
human interpretation of salt bodies. Convolutional neural networks (CNNs) have
been successfully applied in many fields, and several attempts have been made
in the field of seismic imaging. But the high cost of manual annotations by
geophysics experts and scarce publicly available labeled datasets hinder the
performance of the existing CNN-based methods. In this work, we propose a
semi-supervised method for segmentation (delineation) of salt bodies in seismic
images which utilizes unlabeled data for multi-round self-training. To reduce
error amplification during self-training we propose a scheme which uses an
ensemble of CNNs. We show that our approach outperforms state-of-the-art on the
TGS Salt Identification Challenge dataset and is ranked the first among the
3234 competing methods.","One of the major challenges of seismic imaging is localization and delineation of subsurface salt bodies. The precise location of salt deposits helps to identify reservoirs of hydrocarbons, such as crude oil or natural gas, which are trapped by overlying rocksalt formations due to the exceedingly small permeability of the latter. Modern seismic imaging techniques result in large amounts of unlabeled data which have to be interpreted. Unfortunately, the exact identication of large salt deposits is notoriously dicult [21] and often requires manual interpretation ofarXiv:1904.04445v3  [cs.CV]  5 Aug 20192 Yauhen Babakhin, Artsiom Sanakoyeu, Hirotoshi Kitamura Fig. 1. Progress of the validation loss (top) and the validation mAP score (bottom) during training our UResNet34 model on TGS Salt Identication Challenge dataset [22] forK= 3 rounds. Every next round the model converges faster and achieves better local minima. Loss spikes every 50 epochs correspond to the cycles of the cosine annealing learning rate schedule. seismic images by the domain experts. Despite being highly timeconsuming and expensive, manual interpretation induces a subjective human bias, which can lead to potentially dangerous situations for oil and gas company drillers. In recent years, a number of tools for automatic or semiautomatic seismic interpretation have been proposed [37,13,18,53,47,3,8,46] to speedup the inter pretation process and, to some extent, reduce the human bias. However, these methods do not generalize well for complex cases since they rely on handcrafted features. The advent of convolutional neural networks (CNNs) brought signicant ad vancements in dierent problems and several attempts have been made to apply CNNs in the eld of seismic imaging [43,11,45,52]. CNNs overcome the need for manual feature design and show superior performance on the tasks of the salt body delineation compared to the methods based on the handcrafted features. However, a low amount of publicly available annotated seismic images hinder the performance of the existing CNNbased methods since CNNs are notoriously hungry for data. To overcome the shortage of labeled data, we propose a semisupervised method for segmentation of salt bodies in seismic images which can make use of abundant unlabeled data. The unlabeled images are utilized for selftraining [42]. The proposed selftraining procedure (see Fig. 2) is an iterative process which extends the labeled dataset by alternating between training the model and pseudolabeling (i.e. imputing the labels on the unlabeled data). We do K rounds of retraining the model (see the straining in Fig. 1). At the rst round, we train model solely on the available labeled data and then predict labels on the unlabeled data. Every next round we use for training both original labeled data and the pseudolabels obtained at the previous round. The error amplication isSemiSupervised Segmentation of Salt Bodies in Seismic Images using CNNs 3 Fig. 2. The pipeline of the proposed selftraining procedure. We do Krounds of re training the model. Every round we train the model on the available labeled data and predicted condent pseudolabels for the unlabeled data. Allpseudolabels are recalculated at the end of every round. a wellknown problem in selftraining [29] when the error is accumulated during selftraining rounds and the models tend to generate less reliable predictions during the time. To mitigate it we propose to train an ensemble of CNNs and predict labels on the unlabeled data using the average voting of the models in the ensemble. Average voting scheme corrects examples which could be mislabeled by one of the models, hence facilitates more reliable pseudolabeling. Moreover, to further reduce the error amplication we retrain our models from scratch and predict labels for allunlabeled examples every round in similar spirit as [29]. We conduct experiments on the largest available to our knowledge dataset for salt body delineation { TGS Salt Identication Challenge dataset [22]. This dataset was collected by TGS, the world's leading geoscience data company, and was provided in the Kaggle competition. Our approach achieves stateoftheart performance on this dataset featuring the rst place in the global ranking among 3234 competitors. In summary, the contribution of this work is as follows: (i) we propose an iterative selftraining approach for semantic segmentation which benets from unlabeled data; (ii) we build a sophisticated network architecture which is tai lored for the task of salt body delineation (see Fig. 3); (iii) we evaluate our4 Yauhen Babakhin, Artsiom Sanakoyeu, Hirotoshi Kitamura Fig. 3. The outline of the UResNet34/UResNeXt50 architecture proposed. The dif ference between UResNet34 and UResNeXt50 is only in the structure of the encoder blocks (green). We insert scSE modules [39] after each encoder (green) and decoder (purple) blocks. Encoder blocks are connected with the corresponding decoder blocks using skipconnections. We use a Feature Pyramid Attention module (FPA) [15] after the last encoder block. All outputs of the decoder blocks are upsampled to have the same size as the output of the last decoder bock. Obtained feature maps are concate nated together into hypercolumns [16], which are used for prediction of the segmenta tion mask after applying two convolutional layers. approach on a realworld salt body delineation dataset { TGS Salt Identica tion Challenge [22], where the proposed method achieves the stateoftheart performance outperforming allother competing teams. 2 Related work "
50,An Ensemble Noise-Robust K-fold Cross-Validation Selection Method for Noisy Labels.txt,"We consider the problem of training robust and accurate deep neural networks
(DNNs) when subject to various proportions of noisy labels. Large-scale
datasets tend to contain mislabeled samples that can be memorized by DNNs,
impeding the performance. With appropriate handling, this degradation can be
alleviated. There are two problems to consider: how to distinguish clean
samples and how to deal with noisy samples. In this paper, we present Ensemble
Noise-robust K-fold Cross-Validation Selection (E-NKCVS) to effectively select
clean samples from noisy data, solving the first problem. For the second
problem, we create a new pseudo label for any sample determined to have an
uncertain or likely corrupt label. E-NKCVS obtains multiple predicted labels
for each sample and the entropy of these labels is used to tune the weight
given to the pseudo label and the given label. Theoretical analysis and
extensive verification of the algorithms in the noisy label setting are
provided. We evaluate our approach on various image and text classification
tasks where the labels have been manually corrupted with different noise
ratios. Additionally, two large real-world noisy datasets are also used,
Clothing-1M and WebVision. E-NKCVS is empirically shown to be highly tolerant
to considerable proportions of label noise and has a consistent improvement
over state-of-the-art methods. Especially on more difficult datasets with
higher noise ratios, we can achieve a significant improvement over the
second-best model. Moreover, our proposed approach can easily be integrated
into existing DNN methods to improve their robustness against label noise.","Together with the resurgence and remarkable success of DNNs, largescale datasets have become increasingly com mon. For supervised learning tasks, modern DNNs gener ally require the datasets to be annotated with accurate la Equal contribution.bels to achieve high performance. However, to correctly label large amounts of data is very costly and errorprone, even highquality handlabeled benchmark dataset such as ImageNet [Deng et al. , 2009 ]contains mislabeled sam ples[Northcutt et al. , 2019 ]. There exist alternative, lowcost methods, including largescale annotation through crowd sourcing [Sheng et al. , 2008 ]and online web queries [Divvala et al. , 2014 ], but these inevitably yield a higher proportion of incorrect class labels. DNNs are prone to overÔ¨Åtting to corrupted data sam ples, which increases the generalization error of the net work [Zhang et al. , 2017a ]. To address this issue, numerous algorithms have been proposed to train DNNs in a way ro bust to label noise [Wang et al. , 2019; Xu et al. , 2019 ]. The capability of DNNs to Ô¨Åt noisy data has been further stud ied by Chen et al. [2019 ]. They showed that, for symmetric noise, the test accuracy is a quadratic function of the noise ratio, and claim that generalization occurs in the sense of dis tribution. In this paper, we relax their assumptions and give a theoretical analysis of the impact that an imperfect classiÔ¨Åer has. Our Ô¨Åndings demonstrate that, while the noise level has a signiÔ¨Åcant impact, the performance of the classiÔ¨Åer is key. Based on our analysis, we propose ENKCVS, a novel en semble method based on Kfold crossvalidation to increase the generalization performance. We empirically evaluate our solution and demonstrate that it outperforms the stateofthe art, proving the effectiveness of our method. In summary, our contributions are as follows. ‚Ä¢ We propose a novel method (ENKCVS) based on a combination of Kfold crossvalidation and ensemble learning. Samples are selected from the noisy data by keeping those where the predicted label matches the given (noisy) label. Any nonselected samples can then either be discarded or reweighted to have a lower im pact. Mixup [Zhang et al. , 2017b ]is applied during training to augment the data. ‚Ä¢ We further propose a label reweighting scheme for sam ples that are likely erroneous. For these uncertain sam ples, we consider both the given label and a generated pseudo label with the weight set using the entropy of the predicted labels given by ENKCVS. ‚Ä¢ We empirically show that the proposed solution out performs stateoftheart noiserobust methods on imarXiv:2107.02347v1  [cs.LG]  6 Jul 2021age recognition and text classiÔ¨Åcation tasks on multiple datasets. Moreover, our solution can easily be incorpo rated into existing network architectures to enhance their robustness to noisy labels. 2 Related Work "
51,Combining Deep Learning and String Kernels for the Localization of Swiss German Tweets.txt,"In this work, we introduce the methods proposed by the UnibucKernel team in
solving the Social Media Variety Geolocation task featured in the 2020 VarDial
Evaluation Campaign. We address only the second subtask, which targets a data
set composed of nearly 30 thousand Swiss German Jodels. The dialect
identification task is about accurately predicting the latitude and longitude
of test samples. We frame the task as a double regression problem, employing a
variety of machine learning approaches to predict both latitude and longitude.
From simple models for regression, such as Support Vector Regression, to deep
neural networks, such as Long Short-Term Memory networks and character-level
convolutional neural networks, and, finally, to ensemble models based on
meta-learners, such as XGBoost, our interest is focused on approaching the
problem from a few different perspectives, in an attempt to minimize the
prediction error. With the same goal in mind, we also considered many types of
features, from high-level features, such as BERT embeddings, to low-level
features, such as characters n-grams, which are known to provide good results
in dialect identification. Our empirical results indicate that the handcrafted
model based on string kernels outperforms the deep learning approaches.
Nevertheless, our best performance is given by the ensemble model that combines
both handcrafted and deep learning models.","The organizers of the 2020 VarDial Evaluation Campaign (G Àòaman et al., 2020) proposed a shared task targeted towards the geolocation of short texts, e.g. tweets, namely the Social Media Variety Geoloca tion (SMG) task. Typically formulated as a double regression problem, the task is about predicting the location, expressed in latitude and longitude, from where the text received as input was posted on a cer tain social media platform. Twitter and Jodel are the platforms used for data collection, divided by the language area in three subtasks, namely: ‚Ä¢ Standard German Jodels (DEAT)  formed of conversations initiated in Germany and Austria in regional dialectal forms (Hovy and Purschke, 2018). ‚Ä¢ Swiss German Jodels (CH)  based on a smaller number of Jodel conversations from Switzerland (Hovy and Purschke, 2018). ‚Ä¢ BCMS Tweets  from the area of Bosnia and Herzegovina, Croatia, Montenegro and Serbia where the macrolanguage used is BCMS, with both similarities and a fair share of variation among the component languages (Ljube Àási¬¥c et al., 2016). In this paper, we focus only on the second subtask, SMGCH, proposing a variety of handcrafted and deep learning models, as well as an ensemble model that combines all our previous models through metalearning. Our Ô¨Årst model is a Support Vector Regression (SVR) classiÔ¨Åer (Chang and Lin, 2002) based on string kernels, which are known to perform well in other dialect identiÔ¨Åcation tasks (Butnaru and Ionescu, 2018b; Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). Our second model is a characterlevel convolutional neural network (CNN) (Zhang et al., 2015), which is also known to providearXiv:2010.03614v1  [cs.CL]  7 Oct 2020good results in dialect identiÔ¨Åcation (Butnaru and Ionescu, 2019; Tudoreanu, 2019). Due to the high pop ularity and the outstanding results of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) in solving mainstream NLP tasks, we decided to try out a Long ShortTerm Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) based on German BERT embeddings as our third model. Lastly, we combine our three models into an ensemble that employs Extreme Gradient Boosting (XGBoost) (Chen and Guestrin, 2016) as metalearner. We conducted experiments on the development set provided by the organizers, in order to decide which models to choose for our three submissions for the SMGCH subtask. Our results indicate that the ensemble model attains the best results. Perhaps surprisingly, our shallow approach based on string kernels outperforms both deep learning models. Our observations are consistent across the development and the test sets provided by the organizers. The rest of this paper is organized as follows. We present related work on dialect identiÔ¨Åcation and geolocation of short texts in Section 2. Our approaches are described in more detail in Section 3. We present the experiments and empirical results in Section 4. Finally, our conclusions are drawn in Sec tion 5. 2 Related Work "
52,CrowdTeacher: Robust Co-teaching with Noisy Answers & Sample-specific Perturbations for Tabular Data.txt,"Samples with ground truth labels may not always be available in numerous
domains. While learning from crowdsourcing labels has been explored, existing
models can still fail in the presence of sparse, unreliable, or diverging
annotations. Co-teaching methods have shown promising improvements for computer
vision problems with noisy labels by employing two classifiers trained on each
others' confident samples in each batch. Inspired by the idea of separating
confident and uncertain samples during the training process, we extend it for
the crowdsourcing problem. Our model, CrowdTeacher, uses the idea that
perturbation in the input space model can improve the robustness of the
classifier for noisy labels. Treating crowdsourcing annotations as a source of
noisy labeling, we perturb samples based on the certainty from the aggregated
annotations. The perturbed samples are fed to a Co-teaching algorithm tuned to
also accommodate smaller tabular data. We showcase the boost in predictive
power attained using CrowdTeacher for both synthetic and real datasets across
various label density settings. Our experiments reveal that our proposed
approach beats baselines modeling individual annotations and then combining
them, methods simultaneously learning a classifier and inferring truth labels,
and the Co-teaching algorithm with aggregated labels through common truth
inference methods.","Labeled data is essential to guarantee the success of increasingly more com plex classiÔ¨Åers. Unfortunately obtaining large quantities of highqu ality labels can be costprohibitive for several Ô¨Åelds. For example, in the medic al domains, it may take a clinician several hours to annotate the health records of hundreds of patients. One alternative is to gather labels using crowdsourcing , where re motely located workers are utilized to perform the task of labeling th e data. Although these crowdworkersindividually may not be as accurate as an expert, constructing the true label from their aggregated opinions can ap proximate the accuracy of an expert. However, the subjectivity of annotator s and their diÔ¨Äer ent qualiÔ¨Åcations introduce noise to the labeling process. To model t his noise, ‚àóEmory University, Atlanta GA , USA Email: msotood@emory.ed u 1most studies either focus on modeling the reliability of annotatorsan d their cor relation and reÔ¨Çecting it in the label aggregation phase or combining c lassiÔ¨Åer training with learning the annotators‚Äô trust parameters. Yet, lea rning through crowdsourcingbasedmodels can still fail in the presence ofdiÔ¨Äerin g annotations and unreliable users [14]. A promising direction for dealing with noisy labels for training complex classiÔ¨Åers is Coteaching [5]. Under the Coteaching paradigm, two p eer neural networks are trained separately and speciÔ¨Åc samples are exchang ed between the networksto reduce the errorof the two models and yield a more acc uratemodel. As a result, Coteaching methods have shown great promise for co mputer vision problemswithnoisylabels. Coteachingcannaturallycounteractcr owdsourcing noise since it Ô¨Ålters out noisy samples in the beginning and only adds the m at later training stages when they will be valuable. However, Coteach ing treats each sample with the same weight. This can cause the classiÔ¨Åer to inco rrectly learn from samples that may have fewer annotations or diverging hu man labels. To address this limitation, we propose to leverage the certainty of s amples from the label aggregation phase to inform the selection process o f Coteaching, which has not been studied before. Our model, CrowdTeacher, use s a perturba tion scheme based on the uncertainty of the samples to improve the robustness of the Coteaching framework. Given the availability of samples‚Äô unc ertainty from the label aggregation step, our model uses this information t o counter the inherent noise by perturbing the input space. In addition, the fram ework pri oritizes the more conÔ¨Ådent samples of the classiÔ¨Åer during the learn ing process. Thus, we tackle the problem of classiÔ¨Åcation with features and crow dsourcing labels using three mechanisms: ‚Ä¢Estimation of the features‚Äô distributions to generate synthetic d ata which is then used to perturb each sample in an additive manner, proportion al to its estimated label‚Äôs uncertainty. ‚Ä¢EnhancingCoteachingbyknowledgedistillation, i.e. astudentteac hermodel of a simple and a complex network to accommodate smaller tabular dat a. ‚Ä¢Utilization of the perturbed samples as input to the above classiÔ¨Åer t o further diÔ¨Äerentiate uncertain and certain training points based on their los s in each epoch Next, we formally deÔ¨Åne the problem and summarize and delineate whe re and how CrowdTeacher ties into the relevant literature in crowdsou rcing, data augmentation, and learning with noisy labels. 1.1 Problem DeÔ¨Ånition: ClassiÔ¨Åcation with Crowdsourc ing Annotations In practice, there are numerous applications in which the ground tr uth of a classiÔ¨Åcation task is not available, or disputed. For instance in medicin e, mul tiple pathologists do not always necessarily agree on the malignancy s tatus of a tumor in an image [8], or multiple nurses do not all agree on the presen ce of hospitalacquired bedsores for a patient given their charts [15]. Similarly, obtaining ground truth from experts to train reliable classiÔ¨Åers can be expen sive, as in the case of content Ô¨Åltering and regulation of posts on so cial media, 2which are distributed among multiple nonexpert annotators to obt ain some good quality labels [9]. Formally, we deÔ¨Åne learning with crowdsourcing la bels as follows: DeÔ¨Ånition 1. (ClassiÔ¨Åcation with Crowdsourcing Annotations) Consider a set ofRannotators labeling Nsamples with Kpossible classes. Given an answer matrixA‚ààRN√óRwhere each element anrindicates the label for sample n provided by annotator r, and the training feature matrix Xtr‚ààRN√óM, the goal is to train a classiÔ¨Åer that accurately predicts the true labels f or the test data using only its feature matrix Xts. We useKto denote number of classes. Simulated data from the synthesizer used for perturbation is shown by Sand the perturbed samples are denoted with/tildewidestXtr. The set of continuous and discrete features are shown by FcandFd respectively. Table 1 summarizes the notations used throughout t his paper. Table 1: Summary of Notations. Symbol Description NNumber of Samples RNumber of Annotators KNumber of Classes Œ±Perturbation Fraction XtrTraining feature matrix AAnswer matrix of all annotators SSynthetic feature matrix /tildewidestXtrPerturbed training samples feature matrix FcSet of continuous features FdSet of all discrete features PClass probability matrix ciCertainty of ith 1.2 Related works "
53,Using Deep Networks and Transfer Learning to Address Disinformation.txt,"We apply an ensemble pipeline composed of a character-level convolutional
neural network (CNN) and a long short-term memory (LSTM) as a general tool for
addressing a range of disinformation problems. We also demonstrate the ability
to use this architecture to transfer knowledge from labeled data in one domain
to related (supervised and unsupervised) tasks. Character-level neural networks
and transfer learning are particularly valuable tools in the disinformation
space because of the messy nature of social media, lack of labeled data, and
the multi-channel tactics of influence campaigns. We demonstrate their
effectiveness in several tasks relevant for detecting disinformation: spam
emails, review bombing, political sentiment, and conversation clustering.","Electronic communication is more embedded and essential to human life than ever before. This communication in creasingly relies on the distributed, selfpublishing model of social media platforms. The increasing ease of dis tributed communication does not come without drawbacks: disinformation‚Äîdeceptive information spread deliberately to change behavior, inÔ¨Çuence public opinion, or obscure the truth‚Äîhas inÔ¨Åltrated the online information ecosystem, with damaging consequences (Carvalho et al., 2011; Mo canu et al., 2015). Malicious electronic communication takes many forms. It spans from lowlevel social engineering attacks (i.e., phish ing) to more sophisticated, distributed efforts to disseminate state propaganda (Inkster, 2016; Okoro & Nwafor, 2013; Woolley, 2016). We propose that the language characteris 1New Knowledge, Austin, Texas, USA2Algorine, Inc., Austin, Texas, USA3Watson School of Engineering and Applied Science and the Department of Psychology: Cognitive and Brain Sciences, Binghamton University (SUNY), Binghamton, New York, USA. Correspondence to: Numa Dhamani <numa@newknowledge.io >. Appearing at the International Conference on Machine Learning AI for Social Good Workshop , Long Beach, United States, 2019.tics of known and identiÔ¨Åed sources of malicious electronic communication can be used as a signal for the detection and mitigation of these efforts across the diverse (and fractured) electronic communication ecosystem. The motivation of this work is to demonstrate how semantic classiÔ¨Åcation of natural language can be used as a tool for the detection of inÔ¨Çammatory, inauthentic, or otherwise nefarious communication. Characterlevel convolutional neural networks (CNNs) are particularly wellsuited for this task‚Äîas opposed to a wordlevel model‚Äîbecause they allow for nonvernacular discourse, misspelling, and other social media features (e.g., emoticons) to be learned without the constraint of Ô¨Åxed vocabularies (Zhang et al., 2015). We implement an adaptation of a neural network architecture recently demonstrated to be effective for text classiÔ¨Åcation (Zhang et al., 2015; J ¬¥ozefowicz et al., 2016). The method is purely contentbased and does not require any additional metadata beyond the text. To show the effectiveness of this method in relation to malicious communication and disinformation, we present a series of experimental results on semantic classiÔ¨Åcation for spam emails, review bombing, political sentiment, and conversation clustering. 2. Related Work "
54,Robust Training with Ensemble Consensus.txt,"Since deep neural networks are over-parameterized, they can memorize noisy
examples. We address such a memorization issue in the presence of label noise.
From the fact that deep neural networks cannot generalize to neighborhoods of
memorized features, we hypothesize that noisy examples do not consistently
incur small losses on the network under a certain perturbation. Based on this,
we propose a novel training method called Learning with Ensemble Consensus
(LEC) that prevents overfitting to noisy examples by removing them based on the
consensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC
outperforms the current state-of-the-art methods on noisy MNIST, CIFAR-10, and
CIFAR-100 in an efficient manner.","Deep neural networks (DNNs) have shown excellent performance (Krizhevsky et al., 2012; He et al., 2016) on visual recognition datasets (Deng et al., 2009). However, it is difÔ¨Åcult to obtain high quality labeled datasets in practice (Wang et al., 2018a). Even worse, DNNs might not learn patterns from the training data in the presence of noisy examples (Zhang et al., 2016). Therefore, there is an increasing demand for robust training methods. In general, DNNs optimized with SGD Ô¨Årst learn patterns relevant to clean examples under label noise (Arpit et al., 2017). Based on this, recent studies regard examples that incur small losses on the network that does not overÔ¨Åt noisy examples as clean (Han et al., 2018; Shen & Sanghavi, 2019). However, such smallloss examples could be noisy, especially under a high level of noise. Therefore, sampling trainable examples from a noisy dataset by relying on smallloss criteria might be impractical. To address this, we Ô¨Ånd the method to identify noisy examples among smallloss ones based on well known observations: (i) noisy examples are learned via memorization rather than via pattern learning and (ii) under a certain perturbation, network predictions for memorized features easily Ô¨Çuctuate, while those for generalized features do not. Based on these two observations, we hypothesize that out of smallloss examples, training losses of noisy examples would increase by injecting certain perturbation to network parameters, while those of clean examples would not. This suggests that examples that consistently incur small losses under multiple perturbations can be regarded as clean. This idea comes from an artifact of SGD optimization, thereby being applicable to any architecture optimized with SGD. In this work, we introduce a method to perturb parameters to distinguish noisy examples from small loss examples. We then propose a method to robustly train neural networks under label noise, which is termed learning with ensemble consensus (LEC). In LEC, the network is initially trained on the entire training set for a while and then trained on the intersection of smallloss examples of the ensemble of perturbed networks. We present three LECs with different perturbations and evaluate their effectiveness on three benchmark datasets with random label noise (Goldberger & BenReuven, 2016; Ma et al., 2018), openset noise (Wang et al., 2018b), and semantic noise. Our proposed LEC outperforms existing robust training methods by efÔ¨Åciently removing noisy examples from training batches. 1arXiv:1910.09792v3  [cs.LG]  11 Nov 2020Published as a conference paper at ICLR 2020 2 R ELATED WORK "
55,A Free Lunch to Person Re-identification: Learning from Automatically Generated Noisy Tracklets.txt,"A series of unsupervised video-based re-identification (re-ID) methods have
been proposed to solve the problem of high labor cost required to annotate
re-ID datasets. But their performance is still far lower than the supervised
counterparts. In the mean time, clean datasets without noise are used in these
methods, which is not realistic. In this paper, we propose to tackle this
problem by learning re-ID models from automatically generated person tracklets
by multiple objects tracking (MOT) algorithm. To this end, we design a
tracklet-based multi-level clustering (TMC) framework to effectively learn the
re-ID model from the noisy person tracklets. First, intra-tracklet isolation to
reduce ID switch noise within tracklets; second, alternates between using
inter-tracklet association to eliminate ID fragmentation noise and network
training using the pseudo label. Extensive experiments on MARS with various
manually generated noises show the effectiveness of the proposed framework.
Specifically, the proposed framework achieved mAP 53.4% and rank-1 63.7% on the
simulated tracklets with strongest noise, even outperforming the best existing
method on clean tracklets. Based on the results, we believe that building re-ID
models from automatically generated noisy tracklets is a reasonable approach
and will also be an important way to make re-ID models feasible in real-world
applications.","Person reidentiÔ¨Åcation (reID) is to match persons across nonoverlapping cameras. It is one of the core tech niques in intelligent surveillance analysis. Due to the ur gent demand for public safety, it has been an active research Ô¨Åeld over the years. Videobased person reID is the prob lem where subjects to be retrieved are presented as video sequences. Person reID has shown promising results in a fully supervised setting. This learning paradigm assumesthat there is a large number of labeled highquality cross camera training data. But it is of the high cost to collect such a largescale dataset, due to the exponential labeling cost. Besides, a welltrained reID model has been proved to perform much worse in a new domain. To overcome the drawbacks of supervised methods, in the last two years, several works have turned to study un supervised or weakly supervised person reID. SpeciÔ¨Åcally, we focus on unsupervised videobased person reID, where training data can be obtained without human labor by mul tiple object tracking [2] (MOT) algorithms, as shown in Ô¨Åg ure 1. Most of the existing unsupervised video reID meth ods still yield unsatisfactory results. Moreover, these meth ods operate on video reID datasets, such as MARS [28], iLIDSVID [17] and PRID 2011 [7]. It should be noted that, as shown in Ô¨Ågure 1, although these datasets are used in a unsupervised manner, i.e. video sequences without intra and intercamera ID association, the sequences themselves are clean and without noise, and the production of such clean sequences requires substantial human effort as well. The gap between such datasets and MOTgenerated track lets are the noise introduced by MOT algorithms, which is mainly ID fragmentation noise and ID switch noise, along side with detection [13] noise. Some methods have been proposed to exploit tracklets to build a reID model, but only ID fragmentation noise is considered, while ID switch and detection noise are ignored, resulting in a wider gap from being applicable to reallife scenarios. We propose a new trackletbased clustering and Ô¨Åne tuning framework to account for both ID fragmentation noise and ID switch noise in MOTgenerated tracklets. By analyzing characteristics of aforementioned noise, a multi stage clusteringbased method is proposed to reduce noise in the tracklets before feeding them into the unsupervised training pipeline, resulting in signiÔ¨Åcant performance boost. Since raw video of video reID datasets is generally unavail able, a novel algorithm is proposed to generate simulated tracklets from video reID datasets, to assist evaluating our method under various strength of noise.arXiv:2204.00891v1  [cs.CV]  2 Apr 2022Figure 1: Different categories of person reID methods. Supervised methods require full annotation of video sequences. Datasets used by unsupervised video reID methods require human labor to eliminate noise within tracklets generated by MOT algorithms. Our method uses MOT tracklets directly. We summarize our contribution as threefold. 1. Firstly, we propose taking raw tracklets generated by MOT algorithms as input of our method, removing the requirement of human effort completely, resulting in nearly zerocost training data preparation, moving a step closer into solving realistic problems. 2. Secondly, we analyze dominant noise categories in tracklets generated by MOT algorithms, i.e. ID frag mentation and ID switch noise, revealing character istics which is exploited in our novel noise reduction processing. We combine the noise reduction tech niques with a selftraining mechanism in our method, named Trackletbased Multilevel Clustering (TMC). 3. Thirdly, experiments show that our method achieves remarkable performance given that realistic tracklets with noise are used (mAP 55.3% rank1 68.2% on sim ulated tracklets), and, if clean video reID datasets are used instead, outperforms existing unsupervised video reID methods. 2. Related Work "
56,Verifying Inverse Model Neural Networks.txt,"Inverse problems exist in a wide variety of physical domains from aerospace
engineering to medical imaging. The goal is to infer the underlying state from
a set of observations. When the forward model that produced the observations is
nonlinear and stochastic, solving the inverse problem is very challenging.
Neural networks are an appealing solution for solving inverse problems as they
can be trained from noisy data and once trained are computationally efficient
to run. However, inverse model neural networks do not have guarantees of
correctness built-in, which makes them unreliable for use in safety and
accuracy-critical contexts. In this work we introduce a method for verifying
the correctness of inverse model neural networks. Our approach is to
overapproximate a nonlinear, stochastic forward model with piecewise linear
constraints and encode both the overapproximate forward model and the neural
network inverse model as a mixed-integer program. We demonstrate this
verification procedure on a real-world airplane fuel gauge case study. The
ability to verify and consequently trust inverse model neural networks allows
their use in a wide variety of contexts, from aerospace to medicine.","Neural networks have been used to solve a variety of nonlinea r inverse problems such as state estimation [20], inverse computational mechanics [2 4], inversemodel controller design [15], medical imaging [12], and seismic reÔ¨Çectivity estimation [16]. Neural networks are used because they can learn complex functions f rom data and compute outputs quickly, whereas an analytical inverse modeling ap proach may not work for nonlinear systems, and a samplingbased approach may be com putationally expensive. However, neural network inverse models lack accuracy guara ntees, limiting their use in safety critical applications such as transportation. To enable the adoption of these models, we present a method for verifying the accuracy of an i nverse model neural net work. Our approach uses recent advancements in neural netwo rk veriÔ¨Åcation to provide formal guarantees of the correctness of the model over its en tire domain. Inverse problems arise when trying to estimate an unobserve d state from a set of observations, where only the forward model is known (i.e., t he model that maps states to observations). This is in contrast to a Ô¨Åltering setting w here a dynamics model as well as an observation model is often available. Analytical solu tions to inverse problems may not exist for nonlinear systems, and may become intractable when the forward model2 C. Sidrane et al. is highdimensional or stochastic. Samplingbased approx imate inverse models may be slow to query and may not converge to a useable point estimate at all. As a result, data driven approaches may be applied. The forward model is used t o produce a dataset of stateobservation pairs that are then used to train a neural network to output a state for a given observation. Once a neural network inverse model is trained, we want guara ntees that the inverse mapping between observations and states has low error, but t his may be challenging due to the complexity of the forward model and neural network . Prior work [28] has approached this problem using sampling to get stochastic bo unds on the error of an inverse model network. Formal approaches [27] have also bee n applied to 1layer net works where the forward model is replaced with a lookup table so that the Lipschitz constants of the simple network and forward model may be easi ly calculated and the accuracy evaluated at grid points in the domain. In addition to these simplifying assump tions, this approach scales exponentially with the number o f dimensions in the domain, making it unsuitable for largescale problems. We verify ReLUbased inverse model neural networks using mi xedinteger linear programming, a technique that has seen signiÔ¨Åcant recent us age for neural network ver iÔ¨Åcation [1, 5, 7, 8, 19, 25]. This approach allows formal ver iÔ¨Åcation over the entire input domain without a dependence on gridding. Instead of re placing the complex for ward model that maps states to observations with a lookup tab le, we encode it into the mixedinteger linear program, preserving the integrity of the model. This is enabled by overapproximating the nonlinear functions in the forwar d model using a technique adapted from veriÔ¨Åcation of neural network control systems [23]. The nonlinear func tions are overapproximated using piecewise linear constra ints, which can be encoded into a mixedinteger linear program, alongside the inverse model neural network. We then maximize the error between the portion of the state that the inverse model re constructs and the original state values. This produces a ve riÔ¨Åed upper bound on the estimation error of the inverse model. Contributions Our contribution is to provide a new formal approach to verif ying in verse model neural networks by bringing ideas across discip line boundaries ‚Äì from reachability to inverse models. Compared to prior work, our work: 1. Can handle multilayer ReLU neural networks. 2. Does not require gridding the state space and incurring th e curse of dimensionality. 3. Can compare an inverse model neural network to the true non linear forward model, rather than a lookup table. 4. Can verify stochastic forward models with additive noise . 5. Employs intelligent parallelization in order to reduce v eriÔ¨Åcation time. 6. Demonstrates the approach on a realworld case study of ai rcraft fuel measurement. 2 Related Work "
57,Multi-label Iterated Learning for Image Classification with Label Ambiguity.txt,"Transfer learning from large-scale pre-trained models has become essential
for many computer vision tasks. Recent studies have shown that datasets like
ImageNet are weakly labeled since images with multiple object classes present
are assigned a single label. This ambiguity biases models towards a single
prediction, which could result in the suppression of classes that tend to
co-occur in the data. Inspired by language emergence literature, we propose
multi-label iterated learning (MILe) to incorporate the inductive biases of
multi-label learning from single labels using the framework of iterated
learning. MILe is a simple yet effective procedure that builds a multi-label
description of the image by propagating binary predictions through successive
generations of teacher and student networks with a learning bottleneck.
Experiments show that our approach exhibits systematic benefits on ImageNet
accuracy as well as ReaL F1 score, which indicates that MILe deals better with
label ambiguity than the standard training procedure, even when fine-tuning
from self-supervised weights. We also show that MILe is effective reducing
label noise, achieving state-of-the-art performance on real-world large-scale
noisy data such as WebVision. Furthermore, MILe improves performance in class
incremental settings such as IIRC and it is robust to distribution shifts.
Code: https://github.com/rajeswar18/MILe","Largescale datasets with humanannotated labels have been central to the development of modern stateoftheart neural networkbased artiÔ¨Åcial perception systems [ 25,26, 34]. Improved performance on ImageNet [ 18] has led to remarkable progress in tasks and domains that leverage Im ageNet pretraining [ 12,45,73]. However, these weakly annotated datasets and models tend to project a rich, multi label reality into a paradigm that envisions one and only one label per image. This form of simpliÔ¨Åcation often hinders *Equal contribution. // Teacher Student K t K s BCE  loss multilabel  prediction bottleneck BCE  loss singlelabel  groundtruth Copy weights stop gradient Human Car Tree HouseFigure 1. Multilabel Iterated Learning (MILe) builds a multi label representation of the images from singlylabeled groundtruth. In this example, a model produces multilabel binary predictions for the next generation, obtaining Car andHouse for an image weakly labeled with House . model performance by asking models to predict a single la bel, when trained on realworld images that contain multiple objects. Given the importance of the problem, there is growing recognition of singlelabeled datasets as a form of weak supervision and an increasing interest in evaluating the lim its of these singlylabeled benchmarks. A series of recent studies [ 9,57,59,62,68] highlight the problem of label ambiguity in ImageNet. In order to obtain a better estimate of model performance, Beyer et al. [9]and Shankar et al. [57] introduced multilabel evaluation sets. They identiÔ¨Åed softmax crossentropy training as one of the main reasons for low multilabel performance since it promotes label ex clusiveness. They also showed that replacing the softmax with sigmoid activations and casting the output as a set of binary classiÔ¨Åers results in better multilabel validation per formance. Several other studies have explored ways to over come the shortcomings in existing validation procedures by improving the pipelines for gathering labels [6, 51, 61]. In order to obtain a more complete description of images from weaklysupervised or semisupervised data, a number 1arXiv:2111.12172v1  [cs.CV]  23 Nov 2021of methods leverage a noisy signal such as pseudolabels [ 68] or textual descriptions crawled from the web [ 50]. In this work, we observe that the process of building a rich repre sentation of data from a noisy source shares some properties with the process of language emergence studied in the cog nitive science literature. In particular, Kirby [31] proposed that structured language emerged from an intergenerational iterated learning process [ 31,32,33]. According to the theory, a compositional syntax emerges when agents learn by imitation from previous generations in the presence of a learning bottleneck. This bottleneck forces noisy fragments of the language to be forgotten when transmitted to new generations. Conversely, those fragments that can be reused and composed to enrich the language tend to be passed to subsequent generations. We show that the same procedure can be applied to settings that leverage a weak or noisy su pervisory signal such as [ 50,68] to build a richer description of images while reducing the noise. In this work, we propose multilabel iterated learning (MILe) to learn to predict rich multilabel representations from weakly supervised (singlelabeled) training data. We do so by introducing two different learning bottlenecks. First, we replace the standard convolutional neural network out put softmax with a hard multilabel binary prediction. Sec ond, we transmit these binary predictions through successive model generations, with a limited training iterations between each generation. In our experiments, we demonstrate that MILe allevi ates the label ambiguity problem by improving the F1 score of supervised and selfsupervised models on the ImageNet ReaL [ 9] multilabel validation set. In addition, experiments on WebVision [ 40] show that iterated learning increases ro bustness to label noise and spurious correlations. Finally, we show that our approach can help in continual learning scenarios such as IIRC [ 1] where newly introduced labels cooccur with known labels. Our contributions are: ‚Ä¢We propose MILe, a multilabel iterated learning algo rithm for image classiÔ¨Åcation that builds a rich multi label representation of data from weak single labels. ‚Ä¢We show that models trained with MILe are more robust to noise and perform better on ImageNet, ImageNet ReaL, WebVision, and multiple setups such as super vised learning (Section 4.1), outofdistribution gener alization (Section 4.2), selfsupervised Ô¨Ånetuning and semisupervised learning (Section 4.3), and continual learning. ‚Ä¢We provide insights on the predictions made by models trained with iterated learning (Section 4.4). 2. Related Work "
58,Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data.txt,"Jointly extracting entity pairs and their relations is challenging when
working on distantly-supervised data with ambiguous or noisy labels. To
mitigate such impact, we propose uncertainty-aware bootstrap learning, which is
motivated by the intuition that the higher uncertainty of an instance, the more
likely the model confidence is inconsistent with the ground truths.
Specifically, we first explore instance-level data uncertainty to create an
initial high-confident examples. Such subset serves as filtering noisy
instances and facilitating the model to converge fast at the early stage.
During bootstrap learning, we propose self-ensembling as a regularizer to
alleviate inter-model uncertainty produced by noisy labels. We further define
probability variance of joint tagging probabilities to estimate inner-model
parametric uncertainty, which is used to select and build up new reliable
training instances for the next iteration. Experimental results on two large
datasets reveal that our approach outperforms existing strong baselines and
related methods.","Joint extraction involves extracting multiple types of entities and relations between them using a sin gle model, which is necessary in automatic knowl edge base construction (Yu et al., 2020). One way to cheaply acquire a large amount of labeled data for training joint extraction models is through dis tant supervision (DS) (Mintz et al., 2009). DS involves aligning a knowledge base (KB) with an unlabeled corpus using handcrafted rules or logic constraints. Due to the lack of human annotators, DS brings a large proportion of noisy labels, e.g., over 30% noisy instances in some cases (Mintz et al., 2009), making it impossible to learn useful features. The noise can be either false relations due to the aforementioned rulebased matching assump tion or wrong entity tags due to limited coverage over entities in opendomain KBs.Existing distantlysupervised approaches model noise relying either on heuristics such as reinforce ment learning (RL) (Nooralahzadeh et al., 2019; Hu et al., 2021) and adversarial learning (Chen et al., 2021), or patternbased methods (Jia et al., 2019; Shang et al., 2022) to select trustable in stances. Nevertheless, these methods require de signing heuristics or handcrafted patterns which may encourage a model to leverage spurious fea tures without considering the confidence or uncer tainty of its predictions. In response to these problems, we propose UnBED ‚ÄîUncertaintyaware Bootstrap learning for joint Extraction on Distantlysupervised data. UnBED assumes that 1) low data uncertainty in dicates reliable instances using a pretrained lan guage model (PLM) in the initial stage, 2) model should be aware of trustable entity and relation la bels regarding its uncertainty after training. Our bootstrap serves uncertainty as a principle to miti gate the impact of noise labels on model learning and validate input sequences to control the num ber of training examples in each step. Particularly, we quantify data uncertainty of an instance accord ing to its winning score (Hendrycks and Gimpel, 2017) and entropy (Shannon, 1948). We define averaged maximum probability that is estimated by a joint PLM over each token in a sequence to adapt previous techniques in joint extraction scheme. Instances with low data uncertainty are collected to form an initial subset, which is used to tune the joint PLM tagger and facilitate fast convergence. Then, we define parametric uncer tainty in two perspectives‚Äîintermodel and inner model uncertainty. The former is quantified by self ensembling (Wang and Wang, 2022) and serves as a regularizer to improve model robustness against noisy labels during training. The latter is captured by probability variance in MC Dropout (Gal and Ghahramani, 2016) for selecting new confident in stances for the next training iteration. Such twoarXiv:2305.03827v2  [cs.CL]  8 Jun 2023fold model uncertainties reinforce with each other to guide the model to iteratively improve its robust ness and learn from reliable knowledge. 2 Related Work "
59,Learning Noise-Aware Encoder-Decoder from Noisy Labels by Alternating Back-Propagation for Saliency Detection.txt,"In this paper, we propose a noise-aware encoder-decoder framework to
disentangle a clean saliency predictor from noisy training examples, where the
noisy labels are generated by unsupervised handcrafted feature-based methods.
The proposed model consists of two sub-models parameterized by neural networks:
(1) a saliency predictor that maps input images to clean saliency maps, and (2)
a noise generator, which is a latent variable model that produces noises from
Gaussian latent vectors. The whole model that represents noisy labels is a sum
of the two sub-models. The goal of training the model is to estimate the
parameters of both sub-models, and simultaneously infer the corresponding
latent vector of each noisy label. We propose to train the model by using an
alternating back-propagation (ABP) algorithm, which alternates the following
two steps: (1) learning back-propagation for estimating the parameters of two
sub-models by gradient ascent, and (2) inferential back-propagation for
inferring the latent vectors of training noisy examples by Langevin Dynamics.
To prevent the network from converging to trivial solutions, we utilize an
edge-aware smoothness loss to regularize hidden saliency maps to have similar
structures as their corresponding images. Experimental results on several
benchmark datasets indicate the effectiveness of the proposed model.","Visual saliency detection aims to locate salient regions that attract human atten tion. Conventional saliency detection methods [59,46] rely on human designed features to compute saliency for each pixel or superpixel. The deep learning revolution makes it possible to train endtoend deep saliency detection models in a datadriven manner [19,54,41,55,40,7,25,30,38,21,35,34,33,51], outperform ing handcrafted featurebased solutions by a wide margin. However, the success ?Work was done while Jing Zhang was an intern mentored by Jianwen Xie.arXiv:2007.12211v1  [cs.CV]  23 Jul 20202 Jing Zhang, Jianwen Xie, and Nick Barnes Fig. 1. An illustration of our framework. Representation: Each noisy label Yis rep resented as a sum of a clean saliency Sand a noise map . The clean saliency S is predicted from an image Xby an encoderdecoder network f1, and the noise is produced from a Gaussian noise vector Zby a generator network f2. Training: given the observed image Xand the corresponding noisy label Y, (i) the latent vector Z is inferred by MCMC and (ii) the parameters f1;2gof the encoderdecoder f1and the generator f2are updated by the gradient ascent for maximum likelihood. Testing: once the model is learned, the disentangled salicey predictor f1is the desired model for salicey prediction. of deep models mainly depends on a large amount of accurate human labeling [31,3,15], which is typically expensive and timeconsuming. To relieve the burden of pixelwise labeling, weakly supervised [17,31,52] and unsupervised saliency detection models [53,50,24] have been proposed. The for mer direction focuses on learning saliency from cheap but clean annotations, while the latter one studies learning saliency from noisy labels, which are typi cally obtained by conventional handcrafted featurebased methods. In this paper, we follow the second direction and propose a deep latent variable model that we call the noiseaware encoderdecoder to disentangle a clean saliency predictor from noisy labels. In general, a noisy label can be (1) a coarse saliency label generated by algorithmic pipelines using handcrafted features, (2) an imper fect humanannotated saliency label, or even (3) a clean label, which actually is a special case of noisy label, in which noise is none. Aiming at unsupervised saliency prediction, our paper assumes noisy labels to be produced by unsu pervised handcrafted featurebased saliency methods, and places emphasis on disentangled representation of noisy labels by the noiseaware encoderdecoder. Given a noisy dataset D=f(Xi;Yi)gn i=1ofnexamples, where XandY are image and its corresponding noisy saliency label, we intend to disentangle noiseand clean saliency Sfrom each noisy label Y, and learn a clean saliency predictorf1:X!S. To achieve this, we propose a conditional latent variable model, which is a disentangled representation of noisy saliency Y. See Figure 1NoiseAware EncoderDecoder for Saliency Detection 3 for an illustration of the proposed model. In the context of the model, each noisy label is assumed to be generated by adding a specic noise or perturbation  to its clean saliency map Sthat is dependent on its image X. Specically, the model consists of two submodels: (1) saliency predictor f1: an encoderdecoder network that maps an input image Xto a latent clean saliency map S, and (2) noise generator f2: a topdown neural network that produces a noise or error  from a lowdimensional Gaussian latent vector Z. As a latent variable model, the rigorous maximum likelihood learning (MLE) typically requires to compute an intractable posterior distribution, which is an inference step. To learn the latent variable model, two algorithms can be adopted: variational autoencoder (VAE) [13] or alternating backpropagation (ABP) [9,44,60]. VAE approximates MLE by minimizing the evidence lower bound with a separate inference model to approximate the true posterior, while ABP directly targets MLE and computes the posterior via Markov chain Monte Carlo (MCMC). In this paper, we generalize the ABP algorithm to learn the proposed model, which alternates the following two steps: (1) learning back propagation for estimating the parameters of two submodels, and (2) inferential backpropagation for inferring the latent vectors of training examples. As there may exist innite combinations of Sandsuch thatS+perfectly matches the provided noisy label Y, we further adopt the edgeaware smoothness loss [37] to serve as a regularization to force each latent saliency map Sto have a similar structure as its input image X. The learned disentangled saliency predictor f1 is the desired model for testing. Our solution is dierent from existing weak or noisy labelbased saliency approaches [53,50,24,18] in the following aspects: Firstly, unlike [53], we don't assume the saliency noise distribution is a Gaussian distribution. Our noise gen erator parameterized by a neural network is  exible enough to approximate any forms of structural noises. Secondly, we design a trainable noise generator to ex plicitly represent each noise as a nonlinear transformation of lowdimensional Gaussian noise Z, which is a latent variable that need to be inferred during training, while [53,50,24,18] have no noise inference process. Thirdly, we have no constraints on the number of noisy labels generated from each image, while [53,50,24] require multiple noisy labels per image for noise modeling or pseudo la bel generation. Lastly, our edgeaware smoothness loss serves as a regularization to force the produced latent saliency maps to be well aligned with their input images, which is dierent from [18], where object edges are used to produce pseudo saliency labels via multiscale combinatorial grouping (MCG) [1]. Our main contributions can be summarized as follows: {We propose to learn a clean saliency predictor from noisy labels by a novel latent variable model that we call noiseaware encoderdecoder, in which each noisy label is represented as a sum of the clean saliency generated from the input image and a noise map generated from a latent vector. {We propose to train the proposed model by an alternating backpropagation (ABP) algorithm, which rigorously and eciently maximizes the data like lihood without recruiting any other auxiliary model.4 Jing Zhang, Jianwen Xie, and Nick Barnes {We propose to use an edgeaware smoothness loss as a regularization to prevent the model from converging to a trivial solution. {Experimental results on various benchmark datasets show the stateofthe art performances of our framework in the task of unsupervised saliency de tection, and also comparable performances with the existing fullysupervised saliency detection methods. 2 Related Work "
60,Label Contrastive Coding based Graph Neural Network for Graph Classification.txt,"Graph classification is a critical research problem in many applications from
different domains. In order to learn a graph classification model, the most
widely used supervision component is an output layer together with
classification loss (e.g.,cross-entropy loss together with softmax or margin
loss). In fact, the discriminative information among instances are more
fine-grained, which can benefit graph classification tasks. In this paper, we
propose the novel Label Contrastive Coding based Graph Neural Network (LCGNN)
to utilize label information more effectively and comprehensively. LCGNN still
uses the classification loss to ensure the discriminability of classes.
Meanwhile, LCGNN leverages the proposed Label Contrastive Loss derived from
self-supervised learning to encourage instance-level intra-class compactness
and inter-class separability. To power the contrastive learning, LCGNN
introduces a dynamic label memory bank and a momentum updated encoder. Our
extensive evaluations with eight benchmark graph datasets demonstrate that
LCGNN can outperform state-of-the-art graph classification models. Experimental
results also verify that LCGNN can achieve competitive performance with less
training data because LCGNN exploits label information comprehensively.","Applications in many domains in the real world exhibit the favorable property of graph data structure, such as social networks [15], nancial platforms [20] and bioinformatics [5]. Graph classication aims to identify the class labels of graphs in the dataset, which is an important problem for numerous applications. For instance, in biology, a protein can be represented with a graph where each amino acid residue is a node, and the spatial relationships between residues (distances, angles) are the edges of a graph. Classication of graphs representing proteins can help predict protein interfaces [5]. Recently, graph neural networks (GNNs) have achieved outstanding perfor mance on graph classication tasks [29,33]. GNNs aims to transform nodes to lowdimensional dense embeddings that preserve graph structural information ?Two authors contributed equally to this workarXiv:2101.05486v1  [cs.LG]  14 Jan 20212 Y. Ren et al. and attributes [34]. When applying GNNs to graph classication, the standard method is to generate embeddings for all nodes in the graph and then summarize all these node embeddings to a representation of the entire graph, such as using a simple summation or neural network running on the set of node embeddings [31]. For the representation of the entire graph, a supervision component is usually utilized to achieve the purpose of graph classication. A nal output layer to gether with classication loss (e.g.,crossentropy loss together with softmax or margin loss) is the most commonly used supervision component in many existing GNNs [29,28,32,6]. This supervision component focuses on the discriminability of class but ignores the instancelevel discriminative representations. A recent trend towards learning stronger representations to serve classication tasks is to reinforce the model with discriminative information as more as possible [4]. To be explicit, graph representations, which consider both intraclass compact ness and interclass separability [14], are more potent on the graph classication tasks. Inspired by the idea of recent selfsupervised learning [3] and contrastive learning [7,18], the contrastive loss [17] is able to extract extra discriminative information to improve the model's performance. The recent works [8,18,35] of using contrast loss for representation learning are mainly carried out under the setting of unsupervised learning. These contrastive learning models treat each in stance as a distinct class of its own. Meanwhile, discriminating these instances is their learning objective [7]. The series of contrastive learning have been veried eective in learning more negrained instancelevel features in the computer vision [26] domain. Thus we plan to utilize the contrastive learning on graph classication tasks to make up for the shortcomings of supervision components, that is, ignoring the discriminative information on the instancelevel. However, when applying contrastive learning, the inherent large intraclass variations may import noise to graph classication tasks [14]. Besides, existing contrastive learn ing based GNNs (e.g., GCC [18]) detach the model pretraining and netuning steps. Compared with endtoend GNNs, the learned graph representations via contrastive learning can hardly be used in the downstream application tasks directly, like graph classication. To cope with the task of graph classication, we propose the label contrastive coding based graph neural network (LCGNN), which employs Label Contrastive Loss to encourage instancelevel intraclass compactness and interclass sepa rability simultaneously. Unlike existing contrastive learning using a single posi tive instance, the label contrastive coding imports label information and treats instances with the same label as multiple positive instances. In this way, the instances with the same label can be pulled closer, while the instances with dif ferent labels will be pushed away from each other. Intraclass compactness and interclass separability are taken into consideration simultaneously. The label contrastive coding can be regarded as training an encoder for a dictionary look up task [7]. In order to build an extensive and consistent dictionary, we propose a dynamic label memory bank and a momentumupdated graph encoder inspired by the mechanism [7]. At the same time, LCGNN also uses Classication Loss3 Query GraphsKey GraphsGraph  EncoderfqfkGraph  Encoder‚Ä¶Input Graph  Minibatch‚Ä¶‚Ä¶Momentum Update‚Ä¶‚Ä¶Key Graph  Representations Query Graph  RepresentationsMemory Bank‚Ä¶y1y2y3ymym 1RepresentationLabelUpdateLabel Contrastive  LossGraph ClassiÔ¨ÅerClassiÔ¨Åcation  LossTotal  LossGradient Update Fig. 1. The highlevel structure of LCGNN. LCGNN trains the graph encoder fqand the graph classier using a mixed loss. Label Contrastive Loss and Classication Loss constitute the mixed loss. Classication Loss used in LCGNN is crossentropy loss. Label Contrastive Loss is calculated by a dictionary lookup task. The query is each graph of the input graph minibatch, and the dictionary is a memory bank that can continuously update the labelknown graph representations. The graph representation in the memory bank is updated by the graph encoder fk, which is momentumupdated. After training, the learned graph encoder fq, and the graph classier can serve for graph classication tasks. to ensure the discriminability of classes. LCGNN can utilize label information more eectively and comprehensively from instancelevel and classlevel, allowing using fewer label data to achieve comparative performance, which can be con sidered as a kind of label augmentation in essence. We validate the performance of LCGNN on graph classication tasks over eight benchmark graph datasets. LCGNN achieves SOTA performance in seven of the graph datasets. What is more, LCGNN outperforms the baseline methods when using less training data, which veries its ability to learn from label information more comprehensively. The contributions of our work are summarized as follows: {We propose a novel label contrastive coding based graph neural network (LCGNN) to reinforce supervised GNNs with more discriminative informa tion. {The Label Contrastive Loss extends the contrastive learning to the supervised setting, where the label information can be imported to ensure intraclass compactness and interclass separability. {The momentumupdated graph encoder and the dynamic label memory bank are proposed to support our supervised contrastive learning. {We conduct extensive experiments on eight benchmark graph datasets. LCGNN not only achieves SOTA performance on multiple datasets but also can oer comparable results with fewer labeled training data. 2 Related Works "
61,Instance-Dependent Noisy Label Learning via Graphical Modelling.txt,"Noisy labels are unavoidable yet troublesome in the ecosystem of deep
learning because models can easily overfit them. There are many types of label
noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with
IDN being the only type that depends on image information. Such dependence on
image information makes IDN a critical type of label noise to study, given that
labelling mistakes are caused in large part by insufficient or ambiguous
information about the visual classes present in images. Aiming to provide an
effective technique to address IDN, we present a new graphical modelling
approach called InstanceGM, that combines discriminative and generative models.
The main contributions of InstanceGM are: i) the use of the continuous
Bernoulli distribution to train the generative model, offering significant
training advantages, and ii) the exploration of a state-of-the-art noisy-label
discriminative classifier to generate clean labels from instance-dependent
noisy-label samples. InstanceGM is competitive with current noisy-label
learning approaches, particularly in IDN benchmarks using synthetic and
real-world datasets, where our method shows better accuracy than the
competitors in most experiments.","The latest developments in deep neural networks (DNNs) have shown outstanding results in a variety of applications ranging from computer vision [31] to nat ural language processing [48] and medical image anal ysis [47]. Such success is strongly reliant on high capacity models, which in turn, require a massive amount of correctlyannotated data for training [34, 67]. Anno tating a large amount of data is, however, arduous, costly and timeconsuming, and therefore is often done via crowd sourcing [56] that generally produces lowquality annota *arpit.garg@aiml.teamtions. Although that brings down the cost and scales up the process, the tradeoff is the mislabelling of the data, result ing in a deterioration of deep models‚Äô performance [3, 35] due to the memorisation effect [2,35,44,70]. This has, there fore, motivated the research of novel learning algorithms to tackle the label noise problem where data might have been mislabelled. Early work in label noise [17] was carried out under the assumption that label noise was instanceindependent (IIN), i.e., mislabelling occurred regardless of the informa tion about the visual classes present in images. In IIN, we generally have a transition matrix that contains a pre deÔ¨Åned probability of Ô¨Çipping between pairs of labels (e.g., any image showing a cathas a high priori probability of being mislabelled as a dogand low a priori probability of being mislabelled as a car). This type of noise can also be divided into two subtypes: symmetric , where a true label is Ô¨Çipped to another label with equal probability across all classes, and asymmetric , where a true label is more likely to be mislabeled into one of some particular classes [17]. Nev ertheless, the IIN assumption is impractical for many real world datasets because we can intuitively argue that misla bellings mostly occur because of insufÔ¨Åcient or ambiguous information about the visual classes present in images. As a result, recent studies have gradually shifted their focus toward the more realistic scenario of instancedependent noise (IDN), where label noise depends on both the true class label and the image information [62]. Many methods have been introduced to handle not only IIN, but also IDN problems. Those include, but are not limited to, sample selection [12, 27, 33, 61, 72] that detects clean and noisy labels and applies semisupervised learn ing methods on the processed data, robust losses [1, 38, 46] that can work well with either clean or noisy labels, and probabilistic approaches [66] that model the data genera tion process, including how a noisy label is created. De spite some successes, most methods are often demonstrated in IIN settings with simulated symmetric and asymmetric noise. However, their performance is degraded when evalarXiv:2209.00906v1  [cs.CV]  2 Sep 2022uated on IDN problems, which include realworld and syn thetic datasets. Although there are a few studies focusing on the IDN setting [10, 26, 62, 66, 74], their relatively inac curate classiÔ¨Åcation results suggest that the algorithms can be improved further. In this paper, we propose a new method to tackle the IDN problem, called InstanceGM. Our method is designed based on a graphical model that considers the clean label Yas a latent variable and introduces another latent variable Zrepresenting the image feature to model the generation of a label noise ^Yand an image X. InstanceGM integrates generative and discriminate models, where the generative model is based on a variational autoencoder (V AE) [28], except that we replace the conventional mean squared error (MSE) when modelling the likelihood of reconstructed im ages by a continuous Bernoulli distribution [40] that facil itates the training process since it avoids tuning additional hyperparameters. For the discriminative model, to mitigate the problem of only using clean label data during the train ing process, which is a common issue present in the similar graphical model methods [66], we rely on DivideMix [33] that uses both clean and noisylabel data for training by exploring semisupervised learning via MixMatch [5]. Di videMix is shown to be a reasonably effective discrimina tive classiÔ¨Åer for our InstanceGM. In summary, the main contributions of the proposed method are: ‚Ä¢ InstanceGM follows a graphical modelling approach to generate both the image Xand its noisy label ^Y with the true label Yand image feature Zas latent variables. The modelling is associated with the contin uous Bernoulli distribution to model the generation of instanceXto facilitate the training, avoiding tuning of additional hyperparameters (see Remark 3). ‚Ä¢ For the discriminative classiÔ¨Åer of InstanceGM, we re place the commonly used coteaching, which is a dual model that relies only on training samples classiÔ¨Åed as clean, with DivideMix [33] that uses all training sam ples classiÔ¨Åed as clean and noisy. ‚Ä¢ InstanceGM shows stateoftheart results on a va riety of IDN benchmarks, including simulated and realworld datasets, such as CIFAR10 and CI FAR100 [30], Red MiniImageNet from Controlled Noisy Web Labels (CNWL) [65], ANIMAL10N [53] and CLOTHING1M [64]. 2. Related work "
62,Predicting Biomedical Interactions with Higher-Order Graph Convolutional Networks.txt,"Biomedical interaction networks have incredible potential to be useful in the
prediction of biologically meaningful interactions, identification of network
biomarkers of disease, and the discovery of putative drug targets. Recently,
graph neural networks have been proposed to effectively learn representations
for biomedical entities and achieved state-of-the-art results in biomedical
interaction prediction. These methods only consider information from immediate
neighbors but cannot learn a general mixing of features from neighbors at
various distances. In this paper, we present a higher-order graph convolutional
network (HOGCN) to aggregate information from the higher-order neighborhood for
biomedical interaction prediction. Specifically, HOGCN collects feature
representations of neighbors at various distances and learns their linear
mixing to obtain informative representations of biomedical entities.
Experiments on four interaction networks, including protein-protein, drug-drug,
drug-target, and gene-disease interactions, show that HOGCN achieves more
accurate and calibrated predictions. HOGCN performs well on noisy, sparse
interaction networks when feature representations of neighbors at various
distances are considered. Moreover, a set of novel interaction predictions are
validated by literature-based case studies.","ABiological system is a complex network of various molecular entities such as genes, proteins, and other biological molecules linked together by the interactions between these entities. The complex interplay between var ious molecular entities can be represented as interaction networks with molecular entities as nodes and their in teractions as edges. Such a representation of a biological system as a network provides a conceptual and intuitive framework to investigate and understand direct or indi rect interactions between different molecular entities in a biological system. Study of such networks lead to system level understanding of biology [1] and discovery of novel interactions including proteinprotein interactions (PPIs) [2], drugdrug interactions (DDIs) [3], drugtarget interactions (DTIs) [4] and genedisease associations (GDIs) [5]. Recently, the generalization of deep learning to the networkstructured data [6] has shown great promise across various domains such as social networks [7], recommen dation systems [8], chemistry [9], citation networks [10]. These approaches are under the umbrella of graph con volutional networks (GCNs). GCNs repeatedly aggregate feature representations of immediate neighbors to learn the informative representation of the nodes for link pre diction. Although GCN based methods show great suc cess in biomedical interaction prediction [3], [11], the issue with such methods is that they only consider information from immediate neighbors. SkipGNN [12] leverages skip graph to aggregate feature representations from direct and K. KC, R. Li, and A. Haake are with the Department of Computing and Information Sciences, Rochester Institute of Technology, Rochester, NY, 14623. Email:fkk3671, arhics, rxlics g@rit.edu F. Cui is with Thomas H. Gosnell School of Life Sciences, Rochester Institute of Technology, Rochester, NY, 14623. Email: fxcsbi@rit.edusecondorder neighbors and demonstrated improvements over GCN methods in biomedical interaction prediction. However, SkipGNN cannot be applied to aggregate infor mation from higherorder neighbors and thus fail to capture information that resides farther away from a particular interaction [13]. To address the challenge, we propose an endtoend deep graph representation learning framework named higherorder graph convolutional networks (HOGCN) for predicting interactions between pairs of biomedical entities. HOGCN learns a representation for every biomedical entity using an interaction network structure Gand/or features X. In particular, we deÔ¨Åne a higherorder graph convolution (HOGC) layer where the feature representations from k order neighbors are considered to obtain the representation of biomedical entities. The layer can thus learn to mix feature representations of neighbors at various distances for interaction prediction. Furthermore, we deÔ¨Åne a bilinear decoder to reconstruct the edges in the input interaction networkGby relying on feature representations produced by HOGC layers. The encoderdecoder approach makes HOGCN an endtoend trainable model for interaction pre diction. We compare HOGCN‚Äôs performance with that of state oftheart network similaritybased methods [14], network embedding methods [15], [16], and graph convolution based methods [10], [12], [17] for biomedical link prediction. We experiment with various interaction datasets and show that our method makes accurate and calibrated predic tions. HOGCN outperforms alternative methods based on network embedding by up to 30%. Furthermore, HOGCN outperforms graph convolutionbased methods by up to 6%, alluding to the beneÔ¨Åts of aggregating information from higherorder neighbors. We perform a case study on the DDI network andarXiv:2010.08516v1  [cs.LG]  16 Oct 2020JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 observe that aggregating information from higherorder neighborhood allows HOGCN to learn meaningful repre sentation for drugs. Moreover, literaturebased case studies illustrate that the novel predictions are supported by ev idence, suggesting that HOGCN can identify interactions that are highly likely to be a true positive. In summary, our study demonstrates the ability of HOGCN to identify potential interactions between biomedi cal entities and opens up the opportunities to use the biolog ical and physicochemical properties of biomedical entities for a followup analysis of these interactions. 2 R ELATED WORKS "
63,Automated Rip Current Detection with Region based Convolutional Neural Networks.txt,"This paper presents a machine learning approach for the automatic
identification of rip currents with breaking waves. Rip currents are dangerous
fast moving currents of water that result in many deaths by sweeping people out
to sea. Most people do not know how to recognize rip currents in order to avoid
them. Furthermore, efforts to forecast rip currents are hindered by lack of
observations to help train and validate hazard models. The presence of web cams
and smart phones have made video and still imagery of the coast ubiquitous and
provide a potential source of rip current observations. These same devices
could aid public awareness of the presence of rip currents. What is lacking is
a method to detect the presence or absence of rip currents from coastal
imagery. This paper provides expert labeled training and test data sets for rip
currents. We use Faster-RCNN and a custom temporal aggregation stage to make
detections from still images or videos with higher measured accuracy than both
humans and other methods of rip current detection previously reported in the
literature.","RipcurrentsarethemostsigniÔ¨Åcantsafetyrisktoswim mersalongthecoastlinesofoceans,seas,andlargelakes. [8, 7,13]. Themajorityofbeachgoersdonotknowhowtoiden tifyripcurrents,andthereisnorobustandreliablelocation independent method to identify them. Globally there are thousands of drownings each year due to rip currents [23, 42]. A 20 year study by the US Lifesaving Association re ports that 81.9% of the 37,000 beach rescues each year are duetoripcurrents[7]. Therehasbeennodeclineinthenum ber of associated drowning fatalities, despite warning signs and educational material. Rip currents are a wellstudied ocean phenomenon [3, 29, 31]. They are deÔ¨Åned as strong and narrow channels of fastmoving water that Ô¨Çow towards the sea from beaches. When waves break, they form a ‚Äúsetup‚Äù or an increase in mean water level. This setup can vary along a shoreline de pendingontheamountorheightofbreakingwaves. Ripcur rentsformaswatertendstoÔ¨Çowalongshorefromregionsof high setup (larger waves) to regions of lower setup (smaller waves) where currents converge to form a seaward Ô¨Çowing rip. The speed of seaward rips can be quite strong reaching 2 m/s, faster than an Olympic swimmer. There are multiple factorsthatdeterminethelocationandstrengthofrips,such as bathymetry, wave height and direction, tide, and beach shape. Rip currents may either be transient or persistent in space and time. Rips that are frequently found at the same location are usually indicative of a fairly stable bathymetric feature such as a sand bar or reef, or a hard structure such as rocky outcrop, jetty or pier. These bathymetric features results in variations in wave breaking and setup leading to channelizedripcurrentÔ¨Çow. TransientorÔ¨Çashripsareinde pendentofbathymetryandmaymoveupordownthebeach, and may appear or disappear. ORCID(s):Lifeguards are often trained to identify rip currents. However the majority of drownings occur on beaches with outtrainedpersonnel[1,4]. Postedsignscanprovideawarn ing,butthereisevidencethatmostpeopledonotÔ¨Åndexist ing signs helpful in actually identifying rip currents [6]. Experts at the National Oceanic and Atmospheric Ad ministration (NOAA) use images and video to gather statis tics about rip currents [19]. These data are supporting the validation of a rip current forecast model to alert people to potentialhazards[20]. Themostcommonlyusedmethodto visualizeripcurrentsfromvideoistimeaveraging,summa rizing a video as a single image [28]. In [43] boosted cas cade of simple Haar like features, a machine learning tech nique, was used to detect rip currents in time averaged im ages. Howeverthesetimeaverageswhenmanuallyassessed can be misinterpreted. Furthermore, they are not readily available nor interpretable by the average beachgoers, and the process of averaging removes available information. In recent years the coastal engineering community has successfullyuseddeepneuralnetworkstosolvemanyprob lems. ClassiÔ¨Åcation problems such as classifying wave breakingininfraredimagery[9],beachsceneandotherland scapeclassiÔ¨Åcation[11],automatedplanktonimageclassiÔ¨Å cation[41] andoceanfrontrecognition[36]wereformulated as deep learning problems using convolutional neural net works. Furthermore,someregressionproblemssuchasopti calwavegauging[10],trackingremotelysensedwaves[52], typhoon forecasting [33] were also solved using deep neu ral networks. In addition, generative adversarial networks, a type of deep neural networks, were used to improve the quality of downscaling of ocean remote sensing data [18]. Object detection with deep neural networks is well studied in the computer vision community. However most benchmarks and research focus on detecting physical objects with boundaries between what is and is not anobject[16,22,37]. Ripcurrentsareephemeral‚Äúobjects‚Äù de Silva et al.: Preprint submitted to Elsevier Page 1 of 9arXiv:2102.02902v1  [cs.CV]  4 Feb 2021Automated rip current detection Figure 1: A collection of beach scenes, some of which contain dangerous rip currents. Unfortunately these ‚Äúobjects‚Äù do not have clear shape, and most people Ô¨Ånd them hard to identify. This paper describes a computer vision system with detection accuracy higher than both existing published methods and human observers. which are not observable in every frame, and amorphous without clearly deÔ¨Åned boundaries even when observable. It is not obvious whether existing methods are applicable. Figure1providesasetofexamples,illustratingthediÔ¨Éculty of the problem. Our work is aimed at introducing this problem to the coastalengineeringcommunity,andshowingthatobjectde tection methods areapplicable. We gathered training data sets of rip currents and worked with experts at NOAA to ensure that test data were labeled correctly. We use Faster RCNN [50] with a custom temporal aggregation stage that allowed us to achieve detection accuracy that is higher than bothhumansandothermethodsofripcurrentdetectionpre viously reported in the literature. The contributions of this paper are: ‚Ä¢Evidence that region based convolutional neural net works (CNN) approach for object detection is appli cabletoamorphousandephemeralobjectssuchasrip currents ‚Ä¢Analysisshowingripcurrentdetectionaccuracyabove existing published methods ‚Ä¢Data sets of rip current images and video for training and testing The remainder of the paper is organized as follows. All therelatedworkissummarizedinSection2. Wediscusshow thedatawascollectedinSection3. Ourmethodisdiscussed in 4. Results are discussed in 5. Limitations and discussion areinSection6. InSection7weconcludeourpaper. Andin AppendixAweprovidethelinktothesupplementarymate rials. 2. Related Work "
64,EEG-Based Emotion Recognition Using Regularized Graph Neural Networks.txt,"Electroencephalography (EEG) measures the neuronal activities in different
brain regions via electrodes. Many existing studies on EEG-based emotion
recognition do not fully exploit the topology of EEG channels. In this paper,
we propose a regularized graph neural network (RGNN) for EEG-based emotion
recognition. RGNN considers the biological topology among different brain
regions to capture both local and global relations among different EEG
channels. Specifically, we model the inter-channel relations in EEG signals via
an adjacency matrix in a graph neural network where the connection and
sparseness of the adjacency matrix are inspired by neuroscience theories of
human brain organization. In addition, we propose two regularizers, namely
node-wise domain adversarial training (NodeDAT) and emotion-aware distribution
learning (EmotionDL), to better handle cross-subject EEG variations and noisy
labels, respectively. Extensive experiments on two public datasets, SEED and
SEED-IV, demonstrate the superior performance of our model than
state-of-the-art models in most experimental settings. Moreover, ablation
studies show that the proposed adjacency matrix and two regularizers contribute
consistent and significant gain to the performance of our RGNN model. Finally,
investigations on the neuronal activities reveal important brain regions and
inter-channel relations for EEG-based emotion recognition.","EMOTION recognition focuses on the recognition of hu man emotions based on a variety of modalities, such as audiovisual expressions, body language, physiological signals, etc. Compared to other modalities, physiological signals, such as electroencephalography (EEG), electrocar diogram (ECG), electromyography (EMG), etc., have the advantage of being difÔ¨Åcult to hide or disguise. In recent years, due to the rapid development of noninvasive, easy touse and inexpensive EEG recording devices, EEGbased emotion recognition has received an increasing amount of attention in both research [1] and applications [2]. Emotion models can be broadly categorized into discrete models and dimensional models. The former categorizes emotions into discrete entities, e.g., anger, disgust, fear, happiness, sadness, and surprise in Ekman‚Äôs theory [3]. The latter describes emotions using their underlying di mensions, e.g., valence, arousal and dominance [4], which measures emotions from unpleasant to pleasant, passive to active, and submissive to dominant, respectively. EEG signals measure voltage Ô¨Çuctuations from the cortex in the brain and have been shown to reveal important information about human emotional states [5]. For example, greater relative left frontal EEG activity has been observed when experiencing positive emotions [5]. The voltage Ô¨Çuctu ations in different brain regions are measured by electrodes attached to the scalp. Each electrode collects EEG signals in one channel. The collected EEG signals are often analyzed P . Zhong, D. Wang and C. Miao are with the Joint NTUUBC Research Centre of Excellence in Active Living for the Elderly (LILY), Nanyang Technological University, Singapore. P . Zhong and C. Miao are also with the AlibabaNTU Singapore Joint Research Institute and the School of Computer Science and Engineering, Nanyang Technological University, Singapore. Email: peixiang001@e.ntu.edu.sg, fwangdi, ascymiaog@ntu.edu.sgin speciÔ¨Åc frequency bands, namely delta (14 Hz), theta (47 Hz), alpha (813 Hz), beta (1330 Hz), and gamma ( >30 Hz). Many existing EEGbased emotion recognition methods are primarily based on the supervised machine learning approach, wherein features are often extracted from prepro cessed EEG signals in each channel over a time window. Then, a classiÔ¨Åer is trained on the extracted features to recognize emotions. Wang et al. [6] compared power spec tral density features (PSD), wavelet features and nonlinear dynamical features with a Support Vector Machine (SVM) classiÔ¨Åer. Zheng and Lu [7] investigated critical frequency bands and channels using PSD, differential entropy (DE) [8] and PSD asymmetry features, and obtained robust accuracy using deep belief networks (DBN). However, most existing EEGbased emotion recognition approaches do not address the following three challenges: 1) the topological structure of EEG channels are not effectively exploited to learn more discriminative EEG representations; 2) EEG signals vary sig niÔ¨Åcantly across different subjects, which hinders the gener alizability of the trained classiÔ¨Åers in subjectindependent classiÔ¨Åcation settings; and 3) participants may not always generate the intended emotions when watching emotion eliciting stimuli. Consequently, the emotion labels in the collected EEG data may be noisy and inconsistent with the actual elicited emotions. There have been several attempts to address the Ô¨Årst challenge. Zhang et al. [9] and Zhang et al. [10] incorpo rated spatial relations in EEG signals using convolutional neural networks (CNN) and recurrent neural networks (RNN), respectively. However, their approaches require a 2D representation of EEG channels on the scalp, which may cause information loss during Ô¨Çattening because channels are actually arranged in the 3D space. In addition, their approach of using CNNs and RNNs to capture interchannelarXiv:1907.07835v4  [cs.CV]  13 May 20202 relations has difÔ¨Åculty in learning longrange dependencies [11]. Graph neural networks (GNN) has been applied in [12] to capture interchannel relations using an adjacency matrix. However, similar to CNNs and RNNs, the GNN approach [12] only considers relations between the nearest channels, which thus may lose valuable information between distant channels, such as the PSD asymmetry between channels on the left and right hemispheres in the frontal region, which has been shown to be informative in valence prediction [5]. In recent years, several studies [13], [14] attempted to tackle the second challenge by investigating the transfer ability of EEGbased emotion recognition models across subjects. Lan et al. [15] compared several domain adapta tion techniques such as maximum independence domain adaptation (MIDA), transfer component analysis (TCA), subspace alignment (SA), etc. They found that the subject independent classiÔ¨Åcation accuracy can be improved by around 10%. Li et al. [16] applied domain adversarial train ing to lower the inÔ¨Çuence of individual subject on EEG data and obtained improved performance as well. However, their adversarial training does not exploit any graph structure of the EEG signals and only leads to small performance improvement in our experiment (see Section 7.1). To the best of our knowledge, no attempt has been made to address the third challenge, i.e., noisy emotion labels, in EEGbased emotion recognition. In this paper, we propose a regularized graph neural network (RGNN) aiming to address all the three aforemen tioned challenges. Graph analysis for human brain has been studied extensively in the neuroscience literature [17], [18]. However, making an accurate connectome is still an open question and subject to different scales [18]. Inspired by [12], [19], we consider each EEG channel as a node in our graph. Our RGNN model extends the simple graph convolution network (SGC) [20] and leverages the topological structure of EEG channels. SpeciÔ¨Åcally, we propose a sparse adjacency matrix to capture both local and global interchannel rela tions based on the biological topology of human brain [19]. Local interchannel relations connect nearby groups of neu rons and may reveal anatomical connectivity at macroscale [18], [21]. Global interchannel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotionrelated functional connectivity [5], [16]. In addition, we propose a nodewise domain adversar ial training (NodeDAT) method to regularize RGNN for better generalization in subjectindependent classiÔ¨Åcation scenarios. Different from the domain adversarial training in [16], [22], our NodeDAT method provides a Ô¨Ånergrained regularization by minimizing the domain discrepancies be tween features in the source and target domains for each channel/node. Moreover, we propose an emotionaware distribution learning (EmotionDL) method to address the problem of noisy labels in the datasets. Prior studies have shown that noisy labels can adversely impact classiÔ¨Åcation accuracy [23]. Instead of learning the traditional singlelabel classiÔ¨Åcation, our EmotionDL method learns a distribution of labels of the training data and thus acts as a regularizer to improve the robustness of our model against noisy labels. Finally, we conduct extensive experiments to validate the effectiveness of our RGNN model and investigate emotionrelated informative neuronal activities. In summary, the main contributions of this paper are as follows: 1) We propose a regularized graph neural network (RGNN) model to recognize emotions based on EEG signals. Our biologically inspired model captures both local and global interchannel relations. 2) We propose two regularizers: nodewise domain adversarial training (NodeDAT) and emotionaware distribution learning (EmotionDL), to improve the robustness of our model against crosssubject varia tions and noisy labels, respectively. 3) We conduct extensive experiments in both subject dependent and subjectindependent classiÔ¨Åcation settings on two public EEG datasets, namely SEED [7] and SEEDIV [24]. Experimental results demon strate the effectiveness of our proposed model and regularizers. In addition, our RGNN model achieves superior performance over the stateoftheart mod els in most experimental settings. 4) We investigate the emotional neuronal activities and the results reveal that prefrontal, parietal and occipital regions may be the most informative re gions for emotion recognition. In addition, global interchannel relations between the left and right hemispheres are important, and local interchannel relations between (FP1, AF3), (F6, F8) and (FP2, AF4) may also provide useful information. 2 R ELATED WORK "
65,Learning from Multiple Expert Annotators for Enhancing Anomaly Detection in Medical Image Analysis.txt,"Building an accurate computer-aided diagnosis system based on data-driven
approaches requires a large amount of high-quality labeled data. In medical
imaging analysis, multiple expert annotators often produce subjective estimates
about ""ground truth labels"" during the annotation process, depending on their
expertise and experience. As a result, the labeled data may contain a variety
of human biases with a high rate of disagreement among annotators, which
significantly affect the performance of supervised machine learning algorithms.
To tackle this challenge, we propose a simple yet effective approach to combine
annotations from multiple radiology experts for training a deep learning-based
detector that aims to detect abnormalities on medical scans. The proposed
method first estimates the ground truth annotations and confidence scores of
training examples. The estimated annotations and their scores are then used to
train a deep learning detector with a re-weighted loss function to localize
abnormal findings. We conduct an extensive experimental evaluation of the
proposed approach on both simulated and real-world medical imaging datasets.
The experimental results show that our approach significantly outperforms
baseline approaches that do not consider the disagreements among annotators,
including methods in which all of the noisy annotations are treated equally as
ground truth and the ensemble of different models trained on different label
sets provided separately by annotators.","Computeraided diagnosis (CAD) systems for medical imaging analysis are getting more and more successful thanks to the availability of largescale labeled datasets and the advances of supervised learning algorithms [ 1,2]. To reach expertlevel performance, those algorithms usually require highquality label sets, commonly scarce because of the costly and intensive labeling procedures. A typical label collection process in medical imaging is ‚Äú repeatedlabeling ‚Äù, where multiple clinical experts annotate each data instance to overcome human biases [3,4,5]. However, becauseofthediÔ¨ÄerencesfromannotatorbiasesandproÔ¨Åciency, annotations from the repeatedlabeling process often suÔ¨Äer from high interreader variability [ 6,7,8], which could reduce leaning performance if we treat them as groundtruth. Many prior works have been done to mitigate interreader variations in anno tations, which can be categorized into two main groups: (i) onestage approach and (ii) twostage approach. The Ô¨Årst group learns the model, annotators‚Äô proÔ¨Å ciency, and latent true labels jointly. Meanwhile, the second group Ô¨Årst estimates the true label of each instance from its multiple label sets [ 9]. This process is known as ‚Äú truth inference ‚Äù. After that, a supervised learning model is trained on the estimated true labels. All of those approaches show impressive results on both classiÔ¨Åcation and segmentation problems [10, 11]. This work aims at addressing a fundamental question ‚Äú How to train a deep learningbased detector eÔ¨Äectively from a set of possibly noisy labeled data pro vided by multiple annotators? ‚Äù [12]. To this end, we introduce a novel approach that learns from multiple expert annotators to improve the performance of a deep neural network in detecting abnormalities from chest Xray images. The proposed approach, as visualized in Figure 1, consists of two stages. The Ô¨Årst one is truth inference using Weighted Boxes Fusion (WBF) algorithm [ 13] to estimate the true labels and their conÔ¨Ådence scores. The second stage is to train 2an object detector on estimated labels with a reweighted loss function using implicit annotators‚Äô agreement, which is represented by the estimated conÔ¨Ådence scores. For evaluation, we Ô¨Årst simulate and test the proposed approach on a multipleexpertsdetection dataset from MNIST [ 14] called MEDMNIST. We then validate our approach on a realworld chest Xray dataset with radiologist‚Äôs annotations. Experiments on those scenarios demonstrate that the proposed approach provides better detection performance in terms of mAP scores than the baseline of treating multiple annotations as ground truth and the ensemble of models supervised by individual expert annotations. In summary, our main contributions in this work are twofolds: ‚Ä¢First, we introduce a simple yet eÔ¨Äective method that allows a deep learning network to learn from multiple annotators to improve its performance in detecting abnormalities from medical images. The proposed approach aims at estimating the true annotations from multiple experts with conÔ¨Ådence scores and uses these annotations to train a deep learningbased detector. This helps remove uncertainty in the learning process and provides higher label quality to train predictive models. ‚Ä¢Second, the proposed approach demonstrates its eÔ¨Äectiveness on both simulated and real medical imaging datasets by surpassing current state oftheart methods on the context of learning with multiple annotators. In particular, our method is simple and can be applied for a wide range of applications in medical imaging and object detection in general. The codes used in the experiments are available on our Github page at https:// github.com/huyhieupham/learningfrommultipleannotators . We also have made the dataset used in this study available for public access on our project‚Äôs webpage at https://vindr.ai/datasets/cxr . The rest of the paper is organized as follows. Related works on learning from multiple annotators and weighted training techniques are reviewed in Section 2. Section 3 presents the details of the proposed method with a focus on how to 3estimate the ground truth annotations from multiple experts. Section 4 provides comprehensive experiments on a simulated object detection dataset and a real world chest Xray dataset. Section 5 discusses the experimental results, some key Ô¨Åndings, and limitations of this work. Finally, Section 6 concludes the paper. 2. Related works "
66,KCP: Kernel Cluster Pruning for Dense Labeling Neural Networks.txt,"Pruning has become a promising technique used to compress and accelerate
neural networks. Existing methods are mainly evaluated on spare labeling
applications. However, dense labeling applications are those closer to real
world problems that require real-time processing on resource-constrained mobile
devices. Pruning for dense labeling applications is still a largely unexplored
field. The prevailing filter channel pruning method removes the entire filter
channel. Accordingly, the interaction between each kernel in one filter channel
is ignored.
  In this study, we proposed kernel cluster pruning (KCP) to prune dense
labeling networks. We developed a clustering technique to identify the least
representational kernels in each layer. By iteratively removing those kernels,
the parameter that can better represent the entire network is preserved; thus,
we achieve better accuracy with a decent model size and computation reduction.
When evaluated on stereo matching and semantic segmentation neural networks,
our method can reduce more than 70% of FLOPs with less than 1% of accuracy
drop. Moreover, for ResNet-50 on ILSVRC-2012, our KCP can reduce more than 50%
of FLOPs reduction with 0.13% Top-1 accuracy gain. Therefore, KCP achieves
state-of-the-art pruning results.","Deep learning based algorithms have been evolving to achieve signiÔ¨Åcant results in solving complex and vari ous computer vision applications, such as image classiÔ¨Å cation [14, 20, 35], stereo matching [41, 2], and seman tic segmentation [27, 3, 37]. However, real world appli cations often require the deployment of these algorithms on resourcelimited mobile/edge devices. Owing to over parameterization and high computational cost, it is difÔ¨Å cult to run those cumbersome models on small devices [40], thereby hindering the fruits of those stateoftheart (SOTA) intelligent algorithms from beneÔ¨Åting our lives. Neural net work pruning is a technique to strike the balance between accuracy (performance) and cost (efÔ¨Åciency). Filter level Figure 1. KCP benchmarked on CIFAR10. Points that is lo cated near upper left are preferred because they achieved bet ter accuracyFLOPs tradeoff. Baseline points are the original ResNet. Lines are the envelopes of points from different depth ResNet. Our method achieves better efÔ¨Åciency than previous state ofthearts, namely, FPGM [17], LFPC [15], and HRank [24]. Model pruned by KCP are closer to the upper left corner, and our envelope is decently above that of all other works. Figure 2. Results of KCP on pruning dense labeling networks. The upper row show the results of depth estimation; the predicted depth are samples from KITTI2015 [30, 31] generated by the orig inal PSMNet [2] and the KCP processed 50% sparsity model. The lower row shows the result of semantic segmentation; the predicted class label are samples from Cityscapes [5] generated by original HRNet [37] and KCP processed 50% sparsity model. KCP maintains the model performance both quantitatively and qualitatively. 1arXiv:2101.06686v1  [cs.CV]  17 Jan 2021pruning has been proven to be an effective [23, 29, 17, 24, 10, 15] and favorable method because it could result in a more structural model. Despite the numerous Ô¨Ålter pruning algorithms that have been proposed, previous works are mostly evaluated on a sparse labeling classiÔ¨Åcation task. In a practical scenario, dense labeling problems often require realtime processing in real world problems. Hence, the compression of a dense labeling neural network is crucial and demanding, yet this Ô¨Åeld is still quite unexplored. Moreover, Ô¨Ålter channel prun ing removes the entire output channel, which can sometimes lead to suboptimal results in terms of accuracy preserving. A single Ô¨Ålter channel often contains several kernels; thus, the information density is still high. If one removes the en tire channel, the interactions between each kernels are ne glected. In such a situation, each kernel in one Ô¨Ålter may contribute differently to the Ô¨Ånal prediction. Pruning at the level of the channel is equivalent to aggregating the inÔ¨Çu ence of an individual kernel, which dilutes the uniqueness of each kernel. We propose kernel cluster pruning (KCP) to address the issues and Ô¨Åll the gap in dense labeling pruning. The main concept of KCP is to identify the least representational ker nels in each layer and subsequently remove them. The smallest target to be pruned in KCP is the kernel instead of the Ô¨Ålter. First, the cluster center of kernels in each layer are calculated. Thereafter, the kernels that are closest to the cluster center are considered to carry less information and are removed. Because every pixel in the prediction is assigned a label, dense labeling networks are more vulner able to change in parameter than sparse labeling networks. Kernel pruning enables us to compress dense labeling net works more delicately while still maintaining the network structure after pruning. Experiments showed that we not only successfully compressed the dense labeling network but also achieved SOTA results on CIFAR10 [19] and Im ageNet (ILSVRC2012) [34]. Our main contributions are summarized as follows: ‚Ä¢ A novel kernel pruning method, KCP, is proposed. This method can identify the kernels with least rep resentativeness in each layer then prune those kernels without breaking the original network structure. ‚Ä¢ Thorough evaluations of dense labeling neural network pruning are presented. The results show that KCP can effectively prune those networks and retain the accu racy. To the best of our knowledge, this is the Ô¨Årst work that has investigated structured Ô¨Ålter pruning on dense labeling applications. ‚Ä¢ The experiment on benchmark datasets demonstrates the effectiveness of KCP. KCP achieves new SOTA re sults for ResNet on CIFAR10 (c.f. Fig 1, Table 1) and ImageNet (c.f. Table 2).2. Related Works "
67,Exploiting the relationship between visual and textual features in social networks for image classification with zero-shot deep learning.txt,"One of the main issues related to unsupervised machine learning is the cost
of processing and extracting useful information from large datasets. In this
work, we propose a classifier ensemble based on the transferable learning
capabilities of the CLIP neural network architecture in multimodal environments
(image and text) from social media. For this purpose, we used the InstaNY100K
dataset and proposed a validation approach based on sampling techniques. Our
experiments, based on image classification tasks according to the labels of the
Places dataset, are performed by first considering only the visual part, and
then adding the associated texts as support. The results obtained demonstrated
that trained neural networks such as CLIP can be successfully applied to image
classification with little fine-tuning, and considering the associated texts to
the images can help to improve the accuracy depending on the goal. The results
demonstrated what seems to be a promising research direction.","The high cost of processing and extracting useful information for its application in dierent areas of interest is one of the major issues related to unsupervised machine learning. Models based on neural networks need to be trained with sim ilar data to the ones they are going to predict or intend to model. Furthermore, depending on the type of data, its characteristics, dataset size, and other factors, one architecture could be more convenient than another. Usually, the cost of ad equate training consumes too much time and resources that are not available to most organizations. The release of new models based on neural networks, which have been trained with huge amounts of data, allows transferring their knowledge to dierent tasks and areas. In this paper, the goal is to classify images and text from social media to allow obtaining specic and useful information in dierent areas of knowledge of social sciences such as economics or sociology.arXiv:2107.03751v1  [cs.CV]  8 Jul 20212 Lucas et al. In this work, we will rst use CLIP (Contrastive LanguageImage PreTraining) [10] transformer to classify images of a dataset obtained from social networks containing picturetext pairs obtained with a pretrained neural network into 205 labels corresponding to places or scenes. We will then rene such classica tion with the associated texts to check and compare the obtained accuracy. For this purpose, we will use a samplingbased validation since the dataset used is unlabeled. The remainder of the paper is structured as follows: Section 2 shows related work; Section 3 outlines the pieces of the system that we will use in our exper iments; Section 4 describes the image classication work ow, rstly using only the images and secondly by rening the classication with the associated texts from the dataset; Section 5 shows the validation process and the results obtained; nally, in Section 6 we discuss our conclusions and suggest future work. 2 Related work "
68,Learning from Noisy Labels with Coarse-to-Fine Sample Credibility Modeling.txt,"Training deep neural network (DNN) with noisy labels is practically
challenging since inaccurate labels severely degrade the generalization ability
of DNN. Previous efforts tend to handle part or full data in a unified
denoising flow via identifying noisy data with a coarse small-loss criterion to
mitigate the interference from noisy labels, ignoring the fact that the
difficulties of noisy samples are different, thus a rigid and unified data
selection pipeline cannot tackle this problem well. In this paper, we first
propose a coarse-to-fine robust learning method called CREMA, to handle noisy
data in a divide-and-conquer manner. In coarse-level, clean and noisy sets are
firstly separated in terms of credibility in a statistical sense. Since it is
practically impossible to categorize all noisy samples correctly, we further
process them in a fine-grained manner via modeling the credibility of each
sample. Specifically, for the clean set, we deliberately design a memory-based
modulation scheme to dynamically adjust the contribution of each sample in
terms of its historical credibility sequence during training, thus alleviating
the effect from noisy samples incorrectly grouped into the clean set.
Meanwhile, for samples categorized into the noisy set, a selective label update
strategy is proposed to correct noisy labels while mitigating the problem of
correction error. Extensive experiments are conducted on benchmarks of
different modalities, including image classification (CIFAR, Clothing1M etc)
and text recognition (IMDB), with either synthetic or natural semantic noises,
demonstrating the superiority and generality of CREMA.","Deep learning has achieved significant progress in the recognition of multimedia signals (e.g. images, text, speeches). The key to its success is the availability of largescale datasets with reliable manual annotations. Collecting such datasets, however, is timeconsuming and expensive. Some alternative ways to obtain la beled data, such as web crawling [46], inevitably yield samples with noisy labels, which are not appropriate to be directly utilized to train DNN since these com plex models can easily overfitting (i.e., memorizing) noisy labels [2,50]. To handle this problem, classical Learning with Noisy Label (LNL) ap proaches focus on either identifying and dropping noisy samples (i.e., sample selection) [10,14,49,45] or adjusting the objective term of each sample during training (i.e., loss adjustment) [29,48,37]. The former usually make use of small loss trick to select clean samples, and then take them to update DNNs. However, the procedure of sample selection cannot guarantee that the selected clean sam ples are completely clean. In contrast, as indicated in Fig. 1, division relied on statistic metrics can still involve some hard noisy samples in the training set, which will be treated equally as other normal samples in the following training stages. Thus the negative impact brought by wrongly grouped noisy samples can still confuse the optimization process and lower the test performance of DNNs [49]. On the other hand, the latter schemes reweight loss values or update labels by estimating the confidence on how clean a sample is. Typical meth ods include loss correction via an estimated noise transition matrix [29,8,12]. However, estimating an accurate noise transition matrix is practically challeng ing. Recently, there are approaches directly correcting the labels of all training samples [37,48]. However, we empirically find that unconstrained label correc tion in full data can do harm to clean samples and reversely hinder the model performance.CREMA: Coarsetofine learning with noisy labels 3 Towards the problems above, we propose a simple but effective method called CREMA (Coarsetofine sample cREdibility Modeling and Adaptive loss reweighting ), which adaptively reduces the impact of noisy labels via modeling the credibility (i.e., quality) of each sample. In the coarselevel, with the esti mated sample credibility by simple statistic metrics, clean and noisy samples can be roughly separated and handled in a divideandconquer manner. Since it is practically impossible to separate these samples perfectly, for the selected clean samples, we take their historical credibility sequences to adjust the con tribution of each sample to their objective, thus mitigating the negative impact of hard noisy samples (i.e, noisy samples incorrectly grouped into the clean set) in a finegrained manner. As for the separated noisy samples, some of them are actually clean (i.e., hard clean samples) and can be helpful for model training. Thus instead of discarding them as previous sample selection methods [10,45], we make use of them via a selective label correction scheme. The insight behind CREMA is from the observation on the loss value during training on noisy data (illustrated in Fig 1), it can be found that clean and noisy samples manifest distinctive statistical properties during train ing, where clean samples yield relatively smaller loss value [33]and more consistent prediction . Hence these statistical features can be utilized to coarsely model the sample credibility. However, Fig 1 also shows that the full data can not be perfectly separated by simple statistical metrics. This inspires us to adaptively cope with noises of different difficulty levels with more finegrained design. For easily recognized noisy samples, we can directly apply certain label correction schemes while avoiding erroneous correction on normal samples. For samples that fall into the confusing area and hybrid with clean ones, since the coarsely estimated credibility in the current epoch is not informative enough to identify noisy samples, CREMA applies a finegrained likelihood estimator of noisy samples by resorting the historical sequence of sample credibility. This is achieved by maintaining a historical memory bank along with the training process and estimating the likelihood function through a consistency metric and assumption of markov property of the sequence. CREMA is built upon a classic cotraining framework [45,10]. The fine grained sample credibility estimated by one network is used to adjust the loss term of credible samples for the other network. Extensive experiments are con ducted on benchmarks of different modality, including image classification (CI FAR, MNIST, Clothing1M etc) and text recognition (IMDB), with either syn thetic or natural semantic noises, demonstrating the superiority and generality of the proposed method. In a nutshell, the key contributions of this paper include: ‚Ä¢CREMA : a novel LNL algorithm that combats noisy labels via coarsetofine sample credibility modeling. In coarselevel, clean and noisy sets are roughly sep arated and handled respectively, in the spirit of the idea of divideandconquer. Easily recognized noisy samples are handled via a selective label update strategy; ‚Ä¢InCREMA , likelihood estimation of historical credibility sequence is pro posed to help identify hard noisy samples, which naturally plays as the dynamical weight to modulate loss term of each training sample in a finegrained manner;4 Boshen Zhang et al. Model1 Model2Training  DataCredibility Guided  Loss Modulation Selective Label  Distribution LearningSample Credibility  Modeling Module1 Sample Credibility  Modeling Module2ùëøùëøùíÑùíÑùüèùüèùëøùëøùíñùíñùüèùüè ùëøùëøùíÑùíÑùüêùüêùëøùëøùíñùíñùüêùüê ‚Ä¢ùëøùëøùíÑùíÑ:Samples withùíÑùíÑredible label . ‚Ä¢ùëøùëøùíñùíñ:Samples requiring labelùêÆùêÆpdating .ùëæùëæùüèùüè ùëæùëæùüêùüê ‚Ä¢ùëæùëæ:Sample credibility ùíòùíòeight . Fig. 2: The pipeline of CREMA .CREMA trains two parallel networks simultaneously. Clean samples (mostly clean) Xcand noisy samples (mostly noisy) Xuare separated via estimating the credibility of each training data. A selective label distribution learning scheme is applied for easily distinguishable noisy samples in Xu. As for the clean set Xc, likelihood estimation of historical credibility sequence is proposed to handle the hard noisy samples via adaptively modulating their loss term during training. ‚Ä¢CREMA is evaluated on six synthetic and realworld noisy datasets with different modality, noise type and strength. Extensive ablation studies and qual itative analysis are provided to verify the effectiveness of each component. 2 Related Works "
69,A Semi-Supervised Two-Stage Approach to Learning from Noisy Labels.txt,"The recent success of deep neural networks is powered in part by large-scale
well-labeled training data. However, it is a daunting task to laboriously
annotate an ImageNet-like dateset. On the contrary, it is fairly convenient,
fast, and cheap to collect training images from the Web along with their noisy
labels. This signifies the need of alternative approaches to training deep
neural networks using such noisy labels. Existing methods tackling this problem
either try to identify and correct the wrong labels or reweigh the data terms
in the loss function according to the inferred noisy rates. Both strategies
inevitably incur errors for some of the data points. In this paper, we contend
that it is actually better to ignore the labels of some of the data points than
to keep them if the labels are incorrect, especially when the noisy rate is
high. After all, the wrong labels could mislead a neural network to a bad local
optimum. We suggest a two-stage framework for the learning from noisy labels.
In the first stage, we identify a small portion of images from the noisy
training set of which the labels are correct with a high probability. The noisy
labels of the other images are ignored. In the second stage, we train a deep
neural network in a semi-supervised manner. This framework effectively takes
advantage of the whole training set and yet only a portion of its labels that
are most likely correct. Experiments on three datasets verify the effectiveness
of our approach especially when the noisy rate is high.","With the recent development of deep neural networks, we have witnessed great advancements in visual recogni tion tasks such as image classiÔ¨Åcation [43, 51, 44, 46], object detection [7, 55, 45, 13], and semantic segmenta tion [29, 5, 15, 8]. Take the famed object recognition challenge ILSVRC [43] for instance, the InceptionResNet v2 [50] achieves a remarkable top5 accuracy of 95.3% in 2017. The success of the deep neural networks is powered in part by largescale welllabeled training data. However, noisyset modelfortestingrefinednoisysetcorrect/reweightfullysupervisedlearningnoisyset modelfortestingidentifysemisupervisedlearningunlabeledsetlabeledsetOurapproachPreviousapproachesFigure 1. Comparison between our framework and the one of ex isting methods for learning from noisy labels. Instead of trying to correct or reweigh all the labels of the noisy data, we ignore the ambiguous ones and train a deep neural network in a semi supervised fashion. it is actually a very daunting task to laboriously annotate an ImageNetlike training set. On the contrary, it is fairly convenient, fast, and cheap to collect training images from the Web along with their noisy labels. This signiÔ¨Åes the need of alternative approaches to train deep neural networks using such noisy labels. Indeed, the very Ô¨Årst source of training data is often from the Web when we face a new visual recognition task. Therefore, the methods that effectively learn from the noisy labels can sig niÔ¨Åcantly reduce the human labeling efforts, even to zero effort in some scenarios. There has been a rich line of recent works that aim to ad dress the problem of learning from noisy labels. We catego rize them to two groups: one directly learns from the noisy labels and the other relies on an extra set of clean data. For the former, a label cleansing module is often applied in or der to identify the correctly labeled data [4, 11, 3]. Alterna tively, one may model the noise to reweigh the data terms in the loss functions [28, 37, 35]. The performance of these algorithms heavily depend on the precision of the labelarXiv:1802.02679v3  [cs.CV]  21 Mar 2018cleansing or the estimated noisy rates. They perform well when the noise rates can be safely managed [30, 47, 35, 37], but could suffer from the ambiguity between mislabeled ex amples and ‚Äúhard cases‚Äù ‚Äî the data points whose labels are correct but hard to be captured by the neural networks‚Äô classiÔ¨Åcation boundary. For the second group of methods, an extra set of clean data is used to guide the learning agent through the noisy data. Li et al. [27] enforce the network trained from the noisy data to imitate the behavior of another network learned from the clean set. Vahdat [56] constructs an undi rected graphical model to represent the relationship between the clean and noisy data. Veit et al. [59] also use a secondary network to clean the labels of the noisy data such that the main network receives more accurate supervision than from the original training set. An absence of the clean set would prevent these methods from being applied to some situa tions. Besides, like the approaches in the Ô¨Årst group, they still aim to correct the labels of the noisy set and could make mistakes in this procedure. Despite their promising results, both groups of the exist ing methods come with a common caveat ‚Äî they attempt to correct the noisy labels or reweigh the terms of all the data points no matter how difÔ¨Åcult it is to do so. This inevitably incurs errors for some of the data points. In this paper, we contend that it is better to completely ignore the labels of some of the data points than to keep their wrong labels. Af ter all, the wrong labels could mislead the training proce dure of the network. We suggest a twostage framework for the learning from noisy labels. In the Ô¨Årst stage, we iden tify a small portion of data points from the noisy training set of which the labels are correct with a high probability. The noisy labels of the other data points are then removed. In the second stage, we train a deep neural network in a semisupervised manner. This framework effectively takes advantage of the whole training set and yet only a portion of its labels which is most likely correct. Figure 1 contrasts our framework to the one taken by most existing methods. It is worth noting that the Ô¨Årst stage of our framework can be implemented in a variety of ways. Many existing methods for learning from the noisy labels are actually ap plicable. This paper presents our preliminary study by a simple and efÔ¨Åcient selfreÔ¨Åning method for the Ô¨Årst stage and leaves the exploration of more sophisticated approaches to the future work. In particular, we rank all the data points within each class and then keep the labels of the top few. The ranking is performed by the multiway classiÔ¨Åcation neural network learned from the original training set when there is no clean set available, and by a binary classiÔ¨Åer of each class which is trained to differentiate the data of clean and noisy labels when the clean set is given. In the second stage, we apply the temporal ensembling [23] to train a deep neural network in the semisupervised manner. To the bestof our knowledge, this is the Ô¨Årst time that the temporal en sembling is tested on a large set of natural images ‚Äî around 1M images of which the resolutions are about 256x256. It is also worth noting that when there exists a small clean set in addition to the noisy training set, the semisupervised learning approach [25] has been considered a baseline in the experiments of [60]. In a sharp contrast to the observa tions of [38], we show that, under our twostage framework, semisupervised learning methods can actually give rise to stateoftheart results for the task of learning from noisy labels. The rest of this paper is organized as follows. Sec tion 2 discusses several related areas to our method. In Section 3, our twostage approach is described in details. Section 4 shows the experimental results of our approach in two diminutive datasets and a largescale real noisy dataset. Finally, we conclude the paper in Section 5. 2. Related work "
70,Learning to Learn from Noisy Labeled Data.txt,"Despite the success of deep neural networks (DNNs) in image classification
tasks, the human-level performance relies on massive training data with
high-quality manual annotations, which are expensive and time-consuming to
collect. There exist many inexpensive data sources on the web, but they tend to
contain inaccurate labels. Training on noisy labeled datasets causes
performance degradation because DNNs can easily overfit to the label noise. To
overcome this problem, we propose a noise-tolerant training algorithm, where a
meta-learning update is performed prior to conventional gradient update. The
proposed meta-learning method simulates actual training by generating synthetic
noisy labels, and train the model such that after one gradient update using
each set of synthetic noisy labels, the model does not overfit to the specific
noise. We conduct extensive experiments on the noisy CIFAR-10 dataset and the
Clothing1M dataset. The results demonstrate the advantageous performance of the
proposed method compared to several state-of-the-art baselines.","One of the key reasons why deep neural networks (DNNs) have been so successful in image classiÔ¨Åcation is the collections of massive labeled datasets such as COCO [14] and ImageNet [20]. However, it is timeconsuming and expensive to collect such highquality manual annota tions. A single image often requires agreement from mul tiple annotators to reduce label error. On the other hand, there exist other less expensive sources to collect labeled data, such as search engines, social media websites, or re ducing the number of annotators per image. However, those lowcost approaches introduce lowquality annotations with label noise . Many studies have shown that label noise can signiÔ¨Åcantly affect the accuracy of the learned classi Ô¨Åers [2, 23, 32]. In this work, we address the following problem: how to effectively train on noisy labeled datasets? Some methods learn with label noise by relying on hu man supervision to verify seed images [11, 29] or estimatelabel confusion [16, 31]. However, those methods exhibit a disadvantage in scalability for large datasets. On the other hand, methods without human supervision ( e.g. label cor rection [18, 24] and noise correction layers [5, 23]) are scalable but less effective and more heuristic. In this work we propose a metalearning based noisetolerant (MLNT) training to learn from noisy labeled data without human su pervision or access to any clean labels. Rather than design ing a speciÔ¨Åc model, we propose a modelagnostic training algorithm, which is applicable to any model that is trained with gradientbased learning rule. The prominent issue in training DNNs on noisy labeled data is that DNNs often overÔ¨Åt to the noise, which leads to performance degradation. Our method addresses this issue by optimizing for a model‚Äôs parameters that are less prone to overÔ¨Åtting and more robust against label noise. SpeciÔ¨Å cally, for each minibatch, we propose a metaobjective to train the model, such that after the model goes through con ventional gradient update, it does not overÔ¨Åt to the label noise. The proposed metaobjective encourages the model to produce consistent predictions after it is trained on a vari ety of synthetic noisy labels. The key idea of our method is: a noisetolerant model should be able to consistently learn the underlying knowledge from data despite different label noise . The main contribution of this work are as follows. We propose a noisetolerant training algorithm, where a metaobjective is optimized before conventional training. Our method can be theoretically applied to any model trained with gradientbased rule. We aim to optimize for a model that does not overÔ¨Åt to a wide spectrum of artiÔ¨Åcially generated label noise. We formulate our metaobjective as: train the model such that after it learns from various synthetic noisy labels using gradient update, the updated models give consistent predictions with a teacher model. We adapt a selfensembling method to construct the teacher model, which gives more reliable predictions unaf fected by the synthetic noise. We perform experiments on two datasets with syn thetic and realworld label noise, and demonstrate the 1arXiv:1812.05214v2  [cs.LG]  12 Apr 2019advantageous performance of the proposed method in image classiÔ¨Åcation tasks compared to stateoftheart methods. In addition, we conduct extensive ablation study to examine different components of the proposed method. Our code is publicly available1. 2. Related Work "
71,Weakly Supervised Learning with Side Information for Noisy Labeled Images.txt,"In many real-world datasets, like WebVision, the performance of DNN based
classifier is often limited by the noisy labeled data. To tackle this problem,
some image related side information, such as captions and tags, often reveal
underlying relationships across images. In this paper, we present an efficient
weakly supervised learning by using a Side Information Network (SINet), which
aims to effectively carry out a large scale classification with severely noisy
labels. The proposed SINet consists of a visual prototype module and a noise
weighting module. The visual prototype module is designed to generate a compact
representation for each category by introducing the side information. The noise
weighting module aims to estimate the correctness of each noisy image and
produce a confidence score for image ranking during the training procedure. The
propsed SINet can largely alleviate the negative impact of noisy image labels,
and is beneficial to train a high performance CNN based classifier. Besides, we
released a fine-grained product dataset called AliProducts, which contains more
than 2.5 million noisy web images crawled from the internet by using queries
generated from 50,000 fine-grained semantic classes. Extensive experiments on
several popular benchmarks (i.e. Webvision, ImageNet and Clothing-1M) and our
proposed AliProducts achieve state-of-the-art performance. The SINet has won
the first place in the classification task on WebVision Challenge 2019, and
outperformed other competitors by a large margin.","In recent years, the computer vision community has witnessed the signicant success of Deep Neural Networks (DNNs) on several benchmark datasets of image classication, such as ImageNet [1] and MSCOCO [22]. However, obtaining largescale data with clean and reliable labels is expensive and timeconsuming. When noisy labels are introduced in training data, it is widely known that the performance of a deep model can be signicantly degraded [2,36,3,23], which prevents deep models from being quickly employed in realworld noisy scenarios.arXiv:2008.11586v2  [cs.CV]  4 Sep 20202 L. Cheng et al. A common solution is to collect a large amount of image related side infor mation (e.g. surrounding texts, tags and descriptions) from the internet, and directly take them as the groundtruth for model training. Though this solution is more ecient than manual annotation, the obtained labels usually contain noise due to the heterogeneous sources. Therefore, improving the robustness of deep learning models against noisy labels has become a critical issue. To estimate the noise in labels, some works propose new layers [26,27] or loss functions [28,29,30,18] to correct the noisy label during training. However, these works rely on a strict assumption that there is a single transition prob ability between the noisy labels and the groundtruth labels. Owning to this assumption, these methods may show good performance on handcrafted noisy datasets but are inecient on real noisy datasets such as Clothing1M [36]. In some situations, it is possible to annotate a small fraction of training samples as additional supervision. By using additional supervision, works like [31,11,32] could improve the robustness of deep networks against label noises. But still, the requirement on clean samples make them less  exible to apply in large scale realworld scenarios. Many data cleaning algorithms [33,34,35] are developed to discard those sam ples with wrong label ahead of the training procedure. The major diculty of these algorithms is how to distinguish informative hard samples from harmful mislabeled ones. CleanNet [11] achieves stateoftheart performance on the real world noisy dataset Clothing1M [36]. CleanNet generates a single representative sample (class prototype) for each class and uses it to estimate the correctness of sample labels. With the observation that samples have widespread distribution in noisy classes, SMP [20] takes multiple prototypes to represent a noisy class instead of single prototype in CleanNet. In both CleanNet and SMP, extra clean supervision is required to train models. In most of previous works, image related side information or annotations (e.g. titles and tags) from web are commonly regarded as noisy labels. These works may not fully take advantage of the side information. Based on our observations, these image related side information reveal underlying similarity among images and classes, which has great potential to help tackle label noises. By analyzing the label structure and text descriptions, we explore an weaklysupervised learning strategy to deal with noisy samples. For example, the label \apple"" may refer to a fruit or an Apple mobile phone. When acquiring images from web using the label \apple"", images of apple fruits and Apple mobile phones will be wrongly put under a same class. Fortunately, titles or text descriptions about the images could imply the misplacement. In this paper, we propose an ecient weakly supervised learning strategy to evaluate the correctness of each image sample in each class by exploiting the label structure and label descriptions. Moreover, we release a large scale negrained product dataset to facilitate further research on visual recognition. To our knowledge, the proposed product dataset contains the largest number of product categories by now. Our contributions in this paper are summarized as follows:Weakly Supervised Learning with Side Information for Noisy Labeled Images 3 phalarope horseman candie d    apple tulipa  gesnerianacorrect labels noisy labels Fig. 1. Images of WebVision 2019 dataset [37] from the categories of phalarope, horse man, candied apple, tulipa, gesneriana . The dataset was collected from the Internet by textual queries generated from 5, 000 semantic concepts in WordNet. Obviously, each category includes a lot of noisy images as shown above. 1) A weakly supervised learning with side information network (SINet) is proposed for noisy labeled image classication. SINet infers the relationship be tween images and labels without any human annotation, and enable us to train highperformance and robust CNN models against large scale label noises. 2) A noisy and negrained product dataset called AliProducts is released, which contains more than 2 :5 million web images crawled from the Internet by using queries generated from the 50 ;000 negrained semantic classes. In addition, side information (e.g., hierarchical relationships between classes) are also provided for the convenience of extending research. 3) Extensive experiments are conducted on a number of benchmarks, includ ing WebVision, ImageNet, Clothing1M and AliProducts, in which the proposed SINet obtains the stateoftheart performance. Our SINet also won the rst place on the WebVision Challenge 2019, and outperforms the other competitors by a large margin. 2 Related Work "
72,Locally Adaptive Label Smoothing for Predictive Churn.txt,"Training modern neural networks is an inherently noisy process that can lead
to high \emph{prediction churn} -- disagreements between re-trainings of the
same model due to factors such as randomization in the parameter initialization
and mini-batches -- even when the trained models all attain similar accuracies.
Such prediction churn can be very undesirable in practice. In this paper, we
present several baselines for reducing churn and show that training on soft
labels obtained by adaptively smoothing each example's label based on the
example's neighboring labels often outperforms the baselines on churn while
improving accuracy on a variety of benchmark classification tasks and model
architectures.","Deep neural networks (DNNs) have proved to be im mensely successful at solving complex classiÔ¨Åcation tasks across a range of problems. Much of the effort has been spent towards improving their predictive performance (i.e. accuracy), while comparatively little has been done to wards improving the stability of training these models (Zheng et al., 2016). Modern DNN training is inherently noisy due to factors such as the random initialization of network parameters (Glorot & Bengio, 2010), the mini batch ordering (Loshchilov & Hutter, 2015), the effects of various data augmentation (Shorten & Khoshgoftaar, 2019) or preprocessing tricks (Santurkar et al., 2018), and the nondeterminism arising from the hardware (Turner & Nowotny, 2015), all of which are exacerbated by the non convexity of the loss surface (Scardapane & Wang, 2017). This results in local optima corresponding to models that have very different predictions on the same data points. This may seem counterintuitive, but even when the differ ent runs all produce very high accuracies for the classiÔ¨Åca tion task, their predictions can still differ quite drastically 1Google Research, Mountain View, USA. Correspondence to: Dara Bahri <dbahri@google.com >. Proceedings of the 38thInternational Conference on Machine Learning , PMLR 139, 2021. Copyright 2021 by the author(s).as we will show later in the experiments. Thus, even an optimized training procedure can lead to high prediction churn , which refers to the proportion of samplelevel dis agreements between classiÔ¨Åers caused by different runs of the same training procedure1. In practice, reducing such predictive churn can be critical. For example, in a production system, models are often con tinuously improved on by being trained or retrained with new data or better model architectures and training pro cedures. In such scenarios, a candidate model for release must be compared to the current model serving in produc tion. Oftentimes, this decision is conditioned on more than just overall ofÔ¨Çine test accuracy‚Äì in fact, the ofÔ¨Çine metrics are often not completely aligned with the actual goal, es pecially if these models are used as part of a larger system (e.g. maximizing ofÔ¨Çine clickthrough rate vs. maximiz ing revenue or user satisfaction) (Deng et al., 2013; Beel et al., 2013; Dmitriev & Wu, 2016). As a result, these com parisons require extensive and costly live experiments, re quiring human evaluation in situations where the candidate and the production model disagree (i.e. in many situations, the true labels are not available without a manual labeler) (Theocharous et al., 2015; Deng, 2015; Deng & Shi, 2016). In these cases, it can be highly desirable to lower predictive churn. Despite the practical relevance of lowering churn, there has been surprisingly little work done in this area, which we highlight in the related work section. In this work, we fo cus on predictive churn reduction under retraining the same model architecture on an identical train and test set. Our main contributions are as follows: ‚Ä¢ We provide one of the Ô¨Årst comprehensive analyses of baselines to lower prediction churn, showing that pop ular approaches designed for other goals are effective baselines for churn reduction, even compared to meth ods designed for this goal. ‚Ä¢ We improve label smoothing, a global smoothing method popular for calibrating model conÔ¨Ådence, by utilizing the local information leveraged by the k 1Concretely, given two classiÔ¨Åers applied to the same test sam ples, the prediction churn between them is the fraction of test sam ples with different predicted labels.arXiv:2102.05140v2  [cs.LG]  11 Jun 2021Locally Adaptive Label Smoothing for Predictive Churn NN labels thus introducing a locally adaptive label smoothing which we show to often outperform the baselines on a wide range of benchmark datasets and model architectures. ‚Ä¢ We show new theoretical results for the kNN labels suggesting the usefulness of the kNN label. We show under mild nonparametric assumptions that for a wide range ofk, thekNN labels uniformly approximates the optimal soft label and when kis tuned optimally, achieves the minimax optimal rate. We also show that whenkis linear inn, the distribution implied by thekNN label approximates the original distribution smoothed with an adaptive kernel. 2. Related Works "
73,Robust Learning by Self-Transition for Handling Noisy Labels.txt,"Real-world data inevitably contains noisy labels, which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus, to counter the label noise challenge, we propose a novel
self-transitional learning method called MORPH, which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase, the network is updated using all the samples to collect a seed
of clean samples. Then, in the evolution phase, the network is updated using
only the set of arguably clean samples, which precisely keeps expanding by the
updated network. Thus, MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.","In supervised learning for data analysis tasks, deep neural net works (DNNs) have become one of the most popular methods in that traditional machine learning is successfully superseded by recent deep learning in numerous applications [ 13,37]. However, their success is conditioned on the availability of massive data with carefully annotated human labels, which are expensive and timeconsuming to obtain in practice. Some substitutable sources, ‚àóJaeGil Lee is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ‚Äô21, August 14‚Äì18, 2021, Virtual Event, Singapore ¬©2021 Association for Computing Machinery. ACM ISBN 9781450383325/21/08. . . $15.00 https://doi.org/10.1145/3447548.3467222such as Amazon‚Äôs Mechanical Turk and surrounding tags of col lected data, have been widely used to mitigate the high labeling cost, but they often yield samples with noisy labels that may not be true [ 35]. In addition, data labels can be extremely complex even for experts [ 7] and adversarially manipulated by a labelflipping attack [41], thereby being vulnerable to label noise. Modern DNNs are typically trained in an overparameterized regime where the number of the parameters of a DNN far exceeds the size of the training data [ 19]. In principle, such DNNs have the capacity to overfit to any given set of labels, even though part of the lables are significantly corrupted. In the presence of noisy labels, DNNs easily overfit to the entire training data regardless of the ratio of noisy labels, eventually resulting in poor generalization on test data [ 44]. Thus, in this paper, we address an important issue of ‚Äúlearning from noisy labels.‚Äù One of the most common approaches is sample selection , which involves training a DNN for a possibly clean subset of noisy training data [ 5,9,11,12,31,34]. Typically, in each training iteration, a certain number of smallloss training samples are selected as clean ones and subsequently used to robustly update the DNN. This small loss trick is satisfactorily justified by the memorization effect [2] that DNNs tend to first learn simple and generalized patterns and then gradually memorize all the patterns including irregular ones such as outliers and falselabeled samples. Although this family of methods has achieved better general ization by training with selected smallloss training samples, they commonly have the following twoproblems: 1.Discarding Hard Sample : Truelabeled samples with large losses are simply discarded by the smallloss trick, though they have a great impact on generalization [ 4]. This issue can be exac erbated by realworld and asymmetric label noises, where the loss distributions of true and falselabeled samples are overlapped closely [35]. 2.Inefficient Learning : The smallloss trick suffers from confir mation bias [ 38], which is a hazard of favoring the samples se lected at the beginning of training. Hence, recent approaches of ten leverage multiple DNNs to cooperate with one another [ 9,42] or run multiple training rounds to iteratively refine their selected set of clean samples [ 31,33], thus adding heavy computational overhead. In this regard, we have thoroughly investigated the memoriza tion of a DNN on realworld noisy training samples and, conse quently, observed the existence of twolearning periods in Figure 1(a):(i)the ‚Äúnoiserobust‚Äù period where the memorization of false labeled samples is insignificant because the DNN prefers memo rizing easy samples at an early stage and (ii)the ‚Äúnoiseprone‚Äù period where the memorization of falselabeled samples rapidly increases because the DNN eventually begins memorizing all the noisy samples at a late stage of training.arXiv:2012.04337v2  [cs.LG]  7 Jun 2021False Labeled Sample Default MORPH Generalization Improvement 0 40 Epochs 0 40 0 120 40 80 EpochsTrueLabeled Sample Noise Prone Noise Robust Evolution Seeding 120 80 Epochs0%25%50%75%100%Memorization Ratio 120 800%25%50%75%100%Memorization Ratio 50%60%70%80%Test ErrorTransition PointNoise Prone Noise Robust(a) Default. (b) MORPH. (c) Test Error Convergence. Figure 1: Key idea of MORPH: (a) and (b) show the memorization ratio when training a WideResNet168 on a subset of FOOD101N1with the realworld noise of 18.4%, where the memorization ratio is the number of memorized (see Definition 3.1) true or falselabeled samples to the total number of true or falselabeled training samples at each epoch. ‚ÄúDefault‚Äù is a standard training method, and ‚ÄúMORPH‚Äù is our proposed one; (c) contrasts the convergence of their test error. These findings motivate us to come up with an approach that leverages the transitional memorization nature in a single train ing round. In this paper, we propose MORPH , which is a self transitional learning approach that automatically transitions its learning phase when a DNN enters the ‚Äúnoiseprone‚Äù period after the ‚Äúnoiserobust‚Äù period (i.e., the dashed line in Figure 1(b)). Thus, corresponding to these two periods, our key idea divides the train ing process into two learning phases, namely seeding andevolution : 1.Seeding : Owing to the negligible memorization of falselabeled samples, the model update is initiated using allthe training samples in the noiserobust period. As the samples memorized at this time are mostly easy samples with true labels, they are accumulated as a clean seed to derive a maximal safe set in the next phase. Note that MORPH automatically estimates the best phase transition point without anysupervision. 2.Evolution : Without memorizing falselabeled samples, the DNN evolves by being updated only for the maximal safe set in the noiseprone period. Then, the updated DNN recognizes more truelabeled samples previously hard to distinguish and filters out falselabeled samples incorrectly included. This alternating process repeats per iteration so that the maximal safe set is ex panded and refined in the remaining noiseprone period. Through selftransitional learning, MORPH avoids the confirma tion bias by exploiting the noiserobust period with all the training samples, thus eliminating the need for additional DNNs and training rounds. In addition, it incrementally expands the clean seed towards the maximal safe set, which can cover even hard truelabeled sam ples, not just throwing them away. The alternating process in the second phase minimizes the risk of misclassifying falselabeled sam ples as truelabeled ones or vice versa. Hence, as shown in Figure 1(b), MORPH prevents the memorization of falselabeled samples by training with the maximal safe set during the noiseprone period, while increasing the memorization of truelabeled samples. As a result, as shown in Figure 1(c), the generalization performance of a DNN improves remarkably even in realworld noise. Our main contributions are summarized as follows: ‚Ä¢No Supervision for Transition : MORPH performs self transitional learning without any supervision such as a true noise rate and a clean validation set, which are usually hard to acquire in realworld scenarios. 1We used the subset in which correct labels are identified.‚Ä¢Noise Robustness : Compared with stateoftheart methods, MORPH identifies truelabeled samples from noisy data with much higher recall and precision. Thus, MORPH improved the test (or validation) error by up to 27.0ùëùùëù2for three datasets with two synthetic noises and by up to 8.90ùëùùëùand3.85ùëùùëùfor WebVi sion 1.0 and FOOD101N with realworld noise. ‚Ä¢Learning Efficiency : Differently from other methods, MORPH requires neither additional DNNs nor training rounds. Thus, it was significantly faster than others by up to 3.08times. 2 RELATED WORK "
74,Convolutional Neural Networks for User Identificationbased on Motion Sensors Represented as Image.txt,"In this paper, we propose a deep learning approach for smartphone user
identification based on analyzing motion signals recorded by the accelerometer
and the gyroscope, during a single tap gesture performed by the user on the
screen. We transform the discrete 3-axis signals from the motion sensors into a
gray-scale image representation which is provided as input to a convolutional
neural network (CNN) that is pre-trained for multi-class user classification.
In the pre-training stage, we benefit from different users and multiple samples
per user. After pre-training, we use our CNN as feature extractor, generating
an embedding associated to each single tap on the screen. The resulting
embeddings are used to train a Support Vector Machines (SVM) model in a
few-shot user identification setting, i.e. requiring only 20 taps on the screen
during the registration phase. We compare our identification system based on
CNN features with two baseline systems, one that employs handcrafted features
and another that employs recurrent neural network (RNN) features. All systems
are based on the same classifier, namely SVM. To pre-train the CNN and the RNN
models for multi-class user classification, we use a different set of users
than the set used for few-shot user identification, ensuring a realistic
scenario. The empirical results demonstrate that our CNN model yields a top
accuracy of 89.75% in multi-class user classification and a top accuracy of
96.72% in few-shot user identification. In conclusion, we believe that our
system is ready for practical use, having a better generalization capacity than
both baselines.","Nowadays, common mobile device authentication mechanisms such as PINs, graphical passwords and ngerprint scans oer limited security. These mechanisms are susceptible to guessing (or spoong in the case of ngerprint scans) and to side channel attacks [1] such as smudge [2], re ection [3, 4] and video capture attacks [5{7]. On top of this, a fundamental limitation of PINs, passwords, and ngerprint scans is that these mechanisms require explicit user interaction. Due to the world wide adoption of mobile devices and the advancement of technologies, mobile devices are now equipped with multiple sensors such as accelerometers, gyroscopes, magnetometers, among others. The data recorded by these sensors during the interaction of the user with the mobile device can be used as biometric data to identify the user. Indeed, onetime or continuous user identication based on the data collected by the motion sensors of a mobile device is an actively studied task [8{22], that emerged after the integration of motion sensors into commonly used mobile devices. In this paper, we propose a novel deep learning approach that can identify the user from a single tap on smart phone's touchscreen, using the discrete signals recorded by the accelerometer and the gyroscope during the tap gesture. By minimizing the user's interaction during verication and by removing the requirement to explicitly insert PINs, graphical passwords or scan ngerprints, we eliminate many of the enumerated attacks. Our approach is based on transforming the discrete 3axis signals from the accelerometer and the gyroscope into a grayscale image representation that can be provided as input for deep convolutional neural networks (CNNs) [23,24]. Our image representation is based on repeating the six onedimensional (1D) signals using a modied version of de Brujin sequences [25], such that the 3 3 convolutional lters from the rst layer of the CNN get to \see"" every possible tuple of three 1D signals in their receptive eld. After transforming the motion signals accordingly, we pretrain several CNN architectures in a multiclass user classication setting. In the pretraining stage, we can leverage the use of data from multiple users and multiple samples per user. After pretraining and selecting the bestperforming CNN, we can employ the selected CNN as a deep feature extractor that generates useful embeddings for each tap gesture. The generated embeddings can then be used to train a lightweight model, e.g. Support Vector Machines (SVM) [26], to identify the user in a fewshot learning setting. We consider a fewshot learning setting 1arXiv:1912.03760v2  [cs.LG]  23 Mar 2020with 20 samples per user in order to enable a fast registration process (20 taps on the screen are enough), similar in terms of time to the registration processes used by standard ngerprint or face authentication systems. We conduct experiments in order to compare our user identication system based on CNN features with two baselines, one that is based on handcrafted features [18] and one that is based on recurrent neural network (RNN) features [17]. All models are evaluated in a fewshot user identication context using the same classier, namely SVM. Our CNN model (as well as the baseline RNN model) is pretrained on a multiclass user classication task. The users involved in the multiclass user classication experiment are dierent from those involved in the user identication experiment, to simulate a realistic scenario. In order to conduct our experiments, we modify the HMOG data set [20] by extracting shorter signals from the original sessions and by splitting the users in half, using the rst half for the preliminary multiclass user classication experiment and the second half for the user identication experiment. Our SVM based on CNN features proves a higher generalization capacity, surpassing both baselines in the user identication experiments. Moreover, according to McNemar's statistical testing [27] performed at a condence level of 0:01, our improvements over the baselines are signicant. With an accuracy of 96.72%, our SVM based on CNN features seems to be a viable solution for practical usage. In summary, our contribution is threefold: ‚Ä¢We propose a novel grayscale image representation of the discrete signals, designed specically to be useful as input for CNNs. ‚Ä¢We propose to pretrain CNNs on a multiclass user classication task in order to obtain useful embeddings for fewshot user identication. ‚Ä¢We perform comparative experiments showing that our method based on CNN embeddings surpasses both machine learning methods based on handcrafted features and deep learning methods based on RNN embeddings. The rest of this paper is organized as follows. In Section 2, we provide an overview of user identication systems for mobile devices, focusing mainly on systems based on analyzing motion sensors. In Section 3, we present the proposed data representation, our CNN architectures and our user identication model. In Section 4, we present the data set, the evaluation metrics and the performed experiments. In Section 5, we conclude our ndings and propose some future directions of study. 2 Related Work "
75,Normalized Loss Functions for Deep Learning with Noisy Labels.txt,"Robust loss functions are essential for training accurate deep neural
networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown
that the commonly used Cross Entropy (CE) loss is not robust to noisy labels.
Whilst new loss functions have been designed, they are only partially robust.
In this paper, we theoretically show by applying a simple normalization that:
any loss can be made robust to noisy labels. However, in practice, simply being
robust is not sufficient for a loss function to train accurate DNNs. By
investigating several robust loss functions, we find that they suffer from a
problem of underfitting. To address this, we propose a framework to build
robust loss functions called Active Passive Loss (APL). APL combines two robust
loss functions that mutually boost each other. Experiments on benchmark
datasets demonstrate that the family of new loss functions created by our APL
framework can consistently outperform state-of-the-art methods by large
margins, especially under large noise rates such as 60% or 80% incorrect
labels.","Training accurate deep neural networks (DNNs) in the pres ence of noisy (incorrect) labels is of great practical impor tance. Different approaches have been proposed for robust learning with noisy labels. This includes 1) label correction methods that aim to identify and correct wrong labels (Xiao et al., 2015; Vahdat, 2017; Veit et al., 2017; Li et al., 2017b); 2) loss correction methods that correct the loss function based on an estimated noise transition matrix (Sukhbaatar et al., 2014; Reed et al., 2014; Patrini et al., 2017; Han et al., 2018a); 3) reÔ¨Åned training strategies that modify the train ing procedure to be more adaptive to incorrect labels (Jiang et al., 2018; Wang et al., 2018; Tanaka et al., 2018; Ma et al., *Equal contribution1The University of Melbourne, Australia 2Shanghai Jiao Tong University, China. Correspondence to: Yisen Wang<eewangyisen@gmail.com >. Proceedings of the 37thInternational Conference on Machine Learning , Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).2018; Han et al., 2018b); and 4) robust loss functions that are inherently tolerant to noisy labels (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019c). Compared to the Ô¨Årst three approaches that may suffer from inaccurate noise estimation or involve sophisticated training procedure modiÔ¨Åcations, robust loss functions provide a simpler solu tion, which is also the main focus of this paper. It has been theoretically shown that some loss functions such as Mean Absolute Error (MAE) are robust to label noise, while others are not, which unfortunately includes the commonly used Cross Entropy (CE) loss. This has mo tivated a body of work to design new loss functions that are inherently robust to noisy labels. For example, Gen eralized Cross Entropy (GCE) (Zhang & Sabuncu, 2018) was proposed to improve the robustness of CE against noisy labels. GCE can be seen as a generalized mixture of CE and MAE, and is only robust when reduced to the MAE loss. Recently, a Symmetric Cross Entropy (SCE) (Wang et al., 2019c) loss was suggested as a robustly boosted version of CE. SCE combines the CE loss with a Reverse Cross En tropy (RCE) loss, and only the RCE term is robust. Whilst these loss functions have demonstrated improved robustness, theoretically, they are only partially robust to noisy labels. Different from previous works, in this paper, we theoreti cally show that any loss can be made robust to noisy labels, and all is needed is a simple normalization. However, in practice, simply being robust is not enough for a loss func tion to train accurate DNNs. By investigating several robust loss functions, we Ô¨Ånd that they all suffer from an underÔ¨Åt ting problem. Inspired by recent developments in this Ô¨Åeld, we propose to characterize existing loss functions into two types: 1) ‚ÄúActive‚Äù loss, which only explicitly maximizes the probability of being in the labeled class, and 2) ‚ÄúPassive‚Äù loss, which also explicitly minimizes the probabilities of being in other classes. Based on this characterization, we further propose a novel framework to build a new set of robust loss functions called Active Passive Losses (APLs). We show that under this framework, existing loss functions can be reworked to achieve the stateoftheart for training DNNs with noisy labels. Our key contributions are: We provide new theoretical insights into robust loss func tions demonstrating that a simple normalization can make any loss function robust to noisy labels.arXiv:2006.13554v1  [cs.LG]  24 Jun 2020Normalized Loss Functions for Deep Learning with Noisy Labels We identify that existing robust loss functions suffer from an underÔ¨Åtting problem. To address this, we propose a generic framework Active Passive Loss (APL) to build new loss functions with theoretically guaranteed robust ness and sufÔ¨Åcient learning properties. We empirically demonstrate that the family of new loss functions created following our APL framework can out perform the stateoftheart methods by considerable mar gins, especially under large noise rates of 60% or 80%. 2. Related Work "
76,Learning Scene Flow in 3D Point Clouds with Noisy Pseudo Labels.txt,"We propose a novel scene flow method that captures 3D motions from point
clouds without relying on ground-truth scene flow annotations. Due to the
irregularity and sparsity of point clouds, it is expensive and time-consuming
to acquire ground-truth scene flow annotations. Some state-of-the-art
approaches train scene flow networks in a self-supervised learning manner via
approximating pseudo scene flow labels from point clouds. However, these
methods fail to achieve the performance level of fully supervised methods, due
to the limitations of point cloud such as sparsity and lacking color
information. To provide an alternative, we propose a novel approach that
utilizes monocular RGB images and point clouds to generate pseudo scene flow
labels for training scene flow networks. Our pseudo label generation module
infers pseudo scene labels for point clouds by jointly leveraging rich
appearance information in monocular images and geometric information of point
clouds. To further reduce the negative effect of noisy pseudo labels on the
training, we propose a noisy-label-aware training scheme by exploiting the
geometric relations of points. Experiment results show that our method not only
outperforms state-of-the-art self-supervised approaches, but also outperforms
some supervised approaches that use accurate ground-truth flows.","Scene Ô¨Çow estimation is to capture 3D motions of dy namic scenes, which is important for many applications such as robotics and autonomous driving. Recently, directly estimating scene Ô¨Çow from point clouds has received in creasing attention. Nevertheless, it is challenging to esti mate scene Ô¨Çow from point clouds, due to the sparsity and nonuniform density of point clouds. Typical approaches [11, 35, 47, 50] estimate scene Ô¨Çow from point clouds by training neural networks in a fully su pervised manner, which relies on groundtruth scene Ô¨Çow annotation. However, it is expensive and timeconsuming to acquire groundtruth scene Ô¨Çows for realworld point clouds, since such annotations usually need to annotate 3D Figure 1. Illustration of our main idea . We leverage multi modality data ( i.e. monocular RGB images Iand point clouds P) to generate pseudo scene Ô¨Çow labels for point clouds, such that the reliance on groundtruth scene Ô¨Çow is circumvented. With the pseudo labels, our method trains a scene Ô¨Çow network to estimate scene Ô¨Çow from point clouds. motions for every point of a point cloud. To alleviate this is sue, researchers resort to training the scene Ô¨Çow network on labeled synthetic data. However, these methods are limited in the effectiveness and generalization ability in realworld applications due to the domain gap between the synthetic data and realworld data. Alternatively, some selfsupervised methods [21, 24, 31, 52] train the scene Ô¨Çow network by constructing pseudo scene Ô¨Çow labels from point clouds. For example, Mittal et al. [31] approximated pseudo scene Ô¨Çow labels based on the coordinate differences of 3D points, where the closest points to the next point cloud are treated as pseudo cor respondence. These methods circumvent the reliance on groundtruth scene Ô¨Çows. However, they fail to achieve competitive performance compared with fully supervised approaches. To achieve competitive performance without the need for groundtruth scene Ô¨Çow, we seek to generate highquality pseudo scene Ô¨Çow labels for training scene Ô¨Çow networks. However, it is nontrivial to establish highquality pseudo Ô¨Çow labels from the point cloud itself since a raw point cloud consists of only sparse point coordinates. Can we jointly leverage multimodality data ( i.e. monocular RGB images and point clouds) to generate pseudo scene Ô¨Çow la bels for point clouds ? (see Fig. 1). Different from pointarXiv:2203.12655v1  [cs.CV]  23 Mar 2022clouds, monocular images contain rich information such as object appearance and detailed texture information. Such rich information provides discriminative cues for estimating 2D motions, which can be used to facilitate pseudo scene Ô¨Çow label generation. The training dataset only needs to additionally provide RGB images captured by an afford able monocular camera, which is more accessible com pared with expensive scene Ô¨Çow annotation. Different from these methods [21, 24, 31, 52], our method leverages multi modality data for generating pseudo scene Ô¨Çow labels. However, it is challenging to generate pseudo scene Ô¨Çow labels. We can not directly infer 3D motion from the 2D motion only relying on monocular images, despite the rich information of monocular images. To address this issue, we propose a multimodalitybased pseudo scene Ô¨Çow genera tion module, through decomposing the 3D motion of a point into one 2D motion in the XY direction and another 1D motion in the Z direction. We thereby can leverage monoc ular images to capture the 2D motions of points in the im age plane and use point clouds to lift estimated 2D motions to pseudo scene Ô¨Çow labels ( i.e. 3D motions). With the generated pseudo labels, our method can train scene Ô¨Çow networks on point clouds without groundtruth scene Ô¨Çow. Another challenge is that our pseudo scene Ô¨Çow labels are inevitably noisy, compared with groundtruth ones, due to imperfect 2D motion estimation results. The noisy la bels would negatively affect the training of the scene Ô¨Çow network, leading to the degradation of estimation accuracy. To address this issue, we propose a noisylabelaware learn ing scheme for scene Ô¨Çow estimation. Our scheme exploits the geometric information of point clouds to detect inaccu rate labels in a soft manner. With the conÔ¨Ådence scores of pseudo labels, we construct a training loss that less relies on pseudo labels with lower conÔ¨Ådences when training scene Ô¨Çow networks. The main contributions of our work are summarized as follows: (1)We propose a novel method that trains the scene Ô¨Çow network without relying on groundtruth scene Ô¨Çow. (2)We show how to leverage multimodality data, i.e. point clouds and monocular images, for generating pseudo scene Ô¨Çow labels. (3)Our noisylabelaware learning scheme effectively trains the scene Ô¨Çow network by reducing the negative effect of the inherent noise of pseudo labels dur ing training. (4)Experiment results show that our method not only outperforms stateoftheart selfsupervised ap proaches but also outperforms some supervised approaches that use accurate groundtruth annotations, even though our pseudo labels are noisy and inferior. (5)The pseudo scene Ô¨Çow labels allow us to train the scene Ô¨Çow network on large scale realworld LiDAR data without groundtruth scene Ô¨Çow, reducing the domain gap.2. Related Work "
77,Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification.txt,"Employing clustering strategy to assign unlabeled target images with pseudo
labels has become a trend for person re-identification (re-ID) algorithms in
domain adaptation. A potential limitation of these clustering-based methods is
that they always tend to introduce noisy labels, which will undoubtedly hamper
the performance of our re-ID system. To handle this limitation, an intuitive
solution is to utilize collaborative training to purify the pseudo label
quality. However, there exists a challenge that the complementarity of two
networks, which inevitably share a high similarity, becomes weakened gradually
as training process goes on; worse still, these approaches typically ignore to
consider the self-discrepancy of intra-class relations. To address this issue,
in this paper, we propose a multiple co-teaching framework for domain adaptive
person re-ID, opening up a promising direction about self-discrepancy problem
under unsupervised condition. On top of that, a mean-teaching mechanism is
leveraged to enlarge the difference and discover more complementary features.
Comprehensive experiments conducted on several large-scale datasets show that
our method achieves competitive performance compared with the
state-of-the-arts.","Given a query image, person reidentiÔ¨Åcation (reID) aims to match the personofinterest across multiple nonoverlapped cameras distributed in different places. Encouraged by the remarkable success of deep learning methods and the availability of largescale datasets, reID research commu nity has achieved signiÔ¨Åcant progress during the past few years [Zheng et al. , 2016; Ye et al. , 2021 ]. However, as for pedestrian images from an unseen domain, even with a large diversity of training data, person reID model generally expe riences catastrophic performance drops because of the huge domain gaps or scene shifts, which cannot satisfy the need of application in real scenarios. To alleviate this problem, The corresponding author. person A  person Beasy hard  easy hard Figure 1: Illustration of the selfdiscrepancy of intraclass relations for UDA person reID tasks, which is caused by variations in pose, viewpoint and occlusion, etc. For each identity, some easy samples can be assigned with reliable pseudo labels. However, most of hard samples are always given with noisy pseudo labels. unsupervised domain adaptation (UDA) [Ganin and Lempit sky, 2015; Xiang et al. , 2020b; Saito et al. , 2018 ]is there fore proposed to employ the model trained on source dataset with identity labels to perform inference on the target domain. Nevertheless, it still remains an open research challenge in in dustry and academia due to the lack of identity annotations. Currently, there are two main categories of UDA meth ods in reID community. The Ô¨Årst category of imagelevel adaptation aims to eliminate the data distribution discrepancy across source and target domain, such as PTGAN [Wei et al., 2018 ]and SPGAN [Deng et al. , 2018 ]. Although these approaches achieve promising progress, their performance deeply relies on the images generation quality. The second category of clusteringbased adaptation [Song et al. , 2020; Fuet al. , 2019; Fan et al. , 2018 ]deploys clustering algo rithm to generate pseudolabels for unsupervised target im ages during training period. Unfortunately, their abilities are substantially hindered by the inevitable label noises caused by imperfect clustering algorithms. To alleviate this problem, some coteaching based reID approaches [Yang et al. , 2020; Geet al. , 2020; Zhao et al. , 2020; Zhai et al. , 2020 ]have been introduced for combating with noisy labels after clustering. Even though their optimal performance is often achieved by subnetwork‚Äôs discrimination ability, the selfdiscrepancy of intraclass relation (as shown in Figure 1) in target domain still remains unexplored. So a natural question then comes to our attention: how to leverage selfdiscrepancy features of multiple subnetwork, and then optically adapt them to un labelled domain, which has to be fully elaborated. Another challenge we observe is that, as the training process goes on,arXiv:2104.02265v5  [cs.CV]  7 Sep 2021two neural networks in traditional coteaching [Han et al. , 2018 ]tend to converge and unavoidably share a high simi larity, which weakens their complementarity and further im provement in terms of performance. To solve the challenges mentioned above, we propose a simple yet powerful Multiple Coteaching Network MCN that considerably explores the selfdiscrepancy of intraclass relation in target domain, consequently, person reID can be more effectively performed to resist with noisy labels in do main adaptation. In addition, we introduce a meanteaching mechanism to greatly enhance the complementarity and in dependence of collaborative networks, which, in turn, further improves the discriminability of learned representations in a progressive fashion. To the best of our knowledge, this is the Ô¨Årst research effort to exploit the potential of selfdiscrepancy among intraclass to address the UDA problem. Compared with existing coteaching based method [Geet al. , 2020; Zhao et al. , 2020; Zhai et al. , 2020 ], our MCN is different from them in terms of data input andmodel structure :(1) Our work proposes to adopt samples with different discrep ancy granularity ( T1Tn) as asymmetric inputs to multi ple networks, while previous methods applied same dataset as symmetric inputs during training; (2)MEBNet [Zhai et al., 2020 ]used DenseNet121 [Huang et al. , 2017 ], ResNet 50[Heet al. , 2016 ]and Inceptionv3 [Szegedy et al. , 2016 ]as backbone for enhancing the independence and complemen tary, [Geet al. , 2020; Zhao et al. , 2020 ]utilized random eras ing or random seeds for creating a difference, [Geet al. , 2020; Zhai et al. , 2020 ]also adopted symmetrical architecture with soft pseudo labels as well as hard pseudo labels in UDA reID tasks. In contrast, our MCN is only trained based on ResNet 50 with hard pseudo labels, which makes it more Ô¨Çexible and adaptable. In addition, our method can signiÔ¨Åcantly mine the selfdiscrepancy feature in target domain, and a novel mean teaching mechanism is also adopted to enhance the indepen dence and complementary between teacher network and stu dent networks, while previous asymmetric coteaching ap proach [Yang et al. , 2020 ]fails to meet these needs. In total, our contribution can be summarized as follows: 1. We propose a multiple coteaching network MCN to mine the selfdiscrepancy of intraclass relations in target do main for solving noisy labels. 2. A MeanTeaching mechanism is introduced to further enhance the output complementarity in a progressive manner based on proposed MCN method (‚ÄúMCNMT‚Äù for short). 3. Experimental results conducted on several benchmarks demonstrate the effectiveness of our proposed method. 2 Related works "
78,Context-based Virtual Adversarial Training for Text Classification with Noisy Labels.txt,"Deep neural networks (DNNs) have a high capacity to completely memorize noisy
labels given sufficient training time, and its memorization, unfortunately,
leads to performance degradation. Recently, virtual adversarial training (VAT)
attracts attention as it could further improve the generalization of DNNs in
semi-supervised learning. The driving force behind VAT is to prevent the models
from overfitting data points by enforcing consistency between the inputs and
the perturbed inputs. This strategy could be helpful in learning from noisy
labels if it prevents neural models from learning noisy samples while
encouraging the models to generalize clean samples. In this paper, we propose
context-based virtual adversarial training (ConVAT) to prevent a text
classifier from overfitting to noisy labels. Unlike the previous works, the
proposed method performs the adversarial training at the context level rather
than the inputs. It makes the classifier not only learn its label but also its
contextual neighbors, which alleviates the learning from noisy labels by
preserving contextual semantics on each data point. We conduct extensive
experiments on four text classification datasets with two types of label
noises. Comprehensive experimental results clearly show that the proposed
method works quite well even with extremely noisy settings.","Deep neural networks (DNNs) have shown human level performance in various domains, such as image classiÔ¨Åcation, machine translation, and speech recog nition. To achieve such an ability, it is indisputably evident that we have to collect a large amount of train ing data. As labeling such data is laborious and ex pensive, previous works utilize search engine (Blum et al., 2003; Li et al., 2017) or crowdsourcing (Yan et al., 2014; Yu et al., 2018) to collect labeled dataset. Un fortunately, these lowcost approaches introduce low quality annotations with label noise . It causes DNNs to completely memorize such label noises (i.e., nearly 100% training accuracy) (Zhang et al., 2016), deterio rating generalization capability (Fr ¬¥enay and Verleysen, 2013; Sukhbaatar et al., 2014). To deal with label noises, recent works primar ily propose loss correction approaches. These meth ods directly correct loss function (e.g., crossentropy, mean squared error) or the probabilities used to com pute it. For example, (Zhang et al., 2016) learns sam ple weighting scheme via an auxiliary network (called MentorNet ) and applies it to noisy labels such that cor rupt data could get nearly zero sample weights on the loss function. On the one hand, (Han et al., 2018) utilizes an intervention between two same networks with smallloss samples which are considered as clean. However, there exists only a single work to handle label noise on natural language. (Jindal et al., 2019) utilize a noise transition matrix for text classiÔ¨Åcation. This method is useful at text classiÔ¨Åcation on label noises, but it turned out that estimating real noise transition matrix is difÔ¨Åcult (Jiang et al., 2018; Han et al., 2018) *Equal contribution.especially when the number of classes is large. Recently, adversarial training at attracts attention as it could prevent networks from misclassifying an image that contains a small perturbation (e.g., adver sarial examples). The at enforces the classiÔ¨Åer to make consistent predictions on synthetic inputs that have small, approximately worstcase perturbations. (Miyato et al., 2015) extends the idea of at to the semisupervised regime by removing label dependency on perturbations, which is called a virtual adversarial training (V AT). Adversarial training indirectly have a label smoothing effect as it prevents models from pre dicting given labels with high conÔ¨Ådence by anisotropi cally smoothing around each data point. Several recent works have shown that label smoothing is effective as a means of coping with label noise (Lukasik et al., 2020) and preventing memorization (Xie et al., 2016). Based on such observations, we further study whether the ad versarial training methods, which is an indirect label smoothing method, are useful at training a robust clas siÔ¨Åer on label noise. In this paper, we propose a contextbased virtual ad versarial training (ConV AT) to build a robust text clas siÔ¨Åer on label noise. We inherently follow a fundamen tal strategy of V AT. Unlike the previous work (Miayto et al., 2016), ConV AT performs the adversarial training on the contextlevel feature space not the wordlevel. To that end, we solve minmax optimization by follow ing two steps: formulating perturbation andsmooth ing. In the Ô¨Årst phase, we calculate the worstcase per turbation into an adversarial direction that could max imize the classiÔ¨Åcation loss on the given samples. We then minimize the distributional distance between a normal sample and a perturbed sample to learn robust classiÔ¨Åer on adversarial perturbations. This strategy alarXiv:2206.11851v1  [cs.CL]  29 May 2022(a)  (b) Figure 1: Overall training procedure in (a) virtual ad versarial training (b) contextbased virtual adversarial training (ConV AT). Dotted line indicates a duplicated propagation path to generate adversarial perturbation. lows us to train a label noiserobust classiÔ¨Åer without placing a burden in a network computation. In order to show the strength of the proposed method, we conduct extensive experiments on four dif ferent datasets with the different kinds of label noise. Comprehensive evaluation results clearly show that ConV AT outperforms the stateoftheart method in text classiÔ¨Åcation with noisy labels. The indepth anal ysis demonstrates that the proposed method have a strong advantage over previous adversarial methods in terms of time and memory complexity. Our code and dataset are publicly available1. 2. Related Works "
79,Dual GNNs: Graph Neural Network Learning with Limited Supervision.txt,"Graph Neural Networks (GNNs) require a relatively large number of labeled
nodes and a reliable/uncorrupted graph connectivity structure in order to
obtain good performance on the semi-supervised node classification task. The
performance of GNNs can degrade significantly as the number of labeled nodes
decreases or the graph connectivity structure is corrupted by adversarial
attacks or due to noises in data measurement /collection. Therefore, it is
important to develop GNN models that are able to achieve good performance when
there is limited supervision knowledge -- a few labeled nodes and noisy graph
structures. In this paper, we propose a novel Dual GNN learning framework to
address this challenge task. The proposed framework has two GNN based node
prediction modules. The primary module uses the input graph structure to induce
regular node embeddings and predictions with a regular GNN baseline, while the
auxiliary module constructs a new graph structure through fine-grained spectral
clusterings and learns new node embeddings and predictions. By integrating the
two modules in a dual GNN learning framework, we perform joint learning in an
end-to-end fashion. This general framework can be applied on many GNN baseline
models. The experimental results validate that the proposed dual GNN framework
can greatly outperform the GNN baseline methods when the labeled nodes are
scarce and the graph connectivity structure is noisy.","Graph Neural Networks (GNN) have been successfully employed to solve multiple tasks such as node classiÔ¨Åcation, graph completion, and edge prediction across a variety of application domains including computational chemistry Shi et al. [2020], proteinprotein interactions Zitnik and Leskovec [2017] and knowledgebase completion Schlichtkrull et al. [2018]. In particular, many GNN advancements have addressed the typical node classiÔ¨Åcation task in a semisupervised learning setting where only a subset of nodes in the graph are labeled, including the well known Graph Convolutional Networks (GCNs) Kipf and Welling [2017], Graph Attention Networks (GATs) Veli Àáckovi ¬¥c et al. [2018], Topology Adaptive Graph Convolutional Networks (TAGs) Du et al. [2018] and Dynamic Neighborhood Aggregation networks (DNAs) Fey [2019]. These GNN models have achieved great results on the benchmark GNN learning datasets. However, they typically require a large number of labeled nodes as well as reliable/uncorrupted graph structures in order to obtain good performance. Their performance can degrade signiÔ¨Åcantly as the number of labeled nodes becomes scarce Li et al. [2018], Lin et al. [2020] (as shown in Figure 1a) or when the graph structures are noisy or corrupted Wang et al. [2020], Chen et al. [2020] such as many edges are deleted (as shown in Figure 1b). The performance drop of GNNs with limited labeled data can be explained by the inability of GNNs to propagate the label information from the labeled nodes to the rest of the graph. That is, while it is known that deep GNN architectures can cause oversmoothing problems Li et al. [2018], a relatively shallow GNN architecture can fail to propagate messages across the whole grapharXiv:2106.15755v1  [cs.LG]  29 Jun 2021‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè‚óè 455055606570 05101520 # Labeled Nodes / ClassAccuracy(a) Few Labeled Nodes ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè‚óè‚óè ‚óè ‚óè ‚óè ‚óè 5055606570 0.20.40.60.81.0  % of Deleted EdgesAccuracy (b) Noisy Graph Structure Figure 1: Performance degradation of GNN models on CiteSeer: (a) Few Labeled Nodes; (b) Noisy Graph Structures. and cause the classiÔ¨Åer to overÔ¨Åt the small neighborhoods of the labeled nodes. When there are very small numbers of labeled nodes, this will induce serious overÔ¨Åtting problems and degrade the classiÔ¨Åcation performance Li et al. [2018], Sun et al. [2020], Lin et al. [2020]. In addition, GNNs learn discriminative node embeddings for effective node classiÔ¨Åcation by propagating messages across the edges of the graph. Hence noisy or corrupted graph structures can greatly impair the message passing process and degrade the ultimate node classiÔ¨Åcation performance Li et al. [2018], Wang et al. [2020]. On the other hand, the dependence of GNN models on a large number of labeled nodes and reliable graph structures can seriously limit their applications, as in many application domains it is very expensive or difÔ¨Åcult to obtain a large number of labeled examples. It is also very difÔ¨Åcult to guarantee reliable/uncorrupted graph structures given the numerous sources of noises that could damage the graph connectivity structures, ranging from adversarial attacks on the graph structures to data collection or measurement noises Wang et al. [2020], Chen et al. [2020]. Therefore, it is important to develop GNN models that can learn efÔ¨Åciently with few labeled nodes and are robust to corrupted/unreliable graph structures. In this work, we propose a novel dual GNN learning framework that is resilient to low label rates and corrupted/noisy graph structures with deleted edges. The proposed framework is made up of two node prediction modules. The Ô¨Årst module can be treated as a standard primary GNN model which takes the original graph data as input. It suffers from the aforementioned message propagation drawbacks when labeled nodes are scarce and graph structures are noisy. To address this problem, the second GNN module employs a Ô¨Ånegrained spectral clustering method based on the node embedding results of the Ô¨Årst module to construct a new adjacency matrix and hence a new graph structure, aiming to enable effective information propagation across the graph, and facilitate the subsequent node embedding and node classiÔ¨Åcation learning. The two modules coordinate with each other within the integrated dual learning framework to perform endtoend training with a joint objective function. This general dual learning framework can be applied on many standard GNN baseline models. We conduct experiments with four baseline graph neural network learning models: GCNs Kipf and Welling [2017], GATs Veli Àáckovi ¬¥c et al. [2018], TAGs Du et al. [2018], and DNAs Fey [2019]. The experimental results show that the proposed framework can signiÔ¨Åcantly improve the baseline models when the labeled nodes are very scarce and graph structure is noisy/corrupted across multiple benchmark datasets. 2 Related Works "
80,Can Ground Truth Label Propagation from Video help Semantic Segmentation?.txt,"For state-of-the-art semantic segmentation task, training convolutional
neural networks (CNNs) requires dense pixelwise ground truth (GT) labeling,
which is expensive and involves extensive human effort. In this work, we study
the possibility of using auxiliary ground truth, so-called \textit{pseudo
ground truth} (PGT) to improve the performance. The PGT is obtained by
propagating the labels of a GT frame to its subsequent frames in the video
using a simple CRF-based, cue integration framework. Our main contribution is
to demonstrate the use of noisy PGT along with GT to improve the performance of
a CNN. We perform a systematic analysis to find the right kind of PGT that
needs to be added along with the GT for training a CNN. In this regard, we
explore three aspects of PGT which influence the learning of a CNN: i) the PGT
labeling has to be of good quality; ii) the PGT images have to be different
compared to the GT images; iii) the PGT has to be trusted differently than GT.
We conclude that PGT which is diverse from GT images and has good quality of
labeling can indeed help improve the performance of a CNN. Also, when PGT is
multiple folds larger than GT, weighing down the trust on PGT helps in
improving the accuracy. Finally, We show that using PGT along with GT, the
performance of Fully Convolutional Network (FCN) on Camvid data is increased by
$2.7\%$ on IoU accuracy. We believe such an approach can be used to train CNNs
for semantic video segmentation where sequentially labeled image frames are
needed. To this end, we provide recommendations for using PGT strategically for
semantic segmentation and hence bypass the need for extensive human efforts in
labeling.","Semantic segmentation is an extensively studied problem which has been widely addressed using convolutional neural networks (CNNs) recently. CNNs have been shown to perform extremely well on datasets such as Pascal VOC [9], NYUD [33], CityScapes [7], etc. For ecient performance of CNNs, there are certain characteristics of training data which are required: i) the ground truth (GT) training data needs dense pixelwise annotations which requires an enormous amount of human eort. For instance, an image in the Cityscapes dataset takes arXiv:1610.00731v1  [cs.CV]  3 Oct 20162 Mustikovela, Yang, Rother about 1:5hfor dense annotation [7], ii) the training data has to be diverse in the sense that highly similar images do not add much information to the network. Such diversity in training data helps better modelling of the distribution of test scenarios. For semantic video segmentation, continuous annotation of consecutive frames is helpful rather than annotations of discrete and temporally separated frames. In such a case it is again extremely expensive to obtain dense pixelwise anno tation of consecutive images in the video. To this end, we arrive at an impor tant question: can auxiliary ground truth training data obtained by using label propagation help in better performance of a CNNbased semantic segmentation framework? In this work, we explore the possibility of using auxiliary GT, to produce more training data for CNN training. We use the CamVid dataset [5] as an example, which contains video sequences of outdoor driving scenarios. But the methodology can be easily applied to other relevant datasets. The CamVid has training images picked at 1 fpsfrom a 30fpsvideo, leading to one GT training frame for every 30 frames. We propagate the GT labels from these images to the subsequent images using a simple CRFbased, cue integration framework leading to pseudo ground truth (PGT) training images. It can be expected that the new PGT is noisy and has lower quality compared to the actual GT labeling as a result of automaitc label propagation. We train the semantic segmentation network FCN [24] using this data. In this regard, we explore three factors of how the PGT has to be used to enhance the performance of a CNN. 1.Quality  The PGT labeling has to be of good quality in the sense that there should not be too much of wrong labeling. 2.Diversity  The PGT training images have to be dierent compared to the GT images, in order to match the potential diverse test data distribution. 3.Trust  During the error propagation, the PGT has to be weighted with a trust factor in the loss function while training. Further, we systematically analyze the aforementioned dimensions through extensive experimentation to nd the most in uential dimension which improves the performance of the CNN. We perform experiments with two main settings. First, where equal number of PGT and GT training samples are present. Second, the number of samples of PGT is multiple folds larger than GT training samples. Our baseline is obtained by training the FCN only on the GT training images which stands at 49 :6%. From our experiments, we have found that adding PGT to the GT data and training the FCN helps in enhancing the accuracy by 2 :7% to 52:3%. The main contributions of this work are: {We perform exhaustive analysis to nd the in uential factors among Quality, Diversity andTrust which aect the learning in the presence of PGT data. We conclude that PGT images have to be diverse from the GT images in addition to their labeling to be of good quality. Trust on PGT data should be suciently low when there is multiple folds of PGT than GT data.Can Label Propagation help Semantic Segmentation? 3 {We provide application specic recommendations to use PGT data, taking the above factors into account. In the case of semantic video segmentation, when PGT is multiple folds larger than GT, it is advisable to have a low trust on PGT data. In case of image semantic segmentation, diverse high quality PGT data helps in improving the performance. Detailed discussions are further presented in experiments section (sec. 4). 2 Related Work "
81,Selective Pseudo-label Clustering.txt,"Deep neural networks (DNNs) offer a means of addressing the challenging task
of clustering high-dimensional data. DNNs can extract useful features, and so
produce a lower dimensional representation, which is more amenable to
clustering techniques. As clustering is typically performed in a purely
unsupervised setting, where no training labels are available, the question then
arises as to how the DNN feature extractor can be trained. The most accurate
existing approaches combine the training of the DNN with the clustering
objective, so that information from the clustering process can be used to
update the DNN to produce better features for clustering. One problem with this
approach is that these ``pseudo-labels'' produced by the clustering algorithm
are noisy, and any errors that they contain will hurt the training of the DNN.
In this paper, we propose selective pseudo-label clustering, which uses only
the most confident pseudo-labels for training the~DNN. We formally prove the
performance gains under certain conditions. Applied to the task of image
clustering, the new approach achieves a state-of-the-art performance on three
popular image datasets. Code is available at
https://github.com/Lou1sM/clustering.","Clustering is the task of partitioning a dataset into clusters such that data points within the same cluster are similar to each other, and data points from diÔ¨Äerent clusters are diÔ¨Äerent to each other. It is applicable to any set of data for which there is a notion of similarity between data points. It requires no prior knowledge, neither the explicit labels of supervised learning nor the knowledge of expected symmetries and invariances leveraged in selfsupervised learning. The result of a successful clustering is a means of describing data in terms of the cluster that they belong to. This is a ubiquitous feature of human cognition. For example, we hear a sound and think of it as an utterance of the word ‚Äúwater‚Äù, or we see a video of a biomechanical motion and think of it as a jump. This can be further reÔ¨Åned among experts, so that a musician could describe a musical phrase as an English cadence in A major, or a dancer could describe a snippet of ballet as a rightleg fouette into arabesque. When clustering highdimensional data, the curse of dimensionality [ 2] means that many classic algorithms, such as kmeans [ 29] or expectation maximization [ 10], perform poorly. The Euclidean distance, which is the basis for the notion of similarity in the Euclidean space, becomes weaker in higher dimensions [ 51]. Several solutions to this problem have been proposed. In this paper, we consider those termed deep clustering.arXiv:2107.10692v1  [cs.LG]  22 Jul 20212 L. Mahon et al. Deep clustering is a set of techniques that use a DNN to encode the high dimensional data into a lowerdimensional feature space, and then perform clustering in this feature space. A major challenge is the training of the encoder. Much of the success of DNNs as image feature extractors (including [ 24,46]) has been in supervised settings, but if we already had labels for our data, then there would be no need to cluster in the Ô¨Årst place. There are two common approaches to training the encoder. The Ô¨Årst is to use the reconstruction loss from a corresponding decoder, i.e., to train it as an autoencoder [ 47]. The second is to design a clustering loss, so that the encoding and the clustering are optimized jointly. Both are discussed further in Section 2. Our model, selective pseudolabel clustering (SPC), combines reconstruction andclusteringloss.ItusesanensembletoselectdiÔ¨ÄerentlossfunctionsfordiÔ¨Äerent data points, depending on how conÔ¨Ådent we are in their predicted clusters. Ensemble learning is a function approximation where multiple approximating models are trained, and then the results are combined. Some variance across the ensemble is required. If all individual approximators were identical, there would be no gain in combining them. For ensembles composed of DNNs, variance is ensured by the random initializations of the weights and stochasticity of the training dynamics. In the simplest case, the output of the ensemble is the average of each individual output (mean for regression and mode for classiÔ¨Åcation) [36]. When applying an ensemble to clustering problems (referred to as consensus clustering; see [ 3] for a comprehensive discussion), the sets of cluster labels must be aligned across the ensemble. This can be performed eÔ¨Éciently using the Hungarian algorithm. SPC considers a clustered data point to be conÔ¨Ådent if it received the same cluster label (after alignment) in each member of the ensemble. The intuition is that, due to random initializations and stochasticity of training, there is some nonzero degree of independence between the diÔ¨Äerent sets of cluster labels, so the probability that all cluster labels are incorrect for a particular point is less than the probability that a single cluster label is incorrect. Our main contributions are brieÔ¨Çy summarized as follows. ‚ÄìWe describe a generally applicable deep clustering method (SPC), which treats cluster assignments as pseudolabels, and introduces a novel technique to increase the accuracy of the pseudolabels used for training. This produces a better feature extractor, and hence a more accurate clustering. ‚ÄìWeformallyprovetheadvantagesofSPC,givensomesimplifyingassumptions. SpeciÔ¨Åcally, we prove that our method does indeed increase the accuracy of the targets used for pseudolabel training, and this increase in accuracy does indeed lead to a better clustering performance. ‚ÄìWe implement SPC for image clustering, with a stateoftheart performance on three popular image clustering datasets, and we present ablation studies on its main components. The rest of this paper is organized as follows. Section 2 gives an overview of related work. Sections 3 and 4 give a detailed description of SPC and a proof of correctness, respectively. Section 5 presents and discusses our experimental results,Selective PseudoLabel Clustering 3 including a comparison to existing image clustering models and ablation studies on main components of SPC. Finally, Section 6 summarizes our results and gives an outlook on future work. Full proofs and further details are in the appendix. 2 Related Work "
82,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro.txt,"The main contribution of this paper is a simple semi-supervised pipeline that
only uses the original training set without collecting extra data. It is
challenging in 1) how to obtain more training data only from the training set
and 2) how to use the newly generated data. In this work, the generative
adversarial network (GAN) is used to generate unlabeled samples. We propose the
label smoothing regularization for outliers (LSRO). This method assigns a
uniform label distribution to the unlabeled images, which regularizes the
supervised model and improves the baseline. We verify the proposed method on a
practical problem: person re-identification (re-ID). This task aims to retrieve
a query person from other cameras. We adopt the deep convolutional generative
adversarial network (DCGAN) for sample generation, and a baseline convolutional
neural network (CNN) for representation learning. Experiments show that adding
the GAN-generated data effectively improves the discriminative ability of
learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and
DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1
precision over the baseline CNN, respectively. We additionally apply the
proposed method to fine-grained bird recognition and achieve a +0.6%
improvement over a strong baseline. The code is available at
https://github.com/layumi/Person-reID_GAN.","Unsupervised learning can serve as an important auxil iary task to supervised tasks [14, 29, 11, 28]. In this work, we propose a semisupervised pipeline that works on the original training set without an additional data collectio n process. First, the training set is expanded with unlabeled data using a GAN. Then our model minimizes the sum of the supervised and the unsupervised losses through a new ‚àóTo whom all correspondence should be addressed. Figure 1. The pipeline of the proposed method. There are two components: a generative adversarial model [27] for unsupe rvised learning and a convolutional neural network for semisuper vised learning. ‚ÄúReal Data‚Äù represents the labeled data in the giv en training set; ‚ÄúTraining data‚Äù includes both the ‚ÄúReal Data‚Äù and the generated unlabeled data. We aim to learn more discrimin ative embeddings with the ‚ÄúTraining data‚Äù. regularization method. This method is evaluated with per son reID, which aims to spot the target person in different cameras. This has been recently viewed as an image re trieval problem [50]. This paper addresses three challenges. First, current re search in GANs typically considers the quality of the sam ple generation with and without semisupervised learning in vivo [24, 32, 27, 7, 26, 41]. Yet a scientiÔ¨Åc problem re mains unknown: moving the generated samples out of the box and using them in currently available learning frame works. To this end, this work uses unlabeled data produced by the DCGAN model [27] in conjunction with the labeled training data. As shown in Fig. 1, our pipeline feeds the newly generated samples into another learning machine (i.e . a CNN). Therefore, we use the term ‚Äú in vitro ‚Äù to differenti ate our method from [24, 32, 27, 7]; these methods perform semisupervised learning in the discriminator of the GANs (in vivo ). Second, the challenge of performing semisupervised learning using labeled and unlabeled data in CNNbased methods remains. Usually, the unsupervised data is used as a pretraining step before supervised learning [28, 11, 14] . Our method uses all the data simultaneously. In [25, 18, 24, 32], the unlabeled/weaklabeled real data are assignedlabels according to predeÔ¨Åned training classes, but our method assumes that the GAN generated data does not be long to any of the existing classes. The proposed LSRO method neither includes unsupervised pretraining nor la bel assignments for the known classes. We address semi supervised learning from a new perspective. Since the unla beled samples do not belong to any of the existing classes, they are assigned a uniform label distribution over the trai n ing classes. The network is trained not to predict a particul ar class for the generated data with high conÔ¨Ådence. Third, in person reID, data annotation is expensive, be cause one has to draw a pedestrian bounding box and as sign an ID label to it. Recent progress in this Ô¨Åeld can be attributed to two factors: 1) the availability of largesca le re ID datasets [49, 51, 44, 19] and 2) the learned embedding of pedestrians using a CNN [8, 10]. That being said, the number of images for each identity is still limited, as shown in Fig. 2. There are 17.2 images per identities in Market 1501 [49], 9.6 images in CUHK03 [19], and 23.5 images in DukeMTMCreID [30] on average. So using additional data is nontrivial to avoid model overÔ¨Åtting. In the literature , pedestrian images used in training are usually provided by the training sets, without being expanded. So it is unknown if a larger training set with unlabeled images would bring any extra beneÔ¨Åt. This observation inspired us to resort to the GAN samples to enlarge and enrich the training set. It also motivated us to employ the proposed regularization to implement a semisupervised system. In an attempt to overcome the abovementioned chal lenges, this paper 1) adopts GAN in unlabeled data gen eration, 2) proposes the label smoothing regularization fo r outliers (LSRO) for unlabeled data integration, and 3) re ports improvements over a CNN baseline on three person reID datasets. In more details, in the Ô¨Årst step, we train DCGAN [27] on the original reID training set. We gen erate new pedestrian images by inputting 100dim random vectors in which each entry falls within [1, 1]. Some gen erated samples are shown in Fig. 3 and Fig. 5. In the second step, these unlabeled GANgenerated data are fed into the ResNet model [13]. The LSRO method regular izes the learning process by integrating the unlabeled data and, thus, reduces the risk of overÔ¨Åtting. Finally, we eval u ate the proposed method on person reID and show that the learned embeddings demonstrate a consistent improvement over the strong ResNet baseline. To summarize, our contributions are: ‚Ä¢the introduction of a semisupervised pipeline that in tegrates GANgenerated images into the CNN learning machine in vitro ; ‚Ä¢an LSRO method for semisupervised learning. The integration of unlabeled data regularizes the CNN learning process. We show that the LSRO method is Figure 2. The image distribution per class in the dataset Mar ket 1501 [49], CUHK03 [19] and DukeMTMCreID [30]. We observe that all these datasets suffer from the limited images per cl ass. Note that there are only a few classes with more than 20 images . superior to the two available strategies for dealing with unlabeled data; and ‚Ä¢a demonstration that the proposed semisupervised pipeline has a consistent improvement over the ResNet baseline on three person reID datasets and one Ô¨Åne grained recognition dataset. 2. Related Work "
83,Asymmetric Co-Teaching for Unsupervised Cross Domain Person Re-Identification.txt,"Person re-identification (re-ID), is a challenging task due to the high
variance within identity samples and imaging conditions. Although recent
advances in deep learning have achieved remarkable accuracy in settled scenes,
i.e., source domain, few works can generalize well on the unseen target domain.
One popular solution is assigning unlabeled target images with pseudo labels by
clustering, and then retraining the model. However, clustering methods tend to
introduce noisy labels and discard low confidence samples as outliers, which
may hinder the retraining process and thus limit the generalization ability. In
this study, we argue that by explicitly adding a sample filtering procedure
after the clustering, the mined examples can be much more efficiently used. To
this end, we design an asymmetric co-teaching framework, which resists noisy
labels by cooperating two models to select data with possibly clean labels for
each other. Meanwhile, one of the models receives samples as pure as possible,
while the other takes in samples as diverse as possible. This procedure
encourages that the selected training samples can be both clean and
miscellaneous, and that the two models can promote each other iteratively.
Extensive experiments show that the proposed framework can consistently benefit
most clustering-based methods, and boost the state-of-the-art adaptation
accuracy. Our code is available at
https://github.com/FlyingRoastDuck/ACT_AAAI20.","Person reidentiÔ¨Åcation (reID) (Sun et al. 2018; Zheng, Yang, and Hauptmann 2016; Li, Zhu, and Gong 2018b) aims to locate the target person in surveillance videos with a given probe image. With the rapid evolution of deep learning mod els, the accuracy of person reID has been greatly boosted in the public datasets. However, models trained on the source domain often suffer from domain shifts, leading to a perfor mance decline on a different target domain. To alleviate this issue, recent works (Zhong et al. 2019b; Zhong et al. 2018b) make efforts on the unsupervised do This work was done when Fengxiang Yang was an intern at Youtu Lab (yangfx@stu.xmu.edu.cn). yCorresponding Author (zhiming.luo@xmu.edu.cn, winfred sun@tencent.com) Copyright c 2020, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.main adaptation (UDA), which aims to transfer the knowl edge from the labeled source domain to the unlabeled tar get domain. These works mainly lie in two aspects, distri bution aligning (Wei et al. 2018; Deng et al. 2018; Chang et al. 2019; Lin et al. 2018; Wang et al. 2018) and tar get pseudo label discovering (Fan, Zheng, and Yang 2018; Song et al. 2018; Li, Zhu, and Gong 2018a). The former aims to reduce the distribution gap between domains in a common space, such as imagelevel (Wei et al. 2018; Deng et al. 2018) and attributelevel (Chang et al. 2019; Lin et al. 2018; Wang et al. 2018) spaces. The latter attempts to leverage the underlying relations among target samples and predict pseudo labels for model retraining, e.g.assign ing pseudo labels based on clustering (Fan, Zheng, and Yang 2018; Song et al. 2018; Li, Zhu, and Gong 2018a) and k nearest neighbors (Zhong et al. 2019a; Yang et al. 2018). Among them, clustering based methods have reported very competitive accuracy for UDA in person reID. These meth ods usually employ an iterative process of predicting pseudo identities for unlabeled target samples according to the clus ters and Ô¨Ånetuning the model with those predicted samples. Despite their promising results, clustering based methods are restricted by two main drawbacks. On the one hand, the clustering accuracy can not be guaranteed even using the modern approaches, so that pseudo labels assigned by clusters can be noisy. Training the model with noisy labels that assigned to wrong identities will undoubtedly damage the reID performance. On the other hand, most clustering methods tend to leave low conÔ¨Ådence samples as outliers and do not assign cluster labels to them, e.g., DBSCAN (Es ter et al. 1996). These outliers are usually hard samples that encounter high image variations. Without considering such samples during training, the model may have a problem in discriminating high variation testing samples. However, di rectly assigning them to the nearest cluster will bring more noisy labels, hindering the retraining of the model. CoTeaching (CT) (Han et al. 2018) is a commonly used algorithm for training model with noisy labels, which learns two networks by feeding samples with small losses of one network to another. However, most coteaching frameworks utilize symmetric inputs for both networks, which do not effectively apply to the context of clustering based crossarXiv:1912.01349v1  [cs.CV]  3 Dec 20197DUJHW'DWD¬´Ltri 6PDOO/RVV6DPSOHV/DEHOHG2XWOLHUV Ltri /DEHOHG,QOLHUV& &0 00 6PDOO/RVV6DPSOHV6WHS,,QOLHUV2XWOLHUV*HQHUDWLRQ 6WHS,,$V\PPHWULF&R7HD FKLQJ 2XWOLHUV ,QOLHUV Figure 1: The proposed asymmetric coteaching framework (ACT). ‚ÄúM‚Äù and ‚ÄúC‚Äù denote the main model and the collaborator model, respectively. We Ô¨Årst train CNN on the source labeled data and Ô¨Ånetune it on target data with pseudo labels predicted by clustering to get initial weights for ‚ÄúM‚Äù and ‚ÄúC‚Äù. ‚ÄúM‚Äù receives samples as diverse as possible from inliers and outliers, while ‚ÄúC‚Äù takes in samples as pure as possible from inliers during ACT. This process encourages the two models to mutually promote the discriminative ability of each other. More details can be found at Sec. 3.4. domain reID. This is because that the training samples with lowconÔ¨Ådence commonly have large losses during training. Using symmetric inputs leads the model to always select easy samples and ignore the lowconÔ¨Ådence samples within the training minibatch. As a consequence, the second short coming mentioned above will still remain and will lead re ID model to a local minimum. To this end, we choose the stateoftheart clustering based method proposed in (Song et al. 2018) as our baseline, and propose an asymmetric coteaching framework to elim inate the negative effects of the above two shortcomings. SpeciÔ¨Åcally, we Ô¨Årst divide the target samples into inliers and outliers, according to the clustering results (as shown in Fig. 1). In this paper, we regard the lowconÔ¨Ådent samples recognized by the clustering method as outliers while re maining as inliers. After that, our framework is trained with two models. The Ô¨Årst one is the main model which aims to infer samples with small losses from the inliers, while the second one is the collaborator model that estimates sam ples with small losses from the outliers. The samples in ferred/estimated by the certain model are selected for the training of another model. This training process is similar to the traditional coteaching, except that the inputs of the two models are asymmetric, i.e.the data for training the two models comes from two different data Ô¨Çows. In this manner, selecting samples with small losses ensure that the models can be trained with possibly clean data. Moreover, these two models are iteratively promoted by each other. On the one hand, the main model attempts to mine as pure as possible samples from the inliers for maintaining the basic represen tation of the collaborator model. On the other hand, the col laborator model tries to select as diverse as possible samples from the outliers for further improving the discriminative ability of the main model. Our contributions are summarized in threefold:We introduce to employ coteaching technique for re sisting noisy labels generated by clustering in the cross domain person reID. Experiments show that learning with Ô¨Åltered data can consistently improve adaptation ac curacy. We divide the unlabeled target data into inliers and out liers and design an asymmetric coteaching (ACT) frame work to make reID model see hard samples at the early stage of adaptation. Experiments demonstrate that the asymmetric approach is more effective in handling hard samples than the symmetric one. Experiments conducted on three largescale datasets show that our method can apply to various clustering based methods and produces stateoftheart adaptation accu racy in person reID. 2 Related Work "
84,Neighbour Consistency Guided Pseudo-Label Refinement for Unsupervised Person Re-Identification.txt,"Unsupervised person re-identification (ReID) aims at learning discriminative
identity features for person retrieval without any annotations. Recent advances
accomplish this task by leveraging clustering-based pseudo labels, but these
pseudo labels are inevitably noisy which deteriorate model performance. In this
paper, we propose a Neighbour Consistency guided Pseudo Label Refinement
(NCPLR) framework, which can be regarded as a transductive form of label
propagation under the assumption that the prediction of each example should be
similar to its nearest neighbours'. Specifically, the refined label for each
training instance can be obtained by the original clustering result and a
weighted ensemble of its neighbours' predictions, with weights determined
according to their similarities in the feature space. In addition, we consider
the clustering-based unsupervised person ReID as a label-noise learning
problem. Then, we proposed an explicit neighbour consistency regularization to
reduce model susceptibility to over-fitting while improving the training
stability. The NCPLR method is simple yet effective, and can be seamlessly
integrated into existing clustering-based unsupervised algorithms. Extensive
experimental results on five ReID datasets demonstrate the effectiveness of the
proposed method, and showing superior performance to state-of-the-art methods
by a large margin.","Person reidentiÔ¨Åcation (ReID) aims to train a deep model capable of retrieving a person of interest across mul tiple cameras. This task has attracted increasing atten tion, due to its great application in video surveillance sys tem. Although supervised methods have achieved impres sive performances, they require to annotate large amount of crosscamera labels of the surveillance data, which is labor intensive, costly, and eventually leads to limited practical *Equal contribution ‚Ä†Corresponding author.application in realworld scenarios. Therefore, developing effective unsupervised methods for person retrieval from unlabeled data is very appealing and important, not only in academic sector but also for the industrial Ô¨Åelds. Existing stateoftheart unsupervised learning (USL) ReID methods leverage the pseudolabels obtained from unsupervised clustering [9, 15] or knearest neighbor search [28,40] to train the deep model. The training scheme of these methods usually alternates between the following two steps: 1) Generating pseudolabels for the training ex amples through some clusteringbased methods, e.g., DB SCAN [12]; 2) Optimizing the deep neural network with these pseudo labels in a supervised manner by some met ric learning objectives, such as triplet loss [4], InfoNCE loss [16]. Although these pseudolabelbased methods have achieved remarkable performances, there still contains a large performance gap between the purely USL methods and supervised learning methods. The rationale behind this is that the generated pseudo labels inevitably contain a portion of noise, which could signiÔ¨Åcantly deteriorate the model performance due to the model memorization/over Ô¨Åtting to the noisy labels [5, 8]. Therefore, how to mitigate the sideeffects of the inaccurate clustering results automat ically becomes the key issue for these clusteringbased un supervised methods. To address this problem, we propose a neighbour con sistency guided pseudolabel reÔ¨Ånement method (NCPLR) for USL person ReID. Although recent advances in pseudo label reÔ¨Ånement could reduce the label noise to a certain ex tent, these methods usually adopt multiple predictions from auxiliary backbone networks for mutual conÔ¨Årmation of the estimated pseudolabels [14,52], or employ some additional information (e.g., partbased reÔ¨Ånement [8]) to improve the quality of the pseudo labels, resulting in high computation costs during model training. Our proposed NCPLR algo rithm is simple yet effective, just through assembling the predictions of its nearest neighbours to reÔ¨Åne the pseudo la bel. It can be seamlessly integrated into existing clustering based USL methods. Even for learning with noisy labels, although neighbours have often been used to identify mis labeled examples [20], few works have considered the usearXiv:2211.16847v1  [cs.CV]  30 Nov 2022of neighbours to generate or reÔ¨Åne the pseudo labels. The proposed NCPLR method is inspired by the label propagation algorithms [19, 23, 58], which try to transfer labels from the supervised instances to their neighbouring unsupervised examples based on the similarities in the fea ture space. It seeks to transfer labels from its neighbour ing instances, and encourage each example to have similar predictions to its neighbours‚Äô. As we know, existing meth ods for label propagation represent the transductive learn ing, where they could produce labels for the given exam ples during model training. NCPLR can also be regarded as an transductive form of label propagation for pseudolabel reÔ¨Ånement. SpeciÔ¨Åcally, the reÔ¨Åned label for each training example can be obtained by the original clustering result and a weighted combination of its neighbours‚Äô predictions, with weights determined according to their similarities in the feature space. The motivation behind this is that the NCPLR is to enable the incorrect labels among the gener ated pseudo labels to be improved or at least attenuated by the labels of their neighbours [20], relying on the moderate assumption that the prediction of each example should be similar to its nearest neighbours‚Äô. In addition, we consider the USL person ReID as part of the labelnoise learning problem. For labelnoise learning, the key challenge is that the deep model could easily mem orize and overÔ¨Åt to the noisy labels during model train ing, which leads to severe performance degradation. There fore, effective measures should also be taken to address the overÔ¨Åtting problem. In term of this issue, we further lever age the explicit neighbour consistency regularization to en courage each example to have similar predictions as their neighbours‚Äô, and penalize the divergence of each example‚Äôs prediction from a weighted combination of its neighbours‚Äô predictions. Since the similarity graph is computed in the feature space instead of directly using their predictions, the neighbour consistency regularization can be seen as boot strapping the learned feature representations. This could be more helpful to reduce the susceptibility to overÔ¨Åtting and improve the training stability, as the last fully connected classiÔ¨Åcation layers are more prone to memorize/overÔ¨Åt to the noisy labels [29]. The main contributions of this paper are as follows, ‚Ä¢ We propose a neighbour consistency guided pseudo label reÔ¨Ånement algorithm for USL person ReID, which reÔ¨Åne the pseudo label through a weighted com bination of its neighbours‚Äô predictions, with weights determined by their similarities in the feature space. ‚Ä¢ We consider the clusteringbased USL person ReID task as the labelnoise learning problem, and we fur ther leverage the explicit neighbour consistency regu larization to reduce the model susceptibility to over Ô¨Åtting and improve the training stability.‚Ä¢ The proposed NCPLR algorithm is simple yet effec tive, and can be seamlessly integrated into existing clusteringbased unsupervised methods. Besides, ex tensive experimental results demonstrate the effective ness of the proposed method, and the performances are superior to stateoftheart methods by a large margin. 2. Related Work "
85,Supervised Anomaly Detection via Conditional Generative Adversarial Network and Ensemble Active Learning.txt,"Anomaly detection has wide applications in machine intelligence but is still
a difficult unsolved problem. Major challenges include the rarity of labeled
anomalies and it is a class highly imbalanced problem. Traditional unsupervised
anomaly detectors are suboptimal while supervised models can easily make biased
predictions towards normal data. In this paper, we present a new supervised
anomaly detector through introducing the novel Ensemble Active Learning
Generative Adversarial Network (EAL-GAN). EAL-GAN is a conditional GAN having a
unique one generator vs. multiple discriminators architecture where anomaly
detection is implemented by an auxiliary classifier of the discriminator. In
addition to using the conditional GAN to generate class balanced supplementary
training data, an innovative ensemble learning loss function ensuring each
discriminator makes up for the deficiencies of the others is designed to
overcome the class imbalanced problem, and an active learning algorithm is
introduced to significantly reduce the cost of labeling real-world data. We
present extensive experimental results to demonstrate that the new anomaly
detector consistently outperforms a variety of SOTA methods by significant
margins. The codes are available on Github.","ANOMALIES nomalies (also known as outliers, novel ties, or faults) refer to the data points that deviate signiÔ¨Åcantly from the majority of available data [1]. Due to the valid, interesting and potentially valuable patterns they often represent, detecting such data could provide valuable knowledge in various realworld applications, such as Ô¨Å nance fraud detection [2], intrusion detection [3], rare in formation detection in healthcare [4], [5] and video analysis [6], [7]. Three substantial challenges in these applications are: (i) It is very difÔ¨Åcult to obtain sufÔ¨Åcient anomalies and their groundtruths to train an anomaly detector due to the prohibitive cost of collecting and labeling such data. (ii) Anomalies often exhibit very different anomalous behav iors, as a result, training an anomaly detector could be quite a challenge for the commonlyused optimization techniques, which generally assume that data within the same class should be similar to each other [8]. (iii) The available data could present a highly skewed distribution due to the rarity of anomalies. Consequently, most supervised models can easily make biased predictions to the normal data. To address these challenges, researchers generally adopt a twostep unsupervised learning procedure to isolate the Zhi Chen, Jiang Duan and Li Kang are with the School of Economic Infor mation Engineering, Southwestern University of Finance and Economics, Chengdu, China, 611130. Jiang Duan is the corresponding author Email:fchenzhi;duanj t;kanglig@swufe.edu.cn Guoping Qiu is with the College of Electronics and Information Engineer ing, Guangdong Key Laboratory for Intelligent Information Processing, and Shenzhen Institute of ArtiÔ¨Åcial intelligence and Robotics for Society, Shenzhen University, Shenzhen 518060, China, and also with the School of Computer Science, University of Nottingham, Nottingham NG8 1BB, UK. Email: guoping.qiu@nottingham.ac.ukanomalies from normal data: They Ô¨Årst learn to represent all the available data with new representations, e.g., distance metric spaces in [9], [10], [11], representations in a projected space [12], [13], [14], [15], [16], or latent spaces in generative adversarial networks (GANs) [4], [17], [18], [19]. And then, the data which signiÔ¨Åcantly deviates from the established normal proÔ¨Åles will be identiÔ¨Åed as potential anomalies. In most of these unsupervised methods, representation learn ing and anomaly detector training are separated into two independent procedures, therefore, such methods may yield suboptimal representations or representations irrelevant to the anomaly detection task [8]. To avoid this problem, some works have incorporated traditional anomaly scoring metrics into the representation learning objective to improve the quality of learned representations [20]. However, these methods mainly focus on unsupervised solutions, so they share the common shortcoming of unsupervised learning, often identify anomalies that are merely uninteresting data or noise. In recent years, deep learning has gained remark able success in various applications and has also shown promising potential in anomaly detection. However, most existing deep anomaly detectors [21], [22], [23] are unsuper vised methods, therefore they also suffer from the inherent limitations of such methods. Conditional generative adversarial networks (cGANs) [24], [25] have demonstrated exceptional ability in generat ing labeled samples indistinguishable from real world data. It therefore offers an appealing prospect of overcoming the problem of lack of labelled data in supervised anomaly detection by conditionally generating class balanced data. However, despite the promising performance of cGANs in modeling various conditional distributions, the perfor mances of standard cGANs heavily depends on the sizearXiv:2104.11952v1  [cs.LG]  24 Apr 20212 of labeled training data [26]. Very importantly, a standard cGAN is never designed for the class imbalanced scenarios. In anomaly detection, labeled anomalies are rarities and the distribution of the available data is highly skewed. Therefore anomaly detection presents major challenges for a common cGAN. In this paper, we present a supervised anomaly detection method through designing a cGAN featuring an ensemble of discriminators that are trained based on a novel ensemble active learning strategy. At the heart of our new method is the ensemble active learning generative adversarial network (EALGAN) which has the following novel features and advantages: Unique Network Architecture . EALGAN is a conditional GAN featuring a unique one generator vs.multiple discriminators architecture. Each discriminator consists of two classiÔ¨Åers simultaneously performs two classiÔ¨Å cation tasks: (i) An adversarial classiÔ¨Åer performs real and generated data classiÔ¨Åcation, and (ii) an auxiliary classiÔ¨Åer performs anomaly and normal data classiÔ¨Åca tion. An ensemble learning algorithm is developed to train the multiple discriminators to overcome the class imbalanced issue. The auxiliary classiÔ¨Åer acts as an active learning sampler as well as the anomaly detector. Novel Ensemble Learning Loss function . A novel ensemble learning loss function is designed to ensure that the ensemble of discriminators complementing each other. Each discriminator adaptively focuses on the samples wrongly classiÔ¨Åed by the others thus overcoming the problem of bias classiÔ¨Åcation towards the classes with larger number of samples. Active Ensemble Learning Reduces the Cost of Labeling Data . An active sampling strategy is incorporated into the EALGAN framework. The active sampling strategy progressively selects the data carrying the most infor mation but accounting for relatively small proportion of the available real data to not only optimize the discriminator ensemble, but also guide the generator to produce sufÔ¨Åciently balanced fake data. The selected real data and the generated fake data are used to train the discriminators thus signiÔ¨Åcantly reducing the cost of annotating anomaly data for training a fully supervised detector. Empirical results show that in a batchwise training procedure, EALGAN can provide stateoftheart performance by only labeling 5% of the real data in each batch. MultiDiscriminator Ensemble Learning Enhances Regular ization and Generalization . In the proposed EALGAN framework, the discriminator is designed as a multi task neural network. By effectively incorporating mul tiple discriminators into one ensemble with a novel ensemble learning loss function, the EALGAN shows enhanced capability to resist overÔ¨Åtting. This makes it easily to obtain an anomaly detector with optimal generalization or determine the stop node of GAN training by choosing the model with the best empirical performance (please see Fig. 2 and Section 3.3) The EALGAN is a High Quality cGAN . The unique ar chitecture and innovative ensemble learning algorithm have ensured that EALGAN is a highquality condi tional data generator. This is demonstrated by the factthat anomaly detectors trained with only the generated data can be as good as those trained with the real data and that mixing generated data with the real data can further improve performances. State of the Art Performances . Extensive experiments have been carried out on 20 widely used real world anomaly detection benchmark datasets and a variety of synthetic datasets. Performances are compared to 9 stateofthe art anomaly detection methods from 6 categories in the literature. The new EALGAN method consistently outperforms the best methods available, often by signif icant margins, achieving AUC performance improve ments range from 5.7% over the best deep learning detector, to 16.4% over the best traditional detector, and to 16.8% over the best ensemble detector; and achieving Gmean performance improvements range from 37.9% over the best deep learning detector, to 49.2% over the best ensemble detector, and to 52.5% over the best traditional detector. 2 R ELATED WORK "
86,A new semi-supervised self-training method for lung cancer prediction.txt,"Background and Objective: Early detection of lung cancer is crucial as it has
high mortality rate with patients commonly present with the disease at stage 3
and above. There are only relatively few methods that simultaneously detect and
classify nodules from computed tomography (CT) scans. Furthermore, very few
studies have used semi-supervised learning for lung cancer prediction. This
study presents a complete end-to-end scheme to detect and classify lung nodules
using the state-of-the-art Self-training with Noisy Student method on a
comprehensive CT lung screening dataset of around 4,000 CT scans.
  Methods: We used three datasets, namely LUNA16, LIDC and NLST, for this
study. We first utilise a three-dimensional deep convolutional neural network
model to detect lung nodules in the detection stage. The classification model
known as Maxout Local-Global Network uses non-local networks to detect global
features including shape features, residual blocks to detect local features
including nodule texture, and a Maxout layer to detect nodule variations. We
trained the first Self-training with Noisy Student model to predict lung cancer
on the unlabelled NLST datasets. Then, we performed Mixup regularization to
enhance our scheme and provide robustness to erroneous labels.
  Results and Conclusions: Our new Mixup Maxout Local-Global network achieves
an AUC of 0.87 on 2,005 completely independent testing scans from the NLST
dataset. Our new scheme significantly outperformed the next highest performing
method at the 5% significance level using DeLong's test (p = 0.0001). This
study presents a new complete end-to-end scheme to predict lung cancer using
Self-training with Noisy Student combined with Mixup regularization. On a
completely independent dataset of 2,005 scans, we achieved state-of-the-art
performance even with more images as compared to other methods.","The prevalence of lung cancer is increasing yearly and with the highest mortality rate  among other types of cancer [1]. It is estimated that 228,820  new lung cancer cases will be  diagnosed in the year 2020 in the United States  [2]. With the advance of medical a nd surgical  treatment shifting towards precision medicine, early detection of the disease will significantly  improve patients‚Äô survival rate.    To date, most researchers have successfully developed methods to detect or classify lung  nodules independently. However, there are only relatively few methods that can simultaneously  detect and classify nodules from computed tomography (CT) scans [3]. To address  this  shortcoming, we present  in this study  an end toend scheme  to detect and classif y lung nodules  in order to predict lung cancer, which is evaluated on a comprehensive CT lung screeni ng dataset  of around 4,000 CT scans.   A general  difficulty in the medical imaging field is the unavailability of sufficient  labelled /annotated data to validate new deep learning methods  [4]. Relatively small annotated  datasets including LUNA16 and LIDC IDRI  [5,6]  are typically used for lung nodule detection  or classification. The LUNA16 and LIDC IDRI datasets contain detailed annotated locations of  each lung nodule within the CT scans. The National Lung Screening Trial (NLST) lung  screening dataset [7], released by the National Cancer Institute (NCI), is a vast resource of lung  screening CT scans; however, a downside of the NLST dataset is that the ground truth data only  provides a final diagnosis of cancer (i.e., 1) or cancer free (i.e., 0) for each pa tient and does not  provide the labelled/annotated locations of the nodules within the CT scans.   To overcome this deficiency in the NLST dataset, we use Self training with Noisy  Student Training [8] to leverage on the unlabelled data/nodules  to the b est of our knowledge  this has not been done before. Self training has produced state oftheart results on image  classification in ImageNet [8]. Using the self training method, a trained teacher model will  generate pseudo  labels on unlabelled images. T hen, a student model is trained on both labelled  and pseudo labelled images, and noise is simultaneously added to improve the student‚Äôs learning  capacity. In our self training framework, we use our state oftheart Local Global network [9] to  classify unlabelled nodules in the NLST dataset and train a new student model with noise on the  pseudo labelled data. Similar to the results obtained in the original self training paper [8], we  obtained significant improvement in our lung nod ule classification scheme in terms of the area  under the receiver operating characteristic curve (AUC) and we obtained state oftheart results  for lung canc er predic tion on the NLST dataset.   Our main contributions are as follows:  1. We develop a new automati c end toend scheme that simultaneously detects and  classifies  lung nodules to predict  lung cancer from CT scans.   2. We implement the first Selftraining with Noisy Student model for lung cancer prediction  on the unlabelled NLST dataset and show that the results generalize to big independent  datasets.   3. We enhance our state oftheart lung nodule classification scheme, namely the Local  Global network , by implementing the Mixup technique to improve final model  performance  and including  a Maxout Layer to classify  large variations of lung nodules .  4. We train and  develop our new scheme on around 4,000 CT scans and achieve state of theart performance  even with more  images  as compared to other studies .     2. Related work  "
87,Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective.txt,"Knowledge distillation is an effective approach to leverage a well-trained
network or an ensemble of them, named as the teacher, to guide the training of
a student network. The outputs from the teacher network are used as soft labels
for supervising the training of a new network. Recent studies
\citep{muller2019does,yuan2020revisiting} revealed an intriguing property of
the soft labels that making labels soft serves as a good regularization to the
student network. From the perspective of statistical learning, regularization
aims to reduce the variance, however how bias and variance change is not clear
for training with soft labels. In this paper, we investigate the bias-variance
tradeoff brought by distillation with soft labels. Specifically, we observe
that during training the bias-variance tradeoff varies sample-wisely. Further,
under the same distillation temperature setting, we observe that the
distillation performance is negatively associated with the number of some
specific samples, which are named as regularization samples since these samples
lead to bias increasing and variance decreasing. Nevertheless, we empirically
find that completely filtering out regularization samples also deteriorates
distillation performance. Our discoveries inspired us to propose the novel
weighted soft labels to help the network adaptively handle the sample-wise
bias-variance tradeoff. Experiments on standard evaluation benchmarks validate
the effectiveness of our method. Our code is available at
\url{https://github.com/bellymonster/Weighted-Soft-Label-Distillation}.","For deep neural networks (Goodfellow et al., 2016), knowledge distillation (KD) (Ba & Caruana, 2014; Hinton et al., 2015) refers to the technique that uses welltrained networks to guide the train ing of another network. Typically, the welltrained network is named as the teacher network while the network to be trained is named as the student network. For distillation, the predictions from the teacher network are leveraged and referred to as the soft labels (Balan et al., 2015; M ¬®uller et al., 2019). Soft labels generated by the teacher network have been proven effective in largescale em pirical studies (Liang et al., 2019; Tian et al., 2020; Zagoruyko & Komodakis, 2017; Romero et al., 2015) as well as recent theoretical studies (Phuong & Lampert, 2019). However, the reason why soft labels are beneÔ¨Åcial to the student network is still not well explained. Giving a clear theoretical explanation is challenging: The optimization details of a deep network with the common onehot labels are still not wellstudied (Nagarajan & Kolter, 2019), not to mention training with the soft labels. Nevertheless, two recent studies (M ¬®uller et al., 2019; Yuan et al., 2020) shed light on the intuitions about how the soft labels work. SpeciÔ¨Åcally, label smoothing, which is a special case of soft labels based training, is shown to regularize the activations of the penultimate layer to the network (M ¬®uller et al., 2019). The regularization property of soft labels is further explored in (Yuan et al., 2020). They hypothesize that in KD, one main reason why the soft labels work is the regularization introduced by soft labels. Based on the assumption, the authors These authors contributed equally to this work. yWork done while the author was a research intern at Horizon Robotics. 1arXiv:2102.00650v1  [cs.LG]  1 Feb 2021Published as a conference paper at ICLR 2021 design a teacherfree distillation method by turning the predictions of the student network into soft labels. Considering that soft labels are targets for distillation, the evidence of the regularization brought by soft labels drives us to rethink soft labels for KD: Soft labels are both supervisory signals and regularizers. Meanwhile, it is known that there is a tradeoff between Ô¨Åtting the data and imposing regularizations, i.e., the biasvariance dilemma (Kohavi & Wolpert, 1996; Bishop, 2006), but it is unclear how bias and variance change for distillation with soft labels. Since the biasvariance tradeoff is an important issue in statistical learning, we investigate whether the biasvariance tradeoff exists for soft labels and how the tradeoff affects distillation performance. We Ô¨Årst compare the bias and variance decomposition of direct training with that of distillation with soft labels, noticing that distillation results in a larger bias error and a smaller variance. Then, we rewrite distillation loss into the form of a regularization loss adding the direct training loss. Through inspecting the gradients of the two terms during training, we notice that for soft labels, the bias variance tradeoff varies samplewisely. Moreover, by looking into a conclusion from (M ¬®uller et al., 2019), we observe that under the same temperature setting, the distillation performance is nega tively associated with the number of some certain samples. These samples lead to bias increase and variance decrease and we name them as regularization samples. To investigate how regularization samples affect distillation, we Ô¨Årst examine if we can design ad hoc Ô¨Ålters for soft labels to avoid training with regularization samples. But completely Ô¨Åltering out regularization samples also de teriorates distillation performance, leading us to speculate that regularization samples are not well handled by standard KD. In the light of these Ô¨Åndings, we propose weighted soft labels for distil lation to handle the samplewise biasvariance tradeoff, by adaptively assigning a lower weight to regularization samples and a larger weight to the others. To sum up, our contributions are: ‚Ä¢ For knowledge distillation, we analyze how the soft labels work from a perspective of bias variance tradeoff. ‚Ä¢ We discover that the biasvariance tradeoff varies samplewisely. Also, we discover that if we Ô¨Åx the distillation temperature, the number of regularization samples is negatively associated with the distillation performance. ‚Ä¢ We design straightforward schemes to alleviate negative impacts from regularization sam ples and then propose the novel weighted soft labels for distillation. Experiments on large scale datasets validate the effectiveness of the proposed weighted soft labels. 2 R ELATED WORKS "
88,Grounded Recurrent Neural Networks.txt,"In this work, we present the Grounded Recurrent Neural Network (GRNN), a
recurrent neural network architecture for multi-label prediction which
explicitly ties labels to specific dimensions of the recurrent hidden state (we
call this process ""grounding""). The approach is particularly well-suited for
extracting large numbers of concepts from text. We apply the new model to
address an important problem in healthcare of understanding what medical
concepts are discussed in clinical text. Using a publicly available dataset
derived from Intensive Care Units, we learn to label a patient's diagnoses and
procedures from their discharge summary. Our evaluation shows a clear advantage
to using our proposed architecture over a variety of strong baselines.","The ability of recurrent neural networks to model sequential data and capture longterm dependencies makes them powerful tools for natural language processing. These models maintain a state at each time step, representing the relevant history and taskspeciÔ¨Åc beliefs. Based on the current value of this recurrent state and a new input, the state is updated at each time step. Recurrent models have become a popular choice for a variety of natural language processing tasks such as language modeling [Mikolov et al., 2010], text classiÔ¨Åcation [Graves, 2012], or machine translation [Cho et al., 2014a]. The success of this paradigm has been driven in great part by a number of structural innovations since the original version of Elman [1990]. Recurrent cells such as the Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] or Gated Recurrent Units (GRU) [Cho et al., 2014b], for example, alleviate the problem of vanishing gradients [Bengio et al., 1994]. Attention mechanisms [Bahdanau et al., 2014] and Memory Networks [Sukhbaatar et al., 2015] have also signiÔ¨Åcantly increased the expressiveness of recurrent architectures, revealing their potential to tackle more complex tasks such as question answering [Rajpurkar et al., 2016]. One notable property of these models, however, is that they often require signiÔ¨Åcant amounts of training data to perform at their best [Bajgar et al., 2016], which can limit their application domain. In this work, we focus on developing recurrent models for the task of extracting medical concepts from Intensive Care Unit discharge summaries. This is a multiclass, multilabel text classiÔ¨Åcation task with a target vocabulary of several thousand concepts. Given the difÔ¨Åculty to obtain very large medical datasets, there is a need to come up with new, more dataefÔ¨Åcient architectures. To this end, we introduce the Grounded Recurrent Neural Network (GRNN). At a high level, we ground the model‚Äôs hidden state by introducing dimensions whose sole purpose is to track the model‚Äôs belief in the presence of speciÔ¨Åc labels for the current example. We Ô¨Ånd that this addition aids optimization, and outperforms standard recurrent models for text classiÔ¨Åcation. Although each new label adds to the hidden state size, we impose a semi diagonal constraint on the recurrent transition matrices, so that the size of our model grows linearly with the number of labels. We show that this not only lets the model scale with the number of labels, but also helps with optimization. Furthermore, our approach leads to increased interpretability, which is especially appreciated in medical applications. Indeed, even though we do not provide our model with the location of phrases of interest for a labelarXiv:1705.08557v1  [stat.ML]  23 May 2017at training time, we can track changes in the grounded dimensions tied to a speciÔ¨Åc concept as a document is read, indicating evidence in text for or against its presence when the model‚Äôs belief changes drastically. We evaluate our model on the publicly available MIMIC datasets [Saeed et al., 2011, Johnson et al., 2016] to predict ICD9 (International ClassiÔ¨Åcation of Diseases [Organization et al., 1978]) codes given a patient‚Äôs discharge summary text [Perotte et al., 2014, Lita et al., 2008]. These codes are usually determined by humans perusing health records and selecting relevant codes from very long lists. Due to the high number of ICD9 codes, there is signiÔ¨Åcant human error, arising from the cognitive load of such a task [BirmanDeych et al., 2005, Hsia et al., 1988] and differences in human judgment [Pestian et al., 2007]. The effort needed, the errors in the coding process and the inconsistency of labeling can be mitigated through automatically detecting concepts in text or offering suggestions as smarter autocomplete, which motivates our contribution. We also show our model‚Äôs performance on a tag prediction dataset built from StackOverÔ¨Çow data. Section 2 presents relevant previous work, Section 3 describes the model, and we present experimental results in Section 4. Section 5 concludes and outlines possible future research directions. 2 Related Work "
89,Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning.txt,"Learning with noisy-labels has become an important research topic in computer
vision where state-of-the-art (SOTA) methods explore: 1) prediction
disagreement with co-teaching strategy that updates two models when they
disagree on the prediction of training samples; and 2) sample selection to
divide the training set into clean and noisy sets based on small training loss.
However, the quick convergence of co-teaching models to select the same clean
subsets combined with relatively fast overfitting of noisy labels may induce
the wrong selection of noisy label samples as clean, leading to an inevitable
confirmation bias that damages accuracy. In this paper, we introduce our
noisy-label learning approach, called Asymmetric Co-teaching (AsyCo), which
introduces novel prediction disagreement that produces more consistent
divergent results of the co-teaching models, and a new sample selection
approach that does not require small-loss assumption to enable a better
robustness to confirmation bias than previous methods. More specifically, the
new prediction disagreement is achieved with the use of different training
strategies, where one model is trained with multi-class learning and the other
with multi-label learning. Also, the new sample selection is based on
multi-view consensus, which uses the label views from training labels and model
predictions to divide the training set into clean and noisy for training the
multi-class model and to re-label the training samples with multiple top-ranked
labels for training the multi-label model. Extensive experiments on synthetic
and real-world noisy-label datasets show that AsyCo improves over current SOTA
methods.","Deep neural network (DNN) has achieved remarkable success in many Ô¨Åelds, including computer vision [ 15,11], natural language processing (NLP) [ 7,35] and medical im age analysis [ 17,28]. However, the methods from those Decoupling Coteaching+ JoCoR AsyCo A A AB B B!= != !=A A AB B B!= != !=A A AB B BA A AB B Bbatch/epoch 1 batch/epoch 2 batch/epoch 3Figure 1. Comparison of methods Decoupling [ 20], Co teaching+ [ 36], JoCoR [ 29], and our AsyCo. AsyCo coteaches the multiclass model A and the multilabel model B with differ ent training strategies (denoted by the different colours of A&B). The training samples for A and B, represented by the green and red arrows, are formed by our proposed multiview consensus that uses label views from the training set and model predictions to estimate the variables wandÀÜy, which selects clean/noisy sam ples for training A and iteratively relabels samples for training B, respectively. Ô¨Åelds often require massive amount of highquality anno tated data for supervised training [ 6], which is challenging and expensive to acquire. To alleviate such problem, some datasets have been annotated via crowdsourcing [ 32], from search engines [ 27], or with NLP from radiology reports [ 28]. Although these cheaper annotation processes enable the con struction of largescale datasets, they inevitably introduce noisy labels for model training, resulting in DNN model per formance degradation. Therefore, novel learning algorithms are required to robustly train DNN models when training sets containing noisy labels. Previous methods tackle noisylabel learning from differ ent perspectives. For example, some approaches focus on prediction disagreement [36,29,20], which rely on jointly training two models to update their parameters when they disagree on the predictions of the same training samples. These two models generally use the same training strategy, so even though they are trained using samples with diver 1arXiv:2301.01143v1  [cs.CV]  1 Jan 2023gent predictions, both models will quickly converge to select similar clean samples during training, which neutralises the effectiveness of prediction disagreement. Other noisylabel learning methods are based on sample selection [16,9,1] to Ô¨Ånd clean and noisylabel samples that are treated differ ently in the training process. Sampleselection approaches usually assume that samples with small training losses are associated with clean labels, which is an assumption veri Ô¨Åed only at early training stages [ 18,37]. However, such assumption is unwarranted in later training stages because DNN models can overÔ¨Åt any type of noisy label after a cer tain number of epochs, essentially reducing the training loss for all training samples. Stateoftheart (SOTA) noisylabel learning approaches [ 16] have been designed to depend on both prediction disagreement and sample selection meth ods to achieve better performance than either method alone. Nevertheless, these SOTA methods are still affected by the fast convergence of both models and label noise overÔ¨Åtting, which raises the following questions: 1) Are there more effective ways to maximise the prediction disagreement be tween both models, so they consistently produce divergent results during the training procedure? 2) Is there a sam ple selection approach that can better integrate prediction disagreements than the small loss strategy? Motivated by traditional multiview learning [ 3,26] and multilabel learning [ 24], we propose a new noisylabel learn ing method that aims to answer the two questions above. Our method, named Asymmetric Coteaching (AsyCo) and de picted in Fig. 1, is based on two models trained with different learning strategies to maximise their prediction disagreement. One model, the classiÔ¨Åcation net , is trained with conven tional multiclass learning by minimising a cross entropy loss and provide singleclass prediction, and the other, the reference net , is trained with a binary cross entropy loss to enable multilabel learning that is used to estimate the top ranked labels that represent the potentially clean candidate labels for each training sample. The original training labels and the predictions by the training and reference nets enable the formation of three label views for each training sample, allowing us to formulate the multiview consensus that is tightly integrated with the prediction disagreement to select clean and noisy samples for training the multiclass model and to iteratively relabel samples with multiple topranked labels for training the multilabel model. In summary, our main contributions are: ‚Ä¢The new noisylabel coteaching method AsyCo de signed to maximise the prediction disagreement be tween the training of a multiclass and a multilabel model; and ‚Ä¢The novel multiview consensus that uses the disagree ments between training labels and model predictions to select clean and noisy samples for training the multiclass model and to iteratively relabel samples with multiple topranked labels for training the multilabel model. We conduct extensive experiments on both synthetic and realworld noisy datasets that show that AsyCo provides sub stantial improvements over previous stateoftheart (SOTA) methods. 2. Related Work "
90,Neural Ensemble Search via Bayesian Sampling.txt,"Recently, neural architecture search (NAS) has been applied to automate the
design of neural networks in real-world applications. A large number of
algorithms have been developed to improve the search cost or the performance of
the final selected architectures in NAS. Unfortunately, these NAS algorithms
aim to select only one single well-performing architecture from their search
spaces and thus have overlooked the capability of neural network ensemble
(i.e., an ensemble of neural networks with diverse architectures) in achieving
improved performance over a single final selected architecture. To this end, we
introduce a novel neural ensemble search algorithm, called neural ensemble
search via Bayesian sampling (NESBS), to effectively and efficiently select
well-performing neural network ensembles from a NAS search space. In our
extensive experiments, NESBS algorithm is shown to be able to achieve improved
performance over state-of-the-art NAS algorithms while incurring a comparable
search cost, thus indicating the superior performance of our NESBS algorithm
over these NAS algorithms in practice.","Recent years have witnessed a surging interest in design ing wellperforming architectures for different tasks. These architectures are typically manually designed by human experts, which requires numerous trials and errors during this manual design process and therefore is prohibitively costly. Consequently, the increasing demand for develop ing wellperforming architectures in different tasks makes this manual design infeasible. To avoid such human efforts, Zoph and Le [2017] have introduced neural architecture search (NAS) to help automate the design of architectures.Since then, a number of NAS algorithms [Pham et al., 2018, Liu et al., 2019, Chen et al., 2019] have been developed to improve the search efÔ¨Åciency (i.e., search cost) or the search effectiveness (i.e., generalization performance of their Ô¨Ånal selected architectures) in NAS. However, conventional NAS algorithms aim to select only one single architecture from their search spaces and have thus overlooked the capability of other candidate architec tures from the same search spaces in helping improve the performance achieved by their Ô¨Ånal selected single architec ture. That is, neural network ensembles are widely known to be capable of achieving an improved performance com pared with a single neural network in practice [Cortes et al., 2017, Gal and Ghahramani, 2016, Lakshminarayanan et al., 2017]. This naturally begs the question: How to select best performing neural network ensembles with diverse archi tectures from a NAS search space in order to improve the performances achieved by existing NAS algorithms? To the best of our knowledge, only limited efforts (e.g., [Zaidi et al., 2021]) have been devoted to this problem in the NAS lit erature. Unfortunately, the neural ensemble search (NES) algorithm based on random search or evolutionary algorithm in [Zaidi et al., 2021] requires excessive search costs to se lect their Ô¨Ånal neural network ensembles, which will not be affordable in resourceconstrained scenarios. To this end, this paper introduces a novel algorithm, namely neural ensemble search via Bayesian sampling (NESBS), to effectively and efÔ¨Åciently select the wellperforming neural network ensemble with diverse architectures from a search space. We Ô¨Årstly represent the search space as a supernet fol lowing conventional oneshot NAS algorithms and then use the model parameters inherited from this supernet after its model training to estimate the singlemodel performances and also the ensemble performance of independently trained architectures (Sec. 3.1). Next, since both singlemodel per formances and diverse model predictions affect the Ô¨Ånal en semble performance according to [Zhou, 2012], we propose to use a variational posterior distribution of architectures based on a trained supernet to characterize these two factors, Accepted for the 38thConference on Uncertainty in ArtiÔ¨Åcial Intelligence (UAI 2022).arXiv:2109.02533v2  [cs.LG]  17 Jun 2022i.e., singlemodel performances and diverse model predic tions (Sec. 3.2). We then introduce two novel Bayesian sampling algorithms based on the posterior distribution of architectures, i.e., Monte Carlo sampling (MC Sampling) andStein Variational Gradient Descent with regularized di versity (SVGDRD), to effectively and efÔ¨Åciently select en sembles with both competitive singlemodel performances and compelling diverse model predictions (Sec. 3.3), which is also guaranteed to be able to achieve impressive ensem ble performances [Zhou, 2012]. Lastly, we use extensive experiments to show that our NESBS algorithm is indeed able to select wellperforming neural network ensembles effectively and efÔ¨Åciently in practice (Sec. 4). 2 RELATED WORKS & BACKGROUND "
91,MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection.txt,"This paper describes our system for SemEval-2021 Task 5 on Toxic Spans
Detection. We developed ensemble models using BERT-based neural architectures
and post-processing to combine tokens into spans. We evaluated several
pre-trained language models using various ensemble techniques for toxic span
identification and achieved sizable improvements over our baseline fine-tuned
BERT models. Finally, our system obtained a F1-score of 67.55% on test data.","Toxic speech has become a rising issue for social media communities. Abusive content is very di verse and therefore offensive language and toxic speech detection is not a trivial issue. Besides, so cial media moderation of lengthy comments and posts is often a timeconsuming process. In this regard, the task of detecting toxic spans in social media texts deserves close attention. This work is based on the participation of our team, named MIPTNSUUTMN, in Se mEval 2021 Task 5, ‚ÄúToxic Spans Detection‚Äù (Pavlopoulos et al. ,2021 ). Organizers of the shared task provided participants with the trial, train, and test sets of English social media comments annotated at the span level indicating the presence or absence of text toxicity. We formulated the task as a token classiÔ¨Åcation problem and investigated several BERTbased models using twostep knowledge transfer. We found that preliminary Ô¨Ånetuning of the model on data that is close to the target domain im proves the quality of the token classiÔ¨Åcation. The source code of our models is available at https://github.com/morozowdmitry/semeval21 . The paper is organized as follows. A brief re view of related work is given in Section 2. The def inition of the task has been summarized in Section3. The proposed methods and experimental set tings have been elaborated in Section 4. Section 5 contains the results and error analysis respectively. Section 6 is a conclusion. 2 Related Work "
92,Inverse Graph Identification: Can We Identify Node Labels Given Graph Labels?.txt,"Graph Identification (GI) has long been researched in graph learning and is
essential in certain applications (e.g. social community detection).
Specifically, GI requires to predict the label/score of a target graph given
its collection of node features and edge connections. While this task is
common, more complex cases arise in practice---we are supposed to do the
inverse thing by, for example, grouping similar users in a social network given
the labels of different communities. This triggers an interesting thought: can
we identify nodes given the labels of the graphs they belong to? Therefore,
this paper defines a novel problem dubbed Inverse Graph Identification (IGI),
as opposed to GI. Upon a formal discussion of the variants of IGI, we choose a
particular case study of node clustering by making use of the graph labels and
node features, with an assistance of a hierarchical graph that further
characterizes the connections between different graphs. To address this task,
we propose Gaussian Mixture Graph Convolutional Network (GMGCN), a simple yet
effective method that makes the node-level message passing process using Graph
Attention Network (GAT) under the protocol of GI and then infers the category
of each node via a Gaussian Mixture Layer (GML). The training of GMGCN is
further boosted by a proposed consensus loss to take advantage of the structure
of the hierarchical graph. Extensive experiments are conducted to test the
rationality of the formulation of IGI. We verify the superiority of the
proposed method compared to other baselines on several benchmarks we have built
up. We will release our codes along with the benchmark data to facilitate more
research attention to the IGI problem.","In many scenarios, the objects with their features are connected by their interactions as graphs. By analyzing these edge connections and node features throughout various graphs, a graph identiÔ¨Åcation (GI) problem is formed to predict the information or properties of the graphs, such as labels or scores of the target graphs. Many studies have been developed on GI, such as graph classiÔ¨Åcation [ 1,2] and Cocorresponding authors. Preprint. Under review.arXiv:2007.05970v1  [cs.LG]  12 Jul 2020graph regression [ 3]. Formally, methods for GI aggregates the information from nodes and edges to predict or summarize the information of the whole graph. However, it is much more interesting to consider an inverse problem which has never been proposed: Can we use the information of graphs to infer the information of nodes or even edges and subgraphs? In another word, given the labels of graphs, how to Ô¨Ågure out the categories of nodes, edges, or subgraphs? This problem is interesting and very common in the real world. In social media, for example, thinking of hot events that are widely discussed in the form of graphs where users involved as nodes and the cofollowing relationship among users as edges, it is interesting to think how to pick out the malicious users from the mass of users with only the labels of these event topics such as the authenticity of each topic [ 4,5]. In drug discovery, for another example, thinking of molecules as graphs in which atoms as nodes and chemical bonds as edges, we attempt to identify the roles of certain subgraphs, or equivalently functional groups, in each molecular given its chemical or physical properties [6, 7]. In a programming language, for the last example, thinking of control Ô¨Çows for programs as graphs by considering statements as nodes and control Ô¨Çows as edges, can we detect the problematic statements if we have already known which program has bug or not [8]? All these problems can be deÔ¨Åned as a general problem as Inverse Graph IdentiÔ¨Åcation (IGI) that identiÔ¨Åes the nodes in graphs based on the information of graphs. The main difÔ¨Åculty of the IGI task is that the node labels are inaccessible so that all available information for the training model only comes from the labels of graphs. Namely, it is a node identiÔ¨Åcation task where the available training labels are much coarsergrained than the node labels we want to Ô¨Åt. It seems this problem can be resolved from the present perspective of node clustering [ 9] or node identiÔ¨Åcation [ 10]. However, they are different from the problems that we attempt to address. Unlike node clustering which only conducts clustering based on node features or graph structures, IGI is better guaranteed by the given label information of graphs, which, we will demonstrate, closely inÔ¨Çuences the clustering results. It is also different from the node identiÔ¨Åcation task because IGI does not contain any node labels for training. Another similar concept to our IGI is MultipleInstance Learning (MIL) [ 11] that adopts global labels of bags to identify the labels of local instances. However, MIL assumes that each instance is i.i.d. and there are no edge connections involved. Therefore, it is interesting and important to study the IGI problem as a set of new challenges and seek new solutions for it. In this paper, we formally deÔ¨Åne IGI and address out its different problem statements as a set of new challenges. Meanwhile, we focus on a particular study of IGI: A node clustering task by making use of graph labels and node features with an assistance of a hierarchical graph [ 9] that further characterizes the relations among graphs. To address this particular task, we propose a novel model based on Gaussian mixture model (GMM) and graph convolutional network (GCN) named as Gaussian Mixture Graph Convolutional Network (GMGCN). First, the features of each node are updated through the Graph Attention Network (GAT). Then the node features are aggregated by a Gaussian mixture layer (GML) and a new attention pooling layer proposed in the paper. After obtaining the graph representations, we adopt a hierGCN to classify the graphs. SpeciÔ¨Åcally, we design a consensus loss that plays a key role in the training process. Finally, a node clustering is carried out according to the parameters of GML. The main contributions are as follows: 1. New problem: We introduce a new problem called Inverse Graph IdentiÔ¨Åcation (IGI), which tries to identify the nodes in graphs based on the labels of graphs, and we take a formal discussion of the variants of IGI to attract more research attention on this problem. 2. New solution: We propose an effective model called GMGCN based on GMM and GCN to solve a particular study of IGI problem. To the best of our knowledge, this is the Ô¨Årst work to achieve node clustering in graph structure by integrating GMM into GCN. 3. New loss: We propose a consensus loss function to boost the model training through the principle of ""same attraction, opposite repulsion"". Experiments validate that this consensus loss greatly improves the model effect. We validate the proposed GMGCN on various synthetic datasets based on different problem statements and a realworld dataset. The results demonstrate that the proposed method is suitable to solve the IGI task. 2 Related Works "
93,Neural Paraphrase Identification of Questions with Noisy Pretraining.txt,"We present a solution to the problem of paraphrase identification of
questions. We focus on a recent dataset of question pairs annotated with binary
paraphrase labels and show that a variant of the decomposable attention model
(Parikh et al., 2016) results in accurate performance on this task, while being
far simpler than many competing neural architectures. Furthermore, when the
model is pretrained on a noisy dataset of automatically collected question
paraphrases, it obtains the best reported performance on the dataset.","Question paraphrase identiÔ¨Åcation is a widely use ful NLP application. For example, in questionand answer (QA) forums ubiquitous on the Web, there are vast numbers of duplicate questions. Identi fying these duplicates and consolidating their an swers increases the efÔ¨Åciency of such QA forums. Moreover, identifying questions with the same se mantic content could help Webscale question an swering systems that are increasingly concentrating on retrieving focused answers to users‚Äô queries. Here, we focus on a recent dataset published by the QA website Quora.com containing over 400K annotated question pairs containing binary para phrase labels.1We believe that this dataset presents a great opportunity to the NLP research commu nity and practitioners due to its scale and quality; it can result in systems that accurately identify dupli cate questions, thus increasing the quality of many QA forums. We examine a simple model family, thedecomposable attention model of Parikh et al. (2016), that has shown promise in modeling natural 1See https://data.quora.com/FirstQuoraDatasetRelease QuestionPairs.language inference and has inspired recent work on similar tasks (Chen et al., 2016; Kim et al., 2017). We present two contributions. First, to mitigate data sparsity, we modify the input representation of the decomposable attention model to use sums of character ngram embeddings instead of word embeddings. We show that this model trained on the Quora dataset produces comparable or better results with respect to several complex neural ar chitectures, all using pretrained word embeddings. Second, to signiÔ¨Åcantly improve our model perfor mance, we pretrain allour model parameters on the noisy, automatically collected questionparaphrase corpus Paralex (Fader et al., 2013), followed by Ô¨Ånetuning the parameters on the Quora dataset. This twostage training procedure achieves the best result on the Quora dataset to date, and is also sig niÔ¨Åcantly better than learning only the character ngram embeddings during the pretraining stage. 2 Related Work "
94,Semi-Supervised Noisy Student Pre-training on EfficientNet Architectures for Plant Pathology Classification.txt,"In recent years, deep learning has vastly improved the identification and
diagnosis of various diseases in plants. In this report, we investigate the
problem of pathology classification using images of a single leaf. We explore
the use of standard benchmark models such as VGG16, ResNet101, and DenseNet 161
to achieve a 0.945 score on the task. Furthermore, we explore the use of the
newer EfficientNet model, improving the accuracy to 0.962. Finally, we
introduce the state-of-the-art idea of semi-supervised Noisy Student training
to the EfficientNet, resulting in significant improvements in both accuracy and
convergence rate. The final ensembled Noisy Student model performs very well on
the task, achieving a test score of 0.982.","Plant pathology classiÔ¨Åcation is a very challenging and important task. In 2019, more than 20 %of all global crop productions were lost due to pests and other pathogens. This results in massive economic losses and food deÔ¨Åcits in many parts of the world. Therefore, it is very important to be able to develop systems that can detect such diseases. Such systems will have a signiÔ¨Åcant impact on the agricul tural sector, especially in areas where agriculture is a pri mary food source for millions of people. Figure 1: Examples of fungal diseases in apples [22]Currently, plant disease diagnosis is mostly done man ually by human examination. However, this is inefÔ¨Åcient, as it is tedious, timeconsuming, and errorprone. Recently, deep learning and computer vision techniques have shown great promise in this task. These systems have the ability to quickly and reliably make predictions, and have greatly contributed to plant classiÔ¨Åcation in the past few years. However, there are still many challenges. For instance, im ages of different leaves may look very similar with each other. Meanwhile, two images from the same leaf species may have very different images. Plant disease symptoms may also manifest in different ways due to age of infected tissues, genetic variations, and light conditions. In this report, we examine the use of pretrained deep learning models in plant pathology classiÔ¨Åcation. More speciÔ¨Åcally, we examine the stateoftheart EfÔ¨ÅcientNet [21] and a semisupervised approach called Noisy Student training [24]. We demonstrate that the Noisy Student EfÔ¨Å cientNet performs very well on our dataset, and it has sig niÔ¨Åcant improvements over previous attempts at the task. 2. Related Work "
95,SSR: An Efficient and Robust Framework for Learning with Unknown Label Noise.txt,"Despite the large progress in supervised learning with neural networks, there
are significant challenges in obtaining high-quality, large-scale and
accurately labelled datasets. In such a context, how to learn in the presence
of noisy labels has received more and more attention. As a relatively complex
problem, in order to achieve good results, current approaches often integrate
components from several fields, such as supervised learning, semi-supervised
learning, transfer learning and resulting in complicated methods. Furthermore,
they often make multiple assumptions about the type of noise of the data. This
affects the model robustness and limits its performance under different noise
conditions. In this paper, we consider a novel problem setting, Learning with
Unknown Label Noise}(LULN), that is, learning when both the degree and the type
of noise are unknown. Under this setting, unlike previous methods that often
introduce multiple assumptions and lead to complex solutions, we propose a
simple, efficient and robust framework named Sample Selection and
Relabelling(SSR), that with a minimal number of hyperparameters achieves SOTA
results in various conditions. At the heart of our method is a sample selection
and relabelling mechanism based on a non-parametric KNN classifier~(NPK) $g_q$
and a parametric model classifier~(PMC) $g_p$, respectively, to select the
clean samples and gradually relabel the noisy samples. Without bells and
whistles, such as model co-training, self-supervised pre-training and
semi-supervised learning, and with robustness concerning the settings of its
few hyper-parameters, our method significantly surpasses previous methods on
both CIFAR10/CIFAR100 with synthetic noise and real-world noisy datasets such
as WebVision, Clothing1M and ANIMAL-10N. Code is available at
https://github.com/MrChenFeng/SSR_BMVC2022.","It is now commonly accepted that supervised learning with deep neural networks can provide excellent solutions for a wide range of problems, so long as there is sufÔ¨Åcient availability of labelled training data and computational resources. However, these results have been mostly obtained using wellcurated datasets in which the labels are of high quality. In the real world, it is often costly to obtain highquality labels, especially for largescale datasets. A common ¬© 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2111.11288v2  [cs.CV]  7 Oct 20222 CHEN ET AL: SSR: AN EFFICIENT AND ROBUST FRAMEWORK FOR LULN approach is to use semiautomatic methods to obtain the labels (e.g. ‚Äúweblylabelled‚Äù images where the images and labels are obtained by webcrawling). While such methods can greatly reduce the time and cost of manual labelling, they also lead to lowquality noisy labels. In such settings, noise is one of the following two types: closedset noise where the true labels belong to one of the given classes (Set B in Ô¨Åg. 1) and openset noise where the true labels do not belong to the set of labels of the classiÔ¨Åcation problem (Set C in Ô¨Åg. 1). To deal with different types of noise, two main types of methods have been proposed, which we name here as probabilityconsistent methods and probabilityapproximate meth ods. Figure 1: Different ‚Äútigers‚Äù.Probabilityconsistent methods usually model noise patterns directly and propose corresponding proba bilistic adjustment techniques, e.g., robust loss func tions [10, 33, 44] and noise corrections based on noise transition matrix [11]. However, accurate modelling of noise patterns is nontrivial, and of ten cannot even model openset noise. Also, due to the necessary simpliÔ¨Åcations of probabilistic model ling, such methods often perform poorly with heavy and complex noise. More recently, probability approximate methods, that is methods that do not model the noise patterns explicitly become perhaps the dominant paradigm, especially ones that are based on sample selection. Earlier methods often reduce the inÔ¨Çuence of noise samples by selecting a clean subset and training only with it [12, 16, 22, 41]. Recent meth ods tend to further employ semisupervised learning methods, such as MixMatch [4], to fully explore the entire dataset by treating the selected clean subset as labelled samples and the nonselected subset as unlabeled samples [18, 24]. These methods, generally, do not consider the presence of openset noise in the dataset, since most current semisupervised learning methods can not deal with openset noise appropriately. To address this, several methods [27, 35] extend the sample selection idea by further identifying the openset noise and excluding it from the semisupervised training. In general, the above methods make assumptions about the pattern of the noise, such as the conÔ¨Ådence penalty speciÔ¨Åcally for asymmetric noise in DivideMix [18]. However, these mechanisms are often detrimental when the noise pattern does not meet the assumptions ‚Äì for example, explicitly Ô¨Åltering openset noise in the absence of openset noise may result in clean hard samples being removed. Furthermore, due to the complexity of combining multi ple modules, the above methods usually need to adjust complex hyperparameters according to the type and degree of noise. In this paper, we consider a novel problem setting ‚Äî Learning with Unknown Label Noise (LULN ), that is, learning when both the degree and the type of noise are unknown. Striving for simplicity and robustness, we propose a simple method for LUNL , namely Sample Selection and Relabelling (SSR) (section 3.2), with two components that are clearly decoupled: a selection mechanism that identiÔ¨Åes clean samples with correct labels, and a relabelling mechanism that aims to recover correct labels of wrongly labelled noisy samples. These two major components are based on the two simple and necessary assumptions for LULN , namely, that samples with highlyconsistent annotations with their neighbours are often clean, and that very conÔ¨Ådent model predictions are often trustworthy. Once a well labelled subset is constructed this way we use the most basic supervised training scheme with a crossentropy loss. Optionally, a feature consistency loss can be used for all data soCHEN ET AL: SSR: AN EFFICIENT AND ROBUST FRAMEWORK FOR LULN 3 as to deal better with openset noise. Without bells and whistles, such as semisupervised learning, selfsupervised model pre training and model cotraining, our method is shown to be robust to the values of its very few hyperparameters through extensive experiments and ablation studies and to consistently outperform the stateoftheart by a large margin in various datasets. 2 Related Works "
96,Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks.txt,"The quantization of deep neural networks (QDNNs) has been actively studied
for deployment in edge devices. Recent studies employ the knowledge
distillation (KD) method to improve the performance of quantized networks. In
this study, we propose stochastic precision ensemble training for QDNNs (SPEQ).
SPEQ is a knowledge distillation training scheme; however, the teacher is
formed by sharing the model parameters of the student network. We obtain the
soft labels of the teacher by changing the bit precision of the activation
stochastically at each layer of the forward-pass computation. The student model
is trained with these soft labels to reduce the activation quantization noise.
The cosine similarity loss is employed, instead of the KL-divergence, for KD
training. As the teacher model changes continuously by random bit-precision
assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms
the existing quantization training methods in various tasks, such as image
classification, question-answering, and transfer learning without the need for
cumbersome teacher networks.","Deep neural networks (DNNs) have achieved remarkable accuracy for tasks in a wide range of applications, includ ing image processing (He et al. 2016a), machine transla tion (Gehring et al. 2017), and speech recognition (Zhang et al. 2017). These stateoftheart neural networks use very deep models, consuming hundreds of ExaOps of computa tion during training and GBytes of storage for model and data. This complexity poses a tremendous challenge for widespread deployment, especially in resourceconstrained edge environments, leading to a plethora of explorations in model compression that minimize memory footprint and computational complexity while attempting to preserve the performance of the model. Among them, research on quan tized DNNs (QDNNs) focuses on quantizing key data struc tures, namely weights and activations, into lowprecision. Hence, we can save memory access overhead and simplify the arithmetic unit to perform reducedprecision computa tion. There have been extensive studies on QDNNs (Fengfu, Bo, and Bin 2016; Courbariaux, Bengio, and David 2015; Copyright c 2021, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.Choi et al. 2018; Hou and Kwok 2018), but most of them suffer from accuracy loss due to quantization. To enhance the performance of lowcapacity models, knowledge distillation (Hinton, Vinyals, and Dean 2015; Bucilu, Caruana, and NiculescuMizil 2006) (KD) has been widely adopted. KD employs a more accurate model as a teacher network to guide the training of a student model. For the same input, the teacher network provides its pre diction as a soft label, which can be further considered in the loss function to guide the training of the student net work. In the case of QDNNs, the quantized student network can compensate for its accuracy loss via supervision of the teacher model (Mishra and Marr 2018; Polino, Pascanu, and Alistarh 2018; Shin, Boo, and Sung 2019; Kim et al. 2019). However, the need for large and highperformance teacher models introduces signiÔ¨Åcant overhead when applying KD. In particular, KD has not been successfully employed in the emerging study of ondevice training for model adaptation and transfer learning, since the memoryintensive teacher models may not be available once the quantized models are deployed. In this work, we propose a new practical approach to KD for QDNNs, called stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is motivated by an inspiring ob servation about activation quantization. Table 1 shows that the accuracy of the WFA2 (Ô¨Çoat weight and 2bit activation) model improves as the activation precision increases. How ever, the W2AF (2bit weight and Ô¨Çoat activation) model shows the opposite characteristic. The accuracy drops as the weight precision increases for inference. This simple exper iment reveals interesting insights: the activation quantiza tion mostly adds noise to the decision boundary (Boo, Shin, and Sung 2020). Therefore, inference with various activation precision results in selective removal of such noise, leading to diverse guidance that can be exploited for self knowledge distillation. In SPEQ, we form a teacher network that shares the quan tized weights with the student but employs different bit pre cision for activation. The clipping levels of activation are also shared. In fact, the activation precision for the teacher is randomly selected between the low and high precision, such as 2 and 8bit. Since the teacher stochastically applies the target lowbit activation quantization for its soft label com putation, it can experience the impact of quantization for thearXiv:2009.14502v1  [cs.LG]  30 Sep 2020Table 1: CIFAR100 test accuracy (%) in higher precision on the quantized model. ResNet20 is trained with 2bit weight / Ô¨Çoat activation and Ô¨Çoat weight / 2bit activation. (Details in Appendix A.) Trained precision Test accuracy (%) / Inference precision 2bit W, Ô¨Çoat A (W2AF) 65.74 / W2AF 58.01 / W4AF 55.85 / W8AF 54.70 / WFAF Float W, 2bit A (WFA2) 66.93 / WFA2 68.48 / WFA4 68.77 / WFA8 68.71 / WFAF guidance. Furthermore, we reveal that the cosine similarity loss is essential for distilling the knowledge of the teacher of stochastic quantization to the lowprecision student. Although this form of guidance resembles KD, there is a signiÔ¨Åcant difference in that the same model is shared and any other auxiliary models, such as large teacher net works, are unnecessary. The forwardpass computation of the teacher and student in SPEQ can be performed eco nomically as the same weight parameters can be loaded only once. Therefore, the SPEQ can improve the perfor mance much without the overhead of teachermodel search or hyperparameter tuning needed for conventional KD. Fur thermore, since the stochastic precision ensemble provides distinctive knowledge, SPEQ can be combined with the con ventional KD method to further improve the performance of the target QDNNs. We demonstrate the superior performance and efÔ¨Å ciency of our SPEQ on various applications, including CI FAR10/CIFAR100/ImageNet image classiÔ¨Åcation and also transfer learning scenarios such as BERTbased question answering and Ô¨Çower classiÔ¨Åcation. The contributions of our work are summarized as follows: We propose a new practical KD method called SPEQ that can enhance the accuracy of QDNNs QDNNs employing lowprecision bitwidths for weights and activation sig nals. This method can yield better results compared to conventional KDbased QDNN optimization that utilizes large teacher models. We suggest cosine similarity as an essential loss function to effectively distill the knowledge of activation quantiza tion in SPEQ training. We demonstrate that the proposed method outperforms the existing KD methods for training QDNNs with lower training overhead. We conÔ¨Årm this on various models and tasks including image classiÔ¨Åcations, question answering, and transfer learning. We show that the proposed method can be combined to the conventional KD method with a large teacher to fur ther improve the performance of the target model. 2 Related Works "
97,MFNet: Multi-filter Directive Network for Weakly Supervised Salient Object Detection.txt,"Weakly supervised salient object detection (WSOD) targets to train a
CNNs-based saliency network using only low-cost annotations. Existing WSOD
methods take various techniques to pursue single ""high-quality"" pseudo label
from low-cost annotations and then develop their saliency networks. Though
these methods have achieved good performance, the generated single label is
inevitably affected by adopted refinement algorithms and shows prejudiced
characteristics which further influence the saliency networks. In this work, we
introduce a new multiple-pseudo-label framework to integrate more comprehensive
and accurate saliency cues from multiple labels, avoiding the aforementioned
problem. Specifically, we propose a multi-filter directive network (MFNet)
including a saliency network as well as multiple directive filters. The
directive filter (DF) is designed to extract and filter more accurate saliency
cues from the noisy pseudo labels. The multiple accurate cues from multiple DFs
are then simultaneously propagated to the saliency network with a
multi-guidance loss. Extensive experiments on five datasets over four metrics
demonstrate that our method outperforms all the existing congeneric methods.
Moreover, it is also worth noting that our framework is flexible enough to
apply to existing methods and improve their performance.","With the emergence of convolutional neural networks (CNNs) [16], a lot of salient object detection (SOD) meth ods [31, 22, 32, 41] based on CNNs have been proposed *Equal Contributions ‚Ä†Corresponding Author Image CAM Y1 Y2 Ground truth Figure 1. Different pseudo labels synthesized by different refine ment algorithms on class activation map (CAM), in which Y1and Y2represent pseudo labels from pixelwise [4] and superpixel wise [29] refinement algorithms, respectively. and broken the records. However, these CNNsbased SOD methods heavily rely on large amounts of handlabeling data with pixellevel annotations, which are laborintensive and timeconsuming [39]. Due to the high cost of labeling pixellevel annotations, some promising works have been proposed to explore other lowcost alternatives, including scribble [38, 35] and image level category labels [36, 29, 17]. Among them, the cate gory label based methods only require category labels for training, and an overwhelming amount of labels for the existence of object categories are already given ( e.g. Ima geNet [5]). Thus, in this paper, we focus on the imagelevel category label based salient object detection (WSOD1). Previous works on WSOD proposed various techniques such as global smooth pooling [29], multisource supervi sions [36] and alternate optimization [17] to pursue sin 1For convenience, we denote WSOD as methods based on imagelevel category label in this paper.gle ‚Äùhighquality‚Äù pseudo label for training their saliency networks. Though these works have achieved good perfor mance, the generated single ‚Äùhighquality‚Äù pseudo label is usually trapped by its prejudiced characteristics due to the different adopted refinement algorithms. For example, the incomplete deficiency ( 3rdcolumn in Figure 1) and redun dant noise ( 4thcolumn in Figure 1). Instead of pursuing single ‚Äùhighquality‚Äù pseudo labels, we propose to utilize multiple pseudo labels to establish a more robust framework and avoid the negative impacts from the single prejudiced label. To begin with, we adopt two different refinement algorithms, including a pixelwise one [4] and a superpixelwise one [29], to synthesize two different pseudo labels. Both of these two algorithms uti lize abundant appearance information in RGB images to perform refinement for class activation maps (CAMs) [43]. The pixelwise one treats each individual pixel as units, takes its class activation score as clues and then infers its neighbor pixels‚Äô scores, while the superpixelwise one takes superpixels as its operation units. As a result, the synthe sized pseudo labels Y1(from pixelwise algorithm) and Y2 (from superpixelwise algorithm) describe different charac teristics. As is shown in Figure 1, Y1provides better de tailed information, but is usually trapped in incompleteness, while Y2can cover more complete objects but introduces more extra noisy information. These observations drive us to explore how to extract and integrate more comprehensive and robust saliency cues from multiple pseudo labels. The core insight of this work is to adequately excavate the comprehensive saliency cues in multiple pseudo labels and avoid the prejudice of the single label. To be specific, for multiple pseudo labels, we 1) extract abundant accurate multiple saliency cues from multiple noisy labels, and 2) perform integration and propagate the integrated multiple cues to the saliency network. Concretely, our contributions are as follows: ‚Ä¢ We introduce a new framework to utilize multiple pseudo labels for WSOD, which employs more com prehensive and robust saliency cues in multiple labels to avoid the negative impacts of a single label. ‚Ä¢ We design a multifilter directive network (denoted as MFNet), in which multiple directive filters and a multiguidance loss are proposed to extract and in tegrate multiple saliency cues from multiple pseudo labels respectively. ‚Ä¢ Extensive experiments on five benchmark datasets over four metrics demonstrate the superiority of our method as well as the multiple pseudo labels. ‚Ä¢ We also extend the proposed framework to existing method MSW [36] and the prove its effectiveness by achieving 9.1% improvements over Fœâ Œ≤metric on ECSSD dataset.2. Related Work "
98,Product Image Recognition with Guidance Learning and Noisy Supervision.txt,"This paper considers recognizing products from daily photos, which is an
important problem in real-world applications but also challenging due to
background clutters, category diversities, noisy labels, etc. We address this
problem by two contributions. First, we introduce a novel large-scale product
image dataset, termed as Product-90. Instead of collecting product images by
labor-and time-intensive image capturing, we take advantage of the web and
download images from the reviews of several e-commerce websites where the
images are casually captured by consumers. Labels are assigned automatically by
the categories of e-commerce websites. Totally the Product-90 consists of more
than 140K images with 90 categories. Due to the fact that consumers may upload
unrelated images, it is inevitable that our Product-90 introduces noisy labels.
As the second contribution, we develop a simple yet efficient \textit{guidance
learning} (GL) method for training convolutional neural networks (CNNs) with
noisy supervision. The GL method first trains an initial teacher network with
the full noisy dataset, and then trains a target/student network with both
large-scale noisy set and small manually-verified clean set in a multi-task
manner. Specifically, in the stage of student network training, the large-scale
noisy data is supervised by its guidance knowledge which is the combination of
its given noisy label and the soften label from the teacher network. We conduct
extensive experiments on our Products-90 and public datasets, namely Food101,
Food-101N, and Clothing1M. Our guidance learning method achieves performance
superior to state-of-the-art methods on these datasets.","This paper studies a crucial problem in realworld appli cation: recognize products from consumer photos without much supervision. More speciÔ¨Åcally, we want to recognize the Ô¨Ånegrained products taken by consumer‚Äôs mobile cam Figure 1. Example images from our Products90. We illustrate 5 different categories in column. Visually correct images are shown in the Ô¨Årst two rows. Visually confused or unrelated images are shown in the last two rows. eras, with unconstrained viewing directions, cluttered back ground, and different lighting conditions. One can imagine an application that you are recommended where the prod ucts can be found and what the prices are by recognizing your casuallycaptured product photos. To address this realworld product image recognition task, we build a novel largescale dataset, termed as Product90 , which consists of 90 generic product cate gories. Instead of collecting daily images by laborand timeintensive image capturing, we take advantage of the web and download images from the reviews of several e commerce websites where the images are casually captured by consumers. Totally, we collected more than 140k prod uct images from the customer reviews. The associated 90 categories are borrowed the categories of ecommerce web sites. Figure 1 shows some examples of this dataset. We can see there are several challenges brought by Product90 dataset: i)The visual contents in Product90 contains a wide 1arXiv:1907.11384v1  [cs.CV]  26 Jul 2019range of subjects. ii) Some categories are very similar in ap pearance, e.g. Hair Care vs. Body Care. iii) Some photos are not related to the category, which suggests a signiÔ¨Åcant level of noise exists in the dataset. To evaluate product image recognition algorithms on our Product90, we build a small manuallyclean subset for tra ditional training and testing, and remain the rest of Product 90 as noisy data which can be used for extra training. To take full advantage of the small clean training subset and the massive noisy labeled data for daily product recognition, we propose a novel guidance learning framework for noisy data learning. It mainly includes two training stages. At the Ô¨Årst stage, we train a baseline CNN model, or a teacher model, on the full Product90 dataset (without the clean test set). At the second stage, we train a student or target network on the largescale noisy set and the small clean training set with multitask learning. SpeciÔ¨Åcally, in the stage of stu dent training, the largescale noisy data is supervised by the guidance knowledge which consists of two supervision sig nals, namely the noisy ground truths and the soften labels from the teacher network. We fuse these onehot ground truths with the soften multihot labels, and optimize the net work by Kullback‚ÄìLeibler Divergence (KLDiv) loss. Our guidance learning framework considers the follow ing issues. The Ô¨Årst stage of our guidance learning ensures that we can obtain a powerful teacher model instead of us ing the noisy set or clean set only like in [25].We fuse both ground truths and soften labels for the largescale noisy data, since i) the teacher model provides useful informa tion but is far from perfect, and ii) there exist both false and correct labels in noisy labels. In summary, our contributions can be concluded as fol lows: We introduce a new task, i.e. daily product image recognition, and a novel largescale dataset, termed as Product90 which is collected from the reviews of e commerce websites. To advance the performance of daily product image recognition, we propose a generic guidance learning method to take full advantage the small clean subset and the largescale noisy data in Product90. We conduct comprehensive evaluations with our guid ance learning method on our Products90, Food 101 [1], Food101N [14], and Clothing1M [33], and achieve stateoftheart results. 2. Related Work "
99,Capturing cross-session neural population variability through self-supervised identification of consistent neuron ensembles.txt,"Decoding stimuli or behaviour from recorded neural activity is a common
approach to interrogate brain function in research, and an essential part of
brain-computer and brain-machine interfaces. Reliable decoding even from small
neural populations is possible because high dimensional neural population
activity typically occupies low dimensional manifolds that are discoverable
with suitable latent variable models. Over time however, drifts in activity of
individual neurons and instabilities in neural recording devices can be
substantial, making stable decoding over days and weeks impractical. While this
drift cannot be predicted on an individual neuron level, population level
variations over consecutive recording sessions such as differing sets of
neurons and varying permutations of consistent neurons in recorded data may be
learnable when the underlying manifold is stable over time. Classification of
consistent versus unfamiliar neurons across sessions and accounting for
deviations in the order of consistent recording neurons in recording datasets
over sessions of recordings may then maintain decoding performance. In this
work we show that self-supervised training of a deep neural network can be used
to compensate for this inter-session variability. As a result, a sequential
autoencoding model can maintain state-of-the-art behaviour decoding performance
for completely unseen recording sessions several days into the future. Our
approach only requires a single recording session for training the model, and
is a step towards reliable, recalibration-free brain computer interfaces.","Neural decoders require stable neurons in a recorded population in order to accurately predict behaviour such as movement or to allow decoding of stimuli. However, over time instabilities in the recording equipment and drift in neural activity lead to instabilities that prevent reusing a decoder trained on one day for a session recorded on another day [Huber et al., 2012, Ziv et al., 2013, Driscoll et al., 2017]. At the same time, neural population activity is highly structured and often conÔ¨Åned to lowdimensional manifolds [Cunningham and Byron, 2014] that can be recovered using latent variable modelling approaches [Hurwitz et al., 2021]. Importantly, recent work showed that movementrelated latent neural dynamics in population activity from the primate motor cortex is stable and could be recovered over intervals as long as two years [Gallego et al., 2020]. This suggests that despite the variability at the level of single neurons, in each session a subset of neuronsarXiv:2205.09829v2  [qbio.NC]  5 Jan 2023will remain informative about behaviour. A stable crosssession decoder therefore has to be able to identify these neurons and utilise them for decoding. Therefore, here we focus on identifying known recording neurons in unseen sessions. In particular, we hypothesised that a latent encoding of neural activity can be augmented by information about which neurons were seen during training, and at which position in the input. We show that this is sufÔ¨Åcient to decode behaviour (in our case different cued arm movements by a monkey with simultaneous motor cortex recordings) with high accuracy across unseen sessions. We achieve this with a selfsupervised approach through training a recurrent neural network (RNN) to predict original neuron positions following data perturbation in a manner mirroring session to session variability. In essence, the closer our perturbations mimic real intersession variability (as shown in Figure 1), the higher our behaviour prediction performance on an unseen session. These perturbations include adding spikes to existing neurons from randomly generated neurons, removing spikes from existing neurons, shifting the entire neuron population by a constant amount, slightly shifting neurons in time, replacing neurons with randomly generated neurons and eliminating neurons entirely. Original Neurons Lost Neurons Replaced Probe Array Shift Neurons Move Neurons Added Figure 1: Intersession ensemble variability possible when recording from neural populations. Neu rons from the original recording session can be lost to the recording array, new neurons can become visible, neurons can move between electrodes, original neurons can be replaced by unseen neurons and the entire probe array can shift, causing a systematic change in neuron position. In addition, spike sorting can induce variability as the signal to noise ratio of individual neurons changes between sessions. The perturbations we apply to each trial of recordings is in response to each of these sources of variability. We model each unseen test trial as an instance of a perturbed seen train trial and subsequently, our sequential autoencoder model attempts to map each unseen trial to a known trial. This neuron locator RNN is trained to predict original neuron position within a single recording session from many perturbed variations of trials of this training session. Once trained to predict original neuron positions, a separate network, which in this case is a sequential autoencoder based on Latent Factor Analysis via Dynamical Systems (LFADS) [Pandarinath et al., 2017], is trained to predict original unperturbed neural recording trials from perturbed variations of trials from the same session. The encoder of this sequential autoencoder receives as additional input the embedding of the neuron locator RNN activations, conditioning the encoder to produce latent variables which are informative enough to accurately reconstruct the original recording. The encoder produces latent variables which are separated by behaviour (arm movement direction) in a selfsupervised manner, from which behaviour can be predicted without the model being explicitly trained on behaviour. Importantly, the joint neuron locator RNN and LFADS encoder ensemble can predict behaviourally relevant latent variables for unseen recording sessions that yield high decoding accuracy. Currently, there are no existing approaches to accurately predict behaviour from an unseen recording session when training on just one single session. We not only show this is possible with our method, but that our approach is robust to intersession variability for up to 8 days when a sufÔ¨Åcient number of neurons are persistent across sessions. 2 Related Work "
100,AutoEnsemble: Automated Ensemble Search Framework for Semantic Segmentation Using Image Labels.txt,"A key bottleneck of employing state-of-the-art semantic segmentation networks
in the real world is the availability of training labels. Standard semantic
segmentation networks require massive pixel-wise annotated labels to reach
state-of-the-art prediction quality. Hence, several works focus on semantic
segmentation networks trained with only image-level annotations. However, when
scrutinizing the state-of-the-art results in more detail, we notice that
although they are very close to each other on average prediction quality,
different approaches perform better in different classes while providing low
quality in others. To address this problem, we propose a novel framework,
AutoEnsemble, which employs an ensemble of the ""pseudo-labels"" for a given set
of different segmentation techniques on a class-wise level. Pseudo-labels are
the pixel-wise predictions of the image-level semantic segmentation frameworks
used to train the final segmentation model. Our pseudo-labels seamlessly
combine the strong points of multiple segmentation techniques approaches to
reach superior prediction quality. We reach up to 2.4% improvement over
AutoEnsemble's components. An exhaustive analysis was performed to demonstrate
AutoEnsemble's effectiveness over state-of-the-art frameworks for image-level
semantic segmentation.","Generating highquality semantic segmentation predictions using only models trained on imagelevel annotated datasets would enable a new level of applicability. The progress of fully supervised semantic segmentation networks has already helped provide many useful tools and applications. For ex ample, in autonomous and selfdriving vehicles [1, 2], remote sensing [3, 4], facial recognition [5], agriculture [6, 7], and in the medical Ô¨Åeld [8, 9], etc. The downside of those fully supervised semantic segmentation networks (FSSS) is that (b) (c) (a) (e) (f) (d)Fig. 1 . Pseudo labels from (a) DRS, (b) PMM, (c) Puzzle CAM, (d) CLIMS, (e) AutoEnsemble (Ours), (f) Ground truth they require large amounts of pixelwise annotated images. Generating such a training set is very timeconsuming and tedious work. For instance, one frame of the Cityscapes dataset, which contains thousands of pixelwise frame anno tations of street scenes from cities, requires more than an hour of manual userdriven annotation [10]. Furthermore, medical imaging and molecular biology Ô¨Åelds require the knowledge of highly qualiÔ¨Åed and experienced individuals capable of interpreting and annotating the images. Therefore, to reduce the time and resources required for generating pixelwise masks, a wide range of research works focus on developing approaches that focus on weaker kinds of supervision. In this work, we will focus on weak supervision in the form of imagelevel labels. Imagelevel labels give the least amount of supervision for semantic segmentation but are the easiest to acquire. Several works already focus on imagelevel semantic seg mentation techniques, and they consistently reach better and better high scores. Most works are based on Class Activa tion Maps (CAMs) [11]. CAMs localize the object by trainarXiv:2303.07898v2  [cs.CV]  15 Mar 2023ing a DNN model with classiÔ¨Åcation loss and then reusing the learned weights to highlight the image areas responsible for its classiÔ¨Åcation decision. Most imagelevel segmentation approaches aim to improve the CAM baseline by adding addi tional regularizations to the classiÔ¨Åcation loss or reÔ¨Åning the CAM mask afterward. As more and more methods emerge for improving CAM quality, stateoftheart is usually compiled of combinations of regularizations and afterthefact reÔ¨Åne ment. However, when analyzing different imagelevel seg mentation tehniques on a classbyclass basis, we observed that the differences between those approaches vary signiÔ¨Å cantly on speciÔ¨Åc classes, although those methods generate predictions that reach comparable scores on average. Therefore, we are proposing our AutoEnsemble frame work. In our framework, we combine the pseudolabels of multiple imagelevel segmentation tehniques based on the re spective class scores to generate a superset of pseudolabels, combining the upsides of multiple different approaches. Fig. 1 visualizes the gains possible of AutoEnsemble com pared to its best component. We perform extensive experi ments on the PASCAL VOC2012 dataset [12] to prove the effectiveness of the proposed framework in various exper imental settings and compare them with a wide range of stateoftheart techniques to illustrate the beneÔ¨Åts of our approach. The key contribution of this work are: 1. Our novel AutoEnsemble framework improves the pre diction quality of the segmentation mask by combining stateoftheart pseudolabels and classbyclass base. 2. Our AutoEnsemble is not limited by the number or approach of any conventional imagelevel segmenta tion framework to combine their pseudolabels. Since the AutoEnsemble is only used for generating pseudo labels, it will not add more computations for inference predictions. 3. We have presented detailed ablation studies and anal ysis of the results comparing AutoEnsemble to state oftheart methods on the VOC2012 dataset to evaluate our method‚Äôs efÔ¨Åcacy and the improvements achieved using our framework. 2. RELATED WORK "
101,Semi-supervised Stance Detection of Tweets Via Distant Network Supervision.txt,"Detecting and labeling stance in social media text is strongly motivated by
hate speech detection, poll prediction, engagement forecasting, and concerted
propaganda detection. Today's best neural stance detectors need large volumes
of training data, which is difficult to curate given the fast-changing
landscape of social media text and issues on which users opine. Homophily
properties over the social network provide strong signal of coarse-grained
user-level stance. But semi-supervised approaches for tweet-level stance
detection fail to properly leverage homophily. In light of this, We present
SANDS, a new semi-supervised stance detector. SANDS starts from very few
labeled tweets. It builds multiple deep feature views of tweets. It also uses a
distant supervision signal from the social network to provide a surrogate loss
signal to the component learners. We prepare two new tweet datasets comprising
over 236,000 politically tinted tweets from two demographics (US and India)
posted by over 87,000 users, their follower-followee graph, and over 8,000
tweets annotated by linguists. SANDS achieves a macro-F1 score of 0.55 (0.49)
on US (India)-based datasets, outperforming 17 baselines (including variants of
SANDS) substantially, particularly for minority stance labels and noisy text.
Numerous ablation experiments on SANDS disentangle the dynamics of textual and
network-propagated stance signals.","Social media is regarded as a barometer of modern society‚Äôs emo tional state. Billions of social media users express their stances toward events, social issues, and political parties in their tweets, Facebook articles, or blogs. The ‚Äòtarget entity‚Äô may not be explic itly mentioned in the text expressing the stance. Automatic stance detection is a strongly motivated mining operation on social media and networks [ 24]. Some applications include hate speech detection [14], poll prediction [ 13], and rumor veracity detection [ 10]. The im portance of political stance analysis over Twitterlike platforms has increased dramatically in recent times owing to several phenomena ‚Äî sharp increase in partisanship among users, malicious efforts of organized groups to distort popular opinion at a largescale, etc. Leveraging homophily in stance detection. A large number of approaches have been proposed for textbased stance detection [1,24,32]. Relatively few approaches recognize and exploit the fact that text in social networks is accompanied by rich graph structured metadata, e.g., friends, followers/followees, retweetsand replies, hashtags, textauthor associations, etc. [ 7,23]. It is wellknown that homophily (friends have similar taste) and social balance (enemy of an enemy is a friend, etc.) are pervasive in social networks. Therefore, these signals have the potential to improve the accuracy of stance prediction. Consider the tweets shown in Figure 1. The stance labels in this example include proRepublican, antiRepublican, proDemocrat, antiDemocrat, and neutral. Tweets from users 2‚Äì5 clearly express an antiDemocrat stance, while user 6‚Äôs tweet is proRepublican. It may be nontrivial to correctly label the tweet of user 1 (because ‚Äòshe‚Äô is not identified, and the only han dle @realDonaldTrump is uninformative) unless we see whom user 1 follows and what tweets they write. However, previous works connecting network dynamics for stance classification mostly deal with userlevel stance analysis. An overall stance of the user often does not reflect in the tweet. For example, a Democrat supporter can tweet something prodemocrat or antirepublican. Volatile users may often switch their stances depending on the issue. For exam ple, among the tweets we collected and annotated for the analysis presented in this work, about 8%show stance switch. On the other hand, supplementing local, tweetlevel features with homophily driven features might add bias towards the majority stance of the neighbor nodes. Instead, we seek to use homophily as a navigator of distant supervision. The end results are simple, tweetlevel stance classifiers that rely on networklevel information only at training time. Scarcity of labeled data for stance classification. A key hur dle in our setting is the paucity of humanlabeled data. Neural text processors are among the best for stance detection; however, they need large volumes of training data. The rapid pace of information generation and consumption over social media leads to the emer gence of completely new entities and concepts (persons, events, issues, etc.), too fast for the curation of humanlabeled highquality data for each scenario. Semisupervised and active learning are common coping mechanisms. Starting from a small set of instances manually labeled with groundtruth (‚Äògold instances‚Äô), they expose the learner to progressively larger sets of instances that are automat ically and manually labeled, respectively. Label sampling decisions are not usually informed by an overlying network. Proposed method: SANDS Our central research question is the manner in which network signals like homophily can improve semi supervised stance detection. Our investigation results in a system, called SANDS (Stance Analysis via Network Distant Supervision) that starts from a few highquality seed instances, obtains noisy labeling guidance from homophily (between a user and her follow ers), uses multiple views of tweet content with customized feature extraction models, and iteratively train the component learners. In more detail, SANDS uses three kinds of textual feature extrac tors, designed to fit their semantics in the context of Twitter ‚Äî 1arXiv:2201.00614v2  [cs.SI]  5 Jan 2022WSDM ‚Äô22, February 21‚Äì25, 2022, Tempe, AZ, USA1Subhabrata Dutta,2Samiya Caur,3Soumen Chakrabarti,2Tanmoy Chakraborty 123 4 5 6 Figure 1: Stance homophily in Twitter. User 1follows users 2,3,4,5, and 6. All these users carry similar opinion, with user 6‚Äôs tweet showing support to the Republicans while the rest are antiDemocrat. While the tweet posted by user 1does not link any entity related to Republican or Democrats directly to some polarity words (thereby making the stance classification difficult), a classification framework with the knowledge of the rest of the tweets can break the ambiguity. (i) A simple symmetric set aggregated encoding is used for hash tags. (ii) Shortrange contextual text representation is captured through a convolutional network. (iii) Longer range textual con text is captured through a biLSTM. These are combined into two main learners/predictors with somewhat different capabilities and strengths. Rather than throwing these into a standard cotraining or disagreementbased learning scheme, we also make use of network information along with a homophily assumption that followees of a user tend to have the same stance as the user . Therefore, the pre dictors are also applied to recent tweets by followees of the user who posted the given tweet, and their majority vote turned into a suitable loss for the other learner. This novel combination of networkdriven distant supervision and synthetic feature view separation makes SANDS ‚Äôs predictive accuracy considerably superior to many recent and competitive baselines as well as ablations. SANDS is particularly effective at improving the accuracy of minority class labels. Two new datasets and the superiority of SANDS .As part of this work, we offer two large collections of politically tinted tweets from the two most vociferous social media with political content: 59,684and176,619tweets from US and Indiabased users, respec tively, along with the corresponding interuser follow relations; 3,822and4,185tweets respectively among these collections are manually annotated with corresponding stance labels. While using only 1,500samples as labeled data, SANDS achieves 0.55and0.47 macroF1 scores on US and Indiabased datasets, surpassing the best baseline by (absolute) 4% and 5%, respectively. Summary of our major contributions: ‚Ä¢We propose a novel stance classification framework for tweets, SANDS , which employs distant supervision using follow network information to learn tweet stances with frugal labeled data.‚Ä¢Using separate neural feature extractors to focus on local and global contextual features from tweet texts separately, we formu late a learning strategy facilitated by distant network supervision that iteratively trains multiple models with twophased view sep arations: local vs. global textual features within the tweet and candidate tweet vs. followee tweets over the network. ‚Ä¢We present two large collections of tweets from US and India based users, along with manually annotated labels for political stance on subsets of each. ‚Ä¢We perform extensive experiments with multiple supervised and semisupervised methods along with ablation variants to analyze signal importance. Reproducibility: We detail the preprocessing and parameters ofSANDS necessary to reproduce the results in the supplementary material. Also, the source codes are available on this anonymous repository: https://github.com/LCS2IIITD/SANDS. 2 RELATED WORK "
102,Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture.txt,"An accurate language identification tool is an absolute necessity for
building complex NLP systems to be used on code-mixed data. Lot of work has
been recently done on the same, but there's still room for improvement.
Inspired from the recent advancements in neural network architectures for
computer vision tasks, we have implemented multichannel neural networks
combining CNN and LSTM for word level language identification of code-mixed
data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of
93.28% and 93.32% is achieved on our two testing sets.","With the rise of social media, the amount of mine able data is rising rapidly. Countries where bilin gualism is popular, we see users often switch back and forth between two languages while typing, a phenomenon known as codemixing or code switching. For analyzing such data, language tag ging acts as a preliminary step and its accuracy and performance can impact the system results to a great extent. Though a lot of work has been done recently targeting this task, the problem of lan guage tagging in codemixed scenario is still far from being solved. Codemixing scenarios where one of the languages have been typed in its translit erated from possesses even more challenges, espe cially due to inconsistent phonetic typing. On such type of data, context capture is extremely hard as well. Proper context capture can help in solving problems like ambiguity, that is word forms which are common to both the languages, but for which, the correct tag can be easily understood by know ing the context. An additional issue is a lack of available codemixed data. Since most of the tasks require supervised models, the bottleneck of data crisis affects the performance quite a lot, mostly due to the problem of overÔ¨Åtting.In this article, we present a novel architecture, which captures information at both word level and context level to output the Ô¨Ånal tag. For word level, we have used a multichannel neural network (MNN) inspired from the recent works of computer vision. Such networks have also shown promising results in NLP tasks like sen tence classiÔ¨Åcation (Kim, 2014). For context cap ture, we used BiLSTMCRF. The context module was tested more rigorously as in quite a few of the previous work, this information has been sidelined or ignored. We have experimented on Bengali English (BnEn) and HindiEnglish (HiEn) code mixed data. Hindi and Bengali are the two most popular languages in India. Since none of them have Roman as their native script, both are writ ten in their phonetically transliterated from when codemixed with English. 2 Related Work "
103,Language Identification Using Deep Convolutional Recurrent Neural Networks.txt,"Language Identification (LID) systems are used to classify the spoken
language from a given audio sample and are typically the first step for many
spoken language processing tasks, such as Automatic Speech Recognition (ASR)
systems. Without automatic language detection, speech utterances cannot be
parsed correctly and grammar rules cannot be applied, causing subsequent speech
recognition steps to fail. We propose a LID system that solves the problem in
the image domain, rather than the audio domain. We use a hybrid Convolutional
Recurrent Neural Network (CRNN) that operates on spectrogram images of the
provided audio snippets. In extensive experiments we show, that our model is
applicable to a range of noisy scenarios and can easily be extended to
previously unknown languages, while maintaining its classification accuracy. We
release our code and a large scale training set for LID systems to the
community.","Intelligent assistants like Siri1or the Google Assistant2rely on ASR. Current ASR systems require users to manually specify the system‚Äôs correc t input lan guage to work properly. However, as a sensible preprocessing st ep we can infer the spoken language using an automatic LID system. Traditional LI D systems utilize domainspeciÔ¨Åc expert knowledge in the Ô¨Åeld of audio signal pro cessing for extracting handcrafted features from the audio samples. L ately, deep learn ing and artiÔ¨Åcial neural networks have become the stateofthe art for many pattern recognition problems. Deep Neural Networks (DNNs) have become the best performing method for a range of computer vision tasks, suc h as image classiÔ¨Åcation [17,18], or object detection and recognition [14,15]. In this paper, we address the problem of language identiÔ¨Åcation fro m a com puter vision perspective. We extract the target language of a give n audio sam ple by utilizing a hybrid network constructed of a Convolutional Neural Net work (CNN) combined with an Recurrent Neural Network (RNN) . Our contri butionscanbesummarizedasfollows:(1)weproposeahybridCRNN, combining ‚ãÜequal contribution 1https://www.apple.com/ios/siri/ 2https://assistant.google.com/the descriptive powers of CNNs with the ability to capture temporal features of RNNs. (2) We perform extensive experiments with our proposed ne twork and show its applicability to a range of scenarios and its extensibility to new lan guages. (3) We release our code and a large scale training set for LI D systems to the community3. Thepaperisstructuredinthefollowingway:Insection2weintroduc erelated work in the Ô¨Åeld of LID systems. We showcase our system in section 3 and evaluate it on extensive experiments in section 4. We conclude our wo rk in section 5. 2 Related Work "
104,QActor: On-line Active Learning for Noisy Labeled Stream Data.txt,"Noisy labeled data is more a norm than a rarity for self-generated content
that is continuously published on the web and social media. Due to privacy
concerns and governmental regulations, such a data stream can only be stored
and used for learning purposes in a limited duration. To overcome the noise in
this on-line scenario we propose QActor which novel combines: the selection of
supposedly clean samples via quality models and actively querying an oracle for
the most informative true labels. While the former can suffer from low data
volumes of on-line scenarios, the latter is constrained by the availability and
costs of human experts. QActor swiftly combines the merits of quality models
for data filtering and oracle queries for cleaning the most informative data.
The objective of QActor is to leverage the stringent oracle budget to robustly
maximize the learning accuracy. QActor explores various strategies combining
different query allocations and uncertainty measures. A central feature of
QActor is to dynamically adjust the query limit according to the learning loss
for each data batch. We extensively evaluate different image datasets fed into
the classifier that can be standard machine learning (ML) models or deep neural
networks (DNN) with noise label ratios ranging between 30% and 80%. Our results
show that QActor can nearly match the optimal accuracy achieved using only
clean data at the cost of at most an additional 6% of ground truth data from
the oracle.","We are in the era of big data, which are continuously generated on dif ferent web platforms, e.g., social media, and disseminated via search engines often in a casual and unstructured way. Consequently, such a big data experience suffers from diversiÔ¨Åed quality issues, e.g., images tagged with incorrect labels, so called noisy labels. Today‚Äôs easy access to this large amount of data further exasperates the pres ence of extremely noisy data. According to [ 15], noisy data costs the US industry more than $3 trillion per year to cleanse or to mitigate the impact of derived incorrect analyses. While the learning models conveniently leverage such a free source of data, its quality greatly un dermines the learning efÔ¨Åciencies and their associate utilities [ 9]. For example [ 27], using the image classiÔ¨Åer trained from data with highly noisy labels can signiÔ¨Åcantly degrade the classiÔ¨Åcation accuracy and hinder its applicability on different application domains. Another challenge brought by big data is the stream of data genera tion and continuous data curation. On the one hand, this invalidates the assumptions of offline learning scenarios and drastically increases the 1TU Delft, The Netherlands, email: t.younesian@tudelft.nl 2Universit√© Grenoble Alpes, France, email: zilong.zhao@gipsalab.fr 3TU Delft, The Netherlands, email: s.ghiassi@tudelft.nl 4ABB Future Labs, Switzerland. email: robert.birke@ch.abb.com 5TU Delft, The Netherlands, email: lydiaychen@ieee.orgstorage overhead. On the other hand, due to the privacy concern and government regulation, e.g., European GDPR, data shall be closely managed, imposing a limit on using curated data from the public domains. As such, today‚Äôs machine learning models, e.g., classifying images, in reality often encounter such data that arrives in a stream of high velocity and can only be kept for a limited time. It is no mean feat to derive learning models which can cater to such a multifaced challenge, i.e., noisy stream data. Noisy label issue has been a long standing challenge [ 13], from standard machine learning (ML) models to the recent deep neural networks (DNN), whose large learning capacities can have detrimental memorization effects on dirty labels [ 27]. The central theme here is to Ô¨Ålter out the suspicious data which might have corrupted labels via quality estimates. Although such approaches show promising results in combating noisy labels, the applicability to noisy stream data is unfortunately limited, due to their assumption of offline and complete data availability. The other drawback of Ô¨Åltering approaches is the risk of dropping informative data points which can be inÔ¨Çuential for the underlying learning models. For instance, images with corrupted labels can be exactly on the class boundaries. It might be worthwhile to actively cleanse such data due to its high potential in improving the learning tasks, even at a certain expense. Active learning techniques [ 21] are designed to query extra infor mation from the oracle for the data whose (true) labels are not readily available. Such an oracle is assumed to know (uncorrected) labels. Due to the high oracle query cost, only the informative/uncertain data is queried within a certain query budget. The majority of active learn ing approaches focus on the offline scenario and constant budget, except [29] that explores the dynamic budget based on the classiÔ¨Åca tion errors on one by one drifting streaming data. Motivated by its power of cleansing data, [ 2] applies active learning techniques on support vector machines which encounter moderate noise ratios, i.e., roughly 30%. The efÔ¨Åcacy of active learning relies on the identifying the most informative instances based on uncertainty measurements of learning tasks, e.g., class probability [ 18] or entropy value [ 8]. While the related work shows promising results of active learning on noisy labels, it is not clear how active query approach can be adopted when encountering noisy data streams that can be learnt only for a short period of time. In this paper, we focus on a challenging multiclass learning prob lem whose data is streamed and its labels are extremely noisy, i.e., more than half of the given labels of streaming data are wrong. Due to the privacy concerns and limited storage capacity, the data can only be stored for a limited amount of epochs, drastically shortening the data validity for training classiÔ¨Åers. In other words, only a small fraction of data is available for learning the model at any point in time, compared to the ofÔ¨Çine scenario. Our objective is to enhancearXiv:2001.10399v1  [cs.LG]  28 Jan 2020the noiseresiliency of the underlying classiÔ¨Åer by selectively learning from good data as well as noisy labels that are critical to train the classiÔ¨Åer. In order to turn the noisy labels into a learning advantage, we resort to the oracle for recovering their label ground truth under a given query budget. Ultimately, we aim to optimize classiÔ¨Åcation accuracy with a minimum number of oracle queries, in combating stream of noisy labels. To such an end, we design an online active learning algorithm, termed Qualitydriven Active Learning (QActor ), which combines the merits of quality models and typical active learning algorithms. Upon receiving new data instances, QActor Ô¨Årst Ô¨Ålters it via the quality model into ‚Äúclean‚Äù and ‚Äúnoisy‚Äù categories. Second, QAc tor queries from the oracle the true labels of highly uncertain and informative noisy instances. Another unique feature of QActor is that the overall query budget is Ô¨Åxed but the number of queries per batch is dynamically adjusted based on the current training loss value. QActor uses more queries when the loss value increases to avoid incorrectly including noisy labels, and reduced otherwise. We exten sively evaluate QActor on an extensive set of scenarios, i.e., noise ratios, multiclass classiÔ¨Åer models, uncertain measures, and more importantly different data sets. Our results show that in the presence of very large label noise, i.e., up to 80% corrupted labels, QActor can achieve remarkable accuracy, i.e., almost match the optimal accuracy obtained excluding all noisy labels, at the cost of just a small fraction of oracle information, i.e., up to 6% oracle queried labels. Our contributions are three fold. We design a novel and efÔ¨Åcient learning framework, termed QActor , whose core is the combination of quality model and online active learning. Secondly, we propose a dynamic learning strategy that can achieve similar results as the static one. Thirdly, we extensively evaluate the proposed QActor on an extensive set of scenarios and datasets, strongly arguing for the combination of human and artiÔ¨Åcial intelligence. 2 Related Work "
105,Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training.txt,"User-generated reviews can be decomposed into fine-grained segments (e.g.,
sentences, clauses), each evaluating a different aspect of the principal entity
(e.g., price, quality, appearance). Automatically detecting these aspects can
be useful for both users and downstream opinion mining applications. Current
supervised approaches for learning aspect classifiers require many fine-grained
aspect labels, which are labor-intensive to obtain. And, unfortunately,
unsupervised topic models often fail to capture the aspects of interest. In
this work, we consider weakly supervised approaches for training aspect
classifiers that only require the user to provide a small set of seed words
(i.e., weakly positive indicators) for the aspects of interest. First, we show
that current weakly supervised approaches do not effectively leverage the
predictive power of seed words for aspect detection. Next, we propose a
student-teacher approach that effectively leverages seed words in a
bag-of-words classifier (teacher); in turn, we use the teacher to train a
second model (student) that is potentially more powerful (e.g., a neural
network that uses pre-trained word embeddings). Finally, we show that iterative
co-training can be used to cope with noisy seed words, leading to both improved
teacher and student models. Our proposed approach consistently outperforms
previous weakly supervised approaches (by 14.1 absolute F1 points on average)
in six different domains of product reviews and six multilingual datasets of
restaurant reviews.","A typical review of an entity on platforms such as Yelp and Amazon discusses multiple aspects of the entity (e.g., price, quality) in individual re view segments (e.g., sentences, clauses). Consider for example the Amazon product review in Fig ure 1. The text discusses various aspects of the Great price for an excellent LED TVGreat Tv for the price.  Easy to setup.  The audio is ok for the tiny speakers.  The picture is just as good as my panasonic viera 42"" plasma tv.  Much better than the 20"" tube tv. PriceEase of UseImageSound QualityGeneralAspectSentenceFigure 1: Example of product review with aspect an notations: each individual sentence of the review dis cusses a different aspect (e.g., price) of the TV . TV such as price, ease of use, and sound quality. Given the vast number of online reviews, both sell ers and customers would beneÔ¨Åt from automatic methods for detecting Ô¨Ånegrained segments that discuss particular aspects of interest. Finegrained aspect detection is also a key task in downstream applications such as aspectbased sentiment anal ysis and multidocument summarization (Hu and Liu, 2004; Liu, 2012; Pontiki et al., 2016; Ange lidis and Lapata, 2018). In this work, we consider the problem of clas sifying individual segments of reviews to pre deÔ¨Åned aspect classes when ground truth aspect labels are not available. Indeed, reviews are of ten entered as unstructured, freeform text and do not come with aspect labels. Also, it is infeasi ble to manually obtain segment annotations for re tail stores like Amazon with millions of different products. Unfortunately, fully supervised neural networks cannot be applied without aspect labels. Moreover, the topics learned by unsupervised neu ral topic models are not perfectly aligned with the users‚Äô aspects of interest, so substantial human ef fort is required for interpreting and mapping the learned topics to meaningful aspects. Here, we investigate whether neural networks can be effectively trained under this challenging setting when only a small number of descriptive keywords, or seed words , are available for eacharXiv:1909.00415v1  [cs.LG]  1 Sep 2019Aspect Seed Words Price (EN) price, value, money, worth, paid Image (EN) picture, color, quality, black, bright Food (EN) food, delicious, pizza, cheese, sushi Drinks (FR) vin, bi√®re, verre, bouteille, cocktail Ambience (SP) ambiente, mesas, terraza, acogedor, ruido Table 1: Examples of aspects and Ô¨Åve of their corre sponding seed words in various domains (electronic products, restaurants) and languages (‚ÄúEN‚Äù for En glish, ‚ÄúFR‚Äù for French, ‚ÄúSP‚Äù for Spanish). aspect class. Table 1 shows examples of aspects and Ô¨Åve of their corresponding seed words from our experimental datasets (described later in more detail). In contrast to a classiÔ¨Åcation label, which is only relevant for a single segment, a seed word can implicitly provide aspect supervision to poten tially many segments. We assume that the seed words have already been collected either manually or automatically. Indeed, collecting a small1set of seed words per aspect is typically easier than man ually annotating thousands of segments for train ing neural networks. As we will see, even noisy seed words that are only weakly predictive of the aspect will be useful for aspect detection. Training neural networks for segmentlevel as pect detection using just a few seed words is a challenging task. Indeed, as a contribution of this paper, we observe that current weakly supervised networks do not effectively leverage the predic tive power of the available seed words. To address the shortcomings of previous seed wordbased ap proaches, we propose a novel weakly supervised approach, which uses the available seed words in a more effective way. In particular, we con sider a studentteacher framework, according to which a bagofseedwords classiÔ¨Åer (teacher) is applied on unlabeled segments to supervise a sec ond model (student), which can be any supervised model, including neural networks. Our approach introduces several important con tributions. First, our teacher model considers each individual seed word as a (noisy) aspect indicator, which as we will show, is more effective than pre viously proposed weakly supervised approaches. Second, by using only the teacher‚Äôs aspect prob abilities, our student generalizes better than the teacher and, as a result, the student outperforms both the teacher and previously proposed weakly 1In our experiments, we only consider around 30 seed words per aspect. For comparison, the vocabulary of the datasets has more than 10,000 terms.supervised models. Finally, we show how iterative cotraining can be used to cope with noisy seed words: the teacher effectively estimates the pre dictive quality of the noisy seed words in an unsu pervised manner using the associated predictions by the student. Iterative cotraining then leads to both improved teacher and student models. Over all, our approach consistently outperforms exist ing weakly supervised approaches, as we show with an experimental evaluation over six domains of product reviews and six multilingual datasets of restaurant reviews. The rest of this paper is organized as follows. In Section 2 we review relevant work. In Section 3 we describe our proposed weakly supervised ap proach. In Section 4 we present our experimen tal setup and Ô¨Åndings. Finally, in Section 5 we conclude and suggest future work. A preliminary version of this work was presented at the Sec ond Learning from Limited Labeled Data Work shop (Karamanolakis et al., 2019). 2 Related Work and Problem DeÔ¨Ånition "
106,Fine-Grained Named Entity Typing over Distantly Supervised Data Based on Refined Representations.txt,"Fine-Grained Named Entity Typing (FG-NET) is a key component in Natural
Language Processing (NLP). It aims at classifying an entity mention into a wide
range of entity types. Due to a large number of entity types, distant
supervision is used to collect training data for this task, which noisily
assigns type labels to entity mentions irrespective of the context. In order to
alleviate the noisy labels, existing approaches on FGNET analyze the entity
mentions entirely independent of each other and assign type labels solely based
on mention sentence-specific context. This is inadequate for highly overlapping
and noisy type labels as it hinders information passing across sentence
boundaries. For this, we propose an edge-weighted attentive graph convolution
network that refines the noisy mention representations by attending over
corpus-level contextual clues prior to the end classification. Experimental
evaluation shows that the proposed model outperforms the existing research by a
relative score of upto 10.2% and 8.3% for macro f1 and micro f1 respectively.","Named Entity Typing (NET) aims at classifying an entity mention to a set of entity types (e.g., person, location and organization) based on its context. It is one of the crucial components in NLP, as it helps in numerous down stream ing applications, e.g., information retrieval (Carlson et al. 2010), Knowledge Base Construction (KBC) (Dong et al. 2014), question answering (Lee et al. 2006), machine trans lation (Britz et al. 2017), etc. FineGrained Named En tity Typing (FGNET) is an extension of traditional NET to a much wide range of entity types (Corro et al. 2015; Ren et al. 2016a), typically over hundred types arranged in a hierarchical structure. It has shown promising results in different applications including KBC (Dong et al. 2014), re lation extraction (Mitchell et al. 2018), etc. In FGNET, an entity mention is labeled with multiple overlapping entity types based on the context. For instance, in the sentence: ‚ÄúAfter having recorded his role, Trump spent the whole day directing the movie. ‚Äù Trump can be an notated as both actor anddirector at the same time. Ow ing to a broad range of highly correlated entity types with M.A. ALi and W. Wang are the cocorresponding authors. Copyright c 2020, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.small contextual differences (Section 4.6), manual labeling is errorprone and timeconsuming, thus distant supervision is widely used to automatically acquire the training data. Distant supervision follows a twostep approach, i.e., detect ing the entity mentions followed by assigning type labels to the mentions using existing knowledge bases. However, it assigns type labels irrespective of the mention‚Äôs context, which results in high label noise (Ren et al. 2016b). This phenomenon is illustrated in Figure 1, where, for the sen tences denoted as: S1:S4, the entity mention ‚ÄúImran Khan‚Äù is labeled with all possible labels in the knowledgebase fperson, author, athlete, coach, politician g. Whereas, from the contextual perspective, in S1 the mention should be la beled asfperson, athleteg; in S2 it should be assigned labels fperson, authorg, etc. This label noise propagates in model learning, which hinders the improvement in performance. In an attempt to deal with the noisy training data, ex isting research on FGNET relies on the following differ ent approaches: (i) assume all labels to be correct (Ling and Weld 2012; Yogatama, Gillick, and Lazic 2015), which severely affects the model performance; (ii) apply differ ent pruning heuristics to prune the noisy labels (Gillick et al. 2014), however, these heuristics drastically reduce the size of training data; (iii) bifurcate the training data into two categories: clean and noisy, if the type labels corre spond to the same type path or otherwise (Ren et al. 2016a; Abhishek, Anand, and Awekar 2017), they ignore the fact that the labels, even corresponding to the same type path, may be noisy. For these approaches, it is hard to guarantee that underlying modeling assumptions will have a substan tial impact on alleviating the label noise. In addition, these approaches model the entity mentions entirely independent of each other, which hinders effective propagation of label speciÔ¨Åc contextual information across noisy entity mentions. In order to address the challenges associated with the noisy training data, we introduce a novel approach that puts an equal emphasis on analyzing the entity mentions w.r.t labelspeciÔ¨Åc corpuslevel context in addition to the sentencespeciÔ¨Åc context. SpeciÔ¨Åcally, we propose Fine Grained named Entity Typing with ReÔ¨Åned Representations (FGETRR), shown in Figure 2. FGETRR initially uses mention‚Äôs sentencespeciÔ¨Åc context to generate the noisy mention representation (PhaseI). Later, it uses corpuslevel contextual clues to form a sparse graph that surrounds a subarXiv:2004.03554v1  [cs.CL]  7 Apr 2020Entity	Mention:	Imran	Khan Candidate	types	via	Distant	Supervision: {person,	author ,	athlete,	coach,	politician} Entity	T ype	Hierarchy	( Yœà )S1:	The	former	cricket	great	Imran	khan	is	amongst	the	best	all rounders	of	his	time{ person,	athlete,	 author ,	coach,	politician } S2:	In	his	book	""Pakistan:	A	personal	history"",	Imran	khan focused	on	bilateral	ties{ person,	 athlete ,	 author ,	 coach,	politician } S3:	Former	cricketer	Imran	khan	is	selected	as	the	head	trainer	by Pakistan	cricket	board	{ person,	athlete,	 author , 	 coach, 	 politician } S4:	Imran	khan	founded	PTI,	a	centrist	political	party ,	in	1996. { person,	 athlete,	 author ,	coach, 	politician } (a) (b) (c)Root location person or ganization 		 author artist politician coach athleteFigure 1: (a) Entity mention and candidate entity types acquired via distant supervision, (b) Target Entity Type Hierarchy (c) Noisy training data with irrelevant entity types struckthrough. set of noisy mentions with a set of conÔ¨Ådent mentions hav ing high contextual overlap. And, performs edgeweighted attentive graph convolutions to recompute/reÔ¨Åne the repre sentation of noisy mention as an aggregate of the conÔ¨Ådent neighboring mentions lying at multiple hops (PhaseII). Fi nally, the reÔ¨Åned mention representation is embedded along with the type label representations for entity typing. We argue that the proposed framework has following ad vantages: (i) it allows appropriate information sharing by ef Ô¨Åcient propagation of corpuslevel contextual clues across noisy mentions; (ii) it analyzes the aggregated labelspeciÔ¨Åc context, which is more reÔ¨Åned compared with the noisy mentionspeciÔ¨Åc context; (iii) it effectively correlates the lo cal (sentencelevel) and the global (corpuslevel) context to reÔ¨Åne mention‚Äôs representation, required to perform the end task in a robust way. We summarize the major contributions of this paper as follows: We introduce FGETRR, a novel approach for FGNET that pays an equal importance on analyzing the entity mentions with respect to the corpuslevel context in addi tion to the sentencelevel context to perform entity typing in a performanceenhanced fashion. We propose an edgeweighted attentive graph convolution network to reÔ¨Åne the noisy mention representations. To the best of our knowledge, this is the Ô¨Årst work that, in contrast to the existing models that denoise the data at model‚Äôs input, reÔ¨Ånes the representations learnt over dis tantly supervised data. We demonstrate the effectiveness of the proposed model by comprehensive experimentation. FGETRR outper forms the existing research by a margin of upto 10.2% and 8.3% in terms of macrof1 and microf1 scores re spectively. 2 Related Work "
107,Real-time Localized Photorealistic Video Style Transfer.txt,"We present a novel algorithm for transferring artistic styles of semantically
meaningful local regions of an image onto local regions of a target video while
preserving its photorealism. Local regions may be selected either fully
automatically from an image, through using video segmentation algorithms, or
from casual user guidance such as scribbles. Our method, based on a deep neural
network architecture inspired by recent work in photorealistic style transfer,
is real-time and works on arbitrary inputs without runtime optimization once
trained on a diverse dataset of artistic styles. By augmenting our video
dataset with noisy semantic labels and jointly optimizing over style, content,
mask, and temporal losses, our method can cope with a variety of imperfections
in the input and produce temporally coherent videos without visual artifacts.
We demonstrate our method on a variety of style images and target videos,
including the ability to transfer different styles onto multiple objects
simultaneously, and smoothly transition between styles in time.","Color stylization plays a critical role in modern cine matography and video storytelling. It has the powerful abil ity to grab audience attention, elicit emotions, and convey implicit mood. For example, red usually evokes ideas of ac tion, adventure, and strength; orange can represent joy, cre ativity and stimulation; and green signiÔ¨Åes envy and health. In additional to applying color styles globally to the entire video, modern Ô¨Ålmmakers often utilize localized color tone changes; i.e., distinct color palettes applied to certain seg mented objects in the scene, as a powerful tool in video storytelling. Some wellknown examples include the Ô¨Ålm Schindler‚Äôs List [14], in which the famous scene of a girl in a red coat becomes the most memorable symbol of the Ô¨Ålm. Similarly, the Ô¨Ålm Sin City [15] uses high saturation (such as in red and yellow) to colorize certain characters or clothes, contrasting with the Ô¨Ålm‚Äôs classic blackandwhite noir theme. While color stylization is critical to video production, color style creation and editing today is still a labor intensive and timeconsuming process even with the use of professional editing software. Many operations are still manual in the editing process, such as rotoscoping, reÔ¨Åne 1arXiv:2010.10056v1  [cs.CV]  20 Oct 2020ment of segmentation masks, ensuring temporal consistency from frame to frame, and carefully Ô¨Ånetuning color tones. An automatic approach would be incredibly helpful. To automatically apply color styles to videos, a num ber of methods have recently been introduced based on ad vances in deep learning. For instance, several methods have explored the transfer of artistic styles to images or videos [9, 16, 23, 13, 35, 8, 40]. However, due to their nature asartistic style transfer methods, they all introduce unde sirable painterly spatial distortions. Another line of work [28, 24, 22, 45, 43] focuses on photorealistic style trans fer, which requires the output to maintain ‚Äúphotorealism‚Äù; i.e., the output should look as if it was taken by a real cam era (like most stylized Ô¨Ålms). However, since they primar ily target still photography, existing methods often generate visible Ô¨Çicker artifacts when they are applied to videos. To reduce temporal instability, several methods have tried min imizing an optical Ô¨Çow warping loss [12, 11] or sequentially propagating intermediate features [20]. To our knowledge, there is no existing work that can successfully perform pho torealistic localized style transfer on videos with reasonably good runtime speed. In this paper, we present a novel approach that simulta neously addresses three major challenges in localized pho torealistic video style transfer: 1) spatial and temporal style coherence over time, 2) robustness against imperfect seg mentation masks, and 3) highspeed processing. To achieve highspeed performance, we extend our approach from the recent work of Xia et al. [43], which learns local edgeaware afÔ¨Åne transforms from lowresolution content and style in puts, with the results sliced out from a compact transform representation at the full resolution. To minimize spatial ar tifacts and improve the temporal coherence, we propose a novel spatiotemporal feature transfer layer (STAdaIN) that is able to transfer style to local regions and generate tem porally coherent stylized videos. To handle imperfect seg mentation masks, our algorithm learns an enhancement net work to improve the boundaries of masks. Moreover, we compose foreground and background color transform coef Ô¨Åcients in the grid space, resulting in high quality results even if segmentation masks are inaccurate. Experimental results demonstrate that our model generates stylized videos with fewer visual artifacts and better temporal coherence than existing photorealistic style transfer methods. In summary, our contributions in this work are threefold: ‚Ä¢ A differentiable spatiotemporal style transfer layer (STAdaIN) to match the feature statistics of local re gions, generating temporally coherent stylized results (Section 3.2). ‚Ä¢ Mask enhancement and gridspace blending algo rithms that can render natural style transitions between objects given noisy selection masks (Section 3.3). ‚Ä¢ A deep neural network for photorealistic video styletransfer that runs in realtime (26.5 Hz at 10241024 resolution). 2. Related Work "
108,Training Deep Neural Networks on Noisy Labels with Bootstrapping.txt,"Current state-of-the-art deep learning systems for visual object recognition
and detection use purely supervised training with regularization such as
dropout to avoid overfitting. The performance depends critically on the amount
of labeled examples, and in current practice the labels are assumed to be
unambiguous and accurate. However, this assumption often does not hold; e.g. in
recognition, class labels may be missing; in detection, objects in the image
may not be localized; and in general, the labeling may be subjective. In this
work we propose a generic way to handle noisy and incomplete labeling by
augmenting the prediction objective with a notion of consistency. We consider a
prediction consistent if the same prediction is made given similar percepts,
where the notion of similarity is between deep network features computed from
the input data. In experiments we demonstrate that our approach yields
substantial robustness to label noise on several datasets. On MNIST handwritten
digits, we show that our model is robust to label corruption. On the Toronto
Face Database, we show that our model handles well the case of subjective
labels in emotion recognition, achieving state-of-the- art results, and can
also benefit from unlabeled face images with no modification to our method. On
the ILSVRC2014 detection challenge data, we show that our approach extends to
very deep networks, high resolution images and structured outputs, and results
in improved scalable detection.","Currently the predominant systems for visual object recognition and detection (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014) use purely supervised training with regularization such as dropout (Hinton et al., 2012) to avoid overÔ¨Åtting. These systems do not account for missing labels, subjective labeling or inexhaustively annotated images. However, this assumption often does not hold, especially for very large datasets and in highresolution images with complex scenes. For example, in recognition, the class labels may be missing; in detection, the objects in the image may not all be localized; in subjective tasks such as facial emotion recognition, humans may not even agree on the class label. As training sets for deep networks become larger (as they should), the problem of missing and noisy labels becomes more acute, and so we argue it is a fundamental problem for scaling up vision. In this work we propose a simple approach to hande noisy and incomplete labeling in weakly supervised deep learning, by augmenting the usual prediction objective with a notion of perceptual consistency. We consider a prediction consistent if the same prediction is made given similar per cepts, where the notion of similarity incorporates features learned by the deep network. One interpretation of the perceptual consistency objective is that the learner makes use of its rep resentation of the world (implicit in the network parameters) to match incoming percepts to known 1arXiv:1412.6596v3  [cs.CV]  15 Apr 2015Accepted as a workshop contribution at ICLR 2015 categories, or in general structured outputs. This provides the learner justiÔ¨Åcation to ‚Äúdisagree‚Äù with a perceptuallyinconsistent training label, and effectively relabel the data while training. More accurate labels may lead to a better model, which allows further label cleanup, and the learner bootstraps itself in this way. Of course, too much skepticism of the labels carries the risk of ending up with a delusional agent, so it is important to balance the tradeoff between prediction and the learner‚Äôs perceptual consistency. In our experiments we demonstrate that our approach yields substantial robustness to several types of label noise on several datasets. On MNIST handwritten digits (LeCun & Cortes, 1998) we show that our model is robust to label corruption. On the Toronto Face Database (Susskind et al., 2010) we show that our model handles well the case of subjective labels in emotion recognition, achieving stateoftheart results, and can also beneÔ¨Åt from unlabeled face images with no modiÔ¨Åcation to our method. On the ILSVRC2014 detection challenge data (Russakovsky et al., 2014), we show that our approach improves singleshot person detection using a MultiBox network (Erhan et al., 2014), and also improves performance in full 200way detection using MultiBox for region proposal and a deep CNN for postclassiÔ¨Åcation. In section 2 we discuss related work, in section 3 we describe our method along with a probabilistic interpretation and in section 4 we present our results. 2 R ELATED WORK "
109,Diverse Ensembles Improve Calibration.txt,"Modern deep neural networks can produce badly calibrated predictions,
especially when train and test distributions are mismatched. Training an
ensemble of models and averaging their predictions can help alleviate these
issues. We propose a simple technique to improve calibration, using a different
data augmentation for each ensemble member. We additionally use the idea of
`mixing' un-augmented and augmented inputs to improve calibration when test and
training distributions are the same. These simple techniques improve
calibration and accuracy over strong baselines on the CIFAR10 and CIFAR100
benchmarks, and out-of-domain data from their corrupted versions.","Modern neural network models can produce overconÔ¨Ådent or miscalibrated predictions, even when training examples are independent and identically distributed (i.i.d.) to th e test distribution. This miscalibration is exacerbated when the training and testing distributions are different. In safet y critical scenarios, the ability to accurately represent mo del uncertainty is valuable. Such model miscalibration has been shown to reduce when we train an ensemble of models and average their predic tions ( Lakshminarayanan et al. ,2017 ;Ovadia et al. ,2019 ). Ensembles have long been known to improve generalisa tion ( Hansen & Salamon ,1990 ), especially when an en semble is diverse, which is promoted with various tech niques such as using latent variables ( Sinha et al. ,2020 ) or diversityencouraging losses and architecture changes (Kim et al. ,2018 ;Lee et al. ,2016 ;Pang et al. ,2019 ). Re cent work proposes ‚Äòcheap‚Äô ensembles by sharing most of the model parameters across all ensembles, and using rank 1 factors to modify the linear layers in an ensemble member (Wen et al. ,2020 ), making ensembles easier to train and 1School of Informatics, University of Edinburgh. Correspon  dence to: Asa Cooper Stickland <a.cooper.stickland@ed.ac.uk >. Presented at the ICML 2020 Workshop on Uncertainty and Ro bustness in Deep Learning. Copyright 2020 by the author(s).store. Another longstanding way to improve generalization isData Augmentation , i.e. expanding our training set with modiÔ¨Åed copies ( Zhang et al. ,2018 ;Yun et al. ,2019 ; Cubuk et al. ,2019 ). Recent examples include work by Hendrycks et al. (2020 ) and Xie et al. (2019 ). These ap proaches exploit the intuition that a blurry or rotated imag e should have the same class as the original image. This work extends and combines recent work on cheap en sembles and data augmentation. We increase ensemble di versity by applying different augmentations to each ensem ble member. This method improves calibration on an i.i.d. test set, and accuracy and calibration on outofdistribut ion test sets for the CIFAR10 and CIFAR100 datasets. We ad ditionally simplify the idea of ‚Äòmixing‚Äô unaugmented and augmented inputs introduced by Hendrycks et al. (2020 ), and explore adversarial perturbations, which apply more generally and result in better performance on i.i.d. data. 2. Methods "
110,Robust Long-Tailed Learning under Label Noise.txt,"Long-tailed learning has attracted much attention recently, with the goal of
improving generalisation for tail classes. Most existing works use supervised
learning without considering the prevailing noise in the training dataset. To
move long-tailed learning towards more realistic scenarios, this work
investigates the label noise problem under long-tailed label distribution. We
first observe the negative impact of noisy labels on the performance of
existing methods, revealing the intrinsic challenges of this problem. As the
most commonly used approach to cope with noisy labels in previous literature,
we then find that the small-loss trick fails under long-tailed label
distribution. The reason is that deep neural networks cannot distinguish
correctly-labeled and mislabeled examples on tail classes. To overcome this
limitation, we establish a new prototypical noise detection method by designing
a distance-based metric that is resistant to label noise. Based on the above
findings, we propose a robust framework,~\algo, that realizes noise detection
for long-tailed learning, followed by soft pseudo-labeling via both label
smoothing and diverse label guessing. Moreover, our framework can naturally
leverage semi-supervised learning algorithms to further improve the
generalisation. Extensive experiments on benchmark and real-world datasets
demonstrate the superiority of our methods over existing baselines. In
particular, our method outperforms DivideMix by 3\% in test accuracy. Source
code will be released soon.","ClassiÔ¨Åcation problems in realworld typically exhibit a longtailed label distribution, where most classes are associated with only a few examples, e.g., visual recognition [ 1,2,3], instance segmentation [ 4], and text categorization [ 5]. Due to the paucity of training examples, generalisation for tail classes is challenging; moreover, na√Øve learning on such data is susceptible to an undesirable bias towards head classes. Recently, longtailed learning (LTL) has gained renewed interest in the context of deep neural networks [ 6,7,8,9,10,11,12,13]. Two active strands of work involve normalisation of the classiÔ¨Åer‚Äôs weights, and modiÔ¨Åcation of the underlying loss to account for different class penalties. Each of these strands is intuitive, and has been empirically shown to be effective [14]. The abovementioned LTL methods with remarkable performance are mostly trained on clean datasets with highquality human annotations. However, in realworld machine learning applications, annotating a largescale dataset is costly and timeconsuming. Some recent works resort to the large amount of web data as a source of supervision for training deep neural networks [ 15]. While the existing works have shown advantages in various applications [ 16,17], web data is naturally classimbalanced [ 18,19] and accompanied with label noise [ 20,18,21,22]. As a result, it is crucial that deep neural networks can harvest noisy and classimbalanced training data. Although the LTL and noisy label problems have been extensively studied in previous literature, it is still poorly explored when the training dataset follows a longtailed label distribution while contains label noise. We provide a simple visualization of the studied problem in Figure 1a. Without considering label noise, we show that LTL methods severely degrade their performance in experiments. To address this problem, a direct approach is to apply methods equal contributionarXiv:2108.11569v1  [cs.LG]  26 Aug 2021Robust LongTailed Learning under Label Noise A P REPRINT /uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c /uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000013/uni00000014/uni0000004e/uni00000015/uni0000004e/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000026/uni0000004f/uni00000048/uni00000044/uni00000051 /uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c /uni00000037/uni00000048/uni00000056/uni00000057 (a) Problem setup /uni00000030/uni00000044/uni00000051/uni0000005c /uni00000030/uni00000048/uni00000047/uni0000004c/uni00000058/uni00000050 /uni00000029/uni00000048/uni0000005a/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000027/uni0000004c/uni00000059/uni0000004c/uni00000047/uni00000048/uni00000030/uni0000004c/uni0000005b /uni00000035/uni00000052/uni0000002f/uni00000037/uni0000000e (b) CIFAR10 /uni00000030/uni00000044/uni00000051/uni0000005c /uni00000030/uni00000048/uni00000047/uni0000004c/uni00000058/uni00000050 /uni00000029/uni00000048/uni0000005a/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000027/uni0000004c/uni00000059/uni0000004c/uni00000047/uni00000048/uni00000030/uni0000004c/uni0000005b /uni00000035/uni00000052/uni0000002f/uni00000037/uni0000000e (c) CIFAR100 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000048/uni00000055/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018/uni00000035/uni00000048/uni00000053/uni00000055/uni00000048/uni00000056/uni00000048/uni00000051/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni0000001a/uni0000001c/uni00000011/uni00000019 /uni0000001a/uni0000001c/uni00000011/uni00000019 /uni0000001a/uni0000001c/uni00000011/uni0000001a /uni0000001a/uni0000001c/uni00000011/uni00000019 /uni0000001a/uni0000001c/uni00000011/uni00000019 /uni0000001a/uni0000001b/uni00000011/uni00000019 /uni00000019/uni0000001b/uni00000011/uni00000015 /uni00000019/uni0000001b/uni00000011/uni00000014 /uni00000019/uni0000001a/uni00000011/uni00000014 /uni00000019/uni00000019/uni00000011/uni00000015 /uni00000019/uni00000018/uni00000011/uni00000016 /uni00000019/uni00000017/uni00000011/uni00000018 /uni00000019/uni00000019/uni00000011/uni0000001b /uni00000019/uni00000019/uni00000011/uni00000019 /uni00000019/uni00000019/uni00000011/uni00000015 /uni00000019/uni00000019/uni00000011/uni00000013 /uni00000019/uni00000018/uni00000011/uni00000013 /uni00000019/uni00000016/uni00000011/uni0000001a /uni00000019/uni00000014/uni00000011/uni0000001a /uni00000019/uni00000014/uni00000011/uni00000015 /uni00000019/uni00000014/uni00000011/uni00000014 /uni00000019/uni00000013/uni00000011/uni0000001c /uni00000019/uni00000013/uni00000011/uni00000013 /uni00000018/uni0000001c/uni00000011/uni00000015 /uni00000018/uni00000019/uni00000011/uni00000019 /uni00000018/uni00000019/uni00000011/uni00000018 /uni00000018/uni00000019/uni00000011/uni00000015 /uni00000018/uni00000018/uni00000011/uni0000001c /uni00000018/uni00000018/uni00000011/uni00000018 /uni00000018/uni00000017/uni00000011/uni00000019 /uni00000017/uni00000017/uni00000011/uni0000001a /uni00000017/uni00000017/uni00000011/uni00000014 /uni00000017/uni00000017/uni00000011/uni00000013 /uni00000017/uni00000016/uni00000011/uni00000015 /uni00000017/uni00000016/uni00000011/uni00000014 /uni00000017/uni00000015/uni00000011/uni00000019 (d) Confusion matrix Figure 1: (a) Illustration of the studied problem setup. (bc) Comparison of DivideMix and ROLT+ on datasets with imbalance ratio 100and noise level 0:2. (d) We show the test accuracy of NCM classiÔ¨Åer under different noise levels by disentangling representation and classiÔ¨Åer learning. for learning with noisy labels to LTL. One of the most commonly used approaches for learning with noisy labels is DivideMix [ 18], which uses the smallloss criterion to detect label noise. However, we note that using such approach leads to unsatisfactory results in longtailed label distribution, as shown in Figure 1b and 1c. Therefore, it remains a challenge to obtain models that can cope with LTL under label noise. To achieve performance improvement, it is a natural idea to detect noisy data while accommodating class imbalance. It is known that a classiÔ¨Åer trained on longtailed data yields higher accuracy for head classes but hurts tail classes [ 9]. Thus, to detect label noise, it is not trustworthy to use predictions and training losses produced by the biased classiÔ¨Åer. Another commonly used approach for LTL is the nearest class mean (NCM) classiÔ¨Åer that computes class prototypes and performs nearest neighbour search in embedding space [ 9]. In Figure 1d, we test NCM by disentangling the representation and classiÔ¨Åer learning. We Ô¨Årst train a feature extractor, then compute class prototypes by polluting clean labels with different noise level. It depicts that the estimation of class prototypes is robust to label noise. This observation motivates us to explore the geometric information between examples and their class prototypes as a criterion for noise detection. After acquiring the prototypes, we design a classindependent noise detector by treating examples closed to their corresponding prototypes as clean, while others as noisy. Unlike learning from balanced datasets where noisy data can be removed from training [ 23], we claim that each example is signiÔ¨Åcant, especially for tail classes. To this end, we introduce a new soft pseudolabeling mechanism that uses both label smoothing and label guessing to guide the learning of networks. Thanks to the generality of our proposed noise detection method, we can also interpret noisy examples as unlabeled data and incorporate wellestablished semisupervised learning techniques to further improve the generalisation. Our main contributions are: (i) We study the problem of longtailed learning under label noise, which is less explored and is a signiÔ¨Åcant step towards realworld applications; (ii) We Ô¨Ånd that the commonly used smallloss trick fails in longtailed learning. Thus, we establish a novel prototypical noise detection method that overcomes the limitations of smallloss trick; (iii) We propose a robust framework, ROLT. It realizes noise detection that is immune to label distribution, and compensates the problem of data scarcity for tail classes. Our framework can be built on top of semisupervised learning methods without much extra overhead, leading to an improved approach ROLT+. The proposed methods achieve strong empirical performance on benchmark and realworld datasets. 2 Related Work "
111,A Novel Ensemble Deep Learning Model for Stock Prediction Based on Stock Prices and News.txt,"In recent years, machine learning and deep learning have become popular
methods for financial data analysis, including financial textual data,
numerical data, and graphical data. This paper proposes to use sentiment
analysis to extract useful information from multiple textual data sources and a
blending ensemble deep learning model to predict future stock movement. The
blending ensemble model contains two levels. The first level contains two
Recurrent Neural Networks (RNNs), one Long-Short Term Memory network (LSTM) and
one Gated Recurrent Units network (GRU), followed by a fully connected neural
network as the second level model. The RNNs, LSTM, and GRU models can
effectively capture the time-series events in the input data, and the fully
connected neural network is used to ensemble several individual prediction
results to further improve the prediction accuracy. The purpose of this work is
to explain our design philosophy and show that ensemble deep learning
technologies can truly predict future stock price trends more effectively and
can better assist investors in making the right investment decision than other
traditional methods.","Many factors may affect stock prices in various ways. The stock prices change by market forces, which means the stock price changes react to supply and demand in the stock market. If more people want to buy a stock (demand) than sell it (supply), then the price moves up. Similarly, if more people want to sell a stock than buy it, there would be greater supply than demand, and the price would fall. Stock supply and demand are affected by many things. Supply factors include company share issues (e.g., releases new shares to the public), share buybacks (e.g., a company buys back its own shares from investors to reduce supply) and sellers (e.g., the investors responsible for pushing shares back into the market, increasing the supply). Demand factors include company news (e.g., a new product launch, missed targets, good performance), economic factors (e.g., interest rate changes), industry trends (e.g., a booming industry), market sentiment (could be psychological and subjective) and unexpected events (e.g., natural disasters or the death of a government leader). Normally, we can get these supply and demand factors from the Ô¨Ånancial news, companies‚Äô newsletters or their annual reports. For instance, when Apple announces a new product, many people would like to purchase it, and its performance usually would be better soon. Thus more people are interested in the Apple stock, then the Apple stock demand increases, which will lead to a rise in the Apple stock price. On the other hand, when COVID19 spreads around the world, many airlines cut their Ô¨Çights, and it is expected their performance would be bad in a short term. Thus, more people want to sell airline stocks; then the airline stock supply will rise, and their price will go down. If the price goes up, the quantity demanded goes down (but demand itself stays the same). If the price decreases, 1arXiv:2007.12620v1  [qfin.ST]  23 Jul 2020quantity demanded increases. This is the Law of Demand. If the quantity demanded decreases, the stock price probably would fall. Also people‚Äôs sentiment or belief plays a role in determining a stock price. Political situations or international affairs may also affect stock prices. Hence, this is a complicated process among the stock supply, demand, and prices. However, there are a few primary factors that affect the stock supply and demand like company news, company performance, industry performance, investor sentiment (e.g., whether in bull or bear market), and other major economic factors described in [2]. If we focus on the major factors, and trace back the historical stock prices, we may be able to predict future stock prices quite accurately. People usually have a short memory about stock factors. Hence, determining a suitable historical window size is important to correctly predict stock prices. If the window size is too large compared with human memory, many factors or news are forgotten by investors and obsolete already and the prediction will not be good. On the other hand, if the window is too short compared with human memory, many news or sentiments outside the window are still remain in people‚Äôs brain, the prediction will also be bad. Hence, a wrong historical window size is detrimental to our successful stock price predictions. Stock price prediction is a series of continuous predictions since the stock price is constantly chang ing to react to timely news and announcements. Therefore, it is very challenging for computer scientists to use ArtiÔ¨Åcial Intelligence to predict future stock movements because it is hard for a computer to receive the latest information and respond immediately. Computer Scientists are cur rently not particularly successful in stock price prediction for several reasons. Most of the previous works[3][4] often used either textual data like news, twitter, or blogs or numerical data like stock price information instead of using both textual information and statistical information[5]. Since the stock price is related to many factors, only considering one or two factors is unable to provide enough information to forecast the stock price trend. Including as much relevant and useful information as possible will guarantee a better prediction. Furthermore, previous works[3][4] only use the target company‚Äôs information on the training model without considering that the target company‚Äôs competitors or the information of companies in re lated industries. These types of information will also affect the target company‚Äôs stock movement. Therefore, the result is not very satisfactory and persuasive because the information provided is insufÔ¨Åcient. Moreover, some of the previous works, which used the textual information, did not consider time series. However, the timeline is a signiÔ¨Åcant factor for stock price prediction. This paper proposes to use sentiment analysis to extract useful information from multiple textual data sources and a blending ensemble deep learning model to predict future stock movement. The blending ensemble model contains two levels. The Ô¨Årst level contains two Recurrent Neural Net works (RNNs), one LongShort Term Memory network (LSTM) and one Gated Recurrent Units network (GRU), followed by a fully connected neural network as the second level model. The RNNs, LSTM, and GRU models can effectively capture the time series events in the input data, and the fully connected neural network is used to ensemble several individual prediction results to further improve the prediction accuracy. 2 Related Work "
112,Domain-Adversarial Training of Neural Networks.txt,"We introduce a new representation learning approach for domain adaptation, in
which data at training and test time come from similar but different
distributions. Our approach is directly inspired by the theory on domain
adaptation suggesting that, for effective domain transfer to be achieved,
predictions must be made based on features that cannot discriminate between the
training (source) and test (target) domains. The approach implements this idea
in the context of neural network architectures that are trained on labeled data
from the source domain and unlabeled data from the target domain (no labeled
target-domain data is necessary). As the training progresses, the approach
promotes the emergence of features that are (i) discriminative for the main
learning task on the source domain and (ii) indiscriminate with respect to the
shift between the domains. We show that this adaptation behaviour can be
achieved in almost any feed-forward model by augmenting it with few standard
layers and a new gradient reversal layer. The resulting augmented architecture
can be trained using standard backpropagation and stochastic gradient descent,
and can thus be implemented with little effort using any of the deep learning
packages. We demonstrate the success of our approach for two distinct
classification problems (document sentiment analysis and image classification),
where state-of-the-art domain adaptation performance on standard benchmarks is
achieved. We also validate the approach for descriptor learning task in the
context of person re-identification application.","The cost of generating labeled data for a new machine learning task is often an obstacle for applying machine learning methods. In particular, this is a limiting factor for the fur ther progress of deep neural network architectures, that have already brought impressive advances to the stateoftheart across a wide variety of machinelearning tasks and appli cations. For problems lacking labeled data, it may be still possible to obtain training sets that are big enough for training largescale deep models, but that suer from the shift in data distribution from the actual data encountered at \test time"". One important example is training an image classier on synthetic or semisynthetic images, which may come in abundance and be fully labeled, but which inevitably have a distribution that is dierent from real images (Liebelt and Schmid, 2010; Stark et al., 2010; V azquez et al., 2014; Sun and Saenko, 2014). Another example is in the context of sentiment analysis in written reviews, where one might have labeled data for reviews of one type of product ( e.g., movies), while having the need to classify reviews of other products ( e.g., books). Learning a discriminative classier or other predictor in the presence of a shift be tween training and test distributions is known as domain adaptation (DA). The proposed approaches build mappings between the source (trainingtime) and the target (testtime) domains, so that the classier learned for the source domain can also be applied to the target domain, when composed with the learned mapping between domains. The appeal of the domain adaptation approaches is the ability to learn a mapping between domains in the situation when the target domain data are either fully unlabeled ( unsupervised domain annotation ) or have few labeled samples ( semisupervised domain adaptation ). Below, we focus on the harder unsupervised case, although the proposed approach ( domainadversarial learning ) can be generalized to the semisupervised case rather straightforwardly. Unlike many previous papers on domain adaptation that worked with xed feature representations, we focus on combining domain adaptation and deep feature learning within one training process. Our goal is to embed domain adaptation into the process of learning representation, so that the nal classication decisions are made based on features that are both discriminative and invariant to the change of domains, i.e., have the same or very similar distributions in the source and the target domains. In this way, the obtained feedforward network can be applicable to the target domain without being hindered by the shift between the two domains. Our approach is motivated by the theory on domain adaptation (BenDavid et al., 2006, 2010), that suggests that a good representation for crossdomain transfer is one for which an algorithm cannot learn to identify the domain of origin of the input observation. We thus focus on learning features that combine (i) discriminativeness and (ii) domain invariance. This is achieved by jointly optimizing the underlying features as well as two discriminative classiers operating on these features: (i) the label predictor that predicts class labels and is used both during training and at test time and (ii) the domain classier that discriminates between the source and the target domains during training. While the parameters of the classiers are optimized in order to minimize their error on the training set, the parameters of the underlying deep feature mapping are optimized in order to minimize the loss of the label classier and to maximize the loss of the domain classier. The latter 2DomainAdversarial Neural Networks update thus works adversarially to the domain classier, and it encourages domaininvariant features to emerge in the course of the optimization. Crucially, we show that all three training processes can be embedded into an appro priately composed deep feedforward network, called domainadversarial neural network (DANN) (illustrated by Figure 1, page 12) that uses standard layers and loss functions, and can be trained using standard backpropagation algorithms based on stochastic gradi ent descent or its modications ( e.g., SGD with momentum). The approach is generic as a DANN version can be created for almost any existing feedforward architecture that is trainable by backpropagation. In practice, the only nonstandard component of the pro posed architecture is a rather trivial gradient reversal layer that leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during the backpropagation. We provide an experimental evaluation of the proposed domainadversarial learning idea over a range of deep architectures and applications. We rst consider the simplest DANN architecture where the three parts (label predictor, domain classier and feature extractor) are linear, and demonstrate the success of domainadversarial learning for such architecture. The evaluation is performed for synthetic data as well as for the sentiment analysis problem in natural language processing, where DANN improves the stateoftheart marginalized Stacked Autoencoders (mSDA) of Chen et al. (2012) on the common Amazon reviews benchmark. We further evaluate the approach extensively for an image classication task, and present results on traditional deep learning image data sets|such as MNIST (LeCun et al., 1998) and SVHN (Netzer et al., 2011)|as well as on Office benchmarks (Saenko et al., 2010), where domainadversarial learning allows obtaining a deep architecture that considerably improves over previous stateoftheart accuracy. Finally, we evaluate domainadversarial descriptor learning in the context of person reidentication application (Gong et al., 2014), where the task is to obtain good pedes trian image descriptors that are suitable for retrieval and verication. We apply domain adversarial learning, as we consider a descriptor predictor trained with a Siameselike loss instead of the label predictor trained with a classication loss. In a series of experiments, we demonstrate that domainadversarial learning can improve crossdataset reidentication considerably. 2. Related work "
113,Augmentation Strategies for Learning with Noisy Labels.txt,"Imperfect labels are ubiquitous in real-world datasets. Several recent
successful methods for training deep neural networks (DNNs) robust to label
noise have used two primary techniques: filtering samples based on loss during
a warm-up phase to curate an initial set of cleanly labeled samples, and using
the output of a network as a pseudo-label for subsequent loss calculations. In
this paper, we evaluate different augmentation strategies for algorithms
tackling the ""learning with noisy labels"" problem. We propose and examine
multiple augmentation strategies and evaluate them using synthetic datasets
based on CIFAR-10 and CIFAR-100, as well as on the real-world dataset
Clothing1M. Due to several commonalities in these algorithms, we find that
using one set of augmentations for loss modeling tasks and another set for
learning is the most effective, improving results on the state-of-the-art and
other previous methods. Furthermore, we find that applying augmentation during
the warm-up period can negatively impact the loss convergence behavior of
correctly versus incorrectly labeled samples. We introduce this augmentation
strategy to the state-of-the-art technique and demonstrate that we can improve
performance across all evaluated noise levels. In particular, we improve
accuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in
absolute accuracy, and we also improve performance on the Clothing1M dataset.
  (K. Nishi and Y. Ding contributed equally to this work)","Data augmentation is a common method used to expand datasets and has been applied successfully in many com puter vision problems such as image classiÔ¨Åcation [32] and object detection [28], among many others. In particular, *Equal contribution Source code is available at https://github.com/KentoNishi/ AugmentationforLNL .there has been much success using learned augmentations such as AutoAugment [6] and RandAugment [7] which do not require an expert who knows the dataset to curate aug mentation policies. It has been shown that incorporating augmentation policies during training can improve gener alization and robustness [12, 8]. However, few works have explored their efÔ¨Åcacy for the domain of learning with noisy labels (LNL) [21]. Many techniques which tackle the LNL problem make use of the network memorization effect, where correctly la beled data Ô¨Åt before incorrectly labeled data as discovered by Arpit et al. [2]. This phenomenon was successfully ex plored in Deep Neural Networks (DNNs) through model ing the loss function and the training process, leading to the development of approaches such as loss correction [29] and sample selection [10]. Recently, the incorporation of MixUp augmentation [35] has dramatically improved the ability for algorithms to tolerate higher noise levels [1, 14]. While many existing works use the common random Ô¨Çip and crop image augmentation which we refer to as weak augmentation , to the best of our knowledge, no work at the time of writing has explored using more aggressive aug mentation from learned policies such as AutoAugment dur ing training for LNL algorithms. These stronger augmenta tion policies include transformations such as rotate, invert, sheer, etc. We propose to incorporate these stronger aug mentation policies into existing architectures in a strategic way to improve performance. Our intuition is that for any augmentation technique to succeed, they must (1)improve the generalization of the dataset and (2)not negatively im pact the loss modeling and loss convergence behavior that LNL techniques rely on. With this in mind, we propose an augmentation strategy we call Augmented Descent (A UGDESC) to beneÔ¨Åt from data augmentation without negatively impacting the net work memorization effect. Our idea for A UGDESC is to use two different augmentations: a weak augmentation for any loss modeling and pseudolabeling task, and a strong augmentation for the backpropagation step to improve genarXiv:2103.02130v3  [cs.CV]  1 Apr 2021eralization. In this paper, we propose and examine how we can incor porate stronger augmentation into existing LNL algorithms to yield improved results. We provide some answers to this problem through the following contributions: ‚Ä¢ We propose an augmentation strategy, Augmented Descent, which demonstrates stateoftheart perfor mance on synthetic and realworld datasets under noisy label scenarios. We show empirically that this can increase performance across all evaluated noise levels (Section 4.4). In particular, we improve accu racy on the CIFAR10 benchmark at 90% symmetric noise by more than 15% in absolute accuracy, and we also improve performance on the realworld dataset Clothing1M (Section 4.5). ‚Ä¢ We show that there is a large effect on performance de pending on how augmentation is incorporated into the training process (Section 4.2). We empirically deter mine that it is best to use weaker augmentation during earlier epochs followed by stronger augmentations to not adversely affect the memorization effect. We ana lyze the behavior of loss distribution to yield insight to guide effective incorporation of augmentation in future work (Section 4.3). ‚Ä¢ We evaluate the effectiveness of our augmentation methodology by performing generalization studies on existing techniques (Section 4.7). Without tuning any hyperparameters, we were able to improve existing techniques with only the addition of our proposed aug mentation strategy by up to 5% in absolute accuracy. 2. Related Work "
114,Activation Ensembles for Deep Neural Networks.txt,"Many activation functions have been proposed in the past, but selecting an
adequate one requires trial and error. We propose a new methodology of
designing activation functions within a neural network at each layer. We call
this technique an ""activation ensemble"" because it allows the use of multiple
activation functions at each layer. This is done by introducing additional
variables, $\alpha$, at each activation layer of a network to allow for
multiple activation functions to be active at each neuron. By design,
activations with larger $\alpha$ values at a neuron is equivalent to having the
largest magnitude. Hence, those higher magnitude activations are ""chosen"" by
the network. We implement the activation ensembles on a variety of datasets
using an array of Feed Forward and Convolutional Neural Networks. By using the
activation ensemble, we achieve superior results compared to traditional
techniques. In addition, because of the flexibility of this methodology, we
more deeply explore activation functions and the features that they capture.","Most of the recent advancements in the mathematics of neural networks come from four areas: network architec ture (Jaderberg et al., 2015) (Gulcehre et al., 2016a), opti mization method (AdaDelta (Zeiler, 2012) and Batch Nor malization (Ioffe & Szegedy, 2015) ), activations func tions, and objective functions (such as Mollifying Net works (Gulcehre et al., 2016b) ). Highway Networks (Sri vastava et al., 2015) and Residual Networks (He et al., 2016) both use the approach of adding data from a previ ous layer to one further ahead for more effective learning. On the other hand, others use Memory Networks (Weston et al., 2015) to more effectively remember past data and even answer questions about short paragraphs. These new architectures move the Ô¨Åeld of neural networks and ma chine learning forward, but one of the main driving forces that brought neural networks back into popularity is the rec tiÔ¨Åer linear unit (ReLU)(Glorot et al., 2011) (Nair & Hin ton, 2010). Because of trivial calculations in both forwardand backward steps and its ability to more effectively help a network learn, ReLU‚Äôs revolutionized the way neural net works are studied. One technique that universally increases accuracy is en sembling multiple predictive models. There are books and articles that explain the advantages of using multiple mod els rather than a single large model (Zhang & Ma, 2012). When neural networks garnered more popularity in the 90‚Äôs and early 2000‚Äôs, researchers used the same technique of ensembles (Zhou et al., 2002). Additionally, other tech niques may identify as an ensemble. For example, the na ture of Dropout (Srivastava et al., 2014) trains many differ ent networks together by dropping nodes from the entirety of a network. We focus on activation functions rather than expanding the general architectures of networks. Our work can be seen as a layer or neuron ensemble that can be applied to any variety of deep neural network via activation ensembles. We do not focus on creating yet another unique activation function, but rather ensemble a number of proven activation functions in a compelling way. The end result is a novel activation function that is a combination of existing func tions. Each activation function, from ReLU to hyperbolic tangent contain advantages in learning. We propose to use the best parts of each in a dynamic way decided by vari ables conÔ¨Åguring contributions of each activation function. These variables put weights on each activation function un der consideration and are optimized via backpropagation. As data passes through a deep neural network, each layer transforms the data to better interpret and gather features. Therefore, the best possible function at the top of a network may not be optimal in the middle or bottom of a network. The advantage of our architecture is that rather than choos ing activations at speciÔ¨Åed layers or over an entire network, one can give the network the option to choose the best pos sible activation function of each neuron at each layer. Creating an ensemble of activation functions presents some challenges in how to best combine the activation functions to extract as much information from a given dataset as pos sible. In the most basic model ensembles, one can average the given probabilities of multiple models. This is only feasible because the range of values in each model are the same, which is not replicated in most activation functions.arXiv:1702.07790v1  [stat.ML]  24 Feb 2017Activation Ensembles for Deep Neural Networks The difÔ¨Åculty lies in restricting or expanding the range of these functions without losing their inherent performance. An activation ensemble consists of two important parts. The Ô¨Årst is the main parameter attached to each activation function for each neuron. This variable assigns a weight to each activation function considered, i.e., it designs a convex combination of activation functions. The second are a set of ‚Äúoffset‚Äù parameters, and, which we use to dynamically offset normalization range for each function. Training of these new parameters occurs during typical model training and is done through backpropagation. Our work contains two signiÔ¨Åcant contributions. First, we implement novel activation functions as convex combina tions of known functions with interesting properties based upon current knowledge of activation functions and learn ing. Second, we improve the learning of any network con sidered herein, including the wellestablished residual net work in the Cifar100 learning task. 2. Related Work "
115,Local Multi-Label Explanations for Random Forest.txt,"Multi-label classification is a challenging task, particularly in domains
where the number of labels to be predicted is large. Deep neural networks are
often effective at multi-label classification of images and textual data. When
dealing with tabular data, however, conventional machine learning algorithms,
such as tree ensembles, appear to outperform competition. Random forest, being
a popular ensemble algorithm, has found use in a wide range of real-world
problems. Such problems include fraud detection in the financial domain, crime
hotspot detection in the legal sector, and in the biomedical field, disease
probability prediction when patient records are accessible. Since they have an
impact on people's lives, these domains usually require decision-making systems
to be explainable. Random Forest falls short on this property, especially when
a large number of tree predictors are used. This issue was addressed in a
recent research named LionForests, regarding single label classification and
regression. In this work, we adapt this technique to multi-label classification
problems, by employing three different strategies regarding the labels that the
explanation covers. Finally, we provide a set of qualitative and quantitative
experiments to assess the efficacy of this approach.","Multilabel classiÔ¨Åcation is a popular machine learning task, concerned with assigning multiple different labels to a single sample [ 1]. There are plenty of applications employing multilabel classiÔ¨Åcation, such as semantic indexing [ 2] and object detection [ 3]. Multilabel classiÔ¨Åcation has also proven useful in the predictive maintenance [ 4] and Ô¨Ånancial services [ 5] sectors, where tabular data are mainly used. When this sort of data is available, ensemble methods are typically outperforming other families of methods [ 6,7]. Ensembles, however, are intrinsically not explainable. This is an important weakness, as explainability is useful for the vast majority of ML applications, and a necessity when they impact human lives or incur economic costs [8, 9]. This paper focuses on the explainability of random forest (RF) [10] models in the context of multilabel classiÔ¨Åcation. There exists a lot of work on the explainability of RF for regression and single label classiÔ¨Åcation tasks [ 11,12,13, 14]. However, adapting these methods to multilabel tasks, where RF models Ô¨Ånd frequent use [ 15,16,17], is not straightforward. There are also techniques that have been speciÔ¨Åcally designed for multilabel tasks [ 18,19]. These are, however, independent of the explained model‚Äôs architecture, and therefore cannot exploit the speciÔ¨Åc properties of RF models to their beneÔ¨Åt.arXiv:2207.01994v1  [cs.LG]  5 Jul 2022APREPRINT  JULY 6, 2022 To address the lack of RFspeciÔ¨Åc explainability techniques for multilabel classiÔ¨Åcation in the literature, we propose an extension of LionForests (LF) [ 14] towards explaining multilabel classiÔ¨Åcation decisions. We introduce three different strategies concerning the scope of the provided explanation (single label, predicted labelset, label subsets). We compare these strategies against similar stateoftheart techniques, through a set of quantitative and qualitative experiments. The results highlight the conciseness of the explanations of the proposed approach. The rest of this paper is organized as follows. Section 2 discusses relevant research, while Section 3 introduces important concepts of the LF method. Section 4 presents the three novel strategies for explaining multilabel RFs. The experimental procedure, along with the data sets used, and the results, are mentioned in Section 5. Finally, we conclude and propose future steps for this research in Section 6. 2 Related work "
116,KNN-enhanced Deep Learning Against Noisy Labels.txt,"Supervised learning on Deep Neural Networks (DNNs) is data hungry. Optimizing
performance of DNN in the presence of noisy labels has become of paramount
importance since collecting a large dataset will usually bring in noisy labels.
Inspired by the robustness of K-Nearest Neighbors (KNN) against data noise, in
this work, we propose to apply deep KNN for label cleanup. Our approach
leverages DNNs for feature extraction and KNN for ground-truth label inference.
We iteratively train the neural network and update labels to simultaneously
proceed towards higher label recovery rate and better classification
performance. Experiment results show that under the same setting, our approach
outperforms existing label correction methods and achieves better accuracy on
multiple datasets, e.g.,76.78% on Clothing1M dataset.","Deep Neural Networks (DNNs) have achieved remarkable success in various applications including computer vision, speech recognition, and robotics. Supervised learning on DNNs is data hungry. Obtaining a large dataset with la bels at an affordable cost is usually done by crowdsourc ing [33, 36] and web query [25, 34]. Each of those would inevitably introduce a signiÔ¨Åcant amount of noisy labels. On the other hand, DNNs are prone to overÔ¨Åt noisy train ing data [36, 1], and their generalization performance is *shuyukong2020@u.northwestern.edu ‚Ä†you.li@u.northwestern.edu ‚Ä°jwang34@iit.edu ¬ßme@aminrezaei.com ¬∂haizhou@northwestern.edudowngraded as a result. To resist noisy labels in training DNNs, numerous methods have been proposed, including robust loss for mulation [22, 9, 31], curriculum learning [10] and label correction [35, 28]. In this paper, we focus on label cor rection approach which alternatively sanitizes noisy la bels and improves the model performance. Previous work leverages prediction from DNN itself to infer the ground truth labels [35, 28]. However, such prediction is likely to be poisoned by the noise in the training dataset. There fore, we are motivated to seek for a more robust label cor rection approach. In this paper, we leverage the deep KNearest Neigh bors (KNN) algorithm to facilitate learning with noises. The KNN algorithm assumes that similar things exist in close proximity. KNN is a favorable classiÔ¨Åcation ap proach when no prior knowledge on sample distribution are available, and has shown robustness against adversar ial examples [26, 20, 30]. Our approach is based on a key observation that during the learning phase, useful fea tures are learnt in the intermediate layers despite the pres ence of corrupted labels in the dataset. We propose to use those features to discover similarity among samples. Even though the Ô¨Ånal labels are different for two samples belonging to same category, their features share high sim ilarity. Overall, we present a framework that iteratively ap plies deep KNN to infer ground truth labels and retrains the neural network with the predicted labels, thus simul taneously making progress toward higher label recovery and better classiÔ¨Åcation performance. It is a generalized framework that does not require an estimation of noise transition distributions or a clean dataset for reference. We also propose two KNN label correction algorithms. 1arXiv:2012.04224v1  [cs.CV]  8 Dec 2020The Ô¨Årst one, IterKNN, uses all the labels to infer ground truth labels. The second one, SelKNN, selects a certain amount of clean examples as reference for KNN ground truth inference. The selection principle is to Ô¨Ånd sam ples with small cumulative normalized loss because those samples are more likely to have correct labels. We empir ically show that our approach achieves stateofart perfor mance. The contributions of this paper are as follows: ‚Ä¢ To our best knowledge, we are the Ô¨Årst to apply deep KNN for label correction in corrupted training dataset. The features are extracted from intermedi ate layers of neural network. To further mitigate the impact of noisy labels, we propose a loss ranking ap proach to select samples with high conÔ¨Ådence to be labelled correctly as reference for KNN prediction. ‚Ä¢ We explore the beneÔ¨Åts of iterative retraining after label correction. We show that iterative retraining can help neural network escape from overÔ¨Åtting and discover more corrupted labels, thus achieves better performance. ‚Ä¢ We conduct extensive experiments to demonstrate the robustness of deep KNN against label noise. We found out that even though the Ô¨Ånal prediction is cor rupted by the noise in training dataset, CNN can still learn robust and useful features in deep layers to fa cilitate KNN for groundtruth label inference. We also provide insights on how deep feature is better than both of the shallow features and Ô¨Ånal logits with regard to noisy label correction.. 2 Related Work "
117,A Gift from Label Smoothing: Robust Training with Adaptive Label Smoothing via Auxiliary Classifier under Label Noise.txt,"As deep neural networks can easily overfit noisy labels, robust training in
the presence of noisy labels is becoming an important challenge in modern deep
learning. While existing methods address this problem in various directions,
they still produce unpredictable sub-optimal results since they rely on the
posterior information estimated by the feature extractor corrupted by noisy
labels. Lipschitz regularization successfully alleviates this problem by
training a robust feature extractor, but it requires longer training time and
expensive computations. Motivated by this, we propose a simple yet effective
method, called ALASCA, which efficiently provides a robust feature extractor
under label noise. ALASCA integrates two key ingredients: (1) adaptive label
smoothing based on our theoretical analysis that label smoothing implicitly
induces Lipschitz regularization, and (2) auxiliary classifiers that enable
practical application of intermediate Lipschitz regularization with negligible
computations. We conduct wide-ranging experiments for ALASCA and combine our
proposed method with previous noise-robust methods on several synthetic and
real-world datasets. Experimental results show that our framework consistently
improves the robustness of feature extractors and the performance of existing
baselines with efficiency. Our code is available at
https://github.com/jongwooko/ALASCA.","While deep neural networks (DNNs) have high expressive power that leads to promising performances, the success of DNNs heavily relies on the quality of training data, in par ticular, accurately labeled training examples. Unfortunately, labeling largescale datasets is a costly and errorprone pro cess, and even highquality datasets contain incorrect la bels (Nettleton, OrriolsPuig, and Fornells 2010; Zhang et al. 2017a). Hence, mitigating the negative impact of noisy la bels is critical, and many approaches have been proposed to improve robustness against noisy data for learning with noisy labels (LNL). Robustness to label noise is typically pursued by iden tifying noisy samples to reduce their contribution to the loss (Han et al. 2018; Mirzasoleiman, Cao, and Leskovec 2020), correcting labels (Yi and Wu 2019; Li, Socher, and *The two authors contributed equally. Copyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.Hoi 2020), utilizing a robust loss function (Zhang and Sabuncu 2018; Wang et al. 2019). However, one of the biggest challenges of LNL methods involves providing a de pendable criterion for distinguishing clean data from noisy data, such that clean data is fully exploited while Ô¨Åltering noisy data. While these existing methods are partially ef fective in mitigating label noise, their criterion for identify ing noisy examples uses biased posterior information from a linear classiÔ¨Åer or the penultimate layer of the corrupted network. These unpredictable biases can lead to a reduction in the network‚Äôs ability to separate clean and noisy instances (Nguyen et al. 2020; Kim et al. 2021a). To solve this undesired bias, several regularization meth ods (Xia et al. 2021; Cao et al. 2021) have been proposed to enhance the robustness of the feature extractor. However, while existing regularizationbased learning frameworks al leviate the degradation, these methods require multiple train ing stages and considerable computational costs and are dif Ô¨Åcult to apply in practice. Cao et al. (2021) used twostage training to compute the relative datadependent regulariza tion power to conduct Lipschitz regularization (LR) on inter mediate layers. Xia et al. (2021) identiÔ¨Åed and regularized the noncritical parameters that tend to Ô¨Åt noisy labels and require longer training time. Some studies (Zhang and Yao 2020; Zheltonozhskii et al. 2022) have designed contrastive learning frameworks to generate highquality feature extrac tors using unsupervised approaches, which require consid erable computations for high performance. To mitigate these impractical issues, we provide a sim ple yet effective learning framework for a robust feature ex tractor, Adaptive LAbelSmoothing via auxiliary ClAssiÔ¨Åer (ALASCA), with theoretical guarantee and small additional computation. Our proposed method is robust to label noise itself and can further enhance the performance of existing LNL methods. Our main contributions are as follows: ‚Ä¢ We theoretically explain that label smoothing (LS) im plicitly induces LR, which is known to enable robust training with noisy labels (Finlay et al. 2018; Cao et al. 2021). Through theoretical motivations, we empirically show that adaptive LS (ALS) can regularize noisy exam ples while fully exploiting clean examples. ‚Ä¢ To practically implement adaptive LR on the intermedi ate layers, we propose ALASCA, which combines ALS with auxiliary classiÔ¨Åers. To the best of our knowledge,arXiv:2206.07277v2  [cs.LG]  29 Nov 2022this is the Ô¨Årst study to apply auxiliary classiÔ¨Åers under label noise with theoretical evidence. ‚Ä¢ We experimentally demonstrate that ALASCA is uni versal by combining various LNL methods and vali dating that ALASCA consistently boosts robustness on benchmarksimulated and realworld datasets. ‚Ä¢ We verify that ALASCA effectively enhances the robust ness of feature extractors by comparing the quality of subsets on sampleselection methods and robustness to the hyperparameter selection of LNL methods. 2 Related Works "
118,Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision.txt,"Deep learning has emerged as a versatile tool for a wide range of NLP tasks,
due to its superior capacity in representation learning. But its applicability
is limited by the reliance on annotated examples, which are difficult to
produce at scale. Indirect supervision has emerged as a promising direction to
address this bottleneck, either by introducing labeling functions to
automatically generate noisy examples from unlabeled text, or by imposing
constraints over interdependent label decisions. A plethora of methods have
been proposed, each with respective strengths and limitations. Probabilistic
logic offers a unifying language to represent indirect supervision, but
end-to-end modeling with probabilistic logic is often infeasible due to
intractable inference and learning. In this paper, we propose deep
probabilistic logic (DPL) as a general framework for indirect supervision, by
composing probabilistic logic with deep learning. DPL models label decisions as
latent variables, represents prior knowledge on their relations using weighted
first-order logical formulas, and alternates between learning a deep neural
network for the end task and refining uncertain formula weights for indirect
supervision, using variational EM. This framework subsumes prior indirect
supervision methods as special cases, and enables novel combination via
infusion of rich domain and linguistic knowledge. Experiments on biomedical
machine reading demonstrate the promise of this approach.","Deep learning has proven successful in a wide range of NLP tasks (Bahdanau et al., 2014; Bengio et al., 2003; Clark and Manning, 2016; Hermann et al., 2015; Sutskever et al., 2014). The versatility stems from its capacity of learning a compact rep resentation of complex input patterns (Goodfellow This work was conducted at Microsoft Research. Deep LearningProbabilistic LogicKnowledge Virtual Evidence Latent VariableIndirect SupervisionFigure 1 : Deep Probabilistic Logic: A general framework for combining indirect supervision strategies by composing probabilistic logic with deep learning. Learning amounts to maximizing conditional likelihood of virtual evidence given in put by summing up latent label decisions. et al., 2016). However, success of deep learning is bounded by its reliance on labeled examples, which are expensive and timeconsuming to produce. In direct supervision has emerged as a promising di rection for breaching the annotation bottleneck. A powerful paradigm is joint inference (Chang et al., 2007; Poon and Domingos, 2008; Druck et al., 2008; Ganchev et al., 2010), which leverages linguistic and domain knowledge to impose con straints over interdependent label decisions. More recently, another powerful paradigm, often loosely called weak supervision , has gained in popularity. The key idea is to introduce labeling functions to automatically generate (noisy) training examples from unlabeled text. Distant supervision is a promi nent example that used existing knowledge bases for this purpose (Craven and Kumlien, 1999; Mintz et al., 2009). Data programming went further by soliciting labeling functions from domain experts (Ratner et al., 2016; Bach et al., 2017). Indirectsupervision methods have achieved re markable successes in a number of NLP tasks, but they also exhibit serious limitations. Distant su pervision often produces incorrect labels, whereas labeling functions from data programming vary inarXiv:1808.08485v1  [cs.CL]  26 Aug 2018The deletion mutation onexon 19ofEGFR gene was present in16 patients, while theL858Epoint mutation onexon 21wasnoted in10. Allpatients were treated with gefitinib andshowed apartial response . TREAT( Gefitinib, EGFR, L858E )Figure 2 : Example of crosssentence relation extrac tion for precision cancer treatment. quality and coverage, and may contradict with each other on individual instances. Joint inference incurs greater modeling complexity and often requires specialized learning and inference procedures. Since these methods draw on diverse and often orthogonal sources of indirect supervision, com bining them may help address their limitations and amplify their strengths. Probabilistic logic offers an expressive language for such an integration, and is well suited for resolving noisy and contradictory in formation (Richardson and Domingos, 2006). Un fortunately, probabilistic logic generally incurs in tractable learning and inference, often rendering endtoend modeling infeasible. In this paper, we propose deep probabilistic logic (DPL) as a unifying framework for indirect supervision (Figure 1). SpeciÔ¨Åcally, we made four contributions. First, we introduce a modular design to compose probabilistic logic with deep learning, with a supervision module that represents indirect supervision using probabilistic logic, and a predic tion module that performs the end task using a deep neural network. Label decisions are modeled as latent variables and serve as the interface between the two modules. Second, we show that all popular forms of indi rect supervision can be represented in DPL by gen eralizing virtual evidence (Subramanya and Bilmes, 2007; Pearl, 2014). Consequently, these diverse methods can be easily combined within a single framework for mutual ampliÔ¨Åcation. Third, we show that our problem formulation yields a welldeÔ¨Åned learning objective (maximiz ing conditional likelihood of virtual evidence). We proposed a modular learning approach by decom posing the optimization over the supervision and prediction modules, using variational EM, which enables us to apply stateoftheart methods for probabilistic logic and deep learning. Finally, we applied DPL to biomedical machine reading (Quirk and Poon, 2017; Peng et al., 2017). Biomedicine offers a particularly attractive appli cation domain for exploring indirect supervision. Biomedical literature grows by over one millioneach year1, making it imperative to develop ma chine reading methods for automating knowledge curation (Figure 2). While crowd sourcing is hardly applicable, there are rich domain knowledge and structured resources to exploit for indirect supervi sion. Using crosssentence relation extraction and entity linking as case studies, we show that distant supervision, data programming, and joint inference can be seamlessly combined in DPL to substan tially improve machine reading accuracy, without requiring any manually labeled examples.2 2 Related Work "
119,Gaussian Mixture Variational Autoencoder with Contrastive Learning for Multi-Label Classification.txt,"Multi-label classification (MLC) is a prediction task where each sample can
have more than one label. We propose a novel contrastive learning boosted
multi-label prediction model based on a Gaussian mixture variational
autoencoder (C-GMVAE), which learns a multimodal prior space and employs a
contrastive loss. Many existing methods introduce extra complex neural modules
like graph neural networks to capture the label correlations, in addition to
the prediction modules. We find that by using contrastive learning in the
supervised setting, we can exploit label information effectively in a
data-driven manner, and learn meaningful feature and label embeddings which
capture the label correlations and enhance the predictive power. Our method
also adopts the idea of learning and aligning latent spaces for both features
and labels. In contrast to previous works based on a unimodal prior, C-GMVAE
imposes a Gaussian mixture structure on the latent space, to alleviate the
posterior collapse and over-regularization issues. C-GMVAE outperforms existing
methods on multiple public datasets and can often match other models' full
performance with only 50% of the training data. Furthermore, we show that the
learnt embeddings provide insights into the interpretation of label-label
interactions.","In many machine learning tasks, an instance can have sev eral labels. The task of predicting multiple labels is known as multilabel classiÔ¨Åcation (MLC). MLC is common in domains like computer vision (Wang et al., 2016), natural language processing (Chang et al., 2019) and biology (Yu et al., 2013). Unlike the singlelabel scenario, label corre 1Department of Computer Science, Cornell University, Ithaca, USA. Correspondence to: Shufeng Kong <sk2299@cornell.edu >. Proceedings of the 39thInternational Conference on Machine Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy right 2022 by the author(s).lations are more important in MLC. Early works capture the correlations through classiÔ¨Åer chains (Read et al., 2009), Bayesian inference (Zhang & Zhou, 2007), and dimension ality reduction (Bhatia et al., 2015). Thanks to the huge capacity of neural networks (NN), many previous methods can be improved by their neural exten sions. For example, classiÔ¨Åer chains can be naturally en hanced by recurrent neural networks (RNN) (Wang et al., 2016). The nonlinearity of NN alleviates the complex design of feature mapping and many deep models can there fore focus on the loss function, featurelabel and labellabel correlation modeling. One trending direction is to learn a deep latent space shared by features and labels. The encoded samples from the latent space are then decoded to targets. One typical example is C2AE (Yeh et al., 2017), which learns latent codes for both features and labels. The latent codes are passed to a decoder to derive the target labels. C2AE minimizes an `2distance between the feature and label codes, together with a relaxed orthogonality regularization. However, the learnt determin istic latent space lacks smoothness and structures. Small perturbations in this latent space can lead to totally different decoding results. Even if the corresponding feature and la bel codes are close, we cannot guarantee the decoded targets are similar. To address this concern, MPV AE (Bai et al., 2020) proposes to replace the deterministic latent space with a probabilistic space under a variational autoencoder (V AE) framework. The Gaussian latent spaces are aligned with KLdivergence, and the sampling process enforces smooth ness. Similar ideas can be found in (Sundar et al., 2020). However, these methods assume a unimodal Gaussian latent space, which is known to cause overregularization and pos terior collapse (Dilokthanakul et al., 2016; Wu & Goodman, 2018). A better strategy would be to learn a multimodal latent space. It is more reasonable to assume the observed data are generated from a multimodal subspace rather than a unimodal one. Another popular group of methods focuses on better label correlation modeling. Their idea is straightforward: some la bels should be more correlated if they coappear often while others should be less relevant. Existing methods adopt pair wise ranking loss, covariance matrices, conditional randomarXiv:2112.00976v2  [cs.LG]  10 Jun 2022Gaussian Mixture V AE with Contrastive Learning for MultiLabel ClassiÔ¨Åcation Ô¨Åelds (CRF) or graph neural nets (GNN) to this end (Zhang & Zhou, 2013; Bi & Kwok, 2014; Belanger & McCallum, 2016; Lanchantin et al., 2019; Chen et al., 2019b). These methods often either constrain the learning through a pre deÔ¨Åned structure (which requires a larger model size), or aren‚Äôt powerful enough to capture the correlations (such as pairwise ranking loss). Our idea is simple: we learn embeddings for each label class and the inner products between embeddings should reÔ¨Çect the similarity. We further learn feature embeddings whose inner products with label embeddings correspond to featurelabel similarity and can be used for prediction. We assume these embeddings are generated from a probabilistic multimodal latent space shared by features and labels, where we use KLdivergence to align the feature and label latent distributions. On the other hand, one might be concerned that embeddings alone won‚Äôt capture both labellabel and labelfeature correlations, which were usually modeled by extra GNN and covariance matrices in prior works (Lan chantin et al., 2019; Bai et al., 2020). To this end, we stress on the loss function terms rather than extra structure to cap ture these correlations. Intuitively, if two labels coappear often, their embeddings should be close. Otherwise, if two labels seldom coappear, their embeddings should be distant. A tripletlike loss could be naturally applied in this scenario. Nonetheless, its extension, contrastive loss, has shown to be even more effective than the triplet loss by introducing more samples rather than just one triplet. We show that con trastive loss can pull together correlated label embeddings, push away unrelated label embeddings (see Fig. 3), and even perform better than GNNbased or covariancebased methods. Our new model for MLC, contrastive learning boosted Gaus sian mixture variational autoencoder (CGMV AE), allevi ates the overregularization and posterior collapse concerns, and also learns useful feature and label embeddings. C GMV AE is applied to nine datasets and outperforms the existing methods on Ô¨Åve metrics. Moreover, we show that using only 50% of the data, our results can match the full performance of other stateoftheart methods. Ablation studies and interpretability of learnt embeddings will also be illustrated in the experiments. Our contributions can be summarized in three aspects: (i)We propose to use con trastive loss instead of triplet or ranking loss to strengthen the label embedding learning. We empirically show that by using a contrastive loss, one can get rid of heavyduty label correlation modules (e.g., covariance matrices, GNNs) while achieving even better performances. (ii)Though con trastive learning is commonly applied in selfsupervised learning, our work shows that by properly deÔ¨Åning anchor, positive and negative samples, contrastive loss can leverage label information very effectively in the supervised MLC scenario as well. (iii)Unlike prior probabilistic models,CGMV AE learns a multimodal latent space and integrates the probabilistic modeling (V AE module) with embedding learning (contrastive module) synergistically. 2. Methods "
120,Semantic scene synthesis: Application to assistive systems.txt,"The aim of this work is to provide a semantic scene synthesis from a single
depth image. This is used in assistive aid systems for visually impaired and
blind people that allow them to understand their surroundings by the touch
sense. The fact that blind people use touch to recognize objects and rely on
listening to replace sight, motivated us to propose this work. First, the
acquired depth image is segmented and each segment is classified in the context
of assistive systems using a deep learning network. Second, inspired by the
Braille system and the Japanese writing system Kanji, the obtained classes are
coded with semantic labels. The scene is then synthesized using these labels
and the extracted geometric features. Our system is able to predict more than
17 classes only by understanding the provided illustrative labels. For the
remaining objects, their geometric features are transmitted. The labels and the
geometric features are mapped on a synthesis area to be sensed by the touch
sense. Experiments are conducted on noisy and incomplete data including
acquired depth images of indoor scenes and public datasets. The obtained
results are reported and discussed.","In order to accomplish daily tasks, people involve their Ô¨Åve senses, namely sight, hearing, taste, smell and touch. Being deprived of one of these senses will complicate the pro cess of a given task; it will reduce human autonomy, independence and even privacy; the visually impaired Ô¨Ånd di fÔ¨Åculties in their daily life. With the limitations of the classical aid systems such as whi te canes, guide dogs and personal assistants; and with the evolution of technology, many commercial and noncommer cial aid systems were proposed in the last decades. Generally, these latter rely on image processing, artiÔ¨Åcia l intelligence techniques and external sensors in order to o ffer help for the visually impaired and blind people to improve th eir independence in many applications. To transmit instructions, scene description or any other ge nerated output, most of the assistive systems use audiobas ed or vibrationbased output devices. It turns out that these l atter hold hearing and are not too informative. Hence, the necessity of providing a semantic labeling for scene unders tanding that can be exploited by the touch sense. In this work, we propose a framework for semantic scene synth esis. From the depth image, the 3D scene is down scaled and semantically mapped into a synthesis area using t he computed labels and the extracted geometric features of the input point cloud. Two main modules are proposed: the c lassiÔ¨Åcation module and the semantic labeling module. The Ô¨Årst module is based on a deep learning architecture to cl assify depth image segments into seven semantic classes.APREPRINT  APRIL 22, 2021 The semantic labeling is inspired from Braille and Kanji sys tems. This latter is mapped into a touchbased synthesis area that can be used for many applications such as in assisti ve systems for visually impaired and blind people. The remaining sections are structured as follows: in sectio n 2, we present related works to objects classiÔ¨Åcation, scen es understanding and semantic labeling for assistive systems . An overview of the proposed system is presented in section 3. In sections 4 and 5, we describe our approaches for the clas siÔ¨Åcation and the semantic labeling module respectively. Conducted experiments are reported and discussed in sectio n 6. Finally, we conclude with the future works (section 7) and a conclusion (section 8). 2 Related Works "
121,ReSup: Reliable Label Noise Suppression for Facial Expression Recognition.txt,"Because of the ambiguous and subjective property of the facial expression
recognition (FER) task, the label noise is widely existing in the FER dataset.
For this problem, in the training phase, current FER methods often directly
predict whether the label of the input image is noised or not, aiming to reduce
the contribution of the noised data in training. However, we argue that this
kind of method suffers from the low reliability of such noise data decision
operation. It makes that some mistakenly abounded clean data are not utilized
sufficiently and some mistakenly kept noised data disturbing the model learning
process. In this paper, we propose a more reliable noise-label suppression
method called ReSup (Reliable label noise Suppression for FER). First, instead
of directly predicting noised or not, ReSup makes the noise data decision by
modeling the distribution of noise and clean labels simultaneously according to
the disagreement between the prediction and the target. Specifically, to
achieve optimal distribution modeling, ReSup models the similarity distribution
of all samples. To further enhance the reliability of our noise decision
results, ReSup uses two networks to jointly achieve noise suppression.
Specifically, ReSup utilize the property that two networks are less likely to
make the same mistakes, making two networks swap decisions and tending to trust
decisions with high agreement. Extensive experiments on three popular
benchmarks show that the proposed method significantly outperforms
state-of-the-art noisy label FER methods by 3.01% on FERPlus becnmarks. Code:
https://github.com/purpleleaves007/FERDenoise","Facial expression recognition (FER) has become a cru cial service in various realworld applications, such as healthcare [22], surveillance [1], and virtual reality [24]. It aims to recognize specific human emotions from the given facial images. However, for the largescale FER dataset collected from the Internet, it is difficult to achieve high quality annotations due to the subjectivity of annotators and Figure 1. Comparison of label noise suppression process of differ ent noisy label FER methods. The top is the current schemes and the bottom is ReSuP. ReSuP generates more reliable weights by noise modeling and suppresses unreliable weights by unreliability suppression design in addition to suppressing noisy labels. Trian gles represent unreliable weights. the ambiguity of facial expressions, and these lowquality annotations form label noises. Therefore, how to suppress label noises in FER tasks has become a research hotspot and attracted more and more attention[35, 30, 16, 48, 45, 42]. For this challenge, existing FER methods typically in corporate an importance learning branch to estimate the im portance weight of each image, which determines whether the label of the input image is noisy or not [35, 30, 48]. However, we argue that these methods suffer from the low reliability of such noise decision operation. Such opera tion usually generates unreliable weights and makes that some mistakenly abounded clean data are not utilized suf ficiently and some mistakenly kept noised data disturbing the model learning process. These unreliable weights orig inate from two perspectives: the noise decision process and the FER model itself. The former unreliable weights are due to overfitting of the importance learning branch as a result of the strong learning ability of deep neural net works (DNNs) [48]. Furthermore, such noise decision pro cess only considers information from a single sample [35] a batch [30], neglecting global information and resulting in unreliable decision making. In addition to the unreli able weights caused by the noise decision process, the FER model itself unavoidably produces some unreliable outputsarXiv:2305.17895v1  [cs.CV]  29 May 2023(the inputs of the importance learning branch), leading to further unreliable weights. These unreliable weights accu mulate during the entire learning process and affect current and future learning stages. Unfortunately, existing methods do not address how to mitigate the effects of these unreli able weights. Novel methods are required to address these limitations and improve the reliability and accuracy of noisy label FER. In this paper, we present a novel approach called Re Sup. The main objective of ReSup is to suppress noisy labels and unreliable weights, as illustrated in Figure 1. Specifically, instead of directly predicting noised or not, ReSup makes the noise decision by modeling he joint dis tribution of noise and clean labels. This is motivated by the memorization effect of deep neural networks (DNNs), where the model tends to memorize correctly labeled sam ples first [47, 3, 14], leading to noisy samples having higher loss during early epochs of training [2, 46]. But, to achieve optimal distribution modeling, ReSup propose to model the similarity (cosine similarity of predictions and targets) distribution of all samples rather than the loss, which re duces the confusion between noisy and clean distributions. The fitted noise model is then used to provide importance weights for each sample based on its similarity, without us ing neural network branches to avoid overfitting. In addi tion, the proposed scheme can take into account the global distribution of all samples. Furthermore, ReSup mitigate the effect of the unreliable weights by leveraging the agree ment maximization principles [6, 32], which suggest that two different networks would agree on most samples except for noisy samples [38] and thus can filter different types of errors. Inspired by the agreement maximization principles, ReSup employs two different networks to provide impor tance weights to each other, to prevent the accumulation of errors caused by unreliable weights. We also introduce a consistency loss that assigns large losses to samples with small agreement to prevent the model from fitting samples with unreliable weights, since the samples with small agree ments usually are noisy samples. In summary, our contribu tions include: ‚Ä¢ To avoid extra unreliable weights caused by the DNNbased importance learning branch, a novel label noise modeling method based on similarity distribu tion statistics is proposed to estimate the importance weights. ‚Ä¢ We propose ReSup to suppress label noise in FER. Re Sup satisfactorily mitigates the effect of the unreliable weights by leveraging a weight exchange strategy and a consistency loss. ‚Ä¢ Extensive experiment results demonstrate that the pro posed ReSup significantly outperforms stateoftheartnoisy label FER solutions on multiple FER bench marks with different levels of label noise. 2. Related Work "
122,Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee.txt,"Over-parameterized deep neural networks trained by simple first-order methods
are known to be able to fit any labeling of data. Such over-fitting ability
hinders generalization when mislabeled training examples are present. On the
other hand, simple regularization methods like early-stopping can often achieve
highly nontrivial performance on clean test data in these scenarios, a
phenomenon not theoretically understood. This paper proposes and analyzes two
simple and intuitive regularization methods: (i) regularization by the distance
between the network parameters to initialization, and (ii) adding a trainable
auxiliary variable to the network output for each training example.
Theoretically, we prove that gradient descent training with either of these two
methods leads to a generalization guarantee on the clean data distribution
despite being trained using noisy labels. Our generalization analysis relies on
the connection between wide neural network and neural tangent kernel (NTK). The
generalization bound is independent of the network size, and is comparable to
the bound one can get when there is no label noise. Experimental results verify
the effectiveness of these methods on noisily labeled datasets.","Modern deep neural networks are trained in a highly overparameterized regime, with many more trainable parameters than training examples. It is wellknown that these networks trained with simple Ô¨Årstorder methods can Ô¨Åt any labels, even completely random ones (Zhang et al., 2017). Although training on properly labeled data usually leads to good generalization performance, the ability to overÔ¨Åt the entire training dataset is undesirable for generalization when noisy labels are present. Therefore preventing overÔ¨Åtting is crucial for robust performance since mislabeled data are ubiquitous in very large datasets (Krishna et al., 2016). In order to prevent overÔ¨Åtting to mislabeled data, some form of regularization is necessary. A simple such example is early stopping , which has been observed to be surprisingly effective for this purpose (Rolnick et al., 2017; Guan et al., 2018; Li et al., 2019). For instance, training ResNet34 with early stopping can achieve 84% test accuracy on CIFAR10 even when 60% of the training labels are corrupted (Table 1). This is nontrivial since the test error is much smaller than the error rate in training data. How to explain such generalization phenomenon is an intriguing theoretical question. As a step towards a theoretical understanding of the generalization phenomenon for over parameterized neural networks when noisy labels are present, this paper proposes and analyzes two simple regularization methods as alternatives of early stopping: 1.Regularization by distance to initialization . Denote bythe network parameters and by p0qits random initialization. This method adds a regularizer }p0q}2to the training objective. 2.Adding an auxiliary variable for each training example. Let xibe theith training example andfp;qrepresent the neural net. This method adds a trainable variable biand tries to Ô¨Åt theith label using fp;xiq bi. At test time, only the neural net fp;qis used and the auxiliary variables are discarded. 1arXiv:1905.11368v4  [cs.LG]  2 Oct 2020Published as a conference paper at ICLR 2020 These two choices of regularization are well motivated with clear intuitions. First, distance to initialization has been observed to be very related to generalization in deep learning (Neyshabur et al., 2019; Nagarajan and Kolter, 2019), so regularizing by distance to initialization can potentially help generalization. Second, the effectiveness of early stopping indicates that clean labels are somewhat easier to Ô¨Åt than wrong labels; therefore, adding an auxiliary variable could help ‚Äúabsorb‚Äù the noise in the labels, thus making the neural net itself not overÔ¨Åtting. We provide theoretical analysis of the above two regularization methods for a class of sufÔ¨Åciently wide neural networks by proving a generalization bound for the trained network on clean data distribution when the training dataset contains noisy labels. Our generalization bound depends on the (unobserved) clean labels, and is comparable to the bound one can get when there is no label noise, therefore indicating that the proposed regularization methods are robust to noisy labels. Our theoretical analysis is based on the recently established connection between wide neural net and neural tangent kernel (Jacot et al., 2018; Lee et al., 2019; Arora et al., 2019a). In this line of work, parameters in a wide neural net are shown to stay close to their initialization during gradient descent training, and as a consequence, the neural net can be effectively approximated by its Ô¨Årstorder Taylor expansion with respect to its parameters at initialization. This leads to tractable linear dynamics under`2loss, and the Ô¨Ånal solution can be characterized by kernel regression using a particular kernel named neural tangent kernel (NTK). In fact, we show that for wide neural nets, both of our regularization methods, when trained with gradient descent to convergence, correspond to kernel ridge regression using the NTK, which is often regarded as an alternative to early stopping in kernel literature. This viewpoint makes explicit the connection between our methods and early stopping. The effectiveness of these two regularization methods is veriÔ¨Åed empirically ‚Äì on MNIST and CIFAR10, they are able to achieve highly nontrivial test accuracy, on a par with or even better than early stopping. Furthermore, with our regularization, the validation accuracy is almost monotone increasing throughout the entire training process, indicating their resistance to overÔ¨Åtting. 2 R ELATED WORK "
123,Improving Label Quality by Jointly Modeling Items and Annotators.txt,"We propose a fully Bayesian framework for learning ground truth labels from
noisy annotators.
  Our framework ensures scalability by factoring a generative, Bayesian soft
clustering model over label distributions into the classic David and Skene
joint annotator-data model. Earlier research along these lines has neither
fully incorporated label distributions nor explored clustering by annotators
only or data only. Our framework incorporates all of these properties as:
  (1) a graphical model designed to provide better ground truth estimates of
annotator responses as input to \emph{any} black box supervised learning
algorithm, and
  (2) a standalone neural model whose internal structure captures many of the
properties of the graphical model.
  We conduct supervised learning experiments using both models and compare them
to the performance of one baseline and a state-of-the-art model.","The recent interest in few and zeroshot learning and the reemergence of weakly supervised learning speaks to the reality that ground truth labels are a limited resource and that, in many common situations, obtaining them remains a major challenge. Multiple sources estimate the the global costs of human annotators (only one of many sources of labels) to be approaching $1‚Äì3 billion by 2026 and growing [ Met19 ,Res20 ]. Among the costdriving challenges is the noise associated with many of the most common processes for obtaining labels. In this paper, we explore novel graphical and neural models that tie together two rather successful approaches, itemannotators tableaus [ DS79 ], and label distribution learning (LDL) [ Gen16 ], based on converging studies in later research [ VGK+14,LVBH19 ] on the use of clustering to boost the signal of noisy data. We adopt a theoretical framework motivated by the anthropologist Malinowski [ Mal67 ] and Ô¨Årst used by Aroyo and Welty [ AW14 ] in the context of machine learning to characterize meaning as a function of three components: 1) an act (represented by the learning task), 2) the symbols (the labels), and, 3) the referent (the annotators). Human labeling is a special challenge not only due to its great expense but also due to the fact that humans often disagree over the labels that they provide. In fact, it is precisely the problems where disagreement is most common that human input is hardest to replace through automation or sensing. This paper speciÔ¨Åcally addresses the following research questions: RQ1: Do predictive graphical models for LDL that cluster on both item AND annotator distributions outperform those that do not? RQ2: Do predictive neural models for LDL outperform graphical models?arXiv:2106.10600v1  [cs.AI]  20 Jun 2021Preprint, Work in Progress RQ3: Do predictive neural models for LDL that cluster on both item AND annotator distributions outperform those that do not? To help us answer these questions, we contribute two new models. The Ô¨Årst is a generative graphical model that boosts conventional label distribution learning by clustering label distributions jointly in item and annotator label distribution spaces. Previous approaches have studied clustering in one space or the other. This is, to our knowledge, the Ô¨Årst time that clustering has been applied simultaneously to both. Our second model is a neuralbased adaptation of the graphical model. While the graphical model has a sound theoretical foundation, it is somewhat unwieldy from a computational perspective. The neural model sacriÔ¨Åces some rigor for more Ô¨Çexibility and algorithmic efÔ¨Åciency. 2 Related Work "
124,SOMA: Solving Optical Marker-Based MoCap Automatically.txt,"Marker-based optical motion capture (mocap) is the ""gold standard"" method for
acquiring accurate 3D human motion in computer vision, medicine, and graphics.
The raw output of these systems are noisy and incomplete 3D points or short
tracklets of points. To be useful, one must associate these points with
corresponding markers on the captured subject; i.e. ""labelling"". Given these
labels, one can then ""solve"" for the 3D skeleton or body surface mesh.
Commercial auto-labeling tools require a specific calibration procedure at
capture time, which is not possible for archival data. Here we train a novel
neural network called SOMA, which takes raw mocap point clouds with varying
numbers of points, labels them at scale without any calibration data,
independent of the capture technology, and requiring only minimal human
intervention. Our key insight is that, while labeling point clouds is highly
ambiguous, the 3D body provides strong constraints on the solution that can be
exploited by a learning-based method. To enable learning, we generate massive
training sets of simulated noisy and ground truth mocap markers animated by 3D
bodies from AMASS. SOMA exploits an architecture with stacked self-attention
elements to learn the spatial structure of the 3D body and an optimal transport
layer to constrain the assignment (labeling) problem while rejecting outliers.
We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is
more accurate and robust than existing state of the art research methods and
can be applied where commercial systems cannot. We automatically label over 8
hours of archival mocap data across 4 different datasets captured using various
technologies and output SMPL-X body models. The model and data is released for
research purposes at https://soma.is.tue.mpg.de/.","Markerbased optical motion capture (mocap) systems record 2D infrared images of light reÔ¨Çected or emitted by a set of markers placed at key locations on the surface of a subject‚Äôs body. Subsequently, the mocap systems recover the precise position of the markers as a sequence of sparse and unordered points or short tracklets. Powered by years of commercial development, these systems offer high tem poral and spatial accuracy. Richly varied mocap data from such systems is widely used to train machine learning meth ods in action recognition, motion synthesis, human motion modeling, pose estimation, etc. Despite this, the largest ex isting mocap dataset, AMASS [31], has about 45 hours of mocap, much smaller than video datasets used in the Ô¨Åeld. Mocap data is limited since capturing and processing it is expensive. Despite its value, there are large amounts of archival mocap in the world that have never been labeled;arXiv:2110.04431v1  [cs.CV]  9 Oct 2021this is the ‚Äúdark matter‚Äù of mocap. The problem is that, to solve for the 3D body, the raw mocap point cloud (MPC) must be ‚Äúlabeled‚Äù; that is, the points must be assigned to physical ‚Äúmarker‚Äù locations on the subject‚Äôs body. This is challenging because the MPC is noisy and sparse and the labeling problem is ambiguous. Existing commercial tools, e.g. [22, 30], offer partial automation, however none provide a complete solution to automatically handle vari ations in marker layout, i.e. number of markers used and their rough placement on the body, variation in subject body shape and gender, and variation across capture technolo gies namely active vs passive markers or brands of system. These challenges typically preclude costeffective labeling of archival data, and add to the cost of new captures by re quiring manual cleanup. Automating the mocap labeling problem has been ex amined by the research community [15, 17, 20]. Existing methods focus on Ô¨Åxing the mistakes in already labeled markers through denoising [9, 20]. Recent work formulates the problem in a matching framework, directly predicting the label assignment matrix for a Ô¨Åxed number of markers in a restricted setup [15]. In short, the existing methods are limited to a constrained range of motions [15], a single body shape [9, 17, 20], a certain capture scenario, a spe cial marker layout, or require a subjectspeciÔ¨Åc calibration sequence [15, 22, 30]. Other methods require highquality real mocap marker data for training, effectively prohibiting their scalability to novel scenarios [9, 15]. To address these shortcomings we take a datadriven ap proach and train a neural network endtoend with self attention components and an optimal transport layer to pre dict a perframe constrained inexact matching between mo cap points and labels. Having enough ‚Äúreal‚Äù data for train ing is not feasible, therefore we opt for synthetic data. Given a marker layout, we generate synthetic mocap point clouds with realistic noise, and then train a layoutspeciÔ¨Åc network that can cope with realistic variations across a whole mocap dataset. While previous works have exploited synthetic data [17, 20], they are limited in terms of body shapes, motions, marker layouts, and noise sources. Even with a large synthetic corpus of MPC, labeling a cloud of sparse 3D points, containing outliers and missing data, is a highly ambiguous task. The key to the solution lies in the fact that the points are structured, as is their variation with articulated pose. SpeciÔ¨Åcally, they are constrained by the shape and motion of the human body. Given sufÔ¨Åcient training data, our attentional framework learns to exploit lo cal context at different scales. Furthermore, if there were no noise, the mapping between labels and points would be onetoone. We formulate these concepts as a uniÔ¨Åed train ing objective enabling endtoend model training. Specif ically, our formulation exploits a transformer architecture to capture local and global contextual information usingselfattention (Sec. 4.1). By generating synthetic mocap data with varying body shapes and poses, SOMA implic itly learns the kinematic constraints of the underlying de formable human body (Sec. 4.4). A onetoone match be tween 3D points and markers, subject to missing and spuri ous data, is achieved by a special normalization technique (Sec. 4.2). To provide a common output framework, consis tent with [31], we use MoSh [29, 31] as a postprocessing step to Ô¨Åt SMPLX [38] to the labeled points; this also helps deal with missing data caused by occlusion or dropped markers. The SOMA system is outlined in Fig. 3. To generate training data, SOMA requires a rough marker layout that can be obtained by a single labeled frame, which requires minimal effort. Afterward, virtual markers are automatically placed on a SMPLX body and animated by motions from AMASS [31]. In addition to common mocap noise models like occlusions [15, 17, 20], and ghost points [17, 20], we introduce novel terms to vary maker placement on the body surface and we copy noise from real marker data in AMASS (Sec. 4.4). We train SOMA once for each mocap dataset and apart from the one layout frame, we do not require any labeled real data. After training, given a noisy MPC frame as input, SOMA predicts a distribution over labels of each point, including a null la bel for ghost points. We evaluate SOMA on several challenging datasets and Ô¨Ånd that we outperform the current state of the art numer ically while being much more general. Additionally, we capture new MPC data using a Vicon mocap system and compare handlabeled groundtruth to Sh ¬Øogun and SOMA output. SOMA performs similarly compared with the com mercial system. Finally, we apply the method on archival mocap datasets: Mixamo [11], DanceDB [5], and a previ ously unreleased portion of the CMU mocap dataset [12]. In summary, our main contributions are: (1) a novel neu ral network architecture exploiting selfattention to process sparse deformable point cloud data; (2) a system that con sumes mocap point clouds directly and outputs a distribu tion over marker labels; (3) a novel synthetic mocap gener ation pipeline that generalizes to real mocap datasets; (4) a robust solution that works with archival data, different mo cap technologies, poor data quality, and varying subjects and motions; (5) 220 minutes of processed mocap data in SMPLX format, trained models, and code are released for research purposes. 2. Related Work "
125,Identification of Novel Classes for Improving Few-Shot Object Detection.txt,"Conventional training of deep neural networks requires a large number of the
annotated image which is a laborious and time-consuming task, particularly for
rare objects. Few-shot object detection (FSOD) methods offer a remedy by
realizing robust object detection using only a few training samples per class.
An unexplored challenge for FSOD is that instances from unlabeled novel classes
that do not belong to the fixed set of training classes appear in the
background. These objects behave similarly to label noise, leading to FSOD
performance degradation. We develop a semi-supervised algorithm to detect and
then utilize these unlabeled novel objects as positive samples during training
to improve FSOD performance. Specifically, we propose a hierarchical ternary
classification region proposal network (HTRPN) to localize the potential
unlabeled novel objects and assign them new objectness labels. Our improved
hierarchical sampling strategy for the region proposal network (RPN) also
boosts the perception ability of the object detection model for large objects.
Our experimental results indicate that our method is effective and outperforms
the existing state-of-the-art (SOTA) FSOD methods.","The adoption of deep neural network architectures in ob ject detection has led to a signiÔ¨Åcant method in determining the location and the category of objects of interest in an im age. In the presence of abundant training data, object de tection models based on the regionbased convolution neu ral networks (RCNN) architecture reach high accuracy on most object detection tasks. However, preparing largescale annotated training data can be a challenging task in some applications, e.g., miscellaneous disease analysis and indus trial defect detection. In the presence of insufÔ¨Åcient training data, these models easily overÔ¨Åt and fail to generalize well. In contrast, humans are able a novel object class very fast based on a few samples. As a result, it is extremely desirable to develop models that can learn object classes using only a Figure 1: FSOD methods pretrain a model on abundant base classes and then Ô¨Ånetune it on both base and novel classes. few samples, known as fewshot object detection (FSOD). Current FSOD methods are based on pretraining a suit able model on a set of base classes with abundant training data and then Ô¨Ånetuning the model on both the base classes and the novel classes for which only a few samples are ac cessible (see Figure 1). The primary approach in FSOD is to beneÔ¨Åt from ideas in transfer learning or metalearning to learn novel classes through the knowledge obtained dur ing the pretraining stage while maintaining good perfor mance in base classes. Despite recent advances in FSOD, current SOTA methods are still far from getting favorable results on novel classes similar to the base classes. Poten tial reasons for this performance gap include the confusion between visually similar categories, incorrect annotations (label noise), the existence of unseen novel objects during training, etc. Recent FSOD methods have focused on ad dressing these challenges for improved FSOD performance. We study the phenomenon that unlabeled novel object classes that do not belong to either of the base or the la beled novel classes can appear in the training data. For ex ample, we see in Figure. 1 that among baseclass training samples, there are a number of objects that remain unla beled, such as the cow in the image. These unlabeled ob jects can potentially belong to unseen novel classes. Our ex periments demonstrate that this phenomenon exists in PAS CAL VOC [4] and COCO [21] datasets.This phenomenon leads to the objectness inconsistency for the model whenarXiv:2303.10422v1  [cs.CV]  18 Mar 2023recognizing the novel objects: for the novel class, objects are treated as background if their annotations are missing, but they are treated as foreground where they are labeled. Such nonconformity of foreground and background con fuses the model when training the objectness and make the model hard to converge and degrades detection accuracy. To tackle the above challenge, we develop a semi supervised learning method to utilize the potential novel ob jects that appear during training to improve the ability of the model to recognize novel classes. We Ô¨Årst demonstrate the possibility of detecting these unlabeled objects. Our exper iment indicates that some unlabeled class objects are likely to be recognized if they are similar to the training base and novel classes. We collect the unlabeled novel objects from the background proposals by determining whether they are predicted as known classes, and then we give these propos als an extra objectness label in the region proposal network (RPN) so that the model could learn them. We also ana lyze the defect of the standard RPN in detecting objects of different sizes during training and propose a more balanced RPN sampling method so that objects are treated equally in all scales. We provide extensive experimental results to demonstrate the effectiveness of our method on the PAS CAL VOC and COCO datasets. Our contributions include: ‚Ä¢ We modify the anchor sampling strategy so that the anchors are evenly chosen from different layers of the feature pyramid layer in the RCNN architecture. ‚Ä¢ We design a ternary objectness classiÔ¨Åcation in the RPN layer which enables the model to recognize po tential novel class objects to improve consistency. ‚Ä¢ We use contrastive learning in the RPN layer to distin guish between the positive and the negative anchors. 2. Related works "
126,Noisy Machines: Understanding Noisy Neural Networks and Enhancing Robustness to Analog Hardware Errors Using Distillation.txt,"The success of deep learning has brought forth a wave of interest in computer
hardware design to better meet the high demands of neural network inference. In
particular, analog computing hardware has been heavily motivated specifically
for accelerating neural networks, based on either electronic, optical or
photonic devices, which may well achieve lower power consumption than
conventional digital electronics. However, these proposed analog accelerators
suffer from the intrinsic noise generated by their physical components, which
makes it challenging to achieve high accuracy on deep neural networks. Hence,
for successful deployment on analog accelerators, it is essential to be able to
train deep neural networks to be robust to random continuous noise in the
network weights, which is a somewhat new challenge in machine learning. In this
paper, we advance the understanding of noisy neural networks. We outline how a
noisy neural network has reduced learning capacity as a result of loss of
mutual information between its input and output. To combat this, we propose
using knowledge distillation combined with noise injection during training to
achieve more noise robust networks, which is demonstrated experimentally across
different networks and datasets, including ImageNet. Our method achieves models
with as much as two times greater noise tolerance compared with the previous
best attempts, which is a significant step towards making analog hardware
practical for deep learning.","Deep neural networks (DNNs) have achieved unprecedented performance over a wide variety of tasks such as computer vision, speech recognition, and natural language processing. However, DNN inference is typically very demanding in terms of compute and memory resources Li et al. (2019). Consequently, larger models are often not well suited for largescale deployment on edge devices, which typically have meagre performance and power budgets, especially battery powered mobile and IoT devices. To address these issues, the design of specialized hardware for DNN inference has drawn great interest, and is an extremely active area of research (Whatmough et al., 2019). To date, a plethora of techniques have been proposed for designing efÔ¨Åcient neural network hardware (Sze et al., 2017; Whatmough et al., 2019). In contrast to the current status quo of predominantly digital hardware, there is signiÔ¨Åcant research interest in analog hardware for DNN inference. In this approach, digital values are represented by analog quantities such as electrical voltages or light pulses, and the computation itself (e.g., Work performed during Prad Kadambi‚Äôs internship with Arm Research 1arXiv:2001.04974v1  [cs.LG]  14 Jan 2020Preprint multiplication and addition) proceeds in the analog domain, before eventually being converted back to digital. Analog accelerators take advantage of particular efÔ¨Åciencies of analog computation in exchange for losing the bitexact precision of digital. In other words, analog compute is cheap but somewhat imprecise. Analog computation has been demonstrated in the context of DNN inference in both electronic (Binas et al., 2016), photonic (Shen et al., 2017) and optical (Lin et al., 2018) systems. Analog accelerators promise to deliver at least two orders of magnitude better performance over a conventional digital processor for deep learning workloads in both speed (Shen et al., 2017) and energy efÔ¨Åciency (Ni et al., 2017). Electronic analog DNN accelerators are arguably the most mature technology and hence will be our focus in this work. The most common approach to electronic analog DNN accelerator is inmemory computing , which typically uses nonvolatile memory (NVM) crossbar arrays to encode the network weights as analog values. The NVM itself can be implemented with memristive devices, such as metaloxide resis tive randomaccess memory (ReRAM) (Hu et al., 2018) or phasechange memory (PCM) (Le Gallo et al., 2018; Boybat et al., 2018; Ambrogio et al., 2018). The matrixvector operations computed during inference are then performed in parallel inside the crossbar array, operating on analog quan tities for weights and activations. For example, addition of two quantities encoded as electrical currents can be achieved by simply connecting the two wires together, whereby the currents will add linearly according to Kirchhoff‚Äôs current law. In this case, there is almost zero latency or energy dissipation for this operation. Similarly, multiplication with a weight can be achieved by programming the NVM cell conduc tance to the weight value, which is then used to convert an input activation encoded as a voltage into a scaled current, following Ohm‚Äôs law. Therefore, the analog approach promises signiÔ¨Åcantly improved throughput and energy efÔ¨Åciency. However, the analog nature of the weights makes the compute noisy, which can limit inference accuracy. For example, a simple twolayer fullyconnected network with a baseline accuracy of 91:7%on digital hardware, achieves only 76:7%when imple mented on an analog photonic array (Shen et al., 2017). This kind of accuracy degradation is not acceptable for most deep learning applications. Therefore, the challenge of imprecise analog hard ware motivates us to study and understand noisy neural networks , in order to maintain inference accuracy under noisy analog computation. The question of how to effectively learn and compute with a noisy machine is a longstanding prob lem of interest in machine learning and computer science (Stevenson et al., 1990; V on Neumann, 1956). In this paper, we study noisy neural networks to understand their inference performance. We also demonstrate how to train a neural network with distillation and noise injection to make it more resilient to computation noise, enabling higher inference accuracy for models deployed on analog hardware. We present empirical results that demonstrate stateoftheart noise tolerance on multiple datasets, including ImageNet. The remainder of the paper is organized as follows. Section 2 gives an overview of related work. Section 3 outlines the problem statement. Section 4 presents a more formal analysis of noisy neural networks. Section 5 gives a distillation methodology for training noisy neural networks, with exper imental results. Finally, Section 6 provides a brief discussion and Section 7 closes with concluding remarks. 2 R ELATED WORK "
127,Modeling Surface Appearance from a Single Photograph using Self-augmented Convolutional Neural Networks.txt,"We present a convolutional neural network (CNN) based solution for modeling
physically plausible spatially varying surface reflectance functions (SVBRDF)
from a single photograph of a planar material sample under unknown natural
illumination. Gathering a sufficiently large set of labeled training pairs
consisting of photographs of SVBRDF samples and corresponding reflectance
parameters, is a difficult and arduous process. To reduce the amount of
required labeled training data, we propose to leverage the appearance
information embedded in unlabeled images of spatially varying materials to
self-augment the training process. Starting from an initial approximative
network obtained from a small set of labeled training pairs, we estimate
provisional model parameters for each unlabeled training exemplar. Given this
provisional reflectance estimate, we then synthesize a novel temporary labeled
training pair by rendering the exact corresponding image under a new lighting
condition. After refining the network using these additional training samples,
we re-estimate the provisional model parameters for the unlabeled data and
repeat the self-augmentation process until convergence. We demonstrate the
efficacy of the proposed network structure on spatially varying wood, metals,
and plastics, as well as thoroughly validate the effectiveness of the
self-augmentation training process.","Recovering the spatially varying bidirectional surface re/f_lectance distribution function (SVBRDF) from a single photograph under un known natural lighting is a challenging and illposed problem. Often a physically accurate estimate is not necessary, and for many appli cations, such as large scale content creation for virtual worlds and computer games, a physically plausible estimate would already be valuable. Currently, common practice, albeit very timeconsuming, is to rely on skilled artists to, given a single reference image, pro duce a plausible re/f_lectance decomposition. This manual process suggests that given suÔ¨Écient prior knowledge, it is possible to infer a plausible re/f_lectance estimate for a spatially varying material from a single photograph. Datadriven machine learning techniques have been successfully applied to a wide range of underconstrained computer graphics and computer vision problems. In this paper, we follow a similar route and design a Convolutional Neural Network (CNN) to estimate physically plausible SVBRDFs from a single near/f_ield observation of a planar sample of a spatially varying material under unknown natural illumination. However, recovering the SVBRDF from a single photograph is an inherently illconditioned problem, since it is unlikely that each pixel observes a signi/f_icant specular response, making it impossible to derive a full spatially varying specular component without enforcing spatial priors. We therefore estimate areduced SVBRDF de/f_ined by a spatially varying diÔ¨Äuse albedo, homogeneous specular albedo and roughness, and spatially varying surface normals. Training a CNN to estimate such a reduced SVBRDF from a sin gle photograph under unknown natural lighting requires a large set of ‚Äúlabeled‚Äù photographs, i.e., with corresponding re/f_lectance parameters. Gathering such a training dataset is often a tedious and arduous task. Currently, except for specialized materials, very few databases exist that densely cover all possible spatial variations of a material class. Unlabeled data (i.e., a photograph of a spatially ACM Transactions on Graphics, Vol. 36, No. 4, Article 45. Publication date: July 2017.arXiv:1809.00886v1  [cs.GR]  4 Sep 201845:2 ‚Ä¢X. Li et al. varying material) is typically much easier to obtain. Each unlabeled photograph contains an instance of the complex spatially varying re /f_lectance parameters, albeit it observed from a single view and under unknown lighting. This raises the question whether we can exploit this embedded knowledge of the spatially varying distributions to re/f_ine the desired SVBRDFestimation CNN. We propose, in addition to a CNNbased solution for SVBRDF esti mation from a single photograph, a novel training strategy to lever age a large collection of unlabeled data ‚Äìphotographs of spatially varying materials without corresponding re/f_lectance parameters‚Äì to augment the training of a CNN from a much smaller set of labeled training data. To ‚Äúupgrade‚Äù such unlabeled data for training, a pre diction of the unknown model parameters is needed. We propose to use the target CNN itself to generate a provisional estimate of the re/f_lectance properties in the unlabeled photographs. However, we cannot directly use the provisional estimates and the corresponding unlabeled photograph as a valid training pair, since the estimated parameters are likely biased by the errors in the CNN, and hence it misses the necessary information to correct these errors. Our key observation is that for SVBRDF estimation, the exact inverse of the desired CNN is actually known in the form of a physicallybased rendering algorithm, that given any lighting and view parameters, synthesizes a photograph of the estimated re/f_lectance parameters. Under the assumption that the initial CNN trained by the labeled data acts as a reasonable predictor, the resulting provisional re /f_lectance estimates represent reasonable SVBRDFs similar (but not identical) to the SVBRDFs in the unlabeled training photographs. Therefore, instead of directly using the provisional re/f_lectance esti mates and unlabeled photographs as training pairs, we synthesize a new training sample by rendering an image with the provisional re/f_lectance estimates under random lighting and view. After re/f_ining the CNN using this synthesized training data, we can update the provisional re/f_lectance estimates and corresponding synthetic visu alizations, and repeat the process. The proposed selfaugmentation training process progressively re/f_ines the CNN to be coherent with the known inverse process (i.e., rendering algorithm), thereby im proving the accuracy of the target CNN. We demonstrate the ef /f_icacy of our method by training a CNN for diÔ¨Äerent classes of spatially varying materials such as wood, plastics and metals, as well as perform a careful analysis and validation of the proposed selfaugmentation training strategy. 2 RELATED WORK "
128,Are you wearing a mask? Improving mask detection from speech using augmentation by cycle-consistent GANs.txt,"The task of detecting whether a person wears a face mask from speech is
useful in modelling speech in forensic investigations, communication between
surgeons or people protecting themselves against infectious diseases such as
COVID-19. In this paper, we propose a novel data augmentation approach for mask
detection from speech. Our approach is based on (i) training Generative
Adversarial Networks (GANs) with cycle-consistency loss to translate unpaired
utterances between two classes (with mask and without mask), and on (ii)
generating new training utterances using the cycle-consistent GANs, assigning
opposite labels to each translated utterance. Original and translated
utterances are converted into spectrograms which are provided as input to a set
of ResNet neural networks with various depths. The networks are combined into
an ensemble through a Support Vector Machines (SVM) classifier. With this
system, we participated in the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020
Computational Paralinguistics Challenge, surpassing the baseline proposed by
the organizers by 2.8%. Our data augmentation technique provided a performance
boost of 0.9% on the private test set. Furthermore, we show that our data
augmentation approach yields better results than other baseline and
state-of-the-art augmentation methods.","In this paper, we describe our system for the Mask Sub Challenge (MSC) of the INTERSPEECH 2020 Computational Paralinguistics Challenge (ComParE) [1]. In MSC, the task is to determine if an utterance belongs to a person wearing a face mask or not. As noted by Schuller et al. [1], the task of detect ing whether a speaker wears a face mask is useful in modelling speech in forensics or communication between surgeons. In the context of the COVID19 pandemic, another potential applica tion is to verify if people wear surgical masks. We propose a system based on Support Vector Machines (SVM) [2] applied on top of feature embeddings concate nated from multiple ResNet [3] convolutional neural networks (CNNs). In order to improve our mask detection performance, we propose a novel data augmentation technique that is aimed at eliminating biases in the training data distribution. Our data augmentation method is based on (i)training Genera tive Adversarial Networks (GANs) with cycleconsistency loss [4, 5] for unpaired utterancetoutterance translation among two classes (with mask and without mask), and on (ii)generating new training utterances using the cycleconsistent GANs, as signing opposite labels to each translated utterance. While deep neural networks attain stateoftheart results in various domains [3, 6, 7, 8, 9], such models can easily succumbto the pitfall of overÔ¨Åtting [10]. This means that deep models can take decisions based on various biases existing in training data. A notorious example is an image of a wolf being correctly labeled, but only because of the snowy background [11]. In our case, the training samples belonging to one class may have different gender and age distribution than the training samples belonging to the other class, among other unknown biases. In stead of Ô¨Ånding relevant features to discriminate utterances with and without mask, a neural network might consider features for gender prediction or age estimation, which is undesired. With our data augmentation approach, all utterances with mask are translated to utterances without mask and the other way around, as shown in Figure 1. Any potential bias in the distribution of training data samples is eliminated through the compensation that comes with the augmented data samples from the opposite class. This forces the neural networks to discover features that discriminate the training data with respect to the desired task, i.e. classiÔ¨Åcation into mask versus nonmask . We conduct experiments on the Mask Augsburg Speech Corpus (MASC), showing that our data augmentation approach attains superior results in comparison to a set of baselines, e.g. noise perturbation and time shifting, and a set of stateofthe art data augmentation techniques, e.g. speed perturbation [12], conditional GANs [13] and SpecAugment [14]. 2. Related Work "
129,On Robust Learning from Noisy Labels: A Permutation Layer Approach.txt,"The existence of label noise imposes significant challenges (e.g., poor
generalization) on the training process of deep neural networks (DNN). As a
remedy, this paper introduces a permutation layer learning approach termed
PermLL to dynamically calibrate the training process of the DNN subject to
instance-dependent and instance-independent label noise. The proposed method
augments the architecture of a conventional DNN by an instance-dependent
permutation layer. This layer is essentially a convex combination of
permutation matrices that is dynamically calibrated for each sample. The
primary objective of the permutation layer is to correct the loss of noisy
samples mitigating the effect of label noise. We provide two variants of PermLL
in this paper: one applies the permutation layer to the model's prediction,
while the other applies it directly to the given noisy label. In addition, we
provide a theoretical comparison between the two variants and show that
previous methods can be seen as one of the variants. Finally, we validate
PermLL experimentally and show that it achieves state-of-the-art performance on
both real and synthetic datasets.","Deep Neural Networks (DNNs) have achieved outstand ing performance on many vision problems, including image classiÔ¨Åcation (Krizhevsky, Sutskever, and Hinton 2012), ob ject detection (Redmon et al. 2016), semantic segmentation (Long, Shelhamer, and Darrell 2015), and scene labeling (Farabet et al. 2012). The success in these challenges heav ily relies on the availability of huge and correctly labeled datasets, which are very expensive and timedemanding to collect. To overcome this challenge, a number of crowd sourcing platforms such as Amazon‚Äôs Mechanical Turk and nonexpert sources such as Internet Web Images, where la bels are inferred by surrounding text or keywords, have been developed over the past years (Xiao et al. 2015a). Although these methods reduce the labeling cost, the labels derived from these techniques are unreliable due to the high noise rate caused by human annotators or extraction algorithms (Paolacci, Chandler, and Ipeirotis 2010; Scott, Blanchard, and Handy 2013). In addition, more challenging tasks usu ally require domain expert annotators and thus are highly prone to mislabeling (Fr ¬¥enay and Verleysen 2013; Lloyd *These authors contributed equally.et al. 2004), such as breast tumor classiÔ¨Åcation (Lee et al. 2018). Training on such noisy datasets negatively impacts the performance of DDNs (i.e., poor generalization) due to the memorization effect (Maennel et al. 2020). Interestingly, the authors in (Zhang et al. 2021) illustrated that DNNs can eas ily Ô¨Åt randomly labeled training data. This property is es pecially problematic when training in the presence of label noise since typical DNNs tend to memorize the noisy in stances leading to a subpar classiÔ¨Åer. To overcome this impediment, we propose a learnable permutation layer that is applied to one of the training loss arguments (i.e., model prediction or label). Each training sample has an independent permutation layer associated with a learnable parameter . The purpose of the permu tation layer is to correct the loss of noisy samples through permuting predictions or labels during training, allowing the model to learn safely from them. During inference, the per mutation layer is discarded. Our contributions can be sum marized as follows: 1. We propose a permutation layer learning framework PermLL that can effectively learn from datasets contain ing noisy labels. 2. We theoretically analyze two variants of PermLL. The Ô¨Årst approach applies the permutation layer to the pre dictions, while the second applies the permutation to the labels. We show that the approach of learning the labels directly, proposed in Joint Optimization (Tanaka et al. 2018), can be seen as a special case of PermLL. 3. We provide a theoretical analysis of the two proposed methods, showing that applying the permutation layer to the predictions has better theoretical properties. 4. We empirically demonstrate the effectiveness of PermLL on synthetic and real noise, achieving stateoftheart performance on CIFAR10, CIFAR100, and Cloth ing1M. 2 Related Work "
130,How Universal is Genre in Universal Dependencies?.txt,"This work provides the first in-depth analysis of genre in Universal
Dependencies (UD). In contrast to prior work on genre identification which uses
small sets of well-defined labels in mono-/bilingual setups, UD contains 18
genres with varying degrees of specificity spread across 114 languages. As most
treebanks are labeled with multiple genres while lacking annotations about
which instances belong to which genre, we propose four methods for predicting
instance-level genre using weak supervision from treebank metadata. The
proposed methods recover instance-level genre better than competitive baselines
as measured on a subset of UD with labeled instances and adhere better to the
global expected distribution. Our analysis sheds light on prior work using UD
genre metadata for treebank selection, finding that metadata alone are a noisy
signal and must be disentangled within treebanks before it can be universally
applied.","Identifying document genre automatically has long been of interest to the NLP community due to its immediate applications both in document grouping (Petrenz, 2012) as well as taskspeciÔ¨Åc data selec tion (Ruder and Plank, 2017; Sato et al., 2017). Crosslingual genre identiÔ¨Åcation has however remained a challenge, mainly due to the lack of stable crosslingual representations (Petrenz, 2012). Recent work has shown that pretrained masked language models (MLMs) capture monolingual genre (Aharoni and Goldberg, 2020). Do such distinctions man ifest in highly multilingual spaces as well? In this work, we investigate whether this property holds for the genre distribution in the 114 language Universal Dependencies corpus (UD version 2.8; Zeman et al., 2021) using the multilingual mBERT MLM (Devlin et al., 2019). In absence of an exact deÔ¨Ånition of textual genre (Kessler et al., 1997; Webber, 2009; Plank, 2016), this work will focus on the information speciÔ¨Åcally denoted by the genres metadata tag in UD. We hope that an indepth, crosslingual analysis of what this label represents will enable practitioners to better control for the effects of domain shift in their experiments. Previous work using these UD metadata for proxy training data selection have produced mixed results (Stymne, 2020). We investigate possible reasons and identify inconsistencies in genre annotation. The fact that genre labels are only available at the level of treebanks makes it difÔ¨Åcult to gather a clear picture of the sentencelevel genre distribution ‚Äî especially with some treebanks having up to 10 genre labels. We therefore investigate the degree to which instancelevel genre is recoverable using only the treebanklevel metadata as weak supervision. Our contributions entail the, to our knowledge, Ô¨Årst detailed deÔ¨Ånition of all UD metadata genre labels (Section 3), four weakly supervised methods for extracting instancelevel genre across 114 languages (Section 4) as well as genre identiÔ¨Åcation experiments which show that our proposed twostep procedure allows for effective genre recovery in multilingual setups where language relatedness typically outweighs genre similarities (Section 5).1 This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ . 1Code available at https://personads.me/x/syntaxfest2021code .arXiv:2112.04971v1  [cs.CL]  9 Dec 20212 Related Work "
131,Unsupervised Person Re-Identification with Wireless Positioning under Weak Scene Labeling.txt,"Existing unsupervised person re-identification methods only rely on visual
clues to match pedestrians under different cameras. Since visual data is
essentially susceptible to occlusion, blur, clothing changes, etc., a promising
solution is to introduce heterogeneous data to make up for the defect of visual
data. Some works based on full-scene labeling introduce wireless positioning to
assist cross-domain person re-identification, but their GPS labeling of entire
monitoring scenes is laborious. To this end, we propose to explore unsupervised
person re-identification with both visual data and wireless positioning
trajectories under weak scene labeling, in which we only need to know the
locations of the cameras. Specifically, we propose a novel unsupervised
multimodal training framework (UMTF), which models the complementarity of
visual data and wireless information. Our UMTF contains a multimodal data
association strategy (MMDA) and a multimodal graph neural network (MMGN). MMDA
explores potential data associations in unlabeled multimodal data, while MMGN
propagates multimodal messages in the video graph based on the adjacency matrix
learned from histogram statistics of wireless data. Thanks to the robustness of
the wireless data to visual noise and the collaboration of various modules,
UMTF is capable of learning a model free of the human label on data. Extensive
experimental results conducted on two challenging datasets, i.e., WP-ReID and
DukeMTMC-VideoReID demonstrate the effectiveness of the proposed method.","PERSON reidentiÔ¨Åcation is essentially a person retrieval task in a multicamera surveillance network. Given an image or video of a person that we are interested in, it aims to Ô¨Ånd out the images or videos of this person from a large data corpus captured by multiple surveillance cameras. The supervised person reidentiÔ¨Åcation [1], [2], [3], [4], [5], [6], [7], [8] requires massive and exhaustive identity labeling of the crosscamera data, which is laborious and suffers the scalability issue in realworld applications. To bypass the label requirement, unsupervised person reidentiÔ¨Åcation [9], [10], [11], [12], [13], [14], [15] is proposed to directly learn models from unlabeled data. Thanks to its favorable potential, it has received substantial attention from both academia and industry in recent years. The existing unsupervised person reidentiÔ¨Åcation meth ods rely on visual cues for model training. Although re markable progress has been made, the defects of the visual data limit their further improvements. In other words, the occlusion and blur lead to the loss of distinguishing parts of pedestrians. Besides, the changes in viewpoints and clothing cause signiÔ¨Åcant appearance changes in pedestrians. These visual noises can easily mislead existing methods that rely solely on visual clues. When the visual data is unreliable, the performance of these methods cannot be guaranteed. These defects of visual data force us to seek new supplementary information to increase the robustness of the system. Yiheng Liu, Wengang Zhou, Qiaokang Xie, and Houqiang Li are with CAS Key Laboratory of GIP AS, University of Science and Technology of China, Hefei, China. Wengang Zhou and Houqiang Li are also with Institute of ArtiÔ¨Åcial Intelligence, Hefei Comprehensive National Science Center. Email: flyh156, xieqiaokg@mail.ustc.edu.cn,fzhwg, lihqg@ustc.edu.cn. Corresponding authors: Wengang Zhou and Houqiang Li. Wireless  fragment Wireless positioning trajectory Camera monitoring area Sensing area of wireless  fragments Fig. 1. The problem we study in this work. The wireless positioning trajectories of pedestrians carrying mobile phones can be obtained by existing cellular networks and WiFi positioning. Video data are captured when pedestrians walk to the monitoring areas of cameras. We consider the area within a preset sensing radius centered on the camera as the sensing area of the wireless trajectory fragments. When a pedes trian carrying a mobile phone enters the sensing area of the wireless fragments, the fragment of the wireless positioning trajectory within the sensing area is the sensed wireless fragment. The wireless fragment may belong to the same pedestrian as one of the videos captured by this camera during its time range. For the person reidentiÔ¨Åcation task, there have been some attempts to use the multimodal data. Fan et al. [16] propose to use radar signals for supervised person re identiÔ¨Åcation, but additional radar equipment is required. Liu et al . [17] introduce the wireless position trajectories for crossdomain person reidentiÔ¨Åcation. Wireless posi tioning trajectories can be obtained through offtheshelfarXiv:2110.15610v2  [cs.CV]  5 Apr 2023JOURNAL OF LATEX CLASS FILES, VOL. **, NO. **, ** ** 2 equipment, such as cellular networks and WiFi positioning systems. The immutability of signal ID allows it to assist visionbased person reidentiÔ¨Åcation task. However, they need to label the GPS coordinate of each location of the monitoring areas to map the pixel coordinate to the world coordinate for calculating the distance between videos and wireless signals [17]. Such a paradigm suffers the scalability issue since in real scenarios, labeling and maintaining the location information of each position in all monitoring areas extremely consumes human labor. Based on the above discussion, as shown in Fig. 1, we propose a new setting to assist unsupervised person reidentiÔ¨Åcation with wireless trajectories to mitigate the effects of visual noise (Fig. 2). For pedestrians carrying mobile phones, their wireless positioning trajectories can be obtained through existing cellular networks and WiFi positioning. We deÔ¨Åne the circular area whose distance from the surveillance camera is less than a preset distance as the wireless fragment sensing area of this camera. Once a pedestrian carrying a mobile phone enters this area, we assume that a person enters the monitored area and a wireless fragment is sensed. The wireless fragment is the fragment of the wireless positioning trajectory within the circular sensing area. The video of the owner of the wireless trajectory should belong to the set of videos captured by the corresponding camera during the time range of this wireless fragment. This weak relationship allows us to use wireless data to assist person reidentiÔ¨Åcation. Different from [16], our setting does not require new equipment. Compared with [17] which assumes the GPS coordinate labeling on the entire monitoring areas, our weak scene labeling setting only needs to access the locations of cameras. Therefore, our setting is more feasible and scalable for largescale surveillance scenarios than previous settings [16], [17]. Our weak scene labeling makes the monitoring system easier to maintain and deploy, but it also brings great chal lenges. In each surveillance scene, since many pedestrians are carrying mobile phones, multiple videos and multi ple wireless fragments are captured simultaneously. For a wireless fragment, the video of its owner is mixed in the videos captured by the corresponding camera during its time range, but we don‚Äôt know which video corresponds to, since the association between videos and wireless fragments is unknown. This weak physical connection brings great challenges for us to use it to assist person reidentiÔ¨Åcation. In this work, we propose a new task, which is to use wireless positioning trajectories to assist unsupervised per son reidentiÔ¨Åcation under weak scene labeling. To handle the challenges in this new task, we devise a novel unsu pervised multimodal training framework (UMTF), which contains a multimodal data association strategy (MMDA) and a multimodal graph neural network (MMGN). MMDA constructs detailed multimodal data associations through an adaptive clustering method. MMGN passes multimodal messages in the video graph by adaptively learning the ad jacency matrix from multimodal data. By using these mod ules to mine the potential clues in unlabeled multimodal data, UMTF improves the model quality progressively. The experimental results conducted on two challenging datasets, i.e.WPReID and Campus4K, demonstrate the effectiveness of our method. Fig. 2. The occlusion, blur and clothing changes examples on existing person reidentiÔ¨Åcation datasets, which introduce many challenges for existing methods that rely on visual data. Each row in the Ô¨Ågure contains two video sequences belonging to the same person, each with three frames. 2 R ELATED WORKS "
132,Becoming More Robust to Label Noise with Classifier Diversity.txt,"It is widely known in the machine learning community that class noise can be
(and often is) detrimental to inducing a model of the data. Many current
approaches use a single, often biased, measurement to determine if an instance
is noisy. A biased measure may work well on certain data sets, but it can also
be less effective on a broader set of data sets. In this paper, we present
noise identification using classifier diversity (NICD) -- a method for deriving
a less biased noise measurement and integrating it into the learning process.
To lessen the bias of the noise measure, NICD selects a diverse set of
classifiers (based on their predictions of novel instances) to determine which
instances are noisy. We examine NICD as a technique for filtering, instance
weighting, and selecting the base classifiers of a voting ensemble. We compare
NICD with several other noise handling techniques that do not consider
classifier diversity on a set of 54 data sets and 5 learning algorithms. NICD
significantly increases the classification accuracy over the other considered
approaches and is effective across a broad set of data sets and learning
algorithms.","The goal of supervised machine learning is to induce an accurate gen eralizing function from a set of labeled training instances. However, most re alworld data sets are noisy. Generally, two types of noise are considered: attribute noise and label noise. Previous work has found that, in general, labe l noise is more harmful than attribute noise [2, 3]. The consequences of labe l noise, as summarized by Fr¬¥ enay and Verleysen [4], include 1) a deterioration o f classi Ô¨Åcation performance, 2) increased learning requirements and mod el complex ity, and 3) a distortion of observed frequencies. Knowing which inst ances are noisy and/or detrimental is nontrivial as, in most cases, all that is known about a task is contained in the set of training instances. As discussed in the related work section, prior work has examined ha n dling label noise using a variety of approaches that are generally spe ciÔ¨Åc to, or inspired by, an individual learning algorithm or information theo retic measure. One commonly used approach removes the instances tha t are mis classiÔ¨Åed by a learning algorithm [5, 6]. Although such an approach is bia sed towards the learning algorithm that is used, it has generally been sho wn to work well on the examined data sets and learning algorithms especially with the addition of artiÔ¨Åcial noise. However, it has also been shown that the ef Ô¨Åcacy of a speciÔ¨Åc noise handling technique for a given learning algorit hm is dependent upon the data set characteristics and, in some cases, using a noise handling technique reducesthe classiÔ¨Åcation accuracy ‚Äì especially without the addition of artiÔ¨Åcial noise [1, 7]. As ensembles often perform bet ter than any one of its constituent base classiÔ¨Åers in classiÔ¨Åcation [8], other prior work has used ensemble techniques to improve handling class noise [9, 10, 1 1]. For an ensemble to be more accurate than any individual classiÔ¨Åer of the ensem ble, the base classiÔ¨Åers need to be accurate (better than random ) and diverse [12]. Using an ensemble technique for identifying noise implicitly lessens t he dependence on a single hypothesis. However, none of the previous work has explicitly focused on selecting diverse base classiÔ¨Åers when using an e nsemble approach. Inspired by the Ô¨Ånding that noise handling is notalways eÔ¨Écacious and using the principles of why ensembles increase classiÔ¨Åcation accurac y (specif ically diversity), we propose noise identiÔ¨Åcation using classiÔ¨Åer diversity (NICD). NICD Ô¨Årst explicitly selects a set of diverse learning algorith ms where diversity is determined by the predictions of the classiÔ¨Åers. T he di versity lessens the dependence of a noise measure on a speciÔ¨Åc hyp othesis. 2Without diversity, the same hypothesis could be overrepresente d if two hy potheses always classify the same way. We examine using the set of d iverse learning algorithms to 1) Ô¨Ålter the instances, 2) weight the instance s, and 3) as the base classiÔ¨Åers for a voting ensemble. Onaset of54datasetsand5learningalgorithms, wecompareNICDw ith 8 Ô¨Åltering techniques, 2 weighting techniques, and a voting ensemble com posed of diÔ¨Äerent base classiÔ¨Åers ‚Äì all of which do not explicitly take c lassiÔ¨Åer diversity into account. We Ô¨Ånd that using classiÔ¨Åer diversity signiÔ¨Åca ntly im proves the accuracy for Ô¨Åltering, weighting, and voting ensembles across a broadset of data sets, learning algorithms and noise levels ‚Äì demonstrat ing a robustness to label noise. The term broadrefers to the characteristic that the noise handling method was notdeveloped speciÔ¨Åcally for a given set of data sets and learning algorithms. Overall, using NICD in a votin g ensemble to select a diverse set of base classiÔ¨Åers achieves signiÔ¨Åca ntly higher classiÔ¨Åcation accuracy then using a standard noise handling techniq ue. The remainder of the paper is organized as follows. Section 2 reviews prior work in handling label noise. Section 3 then presents our metho dology for selecting diverse learning algorithms. Our experimental method ology is presented in Section 4. The results are provided in Section 5. Sectio n 6 concludes the paper. 2 Related Work "
133,PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training.txt,"Road network and building footprint extraction is essential for many
applications such as updating maps, traffic regulations, city planning,
ride-hailing, disaster response \textit{etc}. Mapping road networks is
currently both expensive and labor-intensive. Recently, improvements in image
segmentation through the application of deep neural networks has shown
promising results in extracting road segments from large scale, high resolution
satellite imagery. However, significant challenges remain due to lack of enough
labeled training data needed to build models for industry grade applications.
In this paper, we propose a two-stage transfer learning technique to improve
robustness of semantic segmentation for satellite images that leverages noisy
pseudo ground truth masks obtained automatically (without human labor) from
crowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid
Pooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation
that uses focal loss, poly learning rate, and context module. We demonstrate
the strengths of our approach through evaluations done on three popular
datasets over two tasks, namely, road extraction and building foot-print
detection. Specifically, we obtain 78.19\% meanIoU on SpaceNet building
footprint dataset, 67.03\% and 77.11\% on the road topology metric on SpaceNet
and DeepGlobe road extraction dataset, respectively.","Maps are common heritage for humanity. Creating and updating maps is an important task with many potential applications in disaster recovery, geospatial search, urban planning, ridehailing industry, etc [ 9]. Today, map features such as roads, building foot prints, and points of interest are primarily created through manual techniques. We address the problem of extracting road networks and building footprints automatically from satellite imagery in this paper. Maps of roads and buildings are expensive to build and maintain. Although modern cartography involves using satellite and aerial imagery along with GPS traces, using these data to update maps involves having humans analyze the data in a time consuming process, and thus maps of rapidly growing cities (where infras tructure is constantly under construction) are still often inaccurate and incomplete outside the urban core. Current approaches to map inference via satellite images [ 10,39] involve extensive annotations of images that is both tedious and time consuming. Currently, we have two large public datasets containing satellite images of road networks with annotations: i) SpaceNet [ 39] and DeepGlobe [ 10]. SpaceNet challenge 3 road extraction [ 39] only provides 2779 training satellite images of Las Vegas, Paris, Shanghai,arXiv:2010.06932v1  [cs.CV]  14 Oct 2020   OpenStreetMap road geometry shapefile Rasterize road network Using Tilemill (GIS software) Our rasterization Using OpenCVFigure 1: Illustration of the rasterization process to con vert OpenStreetMap data into pseudo ground truth mask im age. Top row: rasterization using TileMill software. Bottom row: our rasterization using OpenCV from the correspond ing road network graph. Best viewed in color. Khartoum with image size of 1300√ó1300 with their road center line annotations. ii) DeepGlobe [ 10] has a total of 8570 images with 6226 training, 1243 validation and 1101 testing images with pixelbased annotations, where all pixels belonging to the road are labeled, instead of labeling only centerline. Although seemingly large, these datasets are still not enough to train a robust model for analyzing satellite imagery on a global scale due to challenges posed by spatial variations ‚Äì roads differ in their appearance due to regional terrain, urban vs rural divide, state of economy, e.g., developed vs. developing countries, see Figure 2) and temporal variations resulting in images captured during different seasons with varying lighting conditions, cloud cover, etc. In this paper, we develop an efficient and effective technique to finetune semantic segmentation models robustly on different variances of satellite images. The centered idea is that we leverage OpenStreetMap(OSM) data to generate noisy pseudo ground truth of road network or building footprint and utilize it in the training process. Our contributions are threefold. First, we develop a novel method to generate pseudo road network and building footprint ground truth masks from OSM data without human annotation labor. Sec ond, we propose a twostage transfer learning to utilize the pseudo ground truth masks in the first stage, and finetune the model on high quality annotation data in the second stage. In addition, we also propose some techniques and architectural modifications, i.e., PPLinkNet, to improve the overall performance of the binary se mantic segmentation system to extract road networks and building footprints from satellite images. Finally, we achieve promising im provements on DeepGlobe road network dataset[ 10], SpaceNet road and building footprint dataset [39].2 RELATED WORK "
134,Abstractive Text Classification Using Sequence-to-convolution Neural Networks.txt,"We propose a new deep neural network model and its training scheme for text
classification. Our model Sequence-to-convolution Neural Networks(Seq2CNN)
consists of two blocks: Sequential Block that summarizes input texts and
Convolution Block that receives summary of input and classifies it to a label.
Seq2CNN is trained end-to-end to classify various-length texts without
preprocessing inputs into fixed length. We also present Gradual Weight
Shift(GWS) method that stabilizes training. GWS is applied to our model's loss
function. We compared our model with word-based TextCNN trained with different
data preprocessing methods. We obtained significant improvement in
classification accuracy over word-based TextCNN without any ensemble or data
augmentation.","Ever since humans began to record information in the form of text, it was necessary to classify and manage information in a certain category to store and retrieve information efÔ¨Åciently. This need encouraged many researchers to develop a good text classiÔ¨Åcation technique that can assign predeÔ¨Åned categories to various kinds of text document such as emails, news articles, reviews, or patents. In commercial world, text classiÔ¨Åcation techniques such as Na√Øve Bayes classiÔ¨Åer[ 5], TFIDF[ 37], Support Vector Machines(SVM)[ 15] are already used in various Ô¨Åelds including spam Ô¨Åltering, news categorization, and sentiment analysis. Recent development in deep neural networks[ 17,39,19,38,7] are also achieving excellent results in extracting information from a text and classifying it into certain classes. As Convolutional Neural Networks(CNNs) achieved remarkable results in computer vision[ 32,35, 11,28], researchers also applied CNNs to text classiÔ¨Åcation[ 17,19,38,7] and showed excellent results. Training CNNs on top of pretrained word vectors[ 22,16,27] or characterlevel features[ 38,7] with hyperparameter tuning, they could get similar or outperforming results compared to other text classiÔ¨Åcation models. Although TextCNNs‚Äô performance in text classiÔ¨Åcation is remarkable, they can only be applied to data whose input has Ô¨Åxed size. Since the number of parameters in TextCNN is determined by the length of input text, researchers had to crop or pad input texts into a certain length to train their TextCNN. This can result information loss when classifying longer texts and cause performance degradation. In section 4.2, we show that performance of TextCNNs can be improved by training the model with summaries of input text. There are two ways to generate the summary of a text. One is extractive Preprint. Work in progress.arXiv:1805.07745v6  [cs.CL]  2 Jun 2020summarization, mere selection of a few existing sentences extracted from the source. The other is abstractive summarization, compressed paraphrasing of main contents of source, potentially using vocabulary unseen in the source. Both methods can change texts of various lengths into texts of Ô¨Åxed length still maintaining important features of source texts. TextRank[ 21] is a graphbased ranking model for extractive text summarization. TextRank gives a ranking over all sentences in a text allowing it to extract very short summaries without any training corpora. TextRank is widely used in summarizing structured text like news articles. Many researchers worked with Sequencetosequence Recurrent Neural Networks (Seq2seq RNNs)[ 34,24] to model abstractive text summarization. Using attention mechanism[ 6] that al lows neural networks to focus on different parts of their input, Seq2seq RNNs have been showing signiÔ¨Åcant results in the task of abstractive summarization[29, 24]. In this paper, we introduce Sequencetoconvolution Neural Networks(Seq2CNN) model that consists of two blocks: Sequence Block and Convolution Block. Sequence Block based on Attentional EncoderDecoder Recurrent Neural Networks[ 24] summarizes input texts and feeds them into Convo lution Block. Convolution Block based on TextCNN[ 17] classiÔ¨Åes input texts into certain classes using the summaries provided by Sequential Block. Both blocks share nonstatic word embedding layer, encouraging them to collaborate for performance improvement. Simply connecting two blocks and train them with single endtoend procedure cannot guarantee optimal results because Sequential Block doesn‚Äôt generate proper summaries in early stages of training. To solve this problem, we also propose a new training scheme that gradually shifts from Ô¨Åne tuning for summarization task to Ô¨Ånetuning for classiÔ¨Åcation task as training progresses. Our model is implemented with TensorÔ¨Çow[1]. Code is available at https://github.com/tgisaturday/Seq2CNN. 2 Related Work "
135,Learning from Multiple Annotators by Incorporating Instance Features.txt,"Learning from multiple annotators aims to induce a high-quality classifier
from training instances, where each of them is associated with a set of
possibly noisy labels provided by multiple annotators under the influence of
their varying abilities and own biases. In modeling the probability transition
process from latent true labels to observed labels, most existing methods adopt
class-level confusion matrices of annotators that observed labels do not depend
on the instance features, just determined by the true labels. It may limit the
performance that the classifier can achieve. In this work, we propose the noise
transition matrix, which incorporates the influence of instance features on
annotators' performance based on confusion matrices. Furthermore, we propose a
simple yet effective learning framework, which consists of a classifier module
and a noise transition matrix module in a unified neural network architecture.
Experimental results demonstrate the superiority of our method in comparison
with state-of-the-art methods.","The success of supervised learning applications often relies on largescale welllabeled datasets. Unfortunately, obtaining highquality annotations from experts can be costly in terms of time and money. Alternatively, crowdsourcing provides an inexpensive approach to data labeling by hiring worldwide annotators on public platforms like Amazon Mechanical Turk (AMT). However, crowdsourced labels are usually noisy due to the existence of inexperienced or malicious annotators. Us ing these noisy labels in supervised learning may result in an inaccurate classiÔ¨Åer. A straightforward way to solve this problem is redundant labeling, i.e.,obtaining multiple labels for each instance from multiple annotators. Hence this raises one fundamental problem termed as Learning from Crowds (LFC) [Rodrigues and Pereira, 2018 ]:‚ÄúHow can we learn a good classiÔ¨Åer from a set of possibly noisy labeled data pro vided by multiple annotators?‚Äù To address the above issue, a twostage approach is com monly adopted. First, in answer aggregation stage [Zheng et True label : highway Annotation: streetTrue label: highway Annotation: forest xn tn yn(r)P(r)œâ NR v(r)True label: highway Annotation: coast Figure 1: Top: An example describes various incorrect annotations. The Ô¨Årst randomly Ô¨Çips the true label to one of other classes; the sec ond is that the true label is corrupted to the relevant class according to a Ô¨Åxed probability; the third is that the true label is corrupted to the irrelevant class due to the inÔ¨Çuence of instance features. Bottom: The factor graph of LFC xrepresents the correlation of the instance xn, the true label tn, and crowdsourced labels yn. The annotation depends not only on the true label but also on instance features. al., 2017; Sheng and Zhang, 2019; Jin et al. , 2020 ], the latent true labels are estimated. Then, a classiÔ¨Åer is trained based on the estimated true labels. Alternatively, the onestage ap proach [Raykar et al. , 2009; Tanno et al. , 2019 ]has been shown to be a promising direction that presents a maximum likelihood estimator that jointly learns the classiÔ¨Åer, abilities of multiple annotators, and the latent true labels. Among various research efforts on LFC, the probability transition process from latent true labels to observed crowdsourced la bels is usually modeled with confusion matrices of annota tors, which represents classlevel probability transition. This means that the annotator‚Äôs performance is consistent across different instances within the same class, i.e.,the transition from classjto classlis independent of instance features. However, in the real world, the difÔ¨Åculty of labeling can vary among instances within the same class and the instance fea tures themselves will affect annotators‚Äô performance. Con sider a concrete instantiation of LabelMe dataset [Rodrigues et al. , 2017 ]in Figure 1.(Top), which illustrates various cases of incorrect annotations given the true label ‚Äúhighway‚Äù.arXiv:2106.15146v1  [cs.LG]  29 Jun 2021The Ô¨Årst indicates an inexperienced/malicious annotator who gives a random label ‚Äúcoast‚Äù; the second indicates an annota tor have biased understanding on different classes, preferring to label ‚Äúhighway‚Äù as ‚Äústreet‚Äù, because there is a strong cor relation between those two classes. In both cases, the class level confusion matrix of annotator can be used to character ize their varying abilities and own biases. Nevertheless, the third depicts one instance in class ‚Äúhighway‚Äù contains related visual features of other classes, misleading annotators label it as ‚Äúforest‚Äù, although these two classes are irrelevant. We ar gue that the classlevel confusion matrices cannot completely characterize the performance of multiple annotators across different instances within the same class. This would limit the ability to estimate latent true labels, resulting in suboptimal performance of the classiÔ¨Åer. It is necessary to consider the inÔ¨Çuence of instance features in the process of characterizing performance of multiple annotators for LFC. To address the aforementioned deÔ¨Åciency, this work aims at proposing a novel LFC framework, LFC x, which can learn a classiÔ¨Åer directly from crowdsourced labels provided by multiple annotators. In particular, beyond confusion matrices, LFCxmodels the probability transition process with noise transition matrices by combining the confusion matrices and instance features. To this end, we need to deal with two prac tical challenges. One is how to quantify the inÔ¨Çuence of in stance features on the performance of annotators in order to construct the noise transition matrix, the other is how to incor porate the noise transition matrix into LFC method. To cope with these challenges, Ô¨Årst, we model the correlation among instance features, latent true labels and crowdsourced labels in the probabilistic graphical model to construct the noise transition matrix. Furthermore, the LFC xconsists of two modules: the noise transition matrix module and the classiÔ¨Åer module. These two modules are integrated into an endtoend neural network system through a principled combination for maximizing a likelihood function. The graphical model of the LFCxis presented in Figure 1.(bottom). 2 Related Work "
136,Co-learning: Learning from Noisy Labels with Self-supervision.txt,"Noisy labels, resulting from mistakes in manual labeling or webly data
collecting for supervised learning, can cause neural networks to overfit the
misleading information and degrade the generalization performance.
Self-supervised learning works in the absence of labels and thus eliminates the
negative impact of noisy labels. Motivated by co-training with both supervised
learning view and self-supervised learning view, we propose a simple yet
effective method called Co-learning for learning with noisy labels. Co-learning
performs supervised learning and self-supervised learning in a cooperative way.
The constraints of intrinsic similarity with the self-supervised module and the
structural similarity with the noisily-supervised module are imposed on a
shared common feature encoder to regularize the network to maximize the
agreement between the two constraints. Co-learning is compared with peer
methods on corrupted data from benchmark datasets fairly, and extensive results
are provided which demonstrate that Co-learning is superior to many
state-of-the-art approaches.","The success of deep learning relies on carefully labeled data in largescale. However, precise annotations are extremely expensive and timeconsuming. To alleviate the problem, inexpensive alter natives are often used, such as web crawling [ 35] or completing ‚àóCorresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ‚Äô21, October 20‚Äì24, 2021, Virtual Event, China ¬©2021 Association for Computing Machinery. ACM ISBN 9781450386517/21/10. . . $15.00 https://doi.org/10.1145/3474085.3475622annotations with crowdsourcing and online queries. Unfortunately, these alternative methods can often leads to noisy labels. However, deep neural networks can easily overfit to noisy labels, as shown in recent research [ 18,22,34,40], and this can dramatically degrade the generalization performance. Minibatch 1Minibatch 2Minibatch 3DecouplingCoteachingCoteaching+JoCoRColearning!=!=!=ABBABAABABABABABABABABAB!=!=!=ABABAB Figure 1: Comparison of leading methods in dealing with noisy labels. All the methods use two cooperative modules (A & B). As the training goes, the two become more and more in agreement with each other, but in different ways. Decoupling updates the parameters of the two networks with predictiondisagreed (!=) samples from a minibatch. Coteaching [11] uses smallloss samples of one network to teach its peer network for further training. Coteaching+ [39] first predicts each minibatch with the two networks but uses disagreed (!=) samples only to compute the train ing loss. JoCoR [37] trains the two networks as a whole with a joint loss of weighted goals: making the two predictions agree with each other, and making the predictions stick to groundtrue labels as far as possible. Colearning trains a single shared encoder network with two heads (the self supervised and the noisilysupervised) that constrain each other and maximizes the agreement between them in latent space. Several studies have been conducted to investigate learning with noisy labels, where semisupervised learning frameworks play cru cial roles [ 1,8,17,25,38]. Most of these deploy unsupervised learn ing to obtain information about labelindependent features and then do further training using noisy labels in a supervised learning manner. These methods commonly leverage labels through two views: (1) supervised learning taking advantage of direct supervi sion with labels and (2) unsupervised learning exploiting intrinsic features from data to combat label noise. The core challenge here is how to combine the two views effectively. Arazo et al. [ 1] design a beta mixture model as an unsupervised generative model of sample loss values and then adopt MixUp [ 41] augmentation to assure reli able convergence under extreme noise levels. Li et al. [ 17] models sample loss with Gaussian mixtures to divide training data into unlabeled data and labeled data, and then apply the stateoftheartarXiv:2108.04063v4  [cs.LG]  30 Oct 2021semisupervised learning method MixMatch [ 2] to deal with the divided sets. However, the above methods are not endtoend and the stepbystep learning manner increases complexity in both time and space. Cotraining and model ensembling have been shown to be ben eficial in learning with noisy labels [ 11,23,37,39]. Decoupling [23] trains two networks simultaneously and updates them using instances with different predictions. Coteaching [ 11] also trains two networks simultaneously and selects smallloss data to teach the peer network during the training process. Coteaching+ [ 39] follows a similar scheme as Coteaching but with a scheme which selects smallloss data from disagreement data. JoCoR [ 37] main tains two networks but trains them as a whole with a joint loss to make predictions of them get closer. These methods are based on the assumption that two networks can provide two different views of the data, but the extra information gain they bring is very limited since the differences between two networks of the same architecture mainly come from random initialization. A compar ison between Colearning and other cotraininglike methods is illustrated in Figure 1. Motivated by semisupervised learning and cotraining, we pro pose a simple yet effective learning paradigm called "" Colearning "" to combat the problem with noisy labels, in which selfsupervised learning is introduced as a featuredependent view to assist su pervised learning. Different from those cotraininglike methods, Colearning has a shared feature encoder with two exclusive heads. While the projection head performs a selfsupervised learning and exploits featuredependent information through the intrinsic sim ilarity, the classifier head performs a vanilla supervised learning and learns from labeldependent information. In addition, a struc tural similarity loss is utilized between the two exclusive heads to regularize the classifier and avoid bias caused by noisy labels. This is based on the constraint that output from the projection head and that from the classifier head should share similar structure pairwise. Unlike the other methods, Colearning can be implemented conve niently without the need for such prior knowledge as noise rates, data distributions, and additional clean samples, thus avoiding the needed hyperparameters thereof. We conducted experiments on simulated and realworld noisy datasets, including CIFAR10, CIFAR100, Animal10N [ 32], and Food101N [ 16] datasets. Among these datasets, Animal10N is a noisy dataset that contains confusing images for manual annota tions, and Food101N is a webly dataset that directly collects data from the Web. Comparative results demonstrate that Colearning is superior to stateoftheart methods and robust to highlevel noise in labels. 2 RELATED WORK "
137,PASS: Peer-Agreement based Sample Selection for training with Noisy Labels.txt,"Noisy labels present a significant challenge in deep learning because models
are prone to overfitting. This problem has driven the development of
sophisticated techniques to address the issue, with one critical component
being the selection of clean and noisy label samples. Selecting noisy label
samples is commonly based on the small-loss hypothesis or on feature-based
sampling, but we present empirical evidence that shows that both strategies
struggle to differentiate between noisy label and hard samples, resulting in
relatively large proportions of samples falsely selected as clean. To address
this limitation, we propose a novel peer-agreement based sample selection
(PASS). An automated thresholding technique is then applied to the agreement
score to select clean and noisy label samples. PASS is designed to be easily
integrated into existing noisy label robust frameworks, and it involves
training a set of classifiers in a round-robin fashion, with peer models used
for sample selection. In the experiments, we integrate our PASS with several
state-of-the-art (SOTA) models, including InstanceGM, DivideMix, SSR, FaMUS,
AugDesc, and C2D, and evaluate their effectiveness on several noisy label
benchmark datasets, such as CIFAR-100, CIFAR-N, Animal-10N, Red Mini-Imagenet,
Clothing1M, Mini-Webvision, and Imagenet. Our results demonstrate that our new
sample selection approach improves the existing SOTA results of algorithms.","In deep neural networks (DNNs) and machine learning, it is commonly recognized that having an adequate amount of labeled training data and computational resources leads to exceptional outcomes in various Ô¨Åelds, such as com puter vision [6], natural language processing [62], and in *arpit.garg@adelaide.edu.aumedical domain [33]. Nonetheless, the aforementioned outcomes have predominantly been attained through the utilization of meticulously curated datasets possessing la bels of exceptional quality. The collection of such high caliber labels, particularly for large datasets, can prove to be exorbitantly costly in realworld scenarios [42, 60, 72]. Therefore, alternative cheaper labeling methods, includ ing crowdsourcing [60] and metadata mining [18], have gained traction, but they result in substandard labeling [20]. While these techniques aid in cost reduction and expedite the labeling process, they are susceptible to data mislabel ing [55]. Erroneous labels potentially degrade the performance of DNNs by inducing overÔ¨Åtting through the phenomenon of memorization [1, 39, 43, 50, 76]. This issue has prompted the development of innovative learning algorithms aimed at tackling the problem of noisy labeling. Within the domain of noisy labels, many methods have emerged [19, 39], each tailored to tackle the challenges posed by distinct settings of noise, namely instanceindependent noise (IIN) [22] and instancedependent noise (IDN) [68]. Early studies in the Ô¨Åeld of noisy label operated under the presumption that la bel noise was IIN, that is, mislabeling occurred irrespective of the information regarding the visual classes present in images [22]. In IIN, a transition matrix is generally em ployed, which comprises a predetermined probability of Ô¨Çipping between pairs of labels [72]. Nevertheless, recent studies have progressively redirected the Ô¨Åeld‚Äôs attention to ward the more realistic scenario of IDN [10, 19, 71], where label noise depends on both the true class label and the im age information. Previous techniques for mitigating the impact of noisy label samples frequently involve the manual selection of clean samples to form a clean validation set [56]. The difÔ¨Åculty in obtaining clean validation samples [26], par ticularly for problems with many classes, motivated recent studies to leverage semisupervised learning methods with out relying on clean validation sets [12, 39]. Other ap 1arXiv:2303.10802v1  [cs.CV]  20 Mar 2023A BSmalllossPASS AB CABC AB CBAA BFINE ABFigure 1. Illustration of the comparison between various sample selection strategies, including smallloss [22], featurebased tech niques [32], and our proposed PASS . In smallloss [22] we typically have two networks A & B, with each model using the loss values of the samples to select clean and noisy instances, which are exchanged between the two networks for training them. FINE [32] utilizes a featurebased selection approach based on the similarity to class speciÔ¨Åc eigenvectors in the representation space to select samples that are exchanged between two networks, A and B, for further training. Our PASS maintains three networks (A, B & C), where the prediction agreements between two networks will select the clean samples to be used for training the third network. Note that during training, the two networks used for sample selection will rotate through the three available networks. proaches incorporate robust loss functions [52], designed speciÔ¨Åcally to operate effectively with either clean or noisy labels, as well as probabilistic modeling approaches that model the data generation process [19, 71]. Furthermore, training regularization [43, 64] imposes a penalty term on the loss function during training, thereby reducing overÔ¨Åt ting and ameliorating generalization. Various techniques integrate sample selection strategies as a key algorithmic step [12, 13, 19, 39], allowing for the identiÔ¨Åcation and seg regation of noisy and clean labels. One popular criterion for this sample selection process is the loss value between the prediction of the trained classiÔ¨Åer and its label, whereby it is typically assumed that the noisy data exhibits a large loss [22, 30, 32, 64] or greater magnitude of the gradient during training [63]. Additionally, featurebased sample se lection techniques based on the similarity to principal com ponents of feature representations [32] or K nearest neigh bor (KNN) classiÔ¨Åcation in the feature space [18] have also been considered for sample selection criteria. Nonetheless, we empirically note that separating clean, but hard to clas sify samples from noisy label samples still remains chal lenging for these sample selection processes [64], partic ularly for problems of large noise rates. The use of peer classiÔ¨Åers for noisy label learning problems have been in vestigated for enforcing consistency in the training of the classiÔ¨Åers [22, 47], but not for selecting clean and noisy label samples. We argue in this paper that the prediction agreement between peer classiÔ¨Åers is more effective to se lect clean and noisy label samples than previous approaches because intuitively, such agreement is unlikely to happen, except when the classiÔ¨Åers agree on the clean label. In this paper, we propose a new sample selection criterion based on the predictive probability agreement be tween peer classiÔ¨Åers, which has been shown in computa tional linguistics to be a sample reliability measure [2]. In light of implementation, we train three classiÔ¨Åers simulta neously by using the agreement between two classiÔ¨Åers to select samples to train the remaining classiÔ¨Åer, as shown in Fig. 1. This sample selection relies on a thresholding algorithm [53] that distinguishes samples based on how high the agreement between the peer classiÔ¨Åcation predic tions is. Our proposed method, named as peeragreement based sample selection (PASS), can readily be integrated into existing models in noisy label learning, such as In stanceGM [19], DivideMix [39], SSR [18], FaMUS [70], AugDesc [51], and ContrasttoDivide (C2D) [77]. The pri mary contributions of our method can be delineated as fol lows: ‚Ä¢ We propose a new noisy label sample selection method, PASS, that differentiates clean and noisy la bel samples via the prediction agreement between peer classiÔ¨Åers. ‚Ä¢ We demonstrate that our method can be easily adapted to existing models, including InstanceGM [19], Di videMix [39], SSR [18], FaMUS [70], AugDesc [51], and C2D [77], where we show that PASS enhances the performance of various SOTA approaches on vari ous benchmarks, comprising both simulated and real world datasets, such as CIFAR100 [35], CIFAR N [66], Animal10N [59], Red MiniImagenet from Controlled Noisy Web Labels (CNWL) [70], Cloth ing1M [69], MiniWebvision [40], and Imagenet [16]. It is imperative to provide clarity that our proposition doesnot entail the introduction of a new algorithm for the classi Ô¨Åcation of noisy labels. Instead, we suggest a new method for selecting samples to substantially improve the efÔ¨Åcacy of preexisting noisy label learning algorithms, as shown by our experiments (Section 4). 2. Related Work "
138,Local Boosting for Weakly-Supervised Learning.txt,"Boosting is a commonly used technique to enhance the performance of a set of
base models by combining them into a strong ensemble model. Though widely
adopted, boosting is typically used in supervised learning where the data is
labeled accurately. However, in weakly supervised learning, where most of the
data is labeled through weak and noisy sources, it remains nontrivial to design
effective boosting approaches. In this work, we show that the standard
implementation of the convex combination of base learners can hardly work due
to the presence of noisy labels. Instead, we propose $\textit{LocalBoost}$, a
novel framework for weakly-supervised boosting. LocalBoost iteratively boosts
the ensemble model from two dimensions, i.e., intra-source and inter-source.
The intra-source boosting introduces locality to the base learners and enables
each base learner to focus on a particular feature regime by training new base
learners on granularity-varying error regions. For the inter-source boosting,
we leverage a conditional function to indicate the weak source where the sample
is more likely to appear. To account for the weak labels, we further design an
estimate-then-modify approach to compute the model weights. Experiments on
seven datasets show that our method significantly outperforms vanilla boosting
methods and other weakly-supervised methods.","Weaklysupervised learning (WSL) has gained significant attention as a solution to the challenge of label scarcity in machine learning. WSL leverages weak supervision signals, such as labeling functions or other models, to generate a large amount of weakly labeled data, which is easier to obtain than complete annotations. Despite achieving promising results in various tasks including text classifi cation [ 1], sequence tagging [ 2], and ecommerce [ 3], an empirical study [ 4] reveals that even stateoftheart WSL methods still un derperform fullysupervised methods by significant margins, where the average performance discrepancy is 18.84%, measured by accu racy or F1 score. On the other hand, boosting algorithm is one of the most com monly used approaches to enhance the performance of machine learning models by combining multiple base models [ 5‚Äì9]. For ex ample, AdaBoost [ 5] dynamically adjusts the importance weight of each training example to learn multiple base models and uses a weighted combination to aggregate these base models‚Äô predictions. XGBoost [ 8] iteratively computes the gradients and hessians de fined on a clean training set to fit base learners and combines their predictions via weighted summation. Despite the encouring perfor mance, these boosting algorithms usually assume the availability of a clean labeled dataset. In WSL, however, the imperfect supervision signals interfere with the training data importance reweightingarXiv:2306.02859v1  [cs.LG]  5 Jun 2023KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA Rongzhi Zhang et al. which further prevents us from computing an accurate weight of each base learner. If we naively apply these supervised boosting methods using a weakly labeled dataset, we observe a phenome non called ‚Äú weight domination ‚Äù where the assigned weight of the initial base model is too large and dominates the ensemble model prediction, as shown in Figure 1. A key challenge of adapting boosting methods to the WSL set ting is to accurately compute the importance of each example in the weakly labeled training data for each base learner. Previously, when a clean dataset is provided, the goal of data importance reweighing process is to prioritize instances with large errors for subsequent base learner training. This effectively localizes the base learner to the error region in the label space. However, in WSL, the noisy labels hinder the accurate identification of error instances and thus we need to shift our focus from the label space to the training data space. One potential approach is to partition the weaklylabeled training data into subsets and constructs a mixture of expert mod els (MoE) [ 10] where each expert is localized for one training data subset. Along this line, Tsai et al. [11] propose partitioning the unlabeled dataset into latent semantics subsets and using multiple expert models to discriminate instances. However, this approach assumes the input data naturally reside in a homogeneous feature space and requires a hyperparameter search to appropriately lo calize the expert models. Additionally, the offtheshelf clusters do not adapt during the learning process, which conflicts with the philosophy of boosting methods. We investigate the problem of boosting in the context of weakly supervised learning, where most of the data are labeled by weak sources and only a limited number of data points have accurate labels. To address the difficulties posed by this setting, we introduce LocalBoost , a novel iterative and adaptive framework for WSL boosting. LocalBoost retains the essential concepts of the tradi tional boosting approach while incorporating adaptations specifi cally designed for the WSL scenario, described as follows: ‚Ä¢Base Learner Locality . Motivated by the challenges posed by the data reweighting approach in AdaBoost for WSL and the limi tations of hard clustering in MoE methods, we propose a new approach to base learner localization. In AdaBoost, largeerror instances are assigned with larger weights for model training, however, this approach does not account for the fact that error instances exist in multiple feature regimes that are difficult to capture with weak labels. Additionally, the rigid clusters and fixed expert models in MoE cannot adapt in the iterative learn ing process, hindering the framework‚Äôs ability to dynamically target weak feature regimes and build upon preceding models. To address these issues, our proposed framework LocalBoost assigns base learners to adaptively updated local regions in the embedding space, thereby introducing locality to the base learners. ‚Ä¢Twodimension Boosting . Effective aggregation of localized base learners in WSL goes beyond the simple convex combination in supervised settings (as shown in Sec. 4.2). To account for po tential label noises from weak sources, we aim to learn multiple complementary base learners in LocalBoost . To fulfill this goal, we introduce a weighting function to compute the conditional probability of weak sources that are more likely to annotatea given data instance. We further design a twodimensional boosting framework in LocalBoost , where intersource boost ing and intrasource boosting are performed alternately. The former improves the base learners within a given weak source, while the latter complements the base learners with additional models trained from other weak sources. ‚Ä¢Interactions between Weak and Clean Labels . We incorporate the interactions between weak and clean labels into Local Boost framework in two steps: (1) We compute a mapping between the small clean dataset and the large weakly labeled dataset to localize base learners in the data embedding space. We first identify the errors made by the current model ensemble, and then sample corresponding clusters from the large weak dataset to form the training set for the next base learner. (2) We propose a novel estimatethenmodify approach for computing base learner weights. Initially, the weights are estimated on the large weakly labeled dataset. Then, we refine these estimates by generating multiple perturbations of the model weights and selecting the one that results in the lowest error rate on the small clean dataset as the modified weights. We evaluate LocalBoost on seven datasets including senti ment analysis, topic classification and relation classification from WRENCH [ 4], the standard benchmark for weakly supervised learn ing. The results indicate that LocalBoost achieves superior per formance compared with other stateoftheart methods. Moreover, our analysis further confirms the effectiveness of boosting in two dimensions and incorporating interactions between weak and clean labels. We summarize our key contributions as follows: (1)We present LocalBoost1, a novel weaklysupervised boosting framework that implements progressive intersource and intra source boosting. (2)We incorporate explicit locality into the base learners of the boosting framework, allowing them to specialize in finergrained data regions and perform well in specific feature regimes. (3)We leverage the interactions between weak and clean labels for effective base learner localization and weight estimation. (4)We conduct extensive experiments on seven benchmark datasets and demonstrate the superiority of LocalBoost over WSL and ensemble baselines. 2 RELATED WORK "
139,Recurrent Collective Classification.txt,"We propose a new method for training iterative collective classifiers for
labeling nodes in network data. The iterative classification algorithm (ICA) is
a canonical method for incorporating relational information into
classification. Yet, existing methods for training ICA models rely on the
assumption that relational features reflect the true labels of the nodes. This
unrealistic assumption introduces a bias that is inconsistent with the actual
prediction algorithm. In this paper, we introduce recurrent collective
classification (RCC), a variant of ICA analogous to recurrent neural network
prediction. RCC accommodates any differentiable local classifier and relational
feature functions. We provide gradient-based strategies for optimizing over
model parameters to more directly minimize the loss function. In our
experiments, this direct loss minimization translates to improved accuracy and
robustness on real network data. We demonstrate the robustness of RCC in
settings where local classification is very noisy, settings that are
particularly challenging for ICA.","Data science tasks often require reasoning about net works of connected entities, such as social and informa tion networks. In classication tasks, the connections among network nodes can have important eects on nodelabeling patterns, so models that perform classi cation in networks should consider network structure to fully represent the underlying phenomena. For ex ample, when classifying individuals by their personality traits in a social network, a common pattern is that individuals will communicate with likemined individu als, suggesting that predicted labels should also tendto be uniform among connected nodes. Collective clas sication methods aim to make predictions based on this insight. In this paper, we introduce a collective classication framework that will enable an algorithm to more directly optimize the performance of trained collective classiers. Iterative classication is a framework that enables a variety of supervised learning methods to incorporate information from networks. The base machine learn ing method can be any standard classier that labels examples based on input features. The iterative clas sication algorithm (ICA) operates by using previous predictions about neighboring nodes as inputs to the current predictor. This pipeline creates a feedback loop that allows models to pass information through the net work and capture the eect of structure on classication. In spite of the feedback loop being the most important aspect of ICA, existing approaches train models in a manner that ignores the feedbackloop structure. In this paper, we introduce recurrent collective classica tion (RCC), which corrects this discrepancy between the learning and prediction algorithms, incorporating principles used in deep learning and recurrent neural networks into the training process. Existing learning algorithms for iterative classication resort to an approximation based on the unrealistic assumption that the predicted labels of neighbors are their true classes (Neville and Jensen, 2000; London and Getoor, 2013). This assumption is overly opti mistic. If it were true, iteration would be unnecessary. Because the assumption is overly optimistic, it causes the learned models to cascade and amplify errors when the assumption is broken in early stages of prediction. In contrast, ICA uses predicted neighbor labels as feed back for each subsequent prediction, which means that if the model was trained expecting these predicted la bels to be perfect, it will not be robust to situations where predictions are noisy or inaccurate. In this paper, we correct this faulty assumption and develop an ap proach that trains models for iterative classication by treating the intermediate predictions as latent variables. We compute gradients to the classication loss function using backpropagation through iterative classication.arXiv:1703.06514v1  [cs.LG]  19 Mar 2017Recurrent Collective Classication To compute gradients for ICA, we break down the ICA process into dierentiable (or subdierentiable) opera tions. In many cases, the base classier is dierentiable with respect to its parameters. For example, if it is a logistic regression, it has a wellstudied gradient. ICA also computes dynamic relational features using the predictions of network neighbors. These relational fea tures are also functions through which gradients can be propagated. Finally, because the same baseclassier parameters should be used at all iterations of ICA, we can use methods for recurrent neural networks such as backpropagation through time (BPTT) (Werbos, 1990) to compute the combined gradient. In contrast with existing strategies for training ICA, the resulting training optimization more closely mimics the actual procedure that ICA uses for prediction. The RCC framework accommodates a variety of base classiers and relational feature functions. The only re striction is that they must be dierentiable. Therefore, RCC is nearly as general as ICA, and its prediction procedure is practically identical to ICA. The key dif ference is that the view of the algorithm as nested, dierentiable functions enables a training procedure that is better aligned with RCC and ICA prediction. We evaluate RCC on data where collective classication has previously been shown to be helpful. We demon strate that RCC trains classiers that are robust to situations where local predictions are inaccurate. 2 Related Work "
140,Person Re-Identification with a Locally Aware Transformer.txt,"Person Re-Identification is an important problem in computer vision-based
surveillance applications, in which the same person is attempted to be
identified from surveillance photographs in a variety of nearby zones. At
present, the majority of Person re-ID techniques are based on Convolutional
Neural Networks (CNNs), but Vision Transformers are beginning to displace pure
CNNs for a variety of object recognition tasks. The primary output of a vision
transformer is a global classification token, but vision transformers also
yield local tokens which contain additional information about local regions of
the image. Techniques to make use of these local tokens to improve
classification accuracy are an active area of research. We propose a novel
Locally Aware Transformer (LA-Transformer) that employs a Parts-based
Convolution Baseline (PCB)-inspired strategy for aggregating globally enhanced
local classification tokens into an ensemble of $\sqrt{N}$ classifiers, where
$N$ is the number of patches. An additional novelty is that we incorporate
blockwise fine-tuning which further improves re-ID accuracy. LA-Transformer
with blockwise fine-tuning achieves rank-1 accuracy of $98.27 \%$ with standard
deviation of $0.13$ on the Market-1501 and $98.7\%$ with standard deviation of
$0.2$ on the CUHK03 dataset respectively, outperforming all other
state-of-the-art published methods at the time of writing.","In recent years, Person ReIdentiÔ¨Åcation(reID) has gained a lot of attention due to its foundational role in computer vision based video surveillance applications. Person reID is predominantly considered as a feature embedding problem. Given a query image and a large set of gallery images, person reID generates the feature embedding of each image and then ranks the similarity between query and gallery image vectors. This can be used to reidentify the person in photographs obtained by nearby surveillance cameras. Recently, Vision Transformer (ViT) as introduced by Dosovitskiy et al. [2020] is gaining substantial traction for image recognition problems. While some methods for image classiÔ¨Åcation [Dosovitskiy et al., 2020, Touvron et al., 2020], and for image retrieval [ElNouby et al., 2021] are focused only on theclassiÔ¨Åcation token , some approaches utilize the fact that local tokens , which are also outputs of the transformer encoder, can be used to improve performance of many computer vision applications including image segmentation [Wu et al., 2020, Wang et al., 2021, Chen et al., 2021], object detection [Beal et al., 2020, Wang et al., 2021] and even person reID [He et al., 2021]. Nevertheless, at present, approaches to make use of local and global tokens are an active area of research. Preprint. Under review.arXiv:2106.03720v2  [cs.CV]  8 Jun 2021In the words of Beal et al. [2020], ""The remaining tokens in the sequence are used only as features for the Ô¨Ånal class token to attend to. However, these unused outputs correspond to the input patches , and in theory, could encode local information useful for performing object detection"" .Beal et al. [2020] observed that the local tokens , although theoretically inÔ¨Çuenced by global information, also have substantial correspondence to the original input patches. One might therefore consider the possibility of using these local tokens as an enhanced feature representation of the original image patches to more strongly couple vision transformer encoders to fully connected (FC) classiÔ¨Åcation techniques. This coupling of local patches with FC classiÔ¨Åcation techniques is the primary intuition behind the LATransformer architectural design. Partbased Convolutional Baseline (PCB) [Sun et al., 2018] is a strong convolutional baseline technique for person reID and has inspired many stateoftheart models [Yao et al., 2018, Guo et al., 2019, Zheng et al., 2019]. PCB partitions the feature vector received from the backbone network into six vertical regions and constructs an ensemble of regional classiÔ¨Åers with a voting strategy to determine the predicted class label. A limitation of PCB is that each regional classiÔ¨Åer ignores the global information which is also very important for recognition and identiÔ¨Åcation. Nevertheless, PCB has achieved much success despite this limitation, and as such the design of LATransformer uses a PCBlike strategy to combine globally enhanced local tokens . Our work also improves on the recent results of He et al. [2021], who was the Ô¨Årst to employ Vision Transformers to person reID and achieved results comparable to the current stateoftheart CNN based models. Our approach extends He et al. [2021] in several ways but primarily because we aggregate the globally enhanced local tokens using a PCBlike strategy that takes advantage of the spatial locality of these tokens. Although He et al. [2021] makes use of Ô¨Ånegrained local tokens , it does so with a ShufÔ¨ÇeNet [Zhang et al., 2017] like Jigsaw shufÔ¨Çing step which does not take advantage of the 2D spatial locality information inherent in the ordering of the local tokens. LATransformer overcomes this limitation by using a PCBlike strategy to combine the globally enhanced local tokens while Ô¨Årst preserving their ordering in correspondence with the image dimension. An additional novelty of our approach is the use of blockwise Ô¨Ånetuning which we Ô¨Ånd is able to further improve the classiÔ¨Åcation accuracy of LATransformer for person reID. Blockwise Ô¨Ånetuning is viable as a form of regularization when training models with a large number of parameters over relatively small indomain datasets. Howard and Ruder [2018] advocate for blockwise Ô¨Ånetuning orgradual unfreezing particularly when training language models due to a large number of fully connected layers. As vision transformers also have high connectivity, we Ô¨Ånd that this approach is able to further improve the classiÔ¨Åcation accuracy for LATransformer. This paper is organized as follows: Firstly, we discuss related work involving Transformer archi tectures and other related methodologies in person reID. Secondly, we describe the architecture of LATransformer, including the novel locally aware network and blockwise Ô¨Ånetuning techniques. Finally, we present quantitative results of the person reID including mAP and rank1 analysis on the market1501 and CUHK03 datasets. 2 Related Work "
141,BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification.txt,"Deep learning methods have shown outstanding classification accuracy in
medical imaging problems, which is largely attributed to the availability of
large-scale datasets manually annotated with clean labels. However, given the
high cost of such manual annotation, new medical imaging classification
problems may need to rely on machine-generated noisy labels extracted from
radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been
modelled from datasets with noisy labels, but their training procedure is in
general not robust to noisy-label samples, leading to sub-optimal models.
Furthermore, CXR datasets are mostly multi-label, so current noisy-label
learning methods designed for multi-class problems cannot be easily adapted. In
this paper, we propose a new method designed for the noisy multi-label CXR
learning, which detects and smoothly re-labels samples from the dataset, which
is then used to train common multi-label classifiers. The proposed method
optimises a bag of multi-label descriptors (BoMD) to promote their similarity
with the semantic descriptors produced by BERT models from the multi-label
image annotation. Our experiments on diverse noisy multi-label training sets
and clean testing sets show that our model has state-of-the-art accuracy and
robustness in many CXR multi-label classification benchmarks.","The promising results produced by deep neural networks (DNN) in medical image analysis (MIA) problems [30] is attributed to the availability of largescale datasets with ac curately annotated images. Given the high cost of acquir ing such datasets, the Ô¨Åeld is considering more affordable automatic annotation processes by Natural Language Pro cessing (NLP) approaches that extract multiple labels (each label representing a disease) from radiology reports [20,54]. *First two authors contributed equally to this work. Descriptor Subgraph Sample Graph  Multilabel  (BoMD) Smoothly RelabelMIDFeature extractorRobust Loss  Multiclass  (Noisyrobust)Feature extractorClassifier  Multiclass  (Noisycleaning) Pseudo Labelling Sample Graph  Multiclass  (Graph based) Feature extractorFeature extractor  Multiclass  (Transition matrix) ClassifierTrainsition MatrixClassifierFigure 1: Comparison of multiclass LNL methods [1, 21, 29, 33] and our noisy multilabel approach, BoMD, where the feature extractor returns a single descriptor vper im age,Dis the noisy training set, Cis the clean set, and ~Dis the relabelled training set. BoMD has two compo nents: 1) learning of a bag of multilabel image descriptors (MID)fv1;v2;v3gto represent the image, and 2) smooth relabelling of images driven by a graph structure based on the Ô¨Ånegrained relationships between MID descriptors. However, mistakes made by NLP combined with uncer tain radiological Ô¨Åndings can introduce label noise [42,43], as can be found in NLPannotated Chest Xray (CXR) datasets [20, 54] whose noisy multilabels can mislead su pervised training processes. Nevertheless, even without ad dressing this noisy multilabel problem, current CXR multi label classiÔ¨Åers [3,15,22,37,48,61] show promising results. Although these methods show encouraging multilabel clas siÔ¨Åcation accuracy, there is still potential for improvement that can be realised by properly addressing the noisy multi label learning problem present in CXR datasets [20, 54].arXiv:2203.01937v3  [eess.IV]  12 May 2023Current learning with noisy label (LNL) approaches fo cus on leveraging the hidden cleanlabel information to as sist the training of DNNs (see Fig. 1). This can be achieved with techniques that clean the label noise [26,29], robustify loss functions [21, 31, 33], estimate label transition matri ces [1,12,59,62], smooth training labels [36,56,63], and use graphs to explore the latent structure of data. [21, 57, 58]. These methods have been designed for noisy multiclass problems and do not easily extend to noisy multilabel learning, which is challenging given the potential multi ple label mistakes for each training sample. To the best of our knowledge, the stateoftheart (SOTA) approach that handles noisy multilabel learning is NVUM [31], which is based on an extension of early learning regularisation (ELR) [33]. NVUM shows promising results, but it is chal lenged by the different early convergence patterns of mul tiple labels, which can lead to poor performance for partic ular label noise conditions, as shown in our experiments. Additionally, NVUM is only evaluated on realworld CXR datasets [20, 54] without any systematic assessment of ro bustness to varying label noise conditions, preventing a more complete understanding of its functionality. In this paper, we propose a new solution speciÔ¨Åcally designed for the noisy multilabel problem by answering the following question: can the detection and correction of noisy multilabelled samples be facilitated by leverag ing the semantic information of training labels? available from language models [19, 28, 46]? This question is moti vated by the successful exploration of language models in computer vision [3, 7, 16, 64], with methods that leverage semantic information to inÔ¨Çuence the training of visual de scriptors; an idea that has not been explored in noisy multi label classiÔ¨Åcation. To answer this question, we introduce the 2stage Bag of Multilabel Descriptors (BoMD) method (see Fig. 1) to smoothly relabel noisy multilabel image datasets that can then be used for training common multi label classiÔ¨Åers. The Ô¨Årst stage trains a feature extractor to produce a bag of multilabel image descriptors by promot ing their similarity with the semantic embeddings from lan guage models. For the second stage, we introduce a novel graph structure, where each image is represented by a sub graph built from the multilabel image descriptors, learned in the Ô¨Årst stage, to smoothly relabel the noisy multilabel images. Compared with graphs built directly from a sin gle descriptor per image [21], our graph structure with the multilabel image descriptors has the potential to capture more Ô¨Ånegrained image relationships, which is crucial to deal with multilabel annotation. We also propose a new benchmark to systematically assess noisy multilabel meth ods. In summary, our contributions are: 1. A novel 2stage learning method to smoothly relabel noisy multilabel datasets of CXR images that can then be used for training a common multilabel classiÔ¨Åer;2. A new bag of multilabel image descriptors learning method that leverages the semantic information avail able from language models to represent multilabel im ages and to detect noisy samples; 3. A new graph structure to smoothly relabel noisy multilabel images, with each image being represented by a subgraph of the learned multilabel image descrip tors that can capture Ô¨Ånegrained image relationships; 4. The Ô¨Årst systematic evaluation of noisy multilabel methods that combine the PadChest [6] and Chest X ray 14 [54] datasets. We show the effectiveness of our BoMD on a bench mark that consists of training with two noisy multilabel CXR datasets and testing on three clean multilabel CXR datasets [31]. Results show that our approach has more accurate classiÔ¨Åcations than previous multilabel classiÔ¨Åers developed for CXR datasets and noisylabel classiÔ¨Åers. Re sults on our proposed benchmark show that BoMD is gen erally more accurate and robust than competing methods under our systematic evaluation. 2. Related Works "
142,Segmental Convolutional Neural Networks for Detection of Cardiac Abnormality With Noisy Heart Sound Recordings.txt,"Heart diseases constitute a global health burden, and the problem is
exacerbated by the error-prone nature of listening to and interpreting heart
sounds. This motivates the development of automated classification to screen
for abnormal heart sounds. Existing machine learning-based systems achieve
accurate classification of heart sound recordings but rely on expert features
that have not been thoroughly evaluated on noisy recordings. Here we propose a
segmental convolutional neural network architecture that achieves automatic
feature learning from noisy heart sound recordings. Our experiments show that
our best model, trained on noisy recording segments acquired with an existing
hidden semi-markov model-based approach, attains a classification accuracy of
87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6%
accuracy of the state-of-the-art statistical classifier trained and evaluated
on the same dataset. Our results indicate the potential of using neural
network-based methods to increase the accuracy of automated classification of
heart sound recordings for improved screening of heart diseases.","Heart diseases constitute a signicant global health burden. Just one subset of these dis eases, valvular heart disease (VHD) resulting from rheumatic fever, causes 300,000500,000 preventable deaths each year globally, primarily in developing countries.1,2Early detection of many heart diseases is crucial for optimal treatment management to prevent disease pro gression.3,4In developing countries, the standard practice for screening of heart diseases such as VHD and cardiac arrhythmia is cardiac auscultation to listen for abnormal heart sounds. Patients found to have suspicious abnormalities are then referred to specialists for proper diagnosis by a much more expensive echocardiographic procedure.3Although cardiac auscul tation has been replaced by echocardiography for screening in industrialized countries, the costeectiveness and procedural simplicity of auscultation make it an important screening tool for primary care providers and clinicians in underresourced communities.5,6 The main challenge in cardiac auscultation is the diculty of detecting and interpreting subtle acoustic features associated with heart sound abnormalities. Manual classication of heart sounds suers from high intraobserver variability,7{14causing false positive and false negative results. Much work has been done in trying to improve screening accuracy, including eorts to design devices to record heart sounds and automatically classify them. However, the biggest challenge for this task remains in developing an accurate classier for heart sound This work was nished in May 2016, and remains unpublished until December 2016 due to a request from the data provider.arXiv:1612.01943v1  [cs.SD]  6 Dec 20162 recordings, which are often obtained in noisy environments. Here, we propose a novel approach based on segmental convolutional neural networks to classication of heart sound recordings. Our approach achieves automatic feature learning together with accurate prediction of the abnormality. On noisy recordings, this approach outperforms prior classiers using a stateof theart feature set developed for noiseless recordings. The rest of this paper is organized as follows. In Section 2, we discuss related previous research. In Section 3, we introduce the methods that we used to classify noisy heart sound recordings, including preprocessing of data, the use of traditional classiers, and our segmental convolutional neural network models. Next, in Section 4, we present the performance of our classiers, along with our analysis of these results. We discuss the limitations of our work and future directions in Section 5 and conclude our work in Section 6. 2. Related Work "
143,Adaptive Sample Selection for Robust Learning under Label Noise.txt,"Deep Neural Networks (DNNs) have been shown to be susceptible to memorization
or overfitting in the presence of noisily-labelled data. For the problem of
robust learning under such noisy data, several algorithms have been proposed. A
prominent class of algorithms rely on sample selection strategies wherein,
essentially, a fraction of samples with loss values below a certain threshold
are selected for training. These algorithms are sensitive to such thresholds,
and it is difficult to fix or learn these thresholds. Often, these algorithms
also require information such as label noise rates which are typically
unavailable in practice. In this paper, we propose an adaptive sample selection
strategy that relies only on batch statistics of a given mini-batch to provide
robustness against label noise. The algorithm does not have any additional
hyperparameters for sample selection, does not need any information on noise
rates and does not need access to separate data with clean labels. We
empirically demonstrate the effectiveness of our algorithm on benchmark
datasets.","The deep learning models, which are highly effective in many applications, need vast amounts of training data. Such largescale labelled data is often generated through crowd sourcing or automated labeling, which naturally cause ran dom labelling errors. In addition, subjective biases in hu man annotators too can cause such errors. The training of deep networks is adversely affected by label noise and hence robust learning under label noise is an important problem of current interest. In recent years many different approaches for robust learning of classifiers have been proposed, such as, robust loss functions [9, 6, 53, 42], loss correction [35], meta learning [25, 43], sample reweighting [38, 39, 41, 16, 11], etc. In this paper we present a novel algorithm that adap tively selects samples based on the statistics of observed 1Codes for reproducibility will be made available here: https://github.com/dbp1994/masters_thesis_codes/ tree/main/BAREloss values in a minibatch and achieves good robustness to label noise. Our algorithm does not use any additional sys tem for learning weights for examples, does not need extra data with clean labels and does not assume any knowledge of noise rates. The algorithm is motivated by curriculum learning and can be thought of as a way to design an adap tive curriculum. The curriculum learning [4, 21] is a general strategy of sequencing of examples so that the networks learn from the ‚Äòeasy‚Äô examples well before learning from the ‚Äòhard‚Äô ones. This is often brought about by giving different weights to different examples in the training set. Many of the recent algorithms for robust learning based on sample reweighting can be seen as motivated by a similar idea. A good justifica tion for this approach comes from some recent studies [50] that have shown that deep networks can learn to achieve zero training error on completely randomly labelled data, a phenomenon termed as ‚Äòmemorization‚Äô. Further studies such as [3, 29] have shown that the networks, when trained on noisilylabelled data, learn simpler patterns first before overfitting to the noisilylabelled data. In the last few years, several strategies have been pro posed that aim to select (or give more weightage to) ‚Äòclean‚Äô samples for obtaining a degree of robustness against label noise (e.g., [16, 11, 49, 47, 27, 38, 13]). All such methods essentially employ the heuristic of ‚Äòsmall loss‚Äô for sample selection wherein (a fraction of) smallloss valued samples are preferentially used for learning the network. Many of these methods use an auxiliary network to assess the loss of an example or to learn to map loss values to sample weights. Such methods need additional computing resources to learn multiple networks and may also need separate clean data (without label noise) and the methods involve careful choice of additional hyperparameters. In general, it is difficult to directly relate the loss value of an example with the relia bility of its label. Loss value of any specific example is it self a function of the current state of learning and it evolves with epochs. Loss values of even clean samples may change over a significant range during the course of learning. Fur ther, the loss values achievable by a network even on cleanarXiv:2106.15292v3  [cs.LG]  5 Dec 2022samples may be different for examples of different classes. Motivated by these considerations, we propose a sim ple, adaptive selection strategy called BAtch REweighting (BARE ). Our algorithm utilizes the statistics of loss values in a minibatch to compute the threshold for sample selec tion in that minibatch. Since, it is possible that this auto matically calculated threshold is different for different mini batches even within the same epoch, our method amounts to using a dynamic threshold which naturally evolves as learning proceeds. In addition, while calculating the batch statistics we take into consideration the class labels also and hence the dynamic thresholds are also dependent on the given labels of the examples. The main contribution of this paper is an adaptive sam ple selection strategy for robust learning that is simple to implement, does not need any clean validation data, needs no knowledge at all of the noise rates and also does not have any hyperparameters in the sample selection strategy. It does not need any auxiliary network for sample selec tion. We empirically demonstrate the effectiveness of our algorithm on benchmark datasets: MNIST [22], CIFAR10 [19], and Clothing1M [46] and show that our algorithm is much more efficient in terms of time and has as good or better robustness compared to other algorithms for different types of label noise and noise rates. The rest of the paper is organized as follows: Section 2 discusses related work, Section 3 discusses our proposed al gorithm. Section 4 discusses our empirical results and con cluding remarks are provided in Section 5. 2. Related Work "
144,INN: A Method Identifying Clean-annotated Samples via Consistency Effect in Deep Neural Networks.txt,"In many classification problems, collecting massive clean-annotated data is
not easy, and thus a lot of researches have been done to handle data with noisy
labels. Most recent state-of-art solutions for noisy label problems are built
on the small-loss strategy which exploits the memorization effect. While it is
a powerful tool, the memorization effect has several drawbacks. The
performances are sensitive to the choice of a training epoch required for
utilizing the memorization effect. In addition, when the labels are heavily
contaminated or imbalanced, the memorization effect may not occur in which case
the methods based on the small-loss strategy fail to identify clean labeled
data. We introduce a new method called INN(Integration with the Nearest
Neighborhoods) to refine clean labeled data from training data with noisy
labels. The proposed method is based on a new discovery that a prediction
pattern at neighbor regions of clean labeled data is consistently different
from that of noisy labeled data regardless of training epochs. The INN method
requires more computation but is much stable and powerful than the small-loss
strategy. By carrying out various experiments, we demonstrate that the INN
method resolves the shortcomings in the memorization effect successfully and
thus is helpful to construct more accurate deep prediction models with training
data with noisy labels.","Learning deep neural networks (DNNs) has achieved impressive successes in many research Ô¨Åelds but has suffered from collecting massive cleanannotated training samples such as ImageNet [ 1] and MSCOCO [ 2]. Since annotating procedures are usually done manually by human experts, it is expensive and timeconsuming to get large clean labeled data, which prevents deep learning models from being trained successfully. On the other hand, it is feasible to access numerous data through internet search engines [ 3,4,5,6] or hashtags, whose labels are easy to collect but relatively inaccurate. Thus it becomes to get a spotlight to exploit data sets with corrupted labels instead of clean ones to solve classiÔ¨Åcation tasks with DNNs, which is called the noisy label problem . There have been many kinds of literature dealing with noisy labeled data, and a majority of methods exploited socalled the memorization effect , which is a special characteristic of DNNs that DNNs memorize data eventually (i.e. perfectly classify training data) but memorize clean labeled samples Preprint. Under review.arXiv:2106.15185v1  [cs.LG]  29 Jun 2021earlier and noisy samples later [ 7,8]. Hence, we can identify clean data from the given training data contaminated with noisy labels by choosing samples with small loss values. Due to its simplicity and superiority, many followup studies have been proposed based on the smallloss strategy and achieved great success ([9] and references therein). Figure 1: An illustration of the INN method. Circle and square are inputs with clean label and noisy label, respectively. Numbered dots are the nearest inputs. Each graph presents the value of a given prediction model along the dashed line. The INN method takes an average of the areas under each of the graphs.But the smallloss strategy has several weak nesses. First, during the training phase, it is difÔ¨Åcult to know a training epoch (or iteration) where the discrepancy of loss values between clean data and noisy data is large since it heav ily depends on various factors including data set, model architecture, optimizer type and even learning schedule. Second, it becomes hard to identify cleanannotated samples from training data via the smallloss strategy when the training labels are heavily polluted. Besides, the memo rization effect may not appear when we analyze the data with imbalanced label distribution. As we can obtain imbalanced data frequently in many realworld domains, this shortcoming can be an obstacle for the smallloss strategy applied in many industry Ô¨Åelds. To tackle these issues about the memoriza tion effect, we develop a novel and powerful method called INN (Integration with the Nearest Neighbors). We start with a new and interesting observation that the output values of a trained DNN atneighbor regions of labeled and noisy samples are consistently much different regardless of training epochs. We call this phenomenon the consistency effect . Motivated by the consistency effect, the INN method takes averages of the output values of neighbor regions of a given sample and decides it as noisy if the average is small. See Figure 1 for an illustration of the INN method. In fact, the INN requires more computation than the smallloss method. Still, this additional expense deserves to pay since the INN successfully overcomes the smallloss method‚Äôs limitations. The INN works well even when the training labels are heavily contaminated or has imbalanced distribution, while the smallloss method is in trouble for the situations. The stability and superiority make the INN easily applicable to various supervised learning tasks without much effort. We can also combine the INN with an existing noisylabelproblemsolving learning method based on the smallloss strategy (e.g. DivideMix [ 10]) to construct deep networks of high accuracy. We replace the parts where the memorization effect and loss information are used with the consistency effect and the INN information. We show that these modiÔ¨Åcations enhance prediction performances much, especially when training labels have many noises or imbalanced distribution. This paper is organized as follows. In Section 2, we provide brief reviews for related studies dealing with noisy labels, and detailed descriptions of the INN are given in Section 3. Various experimental analyses including performance test and ablation study are given in Section 4 and Ô¨Ånal concluding remarks follow in Section 5. The key contributions of this work are as follows. ‚Ä¢We Ô¨Ånd a new observation called the consistency effect, that the output values of a trained DNN at neighbor regions of labeled and noisy samples are consistently much different regardless of training epochs. ‚Ä¢Built on the consistency effect, we propose a method called the INN to identify clean annotated data from a given training data. ‚Ä¢We empirically demonstrate that the INN can separate clean and noisy samples accurately and stably even under the heavy label corruption and imbalanced label distribution, and also helpful to construct superior prediction models. 22 Related works "
145,Reliability-Adaptive Consistency Regularization for Weakly-Supervised Point Cloud Segmentation.txt,"Weakly-supervised point cloud segmentation with extremely limited labels is
highly desirable to alleviate the expensive costs of collecting densely
annotated 3D points. This paper explores to apply the consistency
regularization that is commonly used in weakly-supervised learning, for its
point cloud counterpart with multiple data-specific augmentations, which has
not been well studied. We observe that the straightforward way of applying
consistency constraints to weakly-supervised point cloud segmentation has two
major limitations: noisy pseudo labels due to the conventional confidence-based
selection and insufficient consistency constraints due to discarding unreliable
pseudo labels. Therefore, we propose a novel Reliability-Adaptive Consistency
Network (RAC-Net) to use both prediction confidence and model uncertainty to
measure the reliability of pseudo labels and apply consistency training on all
unlabeled points while with different consistency constraints for different
points based on the reliability of corresponding pseudo labels. Experimental
results on the S3DIS and ScanNet-v2 benchmark datasets show that our model
achieves superior performance in weakly-supervised point cloud segmentation.
The code will be released.","Recently, 3D point cloud segmentation has achieved im pressive progresses [8, 24, 25, 29]. However, it is still ex tremely expensive and laborconsuming to collect abundant pointlevel annotations for the model training. Therefore, in order to alleviate huge labeling costs, it is highly de sirable to develop weaklysupervised point cloud segmen tation, which aims to train a satisÔ¨Åed segmentation model *Corresponding author: G. Lin (email: gslin@ntu.edu.sg ) (c) Selected PseudoLabels(b) TrueLabels (d) Discarded PseudoLabels(a) Point CloudsFigure 1. Illustrations of imperfect pseudo labels in the conven tional conÔ¨Ådencebased selection. We use a probability threshold of 0.7 to select highlyconÔ¨Ådent pseudo labels for the model train ing, while they are very noisy (b vs. c) and many discarded pseudo labels (d) are not exploited during training. with scarce labeled points but enormous unlabeled points. To exploit the unlabeled points, existing methods are mainly based on the consistency assumption [1, 37, 43, 44], where the model is encouraged to be consistent under various perturbations, to achieve the local distributional smoothness (LDS). For example, [28] utilizes the predic tions of weakly augmented data to guide the learning of strongly augmented versions, where they select reliable pre dictions as pseudo labels based on the prediction conÔ¨Å dence and use them to enforce the consistency constraints to regularize the model training. Such consistencybased regularization has not been well investigated for weakly supervised point cloud segmentation. For instance, the re cent 1T1C [17] model also leverages the conÔ¨Ådence scores to select reliable predictions as pseudo labels and uses themarXiv:2303.05164v1  [cs.CV]  9 Mar 2023to train the model iteratively, which however is not a con sistency constraint under diversiÔ¨Åed perturbations. This motivates us to study the intuitive idea of applying consistency constraints to improve weaklysupervised point cloud segmentation. The straightforward way is to directly extend the FixMatch [28] from images to point clouds, i.e., selecting conÔ¨Ådent predictions of the weakly augmented point clouds as pseudo labels and applying consistency con straints to guide the predictions of strongly augmented ones. However, such a scheme has two major limitations. First, it is unsatisÔ¨Åed to select reliable predictions based on their conÔ¨Ådences. The examples in Fig. 1 (b, c) illustrate that the scheme may generate highly conÔ¨Ådent but incorrect pseudo labels, which would lead to more noisy supervisions and confuse the model training. Second, for the large amounts of unlabeled points that are deemed unreliable (see Fig. 1 (d)), they are being discarded and not utilized during train ing [17, 28], resulting in a suboptimal performance. We would like to point out these limitations are particu larly noticeable for weaklysupervised point cloud segmen tation, while they might not be so signiÔ¨Åcant in the corre sponding image counterpart . This is because for weakly supervised point cloud segmentation, the human annota tions are much more scarce, e.g. the typical one thing one click (OTOC) setting [17], and the generated point cloud pseudo labels are much more noisy (due to the sparsity of point clouds and lack of neighbor support). Thus, the key questions for weaklysupervised point cloud segmentation are:how to select reliable pseudo labels and how to utilize a large number of unreliable pseudo labels? Our key idea in this work is to select more reli able pseudo labels by considering both prediction conÔ¨Å dence and model uncertainty, and utilize reliable predic tions as hard pseudo labels while using ambiguous predic tions as soft pseudo labels instead of throwing them away . SpeciÔ¨Åcally, we propose a simple yet effective Reliability Adaptive Consistency Network (RACNet), which enforces the consistency constraints on all unlabeled data adaptively based on their pseudo label reliability. To measure the re liability, we jointly use the prediction conÔ¨Ådence and un certainty to divide the initial predictions of unlabeled data into ambiguous and reliable sets, where the uncertainty is measured by computing the statistical variances among the predictions of different augmentations. Considering the am biguous predictions are unreliable, we treat them as soft pseudo labels and apply a consistency loss (KL Divergence) to encourage invariant results of augmented point clouds. Considering the reliable predictions are accurate, we con vert them into onehot pseudo labels and then apply a con sistency loss (Crossentropy Loss) to guide the learning of different augmented data. In addition, to further exploit the reliable set, we also generate mixaugmented point clouds by a pointwise interpolation among multiple offtheshelfbaseaugmentations and then use the onehot pseudo labels to facilitate the model training. We follow the public models [17, 37] to conduct experi ments on two largescale point cloud segmentation datasets: S3DIS [4] and ScanNetv2 [9]. Extensive experiments demonstrate that our RACNet is able to accurately select pseudo labels during the model training and achieves supe rior segmentation performance than other existing methods for weaklysupervised point cloud segmentation, e.g., out performing the DAT model [37] by 1.9% under the OTOC setting on the S3DIS dataset . Besides, our experimental re sults reveal that combining the local shape deformation like PointWolf [14] and the conventional augmentation ( e.g., AfÔ¨Åne Transformations) is able to achieve impressive per formance gains for weaklysupervised point cloud segmen tation. Overall, the contributions in this paper are threefold: ‚Ä¢ We consider the problem of applying consistency based regularization for weaklysupervised point cloud segmentation and identify the two main unique obsta cles: measuring pseudolabel reliability and utilizing unreliable pseudo labels. ‚Ä¢ We propose a novel RACNet to address the two is sues by incorporating uncertainty, which is computed as the discrepancy among different augmentations, to identify more reliable pseudolabels and adaptively applying different consistency constraints to different points based on their reliability. Moreover, we design a mixaugmentation module to generate mixaugmented point clouds to further exploit the reliable set. ‚Ä¢ Our RACNet achieves new stateoftheart perfor mance in weaklysupervised point cloud segmenta tion on S3DIS and ScanNetv2 datasets, with multiple baseaugmentations and mixaugmentations for adap tive consistency training. 2. Related Works "
146,Improving Neural Architecture Search Image Classifiers via Ensemble Learning.txt,"Finding the best neural network architecture requires significant time,
resources, and human expertise. These challenges are partially addressed by
neural architecture search (NAS) which is able to find the best convolutional
layer or cell that is then used as a building block for the network. However,
once a good building block is found, manual design is still required to
assemble the final architecture as a combination of multiple blocks under a
predefined parameter budget constraint. A common solution is to stack these
blocks into a single tower and adjust the width and depth to fill the parameter
budget. However, these single tower architectures may not be optimal. Instead,
in this paper we present the AdaNAS algorithm, that uses ensemble techniques to
compose a neural network as an ensemble of smaller networks automatically.
Additionally, we introduce a novel technique based on knowledge distillation to
iteratively train the smaller networks using the previous ensemble as a
teacher. Our experiments demonstrate that ensembles of networks improve
accuracy upon a single neural network while keeping the same number of
parameters. Our models achieve comparable results with the state-of-the-art on
CIFAR-10 and sets a new state-of-the-art on CIFAR-100.","Designing neural network (NN) architectures is often a demanding process. It often requires signiÔ¨Åcant time, re sources, and human expertise. These challenges are partially addressed by neural architecture search (NAS), which is able to Ô¨Ånd the best convolutional layer or cell that is then used as a building block for the network (Real et al., 2018; Zoph *Equal contribution1Google Research, New York, NY , USA 2Work done as a member of the Google AI Residency pro gram (g.co/brainresidency). Correspondence to: Vladimir Macko <vlejd@google.com >, Charles Weill <weill@google.com >.et al., 2017). However, once a good building block is found, it is still required to manually design the Ô¨Ånal architecture as a combination of multiple blocks. Moreover, there is usually a need to design multiple architectures with differ ent parameter budgets, as different applications might pose different hardware constraints on memory and computation. The critical question is how to upscale a small building block into a large architecture? A common solution is to stack those blocks into a single tower and adjust the width and depth to Ô¨Åll the parameter budget. The solution of having one tower is common also for architectures that are not the result of neural architecture search (Springenberg et al., 2014; Szegedy et al., 2015; He et al., 2016). Recently, it was proposed to construct the network as an ensemble of smaller networks trained in a special way (Dutt et al., 2018). Ensembles of neural networks are known to be much more robust and accurate than individual subnetworks. Ensembles perform well on a wide variety of tasks (Caruana et al., 2004) and are frequently used in the winning solu tions of machine learning competitions (e.g. Kaggle) often consist of ensembles of multiple models. Unlike a single large neural network, an ensemble‚Äôs size is not bounded by training, since each of the component subnetworks can be trained independently, and their outputs computed in paral lel. Ultimately the Ô¨Ånal ensemble‚Äôs size is bounded by its ability to Ô¨Åt on a serving hardware, and latency constraints. The main questions we tackle in this paper are the following: Can ensembles perform better than a single tower model with the same number of parameters? Can we beneÔ¨Åt from sequentially training the component subnetworks, and lever age information acquired from previously trained networks to improve the Ô¨Ånal ensemble performance? Is it possible to construct ensemble architectures automatically or with minimal human expertise? In this work, we present a new paradigm to automatically generate ensembles of subnetworks that achieve high accu racy given a Ô¨Åxed parameter budget. Our AdaNAS algo rithm works in an iterative manner, and it increases the size of each new subnetwork at each iteration until the ensemble hits the budget limit. As we iteratively learn the composition of the ensemble, wearXiv:1903.06236v1  [cs.LG]  14 Mar 2019Improving Neural Architecture Search Image ClassiÔ¨Åers via Ensemble Learning leverage information learned from subnetworks trained in previous iterations. We explore the effects of using ideas from Born Again Networks (BAN) (Furlanello et al., 2018) to the Ô¨Ånal ensemble performance. In addition, we introduce a novel technique called Adaptive Knowledge Distillation (AKD) that extends Born Again Networks to use the previ ous iteration‚Äôs ensemble as a teacher to assist in training the current iteration‚Äôs subnetworks. Resulting models are comprised of multiple separate tow ers that can be easily parallelized at inference time. Our presented technique requires minimal hyperparameter tun ing to achieve these results. Our experiments demonstrate that ensembles of subnetworks improve accuracy upon a single neural network with the same number of parameters. OnCIFAR10 our algorithm achieves error 2:26and on CIFAR100 it achieves error 14:58. To our knowledge, and as we will show in Section 5, our technique achieves a new stateoftheart on CIFAR100 compared to methods that do not use additional regularization or data augmenta tion (e.g., ShakeDrop (Yamada et al., 2018) or AutoAug ment (Cubuk et al., 2018)). This paper has been implemented as an extension of a frame work for the construction and search of boosted ensembles, AdaNet (Cortes et al., 2016; Weill et al., 2018). The code to reproduce our results is available in the AdaNet project repository1. Our implementation uses opensourced code provided by (Zoph et al., 2017) in the TensorFlow Models repository2. This paper is organized as follows. We review previous work in Section 2. In Section 3 we describe the ensembling algorithm. Experiment settings are outlined in Section 4 and Section 5 shows the Ô¨Ånal results. Finally, Section 6 discusses our proposed technique and our Ô¨Åndings. 2. Related work "
147,Deep Neural Network Ensembles.txt,"Current deep neural networks suffer from two problems; first, they are hard
to interpret, and second, they suffer from overfitting. There have been many
attempts to define interpretability in neural networks, but they typically lack
causality or generality. A myriad of regularization techniques have been
developed to prevent overfitting, and this has driven deep learning to become
the hot topic it is today; however, while most regularization techniques are
justified empirically and even intuitively, there is not much underlying
theory. This paper argues that to extract the features used in neural networks
to make decisions, it's important to look at the paths between clusters
existing in the hidden spaces of neural networks. These features are of
particular interest because they reflect the true decision making process of
the neural network. This analysis is then furthered to present an ensemble
algorithm for arbitrary neural networks which has guarantees for test accuracy.
Finally, a discussion detailing the aforementioned guarantees is introduced and
the implications to neural networks, including an intuitive explanation for all
current regularization methods, are presented. The ensemble algorithm has
generated state-of-the-art results for Wide-ResNets on CIFAR-10 (top 5 for all
models) and has improved test accuracy for all models it has been applied to.","Consider a simple feed forward neural network. Dene a hidden space corre sponding to a hidden layer of a neural network as the space containing the outputs of the hidden nodes at that hidden layer for some input. All hidden spaces are composed of perceptrons with respect to the previous layer, each of which has a hyperplane decision boundary. Points in the previous space are mapped to a constant function of their distance to this plane, and depending on the activation function, compressed or stretched. This stretching and compress ing naturally leads to clustering in hidden spaces. The process is repeated for each perceptron comprising the hidden space, where adding a perceptron adds a dimension to the hidden space by projecting the points into a new dimension depending on their distances from the hyperplane of the new perceptron and the activation function. Dene a feature to be a measurable characteristic which a neural network uses to make its classication decision. Unfortunately, these clusters are not features in and of themselves but rather mixtures of features.arXiv:1904.05488v2  [cs.LG]  13 Aug 20192 Sean Tao To extract individual features, then, cluster paths should be examined, where a path is dened per individual input point as the sequence of clusters in the neural network the point belongs to, starting from cluster it belongs to in the input space, then the clusters it belongs to in each hidden space, and nally the cluster it belongs to in the output space. Intuitively, paths represent features because each path denes a region of the input space which will eventually end up at the same cluster in the output space via the same logic pathway used by the neural network. Thus, it immediately follows that a point on a path must be classied similarly as other points on that path, assuming all points in the output space are classied by cluster. This is another way of stating that points on the same path are indistinguishable from each other to the neural network, and since points in dierent paths were dierentiated in some layer, paths separate out features of neural networks. This process is formalized in the algorithm described below. However, besides merely nding the features in a neural network, the paths serve an even more important purpose{they separate the input into regions of condence with respect to the output classication. In particular, paths represent specic features, some of which were found because they are truly useful and others due to overtting. The process of determining \good"" and \bad"" data points, formally dened below, attempts to separate data into these two categories. Informally, \good"" data come from paths that contain many points that are classied correctly, since these are likely not due to random chance and thus are real features. An ensemble algorithm can then be created to combine dierent models, where models only vote on their \good"" data points. This is equivalent to querying neural networks for points where they are condent in their predictions. Points where the model is unsure can then be classied by other models. 2 Related Work "
148,Graph convolutional networks for learning with few clean and many noisy labels.txt,"In this work we consider the problem of learning a classifier from noisy
labels when a few clean labeled examples are given. The structure of clean and
noisy data is modeled by a graph per class and Graph Convolutional Networks
(GCN) are used to predict class relevance of noisy examples. For each class,
the GCN is treated as a binary classifier, which learns to discriminate clean
from noisy examples using a weighted binary cross-entropy loss function. The
GCN-inferred ""clean"" probability is then exploited as a relevance measure. Each
noisy example is weighted by its relevance when learning a classifier for the
end task. We evaluate our method on an extended version of a few-shot learning
problem, where the few clean examples of novel classes are supplemented with
additional noisy data. Experimental results show that our GCN-based cleaning
process significantly improves the classification accuracy over not cleaning
the noisy data, as well as standard few-shot classification where only few
clean examples are used.","Stateoftheart deep learning methods require a large amount of manually la beled data. The need for supervision may be reduced by decoupling represen tation learning from the end task and/or using additional training data that is unlabeled, weakly labeled (with noisy labels), or belong to dierent domains or classes. Example approaches are transfer learning [39], unsupervised representa tion learning [39], semisupervised learning [42], learning from noisy labels [16] and fewshot learning [33]. However, for several classes, only very few or even no clean labeled exam ples might be available at the representation learning stage. Fewshot learning severely limits the number of labeled samples on the end task, while the repre sentation is learned on a large training set of dierent classes [12,33,38]. Nev ertheless, in many situations, more data with noisy labels can be acquired or is readily available for the end task. One interesting mix of fewshot learning with additional largescale data is the work of Douze et al. [5], where labels are propagated from few clean labeled examples to a largescale collection. This collection is unlabeled and actuallyarXiv:1910.00324v3  [cs.CV]  24 Aug 20202 A. Iscen et al. 2. Minimize  Lossclean + Œª Lossnoisy  classify as  negatives classify as  positives  1. Adjacency graph  per class  3. Relevance score output  1.00 1.00 0.01 0.92 0.01  Labeled example  Additional data Class relevance prediction  with GCN  Use for  classiÔ¨Åer  training  0.97 0.97 0.90  0.05  0.80 Query by admiral  Fig. 1. Overview of our cleaning approach for 1shot learning with noisy examples. We use the class name admiral to crawl noisy images from the web and create an adjacency graph based on visual similarity. We then assign a relevance score to each noisy example with a graph convolutional network (GCN). Relevance scores are displayed next to the images. contains data for many more classes than the end task. Their method overall improves the classication accuracy, but at an additional computational cost. It is a transductive method, i.e., instead of learning a parametric classier, the largescale collection is still necessary at inference. In this work, we learn a classier from a few clean labeled examples and additional weakly labeled data, while the representation is learned on dier ent classes, similarly to fewshot learning. We assume that the class names are known, and we use them to search an existing large collection of images with textual description. The result is a set of images with novel class labels, but potentially incorrect (noisy). As shown in Figure 1, we clean this data using agraph convolutional network (GCN) [17], which learns to predict a class rele vance score per image based on connections to clean images in the graph. Both the clean and the noisy images are then used to learn a classier, where the noisy examples weighted by relevance. Unlike most existing work, our method operates independently per class and applies when clean labeled examples are few or even only one per class. We make the following contributions: 1. We learn a classier on a largescale weaklylabeled collection jointly with only a few clean labeled examples. 2. To our knowledge, we are the rst to use a GCN to clean noisy data: we cast a GCN as a binary classier which learns to discriminate clean from noisy data, and we use its inferred \clean"" probabilities as a relevance score per example. 3. We apply our method to two fewshot learning benchmarks and show sig nicant improvement in accuracy, outperforming the method by Douze et al. [5] using the same largescale collection of data without labels.Title Suppressed Due to Excessive Length 3 2 Related work "
149,Unsupervised Neural Aspect Search with Related Terms Extraction.txt,"The tasks of aspect identification and term extraction remain challenging in
natural language processing. While supervised methods achieve high scores, it
is hard to use them in real-world applications due to the lack of labelled
datasets. Unsupervised approaches outperform these methods on several tasks,
but it is still a challenge to extract both an aspect and a corresponding term,
particularly in the multi-aspect setting. In this work, we present a novel
unsupervised neural network with convolutional multi-attention mechanism, that
allows extracting pairs (aspect, term) simultaneously, and demonstrate the
effectiveness on the real-world dataset. We apply a special loss aimed to
improve the quality of multi-aspect extraction. The experimental study
demonstrates, what with this loss we increase the precision not only on this
joint setting but also on aspect prediction only.","Unsupervised aspect extraction is an essential part of natu  ral language processing and usually solved using topic mod elling approaches, which have proven themselves in this tas k. In general, aspect extraction aims to identify the category or multiple categories of a given text. The aspect can be a globa l context of the sentence or a speciÔ¨Åc term in this sentence; a term, in turn, can be either a single word or a collocation. Pr e vious unsupervised approaches achieved signiÔ¨Åcant improv e ment in the task of aspect extraction. The joint task of the aspect and the aspect term pairs extraction is still a challe nge for natural language processing. For example: in the senten ce ‚ÄùBest Pastrami I ever had and great portion without being ridiculous‚Äù the aspect and aspect term pairs ‚ÄùFood Quality: Pastrami‚Äù and ‚ÄùFood Style option: portion‚Äù. Most of the existing approaches apply twostage extrac tion: aspect extraction Ô¨Årst and then aspect term extractio n based on the known aspect. We propose a conjoint solu tion based on the convolutional multiattention mechanism (CMAM). The CMAM was inspired by Inceptionblock in computer vision, where kernels of different sizes allow inc or porating features from different levels of localisation. T hesentence representations built with CMAM capture the fea tures, which are used for aspect predictions, while the at tention detects related terms. Also, the convolutional att en tion does not require much additional time to infer the re sult, which is vital for the realworld application. In orde r to increase the quality of multiaspect extraction, we prop ose a novel loss function  tripletlike aspect spreading (TLAS ), which maximises the distance between topN aspectbased sentence representations and minimises the distance betwe en these representations and corresponding aspect vectors. T his approach allows achieving close to the stateoftheart re sults in aspect extraction with the ability to extract their terms . In summary, the contributions of this paper are: ‚Ä¢CMAM; convolutional multiattention mechanism, which is aimed to build sentence vector representation and to extract aspect terms. ‚Ä¢TLAS  loss function for aspect probabilities distribution modifying. ‚Ä¢The experimental study of the proposed model on SemEval2016 Restaurant dataset and Citysearch cor pus. 2 Related Work "
150,Robustness of convolutional neural networks to physiological ECG noise.txt,"The electrocardiogram (ECG) is one of the most widespread diagnostic tools in
healthcare and supports the diagnosis of cardiovascular disorders. Deep
learning methods are a successful and popular technique to detect indications
of disorders from an ECG signal. However, there are open questions around the
robustness of these methods to various factors, including physiological ECG
noise. In this study we generate clean and noisy versions of an ECG dataset
before applying Symmetric Projection Attractor Reconstruction (SPAR) and
scalogram image transformations. A pretrained convolutional neural network is
trained using transfer learning to classify these image transforms. For the
clean ECG dataset, F1 scores for SPAR attractor and scalogram transforms were
0.70 and 0.79, respectively, and the scores decreased by less than 0.05 for the
noisy ECG datasets. Notably, when the network trained on clean data was used to
classify the noisy datasets, performance decreases of up to 0.18 in F1 scores
were seen. However, when the network trained on the noisy data was used to
classify the clean dataset, the performance decrease was less than 0.05. We
conclude that physiological ECG noise impacts classification using deep
learning methods and careful consideration should be given to the inclusion of
noisy ECG signals in the training data when developing supervised networks for
ECG classification.","(a) Deep learning and physiological ECG signal noise Electrocardiogram (ECG) signals have long been used to support the diagnosis of cardiovascular disorders. Deep learning methods show encouraging results in ECG classiÔ¨Åcation tasks and have recently seen a rapid increase in popularity [1]. Noise and interference on the ECG signal are established causes of error in ECG diagnosis and interpretation [2] and have been noted to affect both manual (clinician) and automated (machine learning) detection of ECG abnormalities [3]. A desirable property of a deep network is that the performance of the network is robust to perturbations in the input data. Network robustness to ECG noise of deep learning methods used to detect cardiovascular disorders is not well understood and there have been no studies directly addressing the issue. Sources of noise that degrade the quality of a dataset include both label noise (in terms of mislabelled data) and ECG signal noise (in terms of physiological noise on the signal). Here we focus on the impact of ECG signal noise, to gain an understanding of how physiological ECG noise impacts the robustness of deep learning methods. (b) Transfer learning with deep networks While custom network architectures can be developed and trained from scratch to classify ECG signals [1], transfer learning is a popular method for utilising pretrained deep networks with new data. Transfer learning refers to the retraining of a pretrained network, for example a network pretrained using ImageNet [4] data can be retrained using ECG data to classify ECG data. This training method is useful when there is a lack of data, computational resources or time, or to prototype models and carry out exploratory analysis. ECG datasets often contain fewer than the large number of samples required to train a deep network from scratch, and in this case transfer learning is an attractive option. Furthermore, many well known network architectures have a demonstrated record of high performance. The focus of this study is to evaluate the robustness of deep learning to physiological ECG noise, rather than to optimise ECG classiÔ¨Åcation performance of a custom architecture. Using an established pretrained network provides a solid foundation for this focus. Convolutional neural networks (CNNs) are a class of deep network that is widely used for image classiÔ¨Åcation and, alongside recurrent neural networks (RNNs), are commonly used for ECG classiÔ¨Åcation [1]. Both 1D CNNs applied to the raw ECG signal and 2D CNNs applied to ECG image transforms have been used to detect cardiovascular disorders from the ECG signal. (c) Detecting cardiovascular disorders from the ECG signal Extensive work has been carried out to develop methods, including deep networks, that extract information from an ECG signal to support clinical decision making [1,5]. ECG image transformations are methods that convert a 1D ECG signal to a 2D image which can then be passed to a 2D CNN for training and classiÔ¨Åcation. The use of ECG image transforms allows both the use of 2D CNNs pretrained on the popular image dataset ImageNet [6], and the exploration of the impact of these image transforms on network robustness to noise. ECG image transforms capture frequency domain or morphology information that describe the underlying signal. Existing ECG image transforms and their applications include: the continuous wavelet transform (scalogram) for biometrics [7], gray level cooccurence matrix for morphological arrhythmia detection [8], recurrence plot to classify arrhythmias [9], distance distribution matrix to identify congestive heart failure [10] and the Symmetric Projection Attractor Reconstruction (SPAR) method for genetic mutation detection [11].3rsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Networks trained to classify ECG image transforms are less common than networks trained to classify the ECG signal directly and their utility for pathology classiÔ¨Åcation is still being explored. In particular, the impact of using ECG image transforms on network robustness is unclear. (d) Objectives The main objectives of this study are to: (i) Study the impact of the inclusion of physiological ECG noise in the input data on classiÔ¨Åcation performance of a CNN (the robustness); (ii) Assess the impact of the inclusion of physiological noise in the training data on the robustness of a CNN; (iii) Determine whether different ECG image transforms increase or decrease robustness to different noise types. 2. Methods "
151,Orientation-aware Vehicle Re-identification with Semantics-guided Part Attention Network.txt,"Vehicle re-identification (re-ID) focuses on matching images of the same
vehicle across different cameras. It is fundamentally challenging because
differences between vehicles are sometimes subtle. While several studies
incorporate spatial-attention mechanisms to help vehicle re-ID, they often
require expensive keypoint labels or suffer from noisy attention mask if not
trained with expensive labels. In this work, we propose a dedicated
Semantics-guided Part Attention Network (SPAN) to robustly predict part
attention masks for different views of vehicles given only image-level semantic
labels during training. With the help of part attention masks, we can extract
discriminative features in each part separately. Then we introduce
Co-occurrence Part-attentive Distance Metric (CPDM) which places greater
emphasis on co-occurrence vehicle parts when evaluating the feature distance of
two images. Extensive experiments validate the effectiveness of the proposed
method and show that our framework outperforms the state-of-the-art approaches.","Vehicle reidentiÔ¨Åcation (reID) aims to match vehicle images in a camera net work. Recently, this task has drawn increasing attention due to practical appli cations such as urban surveillance and traÔ¨Éc Ô¨Çow analysis. While deep Convo lutional Neural Networks (CNN) have shown remarkable performance in vehicle reID over the years [22,23,33], various challenges still hinder the performance of vehicle reID. One of them is that a vehicle captured from diÔ¨Äerent viewpoints usually has dramatically diÔ¨Äerent visual appearances. On the other hand, two diÔ¨Äerent vehicles of the same color and car model are likely to have very sim ilar appearances. As illustrated in the left part of Fig. 1, it is challenging to distinguish vehicles by comparing the features extracted from the whole vehicle images. In such case, the minor diÔ¨Äerences in speciÔ¨Åc parts of vehicle such as decorations or license plates would be a great beneÔ¨Åt to identifying two vehicles. Furthermore, when two vehicles are presented in diÔ¨Äerent orientations, a desired vehicle reID algorithm should be able to focus on the parts (views) that botharXiv:2008.11423v2  [cs.CV]  12 Oct 20202 TsaiShien Chen, ChihTing Liu, ChihWei Wu, and ShaoYi Chien Globalbasedfeature spaceSideSidebasedfeature spaceNeg. Pair Pos. PairSideSemanticsguided Part Attention Network (SPAN) Fig. 1: Concept illustration of Semanticsguided Part Attention Net work. The example images show intraclass diÔ¨Äerence and interclass similarity in the vehicle reID problem. It is challenging to separate the negative images merely based on global feature due to the similar car model and viewpoint. In this example, it is easier to distinguish two vehicles by the sidebased feature. This motivates us to generate the part (view) attention maps and then emphasize the feature of the cooccurrence vehicle parts for better reID matching. appear in the two vehicle images. For example, in the right part of Fig. 1, it is easier to distinguish the vehicles by comparing their side views. To reach this idea, we divide it into two steps. The Ô¨Årst step is to extract the feature from speciÔ¨Åc parts of vehicle im ages. A number of work has been proposed to achieve this purpose by learning orientationaware features. Nonetheless, existing methods either rely on expen sive vehicle keypoints as guidance to learn an attention mechanism for each part of a vehicle [34,11] or use only viewpoint labels but produce noisy and unsteady attention outcome which will thus hinder the network to learn subtle diÔ¨Äerences between vehicles [42]. In this paper, we introduce the Semanticsguided Part At tention Network (SPAN) to generate attention masks for diÔ¨Äerent parts (front, side and rear views) of a vehicle. As shown in Fig. 1, our SPAN learns to produce meaningful attention masks. The masks not only help disentangle features of dif ferent viewpoints but also improve the interpretability of our learning framework. It is also worth noting that, instead of expensive keypoints or pixellevel labels for training, our SPAN requires only imagelevel viewpoint labels which are much easier to be derived from known camera pose and traÔ¨Éc direction. For the second step, we design a Cooccurrence Partattentive Distance Met ric (CPDM) to better utilize the part features when measuring the distance of images. The intuition of this metric is that the network should focus on the parts (views) that both appear in the compared vehicle images. Therefore, the proposed metric allows us to automatically adjust the importance of each part feature distance according to the part visibility in two compared vehicle images. We conduct experiments on two largescale vehicle reID benchmarks and demonstrate that our method outperforms current stateofthearts. AblationOrientationaware Vehicle ReID with Semanticsguided Part Attention Net 3 studies prove that the attention masks generated by SPAN extract helpful part features and our CPDM can better utilize the global and part features to im prove the reID performance. Moreover, qualitative results show that our SPAN can robustly generate meaningful attention maps on vehicles of diÔ¨Äerent types, colors, and orientations. We now highlight our contributions: (1) We propose a Semanticsguided Part Attention Network (SPAN) to generate robust part at tention masks which can be used to extract more discriminative features. (2) Our SPAN only needs imagelevel viewpoint labels instead of expensive keypoints or pixellevel annotations for training. (3) We introduce the Cooccurrence Part attentive Distance Metric (CPDM) to facilitate vehicle reID by focusing on the parts that jointly appear in the compared images. (4) Extensive experiments on public datasets validate the eÔ¨Äectiveness of each component and demonstrate that our method performs favorably against stateoftheart approaches. 2 Related Work "
152,ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification.txt,"We present ENHANCE, an open dataset with multiple annotations to complement
the existing ISIC and PH2 skin lesion classification datasets. This dataset
contains annotations of visual ABC (asymmetry, border, colour) features from
non-expert annotation sources: undergraduate students, crowd workers from
Amazon MTurk and classic image processing algorithms. In this paper we first
analyse the correlations between the annotations and the diagnostic label of
the lesion, as well as study the agreement between different annotation
sources. Overall we find weak correlations of non-expert annotations with the
diagnostic label, and low agreement between different annotation sources. We
then study multi-task learning (MTL) with the annotations as additional labels,
and show that non-expert annotations can improve (ensembles of)
state-of-the-art convolutional neural networks via MTL. We hope that our
dataset can be used in further research into multiple annotations and/or MTL.
All data and models are available on Github:
https://github.com/raumannsr/ENHANCE.","Machine learning oÔ¨Äers many opportunities, but medical imag ing datasets, for example for skin lesion diagnosis, are limited, and overÔ¨Åtting can o ccur. To illustrate, Winkler and colleagues found that superimposed scale bars (Winkler et al., 2021) or skin markings (Winkler et al., 2019) in dermoscopic images may impair the d iagnostic performance of the convolutional neural network (CNN) when unintentionally o verÔ¨Åtting these artifacts during model training. ¬©2021 Raumanns, Schouten, Joosten, Pluim and Cheplygina. Li cense: CCBY 4.0. https://www.melbajournal.org/papers/2021020.html .Raumanns et al. A promising approach to generalize better in small sample si ze settings is multitask learning (MTL), wherethe model has to learn diÔ¨Äerent tasks si multaneously. This approach showedimprovedperformanceinvariousmedicalapplicatio ns, forexample, forbreastlesions (Shi et al., 2019; Liu et al., 2018). However, when moving fro m singletask to multitask models, we need additional annotations. Applying MTL is cha llenging because datasets typically do not have such additional annotations. Further more, building a medical image dataset from scratch with expert annotations is timeconsu ming and costly. We present a dataset of additional annotations for skin lesi on diagnosis based on non expert annotations on three dermatoscopic criteria: asymm etry, border and color (socalled ABC criteria). In dermatology, the use of the ABCDE (asymmet ry, border, color, diame ter, and evolution or elevation) rule is widespread. Howeve r, scoring the diameter (D) and evolution or elevation (E) are more complex tasks and theref ore less suitable for nonexpert annotation. The term nonexpert is deÔ¨Åned here as annotations provided by three diÔ¨Äer ent annotation sources: undergraduate students, crowd wor kers from Amazon MTurk and automated annotations through classic image processing al gorithms. We study the quality of nonexpert annotations from diÔ¨Äerent viewpoints. Firstly, we determine the discriminative power of ABC features for diag nosis. We show to what extent the ABC annotations correlate to the diagnosis, and w e study how we can use ABC annotations to improve the performance of a CNN. Secondl y, the inter agreement level for A, B and C feature between the diÔ¨Äerent annotation so urces. The study ex tends our research on the topic (Raumanns et al., 2020) by usi ng automated annotations as well as comparing the performance on three open source CNN architectures, in partic ular: VGG16 (Simonyan and Zisserman, 2015), Inception v3 ( Szegedy et al., 2016) and ResNet50 (He et al., 2016) encoders. Further, we investigat e whether MTL is also beneÔ¨Å cial for automated annotations and show that the performanc e beneÔ¨Åts from using multiple annotations in MTL. Besides addressing the lack of expert annotations using non expert ones, we make the dataset with collected ABC annotations and code open, elimi nating obstacles for future research. More speciÔ¨Åcally, the investigation addresses t he following research questions: 1.What is the correlation between the ABC annotations and the diagnostic label? 2.How can we use the ABC annotations to improve the performance of a CNN? 3.What is the inter agreement level for A, B and C feature between the diÔ¨Äerent annotation sources? 4.How can CNN performance beneÔ¨Åt from using multiple annotations? Our results give valuable insights into the quality of none xpert annotations. Using the collected nonexpert annotations in three diÔ¨Äerent CNNs , we show that these are of added value for the performance of the models. This suggests that the use of nonexpert annotations might be promising for application in similar d omains. 2ENHANCE (ENriching Health data by ANnotations of Crowd and E xperts) 2. Related Work "
153,KGNN: Harnessing Kernel-based Networks for Semi-supervised Graph Classification.txt,"This paper studies semi-supervised graph classification, which is an
important problem with various applications in social network analysis and
bioinformatics. This problem is typically solved by using graph neural networks
(GNNs), which yet rely on a large number of labeled graphs for training and are
unable to leverage unlabeled graphs. We address the limitations by proposing
the Kernel-based Graph Neural Network (KGNN). A KGNN consists of a GNN-based
network as well as a kernel-based network parameterized by a memory network.
The GNN-based network performs classification through learning graph
representations to implicitly capture the similarity between query graphs and
labeled graphs, while the kernel-based network uses graph kernels to explicitly
compare each query graph with all the labeled graphs stored in a memory for
prediction. The two networks are motivated from complementary perspectives, and
thus combing them allows KGNN to use labeled graphs more effectively. We
jointly train the two networks by maximizing their agreement on unlabeled
graphs via posterior regularization, so that the unlabeled graphs serve as a
bridge to let both networks mutually enhance each other. Experiments on a range
of well-known benchmark datasets demonstrate that KGNN achieves impressive
performance over competitive baselines.","Graphstructured data are ubiquitous in a wide range of domains. Examples include social networks [ 31], biological reaction networks [28], molecules [ 34]. For graphstructured data, one fundamental problem is graph classification, which aims at analyzing and pre dicting the property of the entire graph. Such a problem has various downstream applications, including predicting the properties of molecules [ 13] and analyzing the functionality of compounds [ 19]. Graph classification is typically formalized as a supervised learn ing task, and many recent works propose to use graph neural net works (GNNs) [ 23,24,40] to solve the problem. The basic idea is to learn effective graph representations with nonlinear message passing schemas. At each step, a node receives messages from all its neighbors, which are further aggregated to update the node representation. Finally, a readout function is applied to integrate all the node representations into a representation of the whole graph. With this shared message passing framework, the learned graph representations can implicitly capture the similarity between query graphs and labeled graphs in the latent space. Despite the good performance, GNNs usually require a large amount of labeled data for training and fail to leverage unlabeled data. However, data an notation often requires domain experts, which is highly expensive, especially in specific domains such as biomedicine [13]. This motivates us to study semisupervised graph classifica tion, i.e., using both labeled and unlabeled data for graph classi fication. The unlabeled data serve as a regularizer, which helps a model better explore the inherent graph semantic information even with a limited amount of labeled data. Indeed, there are a handful of works along this line [ 13,24,34], and they typically employ semisupervised learning techniques to train GNN models. These approaches usually integrate selftraining [ 22] or knowledge distillation [ 14] into GNNs. However, these methods suffer from two key limitations: (1) Unable to well explore graph similar ity. Graph classification relies on comparing the query graphs witharXiv:2205.10550v1  [cs.LG]  21 May 2022labeled graphs. Existing methods [ 13,24,34] typically compute the similarity of graphs in an implicit way by projecting graphs into a latent space with a GNN encoder. However, such implicit methods often cannot well explore the similarity of graphs. (2) Suffering from labeled data scarcity . Besides, experimental results show that the performance of existing methods is still unsatisfactory especially when labeled data are very scarce. The reason is that these methods are not able to obtain highquality annotated data to improve model training. Therefore, we are looking for an approach that is able to better consider the relationship among graphs and meanwhile overcome the challenge of scarce labeled data. In this paper, we propose such a method called the Kernelbased Graph Neural Network (KGNN). The key idea of KGNN is to en hance GNNs with graph kernels, which are able to explicitly mea sure the graph similarity of graphs. To leverage graph kernels effectively, we introduce two modules in a KGNN, i.e., a GNNbased network and a kernelbased network. The GNNbased network is parameterized by existing GNNs, which uses message passing mechanisms to learn useful graph representations for graph classi fication. In contrast, the kernelbased network employs a memory network, where the memory stores the given labeled graphs. Given a query graph, we compare it with all the graphs in the memory using graph kernels, and further integrate the labels of the most similar graphs to predict the label of the query graph. The GNNbased network and kernelbased network explore graph similarity from different angles, i.e., message passing and graph kernels respectively. Although they are naturally comple mentary, how to jointly train both networks which enables us to distill the knowledge between each other is nontrivial. We solve the problem with a novel posterior regularization framework [ 7], which encourages both networks to collaborate with each other and max imize their agreement on unlabeled data. Each training iteration of posterior regularization consists of two steps. In the first step, the kernelbased network is updated by projecting the GNNbased network into a regularized subspace, yielding a stronger kernel based network. In the second, we distill the knowledge learned by the kernelbased network into the GNNbased network, so that allowing the GNN to better explore graph similarity and overcome the challenge caused by the scarcity of labeled data. To summarize, the main contributions of this work are as follows: ‚Ä¢We propose a novel approach for semisupervised graph classification, which consists of a GNNbased network and a kernelbased network to fully capture the graph similarity and overcome the scarcity of labeled data. ‚Ä¢We develop a novel posterior regularization framework to combine the advantages of graph neural networks and graph kernels in a principled way, such that they can mutually enhance each other via optimizing the two modules with an EMstyle algorithm. ‚Ä¢We conduct extensive experiments on a range of wellknown benchmarks to demonstrate the effectiveness of our KGNN. 2 RELATED WORK "
154,Neighborhood-Regularized Self-Training for Learning with Few Labels.txt,"Training deep neural networks (DNNs) with limited supervision has been a
popular research topic as it can significantly alleviate the annotation burden.
Self-training has been successfully applied in semi-supervised learning tasks,
but one drawback of self-training is that it is vulnerable to the label noise
from incorrect pseudo labels. Inspired by the fact that samples with similar
labels tend to share similar representations, we develop a neighborhood-based
sample selection approach to tackle the issue of noisy pseudo labels. We
further stabilize self-training via aggregating the predictions from different
rounds during sample selection. Experiments on eight tasks show that our
proposed method outperforms the strongest self-training baseline with 1.83% and
2.51% performance gain for text and graph datasets on average. Our further
analysis demonstrates that our proposed data selection strategy reduces the
noise of pseudo labels by 36.8% and saves 57.3% of the time when compared with
the best baseline. Our code and appendices will be uploaded to
https://github.com/ritaranx/NeST.","In the era of deep learning, neural network models have achieved promising performance in most supervised learn ing settings, especially when combined with selfsupervised learning techniques (Chen et al. 2020; Devlin et al. 2019; Hu et al. 2020; Zhu et al. 2022). However, they still require a sufÔ¨Åcient amount of labels to achieve satisfactory perfor mances on many downstream tasks. For example, in the text domain, curating NLP datasets often require domain experts to read thousands of documents and carefully label them with domain knowledge. Similarly, in the graph domain, molecules are examples naturally represented as graphs, and characterizing their properties relies on density functional theory (DFT) (Cohen, MoriS ¬¥anchez, and Yang 2012) which often takes several hours. Such a dependency on labeled data is one of the barriers to deploy deep neural networks (DNNs) in realworld applications. To better adapt the DNNs to target tasks with limited la bels, one of the most popular approaches is semisupervised learning (SSL), which jointly leverages unlabeled data and *Corresponding author. Copyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.labeled data to improve the model‚Äôs generalization power on the target task (Yang et al. 2021; Wang et al. 2022). Although generative models (Gururangan et al. 2019) and consistency based regularization (Tarvainen and Valpola 2017; Miy ato et al. 2018; Xie et al. 2020a) methods have been pro posed for semisupervised learning, they either suffer from the issue of limited representation power (Tsai, Lin, and Fu 2022) or require additional resources to generate high quality augmented samples (e.g., for text classiÔ¨Åcation, Xie et al. (2020a) generate augmented text via backtranslation, which rely on a Machine Translation model trained with massive labeled sentence pairs). Consequently, they cannot be readily applied to lowresource scenarios. Selftraining is a proper tool to deal with the deÔ¨Åciency of labeled data via gradually enlarging the training set with pseudolabeled data (Rosenberg, Hebert, and Schneiderman 2005). SpeciÔ¨Åcally, it can be interpreted as a teacherstudent framework : the teacher model generates pseudo labels for the unlabeled data, and the student model updates its pa rameters by minimizing the discrepancy between its predic tions and the pseudo labels (Xie et al. 2020b; Mukherjee and Awadallah 2020). Though conceptually simple, selftraining has achieved superior performance for various tasks with limited labels, such as image classiÔ¨Åcation (Sohn et al. 2020; Rizve et al. 2021), natural language understanding (Du et al. 2020), sequence labeling (Liang et al. 2020), and graph learning (Hao et al. 2020). Selftraining has also been suc cessfully extended to other settings including weak super vision (Zhang et al. 2021b) and zeroshot learning (Li, Savarese, and Hoi 2022). However, one major challenge of selftraining is that it suffers from conÔ¨Årmation bias (Arazo et al. 2020) ‚Äî when the teacher model memorizes some biases and generates incorrect pseudo labels, the student model will be rein forced to train with these wrong biases. As a result, the biases may amplify over iterations and deteriorates the Ô¨Å nal performance. To suppress the noisy pseudo labels in selftraining, Xu et al. (2021); Zhang et al. (2021a); Sohn et al. (2020); Kim et al. (2022b) leverage model predic tive conÔ¨Ådence with a thresholding function, Mukherjee and Awadallah (2020); Tsai, Lin, and Fu (2022) propose to leverage model uncertainty to select samples with low un certainty, and Wang et al. (2021) use metalearning to con duct instance reweighting for sequence labeling. AlthougharXiv:2301.03726v2  [cs.LG]  15 Feb 2023these approaches attempt to reduce the label noise, they se lect the data for selftraining based on the model prediction only. However, the predictions of the deep neural network can be overconÔ¨Ådent and biased (Guo et al. 2017; Kong et al. 2020; Kan, Cui, and Yang 2021), and directly using such predictions without any intervention to Ô¨Ålter pseudo la bels cannot effectively resolve the label noise issue. Another problem from selftraining is training instability , as it se lects pseudolabeled data only based on the prediction of the current round. Due to the stochasticity involved in training neural networks (e.g., random initialization, training order), the prediction can be less stable (Yu et al. 2022b), especially for the noisy pseudolabeled data (Xia et al. 2022). Conse quently, the noise in the previous rounds may propagate to later rounds, which deteriorate the Ô¨Ånal performance. Motivated by the above, we propose NeST , a simple yet powerful approach guided by the data representations, to boost the performance of selftraining for fewshot learn ing. Inspired by recent works indicating that the represen tations from deep neural networks can be discriminative and less affected by noisy labels (Li et al. 2021), we harness the features learned from the neural models to select the most reliable samples in selftraining. In addition, several works have indicated that samples within the same category tend to share similar representations, such as categoryguided text mining (Meng et al. 2020) and motifdriven graph learn ing (Zhang et al. 2020). Similarly, we hypothesize that a sample‚Äôs pseudo label is more likely to be correct only if its prediction is similar to the neighbor labeled instances in the embedding space. To fulÔ¨Åll the denoising purpose, NeST creates the neighborhood for each unlabeled data by Ô¨Ånd ing the top knearest labeled samples, then calculates the divergence between its current prediction and the label of its neighbors to rank the unlabeled data. As a result, only the in stances with the lowest divergence will be selected for self training, which mitigates the issue of label noise. Moreover, to robustly select the training samples for selftraining, we aggregate the predictions on different iterations to promote samples that have lower uncertainty over multiple rounds for selftraining. We remark that NeST is an efÔ¨Åcient substitution for exist ing selftraining approaches and can be combined with vari ous neural architectures. The contributions of this paper are: ‚Ä¢ We propose NeST to improve the robustness of self training for learning with few labels only. ‚Ä¢ We design two additional techniques, namely neighborhoodregularized sample selection to re duce label noise, and prediction aggregation to alleviate the training instability issue. ‚Ä¢ Experiments on 4 text datasets and 4 graph datasets with different volumes of labeled data verify that NeST im proves the performance by 1.83% and 2.51% respec tively and saves the running time by 57.3%. 2 Related Work "
155,Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness.txt,"In this paper, we present a strategy for training convolutional neural
networks to effectively resolve interference arising from competing hypotheses
relating to inter-categorical information throughout the network. The premise
is based on the notion of feature binding, which is defined as the process by
which activation's spread across space and layers in the network are
successfully integrated to arrive at a correct inference decision. In our work,
this is accomplished for the task of dense image labelling by blending images
based on their class labels, and then training a feature binding network, which
simultaneously segments and separates the blended images. Subsequent feature
denoising to suppress noisy activations reveals additional desirable properties
and high degrees of successful predictions. Through this process, we reveal a
general mechanism, distinct from any prior methods, for boosting the
performance of the base segmentation network while simultaneously increasing
robustness to adversarial attacks.","The advent of Deep Neural Networks (DNNs) has seen overwhelming improvement in dense image labeling tasks [2, 5, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 27, 30, 40], however, for some common benchmarks [10] the rate of improvement has slowed down. While one might assume that barriers to further improvement require changes at the architectural level, it has also been borne out that pretraining across a variety of datasets [26, 33] can improve performance exceeding improvements seen from changing the model architecture. However, there are challenging scenarios for which DNNs have difÔ¨Åculty on regardless of pretraining or architectural changes, such as highly occluded scenes, or objects appearing out of their normal context [35]. It is not clear though, for dense image labeling tasks, how to resolve these speciÔ¨Åc scenarios for more robust prediction quality on a perpixel level. A question that naturally follows from this line of reasoning is: How can the number of locally challenging cases be increased, or the problem made more difÔ¨Åcult in general? In this paper, we address this problem using a principled approach to improve performance and that also implies a more general form of robustness. As inspiration, we look to a paradigm discussed often in the realm of human vision: the binding problem [34, 37]. The crux of this problem is that given a complex decomposition of an image into features that represent different concepts, or different parts of the image, how does one proceed to successfully relate activations corresponding to common sources in the input image to label a whole from its parts, or separate objects. Motivated by the binding problem, a successful solution in the computer vision domain should rely on both determining correspondences in activations among features that represent disparate concepts, and also to associate activations tied to related features that are subject to spatial separation in the image. To address similar issues for the image classiÔ¨Åcation task, recent studies [36, 38, 39] have considered mixing two image examples with constraints on the distribution of features. However, these methods suffer from biases in the dataset used, as they have no strategy when deciding on which images to mix which is crucial for the dense labeling problem. Additionally, these strategies do not adequately separate information from different sources in the image as they only require the network to make a single (classiÔ¨Åcation) prediction during training. In our work, the means of solving the feature binding problem takes a direct form, which involves training networks on a specially designed dataset of mixed images to simultaneously address problems of dense image labeling [3, 27, 30], and blind source separation [12, 16]. Humans show a surprising level of capability in interpreting a superposition (e.g., average) of two images, both interpreting the contents of each scene and determining the membership of local patterns within a given scene. The underlying premise of this work involves producing networks capable of simultaneously performing dense image labeling for pairs of images while also separating labels according to the source images. If one selects pairs on the basis of a weighted average (see Fig. 1 (left)), this allows treatment of the corresponding dense image labeling problem in the absence of source separation by extension. This process supports several objectives: (i) it signiÔ¨Åcantly increases the number of occurrences that are locally ambiguous that need to be resolved to produce a correct categorical assignment, (ii) it forces broader spatial context to be considered in making categorical assignments, and (iii) it stands to create more powerful networks for standard dense labeling tasks and dealing with adversarial perturbations by forcing explicit requirements on how the network uses the input. The end goal of our procedure is to improve overall performance as well as increase the prediction quality on complex images (see Fig. 1 (right)), heavily occluded scenes, and also invoke robustness to challenging adversarial inputs. Our main contributions are as follows:ISLAM, KOWAL, DERPANIS, BRUCE: CATEGORY DEPENDENT MIXUP 3 ‚Ä¢ To the best of our knowledge we present the Ô¨Årst work which applies image blending to the dense labeling task. To this end, we propose a novel training pipeline which simultaneously solves the problems of dense labeling and blind source separation. ‚Ä¢ We further introduce a new categorical clustering strategy which exploits semantic knowledge of the dataset to mix input images based on their class distributions. ‚Ä¢ We show, through extensive quantitative and qualitative experiments, that our pipeline outperforms recent image blending methods [38, 39] on the PASCAL VOC 2012 dataset [10], while simultaneously improving robustness to adversarial attacks. 2 Related Work "
156,Semantic Identification Attacks on Web Browsing.txt,"We introduce a Semantic Identification Attack, in which an adversary uses
semantic signals about the pages visited in one browsing session to identify
other browsing sessions launched by the same user. This attack allows an adver-
sary to determine if two browsing sessions originate from the same user
regardless of any measures taken by the user to disguise their browser or
network. We use the MSNBC Anonymous Browsing data set, which contains a large
set of user visits (labeled by category) to implement such an attack and show
that even very coarse semantic information is enough to identify users. We
discuss potential counter- measures users can take to defend against this
attack.","Online privacy is becoming an increasingly important issue for users, policy makers, and academic researchers. In re cent years there has been signicant work highlighting the fragility of user privacy by exposing threats ranging from the deanonymization of public data sets to third party tracking in online browsing. In particular, an area of focus has been the ability for ad vertisers to identify users across browsing sessions. Many privacy experts advocate that users should regularly clear browsing cookies to prevent third party trackers from build ing detailed proles and linking their webpage visits. Other experts recommend private browsing modes (such as incog nito mode on Google Chrome) to prevent the browser from storing any persistent browsing data and to ensure that web cookies are never reused. There are even more powerful tools, such as Tor or Brave, a new open source browser fo cused explicitly on blocking advertisements and third party trackers. Unfortunately, these measures are not adequate. In this pa per, we introduce a semantic identication attack , a new attack that can identify users across multiple browsing ses sions. In this attack, an adversary can, in some cases, leverage semantic signals  features derived from the content viewed by a user (words on pages viewed, page titles, links in/out of page, etc)  to identify when two browsing sessions have been launched by the same user. If one of the sessions contains personally identifying information, the adversary now has a way of linking this information to other sessions launched by the user. A user is unlikely to visit the same urls in every session. However, the urls in a given user's browsing sessions are un likely to be drawn completely at random from all the urls on the web. If a user's web browsing is characterized by a set of tasks (checking certain news topics, reading specic blogs, etc), their browsing sessions will consistently contain visits to pages/websites relevant to these tasks. The distri bution of these visits can be viewed as a user specic session ngerprint. Though a user's browsing sessions are likely to deviate and contain other behavior as well, the presence of these pages ngerprints the user to the session. The ability to extract a browsing ngerprint and use it to link together a user's sessions has signicant implications for online privacy. An adversary that could identify the browsing ngerprint for a user could determine with high probability when a certain browsing session belongs to that user even when they use multiple devices, regularly clear cookies or obfuscate their identity through proxies. While there has been prior work in identifying users across browsing sessions, nearly all methods have relied on the browser or network metadata for identifying users. Network signals and browser metadata signatures can be spoofed or disguised through extensions and proxies. However, seman tic data about browsing sessions will still be available to websites and even to third parties such as advertisers. We demonstrate that this kind of data can be used to identify and track users. In this paper, we rst review past work and highlight the dierences between prior approaches and ours. We then ex plain the intuition behind the semantic identication attack and present several dierent strategies. Finally, we evaluate the attacks on a collection of user browsing data and discuss the countermeasures. 2. RELATED WORK "
157,NaturalFinger: Generating Natural Fingerprint with Generative Adversarial Networks.txt,"Deep neural network (DNN) models have become a critical asset of the model
owner as training them requires a large amount of resource (i.e. labeled data).
Therefore, many fingerprinting schemes have been proposed to safeguard the
intellectual property (IP) of the model owner against model extraction and
illegal redistribution. However, previous schemes adopt unnatural images as the
fingerprint, such as adversarial examples and noisy images, which can be easily
perceived and rejected by the adversary. In this paper, we propose
NaturalFinger which generates natural fingerprint with generative adversarial
networks (GANs). Besides, our proposed NaturalFinger fingerprints the decision
difference areas rather than the decision boundary, which is more robust. The
application of GAN not only allows us to generate more imperceptible samples,
but also enables us to generate unrestricted samples to explore the decision
boundary.To demonstrate the effectiveness of our fingerprint approach, we
evaluate our approach against four model modification attacks including
adversarial training and two model extraction attacks. Experiments show that
our approach achieves 0.91 ARUC value on the FingerBench dataset (154 models),
exceeding the optimal baseline (MetaV) over 17\%.","In the past few years, deep neural networks (DDNs) have been applied to a wide range of fields like autonomous driv ing[Gauerhof et al. , 2018 ], face recognition [Parkhi et al. , 2015 ], and intelligent healthcare [Esteva et al. , 2017 ]due to their outstanding performance. While DNNs are prevalent in our lives, training such a model is a nontrivial task as it re quires a large amount of annotated data and powerful comput ing resources. Thus, many companies provide the prediction of their trained model as APIs to profit, such as Machine LearningasaService (MLaaS). In other words, the model is becoming a critical business asset, which requires to be pro tected against illegal redistribution and model extraction. Negative Model Positive Model Source Model Class A Class BFigure 1: The intuition of our proposed NaturalFinger. It leverages GAN to fingerprint the decision difference areas (purple areas) be tween positive models and negative models. Generally, a type of common approach to protect the in tellectual property (IP) of DNN models is model fingerprint ing[Cao et al. , 2021; Yang et al. , 2022; Lukas et al. , 2019 ]. This kind of scheme first constructs a query set based on the source model (protected model) as the fingerprint. Then, the defender queries the suspect model with the query set. If the matching rate between the predicted labels of the suspected model and the query set labels exceeds a threshold, the de fender judges the suspect model as a stolen model. However, previous fingerprinting schemes suffer from the following two problems: 1) Stealthiness. They generate un natural samples to query the suspect model like noisy exam ples, which can be perceived and rejected by the adversary; 2) Robustness. Many fingerprinting schemes [Wang and Chang, 2021; Peng et al. , 2022 ]leverage various adversarial exam ples to fingerprint the decision boundary, which is not robust against adversarial defense. To solve those two problems, we propose NaturalFinger, which generates natural query samples with generative adver sarial networks (GANs) [Goodfellow et al. , 2014a ]. The in tuition of our method is shown in Fig. 1. Our proposed Natu ralFinger fingerprints the decision difference areas (purple ar eas) between positive models (stolen from the source model) and negative models (unrelated to the source model), rather than the decision boundary. Beneficial to this strategy, Natu ralFinger is resistant to various model modifications. What‚Äôs more, the utilization of GAN not only empowers us to gener ate more imperceptible samples, but also enables us to gen erate unrestricted samples to explore the decision boundary.arXiv:2305.17868v1  [cs.CV]  29 May 2023(a) Frog/Bird (b) Bird/Airplane (c) Horse/Deer (d) Airplane/Ship Figure 2: The examples of NaturalFinger. The first label and the second label are predicted by positive models and negative models, respectively. Fig. 2 shows some interesting examples of our scheme. As shown in Fig. 2, those samples contain some contentious fea tures, which causes positive models and negative models to produce different labels. For example, Fig. 2(a) is a bird with green color, positive models predict it as a bird while negative models predict it as a frog. However, simply applying GAN faces the following ques tions. 1) How to generate samples that fingerprint the deci sion difference areas? 2) How to generate natural query sam ples instead of lowquality images as we optimize the input noises? 3) How to reduce overfit when the number of trained models is small? To address the first question, considering samples belong ing to decision difference areas are predicted differently by positive models and negative models, we assign two differ ent labels for them during optimization. For positive models, we assign the original labels predicted by most positive mod els. For negative models, to ease the optimization, we assign them the closest labels to the original labels. In specific, we attack the negative models with a fast gradient sign method (FGSM) [Goodfellow et al. , 2014b ]attack to obtain the ad versarial labels. We then optimize the noises to ensure that all models can correctly predict the generated images. To handle the second question, we utilize the discriminator to constrain the generated samples by adding its loss to the total loss. To tackle the last question, inspired by data augmentation, we apply image transformation to the generated samples before feeding them into the model. In summary, we propose a stealthy, robust, and effective model fingerprinting scheme to protect the IP of the model owner. The main contributions are summarized as follows: ‚Ä¢ We propose NaturalFinger which generates natural and stealthy query samples with GAN. Besides, NaturalFinger fingerprints the decision difference areas instead of the de cision boundary, which is more robust against model mod ification and is more effective against model extraction. ‚Ä¢ To solve the challenges encountered by applying GAN, we propose three tricks including adversarial label, dis criminator loss, and input transformation. Ablation experi ments show that those tricks significantly improve the per formance of NaturalFinger from 0.79 to 0.91 in ARUC. ‚Ä¢ Experiments demonstrate that NaturalFinger is robust against four model modifications and is effective against two model extraction attacks. It attains a 0.91 ARUC value on the FingerBench dataset (154 models), exceeding the optimal baseline (MetaV [Panet al. , 2022 ]) over 17%.2 Related Work "
158,Temporal Continuity Based Unsupervised Learning for Person Re-Identification.txt,"Person re-identification (re-id) aims to match the same person from images
taken across multiple cameras. Most existing person re-id methods generally
require a large amount of identity labeled data to act as discriminative
guideline for representation learning. Difficulty in manually collecting
identity labeled data leads to poor adaptability in practical scenarios. To
overcome this problem, we propose an unsupervised center-based clustering
approach capable of progressively learning and exploiting the underlying re-id
discriminative information from temporal continuity within a camera. We call
our framework Temporal Continuity based Unsupervised Learning (TCUL).
Specifically, TCUL simultaneously does center based clustering of unlabeled
(target) dataset and fine-tunes a convolutional neural network (CNN)
pre-trained on irrelevant labeled (source) dataset to enhance discriminative
capability of the CNN for the target dataset. Furthermore, it exploits
temporally continuous nature of images within-camera jointly with spatial
similarity of feature maps across-cameras to generate reliable pseudo-labels
for training a re-identification model. As the training progresses, number of
reliable samples keep on growing adaptively which in turn boosts representation
ability of the CNN. Extensive experiments on three large-scale person re-id
benchmark datasets are conducted to compare our framework with state-of-the-art
techniques, which demonstrate superiority of TCUL over existing methods.","Person reidentication (reid) is an important problem in computer vision that aims to match images of a person captured by dierent cameras with non overlapping views [31]. It is viewed as a search problem with a goal to retrieve the most relevant images to the top ranks [30]. Many recent works have at tracted extensive research eorts to address the reid problem using deep learning [29,14,8,27,15,23]. In spite of remarkable advancements achieved by deep learn ing methods, most existing techniques adopt a supervised learning paradigm to solve the reid problem [29,14]. Hence, these supervised deep models take an assumption of availability of sucient crossview identity matching pairs ofarXiv:2009.00242v1  [cs.CV]  1 Sep 20202 U. Ali et al. manually labelled training data for each camera network. This assumption limits the generalizing capability of a reid model to multiple camera networks due to lack of labelled training samples under a new environment. As a result, many previous works have addressed this scarcity of labelled training data under a new camera network by focusing on unsupervised or semi supervised learning [10,12,17,3,25]. Most of these works typically deal with rela tively small datasets; without being able to exploit the potential of deep learning methods that require largescale datasets for better performance [10,11,19,28]. More recently, there has been greater emphasis towards clustering and domain adaptation based deep unsupervised methods for person reid [6,21,16]. However, performance of unsupervised learning approaches is much weaker as compared to supervised models. This is due to nonexistence of crossview identity labelled matching pairs, which makes it dicult to learn the discriminative representa tion for recognizing a person under severe changes in visual appearances across dierent camera networks. To improve this situation, we propose to use temporal continuity of images within a camera jointly with spatial similarity of feature maps acrosscameras to generate reliable pseudolabels for training a reid model in a progressive fashion. Temporal information is readily available for images captured by a camera, for example, frame number (Frame ID) of an image in a video sequence is easily ob tainable in an unsupervised manner. The idea is that given an image in a camera, other images of the same identity would exist in the temporal vicinity of that image. However, temporal continuity is only eective within one camera; hence, it cannot be utilized to obtain crosscamera matching identity pairs. To this end, we propose a crosscamera centerbased pseudolabeling method to cluster fea tures by their similarity to centers. Centers are initialized by performing a simple clustering task on features extracted from a baseline model pretrained on irrel evant (source) dataset. An intersection between crosscamera pseudolabels and withincamera temporal vicinity results in highly reliable pseudolabels for ne tuning a reid model. Features extracted from netuned model are then used to generate a new set of more reliable labels for further netuning in a progressive manner. It is also notable here that dierent from videobased method [13,1], our method does not rely on person trajectory obtained by tracking algorithms. Therefore, it is highly applicable to practical scenarios. In summary, the contributions of this paper are as follows: {we propose centerupdate based pseudolabeling approach in order to provide crossview discriminative information for unsupervised person reid task. {we exploit temporal continuity of images within a camera to sample reliable datapoints and utilize selfpaced progressive learning to train an eective re id model. To the best of our knowledge, this is the rst work to use temporal continuity for person reid task. {extensive experiments on three largescale datasets indicate the eectiveness of our method with performance on par with stateoftheart approaches.Temporal Continuity for Person ReId 3 2 Related Work "
159,Unsupervised Anomaly Detection in Stream Data with Online Evolving Spiking Neural Networks.txt,"Unsupervised anomaly discovery in stream data is a research topic with many
practical applications. However, in many cases, it is not easy to collect
enough training data with labeled anomalies for supervised learning of an
anomaly detector in order to deploy it later for identification of real
anomalies in streaming data. It is thus important to design anomalies detectors
that can correctly detect anomalies without access to labeled training data.
Our idea is to adapt the Online evolving Spiking Neural Network (OeSNN)
classifier to the anomaly detection task. As a result, we offer an Online
evolving Spiking Neural Network for Unsupervised Anomaly Detection algorithm
(OeSNN-UAD), which, unlike OeSNN, works in an unsupervised way and does not
separate output neurons into disjoint decision classes. OeSNN-UAD uses our
proposed new two-step anomaly detection method. Also, we derive new theoretical
properties of neuronal model and input layer encoding of OeSNN, which enable
more effective and efficient detection of anomalies in our OeSNN-UAD approach.
The proposed OeSNN-UAD detector was experimentally compared with
state-of-the-art unsupervised and semi-supervised detectors of anomalies in
stream data from the Numenta Anomaly Benchmark and Yahoo Anomaly Datasets
repositories. Our approach outperforms the other solutions provided in the
literature in the case of data streams from the Numenta Anomaly Benchmark
repository. Also, in the case of real data files of the Yahoo Anomaly Benchmark
repository, OeSNN-UAD outperforms other selected algorithms, whereas in the
case of Yahoo Anomaly Benchmark synthetic data files, it provides competitive
results to the results recently reported in the literature.","Unsupervised anomaly discovery in stream data is a research topic that has important practical applications. For example, an Internet system administra tor may be interested in recognition of abnormally high activity on a web page potentially caused by a hacker attack. Unexpected spiking usage of a CPU unit in a computer system could be another example of anomalous behaviour that mayrequireinvestigation. CorrectdetectionandclassiÔ¨Åcationofsuchanomalies may enable optimization of the performance of the computer system. However, in many cases, it is not easy to collect enough training data with labeled anoma lies for supervised learning of an anomaly detector in order to use it later for identiÔ¨Åcation of real anomalies in streaming data. It is thus particularly impor tant to design anomalies detectors that can correctly detect anomalies without access to labeled training data. Moreover, since the characteristic of an input data stream may be changing, the anomaly detector should learn in an online mode. In order to design an eÔ¨Äective anomaly detection system, one can consider adaptation of evolving Spiking Neural Networks (eSNNs) (Kasabov, 2014; Lobo et al., 2018, 2020b; MaciƒÖg et al., 2020) to the task. eSNN is a neural network with an evolving repository of output neurons, in which learning processes, neuronal communication and classiÔ¨Åcation of data instances are based solely on transmission of spikes from input neurons to output neurons (Kasabov, 2014). The spikes increase so called postsynaptic potential values of output neurons, and directly inÔ¨Çuence the classiÔ¨Åcation results. The input layer of neurons in eSNN transforms input data instances into spikes. Depending on the type of input data, the transformation can be carried out by means of the temporal encoding methods such as StepForward or ThresholdBased (Petro et al., 2019; MaciƒÖg et al., 2019) or, alternatively, with the use of Gaussian Receptive Fields (Lobo et al., 2018). The distinctive feature of the eSNN is that its repository of output neurons evolves during the training phase based on candidate output neurons that are created for every new input data sample (Kasabov et al., 2013; Kasabov, 2015). More speciÔ¨Åcally, for each new input value, a new candidate output neuron is created and is either added to the output repository or, based on the provided similarity threshold, is merged with one of the output neurons contained in the repository. Recently, an online variant OeSNN of eSNN was proposed for classiÔ¨Åcation of stream data (Lobo et al., 2018). Contrary to the eSNN architecture, the size of the evolving repository of output neurons in OeSNN is limited. When the repository of output neurons is full and a new candidate output neuron is signiÔ¨Åcantly diÔ¨Äerent from all of the neurons in the repository, an oldest neuron is replaced with the new candidate output neuron. It was claimed in (Lobo 2et al., 2018) that OeSNN is able to make fast classiÔ¨Åcation of input stream data, while preserving restrictive memory limits. Considering all the positive features of eSNN and OeSNN, in this article, we oÔ¨Äer a novel Online evolving Spiking Neural Network for Unsupervised Anomaly Detection (OeSNNUAD) in stream data. Our main contributions presented in this article are as follows: ‚Ä¢We oÔ¨Äer a new OeSNNUAD anomaly detector working online in an unsu pervised way. It adapts the architecture of OeSNN, which also works in an online way, but, unlike OeSNNUAD, requires supervised training. The main distinction between our detector and OeSNN lies in applying diÔ¨Äer ent models of an output layer and diÔ¨Äerent methods of learning and input values classiÔ¨Åcation. While output neurons of OeSNN are divided into separate decision classes, there is no such separation of output neurons in our approach. Rather than that, each new output neuron is assigned an output value, which is Ô¨Årst randomly generated based on recent input values and then is updated in the course of learning of OeSNNUAD. ‚Ä¢As a part of the proposed OeSNNUAD detector, we oÔ¨Äer a new anomaly classiÔ¨Åcation method, which classiÔ¨Åes an input value as anomalous only in the following two cases: 1. if none of output neurons in the repository Ô¨Åres, or otherwise, 2. if an error between an input value and its OeSNNUAD prediction is greater than the average prediction error plus usergiven multiplicity of the standard deviation of the recent prediction errors. This twostep approach to classiÔ¨Åcation of an input value as anomalous or not enables more eÔ¨Äective detection of anomalies in input stream data and to the best of our knowledge was not previously used in the literature. ‚Ä¢We derive the important theoretical property of the OeSNN neuronal model that shows that the values of postsynaptic potential thresholds of all output neurons are the same. This property is inherited by our OeSNNUAD detector. The obtained result eliminates the necessity of recalculation of these thresholds when output neurons of OeSNN, as well as of OeSNNUAD, are updated in the course of the learning process, and increases the speed of classiÔ¨Åcation of input stream data. Moreover, we also prove that Ô¨Åring order values of input neurons do not depend on val ues ofTSandŒ≤parameters, which were previously used in OeSNNs for input value encoding with Gaussian Receptive Fields. ‚Ä¢We prove experimentally that in the case of stream data from the Nu menta Anomaly Benchmark repository (Ahmad et al., 2017b) as well as from the Yahoo Anomaly Datasets repository (Yahoo! Webscope, 2015) the proposed OeSNNUAD detects anomalies in unsupervised way more eÔ¨Äectively than other stateoftheart unsupervised and semisupervised detectors proposed in the literature. 3‚Ä¢Eventually, we argue that the proposed OeSNNUAD is able to make fast detection of anomalies among data stream input values and works eÔ¨Éciently in environments with imposed restrictive memory limits. The paper is structured as follows. In Section 2, we overview the related work. In Section 3, we present the architecture of Online evolving Spiking Neural Networks, whose adaptation proposed by us will be then used in OeSNN UAD. In Section 4, we provide new theoretical properties of neuronal model and input layer encoding of OeSNN, which enable more eÔ¨Äective and eÔ¨Écient detection of anomalies in our OeSNNUAD approach. In Section 5, we oÔ¨Äer our online method to unsupervised anomaly detection in stream data OeSNNUAD. Section 6 presents and discusses the proposed OeSNNUAD algorithm in detail. In Section 7, we present the results of comparative experimental evaluation of the proposed OeSNNUAD detector and stateoftheart unsupervised and semisupervised detectors of anomalies. We conclude our work in Section 8. 2. Related Work "
160,"A Systematic Review of Machine Learning Techniques for Cattle Identification: Datasets, Methods and Future Directions.txt","Increased biosecurity and food safety requirements may increase demand for
efficient traceability and identification systems of livestock in the supply
chain. The advanced technologies of machine learning and computer vision have
been applied in precision livestock management, including critical disease
detection, vaccination, production management, tracking, and health monitoring.
This paper offers a systematic literature review (SLR) of vision-based cattle
identification. More specifically, this SLR is to identify and analyse the
research related to cattle identification using Machine Learning (ML) and Deep
Learning (DL). For the two main applications of cattle detection and cattle
identification, all the ML based papers only solve cattle identification
problems. However, both detection and identification problems were studied in
the DL based papers. Based on our survey report, the most used ML models for
cattle identification were support vector machine (SVM), k-nearest neighbour
(KNN), and artificial neural network (ANN). Convolutional neural network (CNN),
residual network (ResNet), Inception, You Only Look Once (YOLO), and Faster
R-CNN were popular DL models in the selected papers. Among these papers, the
most distinguishing features were the muzzle prints and coat patterns of
cattle. Local binary pattern (LBP), speeded up robust features (SURF),
scale-invariant feature transform (SIFT), and Inception or CNN were identified
as the most used feature extraction methods.","The demand for e cient traceability and identiÔ¨Åcation systems for livestock is growing due to biosecurity and food safety requirements in the supply chain. The advanced technologies of machine learning and computer vision have been applied in precision livestock management, including critical disease detection, vaccination, production management, tracking, health monitoring, and animal wellbeing monitoring (Awad, 2016). ‚ÄòCattle identiÔ¨Åcation‚Äô refers to ‚Äòcattle detection‚Äô and ‚Äòcattle recognition‚Äô (Mahmud et al., 2021). Cattle identiÔ¨Åcation systems start from manual identiÔ¨Åcation to automatic identiÔ¨Åcation with the help of image processing. Traditional cattle identiÔ¨Åcation systems such as ear tagging (Awad, 2016), ear notching (Neary and Yager, 2002), and electronic devices (RuizGarcia and Lunadei, 2011) have been used for individual identiÔ¨Åcation in cattle farming. Disadvantages of these individual identiÔ¨Åcation methods include the possibility of losses, duplication, electronic device malfunctions, and fraud of the tag number (Rossing, 1999; Roberts, 2006). These are the issues and challenges for cattle identiÔ¨Åcation in livestock farm management. With the advent of computervision technology, cattle visual features have gained popularity for cattle identiÔ¨Å cation (Kusakunniran and Chaiviroonjaroen, 2018; Andrew et al., 2016, 2017; de Lima Weber et al., 2020). Visual feature based cattle identiÔ¨Åcation systems are used to detect and classify di erent breeds or individuals based on a set of unique features. In recent years, machine learning (ML) and deep learning (DL) approaches have been widely used for automatic cattle identiÔ¨Åcation using visual features (Andrew et al., 2016; Tharwat et al., 2014b; Andrew et al., 2019; Qiao et al., 2019; Li et al., 2021b). ML and DL are subÔ¨Åelds of artiÔ¨Åcial intelligence that can solve complex problems for automatic decisionmaking. ML is mainly divided into two approaches, such as supervised learning and unsupervised learning. The supervised ML approach is deÔ¨Åned by its use of labelled datasets, whereas the unsu pervised learning uses ML algorithms to analyse and cluster unlabeled datasets. An unsupervised ML approach can detect hidden patterns in data without human supervision (Janiesch et al., 2021). DL approaches are useful in areas with large and highdimensional datasets. Thus, DL models are usually outperformed over traditional ML models in the area of text, speech, image, video, and audio data processing (LeCun et al., 2015). There are two main steps in the development of ML and DL models. In the Ô¨Årst step, a training dataset is used to train the model, and in the second, the model is validated using a separate validation dataset. Thus, a trained model is created that is later used on the test dataset to determine its performance based on the test dataset. The dataset used for ML models includes the features and their corresponding outcomes or labels. The features are extracted from the input data using a feature extraction method. DL algorithms can automatically extract highlevel features from the dataset and learn from these features. Although the implementation of the ML and DL models is straightforward, there are some challenges with selecting algorithms, tuning parameters, and features for better prediction accuracy (Janiesch et al., 2021). Several important review studies have been completed in livestock farm management. Some recent literature re lzheng@csu.edu.au (Lihong Zheng), dave.swain@terracipher.com (Dave L. Swain), shmcgrath@csu.edu.au (Shawn McGrath), jmedway@csu.edu.au (Jonathan Medway) iiviews have addressed various research challenges in livestock farming, such as identiÔ¨Åcation, tracking, and health monitoring, using tagbased, ML, and DL approaches. Recently, Awad (2016) and Kumar and Singh (2020) reviewed the literature on using di erent classical and visual biometrics methods for cattle identiÔ¨Åcation and tracking. Li et al. (2021a) reviewed the deep learningbased approaches for classiÔ¨Åcation, object detection and segmentation, pose es timation, and tracking for di erent kinds of animals such as cattle, pigs, sheep, and poultry. A systematic literature review based on applying ML and DL approaches in precision livestock farming by Garcia et al. (2020) focused on grazing and animal health. Qiao et al. (2021) summarised the ML and DL approaches in precision cattle farming for cattle identiÔ¨Åcation, body condition score evaluation, and live weight estimation. They reviewed a small number of articles (n =13) related to cattle identiÔ¨Åcation using ML and DL approaches. Mahmud et al. (2021) conducted a systematic literature review showing the recent progress of DL applications for cattle identiÔ¨Åcation and health mon itoring. Their review included only a few articles related to cattle identiÔ¨Åcation. Moreover, these review articles focused on the combination of di erent types of challenges (e.g., tracking, pose estimation, weight estimation, identi Ô¨Åcation, and detection) solved by tagbased, ML, and DL methods in precision livestock farming. Thus, they lack in providing a comprehensive review on cattle identiÔ¨Åcation. Also, the existing review articles lack information on ML and DL applications combined for cattle identiÔ¨Åcation as they cover partly either ML or DL for cattle identiÔ¨Åcation. Moreover, the details of the cattle dataset for identiÔ¨Åcation are not discussed. In this context, an extensive systematic literature review is needed, particularly for the challenge of cattle identiÔ¨Åcation addressed by ML and DL approaches. Also, the details of the dataset used in the relevant articles need to be discussed, and the current trend of using ML and DL techniques in cattle identiÔ¨Åcation and future research opportunities with challenges need to be identiÔ¨Åed. This systematic literature review (SLR) aims to summarise and analyse the ML and DL applications used exten sively in cattle identiÔ¨Åcation. A total of 55 articles for cattle identiÔ¨Åcation and detection have been selected for this SLR. The reviewed articles are Ô¨Årst summarised, and then the datasets used in the selected articles are discussed. We then analyse the reviewed articles for trends in using ML and DL approaches for cattle identiÔ¨Åcation in recent years before presenting the feature extraction methods and performance evaluation metrics extracted from the reviewed articles. Finally, the challenges and future research directions in this Ô¨Åeld are discussed. 2. Methodology "
161,Noise-resistant Deep Metric Learning with Ranking-based Instance Selection.txt,"The existence of noisy labels in real-world data negatively impacts the
performance of deep learning models. Although much research effort has been
devoted to improving robustness to noisy labels in classification tasks, the
problem of noisy labels in deep metric learning (DML) remains open. In this
paper, we propose a noise-resistant training technique for DML, which we name
Probabilistic Ranking-based Instance Selection with Memory (PRISM). PRISM
identifies noisy data in a minibatch using average similarity against image
features extracted by several previous versions of the neural network. These
features are stored in and retrieved from a memory bank. To alleviate the high
computational cost brought by the memory bank, we introduce an acceleration
method that replaces individual data points with the class centers. In
extensive comparisons with 12 existing approaches under both synthetic and
real-world label noise, PRISM demonstrates superior performance of up to 6.06%
in Precision@1.","Commonly resulting from human annotation errors or imperfect automated data collection, noisy labels in training data degrade the predictive performance of models trained on them [11, 47, 16]. Manual inspection and correction of labels are labourintensive and hence scale poorly to large datasets. Therefore, training techniques that are robust to incorrect labels in training data play an important role in realworld applications of machine learning. To date, most works on noiseresistant neural networks [11, 17, 31, 52, 47, 48, 16] focus on image classiÔ¨Åcation. Little research effort has been devoted to noiseresistant deep metric learning (DML). The goal of DML is to learn a distance metric that maps similar pairs of data points close together and dissimilar pairs far apart, based on a predeÔ¨Ånednotion for similarity. DML Ô¨Ånds diverse applications such as image retrieval [18, 10, 33], landmark identiÔ¨Åcation [49], and selfsupervised learning [25]. Pairbased loss functions encourages DML networks to distinguish a similar pair of data points from one or more dissimilar pairs. Large batch sizes often lead to improved performance [6, 46, 5], as larger batches are more likely to contain informative examples. Pushing the idea of large batches to an extreme, [46] collects all positive and negative data samples from a memory bank. However, in the pres ence of substantial noise, indiscriminate use of all samples could lower performance. Alternatively, [26] uses learnable class centers to replace individual data samples in order to reduce computational complexity. Nonetheless, the cluster centers can also be sensitive to outliers and label noise. We propose a noiseresistant deep metric learning algo rithm, Probabilistic Rankingbased Instance Selection with Memory (PRISM), which works with both the memory bank approach and the classcenter approach. PRISM computes the probability that a label is clean based on the similarities between the data point and other data points using features extracted during the last several training iterations. This may be seen as modeling the posterior probability of the data label. For data points with high probability, we ex tract their features and insert them into the memory bank, which is used in subsequent model updates. In addition, we develop a smooth top R(sTRM) trick to adjust the thresh old for noisy data identiÔ¨Åcation as well as an acceleration technique that replaces individual data points with the class centers in the probability calculation. We perform extensive empirical evaluations on both syn thetic and real datasets. Inspired by the the ‚Äúnoise cluster‚Äù phenomenon observed from realworld data, we introduce the Small Cluster noise model to mimic openset noise in real data. Experimental results show that PRISM achieves superior performance compared to 12 existing DML and noiseresistant training techniques under symmetric noise, Small Cluster noise, and real noise. In addition, the accel 1arXiv:2103.16047v2  [cs.CV]  12 Apr 2021eration trick speeds up the algorithm by a factor of 6.9 on SOP dataset. The code and data are available at https: //github.com/alibabaedu/Rankingbased InstanceSelection . 2. Related Work "
162,Feature Diversity Learning with Sample Dropout for Unsupervised Domain Adaptive Person Re-identification.txt,"Clustering-based approach has proved effective in dealing with unsupervised
domain adaptive person re-identification (ReID) tasks. However, existing works
along this approach still suffer from noisy pseudo labels and the unreliable
generalization ability during the whole training process. To solve these
problems, this paper proposes a new approach to learn the feature
representation with better generalization ability through limiting noisy pseudo
labels. At first, we propose a Sample Dropout (SD) method to prevent the
training of the model from falling into the vicious circle caused by samples
that are frequently assigned with noisy pseudo labels. In addition, we put
forward a brand-new method referred as to Feature Diversity Learning (FDL)
under the classic mutual-teaching architecture, which can significantly improve
the generalization ability of the feature representation on the target domain.
Experimental results show that our proposed FDL-SD achieves the
state-of-the-art performance on multiple benchmark datasets.","Person reidentication (ReID) aims to match person images across multiple nonoverlapping cameras, which has achieved attention from both industry and academia. Most of the existing person ReID models along the supervised approach [20, 28, 21, 29, 30] have achieved satisfactory performance. However, these models generally perform less well in real applications because they have never been trained to adapt to the application scenes. To address this issue, a new problem referred as to unsu pervised domain adaptation becomes a hot topic in ReID task [47, 38, 23], which focuses on how to adapt a pretrained model from a labeled source domain to an unlabeled target domain. The main challenge of unsupervised domain adaptive person ReID lies in learning feature represen tation with unlabeled target domain data. To solve this challenge, one major line attempts to assign pseudo labels for target samples based on the pretrained model trained with labeled source samples [47, 10, 3, 43], and then netunes the model using the target samples with pseudo labels. Obviously, following this approach, the person ReID performance highly depends on the quality of pseudo labels. Therefore some works focus on obtaining highly dependable pseudo labels. Some of these works con centrate on the clustering process [10, 3, 19], in which target samples are assigned with pseudo labels based on dierent metrics. This process can improve the clustering result. Other works aim at how to eectively utilize target samples based on the clustering results [43, 44]. During the training pro cess of these methods, the pseudo labels with dierent reliability are assigned with dierent weights. However, due to the clustering results are unsatisfactory, all of these methods suer from noisy pseudo labels. Experimental results revealed that a small proportion of the samples are assigned with wrong pseudo labels frequently. These samples can be regarded as hard samples for the unsupervised domain adaptive person ReID task. As is well known, the performance of a welltrained model relies more on hard samples than easy samples. For the same reason, hard samples with stubborn wrong pseudo labels will limit the performance of the ReID model heavily. Unfortunately, existing unsupervised domain adaptive ReID methods can hardly solve this problem. Meanwhile, most of these models only apply supervised loss functions such as the Cross Entropy and Triplet loss based on the pseudo labels to train the model, but neglect unsupervised feature learning without groundtruth labels or pseudo labels, which hiddenly limits the generalization ability of the learned feature representation. To address these two problems, we propose a novel solution FDLSD to resist hard samples and learn features with better generalization ability in a united framework. In order to limit the ill in uence 1arXiv:2201.10212v1  [cs.CV]  25 Jan 2022of these hard samples, we propose a simple but powerful method which is called Sample Dropout (SD) to smooth the distribution of noisy pseudo labels. For most clusteringbased unsupervised domain adaptive ReID works, assigning pseudo labels for all target samples is employed before each ne tuning iterator. But in this paper, we randomly discard a proportion of target samples before each epoch of the training, through which the vicious circle of iterative training caused by hard samples can be broken. In addition to the proposed SD, we also present a new architecture to realize Feature Diversity Learning (FDL) in an unsupervised way, which is believed to suppress the ill eect of wrong pseudo labels and enhance the generalization ability of the feature representation. Overall, the main contributions of this paper can be summarized in three aspects: (1) We propose the Sample Dropout (SD) method to reduce the adverse eect of hard samples on domain adaptation, which can prevent some hard samples from being assigned with wrong pseudo labels all the time, thus breaking the vicious circle caused by these hard samples. (2) We propose the Feature Diversity Learning (FDL) and embed it into a dualbranch architecture to learn feature diversity representation in an unsupervised fashion, which can boost the generalization ability of model on target domain. (3) Extensive experiments on multiple benchmark datasets show that our proposed FDLSD achieves the stateoftheart performance, which demonstrates the eectiveness of our proposed approach. 2 Related Work "
163,Contextual Modulation for Relation-Level Metaphor Identification.txt,"Identifying metaphors in text is very challenging and requires comprehending
the underlying comparison. The automation of this cognitive process has gained
wide attention lately. However, the majority of existing approaches concentrate
on word-level identification by treating the task as either single-word
classification or sequential labelling without explicitly modelling the
interaction between the metaphor components. On the other hand, while existing
relation-level approaches implicitly model this interaction, they ignore the
context where the metaphor occurs. In this work, we address these limitations
by introducing a novel architecture for identifying relation-level metaphoric
expressions of certain grammatical relations based on contextual modulation. In
a methodology inspired by works in visual reasoning, our approach is based on
conditioning the neural network computation on the deep contextualised features
of the candidate expressions using feature-wise linear modulation. We
demonstrate that the proposed architecture achieves state-of-the-art results on
benchmark datasets. The proposed methodology is generic and could be applied to
other textual classification problems that benefit from contextual interaction.","Despite its fuzziness, metaphor is a fundamental feature of language that deÔ¨Ånes the relation be tween how we understand things and how we ex press them (Cameron and Low, 1999). A metaphor is a Ô¨Ågurative device containing an implied map ping between two conceptual domains. These do mains are represented by its two main components, namely the tenor (target domain) and the vehicle (source domain) (End, 1986). According to the conceptual metaphor theory (CMT) of Lakoff and Johnson (1980), which we adopt in this work, aconcept such as ‚Äúliquids‚Äù (source domain/vehicle) can be borrowed to express another such as ‚Äúemo tions‚Äù (target domain/tenor) by exploiting single or common properties. Therefore, the conceptual metaphor ‚ÄúEmotions are Liquids‚Äù can be mani fested through the use of linguistic metaphors such as‚Äúpure love‚Äù ,‚Äústir excitement‚Äù and‚Äúcontain your anger‚Äù . The interaction between the target and the source concepts of the expression is impor tant to fully comprehend its metaphoricity. Over the last couple of years, there has been an increasing interest towards metaphor process ing and its applications, either as part of natural language processing (NLP) tasks such as machine translation (Koglin and Cunha, 2019), text sim pliÔ¨Åcation (Wolska and Clausen, 2017; Clausen and Nastase, 2019) and sentiment analysis (Ren toumi et al., 2012) or in more general discourse analysis use cases such as in analysing political discourse (CharterisBlack, 2011), Ô¨Ånancial report ing (Ho and Cheng, 2016) and health communica tion (Semino et al., 2018). Metaphor processing comprises several tasks including identiÔ¨Åcation, interpretation and cross domain mappings. Metaphor identiÔ¨Åcation is the most studied among these tasks. It is concerned with detecting the metaphoric words or expressions in the input text and could be done either on the sentence, relation or word levels. The difference be tween these levels of processing is extensively stud ied in (Zayed et al., 2020). Identifying metaphors on the wordlevel could be treated as either se quence labelling by deciding the metaphoricity of each word in a sentence given the context or single word classiÔ¨Åcation by deciding the metaphoricity of a targeted word. On the other hand, relation level identiÔ¨Åcation looks at speciÔ¨Åc grammatical relations such as the dobj oramod dependencies and checks the metaphoricity of the verb or the adjective given its association with the noun. InarXiv:2010.05633v1  [cs.CL]  12 Oct 2020relationlevel identiÔ¨Åcation, both the source and target domain words (the tenor and vehicle) are classiÔ¨Åed either as a metaphoric or literal expres sion, whereas in wordlevel identiÔ¨Åcation only the source domain words (vehicle) are labelled. These levels of analysis (paradigms) are already estab lished in literature and adopted by previous re search in this area as will be explained in Sec tion 2. The majority of existing approaches, as well as the available datasets, pertaining to metaphor processing focus on the metaphorical usage of verbs and adjectives either on the word or relation levels. This is because these syntactic types ex hibit metaphoricity more frequently than others ac cording to corpusbased analysis (Cameron, 2003; Shutova and Teufel, 2010). Although the main focus of both the relation level and wordlevel metaphor identiÔ¨Åcation is dis cerning the metaphoricity of the vehicle (source do main words), the interaction between the metaphor components is less explicit in wordlevel analysis either when treating the task as sequence labelling or singleword classiÔ¨Åcation. Relationlevel analy sis could be viewed as a deeper level analysis that captures information that is not captured on the wordlevel through modelling the inÔ¨Çuence of the tenor (e.g.noun) on the vehicle (e.g. verb/adjective). There will be reasons that some downstream tasks would prefer to have such information (i.e. ex plicitly marked relations), among these tasks are metaphor interpretation and crossdomain map pings. Moreover, employing the wider context around the expression is essential to improve the identiÔ¨Åcation process. This work focuses on relationlevel metaphor identiÔ¨Åcation represented by verbnoun and adjectivenoun grammar relations. We propose a novel approach for contextbased textual classiÔ¨Å cation that utilises afÔ¨Åne transformations. In order to integrate the interaction of the metaphor compo nents in the identiÔ¨Åcation process, we utilise afÔ¨Åne transformation in a novel way to condition the neu ral network computation on the contextualised fea tures of the given expression. The idea of afÔ¨Åne transformations has been used in NLPrelated tasks such as visual questionanswering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019).Inspired by the works on visual reasoning, we use the candidate expression of certain grammat ical relations, represented by deep contextualised features, as an auxiliary input to modulate our com putational model. AfÔ¨Åne transformations can be utilised to process one source of information in the context of another. In our case, we want to inte grate: 1) the deep contextualisedfeatures of the candidate expression (represented by ELMo sen tence embeddings) with 2) the syntactic/semantic features of a given sentence. Based on this task, afÔ¨Åne transformations have a similar role to atten tion but with more parameters, which allows the model to better exploit context. Therefore, it could be regarded as a form of a more sophisticated at tention. Whereas the current ‚Äústraightforward‚Äù at tention models are overly simplistic, our model pri oritises the contextual information of the candidate to discern its metaphoricity in a given sentence. Our proposed model consists of an afÔ¨Åne trans form coefÔ¨Åcients generator that captures the mean ing of the candidate to be classiÔ¨Åed, and a neural network that encodes the full text in which the can didate needs to be classiÔ¨Åed. We demonstrate that our model signiÔ¨Åcantly outperforms the stateof theart approaches on existing relationlevel bench mark datasets. The unique characteristics of tweets and the availability of Twitter data motivated us to identify metaphors in such content. Therefore, we evaluate our proposed model on a newly introduced dataset of tweets (Zayed et al., 2019) annotated for relationlevel metaphors. 2 Related Work "
164,Autonomous Learning for Face Recognition in the Wild via Ambient Wireless Cues.txt,"Facial recognition is a key enabling component for emerging Internet of
Things (IoT) services such as smart homes or responsive offices. Through the
use of deep neural networks, facial recognition has achieved excellent
performance. However, this is only possibly when trained with hundreds of
images of each user in different viewing and lighting conditions. Clearly, this
level of effort in enrolment and labelling is impossible for wide-spread
deployment and adoption. Inspired by the fact that most people carry smart
wireless devices with them, e.g. smartphones, we propose to use this wireless
identifier as a supervisory label. This allows us to curate a dataset of facial
images that are unique to a certain domain e.g. a set of people in a particular
office. This custom corpus can then be used to finetune existing pre-trained
models e.g. FaceNet. However, due to the vagaries of wireless propagation in
buildings, the supervisory labels are noisy and weak.We propose a novel
technique, AutoTune, which learns and refines the association between a face
and wireless identifier over time, by increasing the inter-cluster separation
and minimizing the intra-cluster distance. Through extensive experiments with
multiple users on two sites, we demonstrate the ability of AutoTune to design
an environment-specific, continually evolving facial recognition system with
entirely no user effort.","Facial recognition and verification are key components of smart spaces, e.g., offices and buildings for determining who is where. Knowing this information allows a building management system to tailor ambient conditions to particular users, perform automated security (e.g., opening doors for the correct users without the need for a swipe card), and customize smart services (e.g., coffee dispens ing). A vast amount of research over the past decades has gone into designing tailored systems for facial recognition and with the advent of deep learning, progress has accelerated. As an example of a stateoftheart face recognizer, FaceNet achieves extremely high accuracies (e.g., 99.5%) on very challenging datasets through the use of a low dimensional embedding, allowing similar faces to be clustered through their Euclidean distance [ 14,26]. However, when directed transferred to operate in ‚Äòin the wild‚Äô, subject to variable lighting conditions, viewing angle and appearance changes, perfor mance of offtheshelf pretrained classifiers degrades significantly, with accuracies around 15% not being uncommon. The solution to this is to obtain a large, labelled corpus of data for a particular environment, with hundreds of annotated images per user. Given access to such a hypothetical dataset, it is then possible to finetune the pretrained classifier on outdomain data to adapt to the new environment and achieve excellent performance. However, the cost of labelling and updating the corpus (e.g. to enrol new users) is prohibitive for most critical applications and therefore, will naturally limit the use and uptake of facial recog nition as a ubiquitous technology in emerging Internet of Things (IoT) applications. On the other side, people often, but not always, carry smart devices (phones, fitness devices etc). Wang et al. [ 36] advocated that although these devices do not provide finegrained enough positioning capability to act as a proxy for presence, they can be used to indicate that a user might be present in an areaarXiv:1908.09002v1  [cs.CV]  14 Aug 2019with a colocated camera. In this work, we take a step forward and further utilize device presence as weak supervision signals for the purposes of finetuning a classifier. The goal now becomes how to take an arbitrary, pretrained recognition network and tune it from a generic classifier to a highly specific classifier, optimized for a certain environment and group of people. We note that the aim is to make the network better and better at this specific goal, but it would likely perform poorly if transferred directly to a different environment. This is the antithesis of the conventional view of generalized machine learning, but is ideally suited for the problem of environment specific facial recognition, as opposed to generic facial recognition. The technical challenge is that there is not a 1:1 mapping between a face and a wireless identity, rather we need to solve the association between a set of faces and a set of identities over many sessions or occasions. To further complicate the problem, the sets are not pure i.e. the set of faces can contain additional faces from people not of interest (e.g. visitors). Equally well, due to the vagaries of wireless transmission, the set of wireless identifiers will contain additional identifiers e.g. from people in the next office. Furthermore, it is also possible to have missing observations e.g. because a person was not facing the camera or because someone left their phone at home. In this work, we present AutoTune , a system which can be used to gradually improve the performance of facial recognition sys tems in the wild, with zero user effort, tailoring them to the visual specifics of a particular smart space. We demonstrate stateofthe art performance in realworld facial recognition through a number of experiments and trials. In particular, our contributions are: ‚Ä¢We observe and prove that wireless signals of users‚Äô de vices provide valuable, albeit noisy, clues for face recogni tion. Namely, wireless signals can serve as a weak label. Such weak labels can replace the human annotated face images in the wild to save intensive effort. ‚Ä¢We create AutoTune , a novel pipeline to simultaneously label face images in the wild and adapt the pretrained deep neural network to recognize the faces of users in new environments. The key idea is to repeat the faceidentity association and network update in tandem. To cope with observation noise, we propose a novel probabilistic framework in AutoTune and design a new stochastic center loss to enhance the robustness of network finetuning. ‚Ä¢We deployed AutoTune in two realworld environments and experimental results demonstrate that AutoTune is able to achieve >0.85F1score of image labeling in both environ ments, outperforming the best competing approach by >25%. Compared to the best competing approach, using the fea tures extracted from the finetuned model and training a classifier based on the crossmodality labeled images can give a‚àº19%performance gain for online face recognition. The rest of this paper is organized as follows. ¬ß2 introduces the background of this work. System overview is given in ¬ß3. We de scribe the AutoTune solution in ¬ß4 and ¬ß5. System implementation details are given in ¬ß6. The proposed approach is evaluated and compared with state of the art methods in ¬ß7. Finally, we discuss and outlook future work in ¬ß8 and conclude in ¬ß9.2 RELATED WORK "
165,Ex uno plures: Splitting One Model into an Ensemble of Subnetworks.txt,"Monte Carlo (MC) dropout is a simple and efficient ensembling method that can
improve the accuracy and confidence calibration of high-capacity deep neural
network models. However, MC dropout is not as effective as more
compute-intensive methods such as deep ensembles. This performance gap can be
attributed to the relatively poor quality of individual models in the MC
dropout ensemble and their lack of diversity. These issues can in turn be
traced back to the coupled training and substantial parameter sharing of the
dropout models. Motivated by this perspective, we propose a strategy to compute
an ensemble of subnetworks, each corresponding to a non-overlapping dropout
mask computed via a pruning strategy and trained independently. We show that
the proposed subnetwork ensembling method can perform as well as standard deep
ensembles in both accuracy and uncertainty estimates, yet with a computational
efficiency similar to MC dropout. Lastly, using several computer vision
datasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally
demonstrate that subnetwork ensembling also consistently outperforms recently
proposed approaches that efficiently ensemble neural networks.","An effective way to improve model accuracy and conÔ¨Ådence calibration in deep learning is ensembling. One efÔ¨Åcient technique that leverages this idea is ""Monte Carlo (MC) dropout"" [ 12] which extends the popular dropout technique used for regularization during training [ 42]. In MC Dropout, testtime inference involves multiple forward passes through the model, each executed with a different random dropout mask as in during the training phase. This yields an ensemble of predictions which are then averaged. It has been shown that MC dropout implements approximate Bayes averaging [ 12] and empirically yields enhanced uncertainty estimates and accuracy. While MC dropout can improve a baseline model, it is still inferior to explicit ensembles of neural networks trained independently with random initialization (called deep ensembles) [ 30]. Using the perspective of the errorambiguity decomposition [ 52], we can attribute this performance gap to the relatively poor performance of individual models and/or limited diversity in the MC dropout ensemble. We further hypothesize that these issues are largely due to the extensive parameter sharing among MC dropout models and the coupled training process. With this perspective in mind, we explore the idea of creating an ensemble of subnetworks in which a predetermined number of nonoverlapping dropout masks are used. We present an easytoimplement greedy optimization procedure that sequentially computes dropout masks via a recent dropoutmask optimization technique and trains each subnetwork independently. The resulting algorithm enables us to obtain a diverse ensemble of nonoverlapping subnetworks within one deep neural network. That is, we are able create many models out of one1. We experimentally demonstrate that subnetwork 1Hence our title: Ex uno plures. Preprint. Under review.arXiv:2106.04767v1  [cs.LG]  9 Jun 2021ensembling consistently outperforms MC dropout and several other recently proposed approaches that efÔ¨Åciently ensemble neural networks in terms of both accuracy and uncertainty estimates. We also show that our proposed approach achieves results on par with that of deep ensembles, yet with the much better testtime computational efÔ¨Åciency of MC dropout. Summary of Contributions. 1.We present the novel idea of ensembling nonoverlapping subnetworks within one standard neural network architecture. 2.We propose a simple sequential pruning based procedure to enhance the performance of subnetwork ensembling. 3.We demonstrate and discuss the regularization effect achieved by training pruned networks and using a randomized and frozen fully connected layer in the network. 4.Our experiments demonstrate that subnetwork ensembling outperforms MC dropout and several stateoftheart methods for efÔ¨Åcient ensembling. 2 Related Works "
166,Audio Visual Speech Recognition using Deep Recurrent Neural Networks.txt,"In this work, we propose a training algorithm for an audio-visual automatic
speech recognition (AV-ASR) system using deep recurrent neural network
(RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal
Classification (CTC) objective function. The frame labels obtained from the
acoustic model are then used to perform a non-linear dimensionality reduction
of the visual features using a deep bottleneck network. Audio and visual
features are fused and used to train a fusion RNN. The use of bottleneck
features for visual modality helps the model to converge properly during
training. Our system is evaluated on GRID corpus. Our results show that
presence of visual modality gives significant improvement in character error
rate (CER) at various levels of noise even when the model is trained without
noisy data. We also provide a comparison of two fusion methods: feature fusion
and decision fusion.","Audiovisual automatic speech recognition (AVASR) is a case of multimodal analysis in which two modalities (audio and visual) complement each other to recognize speech. Incorporating visual features, such as speaker's lip movements and facial expressions, into automatic speech recognition (ASR) systems has been shown to improve their performances especially under noisy conditions. To this end several methods have been proposed which traditionally include variants of GMM/HMM models[4][2]. More recently AVASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed. Endtoend speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM. The RNN trained with CTC directly learns a mapping between audio feature frames and char acter/phoneme sequences. This method eliminates the need for intermediate 1Version (Aug 2016) accepted in 4th International Workshop on Multimodal pattern recognition of social signals in human computer interaction(MPRSS 2016), a satellite event of the International Conference on Pattern Recognition (ICPR 2016)arXiv:1611.02879v1  [cs.CV]  9 Nov 2016Audio Visual Speech Recognition using Deep Recurrent Neural Networks GMM/HMM training thereby simplifying the training procedure. To our knowl edge, so far AVASR systems based on RNN trained with CTC have not been explored. In this work, we design and evaluate an audiovisual ASR (AVASR) system using deep recurrent neural network (RNN) and CTC objective function. The design of an AVASR system includes the tasks of visual feature engineering, and audiovisual information fusion. Figure 1 shows the AVASR pipeline at test time. This work mainly deals with the visual feature extraction and pro cessing steps and training protocol for the fusion model. Proper visual features are important especially in the case of RNNs as RNNs are dicult to train. Bot tleneck features used in tandem with audio features are known to improve ASR performance [5][10][24]. We employ a similar idea to improve the discriminatory power of video features. We show that this helps the RNN to converge prop erly when compared with raw features. Finally, we compare the performances of feature fusion and decision fusion methods. The paper is organized as follows: Section 2 presents the prior work on AV ASR. Bidirectional RNN and its training using CTC objective function are discussed in Section 3. Section 4 describes the feature extraction steps for audio and visual modalities. In section 5 dierent fusion models are explained. Section 6 explains the training protocols and experimental results. Finally, we summarize our work in 7. Fig. 1. Pipeline of AVASR system using feature fusion method 2 Related Work "
167,Fused Deep Neural Network based Transfer Learning in Occluded Face Classification and Person re-Identification.txt,"Recent period of pandemic has brought person identification even with
occluded face image a great importance with increased number of mask usage.
This paper aims to recognize the occlusion of one of four types in face images.
Various transfer learning methods were tested, and the results show that
MobileNet V2 with Gated Recurrent Unit(GRU) performs better than any other
Transfer Learning methods, with a perfect accuracy of 99% in classification of
images as with or without occlusion and if with occlusion, then the type of
occlusion. In parallel, identifying the Region of interest from the device
captured image is done. This extracted Region of interest is utilised in face
identification. Such a face identification process is done using the ResNet
model with its Caffe implementation. To reduce the execution time, after the
face occlusion type was recognized the person was searched to confirm their
face image in the registered database. The face label of the person obtained
from both simultaneous processes was verified for their matching score. If the
matching score was above 90, the recognized label of the person was logged into
a file with their name, type of mask, date, and time of recognition.
MobileNetV2 is a lightweight framework which can also be used in embedded or
IoT devices to perform real time detection and identification in suspicious
areas of investigations using CCTV footages. When MobileNetV2 was combined with
GRU, a reliable accuracy was obtained. The data provided in the paper belong to
two categories, being either collected from Google Images for occlusion
classification, face recognition, and facial landmarks, or collected in
fieldwork. The motive behind this research is to identify and log person
details which could serve surveillance activities in society-based
e-governance.","Face masks have been made mandatory during those days of pandemic. This has hindered the  identification of individuals in surveillance systems. Research has taken its path in  identifying a person by their individual face features even upon wearing a medica l face mask.  Although the main focus of attention has been only on medical masks, there are other types  of face occlusion, which also gain equal importance in the identification of a person. The  research work has been carried out taking into consideration other types of occluded faces,  initially being limited to four types to launch the face recognition model. The occlusions 2   include medical masks, objects, scarves, hands. Occluded face recognition has become an  outstanding problem in the domain of image pro cessing and computer vision to favor e  governance systems. Face detection has more impact in face recognition where very high  precision is preferred in terms of surveillance. In spite of a drastic development in the domain  of computer vision and machine le arning, a lot of issues related to occluded face detection are  still to be addressed. This has become a high interest area for computer science, where the  focus is not only on static images but also on video frames. Very high accuracy in image  classificati on and object detection has already been achieved [1,2]. However, the detection of  face occlusions is an extremely challenging task for the existing models of face detection  [7,8,9,10]. Even though many methods have been proposed and many are in use for fa ce  detection and occlusion detection, challenges still exist when it comes to video surveillance.  Problems occurring in existing systems are poor datasets and sometimes the existence of  noise due to masks/occlusions. These issues have been studied using li mited datasets  [11,12,13], and the current challenge is to develop an efficient face occlusion detection model  working for a vast dataset.   The model for face area detection proposed in this paper is done using OpenCV Deep Neural  Network (DNN) [3], TensorFl ow [4], Keras [5], and MobileNetV2 architecture [6], which is  used as an image classifier. The proposed model can be integrated into surveillance systems  for better person recognition, even under occlusion.   The main contributions of the paper include :  i. Realtime face occlusion detection through OpenCV DNN. Even faces under  different orientations and occlusions can be detected and the proposed model  outperforms the previous ones.   ii. Person identification with occluded faces.   iii. The fusion of RNN and DNN models to improve the accuracy of existing models.  A state oftheart technique is proposed, namely MobileNetV2 [6] with GRU [14]   component, to provide a precise classification.     The rest of the paper is structured as follows: Section 2 discusses the rela ted work, detailing  the recent technologies for face recognition and person identification; Section 3 discusses the  proposed approach through a fusion between Deep neural network (MobileNetV2) and  Recurrent Neural Network (GRU) accompanied with the ResNet backbone based Caffe  implementation on face detection; Section 4 presents the results and their discussion,  followed by a conclusion and future suggestions in Section 5.      2. Related Work   "
168,An Instance Transfer based Approach Using Enhanced Recurrent Neural Network for Domain Named Entity Recognition.txt,"Recently, neural networks have shown promising results for named entity
recognition (NER), which needs a number of labeled data to for model training.
When meeting a new domain (target domain) for NER, there is no or a few labeled
data, which makes domain NER much more difficult. As NER has been researched
for a long time, some similar domain already has well labelled data (source
domain). Therefore, in this paper, we focus on domain NER by studying how to
utilize the labelled data from such similar source domain for the new target
domain. We design a kernel function based instance transfer strategy by getting
similar labelled sentences from a source domain. Moreover, we propose an
enhanced recurrent neural network (ERNN) by adding an additional layer that
combines the source domain labelled data into traditional RNN structure.
Comprehensive experiments are conducted on two datasets. The comparison results
among HMM, CRF and RNN show that RNN performs bette than others. When there is
no labelled data in domain target, compared to directly using the source domain
labelled data without selecting transferred instances, our enhanced RNN
approach gets improvement from 0.8052 to 0.9328 in terms of F1 measure.","In recent years, Web data and knowledge man agement attracts the interests from industry and research Ô¨Åelds. There are various promising appli cations, such as intelligent recommendation, ma chine Question & Answer, knowledge graph and so on. Named entity recognition is a fundamental and very important step in the automatic informa tion extraction. A recognition method with high quality can directly improve the followup pro cessing results of Web data management products.NER research shows great successes in various domains and becomes a hot topic ( Sun et al. , 2016 ;Eiselt and Figueroa ,2013 ), such as so cial media ( Vavliakis et al. ,2013 ;Yao and Sun , 2016 ), language texts ( Karaa and Slimani ,2017 ), biomedicine ( Song et al. ,2016 ;Amith et al. , 2017 ), and so on. As we know, texts of different domains may vary from features, writing styles and structures. Domain NER meets a challenge that annotating data for new domains is labor intensive. For domain NER, a new domain (target domain) has no or a few labelled data. However, it is nat ural to think that if some similar domain (source domain) with enough lablled data already exists, it is possible to borrow some from this similar do main. Domain adaptation targets at transferring the source domain knowledge to the target do main ( Liu et al. ,2016 )) , which is a effective way to solve the problem of labelling large amount of data on new corpus or domains. In other words, if a NER modle is trained well for some Ô¨Åxed source domain, it is interesting to study how to deploy them across one or more different target domains. In this paper, we focus on domain NER for the politics text domain in Chinese high schools to support an automatic question and answer system (Q&A) that will take the national college entrance examination (NCEE) in the future. However there is no public politics text corpus used by Chinese high school for NER task. Moreover, there is no labeled data. We notice that People‚Äôs Daily corpus is free for public download and similar to politics text. Therefore, we propose an instance transfer based approach for domain NER with enhanced Recurrent Neural Network (RNN). First, we de sign an instance transfer strategy to extract similar sentences from the People‚Äôs Daily corpus (source domain). Here, instances mean labelled data and politics text is our target domain. Then, recurrent neural network (RNN) model can be trained based on the transferred instances. Moreover, we improve the traditional RNN model by enhancing its activation function and structure. Finally, an instance transfer enhanced RNN (ERNN) model is proposed to do NER for politics text target do main, which is trained based on the transferred instances from similar source domain (People‚Äôs daily corpus). Compared with the traditional RNN model, experimental results show that our ERNN with the instance transfer strategy can get im provement from 80.52% to 93.28% in terms of F1 measure. In addition, we consider other situation where a small number of labelled data is available to fur ther investigate the performance of our proposed approach. Since the politics text as target domain data is needed to obtain, we collect texts from high school books and relevant websites and manually label a very small set of them, making labeled data much less than the unlabeled one. Experimental results show that labeling data for target domain is quite useful, reaching at 92.13 in terms of F1 measure. However, our instance transfer based en hanced RNN can further improve the F1 value to 93.81. Finally, we adopt the cotraining approach using our proposed ERNN model and CRF by tak ing advantage of large unannotated target domain data, which get the F1 value of 94.02. The rest of the paper is organized as follows. In Section 2, we introduce related work on NER (especially on domain NER), recurrent neural net work and transfer learning. Section 3elaborates our approach including the instance transfer strat egy, our proposed ERNN. Experiments and results are given in Section 4, then we make the conclu sion and future work in Section i 5. 2 Related Work "
169,Approximating Wisdom of Crowds using K-RBMs.txt,"An important way to make large training sets is to gather noisy labels from
crowds of non experts. We propose a method to aggregate noisy labels collected
from a crowd of workers or annotators. Eliciting labels is important in tasks
such as judging web search quality and rating products. Our method assumes that
labels are generated by a probability distribution over items and labels. We
formulate the method by drawing parallels between Gaussian Mixture Models
(GMMs) and Restricted Boltzmann Machines (RBMs) and show that the problem of
vote aggregation can be viewed as one of clustering. We use K-RBMs to perform
clustering. We finally show some empirical evaluations over real datasets.","There has been considerable amount of work on learning when labeling is expensive, such as tech niques on transductive inference and active learning. With the emergence of crowdsourcing services, like Amazon Mechanical Turk, labeling costs in many applications have dropped dramatically. Large amounts of labeled data can now be gathered at low price. Due to a lack of domain expertise and misaligned incentives, however, labels provided by crowdsourcing workers are often noisy. To overcome the quality issue, each item is usually simultaneously labeled by several workers, and then we aggregate the multiple labels with some manner, for instance, majority voting. An advanced approach for label aggregation is suggested by Dawid and Skene[1]. They assume that each worker has a latent confusion matrix for labeling. The offdiagonal elements represent the probabilities that a worker mislabels an arbitrary item from one class to another while the diagonal elements correspond to her accuracy in each class. Worker confusion matrices and true labels are jointly estimated by maximizing the likelihood of observed labels. One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4]. The method of DawidSkene (1979) implicitly assumes that a worker performs equally well across all items in a common class. In practice, however, it is often the case that one item is more difÔ¨Åcult to label than another. To address this heterogeneous issue, Zhou et al.(2012)[5] propose a minimax entropy principle for crowdsourcing. It results in that each item is associated with a latent confusion vector besides a latent confusion matrix for each worker. Observed labels are determined jointly by worker confusion matrices and item confusion vectors through an exponential family model. Moreover, it turns out that the probabilistic labeling model can be equivalently derived from a natural assumption of objective measurements of worker ability and item difÔ¨Åculty. Such kinds of objectivity arguments have been widely discussed in the literature of mental test theory [6][7]. All the above approaches are for aggregating multiclass labels and In many scenarios, the labels are ordinal. Zhou et. al. (2014)[8] proposed a work to aggregate votes using minimax conditional entropy for ordinal labels. Most of the methods use statistical methods to aggregate the observed labels by transforming them to some probability or entropy measures. But, there has been no work that operates directly on the observed labels. We present a method to learn the aggregates of the votes using clustering. We Ô¨Årst show the formulation that allows us to use clustering as an approximation of the vote aggregationarXiv:1611.05340v2  [cs.LG]  17 Nov 2016stratagem. We Ô¨Årst draw a parallel between the Restricted Boltzmann Machine (RBM) learning and the Expectation Maximization (EM) algorithm of the DavidSkene algorithm and then show that GaussianSoftmax RBMs[9] can be approximated by a Gaussian Mixture Model (GMM), whose speciÔ¨Åc conditions lead to a direct mapping to the traditional Kmeans algorithm[10][11]. To then elucidate the clustering paradigm, we perform clustering using the KRBM model as proposed in [14]. 2 Related Work "
170,APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning.txt,"Practical natural language processing (NLP) tasks are commonly long-tailed
with noisy labels. Those problems challenge the generalization and robustness
of complex models such as Deep Neural Networks (DNNs). Some commonly used
resampling techniques, such as oversampling or undersampling, could easily lead
to overfitting. It is growing popular to learn the data weights leveraging a
small amount of metadata. Besides, recent studies have shown the advantages of
self-supervised pre-training, particularly to the under-represented data. In
this work, we propose a general framework to handle the problem of both
long-tail and noisy labels. The model is adapted to the domain of problems in a
contrastive learning manner. The re-weighting module is a feed-forward network
that learns explicit weighting functions and adapts weights according to
metadata. The framework further adapts weights of terms in the loss function
through a combination of the polynomial expansion of cross-entropy loss and
focal loss. Our extensive experiments show that the proposed framework
consistently outperforms baseline methods. Lastly, our sensitive analysis
emphasizes the capability of the proposed framework to handle the long-tailed
problem and mitigate the negative impact of noisy labels.","Deep Neural Networks (DNNs) have become the default modeling choice for complex problem with largescale labeled data. They have been remark ably successful in supervised learning across a vari ety of domains such as natural language processing and computer vision. Their success relies on the availability of a large amount of labelled data with high quality. In practice, it is usually expensive to acquire clean labels at scale. It either requires multiple blind passes and adjudicators decision or needs quality assurance by auditor. Both are laborintensive and timeconsuming. Recent progress on Ô¨Åne tuning (Devlin et al., 2019; Cui et al., 2018), domain adaptation (Tzeng et al., 2017; Xu et al., 2020; Ganin and Lempitsky, 2015; Xu et al., 2021) and fewshot learning (Brown et al., 2020) alleviate the demand for large volume labeled data. Deep learning models remain dependent on accurate la beled data, in spite of those progress. Another factor that compromises model gener alization is data distribution shift. Data shift can occur from various sources. Longtailed problem is a common example. For example, in order to train a model to classify shopping items, it is difÔ¨Åcult to obtain sufÔ¨Åcient images for rare products (He and McAuley, 2016). It is also a challenge to train a dialog model that is exposed to sufÔ¨Åcient less frequent topics or user intents. When training a model on an imbalanced dataset, model training becomes biased towards the majority classes. With higher number of examples available to learn from, the model learns to perform well on the majority classes but due to the lack of enough examples the model fails to learn meaningful patterns that could aid it in learning the minority classes. The model performance on those tail classes bottlenecks the applications of deep neural networks in practice and thus it is critical to improve on such cases. A number of studies have proposed approaches to mitigate noisy label or longtailed class prob lem. To alleviate impact of noisy label, sample selection (Jiang et al., 2018), label correction (Pa trini et al., 2017; Sanchez et al., 2019), and noise aware losses (Liu et al., 2022; Castells et al., 2020) have been studied. Dataset resampling such as SMOTE (Chawla et al., 2002) is a popular method by selecting a proportion of data to train a network or by learning a weight for each example. The weights are optimized by minimizing the training loss. It is applied in multiple wellknown algo rithms such as AdaBoost (Freund and Schapire, 1997), selfpaced learning (Kumar et al., 2010),arXiv:2302.03488v2  [cs.CL]  2 May 2023and algorithm that emphasizes high variance sam ples (Jiang et al., 2018; Chang et al., 2017). To address problem of longtailed distributions, some studies have modiÔ¨Åed sampling algorithm to en sure all classes are represented equally (Kub√°t and Matwin, 1997; Chawla et al., 2002). Other popular approaches include adjusting loss function (Menon et al., 2021) biased to minor class, and posthoc correction (Kang et al., 2020). However, those methods have contradicting as sumptions. On the one hand, we assign higher weight to clean labelled data in order to mitigate noisy label problem. On the other hand, algo rithms for longtailed problems emphasize minor ity classes that more likely have higher training loss. Therefore, those methods can not handle the problem of concurrent noisy label and longtailed classes. In order to handle noisy label and long tailed problem simultaneously, some works (Shu et al., 2021; Ren et al., 2018) propose a meta learning paradigm that follows a more natural as sumption that the best example weighting should minimize the loss of clean data. Those methods learn instance weights from a small clean dataset and show promising results. Nevertheless, we ar gue that the metalearning paradigm is not general enough, and remains expensive due to the require ment of a balanced meta validation dataset. In our work, we propose the Adaptive Pre training and Adaptive Meta Learning method (APAM), a general framework to handle the concur rent problems of noisy label and longtailed classes together. It naturally subsumes the aforementioned metalearning paradigm (Shu et al., 2021; Ren et al., 2018) as a special case. Our method does not require a balanced meta data to guide reweighting. This fact reduces the amount of clean data needed in meta learning and thus is more feasible and less expensive. Furthermore, we introduce a stage of domain adaptive pretraining (Gururangan et al., 2020) through contrastive learning (Gao et al., 2021) to improve model robustness. On the one hand, the proposed adaptive procedure give lower weights on noisy samples; on the other hand, APAM give higher weights to simultaneously handle the noise and longtailed problems. We further evaluate APAM on two datasets in which it outperforms other methods (Shu et al., 2021; Lin et al., 2017; Cui et al., 2019; Devlin et al., 2019; Gao et al., 2021). In addition, we conduct comprehensive ablation study and sensitivity analysis. Across those experiments, we observe APAM consistently outperforms other methods. To summarize, the main contributions of our work are listed as follows: ‚Ä¢To cope with the concurrent problems of longtail and noisy label in text classiÔ¨Åca tion, we propose a general twostage deep learning framework including domain adap tive pretraining stage and supervised Ô¨Åne tuning through adaptive reweighting stage. It outperforms the stateoftheart methods in evaluation datasets. ‚Ä¢We demonstrate that the proposed adaptive weighting method does not require balanced meta data and thus alleviate the dependence to large amount of meta data in longtailed problem. ‚Ä¢Our experiment results illustrate that do main adaptive contrastive learning consis tently leads to improvement of performance in APAM framework. ‚Ä¢Through a holistic ablation study and sensitiv ity analysis, we demonstrate the contribution of each components of the proposed method and the effectiveness of this method to long tailed problem with noisy label. 2 Related Work "
171,Is your noise correction noisy? PLS: Robustness to label noise with two stage detection.txt,"Designing robust algorithms capable of training accurate neural networks on
uncurated datasets from the web has been the subject of much research as it
reduces the need for time consuming human labor. The focus of many previous
research contributions has been on the detection of different types of label
noise; however, this paper proposes to improve the correction accuracy of noisy
samples once they have been detected. In many state-of-the-art contributions, a
two phase approach is adopted where the noisy samples are detected before
guessing a corrected pseudo-label in a semi-supervised fashion. The guessed
pseudo-labels are then used in the supervised objective without ensuring that
the label guess is likely to be correct. This can lead to confirmation bias,
which reduces the noise robustness. Here we propose the pseudo-loss, a simple
metric that we find to be strongly correlated with pseudo-label correctness on
noisy samples. Using the pseudo-loss, we dynamically down weight
under-confident pseudo-labels throughout training to avoid confirmation bias
and improve the network accuracy. We additionally propose to use a confidence
guided contrastive objective that learns robust representation on an
interpolated objective between class bound (supervised) for confidently
corrected samples and unsupervised representation for under-confident label
corrections. Experiments demonstrate the state-of-the-art performance of our
Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets
including curated data synthetically corrupted with in-distribution and
out-of-distribution noise, and two real world web noise datasets. Our
experiments are fully reproducible github.com/PaulAlbert31/SNCF","Standard supervised datasets for image classification us ing deep learning [ 15,7,20,14] are constituted by large amounts of images gathered from the web which have been True label guess  Turtle Turtle Turtle  Classification loss minimization  True label guess  Turtle Turtle Turtle  Alligator  Classification loss minimization Alligator Alligator Alligator  Pseudo loss filtering  xLabel noise robust algorithm Ours  Figure 1. Two stage label noise mitigation on detected noisy sam ples. Contrary to stateoftheart label noise robust algorithms, we filter out incorrect pseudolabels using the pseudoloss to avoid confirmation bias on incorrect corrections. heavily curated by multiple human annotators. In this paper, we propose to devise an algorithm which aims to train an accurate classification network on a web crawled dataset [ 19,32] where the human curation process was skipped. By doing so, the dataset creation time is greatly reduced but label noise becomes an issue [ 2] and can greatly degrade the classification accuracy [ 42]. To counter the effect of noisy annotations, previous contributions have fo cused on detecting the noisy samples using the natural robust ness of deep learning architectures to noise in early training stages [ 3,4]. These algorithms will identify noisy sam ples because they tend to be learned slower than their clean counterpart [ 17], because of incoherences with the labels of close neighbors in the feature space [ 23,18], a confident prediction from the neural net in a different class than the target class [ 38,21], inconsistent predictions across itera tions [ 22,34], and more. Once the noisy samples are identi fied, a corrected label is produced, yet ensuring that labels are correctly guessed is less studied in the label noise litera ture. Some propositions inspired by semisupervised learn ing [28,41] have been made recently by Li et al. [18] where only pseudolabels whose value in the max softmax bin (con fidence) is superior to a hyperparameter threshold value are kept or by Song et al. [29] where low entropy predictions indicate a confident pseudolabel. This paper proposes toarXiv:2210.04578v2  [cs.CV]  15 Oct 2022focus on the correction of noisy samples once they have been detected. We specifically propose a novel metric, the pseudo loss, which is able to retrieve correctly guessed pseudolabels and that we show to be superior to the pseudolabel confi dence previously used in the semisupervised literature. We find that incorrectly guessed pseudolabels are especially damaging to the supervised contrastive objectives that have been used in recent contributions [ 23,1,18]. We propose an interpolated contrastive objective between classconditional (supervised) for the clean or correctly corrected samples, where we encourage the network to learn similar represen tation for images belonging to the same class; and an unsu pervised objective for the incorrectly corrected noise. This results in P¬Øseudo L¬ØossS¬Øelection (PLS) a twostage noise detection algorithm where the first stage detects all noisy samples in the dataset while the second stage removes incor rect corrections. We then train a neural network to jointly minimize a classification and a supervised contrastive objec tive. We design PLS on synthetically corrupted datasets and validate our findings on two real world noisy web crawled datasets. Figure 1 illustrates our proposed improvement to label noise robust algorithms. Our contributions are: ‚Ä¢A twostage noise detection using a novel metric where we ensure that the corrected targets for noisy samples are accurate; ‚Ä¢A novel softly interpolated confidence guided con trastive loss term between supervised and unsupervised objective to learn robust features from all images; ‚Ä¢Extensive experiments of synthetically corrupted and webcrawled noisy datasets to demonstrate the perfor mance of our algorithm. 2. Related work "
172,Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement.txt,"Generic Image recognition is a fundamental and fairly important visual
problem in computer vision. One of the major challenges of this task lies in
the fact that single image usually has multiple objects inside while the labels
are still one-hot, another one is noisy and sometimes missing labels when
annotated by humans. In this paper, we focus on tackling these challenges
accompanying with two different image recognition problems: multi-model
ensemble and noisy data recognition with a unified framework. As is well-known,
usually the best performing deep neural models are ensembles of multiple
base-level networks, as it can mitigate the variation or noise containing in
the dataset. Unfortunately, the space required to store these many networks,
and the time required to execute them at runtime, prohibit their use in
applications where test sets are large (e.g., ImageNet). In this paper, we
present a method for compressing large, complex trained ensembles into a single
network, where the knowledge from a variety of trained deep neural networks
(DNNs) is distilled and transferred to a single DNN. In order to distill
diverse knowledge from different trained (teacher) models, we propose to use
adversarial-based learning strategy where we define a block-wise training loss
to guide and optimize the predefined student network to recover the knowledge
in teacher models, and to promote the discriminator network to distinguish
teacher vs. student features simultaneously. Extensive experiments on
CIFAR-10/100, SVHN, ImageNet and iMaterialist Challenge Dataset demonstrate the
effectiveness of our MEAL method. On ImageNet, our ResNet-50 based MEAL
achieves top-1/5 21.79%/5.99% val error, which outperforms the original model
by 2.06%/1.14%. On iMaterialist Challenge Dataset, our MEAL obtains a
remarkable improvement of top-3 1.15% (official evaluation metric) on a strong
baseline model of ResNet-101.","THEmodel ensemble approach is a collection of neural networks whose predictions are combined at test stage by weighted averaging or voting. It has been long observed that ensembles of multiple networks are generally much more robust and accurate than a single network if the training data is noisy and intractable to handle. This beneÔ¨Åt has also been exploited indirectly when training a single network through Dropout [3], Dropconnect [4], Stochastic Depth [5], Swapout [6], etc. We extend this idea by forming Zhiqiang Shen, Yutong Zheng, Chenchen Zhu and Marios Savvides are with the Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, P A 15213, USA. Email: fzhiqians, yu tongzh, chenchez, marioss g@andrew.cmu.edu. Zhankui He is with the Department of Computer Science, University of California San Diego, CA 92093, USA. Email: zhh004@eng.ucsd.edu. Wanyun Cui is with Shanghai University of Finance and Economics. E mail: cui.wanyun@sufe.edu.cn. Jiahui Yu is with the Department of Electrical and Computer Engineering, University of Illinois at UrbanaChampaign, Illinois, IL 61801, USA. E mail: jyu79@illinois.edu.ensemble predictions during training, using the outputs of different network architectures with different or identical augmented input. Our testing still operates on a single network, but the supervision labels made on different pre trained networks correspond to an ensemble prediction of a group of individual reference networks. The traditional ensemble, or called true ensemble, has some disadvantages that are often overlooked. 1) Redun dancy: The information or knowledge contained in the trained neural networks are always redundant and has over laps between with each other. Directly combining the pre dictions often requires extra computational cost but the gain is limited. 2) Ensemble is always large and slow: Ensemble requires more computing operations than an individual network, which makes it unusable for applications with limited memory, storage space, or computational power such as desktop, mobile and even embedded devices, and for applications in which realtime predictions are needed. To address the aforementioned shortcomings, in this paper we propose to use a learningbased ensemble method. Our goal is to learn an ensemble of multiple neural networksarXiv:1908.08520v1  [cs.CV]  22 Aug 20192 (a) Standard  (b) Ours Fig. 1: Visualizations of validation images from the Ima geNet dataset [1] by tSNE [7]. We randomly sample 10 classes within 1000 classes. Left is the single model result using the standard training strategy. Right is our MEAL ensemble model result. without incurring any additional testing costs , as shown in Fig. 2. We achieve this goal by leveraging the combination of diverse outputs from different neural networks as su pervisions to guide the target network training. The refer ence networks are called Teachers and the target networks are called Students . Instead of using the traditional one hot vector labels, we use the softlabels that provide more coverage for cooccurring and visually related objects and scenes. We argue that labels should be informative for the speciÔ¨Åc image. In other words, the labels should not be identical for all the given images with the same class. More speciÔ¨Åcally, as shown in Fig. 3, an image of ‚Äútobacco shop‚Äù has similar appearance to ‚Äúlibrary‚Äù should have a different label distribution than an image of ‚Äútobacco shop‚Äù but is more similar to ‚Äúgrocery store‚Äù. It can also be observed that soft labels can provide the additional intra and inter category relations of datasets. To further improve the robustness of student networks, we introduce an adversarial learning strategy to force the student to generate similar outputs as teachers. We propose two different strategies for the generative adversarial train ing: (i) joint training with a uniÔ¨Åed framework; and (ii) al ternately update gradients with separate training processes, i.e., updating the gradients in discriminator and student network iteratively. To the best of our knowledge, there are very few existing works adopting generative adversarial learning to force the student networks to have similar distri bution outputs with the teachers, so our proposed method is a pioneer of this direction for multimodel ensemble. Our experiments show that MEAL consistently improves the accuracy across a variety of popular network architectures on different datasets. For instance, our shakeshake [8] based MEAL achieves 2.54% test error on CIFAR10, which is a relative 11:2%improvement1. On ImageNet, our ResNet 50 based MEAL achieves 21.79%/5.99% val error, which outperforms the baseline by a large margin. Furthermore, we extend our method to the problem of noisy data processing. We propose an iterative reÔ¨Ånement paradigm based on our MEAL method, which can reÔ¨Åne the labels from the teacher networks progressively and provide more accurate supervisions for the student network training. We conduct experiments on iMaterialist Challenge 1. Shakeshake baseline [8] is 2.86%. 1 2 3 4 5 # of ensembles0√ó1√ó2√ó3√ó4√ó5√ó6√óFLOPs FLOPs at Inference Time Snapshot Ensemble (Huang et al. 2017) Our FLOPs at Test TimeFig. 2: Comparison of FLOPs at inference time. Huang et al. [10] employ models at different local minimum for ensembling, which enables no additional training cost, but the computational FLOPs at test time linearly increase with more ensembles. In contrast, our method use only one model during inference time throughout, so the testing cost is independent of # ensembles. Dataset and the results show that our method can vastly improve the performance of base models. To explore what our model actually learned, we visualize the embedded features from the single model and our ensembling model. The visualization is plotted by tSNE tool [7] with the last convlayer features (2048 dimensions) from ResNet50. We randomly sample 10 classes on Ima geNet, results are shown in Fig. 1, it‚Äôs obvious that our model has better feature embedding result. In summary, our contribution in this paper is three fold. An endtoend framework with adversarial learning is designed based on the teacherstudent learning paradigm for deep neural network ensembling and noisy data learning. The proposed method can achieve the goal of ensem bling multiple neural networks with no additional testing cost . The proposed method improves the stateoftheart accuracy on CIFAR10/100, SVHN, ImageNet and iMaterialist Challenge Dataset for a variety of exist ing network architectures. A preliminary version of this manuscript [9] has been published in a previous conference. In this version, we involved and compared two different gradient update strategies for adversarial learning on our proposed MEAL framework. We also provided a novel learning paradigm for how to adopt our method on handling noisy date cir cumstances. Furthermore, we included more experiments, details, analysis and an iterative reÔ¨Ånement strategy with better performance. Currently, there are few works focus ing on adopting generative adversarial learning on feature space for learning identical distributions between teacher and student networks. Thus, this work gives very good and practical guidelines for multimodel learning/ensemble and noisy data reÔ¨Ånement. 2 R ELATED WORK "
173,CascadeML: An Automatic Neural Network Architecture Evolution and Training Algorithm for Multi-label Classification.txt,"Multi-label classification is an approach which allows a datapoint to be
labelled with more than one class at the same time. A common but trivial
approach is to train individual binary classifiers per label, but the
performance can be improved by considering associations within the labels. Like
with any machine learning algorithm, hyperparameter tuning is important to
train a good multi-label classifier model. The task of selecting the best
hyperparameter settings for an algorithm is an optimisation problem. Very
limited work has been done on automatic hyperparameter tuning and AutoML in the
multi-label domain. This paper attempts to fill this gap by proposing a neural
network algorithm, CascadeML, to train multi-label neural network based on
cascade neural networks. This method requires minimal or no hyperparameter
tuning and also considers pairwise label associations. The cascade algorithm
grows the network architecture incrementally in a two phase process as it
learns the weights using adaptive first order gradient algorithm, therefore
omitting the requirement of preselecting the number of hidden layers, nodes and
the learning rate. The method was tested on 10 multi-label datasets and
compared with other multi-label classification algorithms. Results show that
CascadeML performs very well without hyperparameter tuning.","Inmultilabel classiÔ¨Åcation problems a datapoint can be assigned to more than one class, or label, simultaneously [12]. For example, an image can be clas siÔ¨Åed as containing multiple diÔ¨Äerent objects, or music can be labelled with more than one genre. This contrasts with multiclass classiÔ¨Åcation problems in which objects can only belong to a single class. Multilabel classiÔ¨Åcation algo rithms either break the multilabel problem down into smaller multiclass clas siÔ¨Åcation problems‚Äîfor example classiÔ¨Åer chains [23]‚Äîand are known as prob lem transformation methods‚Äîor modify multiclass algorithms to directly train ?This research was supported by Science Foundation Ireland (SFI) under Grant Num ber SFI/12/RC/2289.arXiv:1904.10551v1  [cs.LG]  23 Apr 20192 Pakrashi and Mac Namee on multilabel datasets‚Äîfor example BackPropagation in MultiLabel Learning (BPMLL) [37]‚Äîand are known as algorithm adaptation methods. Automatic machine learning [8], orAutoML, approaches have seen a recent resurgence of interest as researchers look for ways to automatically select opti mal algorithms, features, model architectures, and hyperparameters for machine learning tasks. The AutoML research community has, however, paid very little attention to multilabel classiÔ¨Åcation problems, although there have been some recent eÔ¨Äorts [25,26,33]. TheCascade2 algorithm [21] is an interesting neural network approach that learns model parameters and model architecture at the same time. In Cascade2, which is based on the cascade correlation neural network approach [7], train ing starts with a simple perceptron network, which is grown incrementally by adding new cascaded layers with skiplevel connections as long as performance on a validation dataset improves. Weights in each new layer are trained inde pendently of the overall network which greatly reduces the processing burden of this approach. This paper proposes CascadeML , a new AutoML solution for multilabel classiÔ¨Åcation problems, that is inspired by the Cascade2 algorithm and BPMLL. Improvements are made to both components leading to an implementation that requires minimal hyperparameter or network architecture tuning. In a series of evaluation experiments this approach has been shown to perform very well without the extensive hyperparameter tuning required by stateoftheart multi label classiÔ¨Åcation methods. To the best of authors‚Äô knowledge this is the Ô¨Årst automatic neural network architecture selection and training approach for multi label classiÔ¨Åcation methods. Theremainderofthepaperisstructuredasfollows.Section2discussestheex isting literature including a formal deÔ¨Ånition of multilabel classiÔ¨Åcation and the BPMLL algorithm. Section 2.3 describes the cascade neural network approach and, speciÔ¨Åcally, the Cascade2 algorithm. The proposed CascadeML method is then presented in Section 3. The design of an experiment to evaluate the perfor mance of the CascadeML algorithm, and benchmarking its performance against stateoftheart multilabel classiÔ¨Åcation approaches is described in Section 4. Section 5 presents and discusses the results of this experiment. Finally, Section 6 discusses future research directions and concludes the paper. 2 Related Work "
174,Learning advisor networks for noisy image classification.txt,"In this paper, we introduced the novel concept of advisor network to address
the problem of noisy labels in image classification. Deep neural networks (DNN)
are prone to performance reduction and overfitting problems on training data
with noisy annotations. Weighting loss methods aim to mitigate the influence of
noisy labels during the training, completely removing their contribution. This
discarding process prevents DNNs from learning wrong associations between
images and their correct labels but reduces the amount of data used, especially
when most of the samples have noisy labels. Differently, our method weighs the
feature extracted directly from the classifier without altering the loss value
of each data. The advisor helps to focus only on some part of the information
present in mislabeled examples, allowing the classifier to leverage that data
as well. We trained it with a meta-learning strategy so that it can adapt
throughout the training of the main model. We tested our method on CIFAR10 and
CIFAR100 with synthetic noise, and on Clothing1M which contains real-world
noise, reporting state-of-the-art results.","Modern image classication systems are based on using deep neural network models that are trained on a huge number of labeled images [11]. Due to the extreme cost of labeling such an amount of images and diculty in covering many concepts, researchers recently have looked into methods that generate labels automatically. One signicant line of research exploits available labeled images from nonexperts (e.g. from social networks, online stores) that can be easily retrieved in large quantities but may have been mislabeled [1]. Deep neural networks typically consist of a large number of parameters that are highly shared among feature dimensions and states, enabling  exibility in learning dierent tasks and classes. This  exibility has the advantage to lead to strong discriminative models unless data annotations are corrupted by noise, leading to performance reduction and overtting problems [9]. Recent methods tried to address the problem by using curriculum learning [4], directly estimatingarXiv:2211.04177v1  [cs.CV]  8 Nov 20222 S. Ricci et al. the labels noise in the set [8], or measuring the condence of the network during training [12], also using another cotrained network [7]. The idea was usually to understand mislabeled samples out of distribution and reduce their in uence on the learning by dampening their loss or decreasing their impact directly from the training set. In this paper, we proposed a metalearning approach to address the problem of noisy labels in image classication based on an advisor network, developed to help the classier. While a standard image classication model is trained, the advisor network observes the main network activations and adjusts features at training time when noisy label images are identied as input. This allows the classier model to get information even from mislabeled samples where some noise structure is present. We only retained the main model as the nal classi er, while the advisor was discarded. Unlike the teacherstudent paradigm, the advisor network was not trained to solve the image classication task, but only to help the learning process of the classier model by its altering activations. In summary, our contribution is: {We propose the use of an advisor network, i.e. the use of an additional net work at training time, learned by metalearning, that can adjust activations and gradient of the main network that is being trained. {We develop such concept for the task of image classication, allowing the training of an image classication network in presence of articial label noise. {We test our approach in presence of articial label noise and on a popular noisy dataset, obtaining stateoftheart performance. 2 Related works "
175,Clickbait Identification using Neural Networks.txt,"This paper presents the results of our participation in the Clickbait
Detection Challenge 2017. The system relies on a fusion of neural networks,
incorporating different types of available informations. It does not require
any linguistic preprocessing, and hence generalizes more easily to new domains
and languages. The final combined model achieves a mean squared error of
0.0428, an accuracy of 0.826, and a F1 score of 0.564. According to the
official evaluation metric the system ranked 6th of the 13 participating teams.","Clickbait refers to headlines of web content targeting the human ‚Äúcuriosity gap‚Äù [ 13]. The reader is typically lured into clicking a targetlink by raising interest into the advertised story mentioned in the teaser message, without providing enough details to satisfy the readers curiosity. Such clickbaitlinks often contain videos, picture galleries, or simple listings. The content is mostly of little journalistic quality, but spreads well in social media by referring to soft topics. Content describing such content ( e.g., gossip, food news, or sensational stories) is often observed in tabloid newspapers. The conversion of a newspaper into tabloid format (also referred to as tabloidization) is often considered problematic [ 19]. However, there are also online magazines that provide clickbait titles on more serious topics. According to an analysis all of the top 20 most proliÔ¨Åc English news publishers on Twitter occasionally publish clickbait headlines [ 16]. Depending on the newspaper, percentages of clickbait content ranges from 8 % to an astonishing 51 % In this publication we describe our approach in the Clickbait Detection Challenge 2017 [ 17] to detect clickbait headlines using neural networks. 2. RELATED WORK "
176,Noise-Resilient Ensemble Learning using Evidence Accumulation Clustering.txt,"Ensemble Learning methods combine multiple algorithms performing the same
task to build a group with superior quality. These systems are well adapted to
the distributed setup, where each peer or machine of the network hosts one
algorithm and communicate its results to its peers. Ensemble learning methods
are naturally resilient to the absence of several peers thanks to the ensemble
redundancy. However, the network can be corrupted, altering the prediction
accuracy of a peer, which has a deleterious effect on the ensemble quality. In
this paper, we propose a noise-resilient ensemble classification method, which
helps to improve accuracy and correct random errors. The approach is inspired
by Evidence Accumulation Clustering , adapted to classification ensembles. We
compared it to the naive voter model over four multi-class datasets. Our model
showed a greater resilience, allowing us to recover prediction under a very
high noise level. In addition as the method is based on the evidence
accumulation clustering, our method is highly flexible as it can combines
classifiers with different label definitions.","Ensemble Learning [1] methods combine several algorithms performing the same task to obtain a betterquality group. Ensemble learning methods play on diverse group aspects: the number of algorithms [ 2,3], their weighting based on their contribution [4, 5, 6], and their selection based on their diversity [7]. Ensemble learning methods are well adapted to the distributed setup, where several machines host each a single algorithm and send their results to a central node aggregating the results [ 8,9,10]. They can be adapted to decentralized peertopeer networks [ 9], where a dynamic group collaborate to improve its accuracy by electing a leader or by aggregating the group‚Äôs results. Distributed systems are prone to network failures, where communications are broken between some nodes, or corrupted with noise [ 11]. In addition, nodes can change of behavior if controled by malicious entities. Ensemble methods are resilient to the absence of one or more weak learners thanks to group redundancy [ 7,12]. However, the corruption of a learner‚Äôs predictions is equivalent to a negative change of accuracy, which has a deleterious effect on the group quality. Thus, there are two ways to deal with corrupted computers: detecting inaccurate peers to avoid data pollution or resilience to error. The detection can be done using network monitoring methods, or exploiting trust to weigh peers based on their past contributions. However, this approach is not adapted to a dynamic environment such as a peertopeer network where a peer lifetime is very short and may change temporary behavior. In contrast, being resilient to error is more suitable as all inputs are accepted but more challenging to design as it requires smart correction algorithms.arXiv:2110.09212v1  [cs.LG]  18 Oct 2021NoiseResilient EL using EAC In this article, we propose a noiseresilient ensemble classiÔ¨Åcation method, correcting errors while improving accuracy. The method uses the Evidence Accumulation Clustering approach to rectify class boundaries and correct corrupted labels by performing a local weighted vote. The approach was tested under several noise condition over four datasets and tolerated high noise levels without accuracy degradation. This paper is structured as follows. The Ô¨Årst section presents the related works regarding ensemble learning methods and resilience to error. The second section details the proposed ensemble classiÔ¨Åcation method. The datasets and the classiÔ¨Åers‚Äô setup are detailed in the experimental section, followed by the results. Finally, the paper ends with a discussion and a conclusion. 2 Related Works "
177,Document Domain Randomization for Deep Learning Document Layout Extraction.txt,"We present document domain randomization (DDR), the first successful transfer
of convolutional neural networks (CNNs) trained only on graphically rendered
pseudo-paper pages to real-world document segmentation. DDR renders
pseudo-document pages by modeling randomized textual and non-textual contents
of interest, with user-defined layout and font styles to support joint learning
of fine-grained classes. We demonstrate competitive results using our DDR
approach to extract nine document classes from the benchmark CS-150 and papers
published in two domains, namely annual meetings of Association for
Computational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to
conditions of style mismatch, fewer or more noisy samples that are more easily
obtained in the real world. We show that high-fidelity semantic information is
not necessary to label semantic classes but style mismatch between train and
test can lower model accuracy. Using smaller training samples had a slightly
detrimental effect. Finally, network models still achieved high test accuracy
when correct labels are diluted towards confusing labels; this behavior hold
across several classes.","Fast, lowcost production of consistent and accurate training data enables us to use deep convolutional neural networks (CNN) to downstream document understanding [13,37,42,43]. However, carefully annotated data are difÔ¨Åcult to obtain, especially for document layout tasks with large numbers of labels (timeconsuming annotation) or with Ô¨Ånegrained classes (skilled annotation). In the scholarly document genre, a variety of document formats may not be attainable at scale thus causing imbalanced samples, since authors do not always follow section and format rules [ 10,28]. Different communities (e. g., computational linguistics vs. machine learning, or computer science vs. biology) use different structural and semantic organizations of sections and subsections. ThisarXiv:2105.14931v1  [cs.CV]  20 May 20212 Ling et al. Table and Figure  caption tags  Formatted  at diÔ¨Äerent  locationsExistence of graphics  components Document Domain Randomization: Training Data Generation:  Diverse Ô¨Ågure | table | algorithm | equation style;  Randomized text and page render. Texts in  paragraphs and  section titles. Font type, font  size, Italic or not,  bold or notFigure width relative to  the text / column width  is randomized. Text length; distances to  the Ô¨Ågure and the  subsequent paragraphs. Existence of  caption Fig. 1: Illustration of our document domain randomization (DDR) approach . A deep neural network(CNN)based layout analysis using training pages of 100% ground truth bounding boxes generated solely on simulated pages: lowÔ¨Ådelity textual content and images pasted via constrained layout randomization of Ô¨Ågure/table/algorithm/equa tion, paragraph and caption length, column width and height, twocolumn spacing, font style and size, captioned or not, title height, and randomized texts. Nine classes are used in the real document layout analysis with no additional training data: abstract , algorithm ,author ,bodytext ,caption ,equation ,Ô¨Ågure ,table , and title. Here the colored texts illustrate the semantic information; all text in the training data is black. diversity forces CNN paradigms (e. g., [ 36,43]) to use millions of training samples, sometimes with signiÔ¨Åcant amounts of noise and unreliable annotation. To overcome these training data production challenges, instead of the timeconsuming manual annotating of real paper pages to curate training data, we generate pseudopages by randomizing page appearance and semantic content to be the ‚Äúsurrogate‚Äù of training data. We denote this as document domain randomization (DDR ) (Fig. 1). DDR uses simulationbased training document generation, akin to domain randomization (DR) in robotics [ 20,34,40,41] and computer vision [ 15,29]. We randomize layout and font styles and semantics through graphical depictions in our page generator. The idea is that with enough page appearance randomization, the real page would appear to the model as just another variant. Since we know the boundingbox locations while rendering the training data, we can theoretically produce any number of highly accurate ( 100% ) training sam ples following the test data styles. A key question is what styles and semantics can be randomized to let the models learn the essential features of interest on pseudopages so as to achieve comparable results for label detection in real article pages. We address this question and study the behavior of DDR under numerous attribution settings to help guide the training data preparation. Our contributions are that we:Document Domain Randomization for Deep Learning Document Layout Extraction 3 ‚Äì Create DDR‚Äîa simple, fast, and effective training page preparation method to signiÔ¨Åcantly lower the cost of training data preparation. We demonstrate that DDR achieves competitive performance on the commonly used benchmark CS 150 [ 11], ACL300 of Association for Computational Linguistics (ACL), and VIS300 of IEEE visualization (VIS) on extracting nine classes. ‚Äì Cover realworld page styles using randomization to produce training samples that infer realworld document structures. HighÔ¨Ådelity semantics is not needed for document segmentation, and diversifying the font styles to cover the test data improved localization accuracy. ‚Äì Show that limiting the number of available training samples can lower detec tion accuracy. We reduced the training samples by half each time and showed that accuracy drops at about the same rate for all classes. ‚Äì Validated that CNN models remained reasonably accurate after training on noisy class labels of composed paper pages. We measured noisy data labels at 1‚Äì10% levels to mimic the realworld condition of human annotation with partially erroneous input for assembling the document pages. We show that standard CNN models trained with noisy labels remain accurate on numerous classes such as Ô¨Ågures, abstract, and bodytext. 2 Related Work "
178,Robustness study of noisy annotation in deep learning based medical image segmentation.txt,"Partly due to the use of exhaustive-annotated data, deep networks have
achieved impressive performance on medical image segmentation. Medical imaging
data paired with noisy annotation are, however, ubiquitous, but little is known
about the effect of noisy annotation on deep learning-based medical image
segmentation. We studied the effects of noisy annotation in the context of
mandible segmentation from CT images. First, 202 images of Head and Neck cancer
patients were collected from our clinical database, where the organs-at-risk
were annotated by one of 12 planning dosimetrists. The mandibles were roughly
annotated as the planning avoiding structure. Then, mandible labels were
checked and corrected by a physician to get clean annotations. At last, by
varying the ratios of noisy labels in the training data, deep learning-based
segmentation models were trained, one for each ratio. In general, a deep
network trained with noisy labels had worse segmentation results than that
trained with clean labels, and fewer noisy labels led to better segmentation.
When using 20% or less noisy cases for training, no significant difference was
found on the prediction performance between the models trained by noisy or
clean. This study suggests that deep learning-based medical image segmentation
is robust to noisy annotations to some extent. It also highlights the
importance of labeling quality in deep learning","Deep supervised networks have achieved impressive performance in medical image segmentation partly due to the  use of high quali ty exhaustive annotated data (Hesamian  et al. , 2019 ; Chen  et al. , 2019 ; Liu et al. , 2017 ). However,  in radiation oncology, it is hard or impossible to conduct sufficient high quality image annotation. Besides potential  hurdles of funding acquisitions, time cost and patient privacy, accurate ann otation of medical images always requires  scarce and  expensive medical expertise (Greenspan  et al. , 2016 ) and thereby, medical imaging data paired with noisy  annotation is prevalent, particularly in radiation oncology.    2                                                     S. Yu  et al.   2   An increasing attention has been  paid to the issue of label noise in deeply supervised image classification (Han et  al., 2019 ; Hendrycks  et al. , 2018 ; Tanaka  et al. , 2018 ). These approaches to tackle the label noise could be generally  categorized into two g roups. One tends to analyze the label noise and to develop deep networks with noise robust  loss functions. Reed et al propose d a generic way to tackle inaccurate labels by augmenting the prediction objective  function with a noti on of perceptual consistency  (Reed  et al. , 2014 ). The consistency was defined as the confidence  of predicted labels between different objective estimation computed from the same input data. Further, the authors   introduce d a convex combination of the known labels and predicted labels as the training target in self learning.  Patrini et al present ed two procedures for loss function correction , and both the application domain and the network  architecture  were unknown  (Patrini  et al. , 2017 ). The computing cost is at most a matrix inversion and multiplication.  Both procedures were  prove n to be robust to the noisy data , and importantly, the Hessian of the loss function was  found independent from label noise for the ReLU networks. By generalizing t he categorical cross entropy, Zhang and  Sabuncu develop ed a theoretically grounded set of noise robust loss functions (Zhang and Sabuncu, 2018 ). These  functions could be embedded into any deep ne tworks to yield good performance in a wide range of noisy label  scenarios. And n otably, Luo et al design ed a variance regularization term to penali ze the Jacobian norm of a deep  netwo rk on the whole training set (Luo et al. , 2019 ). Both theoretical ly deduc ed and experimental results show ed that  the regul arization term can decrease the subspace dimensionality, improve the robustness , and generalize well to  label noise.  However,  these approaches require prior knowledge or an accurate estimation of the label noise  distribution , which is not practical in real world applications. The other group tends to figure out and to remove or  correct noisy labels by using a small set of clean data. Misra et al demonstrate d that noisy labels from human centric  annotation are statistically dep endent on the data , and thus, clean labels could be learnt to decouple this kind of  human reporting bias and to improve i mage captioning performance (Misra  et al. , 2016 ). Xiao et al introduce d a  general framework to train deep networks with a limited number of clean samples an d massive  noisy samples (Xiao   et al. , 2015 ). The relationships among  images, class labels , and label noises were  quantified with a probabilistic  graphical model , which was further integrated into an end toend deep learning system. Mirikharaji et al propose d a  practical framework to learn from a limited number of clean samples in the training phase that assign ed higher  weights to pixels with gradient directions closer to tho se of clean data in a meta learning approach (Mirikharaji  et al. ,  2019 ). This kind of approaches  is feasible  but significantly increase s the computing complexity.     Many efforts have been made to tackle label noise in image classification, while little is known about the effect of  annotation quality on object segmentation. Object segmentation can be viewed as pixel wise image classification and  requires highquality exhaustive annotated data for algorithm training . However, in radiation oncology, some organs  atrisk (OARs)  may be roughly  annotated due to the trade off between time spent  and radiati on treatment planning   quality . Such  rough annotation s can mislead deep network  training and result in ambiguous localization of  anatomical structures. This study concerns t he effect of annotation quality on medical image segmentation . It  involves  medical  image  data annotated by dosimetrists in radiation treatment planning and differ s from the 3                                                     S. Yu  et al.   3 aforementioned studies , which  artificially generate noisy labels  and do not reflect reallife scenarios. The primary  purpose of this study is to investigate  whether a deep network trained with noisy data can achieve comparative  performance as that trained with clean data. Specifically, the effect of different ratios  of noisy cases in the training  data is investigated in the context of deep learning based mandi ble image segmentation.     2. Methods and M aterials   "
179,Large Loss Matters in Weakly Supervised Multi-Label Classification.txt,"Weakly supervised multi-label classification (WSML) task, which is to learn a
multi-label classification using partially observed labels per image, is
becoming increasingly important due to its huge annotation cost. In this work,
we first regard unobserved labels as negative labels, casting the WSML task
into noisy multi-label classification. From this point of view, we empirically
observe that memorization effect, which was first discovered in a noisy
multi-class setting, also occurs in a multi-label setting. That is, the model
first learns the representation of clean labels, and then starts memorizing
noisy labels. Based on this finding, we propose novel methods for WSML which
reject or correct the large loss samples to prevent model from memorizing the
noisy label. Without heavy and complex components, our proposed methods
outperform previous state-of-the-art WSML methods on several partial label
settings including Pascal VOC 2012, MS COCO, NUSWIDE, CUB, and OpenImages V3
datasets. Various analysis also show that our methodology actually works well,
validating that treating large loss properly matters in a weakly supervised
multi-label classification. Our code is available at
https://github.com/snucml/LargeLossMatters.","Multilabel classification aims to find all existing objects or attributes in a single image. It is gaining attention since the real world is made up of a scene with multiple objects in it [29,37]. Moreover, some of the singlelabel datasets, also called multiclass datasets, actually have images containing multiple objects [35, 58]. However, the multilabel classi fication task has some fundamental difficulties in making a dataset because it requires annotators to label all categories‚Äô existence/absence for every image. As the number of cate gories and images in the dataset increase, annotation cost becomes tremendous [20]. *Equal contribution. 0 500 1000 1500 2000 2500 Iteration0.000.020.040.060.080.100.12LossEarly learning phase Memorization phaseTrue negative False negativeFigure 1. Memorization in WSML. When training ResNet50 model on PASCAL VOC dataset with partial label, we set all un observed labels as negative. These labels are composed of true negative and false negative. We observe that the model first fits into true negative label (learning), and then fits into false negative (memorization). To alleviate these issues, weakly supervised learning ap proach in multilabel classification task (WSML) has been taken into consideration [2, 18, 38, 52]. In a WSML setting, labels are given as a form of partial label, which means only a small amount of categories is annotated per image. This setting reflects the recently released largescale multilabel datasets [12,20] which provide only partial label. Thus, it is becoming increasingly important to develop learning strate gies with partial labels. There are two naive approaches to train the model with partial labels. One is to train the model with observed labels only, ignoring the unobserved labels. The other is to assume all unobserved labels are negative and incorporate them into training because majorities of labels are negative in a multi label setting [33]. As the second one has a limitation that this assumption produces some noise in a label which ham pers the model learning, previous works [7,9,16,22] mostly follow the first approach and try to explore the cue of un observed labels using various techniques such as bootstrap ping or regularization. However, these approaches include 1heavy computation or complex optimization pipeline. We hypothesize that if label noise can be handled prop erly, the second approach could be a good starting point be cause it has the advantage of incorporating many true neg ative labels into model training. Therefore, we try to look at the WSML problem from the perspective of noisy label learning. Our key observation is about the memorization effect [1] in a noisy label learning literature. It is known that when training a model with a noisy label, the model fits into clean labels first and then starts memorizing noisy labels. Al though previous work showed the memorization effect only in a noisy multiclass classification scenario, we found for the first time that this same effect also happens in a noisy multilabel classification scenario. As shown in Figure 1, during training, the loss value from the clean label (true neg ative) decreases from the beginning while the loss from the noisy label (false negative) decreases from the middle. Based on this finding, we borrow the idea from noisy multiclass literature [13, 17, 24] which selectively trains the model with samples having small loss and adapt this idea into a multilabel scenario. Specifically, by assigning the unknown labels as negative in a WSML setting, label noise appears in the form of false negative. Then we de velop the three different schemes to prevent false negative labels from being memorized into the multilabel classifi cation model by rejecting or correcting large loss samples during training. Our method is light and simple, yet effective. It involves negligible computation overhead and does not require com plex optimization for model training. Nonetheless, our method surpasses the weakly supervised multilabel classi fication performance compared to the stateoftheart meth ods in Pascal VOC 2012 [10], MS COCO [25], NUSWIDE [6], CUB [44], and OpenImages V3 [20] datasets. More over, while some existing methods are only effective in spe cific partial label setting [7, 9, 16], our method is broadly applicable in both artificially created and real partial label datasets. Finally, we provide some analysis about the rea son why our methods work well from various perspectives. To sum up, our contributions are as follows; 1) We empirically show for the first time that the memo rization effect occurs during noisy multilabel classification. 2) We propose a novel scheme for weakly supervised multilabel classification that explicitly utilizes a learning technique with noisy label. 3) Although light and simple, our proposed method achieves stateoftheart classification performance on vari ous partial label datasets. 2. Related Works "
180,Recognizing Abnormal Heart Sounds Using Deep Learning.txt,"The work presented here applies deep learning to the task of automated
cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We
describe an automated heart sound classification algorithm that combines the
use of time-frequency heat map representations with a deep convolutional neural
network (CNN). Given the cost-sensitive nature of misclassification, our CNN
architecture is trained using a modified loss function that directly optimizes
the trade-off between sensitivity and specificity. We evaluated our algorithm
at the 2016 PhysioNet Computing in Cardiology challenge where the objective was
to accurately classify normal and abnormal heart sounds from single, short,
potentially noisy recordings. Our entry to the challenge achieved a final
specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved
the greatest specificity score out of all challenge entries and, using just a
single CNN, our algorithm differed in overall score by only 0.02 compared to
the top place finisher, which used an ensemble approach.","Advances in deep learning [LeCun et al. , 2015 ]are be ing made at a rapid pace, in part due to challenges such as ILSVRC ‚Äì the ImageNet LargeScale Visual Recognition Challenge [Russakovsky et al. , 2015 ]. Successive improve ments in deep neural network architectures have resulted in computer vision systems that are better able to recognize and classify objects in images [Linet al. , 2013; Szegedy et al. , 2015 ]and winning ILSVRC entries [Szegedy et al. , 2014; Heet al. , 2015 ]. While a large focus of deep learning has been on automated analysis of image and text data, advances are also increasingly being seen in areas that require process ing other input modalities. One such area is the medical do main where inputs into a deep learning system could be phys iologic time series data. An increasing number of large scale challenges in the medical domain, such as [Kaggle, 2014 ]and [Kaggle, 2015 ]have also resulted in improvements to deep learning architectures [Liang and Hu, 2015 ].PhysioNet [Goldberger et al. , 2000 ]has held a Comput ing in Cardiology Challenge since 2000 that requires partic ipants to automatically analyze physiologic time series data. The 2016 challenge [Clifford et al. , 2016 ]asked participants to perform automated analysis of phonocardiogram (PCG) waveforms, i.e. heart sound data collected using digital stethoscopes. The objective of the challenge was to accu rately classify normal and abnormal heart sounds. Record ings were collected from both healthy individuals, as well as those with heart disease, including heart valve disease and coronary artery disease. A PCG plot showing the recording of the (normal) sounds made by the heart is given in Figure 1. Figure 1: A phonocardiogram showing the recording of nor mal heart sounds, together with corresponding electrocardio gram tracing. S1is the Ô¨Årst heart sound and marks the begin ning of systole. Source [Springer et al., 2016]. Heart disease remains the leading cause of death globally, resulting in more people dying every year due to cardiovas cular disease compared to any other cause of death [World Health Organization, 2017 ]. Successful automated PCG anal ysis can serve as a useful diagnostic tool to help determine whether an individual should be referred on for expert di agnosis, particularly in areas where access to clinicians and medical care is limited. In this work, we present an algorithm that accepts PCG waveforms as input and uses a deep convolutional neural net work architecture to classify inputs as either normal or abnorarXiv:1707.04642v2  [cs.SD]  19 Oct 2017mal using the following steps: 1. Segmentation of time series A logistic regression hidden semiMarkov model is used to segment incoming heart sound instances into shorter segments beginning at the start of each heartbeat, i.e. the S1heart sound. 2. Transformation of segments into heat maps Using Melfrequency cepstral coefÔ¨Åcients, one dimensional time series input segments are converted into two dimensional spectrograms (heat maps) that capture the timefrequency distribution of signal energy. 3. ClassiÔ¨Åcation of heat maps using a deep neural network A convolutional neural network is trained to perform automatic feature extraction and distinguish between normal and abnormal heat maps. The contributions of this work are as follows: 1. We introduce a deep convolutional neural network ar chitecture designed to automatically analyze physiologic time series data for the purposes of identifying abnor malities in heart sounds. 2. Given the costsensitive nature of misclassiÔ¨Åcation, we describe a novel loss function used to train the above network that directly optimizes the sensitivity and speci Ô¨Åcity tradeoff. 3. We present results from the 2016 PhysioNet Computing in Cardiology Challenge where we evaluated our algo rithm and achieved a Top 10 place Ô¨Ånish out of 48 teams who submitted a total of 348 entries. The remainder of this paper is organized as follows. In Section 2, we discuss related works, including historical ap proaches to automated heart sound analysis and deep learning approaches that process physiologic time series input data. Section 3 introduces our approach and details each step of the algorithm. Section 4 further describes the modiÔ¨Åed cost sensitive loss function used to tradeoff the sensitivity and speciÔ¨Åcity of the network‚Äôs predictions, followed by Section 5, which details the network training decisions and param eters. Section 6 presents results from the 2016 PhysioNet Computing in Cardiology Challenge and in Section 7 we pro vide a Ô¨Ånal discussion and end with conclusions in Section 8. 2 Related Work "
181,Training Data Subset Search with Ensemble Active Learning.txt,"Deep Neural Networks (DNNs) often rely on very large datasets for training.
Given the large size of such datasets, it is conceivable that they contain
certain samples that either do not contribute or negatively impact the DNN's
optimization. Modifying the training distribution in a way that excludes such
samples could provide an effective solution to both improve performance and
reduce training time. In this paper, we propose to scale up ensemble Active
Learning (AL) methods to perform acquisition at a large scale (10k to 500k
samples at a time). We do this with ensembles of hundreds of models, obtained
at a minimal computational cost by reusing intermediate training checkpoints.
This allows us to automatically and efficiently perform a training data subset
search for large labeled datasets. We observe that our approach obtains
favorable subsets of training data, which can be used to train more accurate
DNNs than training with the entire dataset. We perform an extensive
experimental study of this phenomenon on three image classification benchmarks
(CIFAR-10, CIFAR-100 and ImageNet), as well as an internal object detection
benchmark for prototyping perception models for autonomous driving. Unlike
existing studies, our experiments on object detection are at the scale required
for production-ready autonomous driving systems. We provide insights on the
impact of different initialization schemes, acquisition functions and ensemble
configurations at this scale. Our results provide strong empirical evidence
that optimizing the training data distribution can provide significant benefits
on large scale vision tasks.","Deep Neural Networks (DNNs) have become the domi nant approach for addressing supervised learning tasks in volving highdimensional inputs. There is signiÔ¨Åcant inter est in automating the endtoend process of applying DNNs to realworld problems such as training perception systems for autonomous driving [1], [2], [3]. While there has been a considerable effort towards methods and frameworks that automate DNN architecture search [4], [5], [6], [7], [8] and training hyperparameter search [9], [10], [11]; the process of searching for the right training data distribution (also called dataset curation) is still performed by experts, requiring several heuristics and signiÔ¨Åcant manual effort. With the rapid growth in the availability of labeled data for perception tasks, to the order of billions of samples [12], [13], automating the training data subset search would make the application of DNNs much easier for nonexperts, and potentially lead to datasets and models that outperform those that were curated by hand. In this paper, we present a simple yet effective method to perform a training data subset search by using ensemble Active Learning (AL). The typical goal of AL is to select, from a large unlabeled dataset, the smallest possible train ing set to label in order to solve a speciÔ¨Åc task [14]. We instead propose to use AL to build data subsets of a large labeled training dataset that give more accurate DNNs in less training time. We demonstrate that this approach can automatically curate large datasets. We study the impact of K. Chitta is with the Max Planck Institute for Intelligent Systems, T¬® ubingen and University of T¬® ubingen. Work completed during an in ternship at NVIDIA. Email: kashyap.chitta@tue.mpg.de J. M. ¬¥Alvarez, E. Haussmann and C. Farabet are with NVIDIA.key design choices for AL, and the robustness of the selected subsets to changes in model architectures. We tackle two key issues that have not been addressed so far by stateoftheart AL methods. The Ô¨Årst is the dif Ô¨Åculty in scaling the number of models for the popular ensemble AL technique. While it seems intuitive that more ensembles can improve performance, existing studies show no gains in AL performance beyond 10 models, and even recommend the use of only 5 models [15], [16]. In this study, we propose the use of implicit ensembles with hundreds of training checkpoints from different experimental runs, and empirically demonstrate the effectiveness of this approach. Second, we switch to a largescale experimental setting compared to what is typically used for AL experiments. For example, Beluch et al. [17] and Sinha et al. [18] never use more than 30% of the ImageNet dataset, and do not compete with the full dataset performance. In contrast, for ResNet18 training with 80% of ImageNet, we improve the top1 accuracy by 0.5% over a model trained with the entire ImageNet dataset. For object detection, existing studies are limited to acquisition at the order of 10k samples, beyond which improvements become marginal [19], [20], [21], [22]. In this paper, we scale the process to the acquisition of 200k images in each iteration, where the Ô¨Ånal selected subset outperforms training with all the available data. Our exper iments are the Ô¨Årst to provide insights regarding automatic dataset curation at the scale required for productionready autonomous driving systems. To summarize, our contributions are as follows: (1) we propose a simple approach to scale up ensemble AL meth ods to hundreds of models with a negligible computational overhead at train time, and (2) we conduct a detailed empirical study on how to effectively reduce the size ofarXiv:1905.12737v3  [cs.LG]  7 Nov 20202 existing datasets, containing millions of samples, without human curation. Our study provides practical insights by covering several large datasets for object detection and image classiÔ¨Åcation. 2 R ELATED WORK "
182,Towards replacing precipitation ensemble predictions systems using machine learning.txt,"Precipitation forecasts are less accurate compared to other meteorological
fields because several key processes affecting precipitation distribution and
intensity occur below the resolved scale of global weather prediction models.
This requires to use higher resolution simulations. To generate an uncertainty
prediction associated with the forecast, ensembles of simulations are run
simultaneously. However, the computational cost is a limiting factor here.
Thus, instead of generating an ensemble system from simulations there is a
trend of using neural networks. Unfortunately the data for high resolution
ensemble runs is not available. We propose a new approach to generating
ensemble weather predictions for high-resolution precipitation without
requiring high-resolution training data. The method uses generative adversarial
networks to learn the complex patterns of precipitation and produce diverse and
realistic precipitation fields, allowing to generate realistic precipitation
ensemble members using only the available control forecast. We demonstrate the
feasibility of generating realistic precipitation ensemble members on unseen
higher resolutions. We use evaluation metrics such as RMSE, CRPS, rank
histogram and ROC curves to demonstrate that our generated ensemble is almost
identical to the ECMWF IFS ensemble.","Precipitation forecasting is an essential aspect of weather prediction, with signicant implica tions for various sectors, such as agriculture, transportation, water management, and disaster preparedness [23, 30]. In recent decades, the quality of numerical weather prediction has sig nicantly improved, with meteorological centers utilizing numerical models and reanalyses with grid spacing ranging from 10 to 80 km and updated in near realtime [7]. However, while these grid spacing are capable of resolving largescale weather patterns, they are insucient for accu rately representing precipitation in regions with subgridscale orographic variations [17]. Thus, precipitation intensity can exhibit signicant variations over short distances, with scales of 1 km or less, which is far ner than the typical resolution of global weather models. It is imperative 1arXiv:2304.10251v1  [physics.aoph]  20 Apr 2023to enhance the resolution of precipitation forecasts to accurately evaluate potential impacts, particularly for scenarios involving extreme rainfall. Moreover, ensemble weather prediction aims to quantify the dierent sources of uncertainty in numerical weather prediction models, cf. [20]. The most signicant sources of uncertainty are the initial conditions and errors in the numerical model formulation. To address these uncer tainties, an ensemble of perturbed forecasts is generated, in addition to the single deterministic weather forecast, with the overall divergence, or spread, of the ensemble ideally providing a measure of the uncertainty in the deterministic prediction. However, the main constraint in generating the ensemble is still computational, as each ensemble run requires signicant com putational resources, thereby limiting the total number of ensembles that can be computed on an operational basis, to typically less than 100. This article presents a generative deep learning approach to generating ensemble weather forecasts for highresolution precipitation forecasts, without being trained on highresolution data. We use generative adversarial networks (GANs) to learn the complex spatiotemporal patterns of precipitation and generate diverse and realistic precipitation elds. Thus, the main contributions of this paper are: ‚Ä¢Using machine learning to generate realistic precipitation ensemble members from using only the available control forecast; ‚Ä¢Showing that it is possible to generating realistic precipitation ensemble members on un seen (higher) resolutions using lower resolution training data. The remainder of this paper is organized as follows. In Section 2, we provide a brief overview of related work on deep learning in meteorology and ensemble forecasting for precipitation. Section 3 is devoted to the description of our proposed method using a generative deep learning approach. In Section 4 we present the results and verication of our approach. Finally, in Section 5, we present a summary of the work carried out and discuss future directions for research in this area. 2 Related work "
183,Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.txt,"Deep neural networks (DNNs) have achieved tremendous success in a variety of
applications across many disciplines. Yet, their superior performance comes
with the expensive cost of requiring correctly annotated large-scale datasets.
Moreover, due to DNNs' rich capacity, errors in training labels can hamper
performance. To combat this problem, mean absolute error (MAE) has recently
been proposed as a noise-robust alternative to the commonly-used categorical
cross entropy (CCE) loss. However, as we show in this paper, MAE can perform
poorly with DNNs and challenging datasets. Here, we present a theoretically
grounded set of noise-robust loss functions that can be seen as a
generalization of MAE and CCE. Proposed loss functions can be readily applied
with any existing DNN architecture and algorithm, while yielding good
performance in a wide range of noisy label scenarios. We report results from
experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and
synthetically generated noisy labels.","The resurrection of neural networks in recent years, together with the recent emergence of large scale datasets, has enabled superhuman performance on many classiÔ¨Åcation tasks [ 21,28,30]. However, supervised DNNs often require a large number of training samples to achieve a high level of performance. For instance, the ImageNet dataset [ 6] has 3.2 million handannotated images. Although crowdsourcing platforms like Amazon Mechanical Turk have made largescale annotation possible, some error during the labeling process is often inevitable, and mislabeled samples can impair the performance of models trained on these data. Indeed, the sheer capacity of DNNs to memorize massive data with completely randomly assigned labels [ 42] proves their susceptibility to overÔ¨Åtting when trained with noisy labels. Hence, an algorithm that is robust against noisy labels for DNNs is needed to resolve the potential problem. Furthermore, when examples are cheap and accurate annotations are expensive, it can be more beneÔ¨Åcial to have datasets with more but noisier labels than less but more accurate labels [18]. ClassiÔ¨Åcation with noisy labels is a widely studied topic [ 8]. Yet, relatively little attention is given to directly formulating a noiserobust loss function in the context of DNNs. Our work is motivated by Ghosh et al. [9] who theoretically showed that mean absolute error (MAE) can be robust against noisy labels under certain assumptions. However, as we demonstrate below, the robustness of MAE can concurrently cause increased difÔ¨Åculty in training, and lead to performance drop. This limitation is particularly evident when using DNNs on complicated datasets. To combat this drawback, we advocate the use of a more general class of noiserobust loss functions, which encompass both MAE and CCE. Compared to previous methods for DNNs, which often involve extra steps and algorithmic modiÔ¨Åcations, changing only the loss function requires minimal intervention to existing architectures 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr√©al, Canada.arXiv:1805.07836v4  [cs.LG]  29 Nov 2018and algorithms, and thus can be promptly applied. Furthermore, unlike most existing methods, the proposed loss functions work for both closedset and openset noisy labels [ 40]. Openset refers to the situation where samples associated with erroneous labels do not always belong to a ground truth class contained within the set of known classes in the training data. Conversely, closedset means that all labels (erroneous and correct) come from a known set of labels present in the dataset. The main contributions of this paper are twofold. First, we propose a novel generalization of CCE and present a theoretical analysis of proposed loss functions in the context of noisy labels. And second, we report a thorough empirical evaluation of the proposed loss functions using CIFAR10, CIFAR100 and FASHIONMNIST datasets, and demonstrate signiÔ¨Åcant improvement in terms of classiÔ¨Åcation accuracy over the baselines of MAE and CCE, under both closedset and openset noisy labels. The rest of the paper is organized as follows. Section 2 discusses existing approaches to the problem. Section 3 introduces our noiserobust loss functions. Section 4 presents and analyzes the experiments and result. Finally, section 5 concludes our paper. 2 Related Work "
184,Higher-Order Label Homogeneity and Spreading in Graphs.txt,"Do higher-order network structures aid graph semi-supervised learning? Given
a graph and a few labeled vertices, labeling the remaining vertices is a
high-impact problem with applications in several tasks, such as recommender
systems, fraud detection and protein identification. However, traditional
methods rely on edges for spreading labels, which is limited as all edges are
not equal. Vertices with stronger connections participate in higher-order
structures in graphs, which calls for methods that can leverage these
structures in the semi-supervised learning tasks.
  To this end, we propose Higher-Order Label Spreading (HOLS) to spread labels
using higher-order structures. HOLS has strong theoretical guarantees and
reduces to standard label spreading in the base case. Via extensive
experiments, we show that higher-order label spreading using triangles in
addition to edges is up to 4.7% better than label spreading using edges alone.
Compared to prior traditional and state-of-the-art methods, the proposed method
leads to statistically significant accuracy gains in all-but-one cases, while
remaining fast and scalable to large graphs.","Given an undirected unweighted graph and a few labeled ver tices, the graph transductive learning or semisupervised learning (SSL) aims to infer the labels for the remaining unlabeled vertices [1,6,17,30,36,37,39,40]. Graph SSL finds applications in a number of settings: in a social network, we can infer a particular character istic (e.g. political leaning) of a user based on the information of her friends to produce tailored recommendations; in a userproduct bipartite rating network, based on a few manually identified fraud ulent user accounts, SSL is useful to spot other fraudulent accounts [4,10,18,19]; SSL can identify protein functions from networks of their physical interaction using just a few labels [32]. Traditional graph SSL algorithms leverage a key property of real world networks: the homophily of vertices [5,21], i.e., the nearby vertices in a graph are likely to have the same label. However, these methods tend to be limited by the fact that all the neighbors of a vertex are not equal. Consider your own friendship network where you have many acquaintances, but only a few close friends. In fact, This paper is published under the Creative Commons Attribution 4.0 International (CCBY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW ‚Äô20, April 20‚Äì24, 2020, Taipei, Taiwan ¬©2020 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CCBY 4.0 License. ACM ISBN 9781450370233/20/04. https://doi.org/10.1145/3366423.3379997 ?Network structure (clique)331400AliceBCDPQRS=K2K3K4=Figure 1: Graph SSL approaches which take only edges into account incorrectly classify the unlabeled central vertex ‚ÄòAl ice‚Äô as blue. By leveraging higherorder network structures, the proposed HOLS correctly labels Alice as red. prior research has shown that vertices with a strong connection participate in several higherorder structures, such as dense sub graphs and cliques [ 12‚Äì14,27]. Thus, leveraging the higherorder structure between vertices is crucial to accurately label the vertices. Let us elaborate this using a small friendship network example, shown in Figure 1. The central vertex, Alice, participates in a closely knit community with three friends B, C, and D, all of whom know each other. In addition, she has four acquaintances P, Q, R, and S from different walks of her life. Let the vertices be labeled by their ideological beliefs‚Äîvertices B, C, and D have the same blue label; and the rest of the vertices have the red label. Even though Alice has more red connections than blue, the connection between Alice, B, C, and D is stronger as Alice participates in three 3cliques and one 4clique with them. In contrast, Alice has no 3 and 4 cliques with P, Q, R, and S. Owing to the stronger connection with the red nodes, Alice should be labeled red as well. However, traditional graph SSL techniques that rely on edges alone label Alice as blue [ 39,40]. This calls for graph SSL methods that look beyond edges to leverage the signal present in higherorder structures to label vertices. Our present work focuses on three key research questions: ‚Ä¢RQ1. How do the data reveal that higherorder network struc tures are homogeneous in labels? ‚Ä¢RQ2. How can we leverage higherorder network structures for graph SSL in a principled manner? ‚Ä¢RQ3. Do higherorder structures help improve graph SSL? Accordingly, our contributions can be summarized as follows: (i) Analysis: Through an empirical analysis of four diverse real world networks, we demonstrate the phenomenon of higherorder label homogeneity , i.e., the tendency of vertices participating in a higherorder structure (e.g. triangle) to share the same label. (ii) Algorithm: We develop HigherOrder Label Spreading ( HOLS ) to leverage higherorder structures during graph semisupervised learning. HOLS works for any userinputted higherorder structure and in the base case, is equivalent to edgebased label spreading [ 39]. (iii) Effectiveness: We show that label spreading via higherorder structures strictly outperforms label spreading via edges by up to 4.7% statistically significant margin. Notably, HOLS is competitive with recent deep learning based methods, while running 15 √ófaster.arXiv:2002.07833v1  [cs.SI]  18 Feb 2020WWW ‚Äô20, April 20‚Äì24, 2020, Taipei, Taiwan Eswaran, Kumar, and Faloutsos Table 1: Qualitative comparison of HOLS 3 (using edges and triangles) with traditional and recent graph SSL approaches Desiderata LP[40] LS[39] BP[37] Planetoid [36] GCN [17] MixHop [1] HOLS 3 Higherorder structures ? ? ? ‚úì Theoretical guarantees ‚úì ‚úì ? ‚úì Fast algorithm ‚úì ‚úì ‚úì ‚úì For reproducibility, all the code and datasets are available at https://github.com/dhivyaeswaran/hols. 2 RELATED WORK "
185,Robust Data-Driven Discovery of Partial Differential Equations under Uncertainties.txt,"Robust physics (e.g., governing equations and laws) discovery is of great
interest for many engineering fields and explainable machine learning. A
critical challenge compared with general training is that the term and format
of governing equations is not known as a prior. In addition, significant
measurement noise and complex algorithm hyperparameter tuning usually reduces
the robustness of existing methods. A robust data-driven method is proposed in
this study for identifying the governing Partial Differential Equations (PDEs)
of a given system from noisy data. The proposed method is based on the concept
of Progressive Sparse Identification of PDEs (PSI-PDE or $\psi$-PDE). Special
focus is on the handling of data with huge uncertainties (e.g., 50$\%$ noise
level). Neural Network modeling and fast Fourier transform (FFT) are
implemented to reduce the influence of noise in sparse regression. Following
this, candidate terms from the prescribed library are progressively selected
and added to the learned PDEs, which automatically promotes parsimony with
respect to the number of terms in PDEs as well as their complexity. Next, the
significance of each learned terms is further evaluated and the coefficients of
PDE terms are optimized by minimizing the L2 residuals. Results of numerical
case studies indicate that the governing PDEs of many canonical dynamical
systems can be correctly identified using the proposed $\psi$-PDE method with
highly noisy data. One great benefit of proposed algorithm is that it avoids
complex algorithm modification and hyperparameter tuning in most existing
methods. Limitations of the proposed method and major findings are presented.","Despite that many dynamical systems can be well characterized by PDEs derived mathematically/physically from basic principles such as conservation laws, lots of other sys tems have unclear or elusive underlying mechanisms (e.g., ones in neuroscience, Ô¨Ånance, and Email addresses: zzhan506@asu.edu (Zhiming Zhang), corresponding author:yongming.liu@asu.edu (Yongming Liu) Preprint submitted to Elsevier February 15, 2021arXiv:2102.06504v1  [math.NA]  31 Jan 2021ecology). Thus, the governing equations are usually empirically formulated [1]. Datadriven physics discovery of dynamical systems gradually became possible in recent years due to the rapid development and extensive application of sensing technologies and computational power [2]. Over the past years, extensive eÔ¨Äorts have been devoted into discovering represen tative PDEs for complex dynamical systems of which limited prior knowledge are available [1‚Äì4]. Among all the methods investigated for PDE identiÔ¨Åcation [1‚Äì8], sparse regression gains the most attention in recent studies due to its inherent parsimonypromoting advan tage. Considering a nonlinear PDE of the general form ut=N(u;ux;uxx;:::;x ), in which the subscripts denote partial diÔ¨Äerentiation with respect to temporal or spatial coordinate(s), N()is an unknown expression on the right hand side of the PDE. It is usually a nonlinear function of the spatial coordinate x, the measured quantity u(x;t), and its spatial deriva tivesux,uxx, etc. Given time series measurements of uat certain spatial locations, the above equation can be approximated as Ut=(U), in which Utis the discretized form of ut, (U)is a library matrix with each column corresponding to a candidate term in N(). A key assumption in sparse identiÔ¨Åcation is that N()consists of only a few term for a real physical system, which requires the solution of regression (i.e., ) to be a sparse vector with only a limited number of nonzero elements. This assumption promotes a parsimonious form of the learned PDE instead of overÔ¨Åtting the measured data with a complex model containing redundant nonlinear higherorder terms. As pioneering researchers in sparse PDE learning, Rudy et al. [1, 9] modiÔ¨Åed the ridge regression method by imposing hard thresholding which recursively eliminates certain terms with coeÔ¨Écient values below a learned threshold. As pointed out in Limitations of [1, 9] (Section 4 in Supplementary Materials) and following studies [4, 10, 11], the identiÔ¨Åcation quality is very sensitive to data quantity and quality. For example, the terms of the reaction diÔ¨Äusion equation cannot be correctly identiÔ¨Åed using the data with only 0.5% random noise. Furthermore, as indicated in [12], the identiÔ¨Åcation results using this method are susceptible to the selection of hyperparameters of the algorithm, including the regularizer and the initial tolerance which is also the tolerance increment dtol. The hyperparameter tuning is especially critical for cases with noisy measurements. This limitation most probably comes from the hard thresholding in the modiÔ¨Åed algorithm (STRidge). A hard thresholding tends to suppress small coeÔ¨Écients that may not correspond to the most trivial terms of the intermediately learned PDEs. To overcome the challenge of numerical diÔ¨Äerentiation with scarce and noisy data in sparse regression methods, deep learning techniques were incorporated by generating a large quantity of metadata and adopting the automatic diÔ¨Äerentiation function in deep learning frameworks (TensorÔ¨Çow, PyTorch, etc.) [13, 14]. The intermediately learned PDE can be treated as a physics loss term in physicsinformed deep learning [15‚Äì17], and constrained neuralnetworksweredevelopedtoimprovetheperformanceofPDEidentiÔ¨Åcationrecursively [7, 10, 11]. Long et al. [2, 18] used a convolutional architecture and symbolic regression to replace the numerical diÔ¨Äerentiation and sparse regression procedures, respectively. A comprehensive review of the state of the art of PDE learning can be found in [10]. Despite improved performance of PDE identiÔ¨Åcation using these methods, the identiÔ¨Åcation results 2(both PDE forms and coeÔ¨Écients) are lacking robustness in most studies mentioned above. For example, approaches using constrained neural networks introduced more hyperparame ters into the algorithms in addition to those in the used STRidge algorithm, which further increases the challenge of identifying the correct PDE forms since PDE learning problems are sensitive to the hyperparameter tuning. This issue is ampliÔ¨Åed under noisy data, es pecially under high noise levels. Complete diÔ¨Äerent identiÔ¨Åcation results may be obtained under diÔ¨Äerent noise levels using same hyperparameter settings. Thus, a sound robust PDE learning needs to produce stable identiÔ¨Åcation results with respect to diÔ¨Äerent noise levels. Considering the gaps of existing studies in discovering PDEs from complex dynamical systems, a robust method for correctly identifying PDEs is needed to discover the underlying physics of the measured systems that lack prior knowledge of the governing principles. Thus, this study attempts to develop a robust method of PDE identiÔ¨Åcation within the framework of sparse regression. The key idea is to address both sparsity and accuracy of the learned PDE. Special focus is on the automatic and progressive selection of learned PDE forms without complex algorithms with hardtotune hyperparameters [3]. The proposed scheme automatically promotes sparsity in addition to simplicity of the learned model. Finally, the representativeness of each model will be further evaluated by solving its corresponding PDE with given/extracted initial and/or boundary conditions. The coeÔ¨Écients of each term are optimized by minimizing the error of model prediction with the measured data taken as the ground truth. In this way, the PDE that is most likely to represent the intrinsic mechanisms underlying the observed system will be determined. Since the proposed methodology progressively yields a sparse identiÔ¨Åcation of the governing PDE(s) of a given system, it is named the progressive and sparse identiÔ¨Åcation method of PDEs (PSIPDE or  PDE method). The remaining part of this paper is structured as follows. Section 2 establishes the framework of the  PDE method; section 3 presents and discusses the results of discovering govern equations for a variety of dynamical systems using the  PDE method; section 4 concludes this study with remarks and recommendations for future work. 2. Methodology: a robust PDE learning method "
186,Recalling Holistic Information for Semantic Segmentation.txt,"Semantic segmentation requires a detailed labeling of image pixels by object
category. Information derived from local image patches is necessary to describe
the detailed shape of individual objects. However, this information is
ambiguous and can result in noisy labels. Global inference of image content can
instead capture the general semantic concepts present. We advocate that
high-recall holistic inference of image concepts provides valuable information
for detailed pixel labeling. We build a two-stream neural network architecture
that facilitates information flow from holistic information to local pixels,
while keeping common image features shared among the low-level layers of both
the holistic analysis and segmentation branches. We empirically evaluate our
network on four standard semantic segmentation datasets. Our network obtains
state-of-the-art performance on PASCAL-Context and NYUDv2, and ablation studies
verify its effectiveness on ADE20K and SIFT-Flow.","Image analysis is a fundamental problem in computer vision. The task can be framed at different levels of gran ularity. At a Ô¨Åne scale, semantic segmentation labels each pixel to depict semantic elements by detailed shapes and contours. However, detailed semantic segmentation is challeng ing ‚Äì there exists signiÔ¨Åcant ambiguity in Ô¨Ånescale im age patches that can result in noisy semantic segmentation outputs. The main focus of this paper is utilizing holistic information derived from analyzing entire images to Ô¨Ålter equal contribution. Holistic	AnalysisSemanticSegmentationholisticInfo.HolisticFilteringpixellabelsbicyclemotorbikerefinedpixel	labelsbicyclemotorbikebicyclepersoncarmotorbikeFigure 1. An example showing the usage of holistic information in semantic segmentation. We obtain holistic information about the contents of the entire image, and leverage this to Ô¨Ålter out noisy pixel predictions to improve semantic segmentation. In our exam ple, the noisy pixel label ( i.e., bicycle) is removed after holistic Ô¨Åltering since it is not recalled as a likely label via the holistic analysis. noisy lowlevel semantic segmentation. Holistic informa tion about the content of an image is highly valuable for semantic segmentation on local pixels ‚Äì essentially, the ex istence, nonexistence and coexistence information on se mantic classes provides a strong cue in determining the lo cal pixel labels (see Figure 1). Stateoftheart methods for semantic segmentation leverage the successes of Convolutional Neural Networks (CNNs) [18]. CNNs have transformed the Ô¨Åeld of im age classiÔ¨Åcation, especially since the development of AlexNet [16]. There have been many followup CNN ar chitectures to further boost image classiÔ¨Åcation, including VGGNet [28], Google Inception [29], ResNet [11], etc. Se mantic segmentation utilizes these network structures, com bined with dense output structures to label image pixels by semantic categories. A representative work is the Fully Convolutional Network (FCN) [26] that leverages skip fea tures of CNNs to produce a detailed pixel labeling. Another 1arXiv:1611.08061v1  [cs.CV]  24 Nov 2016example is the DeepLab [3] framework, which augments FCN with dilated convolution [32], atrous spatial pyramid pooling and Conditional Random Fields (CRFs), and ob tains stateofart semantic segmentation performance. Common among these previous methods is a focus on (layers of) lowlevel pixel analysis leading to semantic seg mentation. Stateoftheart techniques combine this with sophisticated graphical modelstyle techniques (CRF) and pooling structures to obtain high accuracy. The role of these additional components is to smooth out the noisy pixel la belings that result from the direct CNN analysis. However, we believe that a simpler approach to incorporate this in formation is via a holistic analysis that globally suggests category labels that are likely to be present in the image. To make use of holistic information in semantic segmen tation, we leverage it to Ô¨Ålter out noisy pixel predictions ‚Äì if the holistic information suggests a semantic category is unlikely to be present, then pixels should be unlikely to be predicted as that label. Of course, holistic image analysis is imperfect. We conduct a detailed problem analysis in Sec tion 3 to study how and when the holistic information can help. We utilize these observations to propose a novel neu ral network architecture that uniÔ¨Åes a holistic branch and a detailed semantic segmentation branch. The two branches share common image features in lowlevel layers of convo lution and pooling. In the holistic branch, we then recall information about semantic categories by aggregating over a grid of image patches. In the segmentation branch, we pipe in holistic information to guide segmentation by Ô¨Ålter ing out noisy pixel predictions. Our network is endtoend trainable towards the goal of highquality semantic segmen tation. Contribution. We summarize our main contributions as: First, we advocate to leverage holistic image analysis to guide semantic segmentation, and provide empirical analysis to support our intuition. Second, we invent holistic Ô¨Åltering that enables us to Ô¨Ålter out noisy pixel predictions with the guidance of holistic information. Third, we propose a twostream neural network archi tecture for semantic segmentation. We implement a holistic branch and a segmentation branch, which fa cilitate the Ô¨Çow of global image information to the seg mentation branch. Our approach is general, and could be incorporated into a variety of CNNbased semantic segmentation architectures. Finally, we evaluate our proposed network on PASCALContext [23], ADE20K [35], NYUDv2 [27] and SIFTFlow [20]. Experimental results show that the proposed network improves upon stateoftheart semantic segmentation models such as FCN [26] and DeepLab [3].2. Related Work "
187,SpectraNet: Learned Recognition of Artificial Satellites From High Contrast Spectroscopic Imagery.txt,"Effective space traffic management requires positive identification of
artificial satellites. Current methods for extracting object identification
from observed data require spatially resolved imagery which limits
identification to objects in low earth orbits. Most artificial satellites,
however, operate in geostationary orbits at distances which prohibit ground
based observatories from resolving spatial information. This paper demonstrates
an object identification solution leveraging modified residual convolutional
neural networks to map distance-invariant spectroscopic data to object
identity. We report classification accuracies exceeding 80% for a simulated
64-class satellite problem--even in the case of satellites undergoing constant,
random re-orientation. An astronomical observing campaign driven by these
results returned accuracies of 72% for a nine-class problem with an average of
100 examples per class, performing as expected from simulation. We demonstrate
the application of variational Bayesian inference by dropout, stochastic weight
averaging (SWA), and SWA-focused deep ensembling to measure classification
uncertainties--critical components in space traffic management where routine
decisions risk expensive space assets and carry geopolitical consequences.","The ideal data type for identifying resident space ob jects (RSOs; artificial satellites) is resolved imagery. An alysts can easily interpret the information content of im ages and recent work has demonstrated that deep neural networks provide efficiency enhancements to signal extrac tion [15, 16, 22]. In addition, data collection is passive ‚Äì RSOs always reflect incident sunlight, and ground based telescopes collect without interfering with the target. Unfortunately, smaller and more distant RSOs require *jonathan.gazak.1.ctr@us.af.mil ‚Ä†justin.fletcher.14.ctr@us.af.mil Figure 1: Pristine renders of three satellites, Hubble, DI RECTV , and Almaz [21]. Satellite spectral energy distri butions are complex and vary strongly with orientation and illumination angle. increasingly large telescopes to resolve, placing the major ity of RSOs beyond the limits of current imaging technol ogy. For positive identification of objects beyond low earth orbit (LEO; altitude <1000 km), new approaches are as necessary as they are elusive. One promising technique, spectroscopy, is both passive and distanceinvariant, having been used to study the most distant objects in the universe for well over 50 years [14]. In the field of space domain awareness, spectroscopy has lagged behind its potential for three reasons. First, the in accuracy of material reflection models (bidirectional re flection functions, or BRDFs) does not allow attribution of spectroscopic features to satellite materials and geometries. Second, spectroscopic data is not easy to interpret (see Fig ure 2)‚àíout of reach of human analysts even after extensive expert data reduction. Third, underlying truth data on mate rial and geometry are difficult to obtain as spectra are highly variable based on orientation (Figure 1). These hurdles pre clude transition of spectroscopic solutions beyond labora tory settings. In this work we demonstrate learned spectroscopic pos itive identification by modeling a high yield, low cost sen sor and training convolutional neural networks (CNNs) on simulated output. By eliminating reliance on both physics based priors and exquisite data reduction, our technique (SpectraNet) identifies RSOs with accuracy exceeding 80% under the most difficult condition of random axis orienta 4012arXiv:2201.03614v1  [cs.LG]  10 Jan 2022Figure 2: Top panel : A simulated raw FPA observation of 18 Scorpii, a star which is a close analog of our Sun [3], such that this spectrum and FPA frame are typical of resident space objects reflecting solar radiation. These raw frames are used to train models in this paper. Bottom : The 1D reduced spectrum of 18 Scorpii after a raw FPA frame is fully calibrated [2]. tion and for large numbers of satellite classes. We verify simulated results by collecting on sky spec tral observations of RSOs in geostationary (GEO; altitude >35000 km) orbits. SpectraNet learns to classify objects in our on sky dataset with accuracy of ‚àº72%, in agreement with simulated results given the limited ( ‚àº100 examples per class) dataset. In this paper we contribute: ‚Ä¢ A novel method capable of identifying spatially un resolved artificial satellites ‚àía critical technology for space domain awareness ‚àíby allowing deep Bayesian residual networks to learn spectroscopic features from raw scientific imagery. These models can produce well calibrated softmax probabilities, enabling practical ap plications of SpectraNet at low sample counts. ‚Ä¢ Both real and synthetic datasets representing the spec troscopic satellite identification application domain and baseline analysis of classifier performance on those datasets as a function of observation count (dataset size).1 ‚Ä¢ A framework for designing spectroscopic positive identification systems by experimenting across number of classes, observation quality (signal to noise), and number of examples needed to achieve suitable classi fication performance. In ¬ß2 we discuss topics related to this work, followed by our specific problem formulation in ¬ß3. We describe the datasets used for training in ¬ß4, and the experiments con ducted in ¬ß5, before summarizing results in ¬ß6. 2. Related Works "
188,Scalable domain adaptation of convolutional neural networks.txt,"Convolutional neural networks (CNNs) tend to become a standard approach to
solve a wide array of computer vision problems. Besides important theoretical
and practical advances in their design, their success is built on the existence
of manually labeled visual resources, such as ImageNet. The creation of such
datasets is cumbersome and here we focus on alternatives to manual labeling. We
hypothesize that new resources are of uttermost importance in domains which are
not or weakly covered by ImageNet, such as tourism photographs. We first
collect noisy Flickr images for tourist points of interest and apply automatic
or weakly-supervised reranking techniques to reduce noise. Then, we learn
domain adapted models with a standard CNN architecture and compare them to a
generic model obtained from ImageNet. Experimental validation is conducted with
publicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred.
Results show that low-cost domain adaptation improves results compared to the
use of generic models but also compared to strong non-CNN baselines such as
triangulation embedding.","Many computer vision tasks, including object classiÔ¨Åcatio n and localization, as well as content based retrieval, are in  creasingly tackled with convolutionalneural network (CNN ) architectures. A lot of recent papers [1, 2] focus on improv ing CNN architectures and assume that largescale manually labeled datasets, such as ImageNet [3], are readily availab le. However, while large, the coverage of ImageNet is insufÔ¨Å cientinmanydomains. Forinstance,it illustratesonlya ve ry limited amount of named entities, including tourist points of interest or car brands and models. Such entities can never thelessbeofinterestindomainrelatedapplicationsandg ood qualityvisual resourceswhich illustrate them are assumed to be necessary. Manual resource enrichmentis tediousand not considered as scalable in terms of number of concepts and images. To overcome this issue, this paper focuses on domain adaptationand more precisely on automatic or weakly supervised methods to create the visual resources necessar y fordomainspeciÔ¨Åc CNN training. Due to resource scarcity, domain transfer recently re ceived a particular attention from the computer vision com munity. The authors of [4] have shown that activations of top layers of a CNN can be used effectively for image re trieval. More interestingly, they discoveredthat perform ance increases when the network is retrained with images similar to a speciÔ¨Åc domain (tourist points of interest). However, their method requires important manual intervention, sinc e theyverify200imagesforeachtouristpointofinterest(PO I) included in the training set. Equally important, they assum e thatthepresenceofnoisyimagesreducesthequalityofCNN modelsandcreatean unbalancedtrainingset (i.e. somePOIs are illustrated by 1000 images, while a majority of them are represented by only 100 images). In this paper, we focus on methodstobuildaCNNtrainingsetinafullyautomatedman ner and show that, contrary to the hypothesis of [4], a CNN can actually be trained from scratch with these images. This is probably due to the intrinsic quality of the training data set constitutedwithourmethodbutalsoto itslargersize. The automatic collection of groups of visually coherent images was already addressed in literature. The general ide a consistsofcollecting‚Äúnoisy‚Äùimageswhicharethenrerank ed according to a learned model [5] or use clustering to deter mine visually compact groups [6]. Beyond the fact these works did not aim at determining groups of image for CNN learning, we propose a simpler method, based on a Ô¨Åner do maincharacterization. Hence,ourapproachhasabettercom  putationaltractability,whilepreservingCNNmodelaccur acy. Experiments are carried out on three publicly available datasets which include images related to the tourism do main but have different visual properties. We test with Ox ford5k[7],INRIAHolidays[8]andDiv150Cred[9]andshow we obtain better results than other CNNbased method and amongthebestreportedintheliteratureonthese sets. 2. RELATED WORK "
189,Suppressing Mislabeled Data via Grouping and Self-Attention.txt,"Deep networks achieve excellent results on large-scale clean data but degrade
significantly when learning from noisy labels. To suppressing the impact of
mislabeled data, this paper proposes a conceptually simple yet efficient
training block, termed as Attentive Feature Mixup (AFM), which allows paying
more attention to clean samples and less to mislabeled ones via sample
interactions in small groups. Specifically, this plug-and-play AFM first
leverages a \textit{group-to-attend} module to construct groups and assign
attention weights for group-wise samples, and then uses a \textit{mixup} module
with the attention weights to interpolate massive noisy-suppressed samples. The
AFM has several appealing benefits for noise-robust deep learning. (i) It does
not rely on any assumptions and extra clean subset. (ii) With massive
interpolations, the ratio of useless samples is reduced dramatically compared
to the original noisy ratio. (iii) \pxj{It jointly optimizes the interpolation
weights with classifiers, suppressing the influence of mislabeled data via low
attention weights. (iv) It partially inherits the vicinal risk minimization of
mixup to alleviate over-fitting while improves it by sampling fewer
feature-target vectors around mislabeled data from the mixup vicinal
distribution.} Extensive experiments demonstrate that AFM yields
state-of-the-art results on two challenging real-world noisy datasets: Food101N
and Clothing1M. The code will be available at
https://github.com/kaiwang960112/AFM.","In recent years, deep neural networks (DNNs) have achieved great success in var ious tasks, particularly in supervised learning tasks on largescale image recog nition challenges, such as ImageNet [6] and COCO [21]. One key factor that ?Equallycontributed rst authors.yCorresponding author (yu.qiao@siat.ac.cn)arXiv:2010.15603v1  [cs.CV]  29 Oct 20202 X. Peng, K. Wang, Z. Zeng, Q. Li, J. Yang and Y. Qiao Fig. 1: Suppressing mislabeled samples by grouping and selfattention mixup. Dierent colors and shapes denote given labels and ground truths. Thick and thin lines denote high and low attention weights, respectively. A0;A10;B0, and B10are supposed to be mislabeled samples, and can be suppressed by assigning low interpolation weights in mixup operation. drives impressive results is the large amount of welllabeled images. However, highquality annotations are laborious and expensive, even not always available in some domains. To address this issue, an alternative solution is to crawl a large number of web images with tags or keywords as annotations [8,19]. These annotations provide weak supervision, which are noisy but easy to obtain. In general, noisy labeled examples hurt generalization because DNNs eas ily overt to noisy labels [7,30,2]. To address this problem, it is intuitive to develop noisecleaning methods which aim to correct the mislabeled samples either by joint optimization of classication and relabeling [31] or by iterative selflearning [11]. However, the noisecleaning methods often suer from the main diculty in distinguishing mislabeled samples from hard samples. Another solu tion is to develop noiserobust methods which aims to reduce the contributions of mislabeled samples for model optimization. Along this solution, some methods estimate a matrix for label noise modeling and use it to adapt output proba bilities and loss values [30,35,26]. Some others resort to curriculum learning [4] by either designing a stepwise easytohard strategy for training [10] or intro ducing an extra MentorNet [12] for sample weighting. However, these methods independently estimate the importance weights for individuals which ignore the comparisons among dierent samples while they have been proven to be the key of humans to perceive and learn novel concepts from noisy input images [29]. Some other solutions follow semisupervised conguration where they assume a small manuallyveried set can be used [20,32,15,17]. However, this assump tion may be not supported in realworld applications. With the Vicinal Risk Minimization(VRM) principle, mixup [36,33] exploits a vicinal distribution forSuppressing Mislabeled Data via Grouping and SelfAttention 3 sampling virtual sampletarget vectors, and proves its robustness for synthetic noisy data. But its eectiveness is limited in realworld noisy data [1]. In this paper, we propose a conceptually simple yet ecient training block, termed as Attentive Feature Mixup (AFM), to suppress mislabeled data thus to make training robust to noisy labels. The AFM is a plugandplay block for training any networks and is comprised of two crucial parts: 1) a Groupto Attend (GA) module that rst randomly groups a minibatch images into small subsets and then estimates sample weights within those subsets by an attention mechanism, and 2) a mixup module that interpolates new features and soft labels according to selfattention weights. Particularly, for the GA module, we evaluate three feature interactions to estimate groupwise attention weights, namely con catenation, summary, and elementwise multiplication. The interpolated samples and original samples are respectively fed into an interpolation classier and a normal classier. Figure 1 illustrates how AFM suppress mislabeled data. Gen erally, there exists two main types of mixup: intraclass mixup (Figure 1 (c)) and interclass mixup (Figure 1 (b)). For both types, the interpolations between mis labeled samples and clean samples may become useful for training with adaptive attention weights, i.e. low weights for the mislabeled samples and high weights for the clean samples.In other words, our AFM hallucinates numerous useful noisyreduced samples to guide deep networks learn better representations from noisy labels. Overall, as a noisyrobust training method, our AFM is promising in the following aspects. {It does not rely on any assumptions and extra clean subset. {With AFM, the ratio of harmful noisy interpolations ( i.e. between noisy samples) over all interpolations is largely less than the original noisy ratio. {It jointly optimizes the mixup interpolation weights and classier, suppress ing the in uence of mislabeled data via low attention weights. {It partially inherits the vicinal risk minimization of mixup to alleviate over tting while improves it by sampling less featuretarget vectors around mis labeled data from the mixup vicinal distribution. We validate our AFM on two popular realworld noisylabeled datasets: Food101N [15] and Clothing1M [35]. Experiments show that our AFM outperforms recent stateoftheart methods signicantly with accuracies of 87.23 % on Food101N and82.09 % on Clothing1M. 2 Related Work "
190,Training Convolutional Networks with Noisy Labels.txt,"The availability of large labeled datasets has allowed Convolutional Network
models to achieve impressive recognition results. However, in many settings
manual annotation of the data is impractical; instead our data has noisy
labels, i.e. there is some freely available label for each image which may or
may not be accurate. In this paper, we explore the performance of
discriminatively-trained Convnets when trained on such noisy data. We introduce
an extra noise layer into the network which adapts the network outputs to match
the noisy label distribution. The parameters of this noise layer can be
estimated as part of the training process and involve simple modifications to
current training infrastructures for deep networks. We demonstrate the
approaches on several datasets, including large scale experiments on the
ImageNet classification benchmark.","In recent years, Convolutional Networks (Convnets) (LeCun et al., 1989; Lecun et al., 1998) have shown impressive results on image classiÔ¨Åcation tasks (Krizhevsky et al., 2012; Simonyan & Zisser man, 2014). However, this achievement relies on the availability of large amounts of labeled images, e.g. ImageNet (Deng et al., 2009). Labeling images by hand is a laborious task and impractical for many problems. An alternative approach is to use labels that can be obtained easily, such as user tags from social network sites, or keywords from image search engines. The catch is that these labels are not reliable so they may contain misleading information that will subvert the model during training. But given the abundance of tasks where noisy labels are available, it is important to understand the consequences of training a Convnet on them, and this is one of the contributions of our paper. For image classiÔ¨Åcation in realworld settings, two types of label noise dominate: (i) label Ô¨Çips , where an example has erroneously been given the label of another class within the dataset and (ii) outliers , where the image does not belong to any of the classes under consideration, but mistakenly has one of their labels. Fig. 1 shows examples of these two cases. We consider both scenarios and explore them on a variety of noise levels and datasets. Contrary to expectations, a standard Convnet model (from Krizhevsky et al. (2012)) proves to be surprisingly robust to both types of noise. But inevitably, at high noise levels signiÔ¨Åcant performance degradation occurs. Consequently, we propose a novel modiÔ¨Åcation to a Convnet that enables it to be effectively trained on data with high level of label noise. The modiÔ¨Åcation is simply done by adding a constrained linear ‚Äúnoise‚Äù layer on top of the softmax layer which adapts the softmax output to match the noise distribution. We demonstrate that this model can handle both label Ô¨Çip and outlier noise. As it is a linear layer, both it and the rest of the model can be trained endtoend with conventional back propagation, thus automatically learning the noise distribution without supervision. The model is also easy to implement with existing Convnet libraries (Krizhevsky, 2012; Jia et al., 2014; Collobert et al., 2011) and can readily scale to ImageNetsized problems. 1arXiv:1406.2080v4  [cs.CV]  10 Apr 2015Accepted as a workshop contribution at ICLR 2015 horsedogcathorsecatdogcatdog horsecatcathorsecatdogdogcatLabel Ô¨Çip noiseOutlier noise Figure 1: A toy classiÔ¨Åcation example with 3 classes, illustrating the two types of label noise encoun tered on real datasets. In the label Ô¨Çip case, the images all belong to the 3 classes, but sometimes the labels are confused between them. In the outlier case, some images are unrelated to the classiÔ¨Åcation task but possess one of the 3 labels. 2 R ELATED WORK "
191,Do We Really Need to Collect Millions of Faces for Effective Face Recognition?.txt,"Face recognition capabilities have recently made extraordinary leaps. Though
this progress is at least partially due to ballooning training set sizes --
huge numbers of face images downloaded and labeled for identity -- it is not
clear if the formidable task of collecting so many images is truly necessary.
We propose a far more accessible means of increasing training data sizes for
face recognition systems. Rather than manually harvesting and labeling more
faces, we simply synthesize them. We describe novel methods of enriching an
existing dataset with important facial appearance variations by manipulating
the faces it contains. We further apply this synthesis approach when matching
query images represented using a standard convolutional neural network. The
effect of training and testing with synthesized images is extensively tested on
the LFW and IJB-A (verification and identification) benchmarks and Janus CS2.
The performances obtained by our approach match state of the art results
reported by systems trained on millions of downloaded images.","The recent impact of deep Convolutional Neural Network (CNN) based methods on machine face recognition capabilities has been nothing short of revolutionary. The conditions under which faces can now be recognized and the numbers of faces which systems can now learn to identify improved to the point where some consider machines to be better than humans at this task. This remarkable advancement is partially due to the gradual improvement of new network designs which oer better performance. However, alongside developments in network architectures, it is also the underlying ability of CNNs to learn from massive training sets that allows these techniques to be so eective. Realizing that eective CNNs can be made even more eective by increasing their training data, many begun focusing eorts on harvesting and labeling large image collections to better train their networks. In [35], a standard CNN was trained by Facebook using 4.4 million labeled faces and shown to achieve what was, at the time, state of the art performance on the Labeled Faces in the Wild (LFW) benchmark [11]. Some time later, [24] proposed the VGGFace representation, trained on 2.6 million faces, and Face++ proposed its Megvii arXiv:1603.07057v2  [cs.CV]  11 Apr 20162 I. Masi, A.T. Tran, J. T. Leksut, T. Hassner and G. Medioni Dataset #ID #Img #Img =#ID Google [29] 8M 200M 25 Facebook [35] 4,030 4.4M 1K VGG Face [24] 2,622 2.6M 1K MegaFace [12] 690,572 1.02M 1.5 CASIA [44] 10,575 494,414 46 Aug. pose+shape 10,575 1,977,656 187 Aug. pose+shape+expr 10,575 2,472,070 234 (a)Face set statistics 1001011021031041050200040006000 Subjects (log scale)Images    CASIA WebFace Pose with Shapes Pose, Shapes, Expression(b)Images for subjects Fig. 1: (a) Comparison of our augmented dataset with other face datasets along with the average number of images per subject. (b) Our improvement by augmentation (Aug.) in the distribution of persubject image numbers in order to avoid the longtail eect of the CASIA set [44] (also shown in the last two rows of (a)). System [45], trained on 5 million faces. All, however, pale in comparison to the Google FaceNet [29] which used 200 million labeled faces for its training. Making networks better by collecting and labeling huge training sets is, un fortunately, not an easy game to play. The eort required to download, process and label millions of images from the Internet with reliable subject names is daunting. To emphasize this, the bigger sets, [35] and [29], required the eorts of large scale commercial organizations to assemble (Facebook and Google, resp.) and none of these sets was publicly released by its owners. By comparison, the largest face recognition training set which is publicly available is the CASIA WebFace collection [44] weighing in at a mere 495K images, several orders of magnitudes smaller than the two bigger commercial sets1. But downloading and labeling so many faces is more than just nancially challenging. Fig. 1a provides some statistical information on the larger face sets. Evidently, set sizes increase far faster than the numbers of images persubject. This may imply that nding many images veried as belonging to the same subjects is dicult even when resources are abundant. Regardless of the reason, this is a serious problem: face recognition systems should learn to model not just interclass appearance variations (dierences between dierent people) but also intraclass variations (dierences of appearance that do not change subject label) and so far this has been a challenge for data collection eorts. In light of these challenges, it is natural to ask: is there no alternative to this labor intensive, data download and labeling approach to pushing recognition per formance? Beyond potentially mitigating the challenges of data collection, this question touches a more fundamental issue. Namely, should images be processed with domain specic tools before being analyzed by CNNs, and if so, how? In answer to these questions, we make the following contributions. (1)We propose synthesizing data in addition to collecting it. We in ate the size of an existing training set, the CASIA WebFace collection [44], to several times its 1MegaFace [12] is larger than CASIA, but was designed as a testing set and so provides few images per subject. It was consequently never used for training CNN systems.Do We Really Need to Collect Millions of Faces 3 size using domain specic methods designed for face image synthesis (Fig. 1b). We generate images which introduce new intraclass facial appearance varia tions, including pose (Sec. 3.1), shape (Sec. 3.2) and expression (Sec. 3.3). (2) We describe a novel matching pipeline which uses similar synthesis methods at test time when processing query images. Finally, (3), we rigorously test our ap proach on the LFW [11], IJBA (verication and identication) and CS2 bench marks [14]. Our results show that a CNN trained using these generated faces matches state of the art performance reported by systems trained on millions of faces downloaded from the web and manually processed2. We note that our approach can be considered a novel face data augmenta tion method (Sec. 2): A domain specic data augmentation approach. Curiously, despite the success of existing generic augmentation methods, we are unaware of previous reports of applying the easily accessible approach described here to generate face image training data, or indeed training for any other class. 2 Related work "
192,MEAL: Multi-Model Ensemble via Adversarial Learning.txt,"Often the best performing deep neural models are ensembles of multiple
base-level networks. Unfortunately, the space required to store these many
networks, and the time required to execute them at test-time, prohibits their
use in applications where test sets are large (e.g., ImageNet). In this paper,
we present a method for compressing large, complex trained ensembles into a
single network, where knowledge from a variety of trained deep neural networks
(DNNs) is distilled and transferred to a single DNN. In order to distill
diverse knowledge from different trained (teacher) models, we propose to use
adversarial-based learning strategy where we define a block-wise training loss
to guide and optimize the predefined student network to recover the knowledge
in teacher models, and to promote the discriminator network to distinguish
teacher vs. student features simultaneously. The proposed ensemble method
(MEAL) of transferring distilled knowledge with adversarial learning exhibits
three important advantages: (1) the student network that learns the distilled
knowledge with discriminators is optimized better than the original model; (2)
fast inference is realized by a single forward pass, while the performance is
even better than traditional ensembles from multi-original models; (3) the
student network can learn the distilled knowledge from a teacher model that has
arbitrary structures. Extensive experiments on CIFAR-10/100, SVHN and ImageNet
datasets demonstrate the effectiveness of our MEAL method. On ImageNet, our
ResNet-50 based MEAL achieves top-1/5 21.79%/5.99% val error, which outperforms
the original model by 2.06%/1.14%. Code and models are available at:
https://github.com/AaronHeee/MEAL","The ensemble approach is a collection of neural networks whose predictions are combined at test stage by weighted averaging or voting. It has been long observed that en sembles of multiple networks are generally much more ro bust and accurate than a single network. This beneÔ¨Åt has also been exploited indirectly when training a single net work through Dropout (Srivastava et al. 2014), Dropcon nect (Wan et al. 2013), Stochastic Depth (Huang et al. 2016), Equal contribution. This work was done when Zhankui He was a research intern at University of Illinois at UrbanaChampaign. Copyright c 2019, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved. 1 2 3 4 5 # of ensembles0√ó1√ó2√ó3√ó4√ó5√ó6√óFLOPs FLOPs at Inference Time Snapshot Ensemble (Huang et al. 2017) Our FLOPs at Test TimeFigure 1: Comparison of FLOPs at inference time. Huang et al. (Huang et al. 2017a) employ models at different lo cal minimum for ensembling, which enables no additional training cost, but the computational FLOPs at test time lin early increase with more ensembles. In contrast, our method use only one model during inference time throughout, so the testing cost is independent of # ensembles. Swapout (Singh, Hoiem, and Forsyth 2016), etc. We extend this idea by forming ensemble predictions during training, using the outputs of different network architectures with dif ferent or identical augmented input. Our testing still operates on a single network, but the supervision labels made on dif ferent pretrained networks correspond to an ensemble pre diction of a group of individual reference networks. The traditional ensemble, or called true ensemble, has some disadvantages that are often overlooked. 1) Redun dancy: The information or knowledge contained in the trained neural networks are always redundant and has over laps between with each other. Directly combining the pre dictions often requires extra computational cost but the gain is limited. 2) Ensemble is always large and slow: Ensem ble requires more computing operations than an individual network, which makes it unusable for applications with lim ited memory, storage space, or computational power such as desktop, mobile and even embedded devices, and for appli cations in which realtime predictions are needed. To address the aforementioned shortcomings, in this paarXiv:1812.02425v2  [cs.CV]  25 Jul 2019librarybookshopconfectionerygrocerystoretobaccoshoptoyshopFigure 2: Left is a training example of class ‚Äútobacco shop‚Äù from ImageNet. Right are soft distributions from different trained architectures. The soft labels are more informative and can provide more coverage for visuallyrelated scenes. per we propose to use a learningbased ensemble method. Our goal is to learn an ensemble of multiple neural networks without incurring any additional testing costs . We achieve this goal by leveraging the combination of diverse outputs from different neural networks as supervisions to guide the target network training. The reference networks are called Teachers and the target networks are called Students . Instead of using the traditional onehot vector labels, we use the soft labels that provide more coverage for cooccurring and visu ally related objects and scenes. We argue that labels should be informative for the speciÔ¨Åc image. In other words, the labels should not be identical for all the given images with the same class. More speciÔ¨Åcally, as shown in Fig. 2, an im age of ‚Äútobacco shop‚Äù has similar appearance to ‚Äúlibrary‚Äù should have a different label distribution than an image of ‚Äútobacco shop‚Äù but is more similar to ‚Äúgrocery store‚Äù. It can also be observed that soft labels can provide the additional intra and intercategory relations of datasets. To further improve the robustness of student networks, we introduce an adversarial learning strategy to force the student to generate similar outputs as teachers. Our exper iments show that MEAL consistently improves the accu racy across a variety of popular network architectures on different datasets. For instance, our shakeshake (Gastaldi 2017) based MEAL achieves 2.54% test error on CIFAR10, which is a relative 11:2%improvement1. On ImageNet, our ResNet50 based MEAL achieves 21.79%/5.99% val error, which outperforms the baseline by a large margin. In summary, our contribution in this paper is three fold. An endtoend framework with adversarial learning is de signed based on the teacherstudent learning paradigm for deep neural network ensembling. The proposed method can achieve the goal of ensembling multiple neural networks with no additional testing cost . The proposed method improves the stateoftheart accu racy on CIFAR10/100, SVHN, ImageNet for a variety of existing network architectures. 2. Related Work "
193,Progressive Ensemble Networks for Zero-Shot Recognition.txt,"Despite the advancement of supervised image recognition algorithms, their
dependence on the availability of labeled data and the rapid expansion of image
categories raise the significant challenge of zero-shot learning. Zero-shot
learning (ZSL) aims to transfer knowledge from labeled classes into unlabeled
classes to reduce human labeling effort. In this paper, we propose a novel
progressive ensemble network model with multiple projected label embeddings to
address zero-shot image recognition. The ensemble network is built by learning
multiple image classification functions with a shared feature extraction
network but different label embedding representations, which enhance the
diversity of the classifiers and facilitate information transfer to unlabeled
classes. A progressive training framework is then deployed to gradually label
the most confident images in each unlabeled class with predicted pseudo-labels
and update the ensemble network with the training data augmented by the
pseudo-labels. The proposed model performs training on both labeled and
unlabeled data. It can naturally bridge the domain shift problem in visual
appearances and be extended to the generalized zero-shot learning scenario. We
conduct experiments on multiple ZSL datasets and the empirical results
demonstrate the efficacy of the proposed model.","Despite the effectiveness of deep convolutional neural networks (CNNs) on supervised image classiÔ¨Åcation prob lems, zero shot learning (ZSL) remains a challenging and fundamental problem due to the rapid expansion of image categories and the lacking in labeled training data. As a special unsupervised domain adaptation, ZSL aims to trans fer information from the source domain, a set of training classes with labeled data, to make predictions in the target domain, a set of test classes with only unlabeled data. Dif ferent from standard domain adaptation, in ZSL the labeled training classes and unlabeled test classes have no overlaps‚Äì they are entirely disjoint. Based on the visibility of the instance labels, the training classes and the test classes are usually referred to as seen andunseen classes respectively. Existing zeroshot image recognitions have centered on deploying label embeddings in a common semantic space, e.g., in terms of high level visual attributes, to bridge the domain gap between seen andunseen classes. For example, animals share some common characteristics such as ‚Äòblack‚Äô, ‚Äòyellow‚Äô, ‚Äòspots‚Äô, ‚Äòstripes‚Äô and so on. Thus each animal class, either seen or unseen, can be represented as a binary vector in the semantic attribute space, with each element denoting the appearance/absence of certain attribute. Much ZSL effort in this direction has focused on developing effective mapping models from the input visual feature space to the semantic label embedding space [ 24,10,6,19], or learning suitable compatibility functions between the two spaces [ 2,27,33], to facilitate prediction information transfer from the seen classes to the unseen classes. However, these methods iden tify visualsemantic mappings only on the labeled seen class data, which poses a fundamental domain shift problem due to the appearance variations of visual attributes across seen andunseen classes, and has negative impact on crossclass generalization (i.e., ZSL performance) [11, 18]. In this paper, we propose a novel ZSL framework with an progressive ensemble network to address the domain shift problem and improve the generalization ability of ZSL. Ex isting ZSL works rely on a single set of label embeddings to build interclass label relations for knowledge transfer, which can hardly to be suitable for all the unseen classes. In stead we construct a deep ensemble network that consists of multiple image classiÔ¨Åcation functions with a shared feature extraction convolutional neural network and different label embedding representations. Each label embedding represen tation facilitates information transfer from the seen classes to a subset of unseen classes, while enhancing the diversity of the multiple classiÔ¨Åers. By exploiting multiple classiÔ¨Åers in an ensemble manner, we expect the ensemble network can overcome the prediction noise and class bias in the original label embeddings to gain robust zeroshot predictions. More over, we exploit the unlabeled data from unseen classes in 1arXiv:1805.07473v2  [cs.LG]  6 Apr 2019a progressive ensemble framework to overcome the domain shift problem. In each iteration, we select the most conÔ¨Å dently predicted unlabeled instances from each unseen class under the current ensemble network, and combine these se lected instances and their predicted pseudolabels with the original labeled seen class data together to reÔ¨Åne the en semble network parameters, especially its feature extraction component. By incorporating the unseen class instances into the ensemble network training and dynamically reÔ¨Åne the selected instances in each iteration, we expect the dynamic progressive training process can effectively avoid the issue of overÔ¨Åtting to the seen classes and improve the general ization ability of the ensemble network on unseen classes. With the ensemble network directly handling multiclass classiÔ¨Åcation over all classes, the proposed approach can be conveniently extended to address generalized ZSL. We con duct experiments on three standard ZSL datasets under both conventional ZSL and generalized ZSL settings. The empiri cal results demonstrate the proposed approach outperforms the stateoftheart ZSL methods. 2. Related Work "
194,Label noise detection under the Noise at Random model with ensemble filters.txt,"Label noise detection has been widely studied in Machine Learning because of
its importance in improving training data quality. Satisfactory noise detection
has been achieved by adopting ensembles of classifiers. In this approach, an
instance is assigned as mislabeled if a high proportion of members in the pool
misclassifies it. Previous authors have empirically evaluated this approach;
nevertheless, they mostly assumed that label noise is generated completely at
random in a dataset. This is a strong assumption since other types of label
noise are feasible in practice and can influence noise detection results. This
work investigates the performance of ensemble noise detection under two
different noise models: the Noisy at Random (NAR), in which the probability of
label noise depends on the instance class, in comparison to the Noisy
Completely at Random model, in which the probability of label noise is entirely
independent. In this setting, we investigate the effect of class distribution
on noise detection performance since it changes the total noise level observed
in a dataset under the NAR assumption. Further, an evaluation of the ensemble
vote threshold is conducted to contrast with the most common approaches in the
literature. In many performed experiments, choosing a noise generation model
over another can lead to different results when considering aspects such as
class imbalance and noise level ratio among different classes.","Data quality is of great importance for ML applications and, in particular, for classiÔ¨Åcation tasks. Conventionally in these tasks, a training set of labeled instances is given as input to an ML algorithm, which will acquire useful knowledge to make predictions for new instances. In practice, realworld datasets frequently contain irregularities such as incompleteness, noise, and data inconsistencies that impact ML performance [Han et al., 2012]. In this light, noise detection and Ô¨Åltering are quite relevant techniques for ML [Zhu and Wu, 2004]. According to the literature, noise may occur in both attributes and classes [Zhu and Wu, 2004]. This work focuses on the latter problem, in which an unknown proportion of instances in a dataset are mislabeled because of different reasons. This is a relevant problem since label noise can harm the identiÔ¨Åcation of true class boundaries in a problem, increase the chance of overÔ¨Åtting, and affect learning performance in general [Frenay and Verleysen, 2014]. Previous works successfully adopted the classiÔ¨Åcation noise Ô¨Åltering method [Brodley and Friedl, 1999][Sluban and Lavra Àác, 2015][Guan et al., 2018] for label noise detection, widespread in the literature. In this approach, mislabeled instances in a dataset are identiÔ¨Åed according to the output results of a classiÔ¨Åer or an ensemble of classiÔ¨Åers. For example, in the majority vote for ensemble noise detection, an instance is marked as mislabeled if most classiÔ¨Åers in a pool incorrectly classify it. In the consensus vote for ensemble noise detection, a record is considered noisy if all classiÔ¨Åers in the pool misclassify it.arXiv:2112.01617v1  [cs.LG]  2 Dec 2021Label noise detection under the Noise at Random model with ensemble Ô¨Ålters A P REPRINT As with most ML tasks, empirical evaluation is also crucial in the context of noise detection techniques. Producing a groundtruth dataset for evaluation usually requires additional domain experts to decide which instances were mislabeled. This process can be costly, and experts are not always available. This problem is mitigated when artiÔ¨Åcial datasets are used or when simulated noise is injected into a dataset in a controlled way. The investigation of how noise inÔ¨Çuences the learning process is simpliÔ¨Åed when a systematic addition of noise is performed [Garcia et al., 2019]. Label noise can be injected into a dataset by assuming three distinct models of noise Frenay and Verleysen [2014]: (i) Noisy Completely at Random (NCAR), in which the probability of an instance being noisy is random, (ii) Noisy at Random (NAR), the probability of an instance being noisy depends on its label, and (iii) NonNoisy at Random (NNAR), the probability of an instance being noisy also depends on its attributes. In many previous works [Sluban and Lavra Àác, 2015] [Brodley and Friedl, 1999] [S√°ez et al., 2015] [Garcia et al., 2019], a single noise model is chosen over another to perform experiments. Nevertheless, it is usually not clear how this choice can affect experimental results. Additionally, other aspects, like class distribution, can impact the distribution of noise differently in a dataset depending on the noise type considered. For instance, a human supervisor may Ô¨Ånd it more difÔ¨Åcult to label records from the minority class than the majority class in some contexts. In this work, it is investigated how noise models can inÔ¨Çuence noise detection experiments under different aspects. In contrast to previous studies, the inÔ¨Çuence of distinct label noise models on ensemble noise detection is evaluated in this research under various contexts such as class imbalance, noise distribution, ensemble thresholds, and percentage of noise in data. It is shown that different results are achieved depending on the context. For instance, even under the same noise model (e.g., NAR), a detection technique may have quite distinct performance results if class imbalance changes (e.g., NAR with imbalanced vs. NAR with balanced class distributions). The remainder of this paper is organized as follows. In Section 2, an overview of label noise detection is presented. The proposed methodology is described in Section 3. Experiments are presented in Section 4. Finally, Section 5 summarizes the paper and presents future work. 2 Related Works "
195,Unsupervised Person Re-Identification with Multi-Label Learning Guided Self-Paced Clustering.txt,"Although unsupervised person re-identification (Re-ID) has drawn increasing
research attention recently, it remains challenging to learn discriminative
features without annotations across disjoint camera views. In this paper, we
address the unsupervised person Re-ID with a conceptually novel yet simple
framework, termed as Multi-label Learning guided self-paced Clustering (MLC).
MLC mainly learns discriminative features with three crucial modules, namely a
multi-scale network, a multi-label learning module, and a self-paced clustering
module. Specifically, the multi-scale network generates multi-granularity
person features in both global and local views. The multi-label learning module
leverages a memory feature bank and assigns each image with a multi-label
vector based on the similarities between the image and feature bank. After
multi-label training for several epochs, the self-paced clustering joins in
training and assigns a pseudo label for each image. The benefits of our MLC
come from three aspects: i) the multi-scale person features for better
similarity measurement, ii) the multi-label assignment based on the whole
dataset ensures that every image can be trained, and iii) the self-paced
clustering removes some noisy samples for better feature learning. Extensive
experiments on three popular large-scale Re-ID benchmarks demonstrate that our
MLC outperforms previous state-of-the-art methods and significantly improves
the performance of unsupervised person Re-ID.","Person reidentiÔ¨Åcation (ReID) aims at searching peo ple across nonoverlapping surveillance camera views de ployed at different locations by matching person images [22, 59, 35]. Due to its importance in smart cities and largescale surveillance systems, person ReID is alreadya wellestablished research problem in computer vision [13, 17, 62, 20]. Though great progress has been made in both benchmarks and approaches in recent years, person ReID remains an open challenging problem due to the dif Ô¨Åculty of learning robust and discriminative representation with large variant intraperson appearance and high inter person similarity. Over the past decades, most of the existing person Re ID works focus on feature designing and metric learn ing [25, 4, 46]. Recently, modern deep learning has been applied to the ReID community and achieved signiÔ¨Åcant progress [22, 5, 57]. Most of these works tackle ReID in a supervised learning manner, which are limited by the small scale of ReID datasets. Nevertheless, collecting unlabeled pedestrian images are much easier and cheaper, thus train ing deep networks on large scale unlabeled data becomes increasingly necessary and practical. In fact, unsupervised learning for person ReID has be come a hot topic in more recent years [6, 52, 33, 44]. There mainly exist two types of unsupervised person ReID meth ods. The Ô¨Årst one is based on unsupervised domain adaption (UDA) where the source domain is usually a labeled dataset and the target domain is an unlabeled dataset. Most of these UDA based methods use transfer learning to learn the knowledge in the labeled source ReID dataset and transfer them to target datasets [39, 32, 48, 42]. SpeciÔ¨Åcally, some works use generative adversarial networks (GAN) for trans ferring sample images from the source domain to the tar get domain while preserving the person identity as much as possible [41, 63, 65]. Some others Ô¨Årst train models on the source domain, then leverage selfsupervised learning and clustering to estimate pseudolabels on the target domain it eratively to Ô¨Ånetune the pretrained model [33, 44, 9, 19]. The main disadvantages of unsupervised domain adaption ReID are twofold. On the one hand, it still needs ex pensive labeled data and the performance is usually limited 1arXiv:2103.04580v1  [cs.CV]  8 Mar 2021Figure 1. Comparison of recent unsupervised person ReID meth ods and our multilabel learning guided selfpaced clustering (MLC) method. by the scale of the labeled source dataset. On the other hand, these methods ignore the sample relations between the source and the target datasets. The second type of unsupervised ReID methods are based on fully unsupervised learning. Their goal is to learn discriminative representations in large scale unlabeled data. Most of these methods use clustering to generate pseudo la bels. For example, Lin et al. [27] propose a bottomup clus tering (BUC) framework that trains a network with pseudo labels iteratively. The inaccurate nature of clustering al gorithm on large intraclass variations makes the pseudo labels noisy, which in consequence leads to poor perfor mance. In order to avoid wrong merging and make full use of all the images, Ding et al . [7] propose an elegant and practical densitybased clustering approach by incorporat ing the cluster validity criterion. Wang et al. [34] consider the unsupervised person ReID as a multilabel classiÔ¨Åca tion task to progressively seek true labels. They introduce a Memorybased Multilabel ClassiÔ¨Åcation Loss (MMCL) method, which iteratively predicts multilabels and updates the network with multilabel classiÔ¨Åcation loss. As illus trated in Figure 1, the densitybased clustering strategy tries to keep highpurity samples for model training but it may ignore useful hard samples. The multilabel based strategy keeps all the samples in the memory which may introduce noisy samples in training phase. In this paper, to address the above issues of cluster ing and multilabel strategies, we propose a conceptually novel yet simple framework for unsupervised person Re ID, termed as multilabel learning guided selfpaced clus tering (MLC). SpeciÔ¨Åcally, MLC learns discriminative in formation with three crucial modules, namely a multiscale network (MN), a multilabel learning (ML) module, and a selfpaced clustering (SC) module. The MN module is used to mine the multiscale person features for better similar ity measurement. Comparing to the previous methods that only extracting the global features, the MN module captures more nonsalient or infrequent local information. Local feature learning is demonstrated as an effective strategy to en hance the feature representation [10] which is complemen tary to the global feature [36, 12]. The ML module gener ates a multilabel vector for each image based on a mem ory bank. SpeciÔ¨Åcally, each sample in the memory bank is viewed as a single class, and a sample is assigned with a multihot vector where the corresponding items are acti vated if the sample is similar with those indexed samples in memory. To this end, these images with the same iden tity could get similar multilabel vectors. To avoid train ing noisy samples which may hurt the Ô¨Ånal model, the SC module is added after several training epochs of ML. The SC module mainly removes noisy samples by densitybased clustering algorithm and assigns pseudo labels for multi class training. We jointly train the whole network in an endtoend manner. We evaluate the proposed MLC framework on three largescale datasets including Market1501, DukeMTMC reID, and MSMT17 without leveraging their annotations. Experimental results show that our MLC signiÔ¨Åcantly im proves the performance of unsupervised person ReID with out any annotations and achieves performance superior or comparable to the stateoftheart methods. h 2. Related work "
196,Improving Training on Noisy Stuctured Labels.txt,"Fine-grained annotations---e.g. dense image labels, image segmentation and
text tagging---are useful in many ML applications but they are labor-intensive
to generate. Moreover there are often systematic, structured errors in these
fine-grained annotations. For example, a car might be entirely unannotated in
the image, or the boundary between a car and street might only be coarsely
annotated. Standard ML training on data with such structured errors produces
models with biases and poor performance. In this work, we propose a novel
framework of Error-Correcting Networks (ECN) to address the challenge of
learning in the presence structured error in fine-grained annotations. Given a
large noisy dataset with commonly occurring structured errors, and a much
smaller dataset with more accurate annotations, ECN is able to substantially
improve the prediction of fine-grained annotations compared to standard
approaches for training on noisy data. It does so by learning to leverage the
structures in the annotations and in the noisy labels. Systematic experiments
on image segmentation and text tagging demonstrate the strong performance of
ECN in improving training on noisy structured labels.","The quality of labeled data plays a signiÔ¨Åcant role in the performance of supervised machine learning methods trained on the data ([Nettleton et al., 2010, Hendrycks and Dietterich, 2019]). However, in many settings, it may be difÔ¨Åcult to obtain highquality data, such as due to lim ited time, budget, or expertise dedicated to the annotation process. This is particularly the case for Ô¨Ånegrained an notations ([Heller et al., 2018]), which are labels that are Corresponding author: jamesz@stanford.edu.applied to individual elements of each input data and they often follow certain structures (we will use the terms label andannotation interchangeably in this paper). For examples, in computer vision, semantic segmentation models are trained on image data in which each pixel is la beled for a class (e.g. car, street) [Long et al., 2015]. Such pixellevel labels are not independent, and systematic er rors may be present in the training set and thereby learned by a supervised machine learning algorithm. In natural lan guage processing, the analogous task of nameentity recog nition can be seen as operating on Ô¨Ånegrained structured data, in which word or token is labeled with the entity that it represents [Lample et al., 2016]. Because of the level of precision needed to Ô¨Ånely annotate such structured datasets, it is very common in practice to have datasets with substantial annotation mistakes. This is the case both in widely used public datasets, but even more so in private datasets that are collected and annotated us ing customized processes. In Fig. 1, we provide image and text examples of how various types of complex errors may appear in Ô¨Ånegrain labels. Certain elements could be mis labeled to be a wrong class; entire elements (e.g. a car) could be missing an annotation; often times, the bound aries between different annotated classes (e.g. where does the street begin and sidewalk end) are imprecise. It is im portant to note that the errors in the labels have many struc tures and are not independent. For example, errors tend to locally cluster‚Äîif a whole car is missed by the labeler, then all of its pixels are misannotated. These structured er rors are challenging for standard ML training. Common approaches for training on noisy data are typically devel oped in settings where there is a simple label per data, as is the case in standard classiÔ¨Åcation and regression. They are not wellsuited for Ô¨Ånegrained predictions with structured errors within the label of each data. On the other hand, structures in the label enables us to more easily learn to correct the errors. We leverage this idea in developing the new approach of ErrorCorrecting Networks (ECN). Human label errors are difÔ¨Åcult to avoid because it is ex tremely labor intensive (and tedious) to precisely annotatearXiv:2003.03862v1  [cs.LG]  8 Mar 2020Figure 1: Typical labeling errors in structured data. Here, we show examples of errors that commonly occur in two ma chine learning tasks that operate with structured data: image segmentation (top row) and nameentity recognition (bottom row). Generating precise labels for each pixel or word is very labor intensive. In many images/text, parts of the data is misannotated to be the wrong class (left), or annotations are missing (center), or the borders of the annotations are coarse, e.g. the annotated street doesn‚Äôt reach all the way to the sidewalk (right). Errorcorrection Network (ECN) is a general framework to address these structured mistakes in the labels. and segment all of the individual elements in an image or text. In such settings, it is usually the case that a small amount of samples are known to be highquality data (ei ther by manual quality assurance checks or by dedicating additional annotation resources). In this paper, we pro pose the ECN method to leverage a small amount of the highquality datasets, which we refer to as gold data , to im prove the quality of the entire training dataset. Our method is simple to implement and intuitive, and we demonstrate that, even with a relatively small amount of gold data, we can obtain qualitative and quantitative improvements in both computer vision and natural language applications. Our contributions Structured errors in Ô¨Ånegrained la bels is a prevalent challenge. However, previous work on label noise has focused on the setting where the label is simple (e.g. a class or a value). We propose a novel and intuitive algorithm of Errorcorrection Networks (ECN), which can Ô¨Çexibly correct structured errors across diverse domains. Our experiments demonstrate that ECN can sub stantially improve performance in Ô¨Ånegrained image seg mentation/annotation and in text tagging, which are two important and widely used settings. ECN is computation ally as well as data efÔ¨Åcient‚Äîit works well even when there is only a small number of gold standard labeled samples. To the best of knowledge ECN is the Ô¨Årst Ô¨Çexible method that can correct diverse types of structured label errors.2 RELATED WORKS "
197,Self-supervised Neural Architecture Search.txt,"Neural Architecture Search (NAS) has been used recently to achieve improved
performance in various tasks and most prominently in image classification. Yet,
current search strategies rely on large labeled datasets, which limit their
usage in the case where only a smaller fraction of the data is annotated.
Self-supervised learning has shown great promise in training neural networks
using unlabeled data. In this work, we propose a self-supervised neural
architecture search (SSNAS) that allows finding novel network models without
the need for labeled data. We show that such a search leads to comparable
results to supervised training with a ""fully labeled"" NAS and that it can
improve the performance of self-supervised learning. Moreover, we demonstrate
the advantage of the proposed approach when the number of labels in the search
is relatively small.","Recently there has been an increasing interest in Neural Architecture Search (NAS). NAS algorithms emerge as a powerful platform for discovering superior network architectures, which may save time and effort of humanexperts. The discovered architectures have achieved stateoftheart results in several tasks such as image classiÔ¨Åcation [1, 2] and object detection [3]. The existing body of research on NAS investigated several common search strategies. Reinforcement learning [ 4] and evolutionary algorithms [ 5,6] were proven to be successful but required many computational resources. Various recent methods managed to reduce search time signiÔ¨Åcantly. For example, Liu et al. [ 7] suggested relaxing the search space to be continuous. This allowed them to perform a differentiable architecture search (DARTS), which led to novel network models and required reasonable resources (few days using 14 GPUs). NAS methods learn from labeled data. During the search process, various architectures are considered and their value is estimated based on their performance on annotated examples. However, acquiring large amounts of humanannotated data is expensive and timeconsuming, while unlabeled data is much more accessible. As current NAS techniques depend on annotations availability, their performance deteriorates when the number of annotations per each class becomes small. This remains an open problem for NAS, where research till now has focused on the supervised learning approach. The dependency on labeled data is not unique only to NAS but is a common problem in deep learning. Largescale annotated datasets play a critical role in the remarkable success of many deep neural networks, leading to stateoftheart results in various computer vision tasks. Considering how expensive it is to acquire such datasets, a growing body of research is focused on relieving the need for such extensive annotation effort. One promising lead in this direction is selfsupervised learning (SSL) [ 8,9,10]. Selfsupervised methods learn visual features from unlabeled data. The unlabeled data is used to automatically generate pseudo labels for a pretext task. In the course of training to solve the pretext task, the network learns visual features that can be transferred to solving other tasks with little to no labeled data. Contrastive SSL is a subclass of SSL that has recently gained Preprint. Under review.arXiv:2007.01500v1  [cs.LG]  3 Jul 2020Figure 1: The SSNAS framework. We perform network architecture search in an unsupervised manner (with no data labels) by using a contrastive loss that enforces similarity between two different augmentations of the same input image. Both augmentations pass through the same network (i.e., the weights are shared). attention thanks to its promising results [ 11,12,13,14,15]. This family of techniques contrasts positive samples and negative samples to learn visual representations. Contribution. Inspired by the success of SSL for learning good visual representations, we propose to apply selfsupervision in NAS to rectify its limitation with respect to the availability of data annotations. We propose a SelfSupervised Neural Architecture Search (SSNAS), which unlike conventional NAS techniques, can Ô¨Ånd novel architectures without relying on data annotations. Instead of using selfsupervision to learn visual representations, we employ it to learn the architecture of deep networks (see Figure 1). We apply our new strategy with the popular DARTS [ 7] method. We adopt their differentiable search, which allows using gradientbased optimization, but replace their supervised learning objective with a contrastive loss that requires no labels to guide the search. In particular, we adopt the method used in the SimCLR framework [ 11]. This approach for learning visual representations has recently achieved impressive performance in image classiÔ¨Åcation. We adapt their approach to the architecture search process. We perform a composition of transformations on the inputs, which generates augmented images and look for the model that maximizes the similarity between the representations of the augmented images that originate from the same input image. As the focus of this work is on efÔ¨Åcient search, we limit the used batch sizes to be the ones that can Ô¨Åt a conventional GPU memory. This allows SSNAS to efÔ¨Åciently learn novel network models without using any labeled data. We demonstrate that our selfsupervised approach for NAS achieves results comparable to the ones of its equivalent supervised approach. Moreover, we show that SSNAS not only achieves the same results as supervised NAS, but it also succeeds in some scenarios where the supervised method struggles. SpeciÔ¨Åcally, SSNAS can learn good architectures from data with a small number of annotated examples available for each class. We also demonstrate the potential of using NAS to improve unsupervised learning. We show some examples where SSL applied with the learned architectures generates visual representations that lead to improved performance. Thus, this work shows that SSL can both improve NAS and be improved by it. 22 Related work "
198,Instance-Aware Graph Convolutional Network for Multi-Label Classification.txt,"Graph convolutional neural network (GCN) has effectively boosted the
multi-label image recognition task by introducing label dependencies based on
statistical label co-occurrence of data. However, in previous methods, label
correlation is computed based on statistical information of data and therefore
the same for all samples, and this makes graph inference on labels insufficient
to handle huge variations among numerous image instances. In this paper, we
propose an instance-aware graph convolutional neural network (IA-GCN) framework
for multi-label classification. As a whole, two fused branches of sub-networks
are involved in the framework: a global branch modeling the whole image and a
region-based branch exploring dependencies among regions of interests (ROIs).
For label diffusion of instance-awareness in graph convolution, rather than
using the statistical label correlation alone, an image-dependent label
correlation matrix (LCM), fusing both the statistical LCM and an individual one
of each image instance, is constructed for graph inference on labels to inject
adaptive information of label-awareness into the learned features of the model.
Specifically, the individual LCM of each image is obtained by mining the label
dependencies based on the scores of labels about detected ROIs. In this
process, considering the contribution differences of ROIs to multi-label
classification, variational inference is introduced to learn adaptive scaling
factors for those ROIs by considering their complex distribution. Finally,
extensive experiments on MS-COCO and VOC datasets show that our proposed
approach outperforms existing state-of-the-art methods.","As a fundamental task in computer vision, multilabel image recog nition aims to accurately and simultaneously recognize multiple objects present in an image. Compared to singlelabel image clas sification, multilabel recognition is more challenging because of usually complex scene, more wide label space, and implicit corre lation of objects. In view of the natural cooccurrence of objects in the realworld scene, multilabel image classification is more practical than the singlelabel one, and has received wide attention [3, 10, 12, 20] in recent years. Numerous algorithms have been proposed for multilabel image classification. In early works, deep Convolutional Neural Networks ‚àóBoth authors contributed equally to this research. Sports  Ball Tennis  Racket Person Tie Bike Person Sports  Ball Tennis  Racket Person Tie Bike BikeP(Person )=1.0 P(Bike)=1.0Figure 1: We first construct the directed graph accord ing to the conditional probabilities of labels statistical co occurrence information from training set, and then predict the labels scores of regions extracted from each current im age. We apply the labels scores about regions to enhance their corresponding correlation in the directed graph, which means that the arrow lines among the predicted labels are bold. (CNNs) used for singlelabel recognition [ 13,14,25,26] are lever aged for the multilabel task by treating the multilabel recognition as a set of binary classification tasks. Although boosting the accu racy of multilabel classification, however, this type of methods is still limited due to the ignorance of cooccurrence among objects, which can be reflected in label correlation. To model label dependen cies, three lines of works have been proposed in recent literatures including attention mechanism based [ 26,30], recurrent neural network based [ 28], and graph based algorithms [ 6,18,19]. Specifi cally, in graph based methods, graph convolution is introduced to characterize label correlations by diffusing label dependencies with a label correlation matrix (LCM). With graph inference on labels, graph convolutional neural network (GCN) and its variants [ 6,29] have reported stateoftheart performances in recent literatures.arXiv:2008.08407v1  [cs.CV]  19 Aug 2020The success of GCN [ 16] indicates the significance of label cor relation captured through the LCM for promoting the multilabel classification. In previous GCN [ 6,29] based works, the LCM is global and datasetdependent as it is obtained through the statistic of accessible data. However, in view of the large variation among images, the prior knowledge of label correlation may not well suit for all samples. For instance, the statistical cooccurrence of the bike and person is low, which may mislead the classification for the images of riders. Therefore, individual characteristics of label correlation should also be considered, and used to adaptively mod ify the prior knowledge. Here, we attempt to construct an adaptive individual LCM for each image instance by utilizing the rough classification scores of ROIs. As shown in Figure 1, as an intuitive understanding, if all ROIs indicate high appearance probabilities of some labels, such as person and bike, then the correlation between person and bike in statistical LCM should be accordingly enhanced for this specific image. In this paper, we propose an instanceaware graph convolutional neural network (IAGCN) framework for multilabel classification. The core idea is to adaptively construct one imagedependent label correlation matrix (IDLCM) for each given image, which better favours the graph inference on labels. For framework construction, considering the previous success of GCNbased methods [ 6,29], we build two fused branches of subnetworks in the framework: a global branch modeling the whole image, and an additional region based branch inferring on ROIs. Moreover, graph inference on labels is conducted to inject labelawareness into the both branches. In this process, different from previous works using statistical LCM [ 6], an imagedependent LCM is constructed by fusing both the statistical LCM and an individual one of each image instance. Specifically, the individual LCM of each image is obtained by mining the label dependencies based on the scores of detected ROIs. Considering the contribution differences of ROIs to multilabel classification, during the generation of the individual LCM, variational inference [15] is introduced to learn adaptive scaling factors for those ROIs by considering their complex distribution. As a result, the image dependent LCM is flexible and benefits the proposed framework in adaptively propagating information on labels for each image instance. Finally, the learned features of both the global and local branches are fused and jointly modeled for the multilabel classifica tion. We test the performance on MSCOCO and VOC datasets, and the results show that our proposed approach outperforms existing stateoftheart methods. The main contributions of this paper are as follows: ‚Ä¢We propose a novel IAGCN framework for the multilabel classification task by jointly modeling the global context of the whole image and local dependencies of ROIs with adaptive information propagation on labels. ‚Ä¢A novel imagedependent LCM is constructed based on both the statistical LCM and an individual one of each image, which endows graph convolution flexibility to handle huge correlation variations among numerous image instances. ‚Ä¢We introduce variational inference to explore label depen dencies by considering the distribution of ROI appearances, which results in an adaptive LCM for each image instance.‚Ä¢We report the stateoftheart performances on both MS COCO and VOC datasets, which verifies the effectiveness of our framework. 2 RELATED WORK "
199,End-to-end Recurrent Denoising Autoencoder Embeddings for Speaker Identification.txt,"Speech 'in-the-wild' is a handicap for speaker recognition systems due to the
variability induced by real-life conditions, such as environmental noise and
the emotional state of the speaker. Taking advantage of the principles of
representation learning, we aim to design a recurrent denoising autoencoder
that extracts robust speaker embeddings from noisy spectrograms to perform
speaker identification. The end-to-end proposed architecture uses a feedback
loop to encode information regarding the speaker into low-dimensional
representations extracted by a spectrogram denoising autoencoder. We employ
data augmentation techniques by additively corrupting clean speech with
real-life environmental noise in a database containing real stressed speech.
Our study presents that the joint optimization of both the denoiser and speaker
identification modules outperforms independent optimization of both components
under stress and noise distortions as well as hand-crafted features.","Speech in real life is commonly noisy and under unconstrained conditions that are dicult to predict and aggravate their recognition and understanding. E. RituertoGonz alez Group of Multimedia Processing, Department of Signal Theory and Communications, University Carlos III of Madrid, Av. Universidad 30, Legan es, 28911 Madrid, Spain Email: erituert@ing.uc3m.es C. Pel aezMoreno Group of Multimedia Processing, Department of Signal Theory and Communications, University Carlos III of Madrid, Av. Universidad 30, Legan es, 28911 Madrid, Spain Email: carmen@tsc.uc3m.esarXiv:2003.07688v5  [eess.AS]  16 May 20222 Esther RituertoGonz alez, Carmen Pel aezMoreno Speaker Recognition (SR) systems need high performance under these `real world' conditions. This is extremely dicult to achieve due to both extrinsic and intrinsic variations and is commonly referred to as Speaker Recognition inthewild . Extrinsic variations encompass background chatter and music, environmental noise, reverberation, channel and microphone eects, etc. On the other hand, intrinsic variations are the inherent factors to the speakers themselves present in speech, such as age, accent, emotion, intonation or speaking rate Stoll (2011). Automatic speech recognition (ASR) systems aim to extract the linguistic information from speech in spite of the intrinsic and extrinsic variations Villalba et al. (2020). However, speaker recognition (SR) takes advantage of the intrinsic or idiosyncratic variations to nd out the uniqueness of each speaker. Besides intraspeaker variability (emotion, health, age), the speaker identity results from a complex combination of physiological and cultural aspects. Still, the role of emotional speech has not been deeply explored in SR. Although it could be considered an idiosyncratic trait, it poses a challenge due to the distortions it produces on the speech signal. It in uences the speech spectrum signicantly, having a considerable impact on the features extracted from it and deteriorating the performance of SR systems. At the same time, extrinsic variations have been a long standing challenge aecting the basis of all speech technologies. Deep Neural Networks have given rise to substantial improvements due to their ability to deal with realworld, noisy datasets without the need for handcrafted features specically designed for robustness. One of the most important ingredients to the success of such methods, however, is the availability of large and diverse training datasets. 1.1BINDI, a cyberphysical system to combat genderbased violence Our goal in this paper is to include a robust Speaker Identication module in BINDI, a cyberphysical system designed to combat genderbased violence within the project EMPATIA1. Adopting a multimodal approach RituertoGonz alez et al. (2020), BINDI employs intelligent signal processing to autonomously detect a violent situation by means of biosignals collected by smart sensors {including a microphone{ integrated in wearable edge devices interconnected through the smartphone of the user. One of the edge devices is currently conceptualized as a smart bracelet that includes three dierent physiological sensors: Blood Volume Pulse (BVP), Galvanic Skin Response (GSR) and Skin Temperature (SKT) Miranda et al. (2017); Miranda Calero et al. (2018). The second is designed as a smart pendant equipped with a microelectromechanical microphone and able to acquire audio and speech from the environment and the user RituertoGonz alez et al. (2019). This body area network factor form 1http://portal.uc3m.es/portal/page/portal/inst_estudios_genero/proyectos/ UC3M4SafetyTitle Suppressed Due to Excessive Length 3 allows to gather data from the desired sources of information to feed the dierent intelligent decision engines capable of alerting of a violent situation. The duration of the batteries of Bindi has been identied as one of the key features to make it practical for genderbased violence victims (GVV). To this end, a lightweight aective computing system based on the biosignals captured by the smart bracelet acts as a rst trigger. These biosignals are periodically captured and processed within this edge device that provides a rapid inference schema. Note that the response provided by this rst physiological layer is designed as a lowcost and lowconsumption trigger. Due to the importance of avoiding missing any potentially violent situation, this trigger is oriented towards detecting any possible cues of fear or panicrelated emotions and therefore to achieve a high recall rate. However, due to its constrained computational capacity and resources, its precision is not as good as it could be under ideal conditions. Then, once an alarm is triggered by the bracelet, the pendant begins to acquire audio, which is compressed and sent via Bluetooth Low Energy (BLE) to the smartphone or smartwatch. The purpose of this acquisition is twofold: rst,this audio is encrypted and sent to a secure server to be used as an evidence in a potential trial Campos Gavi~ no and Larrabeiti L opez (2020) and second, it is used to disambiguate and contextualise the previous biosignals and therefore to improve the overall detection rate. Since the processing of the audio signal takes place in the smartphone, it allows for more energyconsuming methods. In particular, we aim at processing the speech contained in the audio to identify and track the user's voice RituertoGonz alez et al. (2018) and to detect fear and panic. Therefore, the speech information complements that carried by the physiological biosignals. In this paper, our scope is limited to the processing of the audio modality and in particular to the Speaker Identication (SI) task. In the threatening situations we intend to detect, identifying the user and achieving high SI rates is crucial, and it is most likely that the speaker is under an intense negative emotional state, such as panic, fear, anxiety, or its more moderate relative, stress. 1.2Contribution In this paper, we address the combined problem of the lack of environmental noise robustness of SR systems and the eects of negative emotional speech on their performance. Our contribution capitalizes on using robust speaker discriminator oriented embeddings extracted from a Recurrent Denoising Autoencoder combined with a Shallow Neural Network {a feedforward neural network{ acting as a backend classier for the task of Speaker Identication, as detailed in Figure 1. This endtoend architecture is designed to work under adverse conditions, both from the point of view of distorted speech due to stressing situations, and environmental noise.4 Esther RituertoGonz alez, Carmen Pel aezMoreno We choose speech recorded under spontaneous stress conditions due to its reallife nature. Induced, simulated or acted emotions {especially negative ones{ are known to be perceived and automatically detected much more strongly than real emotions. This suggests that actors are prone to overacting, which casts doubt on the reliability of these samples Wilting et al. (2006), being a big drawback for devices working in real life conditions such as BINDI. Moreover, we augment our database with synthetic noisy signals by additively contaminating the dataset with environmental noise to emulate speech recorded in real conditions. We discuss a recurrent denoising autoencoder architecture based on Gated Recurrent Units (GRU), where the recurrent architecture targets modelling the temporal context of speech utterances. The encoder network extracts frame level embeddings from the speech spectrograms and is jointly optimized with a feed forward network whose output layer calculates speaker class posteriors. With the help of the denoising module that attempts to remove environmental noise information, and the SNN that targets recognizing the speaker, all information that is not directly employed for speakers' identication is dismissed from the embeddings. In particular, the loss function associated with this last dense network is also fed into the denoising autoencoder to guide its eorts towards the SR task, as will be further described in section 3. Finally, we put forward that these speaker discrimination oriented embeddings are more robust to noise and stress variability than those optimized separately by comparing the eects of automatically extracted embeddings by this twostage connected architecture against the two modules separately, handcrafted features previously demonstrated to be suited for this problem and a frequency recurrent alternative obtained by transposing the inputs to the GRU autoencoder. Moreover, we achieve these results with a computationally lightweight multitask endtoend architecture that extracts emotion and noiserobust speaker discriminant embeddings, in spite of the few datasets available for this purpose. 2 Related Work "
200,Learning Compact Appearance Representation for Video-based Person Re-Identification.txt,"This paper presents a novel approach for video-based person re-identification
using multiple Convolutional Neural Networks (CNNs). Unlike previous work, we
intend to extract a compact yet discriminative appearance representation from
several frames rather than the whole sequence. Specifically, given a video, the
representative frames are selected based on the walking profile of consecutive
frames. A multiple CNN architecture incorporated with feature pooling is
proposed to learn and compile the features of the selected representative
frames into a compact description about the pedestrian for identification.
Experiments are conducted on benchmark datasets to demonstrate the superiority
of the proposed method over existing person re-identification approaches.","Person reidentication(reid) has been widespread concerned recently, as this issue underpins various crit ical applications such as video surveillance, pedestrian tracking and searching. Given a target person appear ing in a surveillance camera, a reid system generally aims to identify it in the other cameras through the whole cameranetwork, i.e., determining whether in stances captured by dierent cameras belong to the same person. However, due to the in uence of clut tered background, occlusions and viewpoint variations across camera views, this task is quite challenging. A reid system may have an image or a video as in put for feature extraction. Since only limited informa tion can be exploited from a single image, it is dicult to overcome the occlusion, cameraview and pose vari ation problems and to capture the varying appearance of a pedestrian performing dierent action primitives. Thus it is better to deal with the videobased reidproblem, as videos inherently contain more temporal information of the moving person than an independent image, not to mention in many practical applications the input are videos to begin with. Besides, video is a sequence of images, so spatial and temporal cues are more abundant in a video than in a image, which can facilitate extracting more features. Figure 1. Salient appearance in person reid. In spite of the rich spacetime information provided by a video sequence, more challenges come along. So far, only a few videobased methods have been pre sented [26], [15], [20]. Most of them focus on investigat ing the temporal information related to person's mo tion, such as their gait, and perhaps even the patterns of how their bodies and clothes move. Although such movement is one type of behavioral biometrics, it is unfortunate that a large number of persons share simi larity in walking manners and related behavior [29] [31]. Moreover, since gait is considered a biometric that is not aected by the appearance of a person, most ap proaches tried to exploit it by working with silhou ettes, which are dicult to extract, especially from surveillance data with cluttered background and occlu sions [15]. Besides, timeseries analysis usually requires extracting information at dierent timescales [20]. In 1arXiv:1702.06294v2  [cs.CV]  20 Sep 2019¬∑¬∑¬∑ CNN CNN CNN CNN Feature Pooling CNN CNN CNN CNN Feature Pooling YES or NO ¬∑¬∑¬∑ CNN based feature  learning Distance metric  learning Representative frame  extraction Query set Gallery setFigure 2. An overview of the proposed videobased reid framework. the person reid problem, gait information often exists in short time, thus the information provided by move ment descriptors is limited. In some cases, it is even harder to distinguish the video representations of dif ferent identities than the stillimage appearance [29]. Unlike previous work, in this paper we intend to extract a compact appearance representation from sev eral representative frames rather than the whole frames for videobased reid. Compared to the temporalbased methods, the proposed appearance model works more similarly to human visual system. Because the visual perception studies on appearance (e.g., color, texture) and motion stimuli have shown that the pattern detec tion thresholds are much lower than the motion detec tion thresholds [24] [13] [5]. Hence, human performs better at identifying the appearance of human body or belongings than the manners of how a person walks. In most cases, people can be distinguished more eas ily from appearance such as clothes and bags on their shoulders than from gait and pose which are generally similar among dierent persons [21], as shown in Fig. 2. So, videos are highly redundant and it is unneces sary to incorporate all frames for person reid. Our study shows that several typical frames with appro priate feature extraction can oer competitive or even better identication performance. More specically, given a walking sequence, we rst split it into a couple of segments corresponding to dif ferent action primitives of a walking cycle. The most representative frames are selected from a walking cy cle by exploiting the local maxima and minima of theFlow Energy Prole (FEP) signal [26]. For each frame, we propose a CNN to learn feature based on person's joint appearance information. Since dierent frames may have dierent discriminative features for recogni tion, by introducing an appearancepooling layer, the salient appearance features of multiple frames are pre served to form a discriminative feature descriptor for the whole video sequence. The central point of our al gorithm lies in the exploration of the key appearance information of a video, contrary to the conventional methods like [20] and [15], which highly rely on accu rate temporal information. 2. Related work "
201,Learning from Noisy Labels with Distillation.txt,"The ability of learning from noisy labels is very useful in many visual
recognition tasks, as a vast amount of data with noisy labels are relatively
easy to obtain. Traditionally, the label noises have been treated as
statistical outliers, and approaches such as importance re-weighting and
bootstrap have been proposed to alleviate the problem. According to our
observation, the real-world noisy labels exhibit multi-mode characteristics as
the true labels, rather than behaving like independent random outliers. In this
work, we propose a unified distillation framework to use side information,
including a small clean dataset and label relations in knowledge graph, to
""hedge the risk"" of learning from noisy labels. Furthermore, unlike the
traditional approaches evaluated based on simulated label noises, we propose a
suite of new benchmark datasets, in Sports, Species and Artifacts domains, to
evaluate the task of learning from noisy labels in the practical setting. The
empirical study demonstrates the effectiveness of our proposed method in all
the domains.","With the recent advancements in deep convolutional neural networks (CNN) [ 11], learning from a clean large scale dataset, e.g., ImageNet [ 17], has been very successful in various visual recognition tasks. However, collecting such datasets is time consuming and expensive. Recent efforts, therefore, have been focused on building and learn ing from an Internetscale dataset with noisy labels such as YFCC100M [ 20] and YouTube8M1. These datasets have the potential of leveraging a seemingly inÔ¨Ånite amount of images and videos on the Internet. But labels in those datasets are noisy in terms of visual correlation and hence challenging for the learning process. Previous approaches tried to circumvent the problem of learning from noisy samples by treating them as statistical outliers and discarding them using some variants of outlier detection methods [ 16,12,18]. However, in practice, it is typical that noisy samples are not statistical outliers but rather some form of signiÔ¨Åcant mass. Existing approaches have shown to produce inferior results on these cases. For ex 1https://research.google.com/youtube8m/ Knowledge	Graph	  Large	Noisy	Dataset	 Bird	 Vertebrate	 Mammal	 Rabbit	Lagomorph	 Leporidae	phylum	 class	 order	 family	Fish	 Impala	 Beetle	Arthropod	 phylum	 Figure 1: Overview of the proposed system to learn from noisy labels by leveraging a knowledge graph. The left panel shows the large scale noisy dataset, out of which we collect a small set of images with clean labels to guide the learning process. On the right panel, we demonstrate the knowledge graph on the species domain constructed from DBpediaWikipedia. ample, images collected by searching polysemy words, such asapple , will show a multimodal distribution of visual con cepts, in which case statistical outlier detection techniques will fail to Ô¨Ågure out which concept to be associated with. Another example, images labeled with basketball on Flickr may contain a signiÔ¨Åcant amount of group shots, selÔ¨Åes, and photos taken before or after the game ‚Äì these are less visually relevant to the event itself but regardless forming a signiÔ¨Åcant mass; statistically, they are not outliers. Recently, Hinton et al. [9] introduced the concept of ‚Äúdis tillation‚Äù to transfer the knowledge learned from one model (expert or teacher model) to another (a lightweight student model), by treating the prediction results produced from the Ô¨Årst model (usually more expensive to train) as the ‚Äúsoft target‚Äù labels for training the second light model (usually trained in a more constrained setting). Inspired by this, we propose a new technique that uses a similar distillation pro cess to learn from noisy datasets. In our scenario, we assume that we have a small clean dataset and a large noisy dataset. The small clean dataset can either be an existing public dataset or labeled from part of the noisy data. Our goal is to use the large amount of noisy data to augment the small 1 arXiv:1703.02391v2  [cs.CV]  7 Apr 2017clean dataset to learn a better visual representation and clas siÔ¨Åer. Concretely, we distill the knowledge learned the small clean dataset to facilitate learning a better model from the entire noisy dataset. This is different from Hinton et al. [9], where distillation is used to transfer knowledge from a better model (e.g., an ensemble model) to guide learning a light but typically inferior model. Furthermore, we propose to integrate a knowledge graph to guide the distillation process, where rich relational information among labels are explicitly encoded in the learning process. This helps the algorithm to disambiguate noisy labels by, e.g., knowing that apple can either be a fruit category or a company name. To evaluate our technique, we collect a suite of new datasets on three topics: sports ,species , and artifacts . Our dataset contains a total of 480K images from 780 class cat egories and exhibit the realworld labeling noise we men tioned above. We build a textual knowledge graph on top of these three topics based on Wikipedia, where labels are related by their deÔ¨Ånitions. We show that, our proposed dis tillation process, as well as leveraging the knowledge graph to guide the distillation process, can achieve the best results on our datasets compared with competing methods. In summary, we make the following contributions: ‚Ä¢We propose a novel algorithm based on a distillation process to learn from noisy data, with a theoretical analysis under some conditions. ‚Ä¢We leverage a knowledge graph to guide the distillation process to further ‚Äúhedge the risk‚Äù of learning from noisy labels. ‚Ä¢We collect several new benchmark datasets with real world labeling noises. We extensively compare with different baselines and show that our proposed algo rithm achieves the best results2. 2. Related Work "
202,Image to Video Domain Adaptation Using Web Supervision.txt,"Training deep neural networks typically requires large amounts of labeled
data which may be scarce or expensive to obtain for a particular target domain.
As an alternative, we can leverage webly-supervised data (i.e. results from a
public search engine) which are relatively plentiful but may contain noisy
results. In this work, we propose a novel two-stage approach to learn a video
classifier using webly-supervised data. We argue that learning appearance
features and then temporal features sequentially, rather than simultaneously,
is an easier optimization for this task. We show this by first learning an
image model from web images, which is used to initialize and train a video
model. Our model applies domain adaptation to account for potential domain
shift present between the source domain (webly-supervised data) and target
domain and also accounts for noise by adding a novel attention component. We
report results competitive with state-of-the-art for webly-supervised
approaches on UCF-101 (while simplifying the training process) and also
evaluate on Kinetics for comparison.","Action recognition in videos is a wellstudied problem in computer vision with many important applications in ar eas such as surveillance, search, and humancomputer in teraction. Training deep neural networks typically requires a large labeled dataset. However, it may be difÔ¨Åcult to ob tain enough labeled data because it may be too scarce or too expensive to obtain. We can instead leverage webly supervised data (i.e. results from a public search engine) which are relatively plentiful but may be noisy. The highlevel overview of our model is shown in Fig ure 1. The noisy web image and web video domains are considered source domains that we want to domain adapt into the target domain. We present a twostage approach to Ô¨Årst learn an image model using a 2DCNN, transfer the learned spatial weights to a 3DCNN, and continue training a video model. Since our goal is to learn a video classiÔ¨Åer, we can potentially learn from web videos only, but we ar Noisy Web  Image  Noisy Web  Video Target Video 2D CNN  3D CNN Domain  Shift Domain Shift  Domain Shift Transfer Learned  Spatial Filters Figure 1: Given weblysupervised images and videos (source domains), we learn a video classiÔ¨Åer for the tar get domain. The model is learned in a twostage process by 1) learning an image model (2DCNN) and 2) transfer ring the spatial Ô¨Ålters to the video model (3DCNN) to con tinue training. The model also accounts for domain shift and noise present in the weblysupervised data. gue that our proposed twostage process is more appropriate for learning from noisy, weblysupervised data. Web videos are likely to be noisier than web images since web videos typically contain many frames that are irrelevant to the tar get concept. Thus it may be easier to learn spatial features Ô¨Årst, based on the relatively cleaner web images, and then learn the temporal features afterward. Previous work [25] has also hypothesized that it may be difÔ¨Åcult to learn both spatial and temporal features simultaneously. We present empirical results in Section 4 showing that our twostage process, which separates learning appearance and temporal features, outperforms a model that learns both jointly. In addition to the challenges of learning the appearance and motion, there are two additional issues with training on weblysupervised data. First, there is potential domain shift between the different domains. For example, compar ing web images and videos, many web images are typically highresolution and shot with highquality cameras, while web videos are typically lower resolution and may contain motion blur and other artifacts. Second, there may be noise present in weblysupervised data that may degrade perfor 1arXiv:1908.01449v1  [cs.CV]  5 Aug 201930  20  10  0 10 2030 20 10 0102030TSNE Plot (before DA) for BalanceBeam web image web video curated video 30  20  10  0 10 20 3020 10 0102030TSNE Plot (before DA) for LongJump web image web video curated video 20  10  0 10 20 3020 10 01020TSNE Plot (before DA) for Surfing web image web video curated video 30  20  10  0 10 20 3040 30 20 10 0102030TSNE Plot (before DA) for ThrowDiscus web image web video curated video 20  10  0 10 2020 10 0102030TSNE Plot (after DA) for BalanceBeam web image web video curated video 20  10  0 10 20 3020 10 01020TSNE Plot (after DA) for LongJump web image web video curated video 20  10  0 10 2020 10 01020TSNE Plot (after DA) for Surfing web image web video curated video 20  10  0 10 20 3030 20 10 0102030TSNE Plot (after DA) for ThrowDiscus web image web video curated videoFigure 2: TSNE Plots. We randomly sampled from the web image ( redpoints), web video ( green points) and target video (blue points) (UCF101 [23]) domains and show the TSNE [28] plots of 4 actions: balance beam, long jump, surÔ¨Ång, and throw discus. The Ô¨Årst row contains the TSNE plot before domain adaptation using pretrained RN34 [11] and the second row shows the same actions after the network has been domain adapted. Plot best viewed in color. mance. For example weblysupervised data may contain either the wrong concept entirely or a mix of relevant and irrelevant concepts (i.e. only a subset of frames in a video may correspond to the target concept). To account for domain shift, domain adaptation has been successfully used for tasks such as mapping from MNIST [14] to StreetView digits [27, 9], RGB to depth im ages [27] and webcam to product images [9]. In our work we incorporate an adversarial training component taken from Generative Adversarial Networks (GAN) [10]. To ac count for the noise present in weblysupervised data, we incorporate a novel attention component to reduce the ef fect of irrelevant examples, inspired by attention models for machine translation [1]. In this work, the target domain consists of curated videos, containing only a single concept or activity. We consider these curated videos to be a separate domain from web images and web videos. We assume there are relatively few irrelevant chunks from videos in the target domain com pared to web videos. For example, this setting may be ap propriate if the target domain was surveillance videos. To check whether there is indeed a difference be tween the separate domains, we extracted embeddings from random images/frames from each domain using ResNet 34 [11] and visualized TSNE [28] plots for four differ ent action categories from UCF101 [23]: Balance Beam, Long Jump, SurÔ¨Ång, Throw Discus. The top row corresponds to the embeddings before domain adaptation (DA) for curated video frames ( blue points), web video frames (green points), and web images ( redpoints). The bottom row corresponds to the embeddings after our DA (detail in Section 3). In the top row, before DA, there are visi bly distinct regions corresponding to the three domains of web images, web videos and curated videos (we used UCF 101 [23] videos), which may indicate domain differences. After DA, the different domains are packed closer together. To summarize, our contributions include: A novel twostage approach to Ô¨Årst learn spatial weights from a 2DCNN and then transfer these weights to a 3DCNN to learn temporal weights. A novel attention component to account for noise present in weblysupervised data. Results competitive with stateoftheart on UCF101 [23], while simplifying training. 2. Related Work "
203,Ensemble Neural Relation Extraction with Adaptive Boosting.txt,"Relation extraction has been widely studied to extract new relational facts
from open corpus. Previous relation extraction methods are faced with the
problem of wrong labels and noisy data, which substantially decrease the
performance of the model. In this paper, we propose an ensemble neural network
model - Adaptive Boosting LSTMs with Attention, to more effectively perform
relation extraction. Specifically, our model first employs the recursive neural
network LSTMs to embed each sentence. Then we import attention into LSTMs by
considering that the words in a sentence do not contribute equally to the
semantic meaning of the sentence. Next via adaptive boosting, we build
strategically several such neural classifiers. By ensembling multiple such LSTM
classifiers with adaptive boosting, we could build a more effective and robust
joint ensemble neural networks based relation extractor. Experiment results on
real dataset demonstrate the superior performance of the proposed model,
improving F1-score by about 8% compared to the state-of-the-art models.","Many NLP tasks have been built on different knowledge bases, such as Freebase and DBPedia. However, the knowl edge bases could not cover all the facts in the real world. Therefore, it is essential to extract more common relational facts automatically in open domain corpus. As known, rela tion extraction (RE) aims at extracting new relation instances that are not contained in the knowledge bases from the un structured open corpus. It aligns the entities in the open cor pus with those in the knowledge bases and retrieves the en tity relations from the real world. For example, if we aim to retrieve a relation from the raw text, ‚Äú Barack Obama mar ried Michelle Obama 10 years ago ‚Äù, a naive approach would be to search the news articles for indicative phrases, such as ‚Äúmarry ‚Äù or ‚Äú spouse ‚Äù. However, the result may be wrong since human language is inherently various and ambiguous. Corresponding authorPrevious supervised RE methods require a large amount of labelled relation training data by humanhand. To address this issue, Mintz et al. [Mintz et al. , 2009 ]proposed an ap proach via aligning the entity in KB for later extraction with out plenty of training corpus. However, their assumption  there is only one relation existing in a pair of entities, was ir rational. Therefore, later researches assumed more than one relation could exist between a pair of entities. Hoffmann et al.[Hoffmann et al. , 2011 ]proposed a multiinstance learn ing model with overlapping relations (MultiR) that combined a sentencelevel extraction model for aggregating the individ ual facts. Surdeanu et al. [Surdeanu et al. , 2012 ]proposed a multiinstance multilabel learning model (MIMLRE) to jointly model the instances of a pair of entities in text and all their labels. The major limitation of the above methods is that they cannot deeply capture the latent semantic infor mation from the raw text. It is also challenging for them to seamlessly integrate semantic learning with feature selection to more accurately perform RE. Recently, deep neural networks are widely explored for re lation extraction and have achieved signiÔ¨Åcant performance improvement [Zeng et al. , 2015; Lin et al. , 2016 ]. Compared with traditional shallow models, deep models can deeply cap ture the semantic information of a sentence. Zeng et al. [Lin et al. , 2016 ]employed CNN with sentencelevel attention over multiple instances to encode the semantics of sentences. Miwa and Bansal [Miwa and Bansal, 2016 ]used a syntax treebased long shortterm memory networks (LSTMs) on the sentence sequences. Ye et al. [Yeet al. , 2017 ]proposed a uni Ô¨Åed relation extraction model that combined CNN with a pair of ranking class ties. However, the main issue of existing deep models is that their performance may not be stable and could not effectively handle the quite imbalanced, noisy, and wrong labeled data in relation extraction even if a large number of parameters in the model. To address the above issues, in this paper we propose a novel ensemble deep neural network model to extract re lations from the corpus via an Adaptive Boosting LSTMs with Attention model (AdaLSTMs). SpeciÔ¨Åcally, we Ô¨Årst choose bidirectional long shortterm memory networks to embed forward and backward directions of a sentence for bet ter understanding the sentence semantics. Considering the fact that the words in a sentence do not contribute equally to the sentence representation, we import attention mechaarXiv:1801.09334v2  [cs.IR]  28 Apr 2018reweighting	the	gradient	ùõøfor	sample	withùê∑#(s&)ùêøùëÜùëáùëÄ,Œ≥,(ùë•)	Œ≥0(ùë•)	Œ≥1(ùë•)	Œ≥#2,(ùë•)	Œ≥#(ùë•)	ùõº,ùõº0ùõº1ùõº#2,UAttentionùëô,ùëô5ùëô62,ùëô6Ensenble	Model		Œ•words	ùëüùë§:of	the	sentence	ùë†1positions	of	ùëüùë§:feature LayerUUUUdropoutùëô0: LSTM networksU: twodirectionalLSTMs‚Äô unit: dropout: LSTMs‚Äô traditional forward: LSTMs‚Äô backward, update with ùê∑<(ùë†1)ùëô1: label of sentence ùë†1ùõº#	ùêøùëÜùëáùëÄùêøùëÜùëáùëÄ0ùêøùëÜùëáùëÄ1ùêøùëÜùëáùëÄ=2,ùêøùëÜùëáùëÄ#Symbols:Figure 1: The framework of AdaLSTMs contains three layers: feature layer, bidirectional Stacked LSTMs‚Äô layer with attention and adaptive boosting layer. siindicates the original input sentence with a pair of entities and their relation. nism to the bidirectional LSTMs. Next we construct multi ple such LSTM classiÔ¨Åers and ensemble their results as the Ô¨Ånal prediction result. Kim and Kang [Kim and Kang, 2010 ] showed that ensemble with neural networks perform better than one single neural network in prediction tasks. Motivated by their work, we import adaptive boosting and tightly cou ple it with deep neural networks to more effectively and ro bustly solve the relation extraction problem. The key role of adaptive boosting in our model is reweighting during the training process. The weight of incorrectly classiÔ¨Åed samples will increase. In other words, the samples classiÔ¨Åed wrongly gain more attention so that the classiÔ¨Åer is forced to focus on these hard examples. Note that attention can distinguish the different importance of words in the sentence, while adap tive boosting can use sample weights to inform the training of neural networks. In a word, the combination of the two can more precisely capture the semantic meaning of the sen tences and better represent them, and thus help us train a more accurate and robust model. We summarize the contributions of this paper as follows. We propose a Multiclass Adaptive Boosting Neural Networks model, which to our knowledge is the Ô¨Årst work that combines adaptive boosting and neural net works for relation extraction. We utilize adaptive boosting to tune the gradient descent in NN training. In this way, a large number of param eters in a single NN can be learned more robustly. The ensembled results on multiple NN models can achieve more accurate and robust relation extraction result. We evaluate the proposed model on a real data set. The results demonstrate the superior performance of the pro posed model which improves F1score by about 8% compared to stateoftheart models. 2 Related Work "
204,Tracking by Prediction: A Deep Generative Model for Mutli-Person localisation and Tracking.txt,"Current multi-person localisation and tracking systems have an over reliance
on the use of appearance models for target re-identification and almost no
approaches employ a complete deep learning solution for both objectives. We
present a novel, complete deep learning framework for multi-person localisation
and tracking. In this context we first introduce a light weight sequential
Generative Adversarial Network architecture for person localisation, which
overcomes issues related to occlusions and noisy detections, typically found in
a multi person environment. In the proposed tracking framework we build upon
recent advances in pedestrian trajectory prediction approaches and propose a
novel data association scheme based on predicted trajectories. This removes the
need for computationally expensive person re-identification systems based on
appearance features and generates human like trajectories with minimal
fragmentation. The proposed method is evaluated on multiple public benchmarks
including both static and dynamic cameras and is capable of generating
outstanding performance, especially among other recently proposed deep neural
network based approaches.","Multiperson localisation and tracking is one of the most active research areas in computer vision as it en ables a variety of applications including sports analysis [18,39,65], robot navigation [10,11] and autonomous driv ing [12, 15, 48]. Despite the impact of deep learning across a multitude of computer vision domains in recent years, within the track ing space it has been applied in a somewhat piecemeal man ner, with it often used for only a speciÔ¨Åc part of the tracking pipeline. For example, techniques such as [34, 58, 62] use DCNNs to model subject appearance within a probabilis tic tracker. We note that to date, complete deep learning solutions for both localisation and tracking have been limited [41]. We believe this is due to the scarcity of training data which is large enough to train a complete deep neu ral network based platform, as well as the complex, vari able length nature of multi person trajectories. We combine this with a deep tracking framework that utilises Long Short Term Memory networks (LSTMs) to capture pedestrian dy namics in the scene and track objects via predicting it‚Äôs fu ture trajectory. In this paper we contribute a novel light weight person detection framework based on Generative Adversarial Net works (GANs) [23], which can be easily trained on the lim ited data available for multi person localisation. We extend the general GAN framework to temporal sequences and ren der a probability map for pedestrians in the given sequence. The temporal structure of the proposed GAN allows us to identify pedestrians more effectively, regardless of the mo tion of other foreground objects in the scene. As illustrated in [42], multi person tracking consists of two subproblems: data association (i.e assigning a unique identiÔ¨Åer to the corresponding targets) and inferring the tra jectories of the targets. In most data association paradigms the researchers utilise an appearance based model [3, 9, 53, 60, 61] to reidentify the targets in the next frame. Yet in crowded environments with a high likelihood of occlu sions, noisy detections, and poor image resolution, appear ance models often fail to generate correct identiÔ¨Åcation of the targets. This results in an unrealistic trajectory gener ation from the tracking process. To counter this problem we propose a novel tracking framework where the object detections in the next frame are associated with targets via considering their predicted short term and long term trajec tories. The trajectory prediction method accounts for the motion of the pedestrian as well as the motion of other peo ple in the local neighbourhood which allows the modelling of the interactions among them. This enables intelligence in the data association process generating human like tra jectories even in the presence of occlusions and other image artefacts. To achieve this, we build upon recent advances [16, 41] in pedestrian trajectory modelling approaches andarXiv:1803.03347v1  [cs.CV]  9 Mar 2018propose a method to capture relationships with neighbour hood dynamics as well as the long term dependencies within the scene context. The major contributions of the proposed work can be summarised as follows: We introduce a novel pedestrian detection platform based on Generative Adversarial Networks (GANs). We develop a robust light weight algorithm for data association in multi person tracking problems with the aid of trajectory prediction. We generate human like trajectory estimates via the as sociation of neighbourhood and scene context in the trajectory prediction framework. We comprehensively evaluate the proposed models on publicly available benchmarks including videos from both static and dynamic cameras. We achieve outstanding performance in the MOT chal lenge benchmark datasets, especially among deep neu ral network based approaches. 2. Related work "
205,NRGNN: Learning a Label Noise-Resistant Graph Neural Network on Sparsely and Noisily Labeled Graphs.txt,"Graph Neural Networks (GNNs) have achieved promising results for
semi-supervised learning tasks on graphs such as node classification. Despite
the great success of GNNs, many real-world graphs are often sparsely and
noisily labeled, which could significantly degrade the performance of GNNs, as
the noisy information could propagate to unlabeled nodes via graph structure.
Thus, it is important to develop a label noise-resistant GNN for
semi-supervised node classification. Though extensive studies have been
conducted to learn neural networks with noisy labels, they mostly focus on
independent and identically distributed data and assume a large number of noisy
labels are available, which are not directly applicable for GNNs. Thus, we
investigate a novel problem of learning a robust GNN with noisy and limited
labels. To alleviate the negative effects of label noise, we propose to link
the unlabeled nodes with labeled nodes of high feature similarity to bring more
clean label information. Furthermore, accurate pseudo labels could be obtained
by this strategy to provide more supervision and further reduce the effects of
label noise. Our theoretical and empirical analysis verify the effectiveness of
these two strategies under mild conditions. Extensive experiments on real-world
datasets demonstrate the effectiveness of the proposed method in learning a
robust GNN with noisy and limited labels.","Graph structured data is very pervasive in realworld, such as social networks [ 9], financial transaction networks [ 37] and traffic net works [ 40]. Graph Neural Networks (GNNs) have shown great abil ity in modeling graph structured data and are attracting increasing attention [ 1,9,15,38]. Generally, GNNs adopt the messagepassing process to update node representations by aggregating the informa tion from their neighbors [ 15,36]. One of the most important and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ‚Äô21, August 14‚Äì18, 2021, Virtual Event, Singapore ¬©2021 Association for Computing Machinery. ACM ISBN 9781450383325/21/08. . . $15.00 https://doi.org/10.1145/3447548.3467364popular tasks that benefits from this messagepassing mechanism is node classification in a semisupervised manner. With this mech anism, labeled nodes can propagate their information to unlabeled nodes [9, 35], thus resulting in superior performance of GNNs. Despite the great performance of GNNs for semisupervised node classification, the majority of existing methods assume the training labels are clean; while for many realworld graphs and applications, the collected labels could be noisy and limited. For instance, for the geolocation prediction in social networks, only a small portion of users will fill in the geolocation; and the provided locations can be noisy because users randomly fill in wrong locations to protect their privacy or users have moved to new locations but forget to update them in social networks [ 21]. Similarly, for bot detection in social media, the labeling process can be tedious, costly, and errorprone, which can end up with limited noisily labeled nodes [17]. The graph with noisy and limited labels could significantly de grade the performance of GNNs for semisupervised node classi fication. First, recent work has shown that neural networks will overfit to the noisy labels and results in poor generalization perfor mance [ 31,42]. As a generalization of neural networks for graphs, GNNs are also likely to have poor performance trained on noisy labels. Second , for graphs, the noisy information can propagate through the network topology. Falsely labeled nodes will nega tively affect their unlabeled neighbors. Since the graph is sparsely labeled, neighbors of falsely labeled nodes are unlikely to accept the information from nodes with true labels to correct the repre sentations. In addition, many unlabeled nodes will only be able to aggregate information from unlabeled nodes when the labels are limited. Thus, the performance of GNNs trained on noisily and sparsely labeled graph would be poor. Though extensive approaches have been proposed for learning with noisy labels such as loss correction [ 7,31] and sample selec tion [ 10,13,19,23,41], they are not directly applicable for learning GNNs with limited noisy labels. First, generally, these methods as sume a large amount of noisy labels are available for learning noise distribution or for sampling correct labels. They are challenged by the small label size. Second , the majority of existing work for noisy labels [ 10,19,23,31] focus on independent and identically distributed (i.i.d) data such as images, which cannot handle the information propagation of noisy labels on graphs. The work on learning a robust GNN with noisy and limited labels is rather lim ited [ 8,43]. Therefore, it is important to develop a robust GNN that could deal with noisy and limited labels. Since the labeled nodes can propagate its information to the un labeled nodes, it is promising to correct the predictions of unlabeled nodes affected by falsely labeled nodes by linking them with nodes of clean labels. However, in practice, we do not know which labels are clean. Alternatively, for an unlabeled node ùë£ùëñ, we propose toarXiv:2106.04714v1  [cs.LG]  8 Jun 2021linkùë£ùëñwith labeled nodes of high feature similarity with ùë£ùëñto make it robust to label noise and facilitate the message passing of GNNs. The basic idea is if two nodes have high feature similarity, they are more likely to have the same label. Thus, if the probability that labeled nodes having correct labels is higher than that of having incorrect labels, by connecting ùë£ùëñwith more labeled nodes of high feature similarity with ùë£ùëñ, we can potentially bring more correct label information to ùë£ùëñ. Our theoretical and empirical analysis in Sec 3.4 verify the effectiveness of linking unlabeled nodes with noisily labeled nodes under mild conditions. In addition, with this strategy, we can first train a classifier to obtain accurate pseudo labels to ease the problem of learning with noisy and limited labels. By extending the label set with pseudo labels, more supervision could be utilized to make predictions for unlabeled nodes. Link ing unlabeled nodes with similar nodes of accurate pseudo labels could further reduce the issue of label noise, which is verified in Sec 3.5. Though promising, there are no existing work exploring these strategies for learning GNNs with noisy and limited labels. Therefore, in this paper, we investigate a novel problem of learn ingNoise Resistant GNNs on sparsely and noisily labeled graphs. In essence, we are faced with two challenges: (i) How to effectively link unlabeled nodes with labeled nodes to alleviate the effects of label noise and benefit the prediction? (ii) Given the graph with noisy and limited labels, how can we obtain accurate pseudo labels? To solve these challenges, we proposed a novel framework named noiseresistant GNN (NRGNN)1. NRGNN adopts a GNNbased edge predictor to predict edges to benefit the classification on graphs with noisy and limited labels. Since the existing edges in the graph generally link nodes in similar attributes [ 24], these edges could provide supervision to train a good edge predictor. The graph den sified by linking unlabeled nodes with similar noisily labeled nodes is utilized to obtain accurate pseudo labels, which extends the label set to provide more supervision for node classification. NRGNN also adopts the edge predictor to link unlabeled with similar ex tended labeled nodes to further reduce the effects of label noise. In summary, our main contributions are: ‚Ä¢We investigate a novel problem of learning noiseresistant GNNs on graphs with noisy and limited labels; ‚Ä¢We propose a new framework which can generate accurate pseudo labels and assign highquality edges between unlabeled nodes and (pseudo) labeled nodes to alleviate label noise issue; ‚Ä¢Theoretical and empirical analysis are conducted to verify the effectiveness of the proposed strategies against label noise; ‚Ä¢Extensive experiments on realworld datasets demonstrate the effectiveness of the proposed NRGNN in node classification on graphs with noisy and limited labels. 2 RELATED WORK "
206,Noisy Heuristics NAS: A Network Morphism based Neural Architecture Search using Heuristics.txt,"Network Morphism based Neural Architecture Search (NAS) is one of the most
efficient methods, however, knowing where and when to add new neurons or remove
dis-functional ones is generally left to black-box Reinforcement Learning
models. In this paper, we present a new Network Morphism based NAS called Noisy
Heuristics NAS which uses heuristics learned from manually developing neural
network models and inspired by biological neuronal dynamics. Firstly, we add
new neurons randomly and prune away some to select only the best fitting
neurons. Secondly, we control the number of layers in the network using the
relationship of hidden units to the number of input-output connections. Our
method can increase or decrease the capacity or non-linearity of models online
which is specified with a few meta-parameters by the user. Our method
generalizes both on toy datasets and on real-world data sets such as MNIST,
CIFAR-10, and CIFAR-100. The performance is comparable to the hand-engineered
architecture ResNet-18 with the similar parameters.","Neural Architecture Search (NAS) is the process of search ing the Architecture of Neural Networks by leveraging the computation rather than doing manually. However, NAS has still not been able to come to the mainstream due to large computational costs and availability of more efÔ¨Åcient alternatives such as transfer learning (Zhuang et al., 2020) or reusing architectures. Research has been done on using Re inforcement Learning(RL) (Zoph & Le, 2016; Baker et al., We would like to acknowledge Google Cloud for computing credits.1NAAMII, Nepal2University College London, UK. Corre spondence to: Suman Sapkota <suman.sapkota@naamii.org.np>, Binod Bhattarai <b.bhattarai@ucl.ac.uk>. DyNN workshop at the 39thInternational Conference on Machine Learning , Baltimore, Maryland, USA, 2022. Copyright 2022 by the author(s).2016) and Genetic Algorithm(GA) (Desell, 2017) for gener ating architecture from a given search space, however, these methods have huge computational costs and produce large carbon footprints (Strubell et al., 2019). Gradientbased path selection methods such as DARTS (Liu et al., 2018) and PCDARTS (Xu et al., 2019) have made NAS more efÔ¨Åcient and accessible. However, it still involves training large parameter models and selecting only a subset for the Ô¨Ånal model. In a manual architecture search process, we start with a baseline model architecture. If the model has poor perfor mance, we test more and more nonlinear models and if the model overÔ¨Åts the dataset we test smaller models, throwing away older models in the process. The initial concern is whether nonlinear capacity can be increased or decreased in the same model while reusing the trained function. To our aid, Network Morphing based methods (Elsken et al., 2017; Lu et al., 2018; Dai et al., 2019; Evci et al., 2022) have been used widely to add neurons, which increase the nonlinearity and capacity of the model. However, Network Morphism based methods are generally paired up with Reinforcement Learning (RL) (Cai et al., 2018) or Bayesian Optimization (Jin et al., 2019), which decide the morphism operation to increase the network ca pacity. This type of solution makes the dynamic nature of neural network a difÔ¨Åcult to understand. To understand the dynamics and to simulate heuristics, we need easily control lable models for changing network capacity or the number of neurons or parameters. Although there are various works on using Network Mor phism for Neural Architecture Search, we Ô¨Ånd that the meth ods are partial, either only adding neurons (Jin et al., 2019; Cai et al., 2018) and layers or not pruning layers (Gordon et al., 2018) to reduce capacity. Furthermore, those methods that add and prune neurons use it on incremental (Dai et al., 2020) or continual (Zhang et al., 2020) learning settings. Our goal to search for architecture depending on the dynam ics requires additional components to change the structure (layers and neurons) of the network itself. This gap mo tivates us to create a Network Morphism based NAS that can change the number of layers and neurons dynamically during the training phase while keeping the search efÔ¨ÅcientarXiv:2207.04467v1  [cs.LG]  10 Jul 2022Noisy Heuristics Neural Architecture Search and simple for the user. We combine multiple ideas and heuristics for creating a framework of Noisy NAS to search for capacity. Ideas from pruning and dropout support our framework for noisy heuristicbased architecture search. When small neurons are pruned, they typically recover the same accu racy and loss (Molchanov et al., 2019) without recovering the function completely. Furthermore, noisy regularization methods like Dropout (Srivastava et al., 2014) and Drop Connect (Wan et al., 2013) suggest that Deep Networks can be trained to be robust to perturbations. We can infer that Neural Networks are robust to the noisy process of addition and pruning of neurons. We can use such a noisy process to try different additions and removals of neurons iteratively which can roughly change the architecture to the desired capacity. The method of adding many neurons and remov ing poorly performing ones could be used to search for the correct place to add new neurons. Furthermore, the dynamics of Biological Neural Networks (BNN) (Wan et al., 2019) suggests that there could exist Neural Networks with dynamically changing architecture in a single model. The dynamic nature of BNN is partly due to neurogenesis (Kumar et al., 2019), neuron and synaptic pruning (Fricker et al., 2018) and neuron migration. We aim to understand the internal workings of ArtiÔ¨Åcial Neural Networks(ANN) and apply dynamics from BNN to close the gap between them. We believe that Dynamic Neural Networks along with Spiking Neural Networks (Tavanaei et al., 2019) could model BNN even better. Our Contribution: Combining the growing and shrinking mechanisms, we are able to get any desired network capacity for the best Ô¨Åtting of the dataset. Such a method is depicted by a generalization curve as shown in Figure 1. We work on the same curve, but instead of trying different capacity mod els, we change the capacity of the existing models towards the best capacity. To this end, we propose a new method for Network Morphism based Neural Architecture search using heuristics. We simplify our search space using multiple heuristics to a manageable number of metaparameters. The major contributions of our work are listed below. 1.We introduce a new method to add new neurons and layers heuristically for Network Morphism based Neu ral Architecture Search. 2.We create a new type of architecture called Hierarchi cal Residual Network for the ease of changing non linearity and number of layers during Network Mor phism. 3.We combine neuron addition, pruning and Hierarchi cal Residual Network to change the capacity of the network noisily during training, which we call Noisy Heuristics NAS. 4.We show that our method is successful in getting perfor Capacity Error/Loss  Test Error  Train Error Decrease Capacity  ( Overfitting )Increase Capacity  ( Underfitting )  Optimal  Capacity Figure 1. Generalization Curve mance near handdesigned architectures like ResNet. 5.We release code for Network Morphism, Opti mizer reusing, Pruning and Noisy Heuristic NAS in the PyTorch (Paszke et al., 2019) framework. https://github.com/tsumansapkota/NoisyHeuristicsNAS 2. Methodology "
207,General audio tagging with ensembling convolutional neural network and statistical features.txt,"Audio tagging aims to infer descriptive labels from audio clips. Audio
tagging is challenging due to the limited size of data and noisy labels. In
this paper, we describe our solution for the DCASE 2018 Task 2 general audio
tagging challenge. The contributions of our solution include: We investigated a
variety of convolutional neural network architectures to solve the audio
tagging task. Statistical features are applied to capture statistical patterns
of audio features to improve the classification performance. Ensemble learning
is applied to ensemble the outputs from the deep classifiers to utilize
complementary information. a sample re-weight strategy is employed for ensemble
training to address the noisy label problem. Our system achieves a mean average
precision (mAP@3) of 0.958, outperforming the baseline system of 0.704. Our
system ranked the 1st and 4th out of 558 submissions in the public and private
leaderboard of DCASE 2018 Task 2 challenge. Our codes are available at
https://github.com/Cocoxili/DCASE2018Task2/.","Audio tagging task is a task to predict the presence or ab sence of certain acoustic events in an audio recording, and it has drawn lots of attention during the last several years. Audio tagging has widely applications, such as surveillance, monitoring, and health care [1]. Historically, audio tagging has been addressed with different handcrafted features and shallowarchitecture classiÔ¨Åers including Gaussian mixture models (GMMs) [2] and nonnegative matrix factorizations (NMFs) [3]. Recently, deep learning approaches such as convolutional neural networks (CNNs) have achieved state oftheart performance for the audio tagging task [4, 5]. Corresponding author.DCASE 2018 Task 2 launched a competition for the gen eral audio tagging task [1] to attract research interests for the audio tagging problem. However, due to the limited size of data and noisy labels [1], general audio tagging remains as a challenge and falls short of accuracy and robustness. The cur rent general audio tagging systems are confronted with sev eral challenges: (1) There are a large amount of event classes in [1] compared with previous audio classiÔ¨Åcation tasks [3, 6]. (2) The imbalance problem could make the model emphasize more on the classes with more training samples and difÔ¨Åcult to learn from the classes with less samples. (3) The data qual ity varies from class to class. For example, some audio clips are manually veriÔ¨Åed in [1] but others are not. Designing su pervised deep learning algorithms that can learn from data sets with noisy labels is an important problem, especially, when the data set is small. In this paper, we aim to build scalable ensemble ap proach with taking the noisy label into account. The pro posed method achieves a stateoftheart performance on the DCASE 2018 Task 2 dataset. The contributions of the pa per are summarized as below: (1) A quantitative comparison is investigated using different convolutional neural network (CNN) architectures inspired from computer vision. These CNN architectures are further deployed for the ensemble learning. (2) We propose to employ statistical features in cluding the skewness and kurtosis of framewise MFCC to improve the performance. (3) A scalable ensemble approach is used to utilize the complementary information of different deep architectures and handcrafted features. (4) A samples reweight strategy is proposed for the ensemble learning to solve the noisy label problem in the dataset. The paper is organized as follows: Section 2 describes the proposed CNNs, statistical features, ensemble learning and sample reweight methods. Section 3 shows experimental re sults. Section 4 concludes and forecasts future work.arXiv:1810.12832v1  [cs.CV]  30 Oct 20182. METHODOLOGY "
208,Uncertainty-Aware Label Refinement for Sequence Labeling.txt,"Conditional random fields (CRF) for label decoding has become ubiquitous in
sequence labeling tasks. However, the local label dependencies and inefficient
Viterbi decoding have always been a problem to be solved. In this work, we
introduce a novel two-stage label decoding framework to model long-term label
dependencies, while being much more computationally efficient. A base model
first predicts draft labels, and then a novel two-stream self-attention model
makes refinements on these draft predictions based on long-range label
dependencies, which can achieve parallel decoding for a faster prediction. In
addition, in order to mitigate the side effects of incorrect draft labels,
Bayesian neural networks are used to indicate the labels with a high
probability of being wrong, which can greatly assist in preventing error
propagation. The experimental results on three sequence labeling benchmarks
demonstrated that the proposed method not only outperformed the CRF-based
methods but also greatly accelerated the inference process.","Linguistic sequence labeling is one of the funda mental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including partofspeech (POS) tagging, text chunking, and named entity recognition (NER). BeneÔ¨Åting from representation learning, neural networkbased approaches can achieve stateof theart performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. United            Arab            EmiratesBLOC          ILOC          ELOCInputTrue LabelDraft LabelRefinement......BORG          ILOC          EORGBLOC          IORG          ELOCX<latexit sha1_base64=""ex7UVx6LxzCcCSDqKZWFuZK2gBU="">AAACzXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoxp0V7APbIsl02obmxWQilKpbf8Ct/pb4B/oX3hlTUIvohCRnzr3nzNx73dj3EmlZrzljbn5hcSm/XFhZXVvfKG5uNZIoFYzXWeRHouU6Cfe9kNelJ33eigV3AtfnTXd0quLNGy4SLwov5Tjm3cAZhF7fY44k6qrDhpyNAkeMroslq2zpZc4COwMlZKsWFV/QQQ8RGFIE4AghCftwkNDThg0LMXFdTIgThDwd57hDgbQpZXHKcIgd0XdAu3bGhrRXnolWMzrFp1eQ0sQeaSLKE4TVaaaOp9pZsb95T7SnutuY/m7mFRArMST2L9008786VYtEH8e6Bo9qijWjqmOZS6q7om5ufqlKkkNMnMI9igvCTCunfTa1JtG1q946Ov6mMxWr9izLTfGubkkDtn+OcxY0KmX7oFy5OCxVT7JR57GDXezTPI9QxRlqqJN3iEc84dk4N1Lj1rj/TDVymWYb35bx8AFBiJNo</latexit>‚á•<latexit sha1_base64=""PcNBZ+k3dnNGojhYt75BSmjl47k="">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoRnBTwT6gLZJMp3VsXiYTsRZX/oBb/THxD/QvvDOmoBbRCUnOnHvPmbn3upEnEmlZrzljZnZufiG/WFhaXlldK65vNJIwjRmvs9AL45brJNwTAa9LIT3eimLu+K7Hm+7wWMWbNzxORBicy1HEu74zCERfMEcS1ehI4fPkoliyypZe5jSwM1BCtmph8QUd9BCCIYUPjgCSsAcHCT1t2LAQEdfFmLiYkNBxjnsUSJtSFqcMh9ghfQe0a2dsQHvlmWg1o1M8emNSmtghTUh5MWF1mqnjqXZW7G/eY+2p7jaiv5t5+cRKXBL7l26S+V+dqkWij0Ndg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWTPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ0albK9V66c7ZeqR9mo89jCNnZpngeo4gQ11Mn7Co94wrNxalwbt8bdZ6qRyzSb+LaMhw/935G5</latexit>‚á•<latexit sha1_base64=""PcNBZ+k3dnNGojhYt75BSmjl47k="">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoRnBTwT6gLZJMp3VsXiYTsRZX/oBb/THxD/QvvDOmoBbRCUnOnHvPmbn3upEnEmlZrzljZnZufiG/WFhaXlldK65vNJIwjRmvs9AL45brJNwTAa9LIT3eimLu+K7Hm+7wWMWbNzxORBicy1HEu74zCERfMEcS1ehI4fPkoliyypZe5jSwM1BCtmph8QUd9BCCIYUPjgCSsAcHCT1t2LAQEdfFmLiYkNBxjnsUSJtSFqcMh9ghfQe0a2dsQHvlmWg1o1M8emNSmtghTUh5MWF1mqnjqXZW7G/eY+2p7jaiv5t5+cRKXBL7l26S+V+dqkWij0Ndg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWTPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ0albK9V66c7ZeqR9mo89jCNnZpngeo4gQ11Mn7Co94wrNxalwbt8bdZ6qRyzSb+LaMhw/935G5</latexit>‚á•<latexit sha1_base64=""PcNBZ+k3dnNGojhYt75BSmjl47k="">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoRnBTwT6gLZJMp3VsXiYTsRZX/oBb/THxD/QvvDOmoBbRCUnOnHvPmbn3upEnEmlZrzljZnZufiG/WFhaXlldK65vNJIwjRmvs9AL45brJNwTAa9LIT3eimLu+K7Hm+7wWMWbNzxORBicy1HEu74zCERfMEcS1ehI4fPkoliyypZe5jSwM1BCtmph8QUd9BCCIYUPjgCSsAcHCT1t2LAQEdfFmLiYkNBxjnsUSJtSFqcMh9ghfQe0a2dsQHvlmWg1o1M8emNSmtghTUh5MWF1mqnjqXZW7G/eY+2p7jaiv5t5+cRKXBL7l26S+V+dqkWij0Ndg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWTPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ0albK9V66c7ZeqR9mo89jCNnZpngeo4gQ11Mn7Co94wrNxalwbt8bdZ6qRyzSb+LaMhw/935G5</latexit>Figure 1: Schematic of label reÔ¨Ånement framework (Cui and Zhang, 2019). The goal is reÔ¨Åning the label of ‚ÄúArab‚Äù using contextual labels and words, while the reÔ¨Ånement of other correct labels may be negatively impacted by incorrect draft labels. creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely on inefÔ¨Åcient Viterbi decoding. Many of the recent methods try to introduce label embeddings to manage longer ranges of dependencies, such as twostage label reÔ¨Ånement (Krishnan and Manning, 2006; Cui and Zhang, 2019) and seq2seq (Vaswani et al., 2016; Zhang et al., 2018) frameworks. In particular, Cui and Zhang (2019) introduced a hierarchicallyreÔ¨Åned representation of marginal label distributions, which predicts a sequence of draft labels in advance and then uses the wordlabel interactions to reÔ¨Åne them. Although these methods can model longer label dependencies, they are vulnerable to error propagation: if a label is mistakenly predicted during inference, the error will be propagated and the other labels conditioned on this one will be impacted (Bengio et al., 2015). As shown in Figure 1, the label attention network (LAN) (Cui andarXiv:2012.10608v1  [cs.CL]  19 Dec 2020Draft Uncertainty ReÔ¨Ånement #Tokens 4 0.018 4 √ô 8 39 8 0.524 8 √ô 4 54 Table 1: Results of LAN with uncertainty estimation evaluated on CoNLL2003 test dataset. 4refers to the correct prediction, and 8refers to the wrong prediction. We use Bayesian neural networks (Kendall and Gal, 2017) to estimate the uncertainty. We can see that the uncertainty value of incorrect prediction is 29 times larger than that of correct predictions, which can effectively indicate the incorrect predictions. Zhang, 2019) would negatively impact the correct predictions in the reÔ¨Ånement stage. There are 39 correct tokens that have been incorrectly modiÔ¨Åed (Table 1). Hence, the model should selectively correct the labels with high probabilities of being incorrect, not all of them. Fortunately, we Ô¨Ånd that uncertainty values estimated by Bayesian neural networks (Kendall and Gal, 2017) can effectively indicate the labels that have a high probability of being incorrect. As shown in Table 11, the average uncertainty value of incorrect prediction is 29 times larger than that of correct predictions for the draft labels. Hence, we can easily set an uncertainty threshold to only reÔ¨Åne the potentially incorrect labels and prevent side effects on the correct labels. In this work, we propose a novel twostage UncertaintyAware label reÔ¨Ånement Network (UANet). At the Ô¨Årst stage, the Bayesian neural networks take a sentence as input and yield all of the draft labels together with corresponding uncertainties. At the second stage, a twostream selfattention model performs attention over label embeddings to explicitly model the label dependencies, as well as context vectors to model the context representations. All of these features are fused to reÔ¨Åne the potentially incorrect draft labels. The above label reÔ¨Ånement operations can be processed in parallel, which can avoid the use of Viterbi decoding of the CRF for a faster prediction. Experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRFbased methods but also signiÔ¨Åcantly accelerated the inference process. The main contributions of this paper can be summarized as follows: 1) we propose the use of Bayesian neural networks to estimate 1We slightly modiÔ¨Åed the code using Bayesian neural networks.the uncertainty of predictions and indicate the potentially incorrect labels that should be reÔ¨Åned; 2) we propose a novel twostream selfattention reÔ¨Åning framework to better model different ranges of label dependencies and wordlabel interactions; 3) the proposed parallel decoding process can greatly speed up the inference process; and 4) the experimental results across three sequence labeling datasets indicate that the proposed method outperforms the other label decoding methods. 2 Related Work and Background "
209,Data-Efficient and Interpretable Tabular Anomaly Detection.txt,"Anomaly detection (AD) plays an important role in numerous applications. We
focus on two understudied aspects of AD that are critical for integration into
real-world applications. First, most AD methods cannot incorporate labeled data
that are often available in practice in small quantities and can be crucial to
achieve high AD accuracy. Second, most AD methods are not interpretable, a
bottleneck that prevents stakeholders from understanding the reason behind the
anomalies. In this paper, we propose a novel AD framework that adapts a
white-box model class, Generalized Additive Models, to detect anomalies using a
partial identification objective which naturally handles noisy or heterogeneous
features. In addition, the proposed framework, DIAD, can incorporate a small
amount of labeled data to further boost anomaly detection performances in
semi-supervised settings. We demonstrate the superiority of our framework
compared to previous work in both unsupervised and semi-supervised settings
using diverse tabular datasets. For example, under 5 labeled anomalies DIAD
improves from 86.2\% to 89.4\% AUC by learning AD from unlabeled data. We also
present insightful interpretations that explain why DIAD deems certain samples
as anomalies.","Anomaly detection (AD) has numerous realworld applications, especially for tabular data, including detection of fraudulent trans actions, intrusions related to cybersecurity, and adverse outcomes Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA. ¬©2023 Copyright held by the owner/author(s). ACM ISBN 9798400701030/23/08. https://doi.org/10.1145/3580305.3599294in healthcare. When the realworld tabular AD applications are considered, there are various challenges constituting a fundamen tal bottleneck for penetration of fullyautomated machine learning solutions: ‚Ä¢Noisy and irrelevant features : Tabular data often contain noisy or irrelevant features caused by measurement noise, outlier fea tures and inconsistent units. Even a change in a small subset of features may trigger anomaly identification. ‚Ä¢Heterogeneous features : Unlike image or text, tabular data fea tures can have values with significantly different types (numeri cal, boolean, categorical, and ordinal), ranges and distributions. ‚Ä¢Small labeled data : In many applications, often a small portion of the labeled data is available. AD accuracy can be significantly boosted with the information from these labeled samples as they may contain crucial information on representative anomalies and help ignore irrelevant ones. ‚Ä¢Interpretability : Without interpretable outputs, humans cannot understand the rationale behind anomaly predictions, that would enable more trust and actions to improve the model performance. Verification of model accuracy is particularly challenging for high dimensional tabular data, as they are not easy to visualize for humans. An interpretable AD model should be able to identify important features used to predict anomalies. Conventional local explainability methods like SHAP [ 20] and LIME [ 25] are pro posed for supervised learning and may not be straightforward to generalize to unsupervised or semisupervised AD. Conventional AD methods fail to address the above ‚Äì their per formance often deteriorates with noisy features (Sec. 6), they cannot incorporate labeled data, and cannot provide interpretability. In this paper, we aim to address these challenges by propos ing a novel framework, Dataefficient Interpretable AD(DIAD ). DIAD‚Äôs model architecture is inspired by Generalized Additive Models (GAMs) and GA2M (see Sec. 3), that have been shown to obtain high accuracy and interpretability for tabular data [ 4,6,16], and have been used in applications like finding outlier patterns and auditing fairness [ 33]. We propose to employ intuitive notions of Partial Identification (PID) as an AD objective and learn them with a differentiable GA2M (NodeGA2M, Chang et al . [5]). Our design is based on the principle that PID scales to highdimensional features and handles heterogeneous features well, while the dif ferentiable GAM allows finetuning with labeled data and retain interpretability. In addition, PID requires clearcut thresholds like trees which are provided by NodeGA2M. While combining PIDarXiv:2203.02034v2  [cs.LG]  4 Jun 2023KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA. Chang, et al. Figure 1: Overview of the proposed DIAD framework. During training, first an unsupervised AD model is fitted employing interpretable GA2M models and PID loss with unlabeled data. Then, the trained unsupervised model is finedtuned with a small amount of labeled data using a differentiable AUC loss. At inference, both the anomaly score and explanations are provided, based on the visualizations of top contributing features. The example sample in the figure is shown to have an anomaly score, explained by the cell size feature having high value. with NodeGA2M, we introduce multiple methodological innova tions, including estimating and normalizing a sparsity metric as the anomaly scores, integrating a regularization for an inductive bias appropriate for AD, and using deep representation learning via finetuning with a differentiable AUC loss. The latter is crucial to take advantage of a small amount of labeled samples well and constitutes a more ‚Äòdataefficient‚Äô method compared to other AD approaches ‚Äì e.g. DIAD improves from 87.1% to 89.4% AUC with 5 labeled anomalies compared to unsupervised AD. Overall, our inno vations lead to strong empirical results ‚Äì DIAD outperforms other alternatives significantly, both in unsupervised and semisupervised settings. DIAD‚Äôs outperformance is especially prominent on large scale datasets containing heterogeneous features with complex relationships between them. In addition to accuracy gains, DIAD also provides a rationale on why an example is classified as anoma lous using the GA2M graphs, and insights on the impact of labeled data on the decision boundary, a novel explainability capability that provides both local and global understanding on the AD tasks. 2 RELATED WORK "
210,FINE Samples for Learning with Noisy Labels.txt,"Modern deep neural networks (DNNs) become frail when the datasets contain
noisy (incorrect) class labels. Robust techniques in the presence of noisy
labels can be categorized into two folds: developing noise-robust functions or
using noise-cleansing methods by detecting the noisy data. Recently,
noise-cleansing methods have been considered as the most competitive
noisy-label learning algorithms. Despite their success, their noisy label
detectors are often based on heuristics more than a theory, requiring a robust
classifier to predict the noisy data with loss values. In this paper, we
propose a novel detector for filtering label noise. Unlike most existing
methods, we focus on each data's latent representation dynamics and measure the
alignment between the latent distribution and each representation using the
eigendecomposition of the data gram matrix. Our framework, coined as filtering
noisy instances via their eigenvectors (FINE), provides a robust detector with
derivative-free simple methods having theoretical guarantees. Under our
framework, we propose three applications of the FINE: sample-selection
approach, semi-supervised learning approach, and collaboration with
noise-robust loss functions. Experimental results show that the proposed
methods consistently outperform corresponding baselines for all three
applications on various benchmark datasets.","Deep neural networks (DNNs) have achieved remarkable success in numerous tasks as the amount of accessible data has dramatically increased [21, 15]. On the other hand, accumulated datasets are typically labeled by a human, a laborintensive job or through web crawling [48] so that they may be easily corrupted ( label noise ) in realworld situations. Recent studies have shown that deep neu ral networks have the capacity to memorize essentially any labeling of the data [49]. Even a small amount of such noisy data can hinder the generalization of DNNs owing to their strong memorization of noisy labels [49, 29]. Hence, it becomes crucial to train DNNs that are robust to corrupted labels. As label noise problems may appear anywhere, such robustness increases reliability in many appli cations such as the ecommerce market [9], medical Ô¨Åelds [45], ondevice AI [46], and autonomous driving systems [11]. To improve the robustness against noisy data, the methods for learning with noisy labels (LNL) have been evolving in two main directions [18]: (1) designing noiserobust objective functions or regular Equal contribution 1Code available at https://github.com/Kthyeon/FINE_official 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.arXiv:2102.11628v3  [cs.LG]  27 Oct 2021FEATURE EXTRACTOR  FEATURE VECTOR CLEAN NOISYLABEL : DOG DETECTION STRATEGY CLEAN NOISYROBUST TRAINOur Contribution : Noise Cleansing  LINEAR CLASSIFIER(a) NoiseCleansingbased Approach CLEAN NOISEùíñùë™ùë≥ùë¨ùë®ùëµ EIGEN DECOMPOSITION GAUSSIAN MIXTURE MODELFIT  (b) FINE Figure 1: Illustration of (a) basic concept of this work and (b) proposed detection framework, FINE. Noisecleansing learning generally separates clean data from the original dataset by using prediction outputs. We propose a novel derivativefree detector based on an unsupervised clustering algorithm on the highorder topological space. FINE measures the alignment of prelogits (i.e., penultimate layer representation vectors) toward the classrepresentative vector that is extracted through the eigen decomposition of the gram matrix of data representations. izations and (2) detecting and cleansing the noisy data. In general, the former noiserobust direction uses explicit regularization techniques [6, 52, 50] or robust loss functions [38, 13, 40, 51], but their performance is far from stateoftheart [49, 26] on datasets with severe noise rates. Recently, re searchers have designed noisecleansing algorithms focused on segregating the clean data (i.e., sam ples with uncorrupted labels) from the corrupted data [19, 14, 47, 18, 32, 42]. One of the popular criteria for the segregation process is the loss value between the prediction of the noisy classiÔ¨Åer and its noisy label, where it is generally assumed that the noisy data have a large loss [19, 14, 47, 18] or the magnitude of the gradient during training [51, 40]. However, these methods may still be bi ased by the corrupted linear classiÔ¨Åer towards label noise because their criterion (e.g., loss values or weight gradient) uses the posterior information of such a linear classiÔ¨Åer [24]. Maennel et al. [31] analytically showed that the principal components of the weights of a neural network align with the randomly labeled data; this phenomenon can yield more negative effects on the classiÔ¨Åer as the number of randomly labeled classes increases. Recently, Wu et al. [42] used an inherent geo metric structure induced by nearest neighbors (NN) in latent space and Ô¨Åltered out isolated data in such topology, and its quality was sensitive to its hyperparameters regarding NN clustering in the presence of severe noise rates. To mitigate such issues for label noise detectors, we provide a novel yet simple detector frame work, Ô¨Åltering n oisy labels via their e igenvectors (FINE) with theoretical guarantees to provide a highquality splitting of clean and corrupted examples (without the need to estimate noise rates). Instead of using the neural network‚Äôs linear classiÔ¨Åer, FINE utilizes the principal components of latent representations made by eigen decomposition which is one of the most widely used unsuper vised learning algorithms and separates clean data and noisy data by these components (Figure 1a). To motivate our approach, as Figure 1b shows, we Ô¨Ånd that the clean data (blue points) are mainly aligned on the principal component (black dotted line), whereas the noisy data (orange points) are not; thus, the dataset is well clustered with the alignment of representations toward the principal component by Ô¨Åtting them into Gaussian mixture models (GMM). We apply our framework to var ious LNL methods: the sample selection approach, a semisupervised learning (SSL) approach, and collaboration with noiserobust loss functions. The key contributions of this work are summarized as follows: ‚Ä¢ We propose a novel framework, termed FINE ( Ô¨Åltering n oisy labels via their e igenvectors ), for detecting clean instances from noisy datasets. FINE makes robust decision boundary for the highorder topological information of data in latent space by using eigen decomposition of their gram matrix. ‚Ä¢ We provide provable evidence that FINE allows a meaningful decision boundary made by eigenvectors in latent space. We support our theoretical analysis with various experimental results regarding the characteristics of the principal components extracted by our FINE detector. ‚Ä¢ We develop a simple sampleselection method by replacing the existing detector method with FINE. We empirically validate that a sampleselection learning with FINE provides consistently superior detection quality and higher test accuracy than other existing alterna tive methods such as the Coteaching family [14, 47], TopoFilter [42], and CRUST [32]. 2‚Ä¢ We experimentally show that our detection framework can be applied in various ways to existing LNL methods and validate that ours consistently improves the generalization in the presence of noisy data: sampleselection approach [14, 47], SSL approach [25], and collaboration with noiserobust loss functions [51, 40, 29]. Organization. The remainder of this paper is organized as follows. In Section 2, we discuss the recent literature on LNL solutions and meaningful detectors. In Section 3, we address our motivation for creating a noisy label detector with theoretical insights and provide our main method, Ô¨Åltering the noisy labels via their eigenvectors (FINE). In Section 4, we present the experimental results. Finally, Section 5 concludes the paper. 2 Related Works "
211,Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification.txt,"Many unsupervised domain adaptive (UDA) person re-identification (ReID)
approaches combine clustering-based pseudo-label prediction with feature
fine-tuning. However, because of domain gap, the pseudo-labels are not always
reliable and there are noisy/incorrect labels. This would mislead the feature
representation learning and deteriorate the performance. In this paper, we
propose to estimate and exploit the credibility of the assigned pseudo-label of
each sample to alleviate the influence of noisy labels, by suppressing the
contribution of noisy samples. We build our baseline framework using the mean
teacher method together with an additional contrastive loss. We have observed
that a sample with a wrong pseudo-label through clustering in general has a
weaker consistency between the output of the mean teacher model and the student
model. Based on this finding, we propose to exploit the uncertainty (measured
by consistency levels) to evaluate the reliability of the pseudo-label of a
sample and incorporate the uncertainty to re-weight its contribution within
various ReID losses, including the identity (ID) classification loss per
sample, the triplet loss, and the contrastive loss. Our uncertainty-guided
optimization brings significant improvement and achieves the state-of-the-art
performance on benchmark datasets.","Person reidentiÔ¨Åcation (ReID) is an important task that matches person images across times/spaces/cameras, which has many applications such as people tracking in smart re tail, image retrieval for Ô¨Ånding lost children. Existing ap proaches achieve remarkable performance when the train ing and testing data are from the same dataset/domain. But they usually fail to generalize well to other datasets where there are domain gaps (Ge, Chen, and Li 2020). To ad dress this practical problem, unsupervised domain adaptive (UDA) person ReID attracts much attention for both the academic and industrial communities, where labeled source domain and unlabeled target domain data are exploited for training. *This work was done when Kecheng Zheng was an intern at MSRA. ‚Ä†Corresponding Author Copyright ¬© 2021, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved. Figure 1: Observations on the relations between the correct ness of pseudo labels and the uncertainty (which we measure by the inconsistency level of the output features of two mod els,i.e., the student model and the teacher model based on the mean teacher method (Tarvainen and Valpola 2017) for the target domain samples (obtained from Duke !Market). We found the uncertainty for samples with wrong/noisy pseudo labels (red curve) is usually larger than those (green curve) with correct/clean pseudo labels. Typical UDA person ReID approaches (Ge, Chen, and Li 2020; Zhai et al. 2020a; Zhong et al. 2019; Zheng et al. 2020; Song et al. 2020) include three steps: feature pretraining with labeled source domain data, clusteringbased pseudo label prediction for the target domain data, and feature rep resentation learning/Ô¨Ånetuning with the pseudolabels. The last two steps are usually iteratively conducted to promote each other. However, the pseudolabels obtained/assigned through clustering usually contain noisy (wrong) labels due to the divergence/domain gap between the source and target data, and the imperfect results of the clustering algorithm. Such noisy labels would mislead the feature learning and harm the domain adaptation performance. Thus, alleviating the negative effects of those samples with unreliable/noisy pseudo labels is important for the success of domain adap tation .arXiv:2012.08733v2  [cs.CV]  17 Dec 2020The challenge lies in 1) how to identify samples that are prone to have noisy pseudo labels; 2) how to alleviate their negative effects during the optimization. In this paper, to an swer the Ô¨Årst question, we have observed abundant samples and analyzed the relationship between the characteristics of the samples and the correctness of pseudo labels. Based on the theory on uncertainty (Kendall and Gal 2017), a model has uncertainty on its prediction of an input sample. Here, we measure the inconsistency level of the output features of two models (the student model and the teacher model based on the mean teacher method (Tarvainen and Valpola 2017)) and take it as the estimated uncertainty of a target domain sample. As shown in Fig. 1, we observe the distribution of the uncertainty (inconsistency levels) for correct/clean pseudo labels and wrong pseudo labels. We found that the uncertainty values for the samples with wrong pseudo labels are usually larger than those with correct pseudo labels. This motivates us to estimate and exploit the uncertainty of sam ples to alleviate the negative effects of noisy pseudo labels, enabling effective domain adaptation. We answer the second question by carefully incorporating the uncertainty of sam ples into classiÔ¨Åcation loss, triplet loss, and contrastive loss, respectively. We summarize our main contributions as follows: ‚Ä¢ We propose a network named Uncertaintyguided Noise Resilient Network (UNRN) to explore the credibility of the predicted pseudo labels of target domain samples for effective domain adaptive person ReID. ‚Ä¢ We develop an uncertainty estimation strategy by calcu lating the inconsistency of two models in terms of their predicted soft multilabels. ‚Ä¢ We incorporate the uncertainty of samples to the ID clas siÔ¨Åcation loss, triplet loss, and contrastive loss through reweighting to alleviate the negative inÔ¨Çuence of noisy pseudo labels. Extensive experiments demonstrate the effectiveness of our framework and the designed components on unsu pervised person ReID benchmark datasets. Our scheme achieves the stateoftheart performance on all the bench mark datasets. 2 Related Work "
212,MarginDistillation: distillation for margin-based softmax.txt,"The usage of convolutional neural networks (CNNs) in conjunction with a
margin-based softmax approach demonstrates a state-of-the-art performance for
the face recognition problem. Recently, lightweight neural network models
trained with the margin-based softmax have been introduced for the face
identification task for edge devices. In this paper, we propose a novel
distillation method for lightweight neural network architectures that
outperforms other known methods for the face recognition task on LFW, AgeDB-30
and Megaface datasets. The idea of the proposed method is to use class centers
from the teacher network for the student network. Then the student network is
trained to get the same angles between the class centers and the face
embeddings, predicted by the teacher network.","The development of edge devices has sparked signicant interest in lightweight face recognition access systems. This type of solution is based on optimized neu ral network architectures for mobile devices. A typical example of such network is MobileFaceNet [2], designed specically for the face recognition on devices with low computing power. The usage of marginbase softmax approach [3,14,21] in the training procedure helps to obtain the stateoftheart performance for the face recognition tasks. Despite fast and compact mobile network architectures give lower face recog nition accuracy than the fullsize ones, in some applications, such as biometric access systems, it nevertheless plays a critical role. Distillation is a method that helps to achieve the highest accuracy for mobile neural network architectures, where the knowledge is transferred from a heavy teacher network to a small student network. In this article we propose a novel distillation method called MarginDistillation to reduce the gap between teacher and student networks dur ing distillation process. The idea of the proposed method is to copy class centers from a teacher networkarXiv:2003.02586v1  [cs.CV]  5 Mar 20202 Svitov David and Alyamkin Sergey to a student network and freeze class centers for the whole distillation proce dure, where the student network is trained to get angles between given class centers and face embeddings the same as in the teacher network. It allows the student network to better reproduce results of the teacher network trained with the marginbased loss function. The main contributions of our work: {We have proposed a novel method for the distillation of neural networks trained with a marginbased softmax. {The proposed method allows a gap reduction between teacher and student networks for face recognition problem. The accuracy of the mobile face recog nition neural network achieved with our method exceeds other known distil lation methods on dierent datasets: LFW [11], AgeDB30 [15] and Mage Face [13] dataset. {In presented work we made direct comparison of dierent distillation meth ods. Code for implemented methods and comparison experiments is available on the github. 2 Related Works "
213,Weakly Supervised Vessel Segmentation in X-ray Angiograms by Self-Paced Learning from Noisy Labels with Suggestive Annotation.txt,"The segmentation of coronary arteries in X-ray angiograms by convolutional
neural networks (CNNs) is promising yet limited by the requirement of precisely
annotating all pixels in a large number of training images, which is extremely
labor-intensive especially for complex coronary trees. To alleviate the burden
on the annotator, we propose a novel weakly supervised training framework that
learns from noisy pseudo labels generated from automatic vessel enhancement,
rather than accurate labels obtained by fully manual annotation. A typical
self-paced learning scheme is used to make the training process robust against
label noise while challenged by the systematic biases in pseudo labels, thus
leading to the decreased performance of CNNs at test time. To solve this
problem, we propose an annotation-refining self-paced learning framework
(AR-SPL) to correct the potential errors using suggestive annotation. An
elaborate model-vesselness uncertainty estimation is also proposed to enable
the minimal annotation cost for suggestive annotation, based on not only the
CNNs in training but also the geometric features of coronary arteries derived
directly from raw data. Experiments show that our proposed framework achieves
1) comparable accuracy to fully supervised learning, which also significantly
outperforms other weakly supervised learning frameworks; 2) largely reduced
annotation cost, i.e., 75.18% of annotation time is saved, and only 3.46% of
image regions are required to be annotated; and 3) an efficient intervention
process, leading to superior performance with even fewer manual interactions.","Coronary artery disease (CAD) is one of the leading causes of death globally [1]. It is primarily caused by ob structive atherosclerotic plaque [2], which narrows the in ner wall of coronary artery and decreases normal myocar dial perfusion, leading to symptoms such as angina and even myocardial infarction [3]. Percutaneous coronary in tervention (PCI) is a minimally invasive surgery to eec tively treat CAD in clinical practice. In such a procedure, a cardiologist delivers a catheter with a premounted stent through coronary arteries to the stenosis lesion. Once the lesion is reached, the stent is deployed against the nar row coronary wall by in ating the delivery balloon. Since target vessels are not directly visible, PCI is performed under image guidance by using Xray angiography to vi sualize coronary arteries for the injection of radiopaque contrast agent. The accurate segmentation of vessels in Corresponding author Email addresses: xiehongzhi@medmail.com.cn (Hongzhi Xie), gulixu@sjtu.edu.cn (Lixu Gu)Xray angiograms (XAs) enables the quantitative analy sis of coronary trees [4] and is fundamental for the safe navigation of intervention devices for PCI surgery. Deep learning with convolutional neural networks (CNNs) has achieved the stateoftheart performance for medical image segmentation [5, 6, 7], including vessel seg mentation in XA [8, 9]. Following the fully supervised learning framework, its success relies heavily on a large amount of precise annotations for all pixels in training im ages to improve generalization capability for unseen test ing images. However, precisely annotating coronary ar teries is costly and requires special expertise, especially for thin branches with tubular appearance and low con trast in XA. To alleviate such heavy annotation burden on the annotator, reducing the amount of precise manual annotations is highly demanded in clinical practice [10]. In contrast, obtaining noisy pseudo labels appears to be less expensive. Specically, vessel enhancement [11] au tomatically extracts vascular structures based on hand craft priors [12], providing a feasible method for generat ing pseudo labels for training CNNs without any manual interaction. This can largely reduce the manual annota Preprint submitted to Elsevier May 28, 2020arXiv:2005.13366v1  [cs.CV]  27 May 2020Xray  Angiogram Pseudo  Label Figure 1: Noisy pseudo labels generated from vessel enhancement, where the systematic errors are highlighted by yellow arrows. tions required for model training, while leading to noise with systematic biases in pseudo labels for structures, such as bifurcation points and thin vessels with small scales, as shown in Fig. 1. These noisy pseudo labels challenge the learning process and cause performance degradation of CNNs at test time [13]. It is desirable to develop a robust training framework against systematic label noise and facilitate segmentation performance close to the fully supervised learning framework. Aimed at robustly learning from noisy labels, some pre vious weakly supervised training frameworks model label noise explicitly as an additional network layer [14, 15, 16] or implicitly using prior knowledge [17, 18]. Among them, researchers have shown that the selfpaced learning paradigm can be substantially eective and scalable [19], owing to its predened selfpaced regularizer [20]. This learning paradigm typically assumes a plain distribution of label noise without systematic biases to specic seg mentation regions and semantic categories. An iterative optimization process is used to facilitate noise robustness of the model. In each iteration, the selfpaced regular izer progressively selects only easy pixels while excluding dicult pixels with potential label noise from model train ing. Noisy labels are modied automatically by updating the segmentation results of training images based on the current model. They are expected to contain fewer er rors than those in previous iterations, providing improved supervision for the next iteration. Unfortunately, this self paced learning paradigm may make the model overt on easy pixels, leading to a poor generalization performance at test time. Moreover, the noise in pseudo labels often contains specic biases due to the inherent limitations of vessel enhancementbased generation process. Using this naive selfpaced learning paradigm alone has only the lim ited ability to correct the erroneous pseudo labels. Manually detecting and correcting potentially erroneous pseudo labels is a practical way to avoid the selfpaced learning being corrupted by systematic errors, while it is still laborintensive and timeconsuming. Suggestive anno tation [21] has been shown to be a more ecient method for interactive renement by intelligently selecting a small number of the most valuable pixels and then querying their labels. It suggests the annotator accurately label only the most uncertain pixels with potentially incorrect labels [22],commonly based on the widely used model uncertainty [23], i.e., the entropy of CNNs. The required annotation cost can be successfully reduced owing to the eective ex ploration of potential errors. However, model uncertainty fails to exploit geometric features derived directly from training images, resulting in redundancy among queries [24, 25] and a low eciency for manual interaction. In contrast, considering the vesselness of pixels is expected to lead to more contextaware uncertainty estimation as it takes advantage of vascular geometric features. Since the model uncertainty and vesselness uncertainty are comple mentary, we believe that their combination would provide more reliable uncertainty estimation that eciently guides user interaction in suggestive annotation. To solve these problems, this paper develops a novel weakly supervised vessel segmentation framework, which learns from costfree but noisy pseudo labels generated from automatic vessel enhancement. Specically, to over come noisy pseudo labels with systematic biases, we pro pose to progressively guide the naive selfpaced learn ing with auxiliary sparse manual annotations, which is called annotationrening selfpaced learning (ARSPL). ARSPL not only exploits the available knowledge from noisy pseudo labels, but also corrects potential errors us ing their corresponding manual annotations. These man ual annotations, even when sparse in training images, play an important role in hedging the risk of learning from noisy pseudo labels. Furthermore, to enable a minimal set of an notations, we propose a modelvesselness uncertainty esti mation for suggestive annotation, which dynamically and compactly takes into account the trained CNN and the geometric features of coronary arteries in XAs. 1.1. Contributions The contributions of this work are threefold. ‚Ä¢First, we propose a novel weakly supervised learning framework in the context of vessel segmentation, aim ing to safely learn from noisy pseudo labels generated by vessel enhancement without performance deterio ration at test time. ‚Ä¢Second, to deal with the biased label noise, we develop online guidance for the naive selfpaced learning based on sparse manual annotations, which is crucial for a signicant segmentation performance boost. ‚Ä¢Third, towards minimal manual intervention, we pro pose a customized vesselness uncertainty based on vascular geometric feature, and then couple it with the widely used model uncertainty by a dynamic tradeo for more ecient suggestive annotation. Experiments demonstrate the eectiveness and e ciency of the proposed framework, where only a very small set of manual annotations can lead to an accurate segmen tation result that is comparable to the fully supervised learning. 21.2. Related Works "
214,NLNL: Negative Learning for Noisy Labels.txt,"Convolutional Neural Networks (CNNs) provide excellent performance when used
for image classification. The classical method of training CNNs is by labeling
images in a supervised manner as in ""input image belongs to this label""
(Positive Learning; PL), which is a fast and accurate method if the labels are
assigned correctly to all images. However, if inaccurate labels, or noisy
labels, exist, training with PL will provide wrong information, thus severely
degrading performance. To address this issue, we start with an indirect
learning method called Negative Learning (NL), in which the CNNs are trained
using a complementary label as in ""input image does not belong to this
complementary label."" Because the chances of selecting a true label as a
complementary label are low, NL decreases the risk of providing incorrect
information. Furthermore, to improve convergence, we extend our method by
adopting PL selectively, termed as Selective Negative Learning and Positive
Learning (SelNLPL). PL is used selectively to train upon expected-to-be-clean
data, whose choices become possible as NL progresses, thus resulting in
superior performance of filtering out noisy data. With simple semi-supervised
training technique, our method achieves state-of-the-art accuracy for noisy
data classification, proving the superiority of SelNLPL's noisy data filtering
ability.","Convolutional Neural Networks (CNNs) have improved the performance of image classiÔ¨Åcation signiÔ¨Åcantly [17, 8, 29, 11, 7, 38]. For this supervised task, huge dataset composed of images and their corresponding labels is re quired for training CNNs. CNNs are powerful tools for classifying images if the corresponding labels are correct. However, accurately labeling a large number of images is daunting and timeconsuming, occasionally yielding mis matched labeling. When the CNNs are trained with noisy data, it can overÔ¨Åt to such a dataset, resulting in poor classi Ô¨Åcation performance. Therefore, training CNNs properly with noisy data is of great practical importance. Many Figure 1: Conceptual comparison between Positive Learn ing(PL) and Negative Learning (NL). Regarding noisy data, while PL provides CNN the wrong information (red balloon), with a higher chance, NL can provide CNN the correct information (blue balloon) because a dog is clearly not a bird. approaches address this problem by applying a number of techniques and regularization terms along with Posi tive Learning (PL), a typical supervised learning method for training CNNs that ‚Äúinput image belongs to this la bel‚Äù [6, 2, 34, 20, 3, 39, 26, 30, 22, 33, 21]. However, when the CNN is trained with images and mismatched la bels, wrong information is being provided to the CNN. To overcome this issue, we suggest Negative Learning (NL), an indirect learning method for training CNN that ‚Äúinput image does not belong to this complementary la bel.‚Äù NL does not provide wrong information as frequently as PL (Figure 1). For example, when training CNN with noisy CIFAR10 using PL, if the CNN receives an image of a dog and the label ‚Äúcar‚Äù, the CNN will be trained to ac knowledge that this image is a car. In this case, the CNN is trained with wrong information. However, with NL, the CNN will be randomly provided with a complementary la bel other than ‚Äúcar,‚Äù for example, ‚Äúbird.‚Äù Training CNN to acknowledge that this image is not a bird is in some way an act of providing CNN the right information because a dog is clearly not a bird. In this manner, noisy data can contribute to training CNN by providing the ‚Äúright‚Äù infor mation with a high chance of not selecting a true label as a complementary label, whereas zero chance is provided inarXiv:1908.07387v1  [cs.LG]  19 Aug 2019PL. Our study demonstrates the effectiveness of NL as it prevents CNN from overÔ¨Åtting to noisy data. Furthermore, utilizing NL training method, we pro pose Selective Negative Learning and Positive Learning (SelNLPL), which combines PL and NL to take full ad vantage of both methods for better training with noisy data. Although PL is unsuitable for noisy data, it is still a fast and accurate method for clean data. Therefore, after train ing CNN with NL, PL begins to train CNN selectively using only training data of high classiÔ¨Åcation conÔ¨Ådence. Through this process, SelNLPL widens the gap between the conÔ¨Ådences of clean data and noisy data, resulting in excel lent performance for Ô¨Åltering noisy data from training data. Subsequently, by discarding labels of Ô¨Åltered noisy data and treating them as unlabeled data, we utilize semi supervised learning for noisy data classiÔ¨Åcation. Based on the superior Ô¨Åltering ability of SelNLPL, we demonstrate that stateoftheart performance on noisy data classiÔ¨Åca tion can be achieved with a simple semisupervised learn ing method. Although this is not the Ô¨Årst time that noisy data classiÔ¨Åcation has been addressed by Ô¨Åltering noisy data [2, 6, 24], the Ô¨Åltering results have not been promis ing owing to the use of PL for noisy data. The main contributions of this paper are as follows: We apply the concept of Negative Learning to the prob lem of noisy data classiÔ¨Åcation. We prove its applica bility by demonstrating that it prevents the CNN from overÔ¨Åtting to noisy data. Utilizing the proposed NL, we introduce a new frame work, called SelNLPL, for Ô¨Åltering out noisy data from training data. Following NL, by selectively applying PL only to training data of high conÔ¨Ådence, we can achieve accurate Ô¨Åltering of noisy data. We achieved stateoftheart noisy data classiÔ¨Åcation re sults with relatively simple semisupervised learning based on the superior noisy data Ô¨Åltering achieved by SelNLPL. Our method does not require any prior knowledge of the type or number of noisy data points. It does not require any tuning of hyperparameters that depend on prior knowledge, making our method applicable in real life. The remainder of this paper is organized as follows: Sec tion 3 describes the overall process of our method with de tailed explanations of each step. Section 4 demonstrates the superior Ô¨Åltering ability of SelNLPL. Section 5 describes the experiments for evaluating our method, and Section 6 describes the experiments to further analyze our method. Finally, we conclude the paper in Section 7.2. Related works "
215,Symmetric Cross Entropy for Robust Learning with Noisy Labels.txt,"Training accurate deep neural networks (DNNs) in the presence of noisy labels
is an important and challenging task. Though a number of approaches have been
proposed for learning with noisy labels, many open issues remain. In this
paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting
to noisy labels on some classes (""easy"" classes), but more surprisingly, it
also suffers from significant under learning on some other classes (""hard""
classes). Intuitively, CE requires an extra term to facilitate learning of hard
classes, and more importantly, this term should be noise tolerant, so as to
avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we
propose the approach of \textbf{Symmetric cross entropy Learning} (SL),
boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy
(RCE). Our proposed SL approach simultaneously addresses both the under
learning and overfitting problem of CE in the presence of noisy labels. We
provide a theoretical analysis of SL and also empirically show, on a range of
benchmark and real-world datasets, that SL outperforms state-of-the-art
methods. We also show that SL can be easily incorporated into existing methods
in order to further enhance their performance.","Modern deep neural networks (DNNs) are often highly complex models that have hundreds of layers and millions of trainable parameters, requiring largescale datasets with clean label annotations such as ImageNet [2] for proper training. However, labeling largescale datasets is a costly and errorprone process, and even highquality datasets are likely to contain noisy (incorrect) labels. Therefore, training accurate DNNs in the presence of noisy labels has become a task of great practical importance in deep learning. Recently, several works have studied the dynamics of DNN learning with noisy labels. Zhang et.al [28] argued that DNNs exhibit memorization effects whereby they Ô¨Årst memorize the training data for clean labels and then subse quently memorize data for the noisy labels. Similar Ô¨Åndings Equal contribution. yCorrespondence to: Yisen Wang (eewangyisen@gmail.com) and Xingjun Ma (xingjun.ma@unimelb.edu.au). (a) CE  clean  (b) CE  noisy (c) LSR  noisy  (d)SL noisy Figure 1: The classwise test accuracy of an 8layer CNN on CIFAR10 trained by (a) CE on clean labels with class biased phenomenon, (b) CE on 40% symmetric/uniform noisy labels with ampliÔ¨Åed classbiased phenomenon and under learning on hard classes ( e.g., class 3), (c) LSR under the same setting to (b) with under learning on hard classes still existing, (d) our proposed SL under the same setting to (b) exhibiting overall improved learning on all classes. are also reported in [1] that DNNs Ô¨Årst learn clean and easy patterns and eventually memorize the wrongly assigned la bels. Further evidence is provided in [13] that DNNs Ô¨Årst learn simple representations via subspace dimensionality compression and then overÔ¨Åt to noisy labels via subspace dimensionality expansion. Different Ô¨Åndings are reported in [19], where DNNs with a speciÔ¨Åc activation function (i.e., tanh) undergo an initial label Ô¨Åtting phase then a sub sequent representation compression phase where the over Ô¨Åtting starts. Despite these important Ô¨Åndings, a complete understanding of DNN learning behavior, particularly their learning process for noisy labels, remains an open question. In this paper, we provide further insights into the learn ing procedure of DNNs by investigating the learning dy namics across classes. While Cross Entropy (CE) loss is the most commonly used loss for training DNNs, we have found that DNN learning with CE can be classbiased :arXiv:1908.06112v1  [cs.LG]  16 Aug 2019(a) CE  clean  (b) CE  noisy  (c) SL  noisy Figure 2: Visualization of learned representations on CIFAR10 using tSNE 2D embeddings of deep features at the last second dense layer with (a) CE on clean labels, (b) CE on 40% symmetric noisy labels, (c) the proposed SL on the same setting to (b). some classes (‚Äúeasy‚Äù classes) are easy to learn and con verge faster than other classes (‚Äúhard‚Äù classes). As shown in Figure 1a, even when labels are clean, the classwise test accuracy spans a wide range during the entire training pro cess. As further shown in Figure 1b, this phenomenon is ampliÔ¨Åed when training labels are noisy: whilst easy classes (e.g., class 6) already overÔ¨Åt to noisy labels, hard classes (e.g., class 3) still suffer from signiÔ¨Åcant under learning (class accuracy signiÔ¨Åcantly lower than clean label setting). SpeciÔ¨Åcally, class 3 (bottom curve) only has an accuracy of60% at the end, considerably less than the >90% ac curacy of class 6 (top curve). Label Smoothing Regular ization (LSR) [21, 17] is a widely known technique to ease overÔ¨Åtting issues, as shown in Figure 1c, which still exhibits signiÔ¨Åcant under learning on hard classes. Comparing the overall test accuracy (solid red curve) in Figure 1, a low test accuracy (under learning) on hard classes is a barrier to high overall accuracy. This is a different Ô¨Ånding from previous belief that poor performance is simply caused by overÔ¨Åtting to noisy labels. We also visualize the learned representa tions for the noisy label case in Figure 2b: some clusters are learned comparably well to those learned with clean la bels (Figure 2a), while some other clusters do not have clear separated boundaries. Intuitively, CE requires an extra term to improve its learning on hard classes, and more importantly, this term needs to be tolerant to label noise. Inspired by the sym metric KLdivergence, we propose such a noise tolerant term, namely Reverse Cross Entropy (RCE), which com bined with CE forms the basis of the approach Symmetric cross entropy Learning (SL). SL not only promotes sufÔ¨Å cient learning (class accuracy close to clean label setting) of hard classes, but also improves the robustness of DNNs to noisy labels. As a preview of this, we can inspect the improved learning curves of classwise test accuracy and representations in Figure 1d and 2c. Under the same 40% noise setting, the variation of classwise test accuracy has been narrowed by SL to 20% with 95% the highest and 75% the lowest (Figure 1d), and the learned representations are of better quality with more separated clusters (Figure 2c), both of which are very close to the clean settings. Compared to existing approaches that often involve ar chitectural or nontrivial algorithmic modiÔ¨Åcations, SL isextremely simple to use. It requires minimal intervention to the training process and thus can be straightforwardly incorporated into existing models to further enhance their performance. In summary, our main contributions are: We provide insights into the classbiased learning pro cedure of DNNs with CE loss and Ô¨Ånd that the under learning problem of hard classes is a key bottleneck for learning with noisy labels. We propose a Symmetric Learning (SL) approach, to simultaneously address the hard class under learning problem and the noisy label overÔ¨Åtting problem of CE. We provide both theoretical analysis and empirical un derstanding of SL. We empirically demonstrate that SL can achieve better robustness than stateoftheart methods, and can be also easily incorporated into existing methods to sig niÔ¨Åcantly improve their performance. 2. Related Work "
216,Correlated Input-Dependent Label Noise in Large-Scale Image Classification.txt,"Large scale image classification datasets often contain noisy labels. We take
a principled probabilistic approach to modelling input-dependent, also known as
heteroscedastic, label noise in these datasets. We place a multivariate Normal
distributed latent variable on the final hidden layer of a neural network
classifier. The covariance matrix of this latent variable, models the aleatoric
uncertainty due to label noise. We demonstrate that the learned covariance
structure captures known sources of label noise between semantically similar
and co-occurring classes. Compared to standard neural network training and
other baselines, we show significantly improved accuracy on Imagenet ILSVRC
2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a
new state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These
datasets range from over 1M to over 300M training examples and from 1k classes
to more than 21k classes. Our method is simple to use, and we provide an
implementation that is a drop-in replacement for the final fully-connected
layer in a deep classifier.","Image classiÔ¨Åcation datasets with many classes and large training sets often have noisy labels [ 2,30]. For example, Imagenet contains many visually similar classes that are hard for human annotators to distinguish [ 10,2]. Datasets such as WebVision where labels are generated automatically by look ing at cooccuring text to images on the Web, contain label noise as this automated process is not 100% reliable [30]. A wide range of techniques for classiÔ¨Åcation under label noise already exist [ 29,23,16,37,24,6,9,36,18]. When an image is mislabeled it is more likely that it gets confused with other related classes, rather than a random class [ 2]. Therefore it is important to take interclass correlation into account when modelling label noise in image classiÔ¨Åcation. Figure 1: Spot the difference? An Appenzeller (left) and EntleBucher (right). Two visually similar Imagenet classes our method learns have highly correlated label noise (aver age validation set covariance of 0.24) given only the stan dard Imagenet ILSVRC12 training labels. We take a principled probabilistic approach to modelling label noise. We assume a generative process for noisy labels with a multivariate Normal distributed latent variable at the Ô¨Ånal hidden layer of a neural network classiÔ¨Åer. The mean and covariance parameters of this Normal distribution are inputdependent (aka heteroscedastic), being computed from a shared representation of the input image. By modelling the interclass noise correlations our method can learn which class pairs are substitutes or commonly cooccur, resulting in noisy labels. See Fig. (1) for an example of two Imagenet classes which our model learns have correlated label noise. We evaluate our method on four largescale image clas siÔ¨Åcation datasets, Imagenet ILSVRC12 and Imagenet 21k[10], WebVision 1.0 [ 30] and JFT [ 21]. These datasets range from over 1M training examples (ILSVRC12) to 300M training examples (JFT) and from 1kclasses (ILSVRC12 & WebVision) to over 21kclasses (Imagenet 21k). We demon strate improved accuracy and negative loglikelihood on all datasets relative to (a) standard neural network training, (b) methods which only model the diagonal of the covariance matrix and (c) methods from the noisy labels literature. We evaluate the effect of our probabilistic label noise model on the representations learned by the network. We show that our method, when pretrained on JFT, learns image 1arXiv:2105.10305v1  [cs.LG]  19 May 2021representations which transfer better to the 19 datasets from the Visual Task Adaptation Benchmark (VTAB) [47]. Contributions. In summary our contributions are: 1.A new method which models interclass correlated label noise and scales to largescale datasets. 2.We evaluate our method on four largescale image clas siÔ¨Åcation datasets, showing signiÔ¨Åcantly improved per formance compared to standard neural network training and diagonal covariance methods. 3.We demonstrate that the learned covariance matrices model correlations between semantically similar or commonly cooccurring classes. 4.On VTAB our method learns more general representa tions which transfer better to 19 downstream tasks. 2. Method "
217,Person Re-identification in the Wild.txt,"We present a novel large-scale dataset and comprehensive baselines for
end-to-end pedestrian detection and person recognition in raw video frames. Our
baselines address three issues: the performance of various combinations of
detectors and recognizers, mechanisms for pedestrian detection to help improve
overall re-identification accuracy and assessing the effectiveness of different
detectors for re-identification. We make three distinct contributions. First, a
new dataset, PRW, is introduced to evaluate Person Re-identification in the
Wild, using videos acquired through six synchronized cameras. It contains 932
identities and 11,816 frames in which pedestrians are annotated with their
bounding box positions and identities. Extensive benchmarking results are
presented on this dataset. Second, we show that pedestrian detection aids
re-identification through two simple yet effective improvements: a
discriminatively trained ID-discriminative Embedding (IDE) in the person
subspace using convolutional neural network (CNN) features and a Confidence
Weighted Similarity (CWS) metric that incorporates detection scores into
similarity measurement. Third, we derive insights in evaluating detector
performance for the particular scenario of accurate person re-identification.","Automated entry and retail systems at theme parks, pas senger Ô¨Çow monitoring at airports, behavior analysis for automated driving and surveillance are a few applications where detection and recognition of persons across a cam era network can provide critical insights. Yet, these two problems have generally been studied in isolation within computer vision. Person reidentiÔ¨Åcation (reID) aims to Ô¨Ånd occurrences of a query person ID in a video sequence, 1L. Zheng, H. Zhang and S. Sun contribute equally. This work was partially sup ported by the Google Faculty Award and the Data to Decisions Cooperative Research Centre. This work was supported in part to Dr. Qi Tian by ARO grant W911NF 1510290 and Faculty Research Gift Awards by NEC Laboratories of America and Blippar. This work was supported in part by National Science Foundation of China (NSFC) 61429201. Project page: http://www.liangzheng.com.cn Raw	video	framesGalleryDetection	result Cam	2,	3,‚Ä¶Cam	1 ‚Ä¶(a)	Pedestrian	Detection	(b)	Person	Reidentification	Figure 1: Pipeline of an endtoend person reID system. It consists of two modules: pedestrian detection and person recognition (to differentiate from the overall reID). This pa per not only benchmarks both components, but also provides novel insights in their interactions. where stateoftheart datasets and methods start from pre deÔ¨Åned bounding boxes, either handdrawn [22, 25, 37] or automatically detected [21, 45]. On the other hand, sev eral pedestrian detectors achieve remarkable performance on benchmark datasets [12, 30], but little analysis is available on how they can be used for person reID. In this paper, we propose a dataset and baselines for practi cal person reID in the wild, which moves beyond sequential application of detection and recognition. In particular, we study three aspects of the problem that have not been con sidered in prior works. First, we analyze the effect of the combination of various detection and recognition methods on person reID accuracy. Second, we study whether detec tion can help improve reID accuracy and outline methods to do so. Third, we study choices for detectors that allow for maximal gains in reID accuracy. Current datasets lack annotations for such combined eval uation of person detection and reID. Pedestrian detection datasets, such as Caltech [10] or Inria [6], typically do not have ID annotations, especially from multiple cameras. On the other hand, person reID datasets, such as VIPeR [16] or CUHK03 [21], usually provide just cropped bounding boxes without the complete video frames, especially at a large scale. As a consequence, a largescale dataset that eval uates both detection and overall reID is needed. To address this, Section 3 presents a novel largescale dataset calledarXiv:1604.02531v2  [cs.CV]  6 Apr 2017PRW that consists of 932identities, with bounding boxes across 11;816frames. The dataset comes with annotations and extensive baselines to evaluate the impacts of detection and recognition methods on person reID accuracy. In Section 4, we leverage the volume of the PRW dataset to train stateoftheart detectors such as RCNN [15], with various convolutional neural network (CNN) architectures such as AlexNet [19], VGGNet [31] and ResidualNet [17]. Several wellknown descriptors and distance metrics are also considered for person reID. However, our joint setup al lows two further improvements in Section 4.2. First, we propose a cascaded Ô¨Ånetuning strategy to make full use of the detection data provided by PRW, which results in im proved CNN embeddings. Two CNN variants, are derived w.r.t the Ô¨Åne tuning strategies. Novel insights can be learned from the new Ô¨Ånetuning method. Second, we propose a ConÔ¨Ådence Weighted Similarity (CWS) metric that incor porates detection scores. Assigning lower weights to false positive detections prevents a drop in reID accuracy due to the increase in gallery size with the use of detectors. Given a dataset like PRW that allows simultaneous eval uation of detection and reID, it is natural to consider whether any complementarity exists between the two tasks. For a particular reID method, it is intuitive that a bet ter detector should yield better accuracy. But we argue that the criteria for determining a detector as better are applicationdependent. Previous works in pedestrian de tection [10, 28, 43] usually use Average Precision or Log Average Miss Rate under IoU >0:5for evaluation. How ever, through extensive benchmarking on the proposed PRW dataset, we Ô¨Ånd in Section 5 that IoU >0:7is a more effec tive rule in indicating detector inÔ¨Çuences on reID accuracy. In other words, the localization ability of detectors plays a critical role in reID. Figure 1 presents the pipeline of the endtoend reID system discussed in this paper. Starting from raw video frames, a gallery is created by pedestrian detectors. Given a query personofinterest, gallery bounding boxes are ranked according to their similarity with the query. To summarize, our main contributions are: A novel largescale dataset, Person ReidentiÔ¨Åcation in the Wild (PRW), for simultaneous analysis of person detection and reID. Comprehensive benchmarking of stateoftheart detec tion and recognition methods on the PRW dataset. Novel insights into how detection aids reID, along with an effective Ô¨Ånetuning strategy and similarity measure to illustrate how they might be utilized. Novel insights into the evaluation of pedestrian detec tors for the speciÔ¨Åc application of person reID. Figure 2: Annotation interface. All appearing pedestrians are annotated with a bounding box and ID. ID ranges from 1 to 932, and 2 stands for ambiguous persons. 2. Related Work "
218,Unsupervised Learning of Visual Representations using Videos.txt,"Is strong supervision necessary for learning a good visual representation? Do
we really need millions of semantically-labeled images to train a Convolutional
Neural Network (CNN)? In this paper, we present a simple yet surprisingly
powerful approach for unsupervised learning of CNN. Specifically, we use
hundreds of thousands of unlabeled videos from the web to learn visual
representations. Our key idea is that visual tracking provides the supervision.
That is, two patches connected by a track should have similar visual
representation in deep feature space since they probably belong to the same
object or object part. We design a Siamese-triplet network with a ranking loss
function to train this CNN representation. Without using a single image from
ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train
an ensemble of unsupervised networks that achieves 52% mAP (no bounding box
regression). This performance comes tantalizingly close to its
ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We
also show that our unsupervised network can perform competitively in other
tasks such as surface-normal estimation.","What is a good visual representation and how can we learn it? At the start of this decade, most computer vision research focused on ‚Äúwhat‚Äù and used handdeÔ¨Åned features such as SIFT [32] and HOG [5] as the underlying visual representation. Learning was often the last step where these lowlevel feature representations were mapped to seman tic/3D/functional categories. However, the last three years have seen the resurgence of learning visual representations directly from pixels themselves using the deep learning and Convolutional Neural Networks (CNNs) [28, 24, 23]. At the heart of CNNs is a completely supervised learning paradigm. Often millions of examples are Ô¨Årst labeled us ing Mechanical Turk followed by data augmentation to cre ate tens of millions of training instances. CNNs are then trained using gradient descent and back propagation. But one question still remains: is strongsupervision necessary for training these CNNs? Do we really need millions of semanticallylabeled images to learn a good representation? ‚Ä¶ ‚Ä¶   ‚Ä¶ ‚Ä¶  Learning to Rank   Conv   Net Conv   Net Conv   Net  Query   (First Frame)  Tracked   (Last Frame)  Negative   (Random)  (a) Unsupervised Tracking in Videos   ùê∑  ,  ùê∑ ,  ùê∑ ,  ùê∑ ,  ùê∑: Distance in deep feature space   (b) Siamese triplet Network  (c) Ranking Objective  Figure 1. Overview of our approach. (a) Given unlabeled videos, we perform unsupervised tracking on the patches in them. (b) Triplets of patches including query patch in the initial frame of tracking, tracked patch in the last frame, and random patch from other videos are fed into our siamesetriplet network for train ing. (c) The learning objective: Distance between the query and tracked patch in feature space should be smaller than the distance between query and random patches. It seems humans can learn visual representations using little or no semantic supervision but our approaches still remain completely supervised. In this paper, we explore the alternative: how we can ex ploit the unlabeled visual data on the web to train CNNs (e.g. AlexNet [24])? In the past, there have been several at tempts at unsupervised learning using millions of static im ages [26, 44] or frames extracted from videos [56, 48, 34]. The most common architecture used is an autoencoder which learns representations based on its ability to recon struct the input images [35, 3, 49, 37]. While these ap proaches have been able to automatically learn V1like Ô¨Ål ters given unlabeled data, they are still far away from su pervised approaches on tasks such as object detection. So, what is the missing link? We argue that static images them selves might not have enough information to learn a good 1arXiv:1505.00687v2  [cs.CV]  6 Oct 2015visual representation. But what about videos? Do they have enough information to learn visual representations? In fact, humans also learn their visual representations not from mil lions of static images but years of dynamic sensory inputs. Can we have similar learning capabilities for CNNs? We present a simple yet surprisingly powerful approach for unsupervised learning of CNNs using hundreds of thou sands of unlabeled videos from the web. Visual tracking is one of the Ô¨Årst capabilities that develops in infants and often before semantic representations are learned1. Taking a leaf from this observation, we propose to exploit visual track ing for learning CNNs in an unsupervised manner. SpeciÔ¨Å cally, we track millions of ‚Äúmoving‚Äù patches in hundreds of thousands of videos. Our key idea is that two patches con nected by a track should have similar visual representation in deep feature space since they probably belong to the same object. We design a Siamesetriplet network with ranking loss function to train the CNN representation. This ranking loss function enforces that in the Ô¨Ånal deep feature space the Ô¨Årst frame patch should be much closer to the tracked patch than any other randomly sampled patch. We demon strate the strength of our learning algorithm using exten sive experimental evaluation. Without using a single image from ImageNet, just 100K unlabeled videos and VOC 2012 dataset, we train an ensemble of AlexNet networks that achieves 52% mAP (no bounding box regression). This per formance is similar to its ImageNetsupervised counterpart, an ensemble which achieves 54:4%mAP. We also show that our network trained using unlabeled videos achieves simi lar performance to its completely supervised counterpart on other tasks such as surface normal estimation. We believe this is the Ô¨Årst time an unsupervisedpretrained CNN has been shown so competitive; that too on varied datasets and tasks. SpeciÔ¨Åcally for VOC, we would like to put our re sults in context: this is the best results tilldate by using only PASCALprovided annotations (next best is scratch at 44%). 2. Related Work "
219,GMNN: Graph Markov Neural Networks.txt,"This paper studies semi-supervised object classification in relational data,
which is a fundamental problem in relational data modeling. The problem has
been extensively studied in the literature of both statistical relational
learning (e.g. relational Markov networks) and graph neural networks (e.g.
graph convolutional networks). Statistical relational learning methods can
effectively model the dependency of object labels through conditional random
fields for collective classification, whereas graph neural networks learn
effective object representations for classification through end-to-end
training. In this paper, we propose the Graph Markov Neural Network (GMNN) that
combines the advantages of both worlds. A GMNN models the joint distribution of
object labels with a conditional random field, which can be effectively trained
with the variational EM algorithm. In the E-step, one graph neural network
learns effective object representations for approximating the posterior
distributions of object labels. In the M-step, another graph neural network is
used to model the local label dependency. Experiments on object classification,
link classification, and unsupervised node representation learning show that
GMNN achieves state-of-the-art results.","We live in an interconnected world, where entities are con nected through various relations. For example, web pages are linked by hyperlinks; social media users are connected through friendship relations. Modeling such relational data is an important topic in machine learning, covering a va riety of applications such as entity classiÔ¨Åcation (Perozzi et al., 2014), link prediction (Taskar et al., 2004) and link 1Mila  Qu ¬¥ebec AI Institute2University of Montr ¬¥eal3Canadian Institute for Advanced Research (CIFAR)4HEC Montr ¬¥eal. Cor respondence to: Meng Qu <meng.qu@umontreal.ca >, Jian Tang <jian.tang@hec.ca >. Proceedings of the 36thInternational Conference on Machine Learning , Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).classiÔ¨Åcation (Dettmers et al., 2018). Many of these applications can be boiled down to the fundamental problem of semisupervised object classiÔ¨Åca tion (Taskar et al., 2007). SpeciÔ¨Åcally, objects1are inter connected and associated with some attributes. Given the labels of a few objects, the goal is to infer the labels of other objects. This problem has been extensively studied in the literature of statistical relational learning (SRL), which develops statistical methods to model relational data. Some representative methods include relational Markov networks (RMN) (Taskar et al., 2002) and Markov logic networks (MLN) (Richardson & Domingos, 2006). Generally, these methods model the dependency of object labels using con ditional random Ô¨Åelds (Lafferty et al., 2001). Because of their effectiveness for modeling label dependencies, these methods achieve compelling results on semisupervised ob ject classiÔ¨Åcation. However, several limitations still remain. (1) These methods typically deÔ¨Åne potential functions in conditional random Ô¨Åelds as linear combinations of some handcrafted feature functions, which are quite heuristic. Moreover, the capacity of such models is usually insufÔ¨Åcient. (2) Due to the complexity of relational structures between objects, inferring the posterior distributions of object labels for unlabeled objects remains a challenging problem. Another line of research is based on the recent progress of graph neural networks (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veli Àáckovi ¬¥c et al., 2018). Graph neural networks approach object classiÔ¨Åcation by learning effective object representations with nonlinear neu ral architectures, and the whole framework can be trained in an endtoend fashion. For example, the graph convolu tional network (GCN) (Kipf & Welling, 2017) iteratively updates the representation of each object by combining its own representation and the representations of the surround ing objects. These approaches have been shown to achieve stateoftheart performance because of their effectiveness in learning object representations on relational data. How ever, one critical limitation is that the labels of objects are independently predicted based on their representations. In other words, the joint dependency of object labels is ignored. 1In this paper, we will use ‚Äúobject‚Äù and ‚Äúnode‚Äù interchangeably to refer to entities in graphs, because they are different terminolo gies used in the literature of statistical relational learning and graph neural networks.arXiv:1905.06214v3  [cs.LG]  23 Jul 2020GMNN: Graph Markov Neural Networks In this paper, we propose a new approach called the Graph Markov Neural Network (GMNN), which combines the ad vantages of both statistical relational learning and graph neural networks. A GMNN is able to learn effective object representations as well as model label dependency between different objects. Similar to SRL methods, a GMNN in cludes a conditional random Ô¨Åeld (Lafferty et al., 2001) to model the joint distribution of object labels conditioned on object attributes. This framework can be effectively and efÔ¨Å ciently optimized with the variational EM framework (Neal & Hinton, 1998), alternating between an inference proce dure (Estep) and a learning procedure (Mstep). In the learning procedure, instead of maximizing the likelihood function, the training procedure for GMNNs optimizes the pseudolikelihood function (Besag, 1975) and parameterizes the local conditional distributions of object labels with a graph neural network. Such a graph neural network can well model the dependency of object labels, and no handcrafted potential functions are required. For inference, since exact inference is intractable, we use a meanÔ¨Åeld approxima tion (Opper & Saad, 2001). Inspired by the idea of amor tized inference (Gershman & Goodman, 2014; Kingma & Welling, 2014), we further parameterize the posterior dis tributions of object labels with another graph neural net work, which is able to learn useful object representations for predicting object labels. With a graph neural network for inference, the number of parameters can be signiÔ¨Åcantly reduced, and the statistical evidence can be shared across different objects in inference (Kingma & Welling, 2014). Our GMNN approach is very general. Though it is designed for object classiÔ¨Åcation, it can be naturally applied to many other applications, such as unsupervised node representation learning and link classiÔ¨Åcation. Experiment results show that GMNNs achieve stateoftheart results on object clas siÔ¨Åcation and unsupervised node representation learning, as well as very competitive results on link classiÔ¨Åcation. 2. Related Work "
220,DNS: Determinantal Point Process Based Neural Network Sampler for Ensemble Reinforcement Learning.txt,"Application of ensemble of neural networks is becoming an imminent tool for
advancing the state-of-the-art in deep reinforcement learning algorithms.
However, training these large numbers of neural networks in the ensemble has an
exceedingly high computation cost which may become a hindrance in training
large-scale systems. In this paper, we propose DNS: a Determinantal Point
Process based Neural Network Sampler that specifically uses k-dpp to sample a
subset of neural networks for backpropagation at every training step thus
significantly reducing the training time and computation cost. We integrated
DNS in REDQ for continuous control tasks and evaluated on MuJoCo environments.
Our experiments show that DNS augmented REDQ outperforms baseline REDQ in terms
of average cumulative reward and achieves this using less than 50% computation
when measured in FLOPS.","In the past decade, reinforcement learning (RL) algorithms powered by highcapacity function approximators such as deep neural networks have been used to master complex sequential decision problems such as Atari games (Mnih et al., 2015), boards games like Chess, Go and Shogi (Silver et al., 2016; 2017; 2018) and robotic manipulation (Liu et al., 2021). Despite having impressive results, deep reinforce ment learning (DRL) algorithms suffer from whole host of problems such as sample inefÔ¨Åciency ( ≈Åukasz Kaiser et al., 2020), overestimation bias (van Hasselt, 2010; Hado van Hasselt et al., 2016; Lan et al., 2020; Anschel et al., 2017; Fujimoto et al., 2018) and imbalance between exploration and exploitation (Lee et al., 2020; Osband et al., 2016). *Equal contribution1Intel Labs2Intel Corporation. Corre spondence to: Hassam Sheikh <hassam.sheikh@intel.com, has samsheikh1@gmail.com >. Proceedings of the 39thInternational Conference on Machine Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy right 2022 by the author(s).Considering the success of ensembles in supervised learn ing, use of ensemble of neural networks is becoming popular in deep reinforcement learning (DRL) and are being used to address the aforementioned issues. For example, in (Lan et al., 2020; Anschel et al., 2017; Fujimoto et al., 2018) have used ensemble to address the overestimation bias problem. In (Chen et al., 2021) proposed REDQ that uses ensemble with high updatetodate ratio to address the sample inef Ô¨Åciency problem. Similarly (Lee et al., 2020) have used ensemble for efÔ¨Åcient exploration. Despite ensembles providing elegant theoretical and practi cal solutions, they introduce new practical problems such as high computation cost and long training times. The high computation cost problem is specially evident in actorcritic settings where DRL algorithms use a high number of critic networks. One such example is the REDQ algorithm that uses ten critic networks and updates all of them in every training step which leads to high computation cost as well as high wallclock time. To address this issue, we present DNS: a Determinantal Point Process based Neural Network Sampler that speciÔ¨Å cally useskDPP (Kulesza & Taskar, 2011) to sample a sub set of neural networks for backpropagation at every training step. DNS uses Centered Kernel Alignment (CKA) (Korn blith et al., 2019) values to form the similarity matrix which are then used by the kDPP to sample the subset on neural networks for backpropagation. The motivation for sampling a subset of networks came from a hypothesis which we show in Section 4.1 that the Qvalues from the critics converge prematurely during training thus eliminating the need of training all the critics at every training step. Additionally, we show that in the event that the CKA ma trix is not positive semideÔ¨Ånite, the closest positive semi deÔ¨Ånite matrix is just a diagonal perturbation of the CKA matrix and its resulting kernel matrix is still Hermitian posi tive semideÔ¨Ånite. We applied DNS on REDQ and performed experiments on MuJoCo environments (Todorov et al., 2012) and show that a simple sampling technique can signiÔ¨Åcantly reduce the training time and computation while maintaining similar performance as training all the networks in the ensemble.arXiv:2201.13357v3  [cs.LG]  17 May 2022DNS: D eterminantal Point Process Based N eural Network S ampler for Ensemble Reinforcement Learning To summarize, our contributions are following: 1.We empirically show that neural network based value function approximators collapse prematurely during training in ensemble reinforcement learning. 2.To address this issue, we propose a Determinantal Point Process based Neural Network Sampler that samples a subset of valuefunction approximators for backpropa gation at every training step. 3.We apply DNS on REDQ, that uses an ensemble of tencritic networks. Our experiments have shown that DNS sampling achieves similar or better results than REDQ in 50% computation when measured in FLOPS. 4.We also provide a theoretical analysis and proof that shows thatkDPP sampling of actionvalue functions leads to lower actionvalue minimization variance than random sampling kactionvalues. Additionally, we show how sufÔ¨Åciency conditions for kDPP sampling can easily be met for the Deep RL use case. 2. Related Work "
221,Uncertainty Based Detection and Relabeling of Noisy Image Labels.txt,"Deep neural networks (DNNs) are powerful tools in computer vision tasks.
However, in many realistic scenarios label noise is prevalent in the training
images, and overfitting to these noisy labels can significantly harm the
generalization performance of DNNs. We propose a novel technique to identify
data with noisy labels based on the different distributions of the predictive
uncertainties from a DNN over the clean and noisy data. Additionally, the
behavior of the uncertainty over the course of training helps to identify the
network weights which best can be used to relabel the noisy labels. Data with
noisy labels can therefore be cleaned in an iterative process. Our proposed
method can be easily implemented, and shows promising performance on the task
of noisy label detection on CIFAR-10 and CIFAR-100.","In the last decade Deep neural networks (DNNs) have proven their predictive power in many supervised learning tasks with complex data patterns. Especially when the train ing set is large, representative, and correctly labeled, DNNs are the current stateoftheart on several learning tasks. Un fortunately, the latter assumption does not hold in many re alistic cases (e.g. medical imaging, crowdsourced label ing), and DNNs have been shown to overÔ¨Åt on noisy labels, leading to poor generalization performance. For example, [33] shows that DNNs can easily Ô¨Åt randomly assigned la bels on the training set, which leads to poor test perfor mance. Therefore, it is important to detect and correct for noisy labels in the training set. We propose an iterative label noise Ô¨Åltering process, based on the predictive uncertainty of the training images. Ensembles [17] and MC dropout [8] are used to obtain un certainty estimates for each image. We show that the un certainties of the noisy images and the uncertainties of the clean images follow two different distributions, enabling the Bosch Center for ArtiÔ¨Åcial Intelligence. First name.last name@de.bosch.comdetection of potentially noisy labels. After the detection of the noisy labels, the detected image could be taken out of the training set, its weight on the loss could be decreased, or it could be relabeled through an oracle or any appropriate relabeling approach. 2. Related Work "
222,PLM: Partial Label Masking for Imbalanced Multi-label Classification.txt,"Neural networks trained on real-world datasets with long-tailed label
distributions are biased towards frequent classes and perform poorly on
infrequent classes. The imbalance in the ratio of positive and negative samples
for each class skews network output probabilities further from ground-truth
distributions. We propose a method, Partial Label Masking (PLM), which utilizes
this ratio during training. By stochastically masking labels during loss
computation, the method balances this ratio for each class, leading to improved
recall on minority classes and improved precision on frequent classes. The
ratio is estimated adaptively based on the network's performance by minimizing
the KL divergence between predicted and ground-truth distributions. Whereas
most existing approaches addressing data imbalance are mainly focused on
single-label classification and do not generalize well to the multi-label case,
this work proposes a general approach to solve the long-tail data imbalance
issue for multi-label classification. PLM is versatile: it can be applied to
most objective functions and it can be used alongside other strategies for
class imbalance. Our method achieves strong performance when compared to
existing methods on both multi-label (MultiMNIST and MSCOCO) and single-label
(imbalanced CIFAR-10 and CIFAR-100) image classification datasets.","The impressive performance of deep learning meth ods has led to the creation of many largescale datasets [6, 12, 27, 23]. Due to the naturally imbalanced distri bution of objects within the world, these datasets contain imbalanced numbers of samples for different classes. The class labels in these datasets form a longtailed distribution: several classes appear frequently (the head classes), while many classes contain few samples (the tail classes). This imbalance causes classiÔ¨Åers to perform poorly, especially Figure 1. The output probability ( ^yc) distributions of a ResNet 32 classiÔ¨Åer trained on artiÔ¨Åcially imbalanced CIFAR10 for pos itive (left) and negative samples (right). For frequent classes, the predicted distribution skews towards 1; for infrequent classes, it skews towards 0. ClassiÔ¨Åers trained using PLM (bottom) reduces this bias, when compared to classiÔ¨Åers trained with binary cross entropy (top). on classes which are infrequent in training. Finding a solu tion to this problem is necessary to successfully scale deep networks to larger realworld datasets which tend to have longtail data distributions. Several recent works [2, 5, 3] attempt to solve the data imbalance issues; however, most tend to have singlelabel assumptions. For example, LDAMDRW [3] performs very well in singlelabel settings, but it assumes a single class label,y, is present for a given sample, x, to compute class margins (x;y)for their proposed loss. Not only that, but the two most common methods for learning longtailed dis tributions, reweighting and resampling, were not designed for data with multiple labels. Our experiments show that reweighting based on the inverse number of samples per forms poorly on multilabel datasets; also, it is difÔ¨Åcult to resample multilabel data due to the cooccurrence of labels within individual samples. Since many realworld applica tions like image tagging [28, 10], recommendation systemsarXiv:2105.10782v1  [cs.CV]  22 May 2021[37, 31], and action detection [12, 9], often involve multi label classiÔ¨Åcation and suffer from imbalanced data, we be lieve that classimbalance methods should be developed for both singlelabel and multilabel settings. Therefore, in this work, we propose a general solution for longtailed imbal ance which works for both multilabel and singlelabel clas siÔ¨Åcation. ClassiÔ¨Åers trained on imbalanced multilabel datasets tend to overpredict frequent classes and underpredict mi nority classes. This behaviour is displayed in Figure 1. When the class is not present within the image, the prob ability output for the frequent classes is skewed towards 1; conversely, when an infrequent class is present within the sample, the classiÔ¨Åer outputs a low probability score. These output probability distributions differ greatly from the ideal distribution (i.e. the groundtruth distribution where all pos itive samples are labeled 1 and all negative samples are la beled 0). We argue, that this behaviour is caused not only by an imbalance in the number of positive samples between different classes, but also by the ratio of positive and nega tive samples for each class . As dataset imbalance increases, the ratio of positive sam ples to negative samples increases for head classes and de creases for tail classes. We Ô¨Ånd that the change in this ra tio greatly impacts the classiÔ¨Åer‚Äôs ability to generalize. If a class has large ratio of positive samples to negative sam ples, the classiÔ¨Åer overpredicts the given class, leading to an increase of false positive predictions; conversely, a small ratio leads to underprediction and an increase of false neg atives. Assuming there is an optimal ratio which can mini mize the over/underpredictions, an algorithm can estimate and leverage this ratio to improve network performance. We present Partial Label Masking (PLM): a novel ap proach for training classiÔ¨Åers on imbalanced multilabel datasets which improves network generalization by lever aging this ratio. By partially masking positive and nega tive labels for frequent and infrequent classes respectively; our method reduces the discrepancy between the classiÔ¨Åers‚Äô output probability distribution and the groundtruth distri bution (as seen in Figure 1). Our method performs this masking stochastically for each sample and it continually adapts the target ratio based on the classiÔ¨Åer‚Äôs output prob abilities. This leads to improved precision on classes with many samples and improved recall on classes with few sam ples. Moreover, our method consistently improves perfor mance on difÔ¨Åcult classes, regardless of the number of sam ples. Our contributions include: (i) we present a general solu tion for data imbalance which balances the ratio between positive and negative samples, (ii) we propose an adap tive strategy to determine the ideal ratio which minimizes the difference between predicted probability and ground truth distributions, (iii) we empirically evaluate our methodon both multilabel datasets (imbalanced MultiMNIST and MSCOCO) and singlelabel datasets (CIFAR10 and CI FAR100), and (iv) we thoroughly analyse our method‚Äôs ability to improve classiÔ¨Åers‚Äô performance on both difÔ¨Åcult and infrequent classes. 2. Method "
223,Tripartite: Tackle Noisy Labels by a More Precise Partition.txt,"Samples in large-scale datasets may be mislabeled due to various reasons, and
Deep Neural Networks can easily over-fit to the noisy label data. To tackle
this problem, the key point is to alleviate the harm of these noisy labels.
Many existing methods try to divide training data into clean and noisy subsets
in terms of loss values, and then process the noisy label data varied. One of
the reasons hindering a better performance is the hard samples. As hard samples
always have relatively large losses whether their labels are clean or noisy,
these methods could not divide them precisely. Instead, we propose a Tripartite
solution to partition training data more precisely into three subsets: hard,
noisy, and clean. The partition criteria are based on the inconsistent
predictions of two networks, and the inconsistency between the prediction of a
network and the given label. To minimize the harm of noisy labels but maximize
the value of noisy label data, we apply a low-weight learning on hard data and
a self-supervised learning on noisy label data without using the given labels.
Extensive experiments demonstrate that Tripartite can filter out noisy label
data more precisely, and outperforms most state-of-the-art methods on five
benchmark datasets, especially on real-world datasets.","Thanks to the largescale datasets with human precisely annotated labels, DNNs achieve a great success. How ever, collecting highquality and extensive data is consid erably costly and timeconsuming. To alleviate this is sue, some cheaper alternatives are often employed, such as webcrawling [23], online queries [4, 20, 34], crowdsourc ing [41, 45] and so on. Unfortunately, they inevitably intro duce some noisy labels. Many studies [3,19,33,46] have re ported that DNNs could easily overÔ¨Åt to the noises, which signiÔ¨Åcantly degrades their generalization performance. There have been many efforts to tackle noisy labels. Many of them reach a consensus that the key is to allevi ate the impact of noisy labels for network training. Loss correction based methods [9, 27, 28, 32] aim to rectify thelosses of noisy labeled data in the training stage, but may mistakenly rectify some clean data. Sampleselection based methods [10, 18, 37, 44] tackle noisy labels by partitioning the training data into clean and noisy subsets, then using them for network training in different ways. The main stream partition criteria are two types: 1) Smallloss cri terion [10] assumes that the losses of noisy labeled data are signiÔ¨Åcantly higher than those of clean data during training. Therefore, they try to Ô¨Ånd a threshold, T, and select the samples, whose losses < T , as clean data. The others are treated as noisy labeled data. 2) Gaussian Mixture Model (GMM) criterion [18] believes that the statistical distribu tion of noisy data losses is different from that of clean data losses. It aims to Ô¨Ånd the probability of a sample being noisy or clean by Ô¨Åtting a mixture model. However, we observe that many realworld noisy labels are introduced between similar categories, especially hap pen among hard samples. For instances, a dolphin is mis labeled as a whale, vice versa, as shown in Fig. 1. In this paper, we deÔ¨Åne ‚Äúhard samples‚Äù as data that distribute close to the decision boundary and are difÔ¨Åcult to be dis tinguished. Our investigation shows, regardless of clean or noisy, the training losses of hard samples are neither small nor signiÔ¨Åcantly different. So, existing sampleselection based methods have two Ô¨Çaws: 1) Low quality of training data partition. They are likely to mistake the hard noisy la beled samples as clean ones solely based on losses, and vice versa. This downgrades the performance of a network be cause the network will learn certain noisy labels and discard some clean data. 2) Ineffective usage of noisy labeled data. Smallloss criterion often drops the noisy samples which is a waste of valuable information. GMM criterion applies the semisupervised learning to reuse noisy labeled samples by assigning pseudolabels to them. But it heavily relies on the discriminative ability of networks. The incorrect relabeling will cause a severe harm to networks as well. To tackle above problems, we propose a novel method: Tripartite that mainly addresses hard samples in realworld datasets. Since hard samples distribute around the decision boundary, predictions of varied networks are often inconsis tent at the early training stage. Meanwhile, the prediction 1arXiv:2202.09579v2  [cs.CV]  19 Mar 2022Data distribution Network Partition criterion Partition result ùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL : keyboard GL :  keyboardùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL: woman GL:  keyboardEasy samplesùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL : whale GL :  dolphin ùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL :  dolphin GL :  dolphin ùë∑ùëªùë≥ ùë∑ùëÆùë≥ 0.5 TL : dolphin GL : womanHard samplesHard Noisy Clean xSmall loss Loss Loss:  <R% Loss:  >R% Loss:  <R% Loss:  <R%  Loss: >R% P1 P2 x xTripartite  GL dolphin Ôºö ùêèùüè:  whale ùêèùüê:  dolphin GL:  keyboard ùêèùüè:  woman ùêèùüê:  woman GL dolphin ùêèùüè:  whale ùêèùüê:  dolphin GL:  woman ùêèùüè:  dolphin ùêèùüê:  dolphin GL :  keyboard ùêèùüè:  keyboard ùêèùüê:  keyboard Divided by Loss distributionLoss xGMM ùêèùêßùê®ùê¢ùê¨ùêû : 0.55 ùêèùêúùê•ùêûùêöùêß: 0.45 ùêèùêúùê•ùêûùêöùêß: 0. 55 ùêèùêßùê®ùê¢ùê¨ùêû : 0.45 ùêèùêúùê•ùêûùêöùêß: 0.90 ùêèùêßùê®ùê¢ùê¨ùêû : 0.10 ùêèùêßùê®ùê¢ùê¨ùêû : 0.85 ùêèùêúùê•ùêûùêöùêß: 0.15 ùêèùêßùê®ùê¢ùê¨ùêû : 0.51 ùêèùêúùê•ùêûùêöùêß: 0.49 Divided by Tripartition RuleP1=GL‚ãÇP2=GL P1=GL‚ãÇP2‚â†GL ‚à™ P1‚â†GL‚ãÇP2=GL P1‚â†GL‚ãÇP2‚â†GLClean Hard NoisyT1 T2Divided by Loss ValueSmall LargeR% 1R%T Clean Noisywoman dolphinwhalekeyboardFigure 1. Comparison with existing partition criteria. From top to bottom are Smallloss criterion, GMM criterion and the proposed Tripartition criterion. TL denotes the true label, GL denotes the given label. P1andP2are the predicted labels of inputs. PTLandPGL denote the predicted probabilities of true and given labels, respectively. Pclean andPnoise denote the predicted probabilities of a sample to be clean and noisy, respectively. In the partition result, the check mark and cross mark denote that the sample is partitioned correctly and incorrectly, respectively. Smallloss criterion sorts losses and selects R% samples with small losses as clean data for training. GMM criterion Ô¨Ånds the probability of a sample being clean or noisy by Ô¨Åtting a mixture model. Neither of them can Ô¨Ålter out hard samples. By contrast, Tripartite divides the training set into hard, noisy, and clean subsets according to the relations between P1,P2and GL. of a network of an easy noisy sample is usually inconsis tent with the given label. Based on this observation, Tri partite can divide training data into hard, noisy and clean subsets. It has the advantage of improving the quality of clean and noisy subsets. To effectively use data, we apply a lowweight training strategy for samples in the hard subset, and employ a selfsupervised training strategy for samples in the noisy subset without using the given labels. Hence, Tripartite is designed toward minimizing the harm of noisy labels and maximizing the value of noisy labeled data. The extensive experiments on Ô¨Åve benchmark datasets demon strate the superior performance of Tripartite compared with the stateoftheart (SOTA) methods. Especially, it shows robustness at a wide range of realworld datasets. The key contributions of our work are threefold. ‚Ä¢ We propose a novel partition criterion, which divides training data into three subsets: hard, noisy, and clean. It alleviates the hard sample selection problem of other cri teria, and largely improves the quality of clean and noisy subsets. ‚Ä¢ We design a lowweight training strategy for hard data and a selfsupervised training strategy for noisy labeleddata, which aim at minimizing the harm of noisy labels and maximizing the value of noisy labeled data. ‚Ä¢ To mimic the noisy label of hard sample in realworld datasets, we create a synthetic classdependent label noise on CIFAR datasets, called realistic noise. It Ô¨Çips labels of samples, which are from two different classes, at con trolled ratios according to their similarity. The details are shown in Supplementary. 2. Related work "
224,Interpretable and Accurate Fine-grained Recognition via Region Grouping.txt,"We present an interpretable deep model for fine-grained visual recognition.
At the core of our method lies the integration of region-based part discovery
and attribution within a deep neural network. Our model is trained using
image-level object labels, and provides an interpretation of its results via
the segmentation of object parts and the identification of their contributions
towards classification. To facilitate the learning of object parts without
direct supervision, we explore a simple prior of the occurrence of object
parts. We demonstrate that this prior, when combined with our region-based part
discovery and attribution, leads to an interpretable model that remains highly
accurate. Our model is evaluated on major fine-grained recognition datasets,
including CUB-200, CelebA and iNaturalist. Our results compare favorably to
state-of-the-art methods on classification tasks, and our method outperforms
previous approaches on the localization of object parts.","Deep models are tremendously successful for visual recognition, yet their results are oftentimes hard to explain. Consider the examples in Fig. 1. Why does a deep model recognize the bird as ‚ÄúYellowheaded Blackbird‚Äù or con sider the person ‚ÄúSmiling‚Äù? While the interpretation of a model can happen at multiple facets, we believe that at least one way of explaining the model is to segment meaningful regions of object parts (e.g., the eyes, mouth, cheek, fore head and neck of a face), and further identify their contri butions towards the decision (e.g., the mouth region is more discriminative for smiling). How can we design an inter pretable deep model that learns to discover object parts and estimates their importance for visual recognition? It turns out that part discovery, i.e., learning object parts without explicit supervision of part annotations, is by itself Input ImagePart Assignment MapPart Attention MapYellowheaded Blackbird Smiling Figure 1. Why does a deep model recognize the bird as ‚ÄúYellow headed Blackbird‚Äù or consider the person ‚ÄúSmiling‚Äù ? We present an interpretable deep model for Ô¨Ånegrained recognition. Given an input image (left), our model is able to segment object parts (middle) and identify their contributions (right) for the decision. Results are from our model trained using only imagelevel labels . a challenging problem. As a baby step, we focus on the task of Ô¨Ånegrained recognition, where the parts belonging to the same super category share common visual patterns. For ex ample, most tails of birds have a similar shape. Our key observation is that features from a convolutional network can be used to group pixels into a set of visually coherent regions [28, 25], from which a subset of discriminative seg ments can be selected for recognition [33, 32, 11]. With only object labels as the guidance , we hope that the group ing will help to Ô¨Ånd visually distinct parts, and the selection process will identify their contributions for classiÔ¨Åcation. A major challenge for our regionbased part discovery is that there is no explicit supervisory signal to deÔ¨Åne part regions. Therefore, prior knowledge about object parts must be incorporated to facilitate the learning. A core innovation of our work is the exploration of a simple prior about object parts: given a single image, the occurrence of a part follows a Ushaped distribution. For example, the head of a bird is likely to occur in most bird images while the legs of a bird might only appear in some images. Surprisingly, we demonstrate that this simple prior, when combined with ourarXiv:2005.10411v1  [cs.CV]  21 May 2020regionbased part discovery, leads to the identiÔ¨Åcation of meaningful object parts. More importantly, the resulting interpretable deep model remains highly accurate. Several recent methods have been developed for discovering parts in Ô¨Ånegrained classiÔ¨Åcation, yet none of them considered the prior we use. To this end, we present our interpretable deep model for Ô¨Ånegrained classiÔ¨Åcation. SpeciÔ¨Åcally, our model learns a dictionary of object parts, based on which a 2D feature map can be grouped into ‚Äúpart‚Äù segments. This is done by com paring pixel features to part representations in a learned dic tionary. Moreover, regionbased features are pooled from the result segments, followed by an attention mechanism to select a subset of segments for classiÔ¨Åcation. Importantly, during training, we enforce a Ushaped prior distribution for the occurrence of each part. This is done by minimiz ing the Earth Mover‚Äôs Distance between our prior and the empirical distribution of part occurrence. During training, our model is only supervised by object labels with our pro posed regularization term. During testing, our model jointly outputs the segments of object parts, the importance of the segmented parts, and the predicted label. The interpretation of our model is thus granted by the part segmentation and the contribution of each part for classiÔ¨Åcation. To evaluate our model, we conduct extensive experi ments using three Ô¨Ånegrained recognition datasets for both interpretability and accuracy. To quantify interpretability, we compare the output region segments from our model to the annotated object parts. For accuracy, we report standard metrics for Ô¨Ånegrained classiÔ¨Åcation. On smaller scale datasets, such as CUB200 [36] and CelebA [56], our model is shown to Ô¨Ånd parts of the birds and faces with low local ization error, while at the same time compares favorably to stateoftheart methods in terms of accuracy. On the more challenging iNaturalist dataset [55], our model improves the accuracy of a strong baseline network (ResNet101) by 5.7% , reduces the object localization error, and demon strates promising qualitative results for part discovery. 2. Related Work "
225,Joint Ranking SVM and Binary Relevance with Robust Low-Rank Learning for Multi-Label Classification.txt,"Multi-label classification studies the task where each example belongs to
multiple labels simultaneously. As a representative method, Ranking Support
Vector Machine (Rank-SVM) aims to minimize the Ranking Loss and can also
mitigate the negative influence of the class-imbalance issue. However, due to
its stacking-style way for thresholding, it may suffer error accumulation and
thus reduces the final classification performance. Binary Relevance (BR) is
another typical method, which aims to minimize the Hamming Loss and only needs
one-step learning. Nevertheless, it might have the class-imbalance issue and
does not take into account label correlations. To address the above issues, we
propose a novel multi-label classification model, which joints Ranking support
vector machine and Binary Relevance with robust Low-rank learning (RBRL). RBRL
inherits the ranking loss minimization advantages of Rank-SVM, and thus
overcomes the disadvantages of BR suffering the class-imbalance issue and
ignoring the label correlations. Meanwhile, it utilizes the hamming loss
minimization and one-step learning advantages of BR, and thus tackles the
disadvantages of Rank-SVM including another thresholding learning step.
Besides, a low-rank constraint is utilized to further exploit high-order label
correlations under the assumption of low dimensional label space. Furthermore,
to achieve nonlinear multi-label classifiers, we derive the kernelization RBRL.
Two accelerated proximal gradient methods (APG) are used to solve the
optimization problems efficiently. Extensive comparative experiments with
several state-of-the-art methods illustrate a highly competitive or superior
performance of our method RBRL.","Traditional supervised singlelabel classication handles the task where each example is assigned to one class label. However, in many realworld classication applications, an example is often associated with a set of class labels. For instance, in text categorization, a document may belong to many labels such as \religion"" and \politics"". This brings the hot research interests of multilabel classication (MLC), which investigates the task where each example may be assigned to multiple class labels simultaneously. So far, MLC has witnessed its success in a wide range of research elds, such as function genomics [5, 9, 55], 2multimedia contents annotation [2, 29, 34], and NLP (e.g., text categorization [27, 31, 39], and information retrieval [52, 61, 16]). As a representative method for MLC, Ranking Support Vector Machine (RankSVM) [9] aims to minimize the empirical Ranking Loss while having a large margin and is enabled to cope with nonlinear cases with the kernel trick [25]. The classimbalance issue usually occurs in MLC, which mainly includes two aspects [53, 46]. On one hand, for a specic class label, the number of posi tive instances is greatly less than that of negative instances. On the other hand, for a specic instance, the number of relevant labels is usually less than that of irrelevant labels. Generally, the pairwise loss, which can be used to optimize imbalancespecic evaluation metrics such as the area under the ROC curve (AUC) and Fmeasure [6, 45], is more able to deal with the classimbalance issue than the pointwise loss. Thus, RankSVM can tackle the second aspect of the classimbalance issue in MLC by the minimization of the pairwise ap proximate ranking loss. Therefore, it can mitigate the negative in uence of the classimbalance issue in MLC. Nevertheless, apart from the rst ranking learn ing step, it needs another thresholding learning step, which is a stackingstyle way to set the thresholding function. Inevitably, each step has the estimation error. As a result, it may cause error accumulation and eventually reduce the nal classication performance for MLC. Therefore, it's better to nd a way to train the model in only one step. Although there are some methods proposed to tackle this issue, such as calibrated RankSVM [23] and RankSVMz [48], the basic idea of these methods is to introduce a virtual zero label for thresholding, 3which increases the number of the hypothesis parameter variables to raise the complexity of the model (i.e., the hypothesis set). Besides, while calibrated RankSVM [23] makes the optimization problem more computationally com plex, RankSVMz [48] makes it train more eciently. Moreover, there is little work to combine with Binary Relevance to address this issue. Binary Relevance (BR) [2] is another typical method, which transforms the MLC task into many independent binary classication problems. It aims to optimize the Hamming Loss and only needs onestep learning. Despite the intuitiveness of BR, it might have the classimbalance issue, especially when the label cardinality (i.e. the average number of labels per example) is low and the label space is large. Besides, it doesn't take into account label correlations, which plays an important role to boost the performance for MLC. Recently, to facilitate the performance of BR, many regularizationbased approaches [50, 51, 24, 47] impose a lowrank constraint on the parameter matrix to exploit the label correlations. However, little work has been done to consider the minimization of theRanking Loss to mitigate the negative in uence of the classimbalance issue and exploit the label correlations simultaneously. Moreover, these lowrank approaches are mostly linear models, which can't capture complex nonlinear relationships between the input and output. To address the above issues, in this paper we propose a novel multilabel classication model, which joints Ranking support vector machine and Binary Relevance with robust Lowrank learning (RBRL). Specically, we incorporate the thresholding step into the ranking learning step of RankSVM via Binary 4Relevance, which makes it train the model in only one step. It can also be viewed as an extension of BR, which aims to additionally consider the minimization of theRanking Loss to boost the performance. Hence, it can enjoy the advantages of RankSVM and BR, and tackle the disadvantages of both. Besides, the low rank constraint on the parameter matrix is employed to further exploit the label correlations. Moreover, to achieve nonlinear multilabel classier, we derive the kernelization of the linear RBRL. What's more, to solve the objective functions for the linear and kernel RBRL eciently, we use the accelerated proximal gradient methods (APG) with a fast convergence rate O(1=t2), wheretis the number of iterations. The contributions of this work are mainly summarized as follows: (1) We present a novel multilabel classication model, which joints RankSVM and BR with robust lowrank learning. (2) Dierent from existing lowrank approaches which are mostly linear mod els, we derive the kernelization RBRL to capture nonlinear relationships between the input and output. (3) For the linear and kernel RBRL, we use two accelerated proximal gradient methods (APG) to eciently solve the optimization problems with fast convergence. (4) Extensive experiments have conrmed the eectiveness of our approach RBRL over several stateoftheart methods for MLC. The rest of this paper is organized as follows. In Section 2, the related 5work about MLC is mainly reviewed. In Section 3, the problem of formulation and the model RBRL are presented in detail. The corresponding optimization algorithms are proposed in Section 4. In Section 5, experimental results are presented. Finally, Section 6 concludes this paper. 2. Related Work "
226,Ensemble Manifold Segmentation for Model Distillation and Semi-supervised Learning.txt,"Manifold theory has been the central concept of many learning methods.
However, learning modern CNNs with manifold structures has not raised due
attention, mainly because of the inconvenience of imposing manifold structures
onto the architecture of the CNNs. In this paper we present ManifoldNet, a
novel method to encourage learning of manifold-aware representations. Our
approach segments the input manifold into a set of fragments. By assigning the
corresponding segmentation id as a pseudo label to every sample, we convert the
problem of preserving the local manifold structure into a point-wise
classification task. Due to its unsupervised nature, the segmentation tends to
be noisy. We mitigate this by introducing ensemble manifold segmentation (EMS).
EMS accounts for the manifold structure by dividing the training data into an
ensemble of classification training sets that contain samples of local
proximity. CNNs are trained on these ensembles under a multi-task learning
framework to conform to the manifold. ManifoldNet can be trained with only the
pseudo labels or together with task-specific labels. We evaluate ManifoldNet on
two different tasks: network imitation (distillation) and semi-supervised
learning. Our experiments show that the manifold structures are effectively
utilized for both unsupervised and semi-supervised learning.","The frontiers of computer vision have been reshaped profoundly in the last few years due to the ever wider deployment of convolutional neural networks (CNNs). A cornucopia of CNNs have been developed and now deÔ¨Åne the state of the art for many vision tasks. On the other hand, manifold learning has been a central concept for many learning algorithms over decades, with applications to tasks as diverse as dimensionality reduction [32, 41], hashing [34], feature encoding [43], clustering [13, 37], semisupervised learning [2], model imitation [9], and visualization [26]. Thus, it stands to reason to devise methods to utilize manifold structures more effectively in training CNNs. c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:1804.02201v1  [cs.CV]  6 Apr 20182 D. DAI, W. LI, T. KROEGER, L. VAN GOOL: ENSEMBLE MANIFOLD SEGMENTATION Figure 1: ManifoldNet Pipeline. The bottom panel shows ensemble manifold segmentation (EMS) for the ensemble of pseudolabels. The top panel features the architecture of the network, which consists of two network streams shared parameters. The left stream is guided by a taskspeciÔ¨Åc loss function ( e.g. classiÔ¨Åcation loss for semisupervised classiÔ¨Åcation and L2 loss for regression) and is trained with samples whose target values are available. The right stream is guided by an ensemble of pseudo classiÔ¨Åcation tasks and trained with the unlabeled samples used for the clustering. The main challenge to train a CNN with manifold structures is to incorporate the latter‚Äôs structure onto the former‚Äôs architecture. Manifold structures are often expressed by a graph or an afÔ¨Ånity matrix of all data samples. This is inconvenient to use with many CNNs, as these are tailored for classiÔ¨Åcation tasks with crisp class labels. Systems do exist for learning CNNs on graphs [4, 10, 28], which accommodate afÔ¨Ånity graphs directly as training data. However, the scalability of this strand of method is limited, since afÔ¨Ånity graphs can potentially be large. In order to seamlessly couple manifold structures with the architecture of modern CNNs, we propose to segment data manifold. It is segmented so that samples that are close to each other fall into the same group or ‚Äòpseudoclass‚Äô. The corresponding pseudolabels are fed to the CNN to train it for classiÔ¨Åcation: grouping similar samples and separating dissimilar ones. This is in line with the aim of manifold learning. Yet, the labels obtained from the segmentation can be noisy due to its unsupervised nature. To mitigate this, we propose ensemble manifold segmentation (EMS) to create an ensemble of segmentations that are accurate individually and mutually diverse. EMS leads to an ensemble of pseudo classiÔ¨Åcation tasks, which results in an ensemble task architecture featuring an ensemble of loss functions. Figure 1 shows the architecture of our method. It consists of two copies of the same network, with shared weight parameters. The right stream is trained with unlabeled data and their pseudolabels as generated fromD. DAI, W. LI, T. KROEGER, L. VAN GOOL: ENSEMBLE MANIFOLD SEGMENTATION 3 ensemble clustering; the stream on the left is trained under ‚Äòreal‚Äô supervision, with training samples whose real target labels are available. The method is dubbed ManifoldNet. Mani foldNet can be trained with only the right stream or with the two streams jointly, depending on the nature of the tasks. For instance, for unsupervised learning tasks such as dimension ality reduction, hashing and unsupervised network imitation (distillation), one can use only the right stream. For tasks such as semisupervised classiÔ¨Åcation, the two streams are trained with different training sets, one labeled, one unlabeled. This Ô¨Çexibility greatly increases the applicability of the method. ManifoldNet translates manifold structures to crisp labels, which gives representational and training advantages with modern CNNs. Apart from being intuitive and easy to im plement, ManifoldNet has additional beneÔ¨Åts. Compared to manifold learning methods [9, 32, 41], it comes naturally with an outofsample ability: the trained CNNs can be used for many tasks, in the same way as standard CNNs can, e.g. as a feature extractor; Compared to deep embedding [42, 46, 48, 49], ManifoldNet is better scalable as it can be trained in a pointwise manner. ManifoldNet is a very general framework and can be easily applied to many different tasks. As Figure 1 shows, much of the work needed to apply it to a new task lies in adopting a taskspeciÔ¨Åc network architecture. The method is orthogonal to the meth ods [11, 12, 45] for selflearning feature representations. ManifoldNet is evaluated on two different tasks: network imitation and semisupervised classiÔ¨Åcation. Experiments show that it effectively utilizes manifold structures for both unsupervised and semisupervised learn ing. 2 Related Work "
227,Five lessons from building a deep neural network recommender.txt,"Recommendation algorithms are widely adopted in marketplaces to help users
find the items they are looking for. The sparsity of the items by user matrix
and the cold-start issue in marketplaces pose challenges for the off-the-shelf
matrix factorization based recommender systems. To understand user intent and
tailor recommendations to their needs, we use deep learning to explore various
heterogeneous data available in marketplaces. This paper summarizes five
lessons we learned from experimenting with state-of-the-art deep learning
recommenders at the leading Norwegian marketplace FINN.no. We design a hybrid
recommender system that takes the user-generated contents of a marketplace
(including text, images and meta attributes) and combines them with user
behavior data such as page views and messages to provide recommendations for
marketplace items. Among various tactics we experimented with, the following
five show the best impact: staged training instead of end-to-end training,
leveraging rich user behaviors beyond page views, using user behaviors as noisy
labels to train embeddings, using transfer learning to solve the unbalanced
data problem, and using attention mechanisms in the hybrid model. This system
is currently running with around 20% click-through-rate in production at
FINN.no and serves over one million visitors everyday.","The common business model of marketplaces is to provide an online platform where sellers post their items for sale and buyers search for items they want to purchase. The items can range from low value ones such as secondhand textbooks and clothes to highvalue ones such as cars and real estate properties. Sellers can also post nontangible items such as job opportunities. In the marketplace industry, a classified is a post from a seller selling one or multiple items, and we refer to it as an ""ad"" in this paper. An example is Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). KDD‚Äô18 Deep Learning Day, August 2018, London, UK ¬©2018 Copyright held by the owner/author(s). ACM ISBN 978xxxxxxxxxx/YY/MM. https://doi.org/10.1145/nnnnnnn.nnnnnnn Figure 1: An example of FINN.no marketplace. It shows the screen shot of a search result page on the left and an ad detail page on the right. shown in Figure 1 with a search result page on the left and an ad detail page on the right. Marketplaces can have multiple verticals, but there are often specialized marketplaces on house, car and job to adapt to the specific characteristics of those categories. In this paper, we use FINN.no (https://finn.no/), the leading online marketplace in Norway, to understand the usercontent interaction mechanism on marketplace and to experiment our recommender system with real users. It is a multivertical marketplace, but for the work described here, we focus only on the torget vertical that contains mostly secondhand daily items such as books, clothes, furnitures and pets. In essence, the FINN.no torget marketplace can be viewed as a spe cial type of ecommerce site that provides secondhand items across many different categories from a very large and scattered non professional seller community. Recommendation for marketplace ads is more challenging than the standard ecommerce product recommendation for the following reasons: 1) Many marketplace sellers are nonprofessional individuals, and the information they provide to describe the items for sale is often of lower quality (incomplete or inaccurate) and less standardized. Thus it is more challenging to solely rely on the contentbased features to iden tify similar items. 2) Location proximity is often neglected in the offtheshelf recommender solutions, but it actually plays an impor tant role in marketplaces, since the potential buyers often preferarXiv:1809.02131v2  [cs.IR]  7 Oct 2018KDD‚Äô18 Deep Learning Day, August 2018, London, UK S. Eide, A. M. √òygard, and N. Zhou to complete the transaction in person to check if the item fits their expectation. 3) Different from ecommerce sites where each product usually has abundant supply, marketplace items are often second hand and therefore unique in some sense. As a result, item volatility in marketplaces is often much higher than in the ecommerce sce nario. While in traditional recommendation scenarios the number of items is much smaller than the number of users, in marketplaces the number of ""active"" items has the same order of magnitude of the number of users. 4) FINN.no does not use the auction method, and most sellers reserve their items for the first buyer whom they reach a deal with. Therefore freshness is also a factor to take into account in recommendation solutions, so that potential buyers can find newly listed items sooner. This highlights the importance of solving the cold start problem for newly listed items. We experimented with a group of hybrid recommenders in pro duction at FINN.no to solve the challenges mentioned above. Among various things we tried, the following five have shown significant improvements in the A/B tests with users: rich user behavior sig nals in Section 4.1, user behavior trained embeddings in Section 4.2, transfer learning for images in Section 4.3, staged training strategy in Section 4.4, and attention mechanism in Section 4.5. 2 RELATED WORK "
228,Quantifying uncertainty for deep learning based forecasting and flow-reconstruction using neural architecture search ensembles.txt,"Classical problems in computational physics such as data-driven forecasting
and signal reconstruction from sparse sensors have recently seen an explosion
in deep neural network (DNN) based algorithmic approaches. However, most DNN
models do not provide uncertainty estimates, which are crucial for establishing
the trustworthiness of these techniques in downstream decision making tasks and
scenarios. In recent years, ensemble-based methods have achieved significant
success for the uncertainty quantification in DNNs on a number of benchmark
problems. However, their performance on real-world applications remains
under-explored. In this work, we present an automated approach to DNN discovery
and demonstrate how this may also be utilized for ensemble-based uncertainty
quantification. Specifically, we propose the use of a scalable neural and
hyperparameter architecture search for discovering an ensemble of DNN models
for complex dynamical systems. We highlight how the proposed method not only
discovers high-performing neural network ensembles for our tasks, but also
quantifies uncertainty seamlessly. This is achieved by using genetic algorithms
and Bayesian optimization for sampling the search space of neural network
architectures and hyperparameters. Subsequently, a model selection approach is
used to identify candidate models for an ensemble set construction. Afterwards,
a variance decomposition approach is used to estimate the uncertainty of the
predictions from the ensemble. We demonstrate the feasibility of this framework
for two tasks - forecasting from historical data and flow reconstruction from
sparse sensors for the sea-surface temperature. We demonstrate superior
performance from the ensemble in contrast with individual high-performing
models and other benchmarks.","1.1 Motivation Datadriven surrogate modeling research has shown great promise in improving the predictability and efÔ¨Åciency of computational physics applications. Among various algorithms, deep learningbased models have been observed to show signiÔ¨Åcant gains in accuracy and timetosolution over classical numericalmethods based techniques. However, the widespread adoption of deep learning models is still limited by its blackbox nature. To that end, uncertainty Email: rmaulik@anl.govarXiv:2302.09748v1  [cs.LG]  20 Feb 2023Neural architecture search ensembles for scientiÔ¨Åc machine learning A P REPRINT quantiÔ¨Åcation methods have been developed to overcome the challenges associated with the blackbox nature of the deeplearning models and establish trustworthiness by providing uncertainty estimates along with the predictions. The data or aleatoric uncertainty is attributed to the noise in the data, for example, low resolution sensors and sparse measurements; the model or epistemic uncertainty is attributed to the lack of training data. The former is inherent to the data and cannot be reduced by collecting more data; the latter is used to characterize the model‚Äôs predictive capability with respect to the training data and it can reduced by collecting appropriate data. In many computational physics applications, it is crucial to effectively quantify both data and modelform uncertainties in predictions from deep learning models. QuantiÔ¨Åcation of such uncertainties alleviates the risks associated with the deployment of such blackbox datadriven models for realworld tasks. Deep ensembles is a promising approach for uncertainty quantiÔ¨Åcation. In this approach, a ensemble of neural networks (NNs) are trained independently but they differ in the way in which they are trained. Consequently, the weights of the neural network parameters will be different. The prediction from these models are then used to improve prediction and estimate uncertainty. Despite its simplicity, deepensemblesbased uncertainty estimation has achieved superior performance over more sophisticated uncertainty quantiÔ¨Åcation methods on a number of benchmarks. Crucial to the effectiveness of uncertainty quantiÔ¨Åcation in the deepensemblebased methods is the diversity of the highperforming models. SpeciÔ¨Åcally, if the models are signiÔ¨Åcantly different from each other and also equally high performing, then the prediction and the uncertainty estimates become accurate [Egele et al., 2022]. However, this poses additional challenges due to the large overhead associated with the manual design of models. Despite one time cost, the development overhead signiÔ¨Åcantly increases the ofÔ¨Çine design and training costs of the ensemble model development and increase amortization time (i.e., the time required to offset ofÔ¨Çine costs). To that end, we explore an integrated automated deep ensemble approach that not only discovers highperforming models but also makes ensemble predictions with quantiÔ¨Åed uncertainty in a scalable manner. SpeciÔ¨Åcally, the highlights of this article are as follows: ‚Ä¢We demonstrate a uniÔ¨Åed strategy for discovering highperforming deep learning models for dynamical systems forecasting and signal recovery with epistemic and aleatoric uncertainty quantiÔ¨Åcation that leverages distributed computing. ‚Ä¢For uncertainty quantiÔ¨Åcation, we use the law of decomposition of variance from members of an ensemble to separately estimate both the aleatoric and epistemic uncertainty. ‚Ä¢We validate our proposed approach for forecasting Ô¨ÇowÔ¨Åelds as well as instantaneous state reconstructions for a realworld, highdimensional scientiÔ¨Åc machine learning problem with complex dynamics, given by the NOAA Optimum Interpolation Data Set. 1.2 Related work "
229,Latent Class-Conditional Noise Model.txt,"Learning with noisy labels has become imperative in the Big Data era, which
saves expensive human labors on accurate annotations. Previous
noise-transition-based methods have achieved theoretically-grounded performance
under the Class-Conditional Noise model (CCN). However, these approaches builds
upon an ideal but impractical anchor set available to pre-estimate the noise
transition. Even though subsequent works adapt the estimation as a neural
layer, the ill-posed stochastic learning of its parameters in back-propagation
easily falls into undesired local minimums. We solve this problem by
introducing a Latent Class-Conditional Noise model (LCCN) to parameterize the
noise transition under a Bayesian framework. By projecting the noise transition
into the Dirichlet space, the learning is constrained on a simplex
characterized by the complete dataset, instead of some ad-hoc parametric space
wrapped by the neural layer. We then deduce a dynamic label regression method
for LCCN, whose Gibbs sampler allows us efficiently infer the latent true
labels to train the classifier and to model the noise. Our approach safeguards
the stable update of the noise transition, which avoids previous arbitrarily
tuning from a mini-batch of samples. We further generalize LCCN to different
counterparts compatible with open-set noisy labels, semi-supervised learning as
well as cross-model training. A range of experiments demonstrate the advantages
of LCCN and its variants over the current state-of-the-art methods.","LARGE SCALE datasets with accurate labels have driven the success of deep neural networks (DNNs) in com puter vision [1], natural language processing [2], and speech recognition [3]. However, for many realworld applications, it is expensive to collect precisely annotated data in large volume. Instead, samples with noisy supervision, as an al ternative to alleviate the annotation burden, can be acquired inexhaustibly on social websites and have shown potential to many applications in the deep learning area [4‚Äì6]. However, it is challenging to train DNNs in the presence of noisy supervision due to the memorization effect [7]. To prevent the degeneration under this setting, several seminal works [8, 9] have been explored from the perspective of reg ularization to avoid overÔ¨Åtting or sample/label reÔ¨Ånement. For example, Arpit et al. [7] applied the dropout regulariza tion to decelerate noise memorization, and Reed et al. [10] explored the selfweighting mechanism via its predictions to weaken the noise effect. Different from the aforementioned methodologies, this study focuses on the third prosperous statistical perspective, learning via noise transition. This line of works place a transition matrix on top of the classiÔ¨Åer to disentangle the noise from the learning procedure. Previous works [11‚Äì13] presents a twostep solution: Ô¨Årst estimate the noise transition, and then freeze it to train the classiÔ¨Åer. However, it is usually impractical to have or approximately have an accurate anchor set [14] as the sufÔ¨Åcient statistics Jiangchao Yao, Zhihan Zhou and Ya Zhang are with Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai 200240, China. Jiangchao Yao and Ya Zhang are also with Shanghai AI Laboratory, Shanghai 200030, China. ymeans Ya Zhang is the corresponding author. Email:fSunarker, zhihanzhou, ya zhangg@sjtu.edu.cn Bo Han is with the Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Kowloon, Hong Kong, China. Email: bhanml@comp.hkbu.edu.hk Ivor W. Tsang is with the A*STAR Centre for Frontier AI Research, Singapore 138632. Email: ivor tsang@ihpc.astar.edu.sgof the noise transition. The alternative ways [15, 16] cast the noise transition as a neural layer and adapt it along with the training. Unfortunately, the current stochastic training as a neural layer to noise transition is illposed, yielding that the optimization depends on the empirically tuning to dodge the undesired local minimums. Actually, the reason of this phenomenon is the backpropagation applied in the neural layer could induce the arbitrary update to the noise transition by a minibatch of samples. The worst case is that some atypical minibatches containing the extreme noise could catastrophically destroy the well estimation regarding the noise transition parameters for the complete dataset. To solve this problem, we extend the conventional CCN into a latent counterpart, i.e.,Latent ClassConditional Noise model (LCCN). The intuition is that LCCN that param eterizes the noise transition under a Bayesian framework can explicitly characterize the dependency on the complete dataset instead of the minibatch of sample statistics. This fundamentally constrains the learning of the noise transition unlike the backpropagation applied in the neural layer, yielding a more stable estimation about the noise transition. Although it apparently builds a more sophisticated model, we show that the optimization for LCCN deduced by Gibbs sampling can be as efÔ¨Åcient as stochastic gradient descent. SpeciÔ¨Åcally, we propose a novel dynamic label regression method and illustrate its learning procedure in Fig. 1 (see the caption for more details). Note that, although our op timization method iteratively infers the latent true labels and applies them for the training of the classiÔ¨Åer and the noise modeling, only a small amount of extra computational cost is introduced. We theoretically provide the convergence analysis and the generalization bound about LCCN under the label noise, and prove that our optimization approach safeguards the update of noise transition by a minibatch. Beyond the aforementioned property of LCCN, we show that it is easy to extend LCCN under more broader set tings. Concretely, by taking the outofdistribution (OOD)arXiv:2302.09595v1  [cs.LG]  19 Feb 2023JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 imagesclassifier networkBayesian noise modelingsamplinglabels noisy labelsWolfCarMouseRose Safeguardedtransition updateLCCN Fig. 1. Dynamic label regression for LCCN. The images and noisy labels are respectively inputted to the classiÔ¨Åer and the safeguarded Bayesian noise modeling to compute the prediction and the conditional transition. Then, the latent labels are sampled based on their product, and then used for the training of the classiÔ¨Åer and the safeguarded Bayesian noise modeling. All components are trained endtoend in a stochastic fashion. samples into account [17], we increase the dimension of the latent label in LCCN to handle the openset label noise. With a slight modiÔ¨Åcation in the inference process of the latent true labels, the semisupervised learning paradigm is seamlessly integrated into the optimization. Considering the advantages of crossmodel training like coteaching and Dividemix [18, 19], we use two LCCNs to construct a divideLCCN model, which achieves the stateoftheart performance on a range of datasets. In summary, the con tribution of this paper can be categorized as follows. We propose a Latent ClassConditional Noise model to explicitly constrain the learning of the noise transi tion when jointly trained with deep neural networks. Unlike the backpropagation that arbitrarily updates the parameters of the transition layer, we introduce an efÔ¨Åcient dynamic label regression1to maintain the stable optimization in learning with noisy labels. We theoretically show the convergence property of LCCN by means of the dynamic label regression, and characterize the generalization bound of LCCN to uncover the factors for learning with label noise. Simultaneously, we prove that our optimization of the noise transition via a minibatch of samples is safely bounded compared to the backpropagation. We show how to extend the original LCCN without much effort to several variants, i.e., LCCNthat handles openset noisy labels when considering the OOD samples, and LCCN+ that is compatible with the semisupervised learning setting when some pre cisely annotated samples are available, and divideL CCN that leverages the advantage of the crossmodel training [18] to boost the performance. We conduct a range of experiments in the popular CIFAR 10, CIFAR100 datasets and large realworld noisy datasets, Clothing1M and WebVision17. Comprehensive results have demonstrated the superior performance of our model com pared with existing stateoftheart methods. 1. Note that, we use the word ‚Äúregression‚Äù to indicate the noisy label is progressively corrected to the groundtruth in the optimization.The rest part of this paper is organized as follows. Sec tion II brieÔ¨Çy reviews the related research of learning with noisy labels in deep learning. Then, we introduce our Latent ClassConditional model and the dynamic label regression method in Section III, where the corresponding theoretical analysis and the further extension of LCCN is also included. We validate the efÔ¨Åciency of our method over a range of experiments in Section IV . Section V concludes the paper. 2 R ELATED WORK "
230,Reinforcement Learning to Rank with Coarse-grained Labels.txt,"Ranking lies at the core of many Information Retrieval (IR) tasks. While
existing research on Learning to Rank (LTR) using Deep Neural Network (DNN) has
achieved great success, it is somewhat limited because of its dependence on
fine-grained labels. In practice, fine-grained labels are often expensive to
acquire, i.e. explicit relevance judgements, or suffer from biases, i.e. click
logs. Compared to fine-grained labels, coarse-grained labels are easier and
cheaper to collect. Some recent works propose utilizing only coarse-grained
labels for LTR tasks. A most representative line of work introduces
Reinforcement Learning (RL) algorithms. RL can help train the LTR model with
little reliance on fine-grained labels compared to Supervised Learning. To
study the effectiveness of the RL-based LTR algorithm on coarse-grained labels,
in this paper, we implement four different RL paradigms and conduct extensive
experiments on two well-established LTR datasets. The results on simulated
coarse-grained labeled dataset show that while using coarse-grained labels to
train an RL model for LTR tasks still can not outperform traditional approaches
using fine-grained labels, it still achieve somewhat promising results and is
potentially helpful for future research in LTR. Our code implementations will
be released after this work is accepted.","Ranking lies at the core of many Information Retrieval (IR) tasks including web search [ 13,35] and recommender system [ 49,50, 55,56]. Learning to rank (LTR) typically applies machine learning techniques for ranking [ 12,24,31,34,37,43,53‚Äì55,57]. One of the popular approaches is to use Supervised Learning [ 7,9,12] with documentlevel relevance annotation data [ 31] to optimize ranking metrics [ 20,42] such as normalized Discounted Cumulative Gain (nDCG), or Expected Reciprocal Rank (ERR). While this method has been proven effective, one of the main drawbacks is that in order to construct the loss function based on these metrics, the model requires finegrained labels, i.e. the explicit relevance judgements of each querydocument pair. On the one hand, such labels can often be expensive to attain (e.g. the high cost of human annotations) or suffer from different biases, e.g. trust bias and qualityofcontext bias for click logs [ 22,23]; on the other hand, myopically optimizing ranking metrics handcrafted from those finegrained labels may not always serve the ultimate goal of the ranking systems (e.g. user satisfaction and engagements) directly. Compared to finegrained labels, coarsegrained labels, such as query reformation, second search result page examination, user scroll patterns, are abundant and can be easily collected from search logs to generate largescale training data [ 15,22,23,29]. Some of recent works study the usage of RL algorithms in LTR task [31,37,43,46,47,60]. While the Supervised Learning approach requires finegrained labels to compute the evaluation metrics and construct the loss function, certain RLalgorithms, such as policy gradient, can directly use rewards from the environment to update the model [ 40]. As a result, we can leverage this feature and train the RLbased model with coarsegrained labels as rewards. Existing research in Reinforcement Learning to Rank can be generally categorized into two learning methods: stepwise learning and SERPlevel1learning. In stepwise learning, the ranking of documents is a sequence of actions in which the model selects the appropriate document for the position in the ranklist [ 43,47]. Thus, a ranklist containing Ndocuments results from Ndiscrete ranking time steps. While this approach utilizes the discreteness of classical RL (i.e., only picking one action per time step), it also requires finegrained documentlevel reward. Here, we pay more focus on situations where such reward is unavailable. SERPlevel learning, on the other hand, returns a ranklist corresponding to a query in each time step [ 31,37]. To be more specific, in contrast 1short for Search Engine Result PagearXiv:2208.07563v1  [cs.IR]  16 Aug 2022Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Zhichao Xu, Anh Tran, Tao Yang, and Qingyao Ai to the stepwise approach, it constructs a loss function based on SERPlevel reward collected from the ranklist constructed by the model. The ranking model can be trained with only coarsegrained SERPlevel rewards with this approach. Because of this nature, we believe this SERPlevel learning approach is better suited for our purpose of training a model on data that lacks finegrained labels. While existing works of RL in LTR mainly focus on improving the performance of the model, we are more interested in seeing whether training RL model without finegrained data can also deliver good performance in the LTR task. In this work, we implement four different RL models, namely, Policy Gradient Rank (PGRank), TopK Offpolicy Correction for Reinforcement Learning (REINFORCE), Deep Deterministic Pol icy Gradient (DDPG), Batch Constrained Deep QLearning (BCQ). Because of the lack of realworld search logs, we first simulate coarsegrained labels from explicit finegrained labels, then use them to train RL algorithms and report the performance. We com pare these results to the classical DNN algorithms trained with finegrained labels. We use two variants of DNN with different loss functions, i.e. CrossEntropy loss [ 2,6] and LambdaRank loss [ 6,42]. We conduct the experiments on two wellestablished public LTR datasets, Yahoo! LETOR [ 10] and MSLR10K [ 32]. Based on the result, we find that the RLbased algorithms are still less effective than the classical approach for ranking tasks. Nevertheless, with more research, it could act as a good alternative for scenarios where the finegrained labels are not available. The rest parts of this paper are organized as follows: we first discuss the related works in ¬ß2; then cover our methods (¬ß3) includ ing the problem formulation (¬ß3.1) and details of RL algorithms we adopt (¬ß3.2). We cover the experimental details in ¬ß4 and analyze the results in ¬ß5. Finally, we conclude this work in ¬ß6. 2 Related Work "
231,Novel Uncertainty Framework for Deep Learning Ensembles.txt,"Deep neural networks have become the default choice for many of the machine
learning tasks such as classification and regression. Dropout, a method
commonly used to improve the convergence of deep neural networks, generates an
ensemble of thinned networks with extensive weight sharing. Recent studies that
dropout can be viewed as an approximate variational inference in Gaussian
processes, and used as a practical tool to obtain uncertainty estimates of the
network. We propose a novel statistical mechanics based framework to dropout
and use this framework to propose a new generic algorithm that focuses on
estimates of the variance of the loss as measured by the ensemble of thinned
networks. Our approach can be applied to a wide range of deep neural network
architectures and machine learning tasks. In classification, this algorithm
allows the generation of a don't-know answer to be generated, which can
increase the reliability of the classifier. Empirically we demonstrate
state-of-the-art AUC results on publicly available benchmarks.","Deep learning (DL) algorithms have successfully solved realworld classiÔ¨Åcation problems from a variety of Ô¨Åelds, including recognizing handwritten digits and identifying the presence of key diagnostic features in medical images [ 18,16]. A typical classiÔ¨Åcation challenge for a DL algorithm consists of training the algorithm on an example data set, then using a separate set of test data to evaluate its performance. The aim is to provide answers that are as accurate as possible, as measured by the true positive rate (TPR) and the true negative rate (TNR). Many DL classiÔ¨Åers, particularly those using a softmax function in the very last layer, yield a continuous score, h; A step function is used to map this continuous score to each of the possible categories that are being classiÔ¨Åed. TPR and TNR scores are then generated for each separate variable that is being predicted by setting a threshold parameter that is applied when mapping hto the decision. Values above this threshold are mapped to positive predictions, while values below it are mapped to negative predictions. The ROC curve is then generated from these pairs of TPR/TPN scores. The performance of binary 1arXiv:1904.04917v1  [stat.ML]  9 Apr 2019classiÔ¨Åers, performance is often evaluated by calculating the area under the ROC curve (AUC) [ ?, hanley1982meaning] the higher the AUC, the better the performance of the algorithm. Many studies show that the AUC achieved by DL algorithms is higher than most, if not all, of the alternative classiÔ¨Åers. Although they achieve high scores for metrics such as AUC, DL algorithms are notorious for being ‚Äúblack box‚Äù models, as it is diÔ¨Écult to obtain insight into how the algorithm arrived at its conclusion. This makes them less reliable, particularly for applications where databased decisions are a Ô¨Årm requirement. One way to mitigate this problem when applying DL algorithms to these datadriven applications is to provide a measure of the classiÔ¨Åcation uncertainty, or the conÔ¨Ådence one has in the classiÔ¨Åcation prediction, along with the prediction of the outcome. This can be accomplished by training a series of similar models that diÔ¨Äer in their initialization parameters, and then calculating the variance between the probabilities for each possible outcome across the diÔ¨Äerently initialized networks. The ground truth of the uncertainty, then, is the variance between these similar models. Reporting a conÔ¨Ådence level along side the prediction helps the model‚Äôs end user to interpret and build trust in the model‚Äôs performance. For Example, this is a critical requirement for algorithms deployed in a medical setting [ 2], where a moderately positive prediction with high uncertainty has very diÔ¨Äerent prognosis and treatment implications than does the same prediction when the uncertainty is low. We show an example of both low variance and high variance predictions, calculated using the ground truth as deÔ¨Åned , in Figure 1a. When assessing uncertainty in DL algorithms such classiÔ¨Åers are frequently permitted to return a ‚Äúdon‚Äôt know‚Äù answer for very low conÔ¨Ådence predictions in the test data. This allows the algorithm to be judged only on the responses in the ‚Äúdo know‚Äù portion of the data set. Consequently, the algorithm generates overall higher quality predictions, while leaving humans to interpret samples for which it would generate poor quality predictions, similar to a triage system The continuous variable hthat is output by the last layer of a DL with a softmax function provides the likelihood of an outcome. However, this likelihood estimation should not be mistaken for a conÔ¨Ådence level in the prediction (see [6]). Instead, recent studies have combined the DL regularization technique of dropout with Bayesian modeling to derive uncertainty estimates in DL classiÔ¨Åers [6,7]. Bayesian approaches provides a natural framework for estimating the uncertainty of the prediction. Furthermore, dropout is a regularization technique that uses an ensemble of models to create highperformance classiÔ¨Åers [ 22,15]. The interpretation of dropout through a Bayesian lens enables derivation of uncertainty estimates in DL classiÔ¨Åers. Within this framework the classiÔ¨Åcation score,h, describes the probability of predicting the correct class. The ROC curve for the model is measured on an averaged value of h, rather than a single value, and the uncertainty is estimated based on the variance of h. This uncertainty measure has been shown to correlate with the performance of the classiÔ¨Åer. Using the averaged hto predict the outcome was suÔ¨Écient to increase classiÔ¨Åcation accuracy in some cases [11]. In this paper, we present a novel method for estimating the uncertainty of 2a DL classiÔ¨Åer. We propose a framework that assigns probability distributions to thinned networks,(i.e., neural networks with a subset of neurons removed), based on the performance of their crossentropy loss function across the test data set. The main contributions of this work are threefold. First, we introduce a statisticalmechanics framework that assigns probability distributions over the ensemble of thinned networks of the dropout. This framework has a Ô¨Çexible variable,, which represents the inverse temperature, and a statistical Ô¨Çuctuation scale. When set to zero, it results in a uniform distribution assumption over the thinned networks and the framework collapses to a Gaussian process. In contrast, a Ô¨Ånite inverse temperature results in a nonuniform distribution, and the framework enables interpretation and reasoning regarding uncertainty. Second, we present a new algorithm, called Loss Variance Monte Carlo Estimate (LoVME), which is based on estimations of the loss variance in the case of a Ô¨Ånite through Monte Carlo sampling. Finally, we illustrate the beneÔ¨Åts of deriving uncertainty through the LoVME algorithm in scenarios where the classiÔ¨Åer can yield a don‚Äôtknow answer. We use the MNIST [ 14] and CIFAR [ 13] data sets to show the performance of our algorithm, and compare our results to stateof theart algorithms for uncertainty in DL. The rest of the paper is organized as follows: First, we introduce related work. We focus on two related bodies of knowledge: recent proposed methods to derive uncertainty measures for test data using the Bayesian interpretation of dropout, and the existing statisticalmechanics frameworks for analyzing distributions over an ensemble of models. Next, we introduce a new methodology for interpreting the ensemble of Neural Networks (NNs) generated by dropout. We present a derivation of a new algorithm, LoVME, which uses Monte Carlo sampling to estimate the loss variance; through this loss variance, it provides estimates of the predicted variable hand the uncertainty of the prediction. We evaluate the performance of the algorithm on multiple data sets and compare it to stateoftheart algorithms. We conclude with a discussion of the advantages of the proposed framework and algorithm, as well as potential extensions and open questions. We conclude with a discussion of the advantages of the proposed framework and algorithm, as well as potential extensions and open questions. 2 Related Work "
232,Learning to segment from misaligned and partial labels.txt,"To extract information at scale, researchers increasingly apply semantic
segmentation techniques to remotely-sensed imagery. While fully-supervised
learning enables accurate pixel-wise segmentation, compiling the exhaustive
datasets required is often prohibitively expensive. As a result, many non-urban
settings lack the ground-truth needed for accurate segmentation. Existing open
source infrastructure data for these regions can be inexact and non-exhaustive.
Open source infrastructure annotations like OpenStreetMaps (OSM) are
representative of this issue: while OSM labels provide global insights to road
and building footprints, noisy and partial annotations limit the performance of
segmentation algorithms that learn from them. In this paper, we present a novel
and generalizable two-stage framework that enables improved pixel-wise image
segmentation given misaligned and missing annotations. First, we introduce the
Alignment Correction Network to rectify incorrectly registered open source
labels. Next, we demonstrate a segmentation model -- the Pointer Segmentation
Network -- that uses corrected labels to predict infrastructure footprints
despite missing annotations. We test sequential performance on the AIRS
dataset, achieving a mean intersection-over-union score of 0.79; more
importantly, model performance remains stable as we decrease the fraction of
annotations present. We demonstrate the transferability of our method to lower
quality data, by applying the Alignment Correction Network to OSM labels to
correct building footprints; we also demonstrate the accuracy of the Pointer
Segmentation Network in predicting cropland boundaries in California from
medium resolution data. Overall, our methodology is robust for multiple
applications with varied amounts of training data present, thus offering a
method to extract reliable information from noisy, partial data.","Processing remotelysensed imagery is a promising approach to evaluate ground conditions at scale for little cost. Algorithms that in take satellite imagery have accurately measured crop type [ 34],[21], cropped area [ 11], building coverage [ 41] [40], urbanization [ 1], and road networks [ 6] [42]. However, successful implementation of image segmentation algorithms for remote sensing applications depends on large amounts of data and highquality annotations. Wealthy, urbanized settings can more readily apply segmentation Figure 1: Types of label noise present in open source data. Building footprints are the class of interest. algorithms, due to either the presence of or the ability to collect significant amounts of carefully annotated data. In contrast, more rural regions often lack the means to exhaustively collect ground truth data. Some open source datasets exist for such settings, and by successfully coupling these annotations with remotely sensed imagery, researchers can gain insights into the status of infrastruc ture and development where wellcurated sources of these data do not exist. [20] [2]. Although these global open source ground truth datasets ‚Äì e.g. OpenStreetMaps (OSM) ‚Äì offer large amounts of labels for use at no cost, the annotations within suffer from multiple types of noise [28] [4]:missing or omitted annotations , defined as objects being present in the image and not existing in the label [ 28];misaligned annotations occur when annotations are translated and/or rotated from its true position [ 38]; and incorrect annotations ‚Äì annotations that do not directly correspond to the object of interest in the image. Figure 1 presents examples of these three types of label noise. Noisy datasets present a training challenge when using tradi tional segmentation algorithms, as the model cannot learn to as sociate image features and target labels when the relationship is obscured by noise. To address the issues of misaligned and omitted annotations, and in order to extract information from imperfect data, we present a simple and generalizable method for pixelwise image segmentation. First, we address annotation misalignment by proposing an Alignment Correction Network (ACN). With a small number of images and human verified ground truth annotations, the ACN learns to correct misaligned labels. Next, the corrected open source annotations are used to train the Pointer Segmenta tion Network (PSN), a model which takes in a point location and identifies the object containing that point. Learning associations from a representative point is a widely acknowledged method of object detection: [ 5] notes that an intuitive way for humans to refer to an object is through the action of pointing. By ‚Äò pointingout ‚Äô thearXiv:2005.13180v1  [cs.CV]  27 May 2020object instance of interest, our network ignores other instances that may not have corresponding annotations, therefore prevent ing performance degradation caused by annotationless instances within the image. As a result, our sequential approach presents a method for handling misaligned data as well as varying levels of label completeness without explicitly changing the loss func tion to compensate for noise. While our approach cannot replace large amounts of carefully annotated outlines, it can complement existing open source datasets and algorithms, reduce the cost of obtaining large amounts of full annotations, and allow researchers to extract information from imperfect datasets. This paper‚Äôs key contributions are as follows: ‚Ä¢We introduce the Alignment Correction Network (ACN), a means to verify and correct misaligned annotations using a small amount of human verified ground truth labeled data. ‚Ä¢We propose the Pointer Segmentation Network (PSN), a model that can reliably predict polygon boundaries on remotely sensed imagery despite omitted training annotations and without requiring any bespoke loss functions. ‚Ä¢We demonstrate the applicability of our methodology to three different segmentation problems: building footprint detection with a highlyaccurate dataset, building footprint detection with noisier training data, and cropland boundary prediction. Taken as a whole, our approach enables resource constrained actors to use large amounts of misaligned and partial labels ‚Äì coupled with a very small amount of human verified ground truth annotations ‚Äì to train image segmentation algorithms for a variety of tasks. The rest of the paper is organized as follows: In Related Work , we discuss related literature; in Methods , we describe our novel methodological contributions; in Results , we present results for the ACN and the PSN for all segmentation tasks; and in Conclusion , we restate our most salient findings. 2 RELATED WORK "
233,Encoding Event-Based Data With a Hybrid SNN Guided Variational Auto-encoder in Neuromorphic Hardware.txt,"Neuromorphic hardware equipped with learning capabilities can adapt to new,
real-time data. While models of Spiking Neural Networks (SNNs) can now be
trained using gradient descent to reach an accuracy comparable to equivalent
conventional neural networks, such learning often relies on external labels.
However, real-world data is unlabeled which can make supervised methods
inapplicable. To solve this problem, we propose a Hybrid Guided Variational
Autoencoder (VAE) which encodes event based data sensed by a Dynamic Vision
Sensor (DVS) into a latent space representation using an SNN. These
representations can be used as an embedding to measure data similarity and
predict labels in real-world data. We show that the Hybrid Guided-VAE achieves
87% classification accuracy on the DVSGesture dataset and it can encode the
sparse, noisy inputs into an interpretable latent space representation,
visualized through T-SNE plots. We also implement the encoder component of the
model on neuromorphic hardware and discuss the potential for our algorithm to
enable real-time learning from real-world event data.","Prior work has demonstrated how online supervised learning with labeled data can be used for tasks such as rapid, eventdriven learn ing from neuromorphic sensor data [ 26]. However, realworld data is unlabeled and, in the case of classification, can have classes that were not anticipated. Therefore, to leverage realworld data, labels must be generated by a supervisor in realtime without a priori knowledge of the number of classes. Additionally, while a trained classifier is trained to generate class labels, it cannot generalize to new classes. This is compounded by the fact that neural network classifiers trained using gradient de scent are usually overconfident of their classification, making the learning of new classes impractical. An alternative approach is to use the intermediate layers of a trained neural network classifier as pseudolabels or features for learning classes. In this work, we formalize this idea using an eventdriven guided Variational Auto Encoder (VAE) which is trained to generate an embedding that disentangles according to labels in a labeled dataset and that gener alizes to new data. The resulting embedding space can then either be used for (pseudo)labels for supervised learning or for measuring data similarity. We focus our demonstration of the guided VAE on a Dynamic Vision Sensor (DVS) gesture learning problem because of the availability of an eventbased gesture dataset and the high relevance of gesture recognition use cases [1]. Neuromorphic Dynamic Vision Sensors (DVS) inspired by the biological retina capture temporal, pixelwise intensity changes as a sparse stream of binary events [ 8]. This approach has key advantages over traditional RGB cameras, such as faster response times, better temporal resolution, and invariance to static image features like lighting and background. Thus, raw DVS sensor data intrinsically emphasizes the dynamic movements that comprise most natural gestures. However, effectively processing DVS event streams remains an open challenge. Events are asynchronous and spatially sparse, making it challenging to directly apply conven tional vision algorithms [8, 9]. Spiking Neural Networks (SNNs) can efficiently process and learn from eventbased data while taking advantage of temporalarXiv:2104.00165v2  [cs.NE]  8 Mar 2022NICE 2022, March 28April 1, 2022, Virtual Event, USA Kenneth Stewart, Andreea Danielescu, Timothy M Shea, and Emre O Neftci Figure 1: The Hybrid GuidedVAE architecture. Streams of gesture events recorded using a Dynamic Vision Sensor (DVS) are input into a Spiking Neural Network (SNN) that encodes the spatiotemporal features of the input data into a latent structure ùëß.ùëÉandùëÑare presynaptic traces and ùëàis the membrane potential of the spiking neuron. For clarity, only a single layer of the SNN is shown here and refractory statesùëÖare omitted. To help disentangle the latent space, a portion of the ùëßequal to the number of target features ùë¶‚àóis input into a classifier that trains each latent variable to en code these features (Exc. Loss). The remaining ùëß, noted\ùëö are input into a different classifier that adversarially trains the latent variables to not encode the target features so they encode for other features instead (Inh. Loss). The latent state ùëßis decoded back into ùë•‚àóusing the conventional deconvolu tional decoder layers. information [ 22]. SNN models emulate the properties of biological neurons and can be used for hierarchical feature extraction from the precise timing of events through eventbyevent processing [ 11]. Recent work demonstrated how SNNs can be trained endtoend using gradient backpropagation in time and standard autodifferen tiation tools, making the integration of SNNs possible as part of modern machine learning and deep learning methods [2, 25, 30]. Here, we take advantage of this capability by incorporating a convolutional SNN into a Variational Autoencoder (VAE) to encode spatiotemporal streams of events recorded by the DVS (Figure 1). The goal of the VAE is to embed the streams of DVS events into a latent space which facilitates the evaluation of event data similarity for semisupervised learning from realworld data. To best use the underlying hardware, we implement a hybrid VAE to process the DVS data, with an SNNbased encoder and a conventional (non spiking) convolutional network decoder. To ensure the latent space represents features which are perceptually salient and useful for recognition, we use a guided VAE to disentangle the features that account for variation in the underlying structure of the data. Our Hybrid GuidedVAE encodes and disentangles the variations of the structure of event data allowing for the clustering of similar patterns, such as similar looking gestures, and assigning of pseudo labels to novel samples. The key contributions of this work are:(1)Endtoend trainable eventbased SNNs for processing neu romorphic sensor data eventbyevent and embedding them in a latent space. (2)A Hybrid GuidedVAE that encodes eventbased camera data in a latent space representation of salient features for clus tering and pseudolabeling. (3)A proofofconcept implementation of the Hybrid Guided VAE on Intel‚Äôs Loihi Neuromorphic Research Processor. The ability to encode event data into a disentangled latent repre sentation is a key feature to enable learning from realworld data for tasks such as midair gesture recognition systems that are less rigid and more natural because they can adapt to each user. 2 RELATED WORK "
234,Iterative Learning with Open-set Noisy Labels.txt,"Large-scale datasets possessing clean label annotations are crucial for
training Convolutional Neural Networks (CNNs). However, labeling large-scale
data can be very costly and error-prone, and even high-quality datasets are
likely to contain noisy (incorrect) labels. Existing works usually employ a
closed-set assumption, whereby the samples associated with noisy labels possess
a true class contained within the set of known classes in the training data.
However, such an assumption is too restrictive for many applications, since
samples associated with noisy labels might in fact possess a true class that is
not present in the training data. We refer to this more complex scenario as the
\textbf{open-set noisy label} problem and show that it is nontrivial in order
to make accurate predictions. To address this problem, we propose a novel
iterative learning framework for training CNNs on datasets with open-set noisy
labels. Our approach detects noisy labels and learns deep discriminative
features in an iterative fashion. To benefit from the noisy label detection, we
design a Siamese network to encourage clean labels and noisy labels to be
dissimilar. A reweighting module is also applied to simultaneously emphasize
the learning from clean labels and reduce the effect caused by noisy labels.
Experiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets
demonstrate that our proposed model can robustly train CNNs in the presence of
a high proportion of open-set as well as closed-set noisy labels.","The success of Convolutional Neural Networks (CNNs) [20] is highly tied to the availability of largescale anno tated datasets, e.g., ImageNet [10]. However, largescale datasets with highquality label annotations are not always available for a new domain, due to the signiÔ¨Åcant time and effort it takes for human experts. There exist several cheap but imperfect surrogates for collecting labeled data, such as crowdsourcing from nonexperts or annotations from the web, especially for images ( e.g., extracting tags from the surrounding text or query keywords from search engines). These approaches provide the possibility to scale the acqui sition of training labels, but invariably result in the intro Figure 1. An illustration of closedset vs openset noisy labels. Figure 2. An overview of our framework that iteratively learns dis criminative representations on a ‚Äújasminecat‚Äù dataset with open set noisy labels. It not only learns a proper decision boundary (the black line separating jasmine and cat) but also pulls away noisy samples (green and purple) from clean samples (blue and red). duction of some noisy (incorrect) labels. Moreover, even highquality datasets are likely to have noisy labels, as data labeling can be subjective and errorprone. The presence of noisy labels for training samples may adversely affect rep resentation learning and deteriorate prediction performance [27]. Training accurate CNNs against noisy labels is there fore of great practical importance. We will refer to samples whose classes are misla beled/incorrectly annotated as noisy samples and denote their labels as noisy labels . Such noisy labels can fall into two types, closedset andopenset . More speciÔ¨Åcally, a closedset noisy label occurs when a noisy sample possesses a true class that is contained within the set of known classes in the training data. While, an openset noisy label occurs when a noisy sample possesses a true class that is not con tained within the set of known classes in the training data. The former scenario has been studied in previous work, but the latter one is a new direction we explore in this paper. 1arXiv:1804.00092v1  [cs.CV]  31 Mar 2018Table 1. Types of labels for a ‚Äújasminecat‚Äù dataset. labeled as ‚Äújasmine‚Äù labeled as ‚Äúcat‚Äù true ‚Äújasmine‚Äù clean closedset true ‚Äúcat‚Äù closedset clean other class images openset openset Figure 1 provides a pictorial illustration of noisy labels, where we have an image dataset with two classes, jasmine (the plant) and cat (the animal). The closedset noisy labels occur when cat and jasmine are mislabeled from one cate gory to the other, but the true labels of these images are still cat or jasmine. The openset noisy labels occur for those images labeled as cat or jasmine, but their true labels are neither cat nor jasmine, e.g., the zoo map and the cartoon character. Table 1 demonstrates all the possible cases on how different samples are labeled in this problem. The left most column speciÔ¨Åes the true class and the other columns specify the type of label in the dataset. Previous work has addressed the noisy label problem explicitly or implicitly in a closedset setting, via either loss correction or noise model based clean label inferring [22, 29, 37, 38]. However, these methods are vulnerable in the more generic openset scenario, as loss or label correc tion may be inaccurate since the true class may not exist in the dataset. Openset noisy labels are likely to occur for sce narios where data are harvested rapidly, or use approximate labels ( e.g., using a search engine query to retrieve images and then labeling the images according to the query key word that was used). To the best of our knowledge, how to address the openset noisy label problem is a new challenge. In this paper, we propose an iterative learning framework that can robustly train CNNs on datasets with openset noisy labels. Our model works iteratively with: (1) a noisy label detector to iteratively identify noisy labels; (2) a Siamese network for discriminative feature learning, which imposes a representation constraint via contrastive loss to pull away noisy samples from clean samples in the deep representa tion space; and (3) a reweighting module on the softmax loss to express a relative conÔ¨Ådence of clean and noisy la bels on the representation learning. A simpliÔ¨Åed illustration of the proposed framework is presented in Figure 2. Our main contributions can be summarized as follows: (1) We identify the openset noisy label problem as a new challenge for representation learning and prediction. (2) We propose an iterative learning framework to ro bustly train CNNs in the presence of openset noisy labels. Our model is not dependent on any assumption of noise. (3) We empirically demonstrate that our model sig niÔ¨Åcantly outperforms stateoftheart noisy label learning models for the openset setting, and has a comparable or even better performance under the closedset setting. 2. Related work "
235,Plug-and-Play Pseudo Label Correction Network for Unsupervised Person Re-identification.txt,"Clustering-based methods, which alternate between the generation of pseudo
labels and the optimization of the feature extraction network, play a dominant
role in both unsupervised learning (USL) and unsupervised domain adaptive (UDA)
person re-identification (Re-ID). To alleviate the adverse effect of noisy
pseudo labels, the existing methods either abandon unreliable labels or refine
the pseudo labels via mutual learning or label propagation. However, a great
many erroneous labels are still accumulated because these methods mostly adopt
traditional unsupervised clustering algorithms which rely on certain
assumptions on data distribution and fail to capture the distribution of
complex real-world data. In this paper, we propose the plug-and-play
graph-based pseudo label correction network (GLC) to refine the pseudo labels
in the manner of supervised clustering. GLC is trained to perceive the varying
data distribution at each epoch of the self-training with the supervision of
initial pseudo labels generated by any clustering method. It can learn to
rectify the initial noisy labels by means of the relationship constraints
between samples on the k Nearest Neighbor (kNN) graph and early-stop training
strategy. Specifically, GLC learns to aggregate node features from neighbors
and predict whether the nodes should be linked on the graph. Besides, GLC is
optimized with 'early stop' before the noisy labels are severely memorized to
prevent overfitting to noisy pseudo labels. Consequently, GLC improves the
quality of pseudo labels though the supervision signals contain some noise,
leading to better Re-ID performance. Extensive experiments in USL and UDA
person Re-ID on Market-1501 and MSMT17 show that our method is widely
compatible with various clustering-based methods and promotes the
state-of-the-art performance consistently.","Person reidentification (ReID), which aims to associate person images captured by disjoint cameras, is of great practical value. Recently, unsupervised learning2 T. Yan et al. Clustering ùí¥ùë°+1 ‡∑®ùí¥ùë°+1Conventional Clustering based Methods Our Method Clustering ùêπùë° Feature  Extraction Network GLC ùëÆùú∂ Feature  Extraction Network ùêπùë°ùí¥ùë°+1 (a) 100 200 300 400 500 Iterations:0.700.720.740.760.780.80NMI Scores Initial Pseudo Labels Corrected Pseudo Labels (b) Fig. 1: (a) Conventional clusteringbased methods alternate between the genera tion of pseudo labels yt+1via clustering the features Ft, and the optimization of network in ( t+1)th epoch. This selftraining manner can produce a great many label noises, substantially hindering the training of EŒ∏. We propose GLC GŒ±as a postprocessing module to refine the initial pseudo labels after each clustering, improving the performance of EŒ∏. Note that GLC is not required during the testing phase. (b) NMI Scores of pseudo labels at different iterations in the GLC training. We early stop the training of GLC (The dotted line), which prevents the overfitting to the label noise, improving the initial noisy pseudo labels. (USL) person ReID [5, 2] and unsupervised domain adaptive (UDA) person ReID [4, 18, 28], which relax the requirement on labeled training data, have received a lot of research interests. Nowadays, clusteringbased approaches [4, 8, 2, 28, 4, 31], which alternate between the generation of pseudo labels and the optimization of feature extraction network as shown in fig. 1 (a), dominate the community of USL and UDA person ReID. Although the accuracy of pseudo labels are gradually improved, there are inevitable pseudo label noises, which will be accumulated during training, leading to degraded the reID performance. Recently, a series of researches have been presented to reduce the adverse impact of noisy pseudo labels, and can be roughly divided into two categories, reliable pseudo label selection methods [5, 18, 27, 23] and pseudo label refinement methods [4, 27]. The former utilize the estimated reliability of pseudo labels to select credible samples or downweight unreliable ones to train the feature ex tractor. The latter achieve label refinement by adopting mutual learning scheme [4], optimal transport algorithm [29] or label propagation [31, 21]. The existing methods mostly adopt the traditional unsupervised clustering algorithms, such as Kmeans [14] and DBSCAN [6], to generate pseudo labels. These algorithms all rely on certain assumptions on data distribution ( e.g., convex shape, similar size and same density of clusters), failing to capture the distribution of complex realworld data thus generating many erroneous pseudo labels. This motivates us to tackle the noisy pseudo label issue by applying the supervised clustering framework, which can perceive the data distribution from the training samples, and generate better pseudo labels. In this paper, we propose a plugandplay graphbased pseudo label correc tion network (GLC) to improve the accuracy of pseudo labels for USL and UDAAbbreviated paper title 3 person reID. GLC is trained to capture the data distribution at each epoch with the supervision of pseudo labels generated by any clustering methods. Al though the supervision signals contain some noise, GLC can learn to correct the initial noisy pseudo labels by means of the relationship constraint between samples on the graph and the earlystop training strategy. GLC is an addon component to any clusteringbased methods which adopt an iterative twostage training scheme. As shown in fig.1(a), GLC acts as a postprocessing module in a plugandplay way, which is optimized separately from the feature extrac tion network to rectify current pseudo labels after each clustering. Thus GLC is widely compatible with the existing pseudo label refinement approaches, and adding it can further reduce the remaining pseudo label noise. Specifically, we first construct a kNN graph with each node denoting a per son image and being connected to its k nearest neighbours. Then we formulate the image clustering task as a problem of the link prediction on the graph, by applying several Graph Convolutional Network layers to aggregate node features and a classification layer to predict whether two nodes should be linked under the supervision of pseudo labels. After the GLC training, each node feature is refined by the similarity to its neighbors and the supervisory information, pos sessing more robustness to the initial noise pseudo labels. Besides, considering that deep neural networks first fit the training data with clean labels3before eventually memorizing the examples with false labels when trained on noisy labels [13], we adopt an earlystop strategy during the GLC training to avoid overfitting to the noisy pseudo labels as illustrated in fig.1(b). Finally, in the GLC inference, we conduct link prediction on the whole graph, and the links with low confidence are cut off. Dynamic clusters are generated to fit the cur rent data distribution, and there are more chances for those false positive image links to be cut off and those false negative image links and outliers to be linked. To further unleash the potential of GLC, on the one hand, we jointly utilize sample features and ID classification scores to measure the node similarity and link more positive image pairs together during the kNN graph construction, considering that classification scores are more robust to the data distribution gap caused by factors like camera variations than raw features [22]. On the other hand, we reinitialize the parameters of GLC at each epoch to reduce the accumulated errors inherited by the network parameters, and we also retrain the feature extractor from scratch after some epochs to start another selftraining process with the corrected pseudo labels, to get rid of the adverse impact of previous noisy pseudo labels. Our main contributions are summarized as follows. (1) We are the first to adopt a neuralnetworkbased supervised clustering framework for USL/UDA person reID, which perceives the data distribution from training samples and adapts better to the dynamic features at each epoch. (2) We propose the plug andplay GLC to learn to refine the initial noisy pseudo labels with the re lationship constrains between samples on the graph and the early stop train 3The clusteringbased methods have a basic assumption that most samples from the same person are given the same pseudo label, and in our practice, GLC first fit them.4 T. Yan et al. ing strategy. (3) Extensive experiments in unsupervised and UDA person ReID on Market1501 and MSMT17 demonstrate the wide compatibility and consis tent performance promotion of our proposed method to various stateoftheart clusteringbased methods. 2 Related Work "
236,An Effective Label Noise Model for DNN Text Classification.txt,"Because large, human-annotated datasets suffer from labeling errors, it is
crucial to be able to train deep neural networks in the presence of label
noise. While training image classification models with label noise have
received much attention, training text classification models have not. In this
paper, we propose an approach to training deep networks that is robust to label
noise. This approach introduces a non-linear processing layer (noise model)
that models the statistics of the label noise into a convolutional neural
network (CNN) architecture. The noise model and the CNN weights are learned
jointly from noisy training data, which prevents the model from overfitting to
erroneous labels. Through extensive experiments on several text classification
datasets, we show that this approach enables the CNN to learn better sentence
representations and is robust even to extreme label noise. We find that proper
initialization and regularization of this noise model is critical. Further, by
contrast to results focusing on large batch sizes for mitigating label noise
for image classification, we find that altering the batch size does not have
much effect on classification performance.","Deep Neural Networks (DNNs) have led to sig niÔ¨Åcant advances in the Ô¨Åelds of computer vi sion (He et al., 2016), speech processing (Graves et al., 2013) and natural language processing (Kim, 2014; Young et al., 2018; Devlin et al., 2018). To be effective, supervised DNNs rely on large amounts of carefully labeled training data. However, it is not always realistic to assume that example labels are clean. Humans make mistakes and, depending on the complexity of the task, there may be disagreement even among expert la belers. To support noisy labels in data, we neednew training methods that can be used to train DNNs directly from the corrupted labels to sig niÔ¨Åcantly reduce human labeling efforts. Zhu and Wu (2004) perform an extensive study on the ef fect of label noise on classiÔ¨Åcation performance of a classiÔ¨Åer and Ô¨Ånd that noise in input features is less important than noise in training labels. In this work, we add a noise model layer on top of our target model to account for label noise in the training set, following (Jindal et al., 2016; Sukhbaatar et al., 2014). We provide extensive experiments on several text classiÔ¨Åcation datasets with artiÔ¨Åcially injected label noise. We study the effect of two different types of label noise; Uni form label Ô¨Çipping (Uni) , where a clean label is swapped with another label sampled uniformly at random; and Random label Ô¨Çipping (Rand) where a clean label is swapped with another label from the given number of labels sampled randomly over a unit simplex. We also study the effect of different initializa tion, regularization, and batch sizes when training with noisy labels. We observe that proper initial ization and regularization helps the noise model learn to be robust to even extreme amounts of noise. Finally, we use lowdimensional projec tions of the features of the training examples to understand the effectiveness of the noise model. The rest of the paper is organized as follows. Section 2 discusses the various approaches in lit erature to handle label noise. In Section 3, we describe the problem statement along with the proposed approach. We describe the experimen tal setup and datasets in Section 4. We empiri cally evaluate the performance of the proposed ap proach along with the discussion in Section 5 and Ô¨Ånally conclude our work in Section 6.arXiv:1903.07507v1  [cs.LG]  18 Mar 20192 Related Work "
237,RetiNet: Automatic AMD identification in OCT volumetric data.txt,"Optical Coherence Tomography (OCT) provides a unique ability to image the eye
retina in 3D at micrometer resolution and gives ophthalmologist the ability to
visualize retinal diseases such as Age-Related Macular Degeneration (AMD).
While visual inspection of OCT volumes remains the main method for AMD
identification, doing so is time consuming as each cross-section within the
volume must be inspected individually by the clinician. In much the same way,
acquiring ground truth information for each cross-section is expensive and time
consuming. This fact heavily limits the ability to acquire large amounts of
ground truth, which subsequently impacts the performance of learning-based
methods geared at automatic pathology identification. To avoid this burden, we
propose a novel strategy for automatic analysis of OCT volumes where only
volume labels are needed. That is, we train a classifier in a semi-supervised
manner to conduct this task. Our approach uses a novel Convolutional Neural
Network (CNN) architecture, that only needs volume-level labels to be trained
to automatically asses whether an OCT volume is healthy or contains AMD. Our
architecture involves first learning a cross-section pathology classifier using
pseudo-labels that could be corrupted and then leverage these towards a more
accurate volume-level classification. We then show that our approach provides
excellent performances on a publicly available dataset and outperforms a number
of existing automatic techniques.","By and large, Optical Coherence Tomography (OCT) has reshaped the Ô¨Åeld of ophthalmology ever since its inception in the early 90s [1]. At its core, OCT uses infraredlight interferometry to image through tissue in order to characterize anatomical structures beyond their surface. Given its simplic ity, affordability and safety, it is no surprise that its use has gained widespread popularity for both disease diagnosis and treatment. Similarly, its use has gained traction in other medical Ô¨Åelds such as for histopathology and skin cancer analysis [2]. Indeed, with an ability to image the posterior part of the eye in 3D ( e.g. the retina) at micrometer resolution, OCT imaging now allows for visualization of most retinal layers [3, 4] and more impor tantly, numerous pathological markers, such as intraretinal Ô¨Çuid, drusens or cysts [5, 6]. As illus trated in Fig. 1, such markers can be observed in OCT crosssectional images, or Bscans and have S. Apostolopoulos and R. Sznitman are with the ARTORG Center, University of Bern, Switzerland. E mail: Ô¨Årstname.lastname@artorg.unibe.ch yC. Ciller is with the Radiology Department, CIBM, Lausanne University and University Hospital, Lau sanne and with the Ophthalmic Technology Group, ARTORG Center Univ. of Bern, Switzerland zS. De Zanet is with the Ecole Polytechnique Federale de Lausanne, Switzerland. xS. Wolf is with the Bern University Hospital, Inselspital, Switzerland. 1arXiv:1610.03628v1  [cs.CV]  12 Oct 2016Figure 1: An example of a Bscan crosssection of a patient with AMD in the foveal pit area. Visible are the multiple retinal layers, including the Retinal Pigment Epithelium (RPE) and Bruch¬¥s Membrane (BM). The latter is perturbed with drusen , which manifest as bumps disrupting this continuous layer. been linked to a number of eye conditions, including AgeRelated Macular Degeneration (AMD) and Diabetic Retinopathy (DR) which currently affect over 8.7% of the world population and 159 million people worldwide, respectively [5, 7, 8]. Moreover, these pathologies are the major cause of blindness in developed countries [9]. Alarmingly, the number of people with either of these dis eases is projected to skyrocket, with AMD affecting an estimated 196 million people by 2020 and 288 million people by 2040 [8]. Genetic factors, race, smoking habits and the ever growing world population are responsible for this pathology growth [10]. While OCT has gained signiÔ¨Åcant importance in recent years for AMD and DR screening [11, 12], the process to do so remains time consuming however. In effect, 3D OCT volumes, also referred to as Cscans , are comprised of 50100 crosssectional Bscans. Traditionally, inspection of each Bscan is necessary in order to properly ruleout most retinal diseases. This process is particularly tedious not only due to its timeconsuming nature, but also due to the multiple crosssections that need to be inspected simultaneously to identify elusive and scarce traces of earlystage ocular diseases. In this context, automated algorithms for pathology identiÔ¨Åcation in OCT volumes would be of great beneÔ¨Åt for clinicians and ophthalmologists, as access to OCT devices becomes common and nationwide screening programs commence [13]. Recently, research has given way to a variety of image processing methods for OCT imaging. Some of these have included: techniques for image denoising [14, 15, 16], strategies for improved image reconstruction [17, 18, 19, 20, 21, 22], dosimetry laser control systems [23, 24, 25] or instrument detection during surgical procedures [26, 27]. More speciÔ¨Åc to pathology identiÔ¨Åcation, various groups have explored automatic detection of retinal pathologies using machine learning techniques, either focusing on segmentation of rel evant pathological markers [28, 29, 30, 31, 32] or classiÔ¨Åcation of 2D Bscans or 3D C scans [33, 34, 32, 35, 36]. While effective to some extent, most of these works have leveraged Bscan level groundtruth information in order to learn classiÔ¨Åcation functions. These more detailed labels are unfortunately often not available and as such, limit the usability of these solutions. To this end, we present a new strategy towards automatic pathology identiÔ¨Åcation in OCT Cscans using only volume level annotations. To do this, we introduce a novel Convolution Neural Network (CNN) architecture, named RetiNet , that directly estimates the state of a Cscan solely using the image data and without needing additional information. At its core, our approach uses (1) a task speciÔ¨Åc volume preprocessing strategy where we Ô¨Çatten and normalize the data in an OCTspeciÔ¨Åc manner, (2) we then train a 2D Bscan CNN using pseudolabels that could be corrupted in order to prelearn Ô¨Ålters that respond to relevant image features and (3) reuse the learned features in a Cscan level CNN that takes a mosaic of Bscans as input and classiÔ¨Åes the entire Cscan at once. Using a publicly available OCT dataset [5], we show that our approach is highly effective at separating AMD 2from control subjects and outperforms existing stateoftheart methods for image classiÔ¨Åcation. In addition, we not only show that RetiNet outperforms excellent recent networks from the computer vision literature trained from scratch, but also surpasses the performance of stateoftheart pre trained networks with adapted Ô¨Ålters. Last, we show how our approach provides high performances in terms accuracy, learning pathologyspeciÔ¨Åc Ô¨Ålters capable to identifying pathological markers effectively. The remainder of this article is organized as follows: The following section discusses the relevant related work. Sec. 3 then describes in detail our approach and the RetiNet architecture. Following this, we describe our experimental section and the evaluation of several baseline strategies in Sec. 4. We then conclude with Ô¨Ånal remarks in Sec. 5. 2 Related Work "
238,Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification.txt,"In this paper, we first introduce batch normalization to the edRVFL network.
This re-normalization method can help the network avoid divergence of the
hidden features. Then we propose novel variants of Ensemble Deep Random Vector
Functional Link (edRVFL). Weighted edRVFL (WedRVFL) uses weighting methods to
give training samples different weights in different layers according to how
the samples were classified confidently in the previous layer thereby
increasing the ensemble's diversity and accuracy. Furthermore, a pruning-based
edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based
on their importance for classification before generating the next hidden layer.
Through this method, we ensure that the randomly generated inferior features
will not propagate to deeper layers. Subsequently, the combination of weighting
and pruning, called Weighting and Pruning based Ensemble Deep Random Vector
Functional Link Network (WPedRVFL), is proposed. We compare their performances
with other state-of-the-art deep feedforward neural networks (FNNs) on 24
tabular UCI classification datasets. The experimental results illustrate the
superior performance of our proposed methods.","Deep learning has been extremely successful in recent years. Ranging from vision and video tasks to natural language processing, these deep neural networks have reached stateoftheart Email addresses: qiushi001@e.ntu.edu.sg (Qiushi Shi), epnsugan@ntu.edu.sg (Ponnuthurai Nagaratnam Suganthan), rakeshku001@e.ntu.edu.sg (Rakesh Katuwal) Preprint submitted to arXiv January 24, 2022arXiv:2201.05809v2  [cs.LG]  21 Jan 2022results in multiple domains [1, 2]. In conventional neural networks, backpropagation methods are used to train a large number of parameters in these models [3]. Although such a training method makes it possible to optimize the parameters, the timeconsuming training process has become a severe problem in recently designed complex neural networks. Also, a BPtrained neural network may fall into a local minimum and gives a suboptimal result [4, 5, 6]. By looking at the Kaggle competitions that have no relation with vision or sequence, we can easily nd that deep learning is not always the best solution for diverse tasks [7, 8]. At the same time, another kind of neural network based on randomization is attracting signif icant attention because of its superiority to overcome the shortcomings of the conventional models [9, 5, 10]. It has been successfully applied to a range of tasks from classication [11, 12, 13], regres sion [14, 15], visual tracking [16], to forecasting [17, 18]. Instead of using backpropagation to train, this randomizationbased neural network frequently uses a closedform solution to optimize param eters in the model [19]. Unlike the BPtrained neural networks which need multiple iterations, the randomizationbased neural networks only need to be trained once by feeding all the samples to the model together. Among these models, Random Vector Functional Link Network (RVFL) [20] is a typical representative with a single hidden layer. Its universal approximation ability has been proved in [21]. The weights and biases are randomly generated in this neural network. And its uniqueness lies in a direct link that connects the information from the input layer to the output layer. However, due to dierent random seeds and perturbations in the training set, this randomized neural network can perform quite dierently in each realization [22]. To increase the performance, stability, and robustness of this model, two improved structures named Deep Random Vector Functional Link Network (dRVFL) and Ensemble Deep Random Vector Functional Link Network (edRVFL) were proposed [23]. The dRVFL network is a deep version of RVFL network, which allows the existence of multiple hidden layers, while edRVFL network treats each hidden layer as a classier to compose an ensemble. However, with the edRVFL network goes deeper, the divergence of the randomized hidden features will become a serious problem. Therefore, using normalization methods to renormalize the hidden features is extremely important for improving the performance of the edRVFL network. In this paper, we employ the batch normalization scheme [24] to do the renormalization work. To the best of our knowledge, this is the rst time that batch normalization is introduced to the randomized neural network. After the renormalization process, the mean and the variance of the 2hidden features will become 0 and 1. Then, we scale and shift these values to increase the expression capacity of the neural network. Besides, there are still some drawbacks to the edRVFL network. Firstly, for every layer (or classier) in the edRVFL network, they share the same training samples. Meanwhile, these training samples have the same weights in the training process. Compared to ensemble methods that using diering training bags for each classier, these ensemble frameworks which utilize similar training sets usually perform worse [25, 26]. Moreover, the testing accuracy for the last few layers may slightly go down when the network becomes deeper. We believe that some inferior features can be generated since we randomly generate the weights for the hidden neurons. And these useless features will propagate to deeper layers inducing further inferior features to decrease the overall testing accuracy. Thus, for solving the rst problem, we introduce a weighting matrix. Each training sample will be allocated a particular weight when performing the closedform solution depending on its performance in the previous layer. Our approach diers from Weighted Extreme Learning Machine [27] which gives weights to each sample for addressing the problem of imbalance learning. The main purpose is to ensure that dierent classiers can have their preference for a particular portion of the training samples that were not classied with high condence in the previous layer. We have also tried to apply the sample weighting method of Adaboost [28]. However, most of the samples will be given weights near zero while only a few can be allocated reasonable weights. Therefore, we propose four dierent weighting methods in this paper, and this improved variant of edRVFL network is named Weighted Ensemble Deep Random Vector Functional Link Network (WedRVFL). Besides, pruning algorithms are widely used to reduce the heavy computational cost of deep neural networks in lowresource settings [29]. Dierent eective techniques have been proposed to cut o the redundant part of the neural network models [30, 31, 32, 33]. In our case, we perform it by selecting some inferior features in the hidden layer and prune them permanently. The selection process can help to prevent the propagation of inferior features and maintain the testing accuracy for deeper layers. We named this improved variant of edRVFL network as Pruningbased Ensemble Deep Random Vector Functional Link Network (PedRVFL). Although there was previous work that applying pruning strategy to the RVFL network in [34], we would like to highlight that our work is dierent from theirs at the following point: They do pruning after training to shrink the size of the neural network. However, we perform pruning during the generation step so that inferior features 3will not propagate to deeper layers. Additionally, we integrate the advantages of WedRVFL and PedRVFL to create a combined model called Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network (WPedRVFL). The key contributions of this paper are summarized as follows: ‚Ä¢We introduce the batch normalization to the edRVFL network for renormalizing the hidden features. ‚Ä¢We employ the weighting scheme to allocate dierent weights to dierent samples in the edRVFL network. We name it WedRVFL network. The weight matrix changes according to the samples' predictions in the previous layers. This method can make sure that each hidden layer in the network has dierent biases for each sample and increase the ensemble classication accuracy. ‚Ä¢We propose pruning based edRVFL network called PedRVFL network. Instead of pruning neurons after the training process, we cut o the inferior neurons according to their importance for classication when we are training the model. This method can prevent the propagation of detrimental features and increase the classication accuracy in deeper layers. ‚Ä¢The combination of weighting and pruning based edRVFL network named WPedRVFL net work is also presented in the paper. ‚Ä¢The empirical results show the superiority of our new methods over 11 stateoftheart methods on 24 UCI benchmark datasets. The rest of the paper is organized as follows: Section 2 outlines the basic concepts of RVFL network and illustrates the ensemble deep version of this structure. Section 3 introduces the re normalization method for the edRVFL network. Then Section 4 gives details about our new pro posed versions of edRVFL network. In Section 5, the performance of our methods, as well as other deep feedforward neural networks (FNNs) and RVFL variants are compared. Finally, conclusions and future research directions are presented in Section 6. 2. Related works "
239,Deep Learning with Label Noise: A Hierarchical Approach.txt,"Deep neural networks are susceptible to label noise. Existing methods to
improve robustness, such as meta-learning and regularization, usually require
significant change to the network architecture or careful tuning of the
optimization procedure. In this work, we propose a simple hierarchical approach
that incorporates a label hierarchy when training the deep learning models. Our
approach requires no change of the network architecture or the optimization
procedure. We investigate our hierarchical network through a wide range of
simulated and real datasets and various label noise types. Our hierarchical
approach improves upon regular deep neural networks in learning with label
noise. Combining our hierarchical approach with pre-trained models achieves
state-of-the-art performance in real-world noisy datasets.","The robustness of deep learning has been studied from different aspects. One of the topics focuses on investigating whether deep neural networks can learn from noisy labels as it can be difÔ¨Åcult to collect data with clean annotations in many real applications [1], [2]. Although deep learning models enjoy certain generalizability for different tasks, they can be very sensitive to label noise as they tend to memorize noise during training due to their expressivity [3], [4]. In addition, there exist different types of label noise [5] and each of them may have its unique effects on the model performance. Deep learning models designed to mitigate label noise can be broadly categorized into two groups: modelbased and modelfree [6]. Modelbased methods depend on ex plicit assumptions about the distribution and the behavior of the noise. Popular techniques in modelbased settings in clude noisy channel [7], [8], [9], data pruning [10], [11], [12] and sample selection [13], [14], [15]. Modelfree methods, on the other hand, aim to improve robustness without ex plicitly modeling the label noise structure. Common model free methods include robust losses such as nonconvex loss [16], [17], [18], generalized crossentropy loss [19], meta learning [20], [21], [22], and regularization [23], [24], [25] among others [26], [27], [28]. We focus on the modelfree setting in this paper due to its broad applicability. Li Chen is Research Scientist with Meta AI. Email: lichen66@fb.com Ningyuan (Teresa) Huang and Cong Mu are PhD students in the Depart ment of Applied Mathematics and Statistics, Johns Hopkins University. Email: nhuang19@jhu.edu, cmu2@jhu.edu Hayden S. Helm, Kate Lytvynets, and Weiwei Yang are with Microsoft Research. Email: haydenshelm@gmail.com, kalytv@microsoft.com, wei wya@microsoft.com Carey E. Priebe is Professor in the Department of Applied Mathemat ics and Statistics (AMS), the Center for Imaging Science (CIS), and the Mathematical Institute for Data Science (MINDS), Johns Hopkins University. Email: cep@jhu.eduWe propose a simple and efÔ¨Åcient hierarchical approach that requires no change of the network architecture and the optimization mechanism. This is in contrast to the existing modelfree methods above that typically require either signiÔ¨Åcant change in network architecture (e.g., co teaching in [14], [29]) or in the optimization procedure (e.g., metalearning in [22], semisupervised learning in [30], [31]). Experiments on the benchmark datasets with synthetic noise suggest that our proposed hierarchical approach can statistically (and operationally) improve the performance of the deep learning model (see Figure 1). This result is similarly observed in a realworld dataset with inherent label noise. Fig. 1. Performance advantages obtained by our hierarchical model (HC) compared to the standard model (FLAT) on ICON94 dataset with uniform noise and noise ratio 2f0%;20%;30%;50%g. The accuracy gain (of HC over FLAT) is both statistically and operationally signiÔ¨Åcant. The rest of this paper is organized as follows. Section 2 provides background on label noise taxonomy and related work on the main directions of this topic. Section 3 intro duces our proposed hierarchical model. Section 4 provides experimental results on different datasets with various types of label noise. Section 5 conducts ablation study on our proposed method. Section 6 discusses our Ô¨Åndings and future work.arXiv:2205.14299v1  [cs.LG]  28 May 2022IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2 2 B ACKGROUND 2.1 Label Noise Taxonomy In this section, we provide different major categorizations of label noise. The simplest type of label noise is known as uniform label noise, where the ground truth labels are changed to the wrong labels uniformly with a probability p. A slightly more complicated type of label noise is class dependent. In classdependent settings there is a noise transition matrix TKK, whereKis the number of classes, that governs the probability of a groundtruth label getting switched to a different label. In particular, if an observation has a ground truth label ithen the probability that the observed class label is jisTij. Hence, the diagonal of the matrixTis proportion of trulyclass iremaining class i. The transition matrix Tneed not be symmetric. A yet more complicated setting is feature and classdependent. That is, the probability of transitioning from class ito class jis a function of both the feature vector and the ground truth labeli. This setting best mimics realworld label noise scenarios as the difÔ¨Åculty in labeling for human annotators is nonuniform for most groundtruth class conditionals. Some authors consider partdependent label noise where the noise only partially depends on an instance [32]. Also related to the label noise taxonomy is the perspec tive of [33] and [34] where they describe the label noise problem in the language of uncertainty. In their character ization, there are two types of uncertainty: aleatoric and epistemic. Aleatoric uncertainty persists in the data even as the number of samples goes to inÔ¨Ånity whereas epistemic uncertainty can be avoided with a sufÔ¨Åcient amount of data. The uniform label noise setting we consider herein falls under aleatoric uncertainty, and the classdependent noise setting under both aleatoric and epistemic uncertainty. 2.2 Related work "
240,Classifying and Segmenting Microscopy Images Using Convolutional Multiple Instance Learning.txt,"Convolutional neural networks (CNN) have achieved state of the art
performance on both classification and segmentation tasks. Applying CNNs to
microscopy images is challenging due to the lack of datasets labeled at the
single cell level. We extend the application of CNNs to microscopy image
classification and segmentation using multiple instance learning (MIL). We
present the adaptive Noisy-AND MIL pooling function, a new MIL operator that is
robust to outliers. Combining CNNs with MIL enables training CNNs using full
resolution microscopy images with global labels. We base our approach on the
similarity between the aggregation function used in MIL and pooling layers used
in CNNs. We show that training MIL CNNs end-to-end outperforms several previous
methods on both mammalian and yeast microscopy images without requiring any
segmentation steps.","High content screening (HCS) technologies that combine automated Ô¨Çuorescence microscopy with high throughput biotechnology have become powerful systems for studying cell biology and for drug screening [1]. These systems can produce more than 105images per day, making their success dependent on automated image analysis. Previous analysis pipelines heavily rely on handtuning the segmentation, feature extraction, and classiÔ¨Åcation steps for each assay. Although comprehen sive tools have become available [2] they are often optimized for mammalian cells and not directly applicable to model organisms such as yeast and C. elegans . Researchers studying these organisms often manually classify cellular patterns by eye [3]. Recent advances in deep learning have proven that deep neural networks trained endtoend can learn powerful feature representations and outperform classiÔ¨Åers built on top of extracted features [4, 5]. While object recognition models have been successfully trained using images with one or a few objects of interest at the center of the image, microscopy images often contain hundreds of cells from the label class, as well as a few outliers. Training similar recognition models on HCS screens is therefore challenging due to the lack of datasets labeled at the single cell level. In this work we describe a convolutional neural network (CNN) that is trained on full resolution microscopy images using multiple instance learning (MIL). The network is designed to produce feature maps for every output category, as proposed for segmentation tasks in [6]. We pose cellular phenotype classiÔ¨Åcation as a special case of MIL, where each element in a classspeciÔ¨Åc feature map is considered an instance and each full resolution microscopy image is considered a bag with a label. Typically binary MIL problems assume that a bag is positive if at least one instance within the bag is positive. This assumption does not hold for HCS images due to heterogeneities within cellular populations and imaging artifacts [7]. We explore the performance of several global pooling operators on this problem and propose a new operator capable of learning the proportion of instances necessary to activate a label. 1arXiv:1511.05286v1  [cs.CV]  17 Nov 2015The main contributions of our work are the following. We present a uniÔ¨Åed view of the classical MIL approaches as pooling layers in CNNs and compare their performances. We propose a novel MIL method, ‚Äúadaptive NoisyAND‚Äù, that is robust to outliers and large numbers of instances. We evaluate our proposed model on both mammalian and yeast datasets, and Ô¨Ånd that our model signiÔ¨Åcantly outperforms previously published results at phenotype classiÔ¨Åcation. Our model is capable of learning a good classiÔ¨Åer for full resolution microscopy images as well as individual cropped cell instances, even though it is only trained using whole image labels. We also demonstrate that the model can localize regions with cells in the full resolution microscopy images and that the model predictions are based on activations from these regions. 2 Related Work "
241,Deep Learning is Robust to Massive Label Noise.txt,"Deep neural networks trained on large supervised datasets have led to
impressive results in image classification and other tasks. However,
well-annotated datasets can be time-consuming and expensive to collect, lending
increased interest to larger but noisy datasets that are more easily obtained.
In this paper, we show that deep neural networks are capable of generalizing
from training data for which true labels are massively outnumbered by incorrect
labels. We demonstrate remarkably high test performance after training on
corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain
test accuracy above 90 percent even after each clean training example has been
diluted with 100 randomly-labeled examples. Such behavior holds across multiple
patterns of label noise, even when erroneous labels are biased towards
confusing classes. We show that training in this regime requires a significant
but manageable increase in dataset size that is related to the factor by which
correct labels have been diluted. Finally, we provide an analysis of our
results that shows how increasing noise decreases the effective batch size.","Deep neural networks are typically trained using supervised learning on large, carefully annotated datasets. However, the need for such datasets restricts the space of problems that can be addressed. This has led to a proliferation of deep learning results on the same tasks using the same wellknown datasets. However, carefully annotated data is difÔ¨Åcult to obtain, especially for classiÔ¨Åcation tasks with large numbers of classes (requiring extensive annotation) or with Ô¨Ånegrained classes (requiring skilled annotation). *Equal contribution1Department of Mathematics, Mas sachusetts Institute of Technology, Cambridge, MA USA 2Department of Computer Science & Cornell Tech, Cornell Uni versity, New York, NY USA3Department of Computer Science, Massachusetts Institute of Technology, Cambridge, MA USA. Cor respondence to: David Rolnick <drolnick@mit.edu >, Andreas Veit<av443@cornell.edu >.Thus, annotation can be expensive and, for tasks requiring expert knowledge, may simply be unattainable at scale. To address this limitation, other training paradigms have been investigated to alleviate the need for expensive an notations, such as unsupervised learning (Le, 2013), self supervised learning (Pinto et al., 2016; Wang & Gupta, 2015) and learning from noisy annotations (Joulin et al., 2016; Natarajan et al., 2013; Veit et al., 2017). Very large datasets (e.g., Krasin et al. (2016); Thomee et al. (2016)) can often be obtained, for example from web sources, with partial or unreliable annotation. This can allow neural net works to be trained on a much wider variety of tasks or classes and with less manual effort. The good performance obtained from these large, noisy datasets indicates that deep learning approaches can tolerate modest amounts of noise in the training set. In this work, we study the behavior of deep neural networks under extremely low label reliability, only slightly above chance. The insights from our study can help guide future settings in which arbitrarily large amounts of data are easily obtainable, but in which labels come without any guarantee of validity and may merely be biased towards the correct distribution. The key takeaways from this paper may be summarized as follows: ‚Ä¢Deep neural networks are able to generalize after training on massively noisy data, instead of merely memorizing noise. We demonstrate that standard deep neural networks still perform well even on training sets in which label accuracy is as low as 1 percent above chance. On MNIST, for example, performance still exceeds 90 percent even with this level of label noise (see Figure 1). This behavior holds, to varying extents, across datasets as well as patterns of label noise, including when noisy labels are biased towards confused classes. ‚Ä¢A sufÔ¨Åciently large training set can accommodate a wide range of noise levels. We Ô¨Ånd that the minimum dataset size required for effective training increases with the noise level (see Figure 9). A large enough training set can accommodate a wide range of noise levels. Increasing the dataset size further, however, arXiv:1705.10694v3  [cs.LG]  26 Feb 2018Deep Learning is Robust to Massive Label Noise does not appreciably increase accuracy (see Figure 8). ‚Ä¢High levels of label noise decrease the effective batch size , as noisy labels roughly cancel out and only a small learning signal remains. As such, dataset noise can be partly compensated for by larger batch sizes and by scaling the learning rate with the effective batch size. 2. Related Work "
242,Deep Self-Learning From Noisy Labels.txt,"ConvNets achieve good results when training from clean data, but learning
from noisy labels significantly degrades performances and remains challenging.
Unlike previous works constrained by many conditions, making them infeasible to
real noisy cases, this work presents a novel deep self-learning framework to
train a robust network on the real noisy datasets without extra supervision.
The proposed approach has several appealing benefits. (1) Different from most
existing work, it does not rely on any assumption on the distribution of the
noisy labels, making it robust to real noises. (2) It does not need extra clean
supervision or accessorial network to help training. (3) A self-learning
framework is proposed to train the network in an iterative end-to-end manner,
which is effective and efficient. Extensive experiments in challenging
benchmarks such as Clothing1M and Food101-N show that our approach outperforms
its counterparts in all empirical settings.","Deep Neural Networks (DNNs) achieve impressive re sults on many computer vision tasks such as image recog nition [13, 33, 34], semantic segmentation [22, 40, 24], ob ject detection [5, 30, 27, 18] and cross modality tasks [20, 21, 41]. However, many of these tasks require largescale datasets with reliable and clean annotations to train DNNs such as ImageNet [2] and MSCOCO [19]. But collect ing largescale datasets with precise annotations is expen sive and timeconsuming, preventing DNNs from being em ployed in realworld noisy scenarios. Moreover, most of the ‚Äúground truth annotations‚Äù are from human labelers, who also make mistakes and increase biases of the data. An alternative solution is to collect data from the Inter net by using different imagelevel tags as queries. These tags can be regarded as labels of the collected images. This solution is cheaper and more timeefÔ¨Åcient than human an notations, but the collected labels may contain noises. A lot of previous work has shown that noisy labels lead to an obvious decrease in performance of DNNs [38, 23, 26]. Therefore, attentions have been concentrated on how to im Prototype Decision Boundary Prototypes Prototypes Prototype Decision BoundaryFigure 1. An example of solving two classes classiÔ¨Åcation problem using different number of prototypes. Left: Original data distribu tion. Data points with the same color belong to the same class. Upper Right : The decision boundary obtained by using a single prototype for each class. Lower Right : The decision boundary obtained by two prototypes for each class. Two prototypes for each class leads to a better decision boundary. prove the robustness of DNNs against noisy labels. Previous approaches tried to correct the noisy labels by introducing a transition matrix [25, 9] into their loss functions, or by adding additional layers to estimate the noises [6, 32]. Most of these methods followed a simple assumption to simplify the problem: There is a single tran sition probability between the noisy label and groundtruth label, and this probability is independent of individual sam ples. But in real cases, the appearance of each sample has much inÔ¨Çuence on whether it can be misclassiÔ¨Åed. Due to this assumption, although these methods worked well on handcrafted noisy datasets such as CIFAR10 [12] with manually Ô¨Çipped noisy labels, their performances were lim ited on real noisy datasets such as Clothing1M [38] and Food101N [15]. Also, noisy tolerance loss functions [35, 39] have been developed to Ô¨Åght against label noises, but they had a simi lar assumption as the above noise correction approaches. So they were also infeasible for realworld noisy datasets. Fur thermore, many approaches [15, 17, 37] solved this probarXiv:1908.02160v2  [cs.CV]  20 Aug 2019lem by using additional supervision. For instance, some of them manually selected a part of samples and asked human labelers to clean these noisy labels. By using extra super vision, these methods could improve the robustness of deep networks against noises. The main drawback of these ap proaches was that they required extra clean samples, mak ing them expensive to apply in largescale realworld sce narios. Among all the above work, CleanNet [15] achieved the existing stateoftheart performance on realworld dataset such as Clothing1M [38]. CleanNet used ‚Äúclass prototype‚Äù (i.e. a representative sample) to represent each class cate gory and decided whether the label for a sample is correct or not by comparing with the prototype. However, CleanNet also needed additional information or supervision to train. To address the above issues, we propose a novel frame work of SelfLearning with MultiPrototypes (SMP), which aims to train a robust network on the real noisy dataset with out extra supervision. By observing the characteristics of samples in the same noisy category, we conjecture that these samples have widely spread distribution. A single class pro totype is hard to represent all characteristics of a category. More prototypes should be used to get a better represen tation of characteristics. Figure 1 illustrated the case and further exploration has been conducted in the experiment. Furthermore, extra information (supervision) is not neces sarily available in practice. The proposed SMP trains in an iterative manner which contains two phases: the Ô¨Årst phase is to train a network with the original noisy label and corrected label generated in the second phase. The second phase uses the network trained in the Ô¨Årst stage to select several prototypes. These prototypes are used to generate the corrected label for the Ô¨Årst stage. This framework does not rely on any assump tion on the distribution of noises, which makes it feasible to realworld noises. It also does not use accessorial neural networks nor require additional supervision, providing an effective and efÔ¨Åcient training scheme. The contributions of this work are summarized as fol lows. (1) We propose an iterative learning framework SMP to relabel the noisy samples and train ConvNet on the real noisy dataset, without using extra clean supervision. Both the relabeling and training phases contain only one single ConvNet that can be shared across different stages, mak ing SMP effective and efÔ¨Åcient to train. (2) SMP results in interesting Ô¨Åndings for learning from noisy data. For exam ple, unlike previous work [15], we show that a single pro totype may not be sufÔ¨Åcient to represent a noisy class. By extracting multiple prototypes for a category, we demon strate that more prototypes would get a better representa tion of a class and obtain better labelcorrection results. (3) Extensive experiments validate the effectiveness of SMP on different realworld noisy datasets. We demonstrate newstateoftheart performance on all these datasets. 2. Related Work "
243,Label Refinement Network for Coarse-to-Fine Semantic Segmentation.txt,"We consider the problem of semantic image segmentation using deep
convolutional neural networks. We propose a novel network architecture called
the label refinement network that predicts segmentation labels in a
coarse-to-fine fashion at several resolutions. The segmentation labels at a
coarse resolution are used together with convolutional features to obtain finer
resolution segmentation labels. We define loss functions at several stages in
the network to provide supervisions at different stages. Our experimental
results on several standard datasets demonstrate that the proposed model
provides an effective way of producing pixel-wise dense image labeling.","We consider the problem of semantic image segmenta tion, where the goal is to densely label each pixel in an image according to the object class that it belongs to. We propose a convolutional neural network architecture called thelabel reÔ¨Ånement network (LRN) that performs semantic segmentation in a coarsetoÔ¨Åne fashion. Deep convolutional neural networks (CNNs) have been successfully applied to a wide variety of visual recognition problems, such as image classiÔ¨Åcation [ 13], object detec tion [ 19], action recognition [ 24], etc. CNNs extract deep feature hierarchies using alternating layers of operations, such as convolution, pooling, etc. Features from the top lay ers of CNNs tend to be invariant to ‚Äúnuisance factors‚Äù such as pose, illumination, small translations, etc. The invariance properties of these features make them particularly useful for vision tasks such as whole image classiÔ¨Åcation. However, these features are not well suited for other vision tasks (e.g. semantic segmentation) that require precise pixelwise infor mation. There have been some recent efforts [ 1,4,8,9,16,17,18] on adapting CNNs for semantic segmentation. Some of these approaches (e.g. [ 9,16,17]) are based on combining the convolutional features from multiple layers in a CNN. Another popular approach (e.g. [ 1,18] is to use upsampling (also known as deconvolution) to enlarge the spatial dimensions of the feature map at the top layer of a CNN, e.g. to the same spatial dimensions as the original image, then predict the pixelwise labels from the enlarged feature map. In both the cases, the Ô¨Ånal fullsized semantic segmentation result is obtained in a ‚Äúsingle shot‚Äù at the very end of the network architecture. In this paper, we introduce a novel CNNbased architec ture for semantic segmentation. Different from previous approaches, our proposed model predicts semantic labels at several different resolutions in a coarsetoÔ¨Åne fashion. We use resized groundtruth segmentation labels as the super vision at each resolution level. The segmentation labels at a coarse scale are combined with convolutional features to produce segmentation labels at a Ô¨Åner scale. See Fig. 1 for an illustration of our model architecture. We make threefold contributions in this paper which are as follows: We introduce a new perspective on the semantic seg mentation (or more generally, pixelwise labeling) prob lem. Instead of predicting the Ô¨Ånal segmentation result in a single shot, we propose to solve the problem in a coarsetoÔ¨Åne fashion by Ô¨Årst predicting a coarse label ing, then progressively reÔ¨Åne the result to get the Ô¨Åner scale results. We propose an endtoend CNN architecture to learn to predict the segmentation labels at multiple resolu tions. Unlike most of the previous methods that only have supervision at the end of their network, our model has supervision at multiple resolutions in the network. Although we focus on semantic image segmentation in this paper, our network architecture is general enough to be used for any pixelwise labeling task. We perform extensive experiments on several standard datasets to demonstrate the effectiveness of our pro posed model. 1arXiv:1703.00551v1  [cs.CV]  1 Mar 2017Figure 1. Overall architecture of the Label ReÔ¨Ånement Network (LRN). LRN is based on encoderdecoder framework. The encoder network produces a sequence of feature maps with decreasing spatial dimensions. The decoder network produces label maps with increasing spatial dimensions. A larger label map is obtained by combining the previous (smaller) label map and the corresponding convolutional features from a layer in the encoder network indicated by the solid line. We use downsampled groundtruth label maps to provide the supervision at each stage of the decoder network. The rest of the paper is organized as follows. Section 2 presents related work. Section 3 discusses the essential background of encoderdecoder architecture. Section 4 intro duces our label reÔ¨Ånement network. Section 5 describes the experiment details and also presents some analysis. Finally, we conclude the paper in Section 6. 2. Related Work "
244,Factorized Distillation: Training Holistic Person Re-identification Model by Distilling an Ensemble of Partial ReID Models.txt,"Person re-identification (ReID) is aimed at identifying the same person
across videos captured from different cameras. In the view that networks
extracting global features using ordinary network architectures are difficult
to extract local features due to their weak attention mechanisms, researchers
have proposed a lot of elaborately designed ReID networks, while greatly
improving the accuracy, the model size and the feature extraction latency are
also soaring. We argue that a relatively compact ordinary network extracting
globally pooled features has the capability to extract discriminative local
features and can achieve state-of-the-art precision if only the model's
parameters are properly learnt. In order to reduce the difficulty in learning
hard identity labels, we propose a novel knowledge distillation method:
Factorized Distillation, which factorizes both feature maps and retrieval
features of holistic ReID network to mimic representations of multiple partial
ReID models, thus transferring the knowledge from partial ReID models to the
holistic network. Experiments show that the performance of model trained with
the proposed method can outperform state-of-the-art with relatively few network
parameters.","Person ReidentiÔ¨Åcation is aimed at identifying the same person across videos captured from different cameras. It is a challenging task mainly due to factors such as back ground clutter, pose, illumination and camera point of view variations. As the prosperous of deep learning, handcrafte d features are replaced by features learned from data by deep convolutional neural networks (CNNs). With feature learn ing, deep network can build up attention mechanism to re duce interference of background clutter or occlusion, and extract discriminative poseinvariant features. It is generally believed that traditional networks extract  ing globally pooled features (like IDE [37]) can only learn Figure 1. SpeciÔ¨Åcation of 7 Views. Holistic : pedestrian image resize to 256 √ó128. Partial Group 1 : Uniformly divide image into 4 stripes to compose Partial Views: Up1(1/42/4), Mid1 (2/4 3/4),Dn1(3/44/4). Partial Group 2 : Uniformly divide image into 7 stripes to compose Partial Views: Up2(1/73/7), Mid2(3/7 5/7), Dn2(5/77/7). All partial images resize to 224 √ó224. to extract salient features during training, and it is hard t o extract local features due to their weak attention mecha nisms. In order to strengthen attention mechanisms in ReID networks, a lot of approaches have recently been proposed. These works commonly employ spatial partition, body parts detection, pose estimation and so on, pushing the perfor mance of ReID to a new level. However, with the complex network structures or huge models these approaches use, ReID models are hard to be commercially used in a large scale or deployed in mobile devices. Therefore, we want to address the problem in a different way. Why traditional ReID networks extracting globally pooled features are in low performance? As the number of identities in the training set is small relative to the numbe r of combinations of latent discriminative features, the com  monly used identity classiÔ¨Åcation loss prone to overfeat to a small subset of discriminative features which are salient . But identities in testing set are totally different from tra in ing set, as a result, the limited features that perfectly cla ssify training identities are not enough to distinguish testing i n 1dividuals. Even if some visual appearances of body parts in testing images are similiar to local areas in training image s, the model still prone to ignore them if they are inconspic uous, because these hard features need not to be leant to lower the loss fucntion. If we train partial ReID models with highresolution par tial images, each partial model can discover more discrim inative features in its restricted region than those found b y holistic model in the same area. Each partial ReID model is an expert for a speciÔ¨Åc region, so the total amount of knowl edge contained in the ensemble of separately trained partia l ReID models will exceed the holistic model. We aim to transfer the knowledge from multiple partial ReID models to a holistic model, while avoiding feature concatenation i n order not to make the dimension of student‚Äôs retrieval fea tures become unacceptably large when the number of teach ers is keep increasing. Our approach is: Firstly, we utilize several sepa rately trained teacher models (holistic or partial) to gen erate enhanced representation features, named Supervisory Representations ( SRs). Secondly, SRis regarded as Ô¨Åne grained highdimensional soft attributes, and the task of a t tributes training corresponding to each teacher is added to the holistic student‚Äôs training system to improve the featu re maps of the student. Thirdly, SRis regarded as anchors for each sample in different feature space of partial ReID rep resentation, and the role of metric learning is achieved by factorize the student model‚Äôs representation to each of the SR‚Äôs feature space and mimic these anchors. The contributions of this paper are: 1) We propose a novel knowledge transfer method, named Factorized Distillation (FD), which can train a holis tic student model by distilling an ensemble of partial mod els, using the way of factorization instead of feature con catination to result in compact retrieval features even whe n the number of teachers is large. 2) Trained by FD, ordinary holistic networks extracting global features can also generate strong attention mech anism, and can directly extract partial features from im ages of high pose variation without incorporating addition al bodyparts detection or pose estimation networks. 3) Extensive experiments on ReID datasets demonstrate that the proposed method can outperform stateoftheart with relatively few network parameters. 2. Related Work "
245,Label-Noise Robust Multi-Domain Image-to-Image Translation.txt,"Multi-domain image-to-image translation is a problem where the goal is to
learn mappings among multiple domains. This problem is challenging in terms of
scalability because it requires the learning of numerous mappings, the number
of which increases proportional to the number of domains. However, generative
adversarial networks (GANs) have emerged recently as a powerful framework for
this problem. In particular, label-conditional extensions (e.g., StarGAN) have
become a promising solution owing to their ability to address this problem
using only a single unified model. Nonetheless, a limitation is that they rely
on the availability of large-scale clean-labeled data, which are often
laborious or impractical to collect in a real-world scenario. To overcome this
limitation, we propose a novel model called the label-noise robust
image-to-image translation model (RMIT) that can learn a clean label
conditional generator even when noisy labeled data are only available. In
particular, we propose a novel loss called the virtual cycle consistency loss
that is able to regularize cyclic reconstruction independently of noisy labeled
data, as well as we introduce advanced techniques to boost the performance in
practice. Our experimental results demonstrate that RMIT is useful for
obtaining label-noise robustness in various settings including synthetic and
real-world noise.","Imagetoimage translation is a problem in which the goal is to translate an image into the corresponding target image. Recently, this problem has been studied actively owing to its high potential for diverse applications, such as colorization [45, 94], super resolution [46, 43], image in painting [63, 29], photographic image synthesis [13, 85], and photo editing [99, 12, 33]. In particular, the introduc tion of generative adversarial networks (GANs) [21] has re sulted in signiÔ¨Åcant advances in this problem and allows for an imagetoimage translation model to be constructed in more challenging but practically important settings. Among them, a wellattended problem is multidomain imagetoimage translation where the goal is to learn map ping among multiple domains. This problem focuses on a dataset that contains multiple domains, such as the RaFD dataset [44] which contains eight facial expression labels (e.g., happy, angry, and sad) and the CelebA dataset [51] which includes 40 facial attribute labels (e.g., hair color, gender, and age). Given such a dataset, the aim of multi domain imagetoimage translation is to construct a genera tor that can translate an image among multiple domains ac cording to the given domain labels (e.g., expression labels and attribute labels). This problem is challenging in terms of scalability. In 1arXiv:1905.02185v1  [cs.CV]  6 May 2019particular, typical onetoone imagetoimage translation models (e.g., [77, 38, 100, 89, 50]) suffer from the difÔ¨Å culty because they require the learning of c(c 1)genera tors to address all mappings among the cdomains. To mit igate this requirement, recent studies (e.g., StarGAN [15]) extend a conventional imagetoimage translation model to the labelconditional setting. By this formulation, they en able mappings among multiple domains do be learned using only a single uniÔ¨Åed model. Nonetheless, a possible limitation is that existing multi domain imagetoimage translation models rely on the availability of largescale cleanlabeled data, the collection of which is often laborious or impractical in a realworld scenario. Indeed, it is demonstrated that when facial ex pression data, which are commonly used as an applica tion of multidomain imagetoimage translation, are col lected through crowdsourcing, the annotation accuracy is low (e.g., 655%accuracy on the FER dataset [20]). This motivates us to address learning using noisy labeled data; however, as shown in Figure 1(b), typical multidomain imagetoimage translation models (e.g., StarGAN in this example) are highly degraded when trained using noisy la beled data. These observations emphasize the insufÔ¨Åciency of the previous models. To overcome this limitation, we propose a labelnoise robust multidomain imagetoimage translation model (RMIT) , which can learn a clean label conditional gener ator even when only noisy labeled data are available. In particular, in StarGAN, a classiÔ¨Åcation loss (which renders a generated image belong to the target domain) and cy cle consistency loss (which encourages the content to be preserved during the translation) are degraded by noisy la bels. To remedy this degradation, we introduce a labelnoise robust classiÔ¨Åcation loss and labelnoise robust cycle con sistency loss. SpeciÔ¨Åcally, although the former has been studied actively in image classiÔ¨Åcation, the latter is unique for multidomain imagetoimage translation and no estab lished method has been devised. Hence, we propose a novel loss called the virtual cycle consistency loss that can impose a cyclic constraint independently of noisy labeled data. Fig ure 1(c) demonstrates the effectiveness of RMIT. As shown in this Ô¨Ågure, RMIT can translate an image conditioned on clean labels even where StarGAN is highly degraded. Recently, a labelnoise effect on DNNs has garnered at tention owing to a gap between theory and practice. To reveal such a gap, empirical studies have been conducted actively in image classiÔ¨Åcation [90, 6, 72]; however, to our knowledge, no previous studies have analyzed such an ef fect on multidomain imagetoimage translation. To ad vance this research, we conducted extensive experiments in various labelnoise settings including synthetic and real world noise and reveal the characteristics of our novel task. Furthermore, we introduced advanced techniques for prac tice and empirically demonstrated their effectiveness.Overall, our contributions are summarized as follows: We propose a novel model called RMIT , in which the goal is to learn a labelnoise robust generator that can translate an image conditioned on clean labels even when the training labels are noisy . We introduce a labelnoise robust classiÔ¨Åcation loss and a labelnoise robust cycle consistency loss into an imagetoimage translation model. In particular, a labelnoise robust cycle consistency loss is unique for our novel task and we devise a novel loss called the virtual cycle consistency loss . We examined the empirical performance through ex tensive experiments including synthetic and realworld noise along with introducing advanced techniques for practice. 2. Related work "
246,Deep pNML: Predictive Normalized Maximum Likelihood for Deep Neural Networks.txt,"The Predictive Normalized Maximum Likelihood (pNML) scheme has been recently
suggested for universal learning in the individual setting, where both the
training and test samples are individual data. The goal of universal learning
is to compete with a ``genie'' or reference learner that knows the data values,
but is restricted to use a learner from a given model class. The pNML minimizes
the associated regret for any possible value of the unknown label. Furthermore,
its min-max regret can serve as a pointwise measure of learnability for the
specific training and data sample. In this work we examine the pNML and its
associated learnability measure for the Deep Neural Network (DNN) model class.
As shown, the pNML outperforms the commonly used Empirical Risk Minimization
(ERM) approach and provides robustness against adversarial attacks. Together
with its learnability measure it can detect out of distribution test examples,
be tolerant to noisy labels and serve as a confidence measure for the ERM.
Finally, we extend the pNML to a ``twice universal'' solution, that provides
universality for model class selection and generates a learner competing with
the best one from all model classes.","In the common situation of supervised machine learning, a trainset consisting of Npairs of examples is given, zN=f(xi;yi)gN i=1, wherex2Xis the data or the feature and y2Yis the label. Then, a new xis given and the task is to predict its corresponding label y. The formal deÔ¨Ånition of the learning problem includes a loss function that measures the accuracy of the prediction. Here we assume that the learner assigns a probability q(jx)to the test label, and we use the logloss to evaluate the performance of the predictor `(q;x;y) = logq(yjx): (1) 1arXiv:1904.12286v2  [cs.LG]  8 Jan 2020Bibas, Fogel and Feder Clearly, a reasonable goal is to Ô¨Ånd the predictor q(jx)with the minimal loss. However, this problem is illposed unless additional assumptions are made. First, a ‚Äúmodel‚Äù class, or ‚Äòhypotheses‚Äù class must be deÔ¨Åned. This class is a set of conditional probability distributions P=fp(yjx); 2g (2) where is a general index set. This is equivalent to saying that there is a set of stochastic functionsfy=g(x); 2gused to explain the relation between xandy. Next, assumptions must be made on how the data and the labels are generated. The most common setting in learning theory is Probably Approximately Correct (PAC), established in Valiant (1984) where xandyare assumed to be generated by some source P(x;y) = P(x)P(yjx).P(yjx)is not necessarily a member of P. The goal is to perform as well as a learner that knows the best member of P. Another possible setting for the learning problem, recently suggested in Fogel and Feder (2018a) following, e.g., Merhav and Feder (1998), is the individual setting, where the data and labels of both the training and test are speciÔ¨Åc and individual values. In this setting the goal is to compete with a ‚Äúgenie‚Äù or a reference learner that knows the desired label value, but is restricted to use a model from the given hypotheses class P, and that does not know which of the samples is the test. This learner then chooses: ^(zN;x;y) = arg max  p(yjx)N i=1p(yijxi) : (3) The logloss diÔ¨Äerence between a universal learner qand the reference is the regret: R(zN;x;y;q ) = logp^(zN;x;y)(yjx) q(yjzN;x): (4) As advocated in Fogel and Feder (2018b), the chosen universal learner solves:   =R(zN;x) = min qmax yR(zN;x;y;q ) (5) This minmax optimal solution, termed Predictive Normalized Maximum Likelihood (pNML), is obtained using ‚Äúequalizer‚Äù reasoning, following Shtarkov (1987): qpNML(yjzN;x) =max 2p(yjzN;x;y) P y2Ymax 2p(yjzN;x;y): (6) and its corresponding regret, independent of the true y, is:   = log8 < :X y2Ymax 2p(yjzN;x;y)9 = ;: (7) Note that this deviates from the commonlyused Empirical Risk Minimization (ERM) approach (Vapnik, 1992), where in the logloss case the learner chooses the model that assigns the maximal probability for the trainset: qERM(yjx) =argmin pNX i=1 log(p(yijxi): (8) 2Deep pNML Nevertheless, it turns out that  can also be used to obtain a bound on the performance of the ERM, see Fogel and Feder (2018a). The pNML has been derived for several model classes in related works (Fogel and Feder, 2018b) such as the barrier (or 1D perceptron) model, and in Bibas et al. (2019) for the linear regression problem. In all these cases the advantages of the pNML and its corresponding learnability measure were discussed. In some cases there are several possible model classes, or several possible algorithms. In this case, one can use a ‚Äôtwiceuniversal‚Äô approach, see Fogel and Feder (2018a), to achieve nearoptimal performance not just within a class but over all possible classes. In this approach, the best pNML learner from each of Kmodel classes is chosen fqkgK k=1. Then, another pNML procedure is executed over all of these learners. This paper‚Äôs contribution is in proposing a scheme of the pNML learner for DNNs hypothesis class. We show that the pNML improves upon the ERM learner performance in accuracy and logloss on the testset, especially in the worstcase performance. For the Ô¨Årst time, we show that the pNML is more robust to noisy labels in the trainset and adversarial attacks in the test. Furthermore, we show that  may serve as a learnability measure for both the pNML and the ERM and that it can be used to point out when the trainset is composed of random labels and when the test sample is out of distribution or adversarial. Finally, we demonstrate that the twice universal pNML scheme over the number of Ô¨Ånetuned layers of DNNs forms has an even superior performance over both the pNML and the ERM. 2. Related Work "
247,Attended Temperature Scaling: A Practical Approach for Calibrating Deep Neural Networks.txt,"Recently, Deep Neural Networks (DNNs) have been achieving impressive results
on wide range of tasks. However, they suffer from being well-calibrated. In
decision-making applications, such as autonomous driving or medical diagnosing,
the confidence of deep networks plays an important role to bring the trust and
reliability to the system. To calibrate the deep networks' confidence, many
probabilistic and measure-based approaches are proposed. Temperature Scaling
(TS) is a state-of-the-art among measure-based calibration methods which has
low time and memory complexity as well as effectiveness. In this paper, we
study TS and show it does not work properly when the validation set that TS
uses for calibration has small size or contains noisy-labeled samples. TS also
cannot calibrate highly accurate networks as well as non-highly accurate ones.
Accordingly, we propose Attended Temperature Scaling (ATS) which preserves the
advantages of TS while improves calibration in aforementioned challenging
situations. We provide theoretical justifications for ATS and assess its
effectiveness on wide range of deep models and datasets. We also compare the
calibration results of TS and ATS on skin lesion detection application as a
practical problem where well-calibrated system can play important role in
making a decision.","Deep Neural Networks (DNNs) show dramatically ac curate results on challenging tasks such as computer vision [14, 39] speech recognition [12] and medical diagnosis [2]. However, in realworld decisionmaking applications, accu racy is not the only element considered and the conÔ¨Ådence of the network is also essential for having a secure and re liable system. In DNNs, conÔ¨Ådence usually corresponds to the output of a softmax layer, which is typically interpreted as the likeliness (probability) of different class occurrence. Label = DermatoÔ¨Åbroma Pred. = DermatoÔ¨Åbroma ConÔ¨Ådence = 0.99 Calib. ConÔ¨Ådence = 0.98 Label = BCC Pred. = BCC ConÔ¨Ådence = 0.99 Calib. ConÔ¨Ådence = 0.99 Label = Melanocytic nevus Pred. = Melanocytic nevus ConÔ¨Ådence = 0.99 Calib. ConÔ¨Ådence = 0.98 Label = Melanoma Pred. = Bowen ConÔ¨Ådence = 0.91 Calib. ConÔ¨Ådence = 0.54 Label = Bowen Pred. = BCC ConÔ¨Ådence = 0.90 Calib. ConÔ¨Ådence = 0.46 Label = Benign keratosis Pred. = Bowen ConÔ¨Ådence = 0.89 Calib. ConÔ¨Ådence = 0.48 Figure 1: Output of the medical assistant system for skin anomaly detection (more details in Section 6.2). Before calibration, the conÔ¨Ådence of the system is high for both correctly and misclassiÔ¨Åed samples. After applying calibra tion, the network keeps the conÔ¨Ådence of correctly classi Ô¨Åed samples high while decrease the conÔ¨Ådence of misclas siÔ¨Åed samples. Most of the time, this value is far from the true probability of each class occurrence, with a tendency to get overconÔ¨Å dent (i.e., output of one class close to 1 and other classes close to 0). In such case, we usually consider the DNN not to be wellcalibrated. Calibration in DNNs is a recent challenge in machine learning community which was not an issue previously for shallow neural networks [35]. Gua et al. [13] studies the role of different parameters which makes a neural network uncalibrated. They show a deep network 1arXiv:1810.11586v3  [cs.LG]  8 May 2019which Ô¨Ånds the optimal weights by minimizing Negative Log Likelihood (NLL) [10] loss function, can reach to the higher accuracy when it gets overÔ¨Åtted to NLL. However, the side effect of overÔ¨Åtting to NLL is to make the network overconÔ¨Ådent. Having calibrated network is important for realworld applications. In a selfdriving car [5] deciding about trans ferring the control of the car to the human observer is taken regarding to the conÔ¨Ådence of the detected objects. In med ical care systems [17], the deadly diseases can be missed when they are wrongly detected as a nonproblematic case with high conÔ¨Ådence. Calibration adds more information to the system which consequences reliability. Figure 1 com pares the output of an overconÔ¨Ådent system and calibrated one for misclassiÔ¨Åed and correctly classiÔ¨Åed samples in a skin lesion detection system. The calibrated networks de crease the conÔ¨Ådence in the case of wrongly detected sam ples while preserves the conÔ¨Ådence for most of correctly classiÔ¨Åed ones. Calibration methods for DNNs are widely investigated in recent literature and can be categorized into two main directions: 1probabilistic approaches 2measure based approaches. Probabilistic approaches generally in clude approximated Bayesian formalism [29, 32, 27, 4]. In practice, the quality of predictive uncertainty in Bayesian based methods relies heavily on the accuracy of sampling approximation and correctly estimated prior distribution. Despite of signiÔ¨Åcant achievements in distribution estima tion, these approaches are complex and suffer from a signif icant computational burden, time and memory complexity. Comparatively, measurebased approaches are more practical. They are generally postprocessing methods that do not need to retrain the network to make it cali brated. Temperature Scaling (TS) [13] is the stateofthe art measurebased approach that comparing to the others, achieves better calibration with minimum computational complexity (optimizing only one parameter Tto soften the softmax) which makes it the most appealing method in prac tice. It also preserves the accuracy rate of the network that can be degraded during the calibration phase. In TS, the best Tparameter is found by minimizing NLL loss with respect ing toTon validation set. One big challenge in real scenar ios to apply TS is gathering enough number of samples for validation set and labeling them by an expert to calibrate an already pretrained model. Asking nonprofessional ex perts to label the samples for decreasing the expenses, may bring labeling noise to the validation set, especially in med ical applications. Therefore being robust to the noise and size of validation set is a rising need in calibration appli cations. Despite of TS interesting results, when the DNN is highly accurate, or the validation set is small or contains noisy labels, TS cannot calibrate the DNN successfully. Contribution: In this paper, we propose a new TS fam ily method which is called Attended Temperature Scaling(ATS) to make a better adjustment of conÔ¨Ådence in DNNs. Comparing to TS algorithm, ATS preserves the time and memory complexity advantage of classic TS as well as intact accuracy while it brings better calibration. It spe cially works properly in the case of smallsize validation set, highly accurate DNNs and validation set with labeling noise in which TS is not functioning well. We analyze the oretically ATS and demonstrate why it works better in these situations. 2. Related Works "
248,DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling.txt,"Learning from noisy labels is a challenge that arises in many real-world
applications where training data can contain incorrect or corrupted labels.
When fine-tuning language models with noisy labels, models can easily overfit
the label noise, leading to decreased performance. Most existing methods for
learning from noisy labels use static input features for denoising, but these
methods are limited by the information they can provide on true label
distributions and can result in biased or incorrect predictions. In this work,
we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic
patterns in the embedding space during the fine-tuning process of language
models to improve noisy label predictions. DyGen uses the variational
auto-encoding framework to infer the posterior distributions of true labels
from noisy labels and training dynamics. Additionally, a co-regularization
mechanism is used to minimize the impact of potentially noisy labels and
priors. DyGen demonstrates an average accuracy improvement of 3.10% on two
synthetic noise datasets and 1.48% on three real-world noise datasets compared
to the previous state-of-the-art. Extensive experiments and analyses show the
effectiveness of each component in DyGen. Our code is available for
reproducibility on GitHub.","In many applications, collecting clean labeled data can be much more costly compared to obtaining noisy labeled data. Noisy labels can be cheaply obtained in large quantities from sources such as crowdsourcing [ 39,46], web annotations [ 8,28], labeling rules [ 11, 60], and search engines [ 51,58]. Using largescale noisy labeled data holds the potential of training powerful deep learning models with reduced data curation costs. Particularly, finetuning pretrained language models (PLMs) with noisy labels have gained interest for a wide range of text analysis tasks [ 1,41,65]. However, the over parameterized PLMs, due to their large size, are prone to overfitting the label noise, leading to decreased performance [ 3,9,63]. This has become a critical challenge that hinders PLMs from delivering satisfactory results when trained with noisy supervision. The problem of learning from noisy supervision has been widely studied in the machine learning community. Existing approachesarXiv:2305.19395v2  [cs.CL]  13 Jun 2023KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA Yuchen Zhuang, Yue Yu, Lingkai Kong, Xiang Chen, & Chao Zhang to this issue can be broadly classified into three categories. 1) Data Cleaning methods [ 2,6,25,34,49,52,55,65] detect noisy samples using specific criteria such as Area Under Margin [ 37] and Data Cartography [ 41] and remove, reweigh, or correct these samples for subsequent model training. 2) Regularization methods design regularized loss functions [ 14,29,31,44,48,64] or train multiple models to regularize each other [ 15,16,42,45,55,65], with the goal of improving robustness under label noise. 3) Noise Transition Estimation methods [ 7,36,50,53,54,63] estimate the transition matrixùëù(Àúùë¶|ùë¶,x)that maps clean labels ùë¶to noisy labels Àúùë¶, condi tioned on input features x. Noisy Prediction Calibration [ 5] is a recent approach that models the transition from noisy predictions to the true labels ùëù(ùë¶|ÀÜùë¶,x). Each of these categories has its own advantages and drawbacks, and their performance depends on the specific nature of the noise and the input features being used. A major challenge in existing methods for learning from noisy supervision is their dependence on either the original input features or the embeddings learned with noisy labels. Both scenarios pose limitations when finetuning PLMs with noisy labels. First, the original input features xfrom PLMs have limited expressivity and therefore cannot effectively distinguish between noisy and clean labels [ 24]. This can hurt the efficiency of data cleaning methods and the models that learn the noisetotruth transitions based on x. Furthermore, the input features may hold some information about the true labels ùë¶, however, they only encompass a limited understanding of the relationship between the true labels ùë¶and the noisy labels Àúùë¶. This limitation leads to a reduced capability for generalization to all types of noise. Second, finetuning PLMs with noisy labels can also hinder the effectiveness of denoising, as label noise can compromise the quality of the learned embeddings. Overfitting to the label noise can cause the model to memorize incorrect labels and mistakenly consider some noisy samples as clean ones, even with metrics in regularization methods during finetuning. This also impedes noise transition estimation methods from accurately modeling the generation of noise. Consequently, many existing studies are grounded in strong assumptions or are hindered by imprecise noise estimation, resulting in inconsistent performance across varying types of label noise [40]. In this work, we have discovered that noisy and clean samples exhibit distinct behaviors in the embedding space during PLM fine tuning with noisy labels. During the early stages of finetuning, we found that the noisy samples tend to be closer to the cluster associ ated with the true label ùë¶. However, as training progresses, these noisy samples are gradually drawn towards the cluster associated with the assigned noisy label Àúùë¶. Therefore, the noisy samples tend to have relatively larger distances to their assigned label clusters due to this training dynamics pattern. Such dynamic patterns can be quantified by the Euclidean distance between each sample and its assigned cluster center at each training epoch. In Figure 1, we visualize the computed distance in the embedding space with the mean (yaxis) and standard deviation (xaxis) over epochs. This plot clearly illustrates that noisy samples tend to have larger means and standard deviations as they move from the true label cluster to the noisy label cluster during training. We thus propose a dynamicsenhanced generative model Dy Gen for denoised finetuning of PLMs. Our model is based on the observation that noisy and clean samples have different dynamicsin the embedding space during the finetuning process. To take advantage of this dynamic pattern, our model treats the true labels as latent variables and infers them from the dynamic patterns and the noisy labels. Our model differs from previous generative denois ing models [ 50,53,54] in its use of features and modeling of how the features and noisy labels are generated. Unlike these previous models, which generate both the noisy label ÀÜùë¶and the input feature xconditioned on the true label ùë¶(ùëù(x,Àúùë¶|ùë¶)), our model leverages dynamic training patterns ùë§and treats the true label ùë¶as the latent encoding of ÀÜùë¶. This makes it easier to learn, as it only requires gen erating the noisy label ÀÜùë¶, and allows for inference of the posterior ùëù(ùë¶|ÀÜùë¶,w)using the variational autoencoding framework. Further more, we can use the discriminative power of the dynamic patterns to induce the prior distribution ùëù(ùë¶|w)of our generative model. To improve robustness in inferring the true label, we also employ a coregularization loss that encourages multiple branches of our generative model to reach a consensus for the posterior ùëù(ùë¶|ÀÜùë¶,w). We have conducted thorough experiments on two datasets with various synthetic noise types and three datasets from the WRENCH benchmark [ 60] with realworld weak label noise. Our method Dy Gen consistently surpasses the stateoftheart baselines, with an average improvement of 2.13%across various noise types and ratios on both synthetic and realworld datasets. Furthermore, DyGen demonstrates remarkable robustness even under extreme label noise ratios, as high as 50%. Additionally, DyGen enhances model calibra tion by generating predicted probabilities that are more accurately aligned with the true label distribution due to its dynamicsbased probabilistic denoising approach. Our contributions are as follows: ‚Ä¢We have discovered that dynamic training patterns in the hidden embedding space during PLM finetuning can effectively distin guish between clean and noisy samples. Utilizing this insight, we have devised a denoised finetuning approach for PLMs. To our knowledge, this is the first time that dynamic training patterns are used to achieve robust finetuning of PLMs with noisy labels. ‚Ä¢We design a generative model that models the reconstruction of the noisy label ÀÜùë¶from the latent true label ùë¶and the training dynamics w. We induce a prior distribution for the latent variable ùë¶based on the dynamics wand present a training procedure based on variational autoencoding. ‚Ä¢To enhance robustness, we employ multiple branches that co regularize each other to reach consensus for the posterior ùëù(ùë¶|ÀÜùë¶,w). ‚Ä¢We have conducted a comprehensive analysis of the noisy learning problems in text data, covering both synthetic and realworld noise scenarios. Our proposed method, DyGen , consistently outperforms other approaches across different types and levels of noise. 2 RELATED WORK "
249,In-Place Scene Labelling and Understanding with Implicit Scene Representation.txt,"Semantic labelling is highly correlated with geometry and radiance
reconstruction, as scene entities with similar shape and appearance are more
likely to come from similar classes. Recent implicit neural reconstruction
techniques are appealing as they do not require prior training data, but the
same fully self-supervised approach is not possible for semantics because
labels are human-defined properties.
  We extend neural radiance fields (NeRF) to jointly encode semantics with
appearance and geometry, so that complete and accurate 2D semantic labels can
be achieved using a small amount of in-place annotations specific to the scene.
The intrinsic multi-view consistency and smoothness of NeRF benefit semantics
by enabling sparse labels to efficiently propagate. We show the benefit of this
approach when labels are either sparse or very noisy in room-scale scenes. We
demonstrate its advantageous properties in various interesting applications
such as an efficient scene labelling tool, novel semantic view synthesis, label
denoising, super-resolution, label interpolation and multi-view semantic label
fusion in visual semantic mapping systems.","Enabling intelligent agents, such as indoor mobile robots, to plan contextsensitive actions in their environ ment requires both a geometric and semantic understanding of the scene. Machine learning methods have proven to be valuable in both geometric and semantic prediction tasks, but the performance of these methods suffers when the dis tribution of the training data does not match the scenes ob served at testtime. Though the issue can be mitigated by gathering costly annotated data or semisupervised learn ing, it is not always feasible in openset scenarios with var ious known and unknown classes. For this reason, it is ad vantageous to have methods that can selfsupervise. In par ticular, there has been recent success in using scenespeciÔ¨Åc methods (e.g. NeRF [16]) that represent the shape and ra diance of a single scene with a neural network trained from scratch using only images and associated camera poses. Semantic scene understanding means attaching class la Fusionvia LearningLabel DenoisingSuperResolutionLabelPropagationLabel Synthesis LabelInterpolationFigure 1: Neural radiance Ô¨Åelds (NeRF) jointly encoding appearance and geometry contain strong priors for segmen tation and clustering. We build upon this to create a scene speciÔ¨Åc 3D semantic representation, SemanticNeRF, and show that it can be efÔ¨Åciently learned with inplace super vision to perform various potential applications. bels to a geometric model. The tasks of estimating the geometry of a scene and predicting its semantic labels are strongly related, as parts of a scene that have similar shape are more likely to belong to the same semantic category than those which differ greatly. This has been shown in work on multitask learning [9, 33], where networks that simultane ously predict both shape and semantics perform better than when the tasks are tackled separately. Unlike scene geometry, however, semantic classes are a humandeÔ¨Åned concept and it is not possible to semanti cally label a novel scene in a purely selfsupervised man ner. The best that could be achieved would be to cluster selfsimilar structures of a scene into categories; but some labelling would always be needed to associate these clusters with humandeÔ¨Åned semantic classes. In this paper, we show how to design a scenespeciÔ¨Åc network for joint geometric and semantic prediction and train it on images from a single scene with only weak se mantic supervision (and no geometric supervision). Be cause our single network must generate both geometry andarXiv:2103.15875v2  [cs.CV]  21 Aug 2021semantics, the correlation between these tasks means that semantics prediction can beneÔ¨Åt from the smoothness, co herence and selfsimilarity learned by selfsupervision for geometry. In addition, multiview consistency is inherent to the training process and enables the network to produce accurate semantic labels of the scene, including for views that are substantially different from any in the input set. Our system takes as input a set of RGB images with as sociated known camera poses. We also supply some partial or noisy semantic labels for the images, such as ground truth labels for a small fraction of the images, or noisy or coarse label maps for a higher number of images. We train our net work to jointly produce implicit 3D representations of both the geometry and semantics for the whole scene. We evaluate our system both quantitatively and qualita tively on scenes from the Replica dataset [28], and quali tatively on realworld scenes from the ScanNet dataset [3]. Generating dense semantic labels for a whole scene from partial or noisy input labels is important for practical appli cations, like when a robot encounters a new scene and either only a small amount of insitu labelling is feasible, or only an imperfect singleview network is available. 2. Related Work "
250,"aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF, and Transfer Learning.txt","We describe our system for SemEval-2020 Task 11 on Detection of Propaganda
Techniques in News Articles. We developed ensemble models using RoBERTa-based
neural architectures, additional CRF layers, transfer learning between the two
subtasks, and advanced post-processing to handle the multi-label nature of the
task, the consistency between nested spans, repetitions, and labels from
similar spans in training. We achieved sizable improvements over baseline
fine-tuned RoBERTa models, and the official evaluation ranked our system 3rd
(almost tied with the 2nd) out of 36 teams on the span identification subtask
with an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams
on the technique classification subtask with an F1 score of 0.62.","The proliferation of disinformation online, commonly known as ‚Äúfake news‚Äù, has given rise to a lot of research on automatic fake news detection. However, most of the efforts have focused on checking whether a piece of information is factually correct, and little attention has been paid to the propaganda techniques that malicious actors use to spread their message. SemEval2020 Task 11 (Da San Martino et al., 2020a) aims to bridge this gap. It focused on detecting the use of propaganda techniques in news articles,1creating a dataset that extends (Da San Martino et al., 2019b), and offering two subtasks: span identiÔ¨Åcation (SI): detecting the propaganda spans in an article; technique classiÔ¨Åcation (TC): detecting the type of propaganda used in a given text span. Below, we describe the systems we built for these two subtasks. At the core of our systems is RoBERTa (Liu et al., 2019), a pretrained model based on the Transformer architecture (Vaswani et al., 2017). However, we improved over RoBERTa by adding extra layers in the neural network architecture, and we further added some postprocessing steps. We further applied transfer learning between the two subtasks, and Ô¨Ånally, we combined different models into an ensemble.2 2 Related Work "
251,Joint Optimization Framework for Learning with Noisy Labels.txt,"Deep neural networks (DNNs) trained on large-scale datasets have exhibited
significant performance in image classification. Many large-scale datasets are
collected from websites, however they tend to contain inaccurate labels that
are termed as noisy labels. Training on such noisy labeled datasets causes
performance degradation because DNNs easily overfit to noisy labels. To
overcome this problem, we propose a joint optimization framework of learning
DNN parameters and estimating true labels. Our framework can correct labels
during training by alternating update of network parameters and labels. We
conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset.
The results indicate that our approach significantly outperforms other
state-of-the-art methods.","DNNs trained on largescale datasets have achieved impressive results on many classiÔ¨Åcation problems. Generally, accurate labels are necessary to eÔ¨Äectively train DNNs. However, many datasets are constructed by crawling images and labels from websites and often contain incorrect noisy labels ( e.g., YFCC100M [17], Clothing1M [21]). This study addresses the following question: how can we eÔ¨Äectively train DNNs on noisy labeled datasets without manually cleaning the data? The prominent issue in training DNNs on noisy la beled datasets is that DNNs can learn or memorize, any training dataset , and this implies that DNNs are subject to total overÔ¨Åtting on noisy data. To address this problem, commonly used regulariza tion techniques including dropout and early stopping are helpful. However, these methods do not guarantee optimization because they prevent the networks from reducing the training loss. Another method involves using prior knowledge, such as the confusion matrix between clean and noisy labels, which typically cannot be used in real settings. Figure 1. The concept of our joint optimization framework. Noisy labels are reassigned to the probability output by CNNs. Network parameters and labels are alternatively updated for each epoch. Consequently, we need a new framework of opti mization. In this study, we propose an optimization framework for learning on a noisy labeled dataset. We propose optimizing the labels themselves as opposed to treating the noisy labels as Ô¨Åxed. The joint opti mization of network parameters and the noisy labels corrects inaccurate labels and simultaneously improves the performance of the classiÔ¨Åer. Fig. 1 shows the con cept of our proposal. The main contributions are as follows. 1. We propose a joint optimization framework for learning on noisy labeled datasets. Our optimiza tionproblemhastwooptimizationnetworkparam eters and class labels that are optimized by an al ternating strategy. 2. We observe that a DNN trained on noisy labeled datasets does not memorize noisy labels and main tains high performance for clean data under a high learning rate. This reinforces the Ô¨Åndings of Arpit et al.[1]thatsuggestthatDNNsÔ¨Årstlearnssimple patterns and subsequently memorize noisy data. 3. We evaluate the performance on synthetic and real noisy datasets. We demonstrate stateofthe art performance on the noisy CIFAR10 dataset and a comparable performance on the Clothing1M dataset [21]. 1arXiv:1803.11364v1  [cs.CV]  30 Mar 20182. Related Works "
252,Model-based 3D Hand Reconstruction via Self-Supervised Learning.txt,"Reconstructing a 3D hand from a single-view RGB image is challenging due to
various hand configurations and depth ambiguity. To reliably reconstruct a 3D
hand from a monocular image, most state-of-the-art methods heavily rely on 3D
annotations at the training stage, but obtaining 3D annotations is expensive.
To alleviate reliance on labeled training data, we propose S2HAND, a
self-supervised 3D hand reconstruction network that can jointly estimate pose,
shape, texture, and the camera viewpoint. Specifically, we obtain geometric
cues from the input image through easily accessible 2D detected keypoints. To
learn an accurate hand reconstruction model from these noisy geometric cues, we
utilize the consistency between 2D and 3D representations and propose a set of
novel losses to rationalize outputs of the neural network. For the first time,
we demonstrate the feasibility of training an accurate 3D hand reconstruction
network without relying on manual annotations. Our experiments show that the
proposed method achieves comparable performance with recent fully-supervised
methods while using fewer supervision data.","Reconstructing 3D human hands from a single image is important for computer vision tasks such as handrelated action recognition, augmented reality, sign language trans lation, and humancomputer interaction [21, 33, 43]. How ever, due to the diversity of hands and the depth ambiguity in monocular 3D reconstruction, imagebased 3D hand re construction remains a challenging problem. In recent years, we have witnessed fast progress in re covering 3D representations of human hands from images. In this Ô¨Åeld, most methods were proposed to predict 3D hand pose from the depth image [1, 10, 15, 22, 49] or the *Work done during an internship at Tencent AI Lab. ‚Ä†Corresponding author: tuzhigang@whu.edu.cn Training Phase Testing Phase Input Image Output Joints 3D ReconstructionSingle View Images Detected 2D Keypoints Figure 1: Given a collection of unlabeled hand images, we learn a 3D hand reconstruction network in a selfsupervised manner. Top: the training uses a collection of unlabeled hand images and their corresponding noisy detected 2D keypoints. Bottom: our model outputs accurate hand joints and shapes, as well as vivid textures. RGB image [2, 8, 24, 37, 52]. However, the surface infor mation is needed in some applications such as grasping an object by a virtual hand [21], where the 3D hand pose rep resented by sparse joints is not sufÔ¨Åcient. To better display the surface information of the hand, previous studies pre dict the triangle mesh either via regressing pervertex coor dinate [16, 29] or by deforming a parametric hand model [19, 20]. Outputting such highdimensional representation from 2D input is challenging for neural networks to learn, thus resulting in the training process relying heavily on 3D annotations such as dense hand scans, modelÔ¨Åtted paramet ric hand mesh, or humanannotated 3D joints. Besides, the hand texture is important in some applications, such as vivid hands reconstruction in immersive virtual reality. But only recently has a study exploring parametric texture estimation in a learningbased hand recovery system [35], while most 1arXiv:2103.11703v1  [cs.CV]  22 Mar 2021Approach Supervision Outputs [35] 3DM, 3DJ, 2DKP, I, TI 3DM, 3DJ, Tex [20, 30] 3DM, 3DJ 3DM, 3DJ [48] 3DM*, 3DJ, 2DKP, 2DS, Syn 3DM, 3DJ [16] 3DM*, 3DJ, 2DKP, D* 3DM, 3DJ [29] 3DM*, D2DKP 3DM, 3DJ [51] 3DJ, 2DKP, Mo 3DM, 3DJ [3, 7, 50] 3DJ, 2DKP, 2DS 3DM, 3DJ [52] 3DJ, 2DKP, 2DS 3DJ [24] 3DJ, 2DKP 3DJ [4] 3DJ*, 2DKP, 2DS 3DM, 3DJ, Tex [37] 3DJ*, 2DKP 3DJ [8] 2DKP, D 3DJ Ours D2DKP, I 3DM, 3DJ, Tex Table 1: A comparison of some representative 3D hand recovery approaches with highlighting the differences between the supervi sion and the outputs. We use the weakest degree of supervision and output the most representations. 3DM: 3D mesh, 3DJ: 3D joints, I: input image, TI: an additional set of images with clear hand tex ture, Tex: texture, 2DKP: 2D keypoints, 2DS: 2D silhouette, D: depth, D2DKP: detected 2D keypoints, Syn: extra synthetic se quence data, Mo: extra motion capture data. * indicates that the study uses multiple datasets for training, and at least one dataset used the supervision item. previous works do not consider texture modeling. Our key observation is that the 2D cues in the image space are closely related to the 3D hand model in the real world. The 2D hand keypoints contain rich structural infor mation, and the image contains texture information. Both are important for reducing the use of expensive 3D annota tions but have not been investigated much. In this way, we could directly use 2D annotations and the input image to learn the structural and texture representations without us ing 3D annotations. However, it is still laborconsuming to annotate 2D hand keypoints. To completely save the cost of manual annotation, we propose to extract some geomet ric representations from the unlabeled image to help shape reconstruction and use the texture information contained in the input image to help texture modeling. Motivated by the above observations, this work seeks to train an accurate and robust 3D hand reconstruction net work only using supervision signals obtained from the in put images and eliminate all manual annotations, which is the Ô¨Årst attempt in this task. To this end, we use an offthe shelf 2D keypoint detector [9] to produce some noisy 2D keypoints and supervise the hand reconstruction by these noisy detected 2D keypoints and the input image. To bet ter achieve this goal, there are several issues that need to be addressed. First, how to efÔ¨Åciently use jointwise 2D keypoints to supervise the illposed monocular 3D hand re construction? Second, since our setting does not use any ground truth annotation, how do we handle the noise in the 2D detection output? To address the Ô¨Årst issue, a modelbased autoencoder is presented to estimate 3D joints and shape, where the output3D joints are projected into image space and forced to align with the detected keypoints during training. However, if we only align keypoints in image space, invalid hand poses of ten occur. This may be an invalid 3D hand conÔ¨Ågure that could be projected to be the correct 2D keypoints. Also, 2D keypoints cannot reduce the scale ambiguity of the pre dicted 3D hand. Thus, we design a series of priors em bedded in the modelbased hand representations to help the neural network output hand with a reasonable pose and size. To address the second issue, a trainable 2D keypoint es timator and a novel 2D3D consistency loss are proposed. The 2D keypoint estimator outputs jointwise 2D keypoints and the 2D3D consistency loss links the 2D keypoint esti mator and the 3D reconstruction network to make the two mutually beneÔ¨Åcial to each other during the training. In ad dition, we Ô¨Ånd that the detection accuracy of different sam ples varies greatly, thus we propose to distinguish each de tection item to weigh its supervision strength accordingly. In summary, we present a S2HAND (selfsupervised 3D hand reconstruction) model which enables us to train a neural network that can predict 3D pose, shape, texture, and camera viewpoint from a hand image without any ground truth annotation, except that we use the outputs from a 2D keypoint detector (Fig. 1). Our main contributions are summarized as follows: We present the Ô¨Årst selfsupervised 3D hand recon struction network, which accurately outputs 3D joints, mesh, and texture from a single image, without using any annotated training data. We exploit an additional trainable 2D keypoint estima tor to boost the 3D reconstruction through a mutual improvement manner, in which a novel 2D3D consis tency loss is proposed. We introduce a hand texture estimation module to learn vivid hand texture through selfsupervision. We benchmark selfsupervised 3D hand reconstruction on some currently challenging datasets, where our self supervised method achieves comparable performance to previous fullysupervised methods. 2. Related Work "
253,Estimating Instance-dependent Bayes-label Transition Matrix using a Deep Neural Network.txt,"In label-noise learning, estimating the transition matrix is a hot topic as
the matrix plays an important role in building statistically consistent
classifiers. Traditionally, the transition from clean labels to noisy labels
(i.e., clean-label transition matrix (CLTM)) has been widely exploited to learn
a clean label classifier by employing the noisy data. Motivated by that
classifiers mostly output Bayes optimal labels for prediction, in this paper,
we study to directly model the transition from Bayes optimal labels to noisy
labels (i.e., Bayes-label transition matrix (BLTM)) and learn a classifier to
predict Bayes optimal labels. Note that given only noisy data, it is ill-posed
to estimate either the CLTM or the BLTM. But favorably, Bayes optimal labels
have less uncertainty compared with the clean labels, i.e., the class
posteriors of Bayes optimal labels are one-hot vectors while those of clean
labels are not. This enables two advantages to estimate the BLTM, i.e., (a) a
set of examples with theoretically guaranteed Bayes optimal labels can be
collected out of noisy data; (b) the feasible solution space is much smaller.
By exploiting the advantages, we estimate the BLTM parametrically by employing
a deep neural network, leading to better generalization and superior
classification performance.","The study of classiÔ¨Åcation in the presence of noisy labels has been of interest for three decades (Angluin & Laird, 1988), but becomes more and more important in weakly 1University of Technology Sydney2Xidian University3Hong Kong Baptist University4Computer Science and Engineering, UC Santa Cruz5RIKEN Center for Advanced Intelligence Project 6TML Lab, Sydney AI Centre, The University of Sydney. Corre spondence to: Tongliang Liu <tongliang.liu@sydney.edu.au >. Proceedings of the 39thInternational Conference on Machine Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy right 2022 by the author(s).supervised learning (Thekumparampil et al., 2018; Li et al., 2020b; Guo et al., 2018; Xiao et al., 2015; Zhang et al., 2017a; Yang et al., 2021b;a). The main reason behind this is that datasets are becoming bigger and bigger. To improve annotation efÔ¨Åciency, these largescale datasets are often collected from crowdsourcing platforms (Yan et al., 2014), online queries (Blum et al., 2003), and image engines (Li et al., 2017), which suffer from unavoidable label noise (Yao et al., 2020a). Recent researches show that the label noise signiÔ¨Åcantly degenerates the performance of deep neural networks, since deep models easily memorize the noisy labels (Zhang et al., 2017a; Yao et al., 2020a). Generally, the algorithms for combating noisy labels can be categorized into statistically inconsistent algorithms and statistically consistent algorithms . The statistically inconsis tent algorithms are heuristic, such as selecting possible clean examples to train the classiÔ¨Åer (Han et al., 2020; Yao et al., 2020a; Yu et al., 2019; Han et al., 2018b; Malach & Shalev Shwartz, 2017; Ren et al., 2018; Jiang et al., 2018), re weighting examples to reduce the effect of noisy labels (Ren et al., 2018), correcting labels (Ma et al., 2018; Kremer et al., 2018; Tanaka et al., 2018; Wang et al., 2022), or adding regularization (Han et al., 2018a; Guo et al., 2018; Veit et al., 2017; Vahdat, 2017; Li et al., 2017; 2020b; Wu et al., 2020). These approaches empirically work well, but there is no theoretical guarantee that the learned classiÔ¨Åers can converge to the optimal ones learned from clean data. To address this limitation, algorithms in the second category aim to design classiÔ¨Åerconsistent algorithms (Yu et al., 2017; Zhang & Sabuncu, 2018; Kremer et al., 2018; Liu & Tao, 2016; Northcutt et al., 2017; Scott, 2015; Natarajan et al., 2013; Goldberger & BenReuven, 2017; Patrini et al., 2017; Thekumparampil et al., 2018; Yu et al., 2018; Liu & Guo, 2020; Xu et al., 2019; Xia et al., 2020b), where classi Ô¨Åers learned on noisy data will asymptotically converge to the optimal classiÔ¨Åers deÔ¨Åned on the clean domain. Thelabel transition matrix T(x)plays an important role in building statistically consistent algorithms. Traditionally, the transition matrix T(x)is deÔ¨Åned to relate clean distribu tion and noisy distribution, where T(x) =P(~YjY;X = x)andXdenotes the random variable of instances/features, ~Yas the variable for the noisy label, and Yas the variablearXiv:2105.13001v3  [cs.LG]  14 Jul 2022Estimating Instancedependent Bayeslabel Transition Matrix using a Deep Neural Network .05.85.05.02.03clean class posterior.25.55.05.05.10noisy class posterior01000Bayes class posterior Figure 1. The noisy class posterior is learned from noisy data. Bayes optimal label can be inferred from the noisy class posterior if the noisy rate is controlled. Also, the Bayes optimal label is less uncertain since the Bayes class posterior is onehot vector. for the clean label. The above matrix is denoted as the cleanlabel transition matrix , which is widely used to learn aclean label classiÔ¨Åer by employing the noisy data. The learned clean label classiÔ¨Åer is expected to predict a prob ability distribution over a set of predeÔ¨Åned classes given an input, i.e. clean class posterior probability P(YjX). The clean class posterior probability is the distribution from which clean labels are sampled. However, Bayes optimal labelsY,i.e., the class labels that maximize the clean class posteriors YjX:= arg max YP(YjX), are mostly used as the predicted labels and for computing clas siÔ¨Åcation accuracy. Motivated by this, in this paper, we propose to directly model the transition matrix T(x)that relates Bayes optimal distribution andnoisy distribution , i.e.,T(x) =P(~YjY;X=x), whereYdenotes the variable for Bayes optimal label . The Bayes optimal la bel classiÔ¨Åer can be learned by exploiting the Bayeslabel transition matrix directly. Studying the transition between Bayes optimal distribution and noisy distribution is considered advantageous to that of studying the transition between clean distribution and noisy distribution. The main reason is due to that the class posteriors ofBayes optimal labels areonehot vectors while those of clean labels are not. Two advantages can be in troduced by this to better estimate the instancedependent transition matrix: (a) A set of examples with theoretically guaranteed Bayes optimal labels can be collected out of noisy data . The intrinsic reason that Bayes optimal labels can be inferred from the noisy data while clean labels can not is that the Bayes optimal labels are deterministic while clean labels are stochastic ; the Bayes optimal labels are the labels that maximize theclean class posteriors while clean labels are sampled from the clean class posteriors . In the presence of label noise, the labels that maximize thenoisy class posteriors could be identical to those that maximize theclean class posteriors (Bayes optimal labels) under mild conditions; e.g., see, Cheng et al. (2020). Therefore some instances‚Äô Bayes optimal labels can be inferred from their noisy class posteriors while their clean labels are impossible to infer since the clean class posteriors are unobservable, as shown in Figure 1. (b) The feasible solution space of the Bayeslabel transition matrix is much smaller than that of the cleanlabel transition matrix. This is because that Bayes optimal labels have less uncertainty compared with the clean labels. The transition matrix deÔ¨Åned by Bayesoptimal labels and the noisy labels therefore has less hy pothesis complexity (Liu et al., 2017), and can be estimated more efÔ¨Åciently with the same amount of training data. These two advantages naturally motivate us to collect a set of examples and exploit their Bayes optimal labels to ap proximate the Bayeslabel transition matrix T(x). Due to the high complexity of the instancedependent matrix T(x), we simplify its estimation by parameterizing it us ing a deep neural network. The collected examples, inferred Bayes optimal labels, and their noisy labels are served as data points to optimize the deep neural network to approxi mate theT(x). Compared with the previous method (Xia et al., 2020a), which made assumptions and leveraged hand crafted priors to approximate the instancedependent transi tion matrices, we train a deep neural network to estimate the instancedependent label transition matrix with a reduced feasible solution space, which achieves lower approxima tion error, better generalization, and superior classiÔ¨Åcation performance. Before delving into details, we summarize our main contri butions as below: ‚Ä¢In instancedependent labelnoise learning, compared with the cleanlabel transition matrix, this paper pro poses to study the transition probabilities between Bayes optimal labels and noisy labels, i.e.,Bayeslabel transition matrix , which is easier to be parametrically learned because of the certainty and accessibility of the Bayes optimal labels. ‚Ä¢This paper proposes to leverage a deep neural network to capture the noisy patterns and generate the transi tion matrix for each input instance; it is the Ô¨Årstone that estimates the instancedependent label transition matrix in a parametric way. ‚Ä¢The effectiveness of the proposed method is veriÔ¨Åed on three synthetic noisy datasets and a largescale real world noisy dataset, signiÔ¨Åcant performance improve ments on both synthetic and realworld noisy datasets and all experiment settings are achieved. 2. Related Work "
254,Learning Debiased Classifier with Biased Committee.txt,"Neural networks are prone to be biased towards spurious correlations between
classes and latent attributes exhibited in a major portion of training data,
which ruins their generalization capability. We propose a new method for
training debiased classifiers with no spurious attribute label. The key idea is
to employ a committee of classifiers as an auxiliary module that identifies
bias-conflicting data, i.e., data without spurious correlation, and assigns
large weights to them when training the main classifier. The committee is
learned as a bootstrapped ensemble so that a majority of its classifiers are
biased as well as being diverse, and intentionally fail to predict classes of
bias-conflicting data accordingly. The consensus within the committee on
prediction difficulty thus provides a reliable cue for identifying and
weighting bias-conflicting data. Moreover, the committee is also trained with
knowledge transferred from the main classifier so that it gradually becomes
debiased along with the main classifier and emphasizes more difficult data as
training progresses. On five real-world datasets, our method outperforms prior
arts using no spurious attribute label like ours and even surpasses those
relying on bias labels occasionally.","Most supervised learning algorithms for classiÔ¨Åcation rely on the empirical risk minimization (ERM) principle [ 41]. However, ERM has been known to cause a learned classiÔ¨Åer to be biased toward spurious correlations between predeÔ¨Åned classes and latent attributes that appear in a majority of training data [ 12]. In the case of hair color classiÔ¨Åcation, for example, when most people with blondhair (i.e., target class) are female (i.e., latent attribute) in a dataset, a classiÔ¨Åer learned by ERM exploits female as a shortcut for the classiÔ¨Åcation due to its spurious correlation with blondhair , and often misclassiÔ¨Åes nonblondehaired women as blondhair in consequence. We call data with such spurious correlations and holding a majority of training data biasguiding samples , and the other biasconÔ¨Çicting samples , respectively. The issue of model bias has often been addressed by exploiting explicit spurious attribute labels [ 22,29,37,2,40,39,46] or knowledge about bias types given a priori [ 3]. However, these methods are impractical because such supervision and prior knowledge are costly, and the methods demand extensive post hoc analysis. Hence, a body of research has been conducted for learning debiased classiÔ¨Åers with no additional label for spurious attributes [ 43,28,30,33,23,27]. A common approach in this line of work is to employ an intentionally biased classiÔ¨Åer as an auxiliary module [ 30,33,23,27]. In this approach, samples that the biased classiÔ¨Åer has trouble handling are regarded as biasconÔ¨Çicting ones and assigned large weights when used for training the main classiÔ¨Åer to reduce the effect of biasguiding counterparts. Although it has driven remarkable success, this approach has drawbacks due to the use of a single biased classiÔ¨Åer. First, the quality of the biased classiÔ¨Åer could vary by hyperparameters [ 30] and its initial parameter values [ 11]. Further, data that the biased classiÔ¨Åer fails to handle could include not only biasconÔ¨Çicting samples but also biasguiding ones, which differs by the quality of the classiÔ¨Åer. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2206.10843v5  [cs.LG]  1 May 2023(a)  (b)  (c) Figure 1: Analysis on the instability of a single biased classiÔ¨Åer in mining and weighting bias conÔ¨Çicting samples. The experiments are conducted on the CelebA dataset, in which samples with blond andmale attributes are biasconÔ¨Çicting. (a) The ratio of biasconÔ¨Çicting samples to all incorrectly predicted by a single biased classiÔ¨Åer. The ratio highly Ô¨Çuctuates by the learning rate of the classiÔ¨Åer and varies up to 4%p due to different initialization even with a Ô¨Åxed learning rate, meaning that a single biased classiÔ¨Åer is sensitive to hyperparameters. (b) Disagreement on predictions of biased classiÔ¨Åers. For pairs of biased classiÔ¨Åers initialized differently, we measure the number of biasconÔ¨Çicting samples for which the classiÔ¨Åers predict differently. The results suggest that individual biased classiÔ¨Åers are sensitive to initialization. (c) Comparisons between a single biased classiÔ¨Åer and a committee of biased classiÔ¨Åers in terms of enrichment [ 30]. Higher enrichment implies more precise mining and weighting of biasconÔ¨Çicting samples; the formal deÔ¨Ånition of enrichment is given in Appendix A.2. The committee clearly outperforms the single biased classiÔ¨Åer in terms of the enrichment. These drawbacks limit the reliability and performance of debiasing methods depending on a single biased classiÔ¨Åer, as demonstrated in Figure 1. To overcome these limitations, we propose a new method using a committee of biased classiÔ¨Åers as the auxiliary module, coined learning with biased committee (LWBC). LWBC identiÔ¨Åes biasconÔ¨Çicting samples and determines their weights through consensus on their prediction difÔ¨Åculty within the committee. To this end, the committee is built as a bootstrapped ensemble, i.e., each of its classiÔ¨Åers is trained from a randomly sampled subset of the entire training dataset. This strategy not only guarantees the diversity among the classiÔ¨Åers, but also lets a majority of the classiÔ¨Åers be biased since random subsets of training data are highly likely to be dominated by biasguiding samples. Accordingly, a majority of the committee tends to classify biasguiding samples correctly and fail to deal with biasconÔ¨Çicting ones. The consensus on prediction difÔ¨Åculty within the committee thus gives a strong cue for identifying and weighting biasconÔ¨Çicting samples. Also, using the consensus of multiple classiÔ¨Åers enables LWBC to be robust to the varying quality of individual classiÔ¨Åers and consequently to focus more precisely on biasconÔ¨Çicting samples, as shown in Figure 1. Moreover, unlike the biased classiÔ¨Åer trained independently of the main classiÔ¨Åer in the previous work, the committee in LWBC is trained with knowledge of the main classiÔ¨Åer as well as the random subsets of training data to serve the main classiÔ¨Åer better. SpeciÔ¨Åcally, the knowledge is distilled in the form of classiÔ¨Åcation logits of the main classiÔ¨Åer [ 18], and each classiÔ¨Åer of the committee utilizes the knowledge as pseudo labels of training data other than its own training set. We expect that this strategy allows the committee to become debiased gradually so that it does not give large weights to easy biasconÔ¨Çicting samples, i.e., those already well handled by the main classiÔ¨Åer, and focuses more on difÔ¨Åcult ones. Note that, even with this strategy, the classiÔ¨Åers of the committee are still biased differently due to their different training sets with groundtruth labels. Finally, we further improve the proposed method by adopting a selfsupervised representation as the frozen backbone of the committee and the main classiÔ¨Åer. Since selfsupervised learning is not dependent on class labels, it is less affected by the spurious correlations between classes and latent attributes, leading to a robust and lessbiased representation. Also, by installing the committee and the main classiÔ¨Åer on top of the representation, the classiÔ¨Åers can be implemented efÔ¨Åciently in both space and time while enjoying the rich and biasfree features given by the backbone. LWBC is validated extensively on Ô¨Åve realworld datasets. It substantially outperforms existing methods using no bias label and even occasionally surpasses previous arts demanding bias labels. We also demonstrate that all of the main components, i.e., the use of the committee, its training with 2knowledge transfer, and the selfsupervised learning, contribute to the outstanding performance. The main contribution of this paper is fourfold: ‚Ä¢We present LWBC, a new approach to learning a debiased classiÔ¨Åer with no spurious attribute label. The use of consensus within the committee allows LWBC to address limitations of previous work relying on a single biased classiÔ¨Åer. ‚Ä¢We propose to learn the committee using knowledge of the main classiÔ¨Åer, unlike the previous work whose auxiliary modules do not consider the main classiÔ¨Åer. ‚Ä¢We investigate the potential of selfsupervised learning for debiasing, and Ô¨Ånd that it is a solid yet unexplored baseline for the task. ‚Ä¢LWBC demonstrates superior performance on Ô¨Åve realworld datasets. It outperforms existing methods using no additional supervision like ours and even surpasses those relying on spurious attribute labels occasionally. 2 Related work "
255,Learning from Noisy Labels with Decoupled Meta Label Purifier.txt,"Training deep neural networks(DNN) with noisy labels is challenging since DNN
can easily memorize inaccurate labels, leading to poor generalization ability.
Recently, the meta-learning based label correction strategy is widely adopted
to tackle this problem via identifying and correcting potential noisy labels
with the help of a small set of clean validation data. Although training with
purified labels can effectively improve performance, solving the meta-learning
problem inevitably involves a nested loop of bi-level optimization between
model weights and hyper-parameters (i.e., label distribution). As compromise,
previous methods resort to a coupled learning process with alternating update.
In this paper, we empirically find such simultaneous optimization over both
model weights and label distribution can not achieve an optimal routine,
consequently limiting the representation ability of backbone and accuracy of
corrected labels. From this observation, a novel multi-stage label purifier
named DMLP is proposed. DMLP decouples the label correction process into
label-free representation learning and a simple meta label purifier. In this
way, DMLP can focus on extracting discriminative feature and label correction
in two distinctive stages. DMLP is a plug-and-play label purifier, the purified
labels can be directly reused in naive end-to-end network retraining or other
robust learning methods, where state-of-the-art results are obtained on several
synthetic and real-world noisy datasets, especially under high noise levels.","Deep learning has achieved signiÔ¨Åcant progress on vari ous recognition tasks. The key to its success is the availabil Yuanpeng Tu, Boshen Zhang, Yuxi Li contribute equally to this work. Fully  supervisionùë•ùë°Update weights:ùë§ùë°‚Ä≤ ùêøùë£ùëéùëôSelf supervision ùë§ùë°‚àóCoupled  optimization loop Decouplingùêøùë£ùëéùëô (a) Traditional (b) DMLPùë¶ùë° ùë•ùë° ùë¶ùë°ùë•ùë£ùë¶ùë£ ùë•ùë£ùë¶ùë£Updated label :ùë¶ùë°‚Ä≤ ùë¶ùë°‚àóùë¶ùë°‚àó Fixed weightsUpdate labe lFigure 1. (a) Traditional coupled alternating update to solve meta label puriÔ¨Åcation problem, and (b) the proposed DMLP method that decouples the label puriÔ¨Åcation process into representation learning and a simple nonnested meta label puriÔ¨Åer. ity of largescale datasets with reliable annotations. Collect ing such datasets, however, is timeconsuming and expensive. Easy ways to obtain labeled data, such as web crawling [31], inevitably yield samples with noisy labels, which is not appropriate to be directly utilized to train DNN since these complex models are vulnerable to memorize noisy labels [2]. Towards this problem, numerous Learning with Noisy Label (LNL) approaches were proposed. Classical LNL methods focus on identifying the noisy samples and reduc ing their effect on parameter updates by abandoning [12] or assigning smaller importance. However, when it comes to extremely noisy and complex scenarios, such scheme struggles since there is no sufÔ¨Åcient clean data to train a dis criminative classiÔ¨Åer. Therefore, label correction approaches are proposed to augment clean training samples by revising noisy labels to underlying correct ones. Among them, meta learning based approaches [9, 16, 25] achieve stateoftheart performance via resorting to a small clean validation setarXiv:2302.06810v3  [cs.CV]  17 Feb 2023and taking noisy labels as hyperparameters, which provides sound guidance toward underlying label distribution of clean samples. However, such meta puriÔ¨Åcation inevitably in volves a nested bilevel optimization problem on both model weight and hyperparameters (shown as Fig. 1 (a)), which is computationally infeasible. As a compromise, the alternat ing update between model weights and hyperparameters is adopted to optimize the objective [9, 16, 25], resulting in a coupled solution for both representation learning and label puriÔ¨Åcation. Empirical observation. Intuitively, alternate optimiza tion over a large search space (model weight and hyper parameters) may lead to suboptimal solutions. To investi gate how such approximation affects results in robust learn ing, we conduct empirical analysis on CIFAR10 [14] with recent label puriÔ¨Åcation methods MLC [40] and MSLC [9], which consist of a deep model and a meta label correction network, and make observation as Fig. 2. Coupled optimization hinders quality of corrected la bels. We Ô¨Årst compare the Coupled meta corrector MLC with its extremely Decoupled variant where the model weights are Ô¨Årst optimized for 70epochs with noisy labels and get Ô¨Åxed, then labels are puriÔ¨Åed with the guidance of validation set. We adopt the accuracy of corrected label to measure the performance of puriÔ¨Åcation. From Fig. 2 (a), we can clearly observe that compared with Decoupled counterpart, joint optimization yields inferior correction per formance, and these miscorrection will reversely affect the representation learning in coupled optimization. Coupled optimization hinders representation ability. We investigate the representation quality by evaluating the linear prob accuracy [6] of extracted feature in Fig. 2 (b). We Ô¨Ånd the representation quality of Coupled training is much worse at the beginning, which leads to slow and un stable representation learning in the later stage. To further investigate the effect on representation learning, we also resort to a well pretrained backbone with selfsupervised learning [5] as initialization, recent research [39] shows pre trained representation is substantially helpful for LNL frame work. However, we Ô¨Ånd this conclusion does not strictly hold for coupled meta label correctors. As shown in Fig. 2 (c), by comparing the classiÔ¨Åcation accuracy from classi Ô¨Åer of MLC/MSLC, we observe the pretrained model only brings marginal improvement if model weights is still cou pled with hyperparameters. In contrast, when the weight of backbone is Ô¨Åxed and decoupled from the label puriÔ¨Åcation and classiÔ¨Åer, the improvement becomes more signiÔ¨Åcant. Decoupled Meta PuriÔ¨Åcation. From the observation above, we Ô¨Ånd the decoupling between model weights and hyperparameters of meta correctors is essential to label ac curacy and Ô¨Ånal results. Therefore, in this paper, we aim at detaching the meta label puriÔ¨Åcation from representation learning and designing a simple meta label puriÔ¨Åer whichis more friendly to optimization of label distribution prob lem than existing complex meta networks [9, 40]. Hence we propose a general multistage label correction strategy, named Decoupled Meta Label PuriÔ¨Åer (DMLP). The core of DMLP is a metalearning based label puriÔ¨Åer, however, to avoid solving the bilevel optimization with a coupled solution, DMLP decouples this process into selfsupervised representation learning and a linear metalearner to Ô¨Åt un derlying correct label distribution (illustrated as Fig. 1 (b)), thus simpliÔ¨Åes the label puriÔ¨Åcation stage as a singlelevel optimization problem. The simple metalearner is carefully designed with two mutually reinforcing correcting processes, named intrinsic primary correction (IPC) and extrinsic aux iliary correction (EAC) respectively. IPC plays the role of purifying labels in a global sense at a steady pace, while EAC targets at accelerating the puriÔ¨Åcation process via looking ahead (i.e., training with) the updated labels from IPC. The two processes can enhance the ability of each other and form a positive loop of label correction. Our DMLP framework is Ô¨Çexible for application, the puriÔ¨Åed labels can either be directly applied for naive endtoend network retraining, or exploited to boost the performance of existing LNL frame works. Extensive experiments conducted on mainstream benchmarks, including synthetic (noisy versions of CIFAR) and realworld (Clothing1M) datasets, demonstrate the supe riority of DMLP. In a nutshell, the key contributions of this paper include: We analyze the necessity of decoupled optimization for label correction in robust learning, based on which we propose DMLP, a Ô¨Çexible and novel multistage label puriÔ¨Åer that solves bilevel metalearning problem with a decoupled manner, which consists of representation learning and non nested meta label puriÔ¨Åcation; In DMLP, a novel nonnested meta label puriÔ¨Åer equipped with two correctors, IPC and EAC is proposed. IPC is a global and steady corrector, while EAC accelerates the correction process via training with the updated labels from IPC. The two processes form a positive training loop to learn more accurate label distribution; Deep models trained with puriÔ¨Åed labels from DMLP achieve stateoftheart results on several synthetic and real world noisy datasets across various types and levels of label noise, especially under high noise levels. Extensive ablation studies are provided to verify the effectiveness. 2. Related Works "
256,Attend in groups: a weakly-supervised deep learning framework for learning from web data.txt,"Large-scale datasets have driven the rapid development of deep neural
networks for visual recognition. However, annotating a massive dataset is
expensive and time-consuming. Web images and their labels are, in comparison,
much easier to obtain, but direct training on such automatically harvested
images can lead to unsatisfactory performance, because the noisy labels of Web
images adversely affect the learned recognition models. To address this
drawback we propose an end-to-end weakly-supervised deep learning framework
which is robust to the label noise in Web images. The proposed framework relies
on two unified strategies -- random grouping and attention -- to effectively
reduce the negative impact of noisy web image annotations. Specifically, random
grouping stacks multiple images into a single training instance and thus
increases the labeling accuracy at the instance level. Attention, on the other
hand, suppresses the noisy signals from both incorrectly labeled images and
less discriminative image regions. By conducting intensive experiments on two
challenging datasets, including a newly collected fine-grained dataset with Web
images of different car models, the superior performance of the proposed
methods over competitive baselines is clearly demonstrated.",2. Related Work 2 
257,Essence Knowledge Distillation for Speech Recognition.txt,"It is well known that a speech recognition system that combines multiple
acoustic models trained on the same data significantly outperforms a
single-model system. Unfortunately, real time speech recognition using a whole
ensemble of models is too computationally expensive. In this paper, we propose
to distill the knowledge of essence in an ensemble of models (i.e. the teacher
model) to a single model (i.e. the student model) that needs much less
computation to deploy. Previously, all the soften outputs of the teacher model
are used to optimize the student model. We argue that not all the outputs of
the ensemble are necessary to be distilled. Some of the outputs may even
contain noisy information that is useless or even harmful to the training of
the student model. In addition, we propose to train the student model with a
multitask learning approach by utilizing both the soften outputs of the teacher
model and the correct hard labels. The proposed method achieves some surprising
results on the Switchboard data set. When the student model is trained together
with the correct labels and the essence knowledge from the teacher model, it
not only significantly outperforms another single model with the same
architecture that is trained only with the correct labels, but also
consistently outperforms the teacher model that is used to generate the soft
labels.","Automatic speech recognition (ASR), especially nearÔ¨Åeld speech recognition, has achieved great progress in recent years [1, 2]. But the problem of lowresource (i.e. limited training data) speech recognition is ubiquitous since a large amount of annotated data is not available for most languages used in the world. How to train an accurate model with limited training data remains a challenging problem. Various methods have been proposed to fully utilize the limited training data to improve the recognition accuracy of the speech recognition system trained on it. Data augmentation has been shown to be an simple yet ef fective approach to increase the quantity and diversity of the data [3]. It has almost become part of the standard pipeline in data preprocessing for speech recognition. Corrupting clean data with noise [4], vocal tract length perturbation (VTLP [5]), speedperturbation [6] have been widely adopted to improve the performance. Model fusion is another technique to combine information at the other level of the acoustic modeling pipeline. By combin ing neural networks with distinct, complementary architectures [7, 8, 9], the whole system is able to capitalize on each archi tectures strengths to improve the system accuracy. However, usually it is impossible to deploy such a fused model to a large # Both authors contribute equally to this work.number of users since the computation is too expensive, espe cially if the individual models are large and complicated neural nets such as Long ShortTerm Memory (LSTM [10]) and Con volutional LSTM Deep Neural Networks (CLDNN [11]). An effective method to deal with this problem is Knowledge Distil lation (KD [12]). Knowledge Distillation (KD) is a special transfer learning technology. It involves training a new model which is usu ally called the student orstudent model . The knowledge in a welltrained model (the teacher model , usually an ensemble of models) is compressed and transferred to the student model. This student model can be trained on a separate transfer data set without correct labels or on the original training data set where correct labels are available. Various strategies have been proposed to compress and dis till the knowledge of the teacher model to the student model. In the simplest form, the knowledge from the teacher model is distilled to the student model by training it with soften output probabilities produced by the teacher model [12, 13]. Instead of combining the outputs of individual teacher model from an ensemble, Fukuda et.al. [9] proposed to update the parame ters of the student by randomly switching teacher labels at the minibatch level, or to train the student model on multiple output labels from various teacher models. Huang et.al. [14] proposed to use sequencelevel knowledge distillation instead of frame level knowledge distillation. Similar sequencelevel distillation was also explored in [15]. In the above research, none of the dis tilled student models are able to outperform their corresponding teacher models. There are still a big gap between the teacher models and the student models in terms of recognition accu racy. In our opinion, one of the reasons is that only the outputs of the teacher model are used to train the student model. The other reason might be that the student model is driven to learn everything from the teacher model, including not only valuable information that is beneÔ¨Åcial to the generalization of the student model but also garbage information that is noisy and harmful. To deal with this problem, we propose to train the student model with a multitask learning approach in order to utilize the correct hard labels. In addition, we propose to only dis till the essence knowledge of the teacher model to the student model. One obvious problem is how to select the knowledge produced by the teacher model? We propose a simple yet ef fective method to select valuable information from the outputs of the teacher model. The student model is trained with cor rect labels and the selected knowledge from the teacher model. Surprisingly, the student model (with much less model param eters than the teacher model) trained with the proposed method consistently outperforms the teacher model. The rest of this paper is organized as follows: Section 2 introduces the technologies used in our experiments, includ ing data augmentation, model fusion and knowledge distilla tion. We introduce a method to select salient information forarXiv:1906.10834v1  [cs.CL]  26 Jun 2019the outputs of the teacher model. The experimental setup and results are presented in Section 3. Finally, a simple conclusion is drawn in Section 4. 2. Methodology "
258,Semi-supervised Acoustic Event Detection based on tri-training.txt,"This paper presents our work of training acoustic event detection (AED)
models using unlabeled dataset. Recent acoustic event detectors are based on
large-scale neural networks, which are typically trained with huge amounts of
labeled data. Labels for acoustic events are expensive to obtain, and relevant
acoustic event audios can be limited, especially for rare events. In this paper
we leverage an Internet-scale unlabeled dataset with potential domain shift to
improve the detection of acoustic events. Based on the classic tri-training
approach, our proposed method shows accuracy improvement over both the
supervised training baseline, and semisupervised self-training set-up, in all
pre-defined acoustic event detection tasks. As our approach relies on ensemble
models, we further show the improvements can be distilled to a single model via
knowledge distillation, with the resulting single student model maintaining
high accuracy of teacher ensemble models.","Acoustic event detection (AED) is the task of detecting whether certain events occur in an audio clip. It can be applied in many areas such as surveillance [1, 2], and recommenda tion systems [3]. Conventionally AED has been addressed with automatic speech recognition techniques, e.g. with fea tures such as melfrequency cepstrum coefÔ¨Åcients (MFCC) and classiÔ¨Åers based on hidden markov model (HMM). In recent years, with the advances in speech recognition [4] and image recognition [5] as well as size increasing of datasets [6][7], there are more deep learning based approaches applied to tackle AED tasks. For instance, the recently proposed Au dioset [6] comprises 1,789,621 10second audio segments from a wide domain of 632 categories. Convolutional neu ral network (CNN) [8, 9] or CNNbased approaches (e.g convolutional recurrent neural network [10, 11]) are used and have shown improvements over traditional approaches. Though accuracy has been much improved in many AED tasks, stateoftheart models often requires large number oflabeled training data. Labeled data can be be quite limited under certain scenarios (e.g., for rare events [12]). The focus of this paper is on leveraging unlabeled audios to improve accuracy for AED. Our main contributions include the following: (1). We propose an ensemble method based on the classic tritraining that shows improvements in all acoustic events we investigate in a realistic semisupervised setting (Internetscale unlabeled dataset with domain shift) (2). We show the improvements of the ensembled models can be distilled into a single model via knowledge distillation. As a result, there is no increase of computational costs during inference. 2. RELATED WORK "
259,Towards Unsupervised Deep Graph Structure Learning.txt,"In recent years, graph neural networks (GNNs) have emerged as a successful
tool in a variety of graph-related applications. However, the performance of
GNNs can be deteriorated when noisy connections occur in the original graph
structures; besides, the dependence on explicit structures prevents GNNs from
being applied to general unstructured scenarios. To address these issues,
recently emerged deep graph structure learning (GSL) methods propose to jointly
optimize the graph structure along with GNN under the supervision of a node
classification task. Nonetheless, these methods focus on a supervised learning
scenario, which leads to several problems, i.e., the reliance on labels, the
bias of edge distribution, and the limitation on application tasks. In this
paper, we propose a more practical GSL paradigm, unsupervised graph structure
learning, where the learned graph topology is optimized by data itself without
any external guidance (i.e., labels). To solve the unsupervised GSL problem, we
propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME
for abbreviation) with the aid of self-supervised contrastive learning.
Specifically, we generate a learning target from the original data as an
""anchor graph"", and use a contrastive loss to maximize the agreement between
the anchor graph and the learned graph. To provide persistent guidance, we
design a novel bootstrapping mechanism that upgrades the anchor graph with
learned structures during model learning. We also design a series of graph
learners and post-processing schemes to model the structures to learn.
Extensive experiments on eight benchmark datasets demonstrate the significant
effectiveness of our proposed SUBLIME and high quality of the optimized graphs.","Recent years have witnessed the prosperous development of graph based applications in numerous domains, such as chemistry, bioin formatics and cybersecurity. As a powerful deep learning tool to model graphstructured data, graph neural networks (GNNs) have drawn increasing attention and achieved stateoftheart perfor mance in various graph analytical tasks, including node classifica tion [ 22,40], link prediction [ 21,32], and node clustering [ 42,55]. GNNs usually follow a messagepassing scheme, where node repre sentations are learned by aggregating information from the neigh bors on an observed topology (i.e., the original graph structure). Most GNNs rely on a fundamental assumption that the original structure is credible enough to be viewed as groundtruth informa tion for model training. Such assumption, unfortunately, is usually violated in realworld scenarios, since graph structures are usu ally extracted from complex interaction systems which inevitably contain uncertain, redundant, wrong and missing connections [ 45]. Such noisy information in original topology can seriously damage the performance of GNNs. Besides, the reliance on explicit struc tures hinders GNNs‚Äô broad applicability. If GNNs are capable of uncovering the implicit relations between samples, e.g., two images containing the same object, they can be applied to more general domains like vision and language. To tackle the aforementioned problems, deep graph structure learning (GSL) is a promising solution that constructs and improves the graph topology with GNNs [ 7,12,20,58]. Concretely, these methods parameterize the adjacency matrix with a probabilistic model [ 12,45], full parameterization [ 20] or metric learning model [7,11,53], and jointly optimize the parameters of the adjacency matrix and GNNs by solving a downstream task (i.e., node classifi cation) [ 58]. However, existing methods learn graph structures in a supervised scenario, which brings the following issues: (1) The reliance on label information. In supervised GSL methods, human annotated labels play an important role in providing supervision signal for structure improvement. Such reliance on labels limits the application of supervised GSL on more general cases where annotation is unavailable. (2) The bias of learned edge distribution. Node classification usually follows a semisupervised setting, where only a small fraction of nodes (e.g., 140/2708 in Cora dataset) are under the supervision of labels. As a result, the connections among these nodes and their neighbors would receive more guidance inarXiv:2201.06367v1  [cs.LG]  17 Jan 2022WWW ‚Äô22, April 25‚Äì29, 2022, Lyon, France. Liu et al. Node Labels DataLearnedGraph ‚Ä¶‚Ä¶‚Ä¶GNNbased Model‚Ä¶‚Ä¶‚Ä¶ImproveSuperviseInput DataInputLearnedGraphImproveGNNbased ModelBenefitBenefitNode ClassificationNode ClusteringLinkPrediction‚Ä¶Downstream TasksNode ClassificationTask for Supervision (a) Supervised GSL paradigm. Node Labels DataLearnedGraph ‚Ä¶‚Ä¶‚Ä¶GNNbased Model‚Ä¶‚Ä¶‚Ä¶ImproveSuperviseInput DataInputLearnedGraphImproveGNNbased ModelBenefitBenefitNode ClassificationNode ClusteringLinkPrediction‚Ä¶Downstream TasksNode ClassificationTask for Supervision (b) Our proposed unsupervised GSL paradigm. Figure 1: Concept maps of (a) the existing supervised GSL paradigm and (b) our proposed unsupervised GSL paradigm. structure learning, while the relations between nodes far away from them are rarely discovered by GSL [ 11]. Such imbalance leads to the bias of edge distribution, affecting the quality of the learned structures. (3) The limitation on downstream tasks. In existing meth ods, the structure is specifically learned for node classification, so it may contain more taskspecific information rather than general knowledge. Consequently, the refined topology may not benefit other downstream tasks like link prediction or node clustering, indicating the poor generalization ability of the learned structures. To address these issues, in this paper, we investigate a novel un supervised learning paradigm for GSL, namely unsupervised graph structure learning . As compared in Fig. 1, in our learning paradigm, structures are learned by data itself without any external guidance (i.e., labels), and the acquired universal, edgeunbiased topology can be freely applied to various downstream tasks. In this case, one natural question can be raised: how to provide sufficient supervi sion signal for unsupervised GSL? To answer this, we propose a novel StrUcture Bootstrapping contrastive LearnIng fra MEwork (SUBLIME for abbreviation) to learn graph structures with the aid of selfsupervised contrastive learning [ 25]. Concretely, our method constructs an ‚Äúanchor graph‚Äù from the original data to guide struc ture optimization, with a contrastive loss to maximize the mutual information (MI) between anchor graph and the learned structure. Through maximizing their consistency, informative hidden connec tions can be discovered, which well respects the node proximity conveyed by the original features and structures. Meanwhile, as we optimize the contrastive loss on the representations of every node, all potential edge candidates will receive the essential super vision, which promotes a balanced edge distribution in the inferred topology. Furthermore, we design a bootstrapping mechanism to update anchor graph with the learned edges, which provides a selfenhanced supervision signal for GSL. Besides, we carefully de sign multiple graph learners and postprocessing schemes to model graph topology for diverse data. In summary, our core contributions are threefold: ‚Ä¢Problem. We propose a novel unsupervised learning para digm for graph structure learning, which is more practical and challenging than the existing supervised counterpart. To the best of our knowledge, this is the first attempt to learn graph structures with GNNs in an unsupervised setting. ‚Ä¢Algorithm. We propose a novel unsupervised GSL method SUBLIME , which guides structure optimization by maximiz ing the agreement between the learned structure and a crafted selfenhanced learning target with contrastive learning.‚Ä¢Evaluations. We perform extensive experiments to corrobo rate the effectiveness and analyze the properties of SUBLIME via thorough comparisons with stateoftheart methods on eight benchmark datasets. 2 RELATED WORK "
260,A Hierarchical Matcher using Local Classifier Chains.txt,"This paper focuses on improving the performance of current convolutional
neural networks in visual recognition without changing the network
architecture. A hierarchical matcher is proposed that builds chains of local
binary neural networks after one global neural network over all the class
labels, named as Local Classifier Chains based Convolutional Neural Network
(LCC-CNN). The signature of each sample as two components: global component
based on the global network; local component based on local binary networks.
The local networks are built based on label pairs created by a similarity
matrix and confusion matrix. During matching, each sample travels through one
global network and a chain of local networks to obtain its final matching to
avoid error propagation. The proposed matcher has been evaluated with image
recognition, character recognition and face recognition datasets. The
experimental results indicate that the proposed matcher achieves better
performance when compared with methods using only a global deep network.
Compared with the UR2D system, the accuracy is improved significantly by 1% and
0.17% on the UHDB31 dataset and the IJB-A dataset, respectively.","Visual recognition is one of the hottest topics in the Ô¨Åelds of computer vision and machine learning. In recent years, many deep learning models have been built to set new stateoftheart results in image classiÔ¨Åcation, object detection and many other visual recognition tasks [1, 2, 3]. Among these tasks, most of the breakthroughs are achieved with deep Convolutional Neural Networks (CNN) [4]. CNN was Ô¨Årst proposed in the late 1990s by LeCun et al. [4, 5]. It was quickly overwhelmed by the combination of other shallow descriptors (such as SIFT [6], HOG [7], bag of words [8]) with Support Vector Machine (SVM) [9]. In recent Preprint submitted to Elsevier May 8, 2018arXiv:1805.02339v1  [cs.CV]  7 May 2018Figure 1: An example of the proposed LCCCNN matcher is depicted with the combination of global model and local models. The matching paths of the two testing images are indicated in red and cyan, respectively. years, with the increase of image recognition data size and computation power, CNN is becoming more and more popular and dominant. Krizhevsky et al. [10] proposed the classic eight layer CNN model (AlexNet) with Ô¨Åve convolutional and three fully connected layers. The model is trained via backpropagation through layers and performs extremely well in domains with a large amount of training data. Since then, many new CNN models have been constructed with larger sizes and different architectures to improve the performance. A series of improvements were achieved by VGG [11], GooLeNet [12], ResNet [13, 14] and so on. How ever, a larger model creates a larger number of parameters and larger computa tional complexity. Methods for compressing network and accelerating training and testing computation have also been developed [15, 16, 17, 18]. Overall, the previous deep face networks have two limitations. (a) data size: training a larger size global model requires more training data, which can be costly and not applicable in certain applications. (b) local information: one deep neural network built over all the class labels may ignore the pairwise local correlations between different labels, which can be used to improve overall performance. This paper overcomes the limitations (a) and (b) by introducing a hierarchi cal matcher that builds chains of local binary CNN classiÔ¨Åers after the global CNN classiÔ¨Åer over all the class labels. Moreover, it is a method to improve face recognition performance without changing the architecture of the CNN network. 2Hereafter, these two types of classiÔ¨Åers are referred as local model and global model. The motivation behind this is that a global model focuses more on the global discriminative features over all the class labels and tends to misclassify samples from visually similar classes. With fewer labels, a local model can ex ploit more local discriminative features for the related labels and can be used to correct the matching result of the global model. Especially when the same train ing data and network architecture are used for both global and local models, each local model converges fast and achieves better accuracy than the global model. Also, local models can be trained in parallel, which avoids excessive increase in computational complexity. In addition, when data size is limited, a local model can explore more pairwise label correlations than the global model. To limit the complexity of the proposed matcher, only binary local models are built in this paper. Take CIFAR10 dataset [19] for example. Figure 1 depicts the intuition of the proposed matcher. It can be observed that for the ‚Äúdog‚Äù image, the binary local model between ‚Äúcat‚Äù label and ‚Äúdog‚Äù label is used to correct the mistake of the global model. Importantly, local models can be built one after another, which leads to a chain of local models to boost performance and avoid error propaga tion. For the ‚Äúdeer‚Äù image, a chain of two local models (between ‚Äúdog‚Äù label and ‚Äúhorse‚Äù label, ‚Äúdog‚Äù label and ‚Äúdeer‚Äù label) are built to improve the matching of the global model. The contributions of this paper are improving recognition performance by the following two techniques: (i) fully exploring training data by proposing a hier archical matcher, where the contributions of global model and local models are combined. (ii) making use of pairwise label information to local model chains, which adaptively select a small set of label pairs to build local models. The pair wise correlations between different labels are learned based on their relationships in the score matrices. These correlations are not well explored in global model based methods. Parts of this work on face recognition have been published in Zhang et al. [20]. In this paper, it is extended by providing: (i) a signature to store image information with global model and local model components; (ii) the Ô¨Ånetuning process of local models; (iii) more general applications of image recognition and character recognition; (iv) the evaluation on the poseinvariant 3Daided 2D face recognition system (UR2D) [21]. The rest of this paper is organized as follows: Section 2 presents related work. Section 3 and Section 4 describe the signature and the hierarchical matcher. The experimental design, results, and analysis are presented in Section 5. Section 6 concludes the paper. 32. Related work "
261,Person Re-Identification by Deep Joint Learning of Multi-Loss Classification.txt,"Existing person re-identification (re-id) methods rely mostly on either
localised or global feature representation alone. This ignores their joint
benefit and mutual complementary effects. In this work, we show the advantages
of jointly learning local and global features in a Convolutional Neural Network
(CNN) by aiming to discover correlated local and global features in different
context. Specifically, we formulate a method for joint learning of local and
global feature selection losses designed to optimise person re-id when using
only generic matching metrics such as the L2 distance. We design a novel CNN
architecture for Jointly Learning Multi-Loss (JLML) of local and global
discriminative feature optimisation subject concurrently to the same re-id
labelled information. Extensive comparative evaluations demonstrate the
advantages of this new JLML model for person re-id over a wide range of
state-of-the-art re-id methods on five benchmarks (VIPeR, GRID, CUHK01, CUHK03,
Market-1501).","Person reidentiÔ¨Åcation (reid) is about matching identity classes in detected person bounding box images from non overlapping camera views over distributed open spaces. This is an inherently challenging task because person visual ap pearance may change dramatically in different camera views from different locations due to unknown changes in human pose, illumination, occlusion, and background clutter [Gong et al. , 2014 ]. Existing person reid studies typically focus on either feature representation [Gray and Tao, 2008; Faren zena et al. , 2010; Kviatkovsky et al. , 2013; Zhao et al. , 2013; Liao et al. , 2015; Matsukawa et al. , 2016a; Ma et al. , 2017 ] or matching distance metrics [Koestinger et al. , 2012; Xiong et al. , 2014; Zheng et al. , 2013; Wang et al. , 2014b; Paisitkri angkrai et al. , 2015; Zhang et al. , 2016; Wang et al. , 2016b; Wang et al. , 2016c; Wang et al. , 2016d; Chen et al. , 2017b ]or their combination in deep learning framework [Liet al. , 2014; Ahmed et al. , 2015; Wang et al. , 2016a; Xiao et al. , 2016; Subramaniam et al. , 2016; Chen et al. , 2017a ]. Regard less, the overall objective is to obtain a view and locationinvariant (crossdomain) representation. We consider that learning any matching distance metric is intrinsically learn ing a global feature transformation across domains (two dis joint camera views) therefore obtaining a ‚Äúnormalised‚Äù fea ture representation for matching. Most reid features are typically handcrafted to encode local topological and/or spatial structural information, by different image decomposition schemes such as horizontal stripes [Gray and Tao, 2008; Kviatkovsky et al. , 2013 ], body parts [Farenzena et al. , 2010 ], and patches [Zhao et al. , 2013; Matsukawa et al. , 2016a; Liao et al. , 2015 ]. These lo calised features are effective for mitigating the person pose and detection misalignment in reid matching. More recent deep reid models [Xiao et al. , 2016; Wang et al. , 2016a; Chen et al. , 2017a; Ahmed et al. , 2015 ]beneÔ¨Åt from the availability of larger scale datasets such as CUHK03 [Liet al., 2014 ]and Market1501 [Zheng et al. , 2015 ]and from lessons learned on other vision tasks [Krizhevsky et al. , 2012; Girshick et al. , 2014 ]. In contrast to local handcrafted fea tures, deep models, in particular Convolutional Neural Net works (CNN) [LeCun et al. , 1998 ], favour intrinsically in learning global feature representations with a few exceptions. They have been shown to be effective for reid. We consider that either local or global feature learning alone is suboptimal. This is motivated by the human vi sual system that leverages both global (contextual) and local (saliency) information concurrently [Navon, 1977; Torralba et al. , 2006 ]. This intuition for joint learning aims to ex tract correlated complementary information in different con text whilst satisfying the same learning constraint1therefore achieving more reliable recognition. To that end, we need to address a number of nontrivial problems: (i) the model learn ing behaviour in satisfying the same label constraint may be different at the local and global levels; (ii) any complemen tary correlation between local and global features is unknown and may vary among individual instances, therefore must be learned and optimised consistently across data; (iii) People‚Äôs appearance in public scenes is diverse in both pattens and conÔ¨Ågurations. This makes it challenging to learn correla tions between local and global features for all appearances . This work aims to formulate a deep learning model for 1In person reid context, the learning constraint refers to the im age person identity label supervision.arXiv:1705.04724v2  [cs.CV]  23 May 2017jointly optimising local and global feature selections concur rently and to improve person reid using only generic match ing metrics such as the L2 distance. We explore a deep learn ing approach for its potential superiority in learning from large scale data [Xiao et al. , 2016; Chen et al. , 2017a ]. For the bounding box image based person reid, we consider the entire person in the bounding box as a global scene context and body parts of the person as local information sources , both are subject to the surrounding background clutter within a bounding box, and potentially also misalignment and partial occlusion from bounding box detection. In this setting, we wish to discover and optimise jointly correlated complemen tary feature selections in the local and global representations, both subject to the same label constraint concurrently. Whilst the former aims to address pose/detection misalignment and occlusion by localised Ô¨Ånegrained saliency information, the latter exploits holistic coarsegrained context for more robust global matching. To that end, we formulate a deep twobranch CNN archi tecture, with one branch for learning localised feature se lection (local branch) and the other for learning global fea ture selection (global branch). Importantly, the two branches are not independent but synergistically correlated and jointly learned concurrently. This is achieved by: (i) imposing inter branch interaction between the local and global branches, and (ii) enforcing a separate learning objective loss function to each branch for learning independent discriminative capabil ities, whilst being subject to the same class label constraint. Under such balancing between interaction and independence, we allow both branches to be learned concurrently for max imising their joint optimal extraction and selection of dif ferent discriminative features for person reid. We call this model the Joint Learning MultiLoss (JLML) CNN model. To minimise poor learning due to inherent noise and potential covariance, we introduce a structured feature selective and discriminative learning mechanism into both the local and global branches subject to a joint sparsity regularisation. The contributions of this work are: (I)We propose the idea of learning concurrently both local and global feature selections for optimising feature discriminative capabilities in different context whilst performing the same person reid tasks. This is currently understudied in the person reid liter ature to our best knowledge. (II)We formulate a novel Joint Learning MultiLoss (JLML) CNN model for not only learn ing both global and local discriminative features in different context by optimising multiple classiÔ¨Åcation losses on the same person label information concurrently, but also utilis ing their complementary advantages jointly in coping with lo cal misalignment and optimising holistic matching criteria for person reid. (III) We introduce a structured sparsity based feature selection learning mechanism for improving multi loss joint feature learning robustness w.r.t. noise and data co variance between local and global representations. Extensive comparative evaluations demonstrate the superiority of the proposed JLML model over a wide range of existing stateof theart reid models on Ô¨Åve benchmark datasets VIPeR [Gray and Tao, 2008 ], GRID [Loyet al. , 2009 ], CUHK01 [Liet al. , 2012 ], CUHK03 [Liet al. , 2014 ], and Market1501 [Zheng et al. , 2015 ].2 Related Works "
262,Exploiting Temporal Coherence for Self-Supervised One-shot Video Re-identification.txt,"While supervised techniques in re-identification are extremely effective, the
need for large amounts of annotations makes them impractical for large camera
networks. One-shot re-identification, which uses a singular labeled tracklet
for each identity along with a pool of unlabeled tracklets, is a potential
candidate towards reducing this labeling effort. Current one-shot
re-identification methods function by modeling the inter-relationships amongst
the labeled and the unlabeled data, but fail to fully exploit such
relationships that exist within the pool of unlabeled data itself. In this
paper, we propose a new framework named Temporal Consistency Progressive
Learning, which uses temporal coherence as a novel self-supervised auxiliary
task in the one-shot learning paradigm to capture such relationships amongst
the unlabeled tracklets. Optimizing two new losses, which enforce consistency
on a local and global scale, our framework can learn learn richer and more
discriminative representations. Extensive experiments on two challenging video
re-identification datasets - MARS and DukeMTMC-VideoReID - demonstrate that our
proposed method is able to estimate the true labels of the unlabeled data more
accurately by up to $8\%$, and obtain significantly better re-identification
performance compared to the existing state-of-the-art techniques.","Person reidentication (reID) aims to solve the challenging problem of matching identities across nonoverlapping views in a multicamera system. The surge of deep neural networks in computer vision [ 17,30] has been re ected in person reID as well, with impressive performance over a wide variety of datasets [ 33,5]. However, this improved performance has predominantly been achieved through supervised learning , facilitated by the availability of large amounts of annotated data. However, acquiring identity labels for a large set of unlabeled tracklets is an extremely timeconsuming and cumbersome task. Consequently, methods which can ameliorate this annotation problem and work with limited supervision, such asunsupervised learning orsemisupervised learning techniques, are of primary importance in the context of person reID.arXiv:2007.11064v1  [cs.CV]  21 Jul 20202 D. S. Raychaudhuri and A. K. RoyChowdhury Du Dp <latexit sha1_base64=""gQaKotmuivFPL8j9BV8bhOUQI98="">AAACBHicbVDLSsNAFL3xWesr6rKbwSK4sSRS0WVRFy4r2Ae0IUym03boZBJmJkIJXbjxV9y4UMStH+HOv3HSZlFbDwycOede7r0niDlT2nF+rJXVtfWNzcJWcXtnd2/fPjhsqiiRhDZIxCPZDrCinAna0Exz2o4lxWHAaSsY3WR+65FKxSLxoMcx9UI8EKzPCNZG8u1SN8R6SDBPbyd+cjb/i3277FScKdAycXNShhx13/7u9iKShFRowrFSHdeJtZdiqRnhdFLsJorGmIzwgHYMFTikykunR0zQiVF6qB9J84RGU3W+I8WhUuMwMJXZkmrRy8T/vE6i+1deykScaCrIbFA/4UhHKEsE9ZikRPOxIZhIZnZFZIglJtrkVjQhuIsnL5PmecWtVi7uq+XadR5HAUpwDKfgwiXU4A7q0AACT/ACb/BuPVuv1of1OStdsfKeI/gD6+sXMn6YdQ==</latexit> ,y1 <latexit sha1_base64=""Fap1BxZBD3pPZoAfPZ+Nji5Dje8="">AAAB7HicbVBNS8NAEJ2tX7V+VT16WSyCBymJVPRY9OKxgmkLbSib7aZdutmE3Y0QQn+DFw+KePUHefPfuG1z0NYHA4/3ZpiZFySCa+M436i0tr6xuVXeruzs7u0fVA+P2jpOFWUejUWsugHRTHDJPMONYN1EMRIFgnWCyd3M7zwxpXksH02WMD8iI8lDTomxkneBs4E7qNacujMHXiVuQWpQoDWofvWHMU0jJg0VROue6yTGz4kynAo2rfRTzRJCJ2TEepZKEjHt5/Njp/jMKkMcxsqWNHiu/p7ISaR1FgW2MyJmrJe9mfif10tNeOPnXCapYZIuFoWpwCbGs8/xkCtGjcgsIVRxeyumY6IINTafig3BXX55lbQv626jfvXQqDVvizjKcAKncA4uXEMT7qEFHlDg8Ayv8IYkekHv6GPRWkLFzDH8Afr8Ac3gjgk=</latexit> ,y2 <latexit sha1_base64=""M2n2UsnA+Xmqun0aGgepZn6Y+DU="">AAAB7HicbVBNS8NAEJ31s9avqkcvi0XwICUpFT0WvXisYNpCG8pmu2mXbjZhdyOE0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmBYng2jjON1pb39jc2i7tlHf39g8OK0fHbR2nijKPxiJW3YBoJrhknuFGsG6iGIkCwTrB5G7md56Y0jyWjyZLmB+RkeQhp8RYybvE2aA+qFSdmjMHXiVuQapQoDWofPWHMU0jJg0VROue6yTGz4kynAo2LfdTzRJCJ2TEepZKEjHt5/Njp/jcKkMcxsqWNHiu/p7ISaR1FgW2MyJmrJe9mfif10tNeOPnXCapYZIuFoWpwCbGs8/xkCtGjcgsIVRxeyumY6IINTafsg3BXX55lbTrNbdRu3poVJu3RRwlOIUzuAAXrqEJ99ACDyhweIZXeEMSvaB39LFoXUPFzAn8Afr8Ac9kjgo=</latexit> ,y3 <latexit sha1_base64=""6ABukcEhl/S5IZDDg2bAj8r4dmI="">AAAB7HicbVBNS8NAEJ31s9avqkcvi0XwICXRih6LXjxWMG2hDWWz3bRLN5uwuxFC6G/w4kERr/4gb/4bt20O2vpg4PHeDDPzgkRwbRznG62srq1vbJa2yts7u3v7lYPDlo5TRZlHYxGrTkA0E1wyz3AjWCdRjESBYO1gfDf1209MaR7LR5MlzI/IUPKQU2Ks5J3jrH/Zr1SdmjMDXiZuQapQoNmvfPUGMU0jJg0VROuu6yTGz4kynAo2KfdSzRJCx2TIupZKEjHt57NjJ/jUKgMcxsqWNHim/p7ISaR1FgW2MyJmpBe9qfif101NeOPnXCapYZLOF4WpwCbG08/xgCtGjcgsIVRxeyumI6IINTafsg3BXXx5mbQuam69dvVQrzZuizhKcAwncAYuXEMD7qEJHlDg8Ayv8IYkekHv6GPeuoKKmSP4A/T5A9Dojgs=</latexit> Dl <latexit sha1_base64=""3EmSqPnCAczazbvqm9ZIRqbf6i8="">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakosuiLlxWsA9oh5JJM21oJjMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z4cPxZcG8f5RoW19Y3NreJ2aWd3b/+gfHjU0lGiKGvSSESq4xPNBJesabgRrBMrRkJfsLY/vs389oQpzSP5aKYx80IylDzglBgreb2QmBElIr2b9UW/XHGqzhx4lbg5qUCORr/81RtENAmZNFQQrbuuExsvJcpwKtis1Es0iwkdkyHrWipJyLSXzkPP8JlVBjiIlH3S4Ln6eyMlodbT0LeTWUi97GXif143McG1l3IZJ4ZJujgUJAKbCGcN4AFXjBoxtYRQxW1WTEdEEWpsTyVbgrv85VXSuqi6terlQ61Sv8nrKMIJnMI5uHAFdbiHBjSBwhM8wyu8oQl6Qe/oYzFaQPnOMfwB+vwB/mWSQQ==</latexit> Du <latexit sha1_base64=""q7p/cSbAE+oZsy0VhzL6amDzHH0="">AAAB9HicbVDLSsNAFL2pr1pfVZduBovgqiRS0WVRFy4r2Ae0oUymk3boZBLnUSih3+HGhSJu/Rh3/o2TNgttPTBwOOde7pkTJJwp7brfTmFtfWNzq7hd2tnd2z8oHx61VGwkoU0S81h2AqwoZ4I2NdOcdhJJcRRw2g7Gt5nfnlCpWCwe9TShfoSHgoWMYG0lvxdhPSKYp3ezvumXK27VnQOtEi8nFcjR6Je/eoOYmIgKTThWquu5ifZTLDUjnM5KPaNogskYD2nXUoEjqvx0HnqGzqwyQGEs7RMazdXfGymOlJpGgZ3MQqplLxP/87pGh9d+ykRiNBVkcSg0HOkYZQ2gAZOUaD61BBPJbFZERlhiom1PJVuCt/zlVdK6qHq16uVDrVK/yesowgmcwjl4cAV1uIcGNIHAEzzDK7w5E+fFeXc+FqMFJ985hj9wPn8ADBiSSg==</latexit> ,ÀÜy1 <latexit sha1_base64=""Mz+/Xegtme+tCy8Uq5DiLi6SRNg="">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBg5REKnosevFYwX5AGspmu2mXbnbD7kQIoT/DiwdFvPprvPlv3LY5aOuDgcd7M8zMCxPBDbjut1NaW9/Y3CpvV3Z29/YPqodHHaNSTVmbKqF0LySGCS5ZGzgI1ks0I3EoWDec3M387hPThiv5CFnCgpiMJI84JWAl/wL3xwTybDrwBtWaW3fnwKvEK0gNFWgNql/9oaJpzCRQQYzxPTeBICcaOBVsWumnhiWETsiI+ZZKEjMT5POTp/jMKkMcKW1LAp6rvydyEhuTxaHtjAmMzbI3E//z/BSimyDnMkmBSbpYFKUCg8Kz//GQa0ZBZJYQqrm9FdMx0YSCTaliQ/CWX14lncu616hfPTRqzdsijjI6QafoHHnoGjXRPWqhNqJIoWf0it4ccF6cd+dj0Vpyiplj9AfO5w+e/pDW</latexit> ,ÀÜy2 <latexit sha1_base64=""VqAU90ADA5SOgeXs9znx/XWM3k4="">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4kJKUih6LXjxWsB/QhLLZbtqlm03YnQgl9G948aCIV/+MN/+NSZuDtj4YeLw3w8w8P5bCoG1/W2vrG5tb26Wd8u7e/sFh5ei4Y6JEM95mkYx0z6eGS6F4GwVK3os1p6Evedef3OV+94lrIyL1iNOYeyEdKREIRjGT3Evijimm09mgXh5UqnbNnoOsEqcgVSjQGlS+3GHEkpArZJIa03fsGL2UahRM8lnZTQyPKZvQEe9nVNGQGy+d3zwj55kyJEGks1JI5urviZSGxkxDP+sMKY7NspeL/3n9BIMbLxUqTpArtlgUJJJgRPIAyFBozlBOM0KZFtmthI2ppgyzmPIQnOWXV0mnXnMatauHRrV5W8RRglM4gwtw4BqacA8taAODGJ7hFd6sxHqx3q2PReuaVcycwB9Ynz/YiZDr</latexit> Dp <latexit sha1_base64=""7fX8s2RQq+N/WrsXRY3Pb+ZPzKA="">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsxIRZdFXbisYB/QDiWTZtrQTCYmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cQHKmjet+O4W19Y3NreJ2aWd3b/+gfHjU0nGiCG2SmMeqE2BNORO0aZjhtCMVxVHAaTsY32Z+e0KVZrF4NFNJ/QgPBQsZwcZKfi/CZkQwT+9mfdkvV9yqOwdaJV5OKpCj0S9/9QYxSSIqDOFY667nSuOnWBlGOJ2VeommEpMxHtKupQJHVPvpPPQMnVllgMJY2ScMmqu/N1IcaT2NAjuZhdTLXib+53UTE177KRMyMVSQxaEw4cjEKGsADZiixPCpJZgoZrMiMsIKE2N7KtkSvOUvr5LWRdWrVS8fapX6TV5HEU7gFM7Bgyuowz00oAkEnuAZXuHNmTgvzrvzsRgtOPnOMfyB8/kDBISSRQ==</latexit> Extract featuresAssign pseudolabels Temporal consistency loss (         ) + CrossEntropy loss (      )Dl,Dp,Du <latexit sha1_base64=""rcTsecNPSiR4/6+6HJ+XvHwJnNc="">AAACEnicbVDLSsNAFL2pr1pfUZdugkVQkJJIRZdFXbisYB/QhjCZTtqhk0mYmQgl9Bvc+CtuXCji1pU7/8ZJm0UfHhg495x7mXuPHzMqlW3/GoWV1bX1jeJmaWt7Z3fP3D9oyigRmDRwxCLR9pEkjHLSUFQx0o4FQaHPSMsf3mZ+64kISSP+qEYxcUPU5zSgGCkteeZZN0RqgBFL78YeO5+t4rkq8cyyXbEnsJaJk5My5Kh75k+3F+EkJFxhhqTsOHas3BQJRTEj41I3kSRGeIj6pKMpRyGRbjo5aWydaKVnBZHQjytros5OpCiUchT6ujNbUi56mfif10lUcO2mlMeJIhxPPwoSZqnIyvKxelQQrNhIE4QF1btaeIAEwkqnWNIhOIsnL5PmRcWpVi4fquXaTR5HEY7gGE7BgSuowT3UoQEYnuEV3uHDeDHejE/ja9paMPKZQ5iD8f0HJL2eaQ==</latexit> Dl,Dp <latexit sha1_base64=""sCj4Y+2Z6+KF2h/EUXGp+D3lFbM="">AAACBHicbVDLSsNAFL3xWesr6rKbwSK4kJJIRZdFXbisYB/QhjCZTtuhk0mYmQgldOHGX3HjQhG3foQ7/8ZJm0VtPTBw5px7ufeeIOZMacf5sVZW19Y3Ngtbxe2d3b19++CwqaJEEtogEY9kO8CKciZoQzPNaTuWFIcBp61gdJP5rUcqFYvEgx7H1AvxQLA+I1gbybdL3RDrIcE8vZ34/Gz+F/t22ak4U6Bl4uakDDnqvv3d7UUkCanQhGOlOq4Tay/FUjPC6aTYTRSNMRnhAe0YKnBIlZdOj5igE6P0UD+S5gmNpup8R4pDpcZhYCqzJdWil4n/eZ1E96+8lIk40VSQ2aB+wpGOUJYI6jFJieZjQzCRzOyKyBBLTLTJrWhCcBdPXibN84pbrVzcV8u16zyOApTgGE7BhUuowR3UoQEEnuAF3uDderZerQ/rc1a6YuU9R/AH1tcvIsuYaw==</latexit> Du <latexit sha1_base64=""q7p/cSbAE+oZsy0VhzL6amDzHH0="">AAAB9HicbVDLSsNAFL2pr1pfVZduBovgqiRS0WVRFy4r2Ae0oUymk3boZBLnUSih3+HGhSJu/Rh3/o2TNgttPTBwOOde7pkTJJwp7brfTmFtfWNzq7hd2tnd2z8oHx61VGwkoU0S81h2AqwoZ4I2NdOcdhJJcRRw2g7Gt5nfnlCpWCwe9TShfoSHgoWMYG0lvxdhPSKYp3ezvumXK27VnQOtEi8nFcjR6Je/eoOYmIgKTThWquu5ifZTLDUjnM5KPaNogskYD2nXUoEjqvx0HnqGzqwyQGEs7RMazdXfGymOlJpGgZ3MQqplLxP/87pGh9d+ykRiNBVkcSg0HOkYZQ2gAZOUaD61BBPJbFZERlhiom1PJVuCt/zlVdK6qHq16uVDrVK/yesowgmcwjl4cAV1uIcGNIHAEzzDK7w5E+fFeXc+FqMFJ985hj9wPn8ADBiSSg==</latexit> Dl <latexit sha1_base64=""3EmSqPnCAczazbvqm9ZIRqbf6i8="">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakosuiLlxWsA9oh5JJM21oJjMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z4cPxZcG8f5RoW19Y3NreJ2aWd3b/+gfHjU0lGiKGvSSESq4xPNBJesabgRrBMrRkJfsLY/vs389oQpzSP5aKYx80IylDzglBgreb2QmBElIr2b9UW/XHGqzhx4lbg5qUCORr/81RtENAmZNFQQrbuuExsvJcpwKtis1Es0iwkdkyHrWipJyLSXzkPP8JlVBjiIlH3S4Ln6eyMlodbT0LeTWUi97GXif143McG1l3IZJ4ZJujgUJAKbCGcN4AFXjBoxtYRQxW1WTEdEEWpsTyVbgrv85VXSuqi6terlQ61Sv8nrKMIJnMI5uHAFdbiHBjSBwhM8wyu8oQl6Qe/oYzFaQPnOMfwB+vwB/mWSQQ==</latexit> Dp <latexit sha1_base64=""7fX8s2RQq+N/WrsXRY3Pb+ZPzKA="">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsxIRZdFXbisYB/QDiWTZtrQTCYmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cQHKmjet+O4W19Y3NreJ2aWd3b/+gfHjU0nGiCG2SmMeqE2BNORO0aZjhtCMVxVHAaTsY32Z+e0KVZrF4NFNJ/QgPBQsZwcZKfi/CZkQwT+9mfdkvV9yqOwdaJV5OKpCj0S9/9QYxSSIqDOFY667nSuOnWBlGOJ2VeommEpMxHtKupQJHVPvpPPQMnVllgMJY2ScMmqu/N1IcaT2NAjuZhdTLXib+53UTE177KRMyMVSQxaEw4cjEKGsADZiixPCpJZgoZrMiMsIKE2N7KtkSvOUvr5LWRdWrVS8fapX6TV5HEU7gFM7Bgyuowz00oAkEnuAZXuHNmTgvzrvzsRgtOPnOMfyB8/kDBISSRQ==</latexit> LossTracklet Shared weights yi <latexit sha1_base64=""k2h6y3oV/izAuo/CNcIwRpd2x2s="">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48V7Qe0oWy2k3bpZhN2N0Io/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHkyXoR3QoecgZNVZ6yPq8X664VXcOskq8nFQgR6Nf/uoNYpZGKA0TVOuu5ybGn1BlOBM4LfVSjQllYzrErqWSRqj9yfzUKTmzyoCEsbIlDZmrvycmNNI6iwLbGVEz0sveTPzP66YmvPYnXCapQckWi8JUEBOT2d9kwBUyIzJLKFPc3krYiCrKjE2nZEPwll9eJa2LqlerXt7XKvWbPI4inMApnIMHV1CHO2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gBkro3h</latexit> ÀÜyi <latexit sha1_base64=""7N1WW3+bsLR4cWuDWp+qIlTVzMA="">AAAB8HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48V7Ie0oWy223bpbhJ2J0II/RVePCji1Z/jzX/jts1BWx8MPN6bYWZeEEth0HW/ncLa+sbmVnG7tLO7t39QPjxqmSjRjDdZJCPdCajhUoS8iQIl78SaUxVI3g4mtzO//cS1EVH4gGnMfUVHoRgKRtFKj70xxSyd9kW/XHGr7hxklXg5qUCORr/81RtELFE8RCapMV3PjdHPqEbBJJ+WeonhMWUTOuJdS0OquPGz+cFTcmaVARlG2laIZK7+nsioMiZVge1UFMdm2ZuJ/3ndBIfXfibCOEEessWiYSIJRmT2PRkIzRnK1BLKtLC3EjammjK0GZVsCN7yy6ukdVH1atXL+1qlfpPHUYQTOIVz8OAK6nAHDWgCAwXP8ApvjnZenHfnY9FacPKZY/gD5/MHM+yQrg==</latexit> LabelPseudolabelModelModel (weights fixed)Pseudo labeled dataLabeled dataUnlabeled data Current iterationNext iteration Fig. 1. A schematic illustration of the proposed framework . Our method makes use of both labeled and unlabeled tracklets at every iteration of model training. The rst step involves learning the parameters of the deep model by using temporal consistency as selfsupervision and, additionally, softmax loss on the minimal set of annotated tracklets. Next, this model is used to predict pseudolabels on a few condent samples. These two steps alternate, one after the other, until the entire unlabeled set has been incorporated in terms of pseudolabels. In this work, we focus on the semisupervised task in video person reID, specically, the oneshot setting, where only one tracklet per identity is labeled. The objective of the learning process is to utilize this small labeled set along with a larger unlabeled set of tracklets to obtain a reID model. The key challenge involved with the oneshot task is guring out the interrelationships which exist amongst the labeled and unlabeled instances. Stateoftheart oneshot methods try to address this by estimating the labels of the unlabeled tracklets (pseudo labels) and then utilizing a supervised learning strategy. Some works employ a static sampling strategy [ 37,22], where pseudolabels with a condence score above a predened threshold are selected for supervised learning. More recent works [ 35,34] make use of a progressive sampling strategy, where a subset of the pseudolabeled samples are selected with the size of the subset expanding with each iteration. This prevents an in ux of noisy pseudolabels, and thus, averts the situation of conrmation bias [ 1]. However, in an eort to control the number of noisy pseudolabels, most of these methods discard a signicant portion of the unlabeled set at each learning iteration; thus, the information in the unlabeled set is not maximally utilized for training the model. Due to this inecient usage of the unlabeled set and the limited number of labeled instances, propagating beliefs directly from the labeled to the unlabeled set is insucient to fully capture the relationships which exist amongst instances of the unlabeled set. To resolve this issue of inecient usage of the unlabeled data, we draw inspiration from the eld of selfsupervised visual representation learning [ 16].Exploiting Temporal Coherence for SelfSupervised Oneshot Video ReID 3 We propose using temporal coherence [29,25,23] as a form of selfsupervision to maximally utilize the unlabeled data and learn discriminative person specic representations. Temporal coherence is motivated by the fact that features corresponding to a person in a tracklet should be focused on the discriminative aspects related to the person, such as clothing and gait, and ignore background nuances such as illumination and occlusion (see Fig. 2). This naturally suggests that features should be temporally consistent across the entire duration of the tracklet as the person in a tracklet remains constant. Thus, we propose a new framework, Temporally Consistent Progressive Learning (TCPL), which unies this notion of temporal coherence with a progressive pseudolabeling strategy [35]. An overview of our framework is presented in Fig. 1. In this paper, we propose two novel losses to learn such temporally consistent features: Intrasequence temporal consistency loss and the Intersequence temporal consistency loss . Both of these losses apply consistency regularization on the temporal dimension of a tracklet. While the rst loss employs a local level of consistency by operating on a specic tracklet, the second loss extends it by applying temporal consistency both within and across tracklets. Using such selfsupervised losses, our framework can use the unlabeled data at each iteration of learning, allowing maximal information to be extracted out of it. Additionally, by exploiting two levels of consistency, as explained above, TCPL can better model the relationships amongst the unlabeled instances without being limited by the number of labeled instances. Thus, our framework addresses both the drawbacks associated with the current crop of methods and achieves stateoftheart performance in the oneshot person reID task. Main contributions. Our main contributions are summarised as follows: We introduce a new framework, Temporally Consistent Progressive Learning , which unies selfsupervision and pseudolabeling to maximally utilize the labeled and unlabeled data eciently for oneshot video person reID. We introduce two novel selfsupervised losses, the Intrasequence temporal consistency loss and the Intersequence temporal consistency loss , to imple ment temporal consistency and empirically demonstrate their benets in learning richer and more discriminative feature representations. We demonstrate that this intelligent use of the unlabeled data through self supervision, unlike previous pseudolabeling methods, leads to signicantly better label estimation and superior results on the oneshot video reID task, outperforming the stateofthe art oneshot video reID methods on the MARS and DukeMTMCVideoReID datasets. 2 Related works "
263,Memorization-Dilation: Modeling Neural Collapse Under Label Noise.txt,"The notion of neural collapse refers to several emergent phenomena that have
been empirically observed across various canonical classification problems.
During the terminal phase of training a deep neural network, the feature
embedding of all examples of the same class tend to collapse to a single
representation, and the features of different classes tend to separate as much
as possible. Neural collapse is often studied through a simplified model,
called the unconstrained feature representation, in which the model is assumed
to have ""infinite expressivity"" and can map each data point to any arbitrary
representation. In this work, we propose a more realistic variant of the
unconstrained feature representation that takes the limited expressivity of the
network into account. Empirical evidence suggests that the memorization of
noisy data points leads to a degradation (dilation) of the neural collapse.
Using a model of the memorization-dilation (M-D) phenomenon, we show one
mechanism by which different losses lead to different performances of the
trained network on noisy data. Our proofs reveal why label smoothing, a
modification of cross-entropy empirically observed to produce a regularization
effect, leads to improved generalization in classification tasks.","The empirical success of deep neural networks has accelerated the introduction of new learning algorithms and triggered new applications, with a pace that makes it hard to keep up with profound theoretical foundations and insightful explanations. As one of the few yet particularly appealing theo retical characterizations of overparameterized models trained for canonical classiÔ¨Åcation tasks, Neural Collapse (NC) provides a mathematically elegant formalization of learned feature representations Papyan et al. (2020). To explain NC, consider the following setting. Suppose we are given a balanced datasetD=n (x(k) n;yn)o k2[K];n2[N]XY in the instance space X=Rdand label spaceY= [N] := f1;:::;Ng, i.e. each class n2[N]has exactlyKsamples x(1) n;:::;x(K) n. We consider network architectures commonly used in classiÔ¨Åcation tasks that are composed of a feature engineering part g:X!RM(which maps an input signal x2X to its feature representation g(x)2RM) and a linear classiÔ¨Åer W() +bgiven by a weight matrix W2RNMas well as a bias vector b2RN. Letwndenote the row vector of Wassociated with class n2[N]. During training, both classiÔ¨Åer components are simultaneously optimized by minimizing the crossentropy loss. *These authors contributed equally to this work. 1arXiv:2206.05530v3  [cs.LG]  4 Apr 2023Published as a conference paper at ICLR 2023 Denoting the feature representations g(x(k) n)of the sample x(k) nbyh(k) n, the class means and the global mean of the features by hn:=1 KKX i=1h(k) n;h:=1 NNX n=1hn; NC consists of the following interconnected phenomena (where the limits take place as training progresses): (NC1) Variability collapse. For each class n2[N], we have1 KPK k=1   h(k) n hn   2 !0: (NC2) Convergence to simplex equiangular tight frame (ETF) structure. For anym;n2[N] withm6=n, we have khn hk2 khm hk2!0;andhn h khn hk2;hm h khm hk2 ! 1 N 1: (NC3) Convergence to selfduality. For anyn2[N], it holds hn h khn hk2 wn kwnk2!0: (NC4) SimpliÔ¨Åcation to nearest class center behavior. For any feature representation u2RM, it holds arg max n2[N]hwn;ui+bn!arg min n2[N]ku hnk2: In this paper, we consider a well known simpliÔ¨Åed model, in which the features h(k) nare not parameterized by the feature engineering network gbut are rather free variables. This model is often referred to as layerpeeled model or unconstrained features model, see e.g. Lu & Steinerberger (2020); Fang et al. (2021); Zhu et al. (2021). However, as opposed to those contributions, in which the features h(k) ncan take any value in RM, we consider here the case h(k) n0(understood component wise). This is motivated by the fact that features are typically the outcome of some nonnegative activation function, like the RectiÔ¨Åed Linear Unit (ReLU) or sigmoid. Moreover, by incorporating the limited expressivity of the network to the layerpeeled model, we propose a new model, called memorizationdilation (MD). Given such model assumptions, we formally prove advantageous effects of the socalled label smoothing (LS) technique Szegedy et al. (2015) (training with a modiÔ¨Åcation of crossentropy (CE) loss), in terms of generalization performance. This is further conÔ¨Årmed empirically. 2 R ELATED WORK "
264,Dual-Correction Adaptation Network for Noisy Knowledge Transfer.txt,"Previous unsupervised domain adaptation (UDA) methods aim to promote target
learning via a single-directional knowledge transfer from label-rich source
domain to unlabeled target domain, while its reverse adaption from target to
source has not jointly been considered yet so far. In fact, in some real
teaching practice, a teacher helps students learn while also gets promotion
from students to some extent, which inspires us to explore a dual-directional
knowledge transfer between domains, and thus propose a Dual-Correction
Adaptation Network (DualCAN) in this paper. However, due to the asymmetrical
label knowledge across domains, transfer from unlabeled target to labeled
source poses a more difficult challenge than the common source-to-target
counterpart. First, the target pseudo-labels predicted by source commonly
involve noises due to model bias, hence in the reverse adaptation, they may
hurt the source performance and bring a negative target-to-source transfer.
Secondly, source domain usually contains innate noises, which will inevitably
aggravate the target noises, leading to noise amplification across domains. To
this end, we further introduce a Noise Identification and Correction (NIC)
module to correct and recycle noises in both domains. To our best knowledge,
this is the first naive attempt of dual-directional adaptation for noisy UDA,
and naturally applicable to noise-free UDA. A theory justification is given to
state the rationality of our intuition. Empirical results confirm the
effectiveness of DualCAN with remarkable performance gains over
state-of-the-arts, particularly for extreme noisy tasks (e.g., ~+ 15% on Pw->Pr
and Pr->Rw of Office-Home).","DEEP neural network has achieved remarkable success in many applications, such as image classiÔ¨Åcation and semantic segmentation. However, it relies on large scared and highquality annotated data, which is usually difÔ¨Åcult to collect. Unsupervised domain adaptation (UDA) [1], which aims to adopt a fullylabeled source domain to help the learning of unlabeled target domain, has attracted much attention in recent years. Most UDA methods trans fer knowledge from source to target by learning domain invariant representation across domains, mainly including discrepancybased [2], [3] and adversarialbased methods [4], [5], [6]. Discrepancybased methods explicitly reduce the distribution discrepancy between domains by minimizing some distance metric, such as Maximum Mean Discrep ancy (MMD) [3], Correlation Alignment (CORAL) [7] and Wasserstein distance [8]. Adversarialbased methods align feature distributions across domains by adversarial training between feature generator and domain discriminator [5], or between different classiÔ¨Åers [2]. In real UDA tasks, the source domain usually involves noises, further giving rise to noisy UDA [9], [10]. For ex ample, source data collected from crowdsourced platforms Yunyun Wang and Weiwen Zheng are with the Jiangsu Key Labo ratory of Big Data Security & Intelligent Processing, Computer Sci ence and Engineering, Nanjing University of Posts & Telecommuni cations, Nanjing 210046, China. Email: wangyunyun@njupt.edu.cn, 1020041209@njupt.edu.cn. Songcan Chen is with the MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Computer Science & Technology/AI, Nanjing University of Aeronautics & Astronautics, Nanjing 210023, China. E mail: s.chen@nuaa.edu.cn. Corresponding Author: Songcan Chen (s.chen@nuaa.edu.cn). Manuscript received April 19, 2005; revised August 26, 2015.or internet medias will inevitably be corrupted by noises over both features and labels. The feature noise corrupts original features and may increase the difÔ¨Åculty of do main alignment, while label noise worsens the expected risk of classiÔ¨Åcation, thus incurs misclassiÔ¨Åcation of target instances. It makes previous UDA methods easy to fail in noisy environments. Recently, some researches [9], [10], [11], [12] have been dedicated to noisy UDA learning, which can be mainly divided into two categories. One kind of meth ods, such as Transferable Curriculum Learning (TCL) [9] and Robust Domain Adaptation (RDA) [10], adopts small loss criterion to separate source instances into clean and noisy parts, then transfer source knowledge to target with only clean instances detected. The other kind, including Noisy Universal Domain Adaptation (Noisy UniDA) [11] and Noise Resistible MutualTraining (NRMT) [12], uses colearning strategy with multiple classiÔ¨Åers to reduce the impact of label noise in adaptation. Those previous UDA methods all adopt a single  directional knowledge transfer from labeled source to un labeled target for helping the target learning. However, in some real teaching practice, a teacher helps students learn, while also gets promotion from students to some extent. Inspired by such a philosophy, the reverse adaptation from the target should intuitively be able to boost the source learning as well, especially for weak noisy sources. To our best knowledge, however, it has not been jointly considered in UDA so far. In this paper, we attempt to explore a dual directional knowledge transfer between domains, and pro pose a DualCorrection Adaptation Network (DualCAN) to achieve mutual promotion and cooperation across domains. However, it is worth noting that there is asymmetrical label knowledge between domains, since target domainarXiv:2207.04423v1  [cs.CV]  10 Jul 2022JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 contains much less label information than the source, thus the transfer from target to source poses a more difÔ¨Åcult challenge than the sourcetotarget counterpart. First, the target pseudolabels predicted by source commonly involve noises due to transfer and model bias, hence in the reverse adaptation, they may hurt the source performance, and consequently bring a negative targettosource transfer. Sec ondly, the source domain usually contains innate noises in real tasks. It will inevitably aggravate the target noises, and incurs noise ampliÔ¨Åcation across domains. To address those issues, we further introduce a crucial Noise IdentiÔ¨Åcation and Correction (NIC) module in DualCAN to correct the noises in both domains. After that, those corrected instances are further recycled in learning rather than simply dis carded, in order for a full knowledge utilization, especially in high noisy environment. In implementing DualCAN, knowledge transfer iterates back and forth between sourcetotarget (ST) task and target tosource (TS) task. In ST, source knowledge is adapted to generate target pseudolabels, and the pseudolabels are fur ther corrected by NIC with selfsupervised knowledge. In TS, the target knowledge is transferred reversely to correct source noise and further boost source learning. With such a dualdirectional knowledge transfer between domains, noises in both domains are corrected collaboratively, and performances in both domains are promoted mutually. It is analogous to the philosophy that teaching beneÔ¨Åts both the teacher and students alike. Quite naturally, dualdirectional transfer can be adopted for both noisy and noisefree UDA tasks, while its learning concept is also applicable for some other learning tasks, for example, using downstream tasks to reversely help feature learning in selfsupervised learn ing. The main contributions of this paper are summarized as follows: A DualCorrection Adaptation Network (DualCAN) is proposed for noisy UDA learning. To our best knowledge, this is the Ô¨Årst work of dualdirectional adaptation to mutually promote learning and correct noises in both domains. The noisy instances are corrected and recycled by a Noise IdentiÔ¨Åcation and Correction (NIC) module, in order to prevent noise ampliÔ¨Åcation aross domains, and achieve a full knowledge utilization, especially in high noisy environment. A theory justiÔ¨Åcation is given to state the rationality of DualCAN. Moreover, empirical comparisons are conducted in realworld tasks under different noisy settings, in order to conÔ¨Årm the effectiveness of pro posal. The rest of the paper is organized as follows, section 2 introduces the related works, section 3 gives the preliminar ies, the proposed DualCAN is described in details in section 4, and the comparison results are given in section 5. Finally, section 6 is the conclusion. 2 R ELATED WORK "
265,Countering Adversarial Examples: Combining Input Transformation and Noisy Training.txt,"Recent studies have shown that neural network (NN) based image classifiers
are highly vulnerable to adversarial examples, which poses a threat to
security-sensitive image recognition task. Prior work has shown that JPEG
compression can combat the drop in classification accuracy on adversarial
examples to some extent. But, as the compression ratio increases, traditional
JPEG compression is insufficient to defend those attacks but can cause an
abrupt accuracy decline to the benign images. In this paper, with the aim of
fully filtering the adversarial perturbations, we firstly make modifications to
traditional JPEG compression algorithm which becomes more favorable for NN.
Specifically, based on an analysis of the frequency coefficient, we design a
NN-favored quantization table for compression. Considering compression as a
data augmentation strategy, we then combine our model-agnostic preprocess with
noisy training. We fine-tune the pre-trained model by training with images
encoded at different compression levels, thus generating multiple classifiers.
Finally, since lower (higher) compression ratio can remove both perturbations
and original features slightly (aggressively), we use these trained multiple
models for model ensemble. The majority vote of the ensemble of models is
adopted as final predictions. Experiments results show our method can improve
defense efficiency while maintaining original accuracy.","Adversarial attack presents a major challenge for the prevalent deep neural networks used for image classiÔ¨Åca tion and recognition [37]. Several countermeasures havebeen proposed against adversarial examples, mainly includ ing modelspeciÔ¨Åc hardening strategies and modelagnostic defenses. Typical modelspeciÔ¨Åc solutions like ‚Äúadversarial training‚Äù [17, 27, 34, 33, 29] can rectify the model param eters to mitigate the attacks by using the iterative retraining procedure or modifying the inner architecture. However, it is generally believed that, network‚Äôs architectural elements would matter little unless making them larger and deeper in improving adversarial robustness. In contrast, model agnostic solutions like input dimension reduction or direct JPEG compression [8, 4], become more feasible and prac tical, which attempt to remove adversarial perturbations by input transformations before feeding them into neural net work classiÔ¨Åers. For mitigating adversarial examples, standard JPEG compression has been explored in [8, 4]. But, in these works, they have shown that JPEG cannot achieve a good balance between countering adversarial examples and clas sifying benign images, i.e., lower quality factor (QF) for JPEG compression achieves better defense efÔ¨Åciency but causes a signiÔ¨Åcant feature loss on benign images. To re solve this problem, we Ô¨Årst optimize the JPEG based trans formation process in this work, to improve defense efÔ¨Å ciency against adversarial examples and maintain classiÔ¨Å cation accuracy on benign images. Firstly, we analyze the distributions of the DCT coefÔ¨Åcients for 6 color channels (i.e., R, G, B, Y , Cb, Cr) on both benign images and polluted images to Ô¨Ånd out adversarial perturbations‚Äô distribution at all 64 frequency bands. With the frequency analysis, we then divide the frequency coefÔ¨Åcients into two types, i.e., the original favored (OF) band and the adversarial favored (AF) band. Finally, the corresponding defensive quantiza tion parameters for these two bands are derived, where the 1arXiv:2106.13394v1  [cs.CV]  25 Jun 2021Model 1  Model 2  Model 3 Prediction  Finetune Test Model setModel 0 Model 4   Transform4 Transform3 Transform2  Adversarial   exampleTransform1 Gaussian noise Gaussian noise Gaussian noise Gaussian noiseOriginal imageFigure 1. Overview of our combination method. Different trans form modules represent different level of compression for the orig inal images. The initial model, i.e., model 0, is the pretrained model on benign images of ImageNet dataset, such as ResNet or Inceptionv4. number of the DCT coefÔ¨Åcients that should be included in each type of band is jointly optimized. With the purpose to further achieve both accuracy and robustness, we Ô¨Ånetune the model with our own pre processed images. Firstly, as a data augmentation, train ing images will be compressed using our compression al gorithm. This idea shares the similar spirit of the image croppingrescaling method proposed in [13], in which, the neural network retrained on randomly croppedrescaled images yields better performance than other input transfor mations. Secondly, Gaussian noise is added to the com pressed images to mimic the adversarial perturbations (de tailed in Section 3.4), which is based on the fact that strong adversaries are not necessarily needed during adversarial training as demonstrated in [33]. However, as the model is commonly noisy trained with compressed images of a cer tain level of quality, there still exists unavoidable tradeoff between robustness and accuracy. To achieve the best bal ance, we generate a number of classiÔ¨Åers by Ô¨Ånetuning the neural network using a variety of degrees of compression quality images during aforementioned preprocessing. Af ter having obtained a set of classiÔ¨Åers, the Ô¨Ånal prediction value if chosen to be the label maximizes the average conÔ¨Å dence (i.e., the output of Softmax layer) of each classiÔ¨Åer. Figure 1 shows an overview of our overall method, where we combine the input transformation and the noisy training. The preprocessing is implemented by using a pro posed compression followed by adding the general Gaus sian noise. The model set is realized by Ô¨Ånetuning the models with compressed images at different compressionlevel. The initial model used for Ô¨Ånetuning is the pre trained model on benign images. The models retrained with different compression levels are ultimately utilized together in an ensemble defense. Experimental results demonstrate the defense efÔ¨Åciency and the legitimate classiÔ¨Åcation efÔ¨Å ciency of the proposed algorithm against a variety of adver sarial examples in the graybox, blackbox and whitebox scenarios. The implementation code of the algorithm pro posed will be made publicly available. 2. Related Works "
266,Viewpoint-Aware Channel-Wise Attentive Network for Vehicle Re-Identification.txt,"Vehicle re-identification (re-ID) matches images of the same vehicle across
different cameras. It is fundamentally challenging because the dramatically
different appearance caused by different viewpoints would make the framework
fail to match two vehicles of the same identity. Most existing works solved the
problem by extracting viewpoint-aware feature via spatial attention mechanism,
which, yet, usually suffers from noisy generated attention map or otherwise
requires expensive keypoint labels to improve the quality. In this work, we
propose Viewpoint-aware Channel-wise Attention Mechanism (VCAM) by observing
the attention mechanism from a different aspect. Our VCAM enables the feature
learning framework channel-wisely reweighing the importance of each feature
maps according to the ""viewpoint"" of input vehicle. Extensive experiments
validate the effectiveness of the proposed method and show that we perform
favorably against state-of-the-arts methods on the public VeRi-776 dataset and
obtain promising results on the 2020 AI City Challenge. We also conduct other
experiments to demonstrate the interpretability of how our VCAM practically
assists the learning framework.","Vehicle reidentiÔ¨Åcation (reID) aims to match images of the same vehicle captured by a camera network. Re cently, this task has drawn increasing attention because of its wide applications such as analyzing and predict ing trafÔ¨Åc Ô¨Çow. While several existing works obtained great success with the aid of Convolutional Neural Net work (CNN) [15, 16, 24], various challenges still hinder the performance of vehicle reID. One of them is that a ve hicle captured from different viewpoints usually has dra matically different visual appearances. To reduce this intra class variation, some works [25, 11, 34] guide the feature learning framework by spatial attention mechanism to ex tract viewpointaware features on the meaningful spatial lo Figure 1: Illustration of Viewpointaware Channelwise Attention Mechanism (VCAM). In the vehicle reID task, the channelwise feature maps are essentially the detectors for speciÔ¨Åc vehicle parts, such as Rear Windshield andTires . Our VCAM enables the framework to empha size (i.e. attentive weight >0:5) the features extracted from the clearly visible vehicle parts which are usually helpful for reID matching while ignore (i.e. attentive weight < 0:5) the others which are usually meaningless. cation. However, the underlying drawback is that the ca pability of the learned network usually suffers from noisy generated spatial attention maps. Moreover, the more pow erful spatial attentive model may rely on expensive pixel level annotations, such as vehicle keypoint labels, which are 1arXiv:2010.05810v1  [cs.CV]  12 Oct 2020impractical in realworld scenario. In view of the above ob servations, we choose to explore another type of attention mechanism in our framework that is only related to high level vehicle semantics. Recently, a number of works adopt channelwise atten tion mechanism [8, 3, 26, 29] and achieve great success in several different tasks. Since a channelwise feature map is essentially a detector of the corresponding semantic at tributes, channelwise attention can be viewed as the pro cess of selecting semantic attributes which are meaningful or potentially helpful for achieving the goal. Such char acteristic could be favorable in the task of vehicle reID. SpeciÔ¨Åcally, channelwise feature maps usually represent the detectors of discriminative parts of vehicle, such as rear windshield or tires. Considering that the vehicle parts are not always clearly visible in the image, with the aid of channelwise attention mechanism, the framework should therefore learn to assign larger attentive weight and, con sequently, emphasize on the channelwise feature maps ex tracted from the visible parts in the image. Nonetheless, the typical implementation of channelwise attention mech anism [8, 3] generates the attentive weight of each stage, explicitly each bottleneck, based on the representation ex tracted from that stage in the CNN backbone. We Ô¨Ånd that the lack of semantic information in the lowlevel represen tations extracted from the former stages may result in unde sirable attentive weight, which would limit the performance in vehicle reID. As an alternative solution, in this paper, we pro pose a novel attentive mechanism, named Viewpointaware Channelwise Attention Mechanism (VCAM) , which adopts highlevel information, the ‚Äúviewpoint‚Äù of captured image, to generate the attentive weight. The motivation is that the visibility of vehicle part usually depends on the viewpoint of the vehicle image. As shown in Fig. 1, with our VCAM, the framework successfully focuses on the clearly visible vehicle parts which are relatively beneÔ¨Åcial to reID match ing. Combined with VCAM, our feature learning frame work is as follows. For every given image, our framework Ô¨Årst estimates the viewpoint of input vehicle image. After wards, based on the viewpoint information, VCAM accord ingly generates the attentive weight of each channel of con volutional feature. ReID feature extraction module is then incorporated with the channelwise attention mechanism to Ô¨Ånally extract viewpointaware feature for reID matching. Extensive experiments prove that our method outper forms stateofthearts on the largescale vehicle reID benchmark: VeRi776 [15, 16] and achieves promising re sults in the 2020 Nvidia AI City Challenge1, which holds competition on the other largescale benchmark, CityFlow ReID [24]. We additionally analyze the attentive weights generated by VCAM in interpretability study to explain how 1https://www.aicitychallenge.org/VCAM helps to solve reID problem in practice. We now highlight our contributions as follows: ‚Ä¢ We propose a novel framework which can beneÔ¨Åt from channelwise attention mechanism and extract viewpointaware feature for vehicle reID matching. ‚Ä¢ To the best of our knowledge, we are the Ô¨Årst to show that viewpointaware channelwise attention mecha nism can obtain great improvement in the vehicle reID problem. ‚Ä¢ Extensive experiments on public datasets increase the interpretability of our method and also demon strate that the proposed framework performs favorably against stateoftheart approaches. 2. Related Work "
267,AutoWS: Automated Weak Supervision Framework for Text Classification.txt,"Creating large, good quality labeled data has become one of the major
bottlenecks for developing machine learning applications. Multiple techniques
have been developed to either decrease the dependence of labeled data
(zero/few-shot learning, weak supervision) or to improve the efficiency of
labeling process (active learning). Among those, Weak Supervision has been
shown to reduce labeling costs by employing hand crafted labeling functions
designed by domain experts. We propose AutoWS -- a novel framework for
increasing the efficiency of weak supervision process while decreasing the
dependency on domain experts. Our method requires a small set of labeled
examples per label class and automatically creates a set of labeling functions
to assign noisy labels to numerous unlabeled data. Noisy labels can then be
aggregated into probabilistic labels used by a downstream discriminative
classifier. Our framework is fully automatic and requires no hyper-parameter
specification by users. We compare our approach with different state-of-the-art
work on weak supervision and noisy training. Experimental results show that our
method outperforms competitive baselines.","Text classiÔ¨Åcation is among the most popular Natural Language Processing (NLP) tasks, and has important applications in realworld, e.g., product categorization. The advent of Deep Learning has brought the stateoftheart to a wide variety of text classiÔ¨Åcation tasks (Mi naee et al., 2021). However, this comes with a price: deep learning based models usually require large amounts of annotated training data to achieve superior performance. In many scenarios, manual data annotation is expensive in terms of cost and eÔ¨Äort, especially whensubject matter experts (SME) must involve and/or classiÔ¨Åcation tasks are scaled to mil lions of instances with hundreds to thousands of classes. To reduce the manual annotation eÔ¨Äort, machine learning research have explored weak supervision approaches, i.e., possibilities to build prediction models using limited, noisy or imprecise labeled data. Weak supervision concerns special conditions of supervised learn ing in which annotated training data may be incomplete, inexact or inaccurate (Zhou, 2018). In this paper, we study incomplete supervision, i.e., data has ground truth labels but is too small to train a performant model, and focus on semisupervised learning techniques to lever age numerous amount of indomain unlabeled data to improve prediction performance. Prior studies have covered diÔ¨Äerent methods to assign noisy training labels to unlabeled data including crowdsourcing (Dalvi et al., 2013; Joglekar et al., 2015; Yuen et al., 2011; Zhang et al., 2016), distant supervision (HoÔ¨Ämann et al., 2011; Mintz et al., 2009; Smirnova and Cudr√©Mauroux, 2018; Takamatsu et al., 2012), heuristic rules (Awasthi et al., 2020; Ratner et al., 2017, 2016; Varma et al., 2017). Pre trained language models gain much attention recently because they can be Ô¨Ånetuned with little annotated data thanks to their great gen eralization capability (Perez et al., 2021; Rad ford et al., 2019). The above weak supervi sion sources are termed labeling functions (LF), which may vary in terms of error rate, coverage and probably generate conÔ¨Çict labels (Zhang et al., 2022, 2021). Researchers have developed label models that aggregate output of label ing functions to generate conÔ¨Ådenceweighted or probabilistic labels, which are consequently used to train an end model (e.g., Ô¨Ånal text classiÔ¨Åer). In this study, we propose AutoWS ‚Äì an auarXiv:2302.03297v1  [cs.CL]  7 Feb 2023tomated endtoend weak supervision frame work for text classiÔ¨Åcation. AutoWS provides a wide range of machinelearning based label ing functions ranging from statistical models to transformerbased language models, and dif ferent label models. Aiming to provide users a fully automated framework, AutoWS imple ments a simple yet eÔ¨Äective evaluation process even it is provided just a small labeled dataset. With our proposed evaluation process, only top labeling functions and the best label model are selected to produce Ô¨Ånal labels to unlabeled data. Contributions of our study are followings: ‚Ä¢AutoWSisafullyautomatedframeworkso that users neither need to provide any la beling heuristics/functions nor tune hyper parameters. ‚Ä¢Our experiments cover a wide variety of data domains including news, users‚Äô request, product titles, and emphasizes datasets with a large number of classes. While weak supervision has been studied for many years, its application on many class classiÔ¨Åcation tasks was not compre hensively evaluated. ‚Ä¢With a capability of selecting top labeling functions and best label models, AutoWS outperforms prior studies on benchmark data. 2 Related Work "
268,Video Representation Learning and Latent Concept Mining for Large-scale Multi-label Video Classification.txt,"We report on CMU Informedia Lab's system used in Google's YouTube 8 Million
Video Understanding Challenge. In this multi-label video classification task,
our pipeline achieved 84.675% and 84.662% GAP on our evaluation split and the
official test set. We attribute the good performance to three components: 1)
Refined video representation learning with residual links and hypercolumns 2)
Latent concept mining which captures interactions among concepts. 3) Learning
with temporal segments and weighted multi-model ensemble. We conduct
experiments to validate and analyze the contribution of our models. We also
share some unsuccessful trials leveraging conventional approaches such as
recurrent neural networks for video representation learning for this
large-scale video dataset. All the codes to reproduce our results are publicly
available at https://github.com/Martini09/informedia-yt8m-release.","Ranging from booming personal video collections, surveillance recordings, and professional video documen tary archives, we have witnessed an unprecedented growth of a wide range of video data. Numerous methods have been invented to understand video contents and enable searching over huge volumes of accumulated video data. Recently re leased largescale video datasets such as Google‚Äôs YouTube 8 Million (Youtube8M) video collection bring advance ments in video understanding tasks and create new pos sibilities for many emerging applications such as person alized assistant like Google Home and Microsoft Cortana. Youtube 8M is a multilabel video classiÔ¨Åcation benchmark composed of preextracted Inceptionv3 features [23], la bels and their hierarchy in the knowledge graph over more than 8 million videos. The quantity makes Youtube8M a unique video classi 1https://github.com/Martini09/ informediayt8mreleaseÔ¨Åcation testbed. There are 5.7 millions training videos, 1.6 millions validation videos, and 0.8 testing videos respec tively. The length of videos range from 120 to 500 seconds. Framelevel features are extracted under 1 frame per second (FPS) sampling rate. Video level features are meanpooled from framelevel features. The size of topic theme pool is 4,716. Each video is with 3.4 labels on average. In compar ison to other weaklylabeled datasets [11], the precision is reasonably good (85%) while recall remains poor. Learning an effective model for video understanding at this scale is challenging for the three reasons: First, al though effort in extracting features at a scale of 8 million is alleviated, the provided framelevel features are prepos sessed with some unknown PCA and followed by a simple mean pooling to generate videolevel representation. We propose to learn an attentive pooling kernel followed by a reÔ¨Åned representation learning module to further boost model performance. Second, labels (classes/concepts) are assumed to be independent in the Youtube 8M dataset, which fails to capture the authentic underlying relationship (such as cooccurrence, exclusion and hierarchy) between concepts. We address this issue by learning and incorpo rating latent concepts for multilabel classiÔ¨Åcation. Third, multimodel ensemble at this scale is underexplored. We design a systematic model ensemble scheme and quantify the importance over heterogeneous models. Our contribution in this paper is threefold: 1) We in vestigate feasible neural architectures to enhance mixture of experts (MoE) model with reÔ¨Åned representation learn ing via residual links and hypercolumns. 2) We introduce a novel latent concept learning layer to capture relationships among concepts 3) We incorporate temporal segment data augmentation and leaveoneout ensemble to further boost classiÔ¨Åcation accuracy. 2. Related work "
269,Boundary Regularized Building Footprint Extraction From Satellite Images Using Deep Neural Network.txt,"In recent years, an ever-increasing number of remote satellites are orbiting
the Earth which streams vast amount of visual data to support a wide range of
civil, public and military applications. One of the key information obtained
from satellite imagery is to produce and update spatial maps of built
environment due to its wide coverage with high resolution data. However,
reconstructing spatial maps from satellite imagery is not a trivial vision task
as it requires reconstructing a scene or object with high-level representation
such as primitives. For the last decade, significant advancement in object
detection and representation using visual data has been achieved, but the
primitive-based object representation still remains as a challenging vision
task. Thus, a high-quality spatial map is mainly produced through complex
labour-intensive processes. In this paper, we propose a novel deep neural
network, which enables to jointly detect building instance and regularize noisy
building boundary shapes from a single satellite imagery. The proposed deep
learning method consists of a two-stage object detection network to produce
region of interest (RoI) features and a building boundary extraction network
using graph models to learn geometric information of the polygon shapes.
Extensive experiments show that our model can accomplish multi-tasks of object
localization, recognition, semantic labelling and geometric shape extraction
simultaneously. In terms of building extraction accuracy, computation
efficiency and boundary regularization performance, our model outperforms the
state-of-the-art baseline models.","Acquiring information about the structure on the surface of th e  earth without making physical conta ct is generally achieved by  the remote sensing techniques (Campbell, Wynne, 2011) .  Applications like digital mapping, land use analysis, disaster  monitoring and climate modelling largely use satellite images.  The satellite images have been important in the  creation of the  digital maps for the Geographic Information System (GIS) and  the building footprint information is playing an instrumental role  in urban planning, smart city construction and many others. In  addition, the building footprints with regulariz ed boundaries are  able to produce polygons of vector representation, which hold  stronger transferability over multiple GIS platforms therefore  having an expensive domain of applications. For example, the  regularized building polygons can produce more accur ate 3D  building models. Nonetheless, as satellite images are readily  available and accessible, so there is always a demand for better  quality of the building footprints. This demand has not yet been  properly fulfilled due to numerous challenges. Firstly, t he  building footprints on the GIS maps need the manual or semi  automatic procedure to reach the high precision, which is quite  timeconsuming and labour intensive. Secondly, the enormous  diversity of the outlooks of the building roofs creates barriers for  large scale building footprint extraction. Also, the geometric  potential of the satellite images has not been exploited. Due to  the pixel wised and grid based representation of the images, it‚Äôs  fairly demanding to learn the geometric information of polygon   shapes. In recent years, deep learning has brought a revolution in  Artificial Intelligence (AI).     Deep learning is largely been used in the fields of computer  vision, speech recognition, natural language processing, etc and   it can give some exceptional results as compared to many  traditional techniques in the remote sensing dom ain as well.    Motivated  by the challenges of regularized building footprint  extraction and to exploit the potential of deep learning, in this  study, we will present our model which is based on deep neural  networks. The model utilizes the spatial, semantic  and geometric  information to perform automatic building footprint extraction  and handle the problem of building boundary regularization.      Our general framework illustrated in Fig 1, takes advantage of  the typical supervised learning mechanism and can be considered  as the instance segmentation model combined with geometric  learning. Our framework can simultaneously recognize and  localize multiple objects, assign semantic labels at pixel level  and predict polygons of geometric shapes.             Figure 1. Framew ork of the Study       2. RELATED WORK   "
270,PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification.txt,"We propose using an adversarial autoencoder (AAE) to replace generative
adversarial network (GAN) in the private aggregation of teacher ensembles
(PATE), a solution for ensuring differential privacy in speech applications.
The AAE architecture allows us to obtain good synthetic speech leveraging upon
a discriminative training of latent vectors. Such synthetic speech is used to
build a privacy-preserving classifier when non-sensitive data is not
sufficiently available in the public domain. This classifier follows the PATE
scheme that uses an ensemble of noisy outputs to label the synthetic samples
and guarantee $\varepsilon$-differential privacy (DP) on its derived
classifiers. Our proposed framework thus consists of an AAE-based generator and
a PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands
Dataset Version II, the proposed PATE-AAE improves the average classification
accuracy by +$2.11\%$ and +$6.60\%$, respectively, when compared with
alternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while
maintaining a strong level of privacy target at $\varepsilon$=0.01 with a fixed
$\delta$=10$^{-5}$.","The speech signal contains a rich set of information [1] that encompasses gender, accent, speaking environment, and other speaker characteristics; therefore, protecting data privacy be comes a raising concern when speech data is used to deploy commercial speech applications. In recent years, public regu lations, e.g., GDPR [2] and CCPA [3], have been proposed to establish new guidelines related to data privacy measurement and identity protection in enduser applications. Recent works on model inversion attacks [4, 5] indeed highlighted the impor tance of data privacy when the original data proÔ¨Åle (e.g., facial images [4]) could be recovered from a machine learning model by using queryfree optimization techniques. Differential privacy [6] (DP) is an effective mechanism for ensuring individual data protection, and it has been deployed in several industrial systems [7, 8]1to protect customer‚Äôs sensi tive information by exploiting a sophisticated noisy perturbation scheme. The ""DP mechanism [6] provides a way to quantify a privacy loss and set up a privacy budget (e.g., a minimum "" value) for a given dataset. However, ""differential private mod els [7] need to be reÔ¨Åned in order to improve a degraded pre diction accuracy [9] caused by the DP noise. The private aggre 1Apple has also applied differential privacy with a pri vacy budget ( ""=8) based on an ofÔ¨Åcial document in https: //www.apple.com/privacy/docs/Differential_ Privacy_Overview.pdf .gation of teacher ensembles [10] (PATE) is a recently proposed solution that aims to combat the accuracy loss of the machine learning models while ensuring privacy requirements. PATE follows a teacherstudent architecture [11], where the teacher is an ensemble model. The underpinning idea in PATE is to leverage upon noisy outputs of aggregated teacher models to (re)label nonsensitive public data with DP guarantees. The PATE method and its improved version [12] were proven useful in reducing the model accuracy drop through a voting process during the noisy ensemble. Nonetheless, the teacherstudent learning process highly depends on a hypothesis [10, 12, 13] that there exists a sufÔ¨Åcient amount of public (nonsensitive) data to train the model. PATEGAN [13] tries to overcome this issue by incorporating a generative block jointly trained with the PATE block; the goal is providing enough synthetic data to train deep models effectively. Unfortunately, PATEGAN does not work well for high dimensional data synthesis (e.g., images), as demonstrated in recent studies [14, 15]. Moreover, generating speech samples is a challenging task, as shown in recent studies about neural vocoders [16, 17]. Teacher  Models Teacher  Models (b) Teacher  Models (c) Student  Model Unlabeled Data  (NonSensitive) Sensitive Data  (e.g., Identity) (a) Generative  Model  Label with noisy (differentiallyprivate)  ensemble output Public Accessible  (queryfree)  Synthesis  Figure 1: Private aggregation of teachers ensemble (PATE) learning process [10, 12]: (a) the teacher prediction models training from sensitive data; (b) a joint generative model (e.g., adversarial autoencoder [18] for audio synthesis in this study); (c) student prediction model training from nonsensitive data. In this study, we introduce an adversarial autoencoder [18] (AAE) based model into PATE to improve the generative pro cess for privacypreserving speech classiÔ¨Åcation. PATEAAE Ô¨Årst adapts an autoencoder to minimize a reconstruction loss, training on sensitive data. As shown in Fig. 1(a), the gener ative model produces synthetic data as nonsensitive samples. Meanwhile, the training data are divided into Iisolated sub sets to train individual teacher classiÔ¨Åers. For instance, Iis equal to 3in Fig. 1(b). The teacher classiÔ¨Åers then undergo an output aggregation process to generate noisy labels, which ensures the""differentially private protection. Finally, a student classiÔ¨Åer uses the labeled synthetic samples (nonsensitive data) for training its model. The proposed PATEAAE framework isarXiv:2104.01271v2  [cs.SD]  15 Jun 2021assessed with the Google Speech Commands Dataset Version II [19]. Our experimental evidence demonstrates competitive results in terms of synthetic sample quality and classiÔ¨Åcation accuracy with a strong ""DP guarantee ( "" < 1) considering established privacypreserving learning (PPL) works [13, 20]. To the best of the authors‚Äô knowledge, this is the Ô¨Årst attempt to introduce the PATE architecture into a speech classiÔ¨Åcation task. Moreover, the proposed solution is beneÔ¨Åted from adver sarial autoencoder block, with advantages over existing GAN solutions [4, 13] of having a better testlikelihood estimation. 2. Related Work "
271,Recurrent Neural Networks for Person Re-identification Revisited.txt,"The task of person re-identification has recently received rising attention
due to the high performance achieved by new methods based on deep learning. In
particular, in the context of video-based re-identification, many
state-of-the-art works have explored the use of Recurrent Neural Networks
(RNNs) to process input sequences. In this work, we revisit this tool by
deriving an approximation which reveals the small effect of recurrent
connections, leading to a much simpler feed-forward architecture. Using the
same parameters as the recurrent version, our proposed feed-forward
architecture obtains very similar accuracy. More importantly, our model can be
combined with a new training process to significantly improve re-identification
performance. Our experiments demonstrate that the proposed models converge
substantially faster than recurrent ones, with accuracy improvements by up to
5% on two datasets. The performance achieved is better or on par with other
RNN-based person re-identification techniques.","Person reidentification consists of associating different tracks of a person as they are captured across a scene by different cameras. There are many applications for this task. The most obvious one is videosurveillance. It is common in public spaces to deploy net works of cameras with nonoverlapping field of views that capture different areas. These networks produce large amount of data and it can be very timeconsuming to manually analyze the video feeds to keep track of the actions of a single person as they move across the various fields of view. Person reidentification allows this task to be automated and makes it scalable to keep track of the trajectories of a high number of different identities. Solving this problem can also be critical for home automation, where it is important to keep track of the location of a user as they move across the different rooms, for singlecamera person tracking in order to recover from occlusions, or for crowd dynamics understanding, among other tasks. The challenges inherent to this task are the variations in background, body pose, illumination and viewpoint. It is important to represent a person using a descriptor that is as robust as possible to these variations, while still being discriminative enough to be characteristic of a single person‚Äôs identity. A subclass of this problem is videobased reidentification, where the goal is to match a video of a person against a gallery of videos captured by different cameras, by opposition to imagebased (or singleshot) reidentification, where only a single view of a person is provided.Person reidentification has recently received rising attention due to the much improved performance achieved by methods based on deep learning. For videobased reidentification, it has been shown that representing videos by aggregating visual informa tion across the temporal dimension was particularly effective. Re current Neural Networks (RNNs) have shown promising results for performing this aggregation in multiple independent works [3,20,27,29,30,32,38]. In this paper, we analyze one type of archi tecture that uses RNNs for video representation. The contributions of this work are the following. We show that the recurrent network architecture can be replaced with a simpler nonrecurrent architec ture, without sacrificing the performance. Not only does this lower the complexity of the forward pass through the network, making the feature extraction easier to parallelize, but we also show that this model can be trained with an improved process that boosts the final performance while converging substantially faster. Finally, we obtain results that are on par or better than other published work based on RNN, but with a much simpler technique. 2 RELATED WORK "
272,Deep 3D Face Identification.txt,"We propose a novel 3D face recognition algorithm using a deep convolutional
neural network (DCNN) and a 3D augmentation technique. The performance of 2D
face recognition algorithms has significantly increased by leveraging the
representational power of deep neural networks and the use of large-scale
labeled training data. As opposed to 2D face recognition, training
discriminative deep features for 3D face recognition is very difficult due to
the lack of large-scale 3D face datasets. In this paper, we show that transfer
learning from a CNN trained on 2D face images can effectively work for 3D face
recognition by fine-tuning the CNN with a relatively small number of 3D facial
scans. We also propose a 3D face augmentation technique which synthesizes a
number of different facial expressions from a single 3D face scan. Our proposed
method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC
datasets, without using hand-crafted features. The 3D identification using our
deep features also scales well for large databases.","Face recognition has been an active research topic for many years. It is a challenging problem because the facial appearance and surface of a person can be vary greatly due to changes in pose, illumination, makeup, expression or hard occlusions. Recently, the performance of 2D face recognition sys tems [24, 29, 34] was boosted signiÔ¨Åcantly with the pop ularization of deep convolutional neural networks (CNN). It turns out that recent methods using CNN feature extrac tors trained on a massive dataset outperform conventional methods using handcrafted feature extractors, such as Lo cal Binary Pattern [2] or Fisher vectors [30]. Deep learning approaches require a large dataset to learn a face represen tation which is invariant to different factors, such as expres sions or poses. Large scale datasets of 2D face images can be easily obtained from the web. FaceNet [29] uses about (a) 2D Face recognitions with deep learning Deep CNN Dataset # ID # Img  FaceNet 8M 200M VGG Face 2.7K 2.6M DeepFace 4K 4.4M Dataset # ID # Img  ND 2006 0.8K 13K Bosphorus 0.1K 4K  BU3DFE 0.1K 2.5K‚Ä¶2D Faces in VGG Face 2D Face dataset  3D Face dataset Augmented 3D faces  (b) An illustration of proposed 3D face recognition with deep  learning Deep CNN= + ‚Ä¶ Figure 1: Challenges in 3D face recognition with deep learning due to the absence of massive datasets. (a) Datasets for 2D are large enough (200M at most) to train a DCNN. Images in VGG Face contain rich variations of expression, pose, occlusion, and illumination. (b) The number of 3D images is very limited (13K at most). Therefore, it is im portant to augment 3D faces to add variations for leaning a robust representation. 200M face images of 8M independent people as training data. VGG Face [24] assembled a massive training dataset containing 2.6M face images over 2.7K identities. With 3D modalities, recent research [19, 21, 31] has fo cused on Ô¨Ånding robust feature points and descriptors based on geometric information of a 3D face in a handcrafted manner. Those methods achieve good recognition perfor mances but involve relatively complex algorithmic opera tions to detect key feature points and descriptors as com pared to endtoend deep learning models. While some of these methods can do veriÔ¨Åcation in realtime, they often do not scale well for identiÔ¨Åcation tasks where a probe scan needs to be matched with a largescale gallery set. Com pared to publicly available 2D face databases, 3D scans are hard to acquire, and the number of scans and subjects in 1arXiv:1703.10714v1  [cs.CV]  30 Mar 2017public 3D face databases is limited. According to the sur vey in [25], the biggest 3D dataset is ND 2006 [10] which contains 13,450 scans over 888 individuals. It is small com pared to publicly available labeled 2D faces, and may not be sufÔ¨Åcient to train a deep convolutional neural network from scratch. Figure 1 shows available face datasets for both 2D and 3D and exhibits the challenges of 3D face recognition with deep learning. As a result, coping with the limited amount of available 3D data is challenging. We propose to leverage existing networks, trained for 2D face recognition, and Ô¨Ånetune them with a small amount of 3D scans in order to perform 3D to 3D surface matching. Another challenge intrinsic to the recognition tasks comes from the need to minimize intraclass variances (e.g., differences in the same individual under expression varia tions) while maximizing interclass variances (differences between persons). For faces, variations in expression im pact the 3D structure and can degrade the performance of recognition systems [25]. To address this issue, we propose to augment our 3D face database with synthesized 3D face data considering facial expressions. To augment training data, we use multilinear 3D morphable models in which the shape comes from the Basel Face Model [26], and the expression comes from FaceWarehouse [6]. To pass our 3D data to the 2Dtrained CNN, we project the point clouds onto a 2D image plane with an orthographic projection. To make our system robust to small alignment error, each 3D shape is augmented by rigid transformations: 3D rotations and translations before the projection. Some random patches are also added to the 3D data to simulate random occlusions (e.g., facial hair, covering by hands or artifacts). We Ô¨Ånetune a deep CNN trained for 2D face recognition, VGGFace [24], with the augmented data. Fig ure 2 illustrates our proposed method. We report perfor mances on standard public 3D databases: Bosphorus [28], BU3DFE [39], and 3DTEC [36]. Our contributions are as follows: 1. To our knowledge, this work is the Ô¨Årst to use a deep convolutional neural network for 3D face recognition. We frontalize a 3D scan, generate a 2.5D depth map, extract deep features to represent the 3D surface, and match the feature vector to perform 3D face recogni tion. 2. We propose a 3D face augmentation method that gen erates a number of person speciÔ¨Åc 3D shapes with ex pression changes from a single raw 3D scan, which allows us to enlarge a limited 3D dataset and improve the performance of 3D face recognition in the presence of expression variations. 3. We have validated our approach on 3 standard datasets. Our method shows comparable results to the state ofthe art algorithms while enabling efÔ¨Åcient 3D match ing for largescale galleries. An overview of our framework is presented in Figure 2. The rest of the paper is organized as follows: Section 2 re views the related work. Section 3 describes our proposed method. Our augmentation and performances on the public 3D databases are evaluated in Section 4. Section 5 con cludes the paper. 2. Related work "
273,Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling.txt,"Social media has effectively become the prime hub of communication and
digital marketing. As these platforms enable the free manifestation of thoughts
and facts in text, images and video, there is an extensive need to screen them
to protect individuals and groups from offensive content targeted at them. Our
work intends to classify codemixed social media comments/posts in the Dravidian
languages of Tamil, Kannada, and Malayalam. We intend to improve offensive
language identification by generating pseudo-labels on the dataset. A custom
dataset is constructed by transliterating all the code-mixed texts into the
respective Dravidian language, either Kannada, Malayalam, or Tamil and then
generating pseudo-labels for the transliterated dataset. The two datasets are
combined using the generated pseudo-labels to create a custom dataset called
CMTRA. As Dravidian languages are under-resourced, our approach increases the
amount of training data for the language models. We fine-tune several recent
pretrained language models on the newly constructed dataset. We extract the
pretrained language embeddings and pass them onto recurrent neural networks. We
observe that fine-tuning ULMFiT on the custom dataset yields the best results
on the code-mixed test sets of all three languages. Our approach yields the
best results among the benchmarked models on Tamil-English, achieving a
weighted F1-Score of 0.7934 while scoring competitive weighted F1-Scores of
0.9624 and 0.7306 on the code-mixed test sets of Malayalam-English and
Kannada-English, respectively.","Socialmediahasbecomeapopularcontrivanceofcommunicationinthe21stcenturyandisthe‚Äúdemocratisationof information‚Äù by converting people into publishers from the conventional readers (Nasir Ansari et al., 2018). 53.6% of the world‚Äôs population use social media (ChaÔ¨Äey, 2021) which comprises a vivid structure between users from various backgrounds (Kapoor et al., 2018). With its free expressing environment, it witnesses much content, including images, videosandcommentsfromvariousagegroupsbelongingtodiverseregions,languagesandinterests. Whilethebasic ideaofsocialmediaremainstobecommunicationandentertainment,usersareseenusingrudeanddefamatorylanguage to express their views. Users might not appreciate such comments or posts and might be inÔ¨Çuential on teenagers. OÔ¨Äensive posts targeted on a group or an individual can lead to frustration, depression and distress (Kawate and Patil, 1https://github.com/adeepH/DravidianOLI <Equal Contribution < <Corresponding Author adeeph18c@iiitt.ac.in (A. Hande); karthikp18c@iiitt.ac.in (K. Puranik); konthalay18c@iiitt.ac.in (K. Yasaswini); rubapriyadharshini.a@gmail.com (R. Priyadharshini); sajeethas@esn.ac.lk (S. Thavareesan); anbu.1318@gmail.com (A. Sampath); kogilavani.sv@gmail.com (K. Shanmugavadivel); theni_d@ssn.edu.in (D. Thenmozhi); bharathi.raja@insightcentre.org (B.R. Chakravarthi) ORCID(s):0000000220034836 (A. Hande); 0000000255362258 (K. Puranik); 0000000158454759 (K. Yasaswini); 0000000323231701 (R. Priyadharshini); 0000000262525393 (S. Thavareesan); 0000000302268150 (A. Sampath); 000000020715143X (K. Shanmugavadivel); 0000000306816628 (D. Thenmozhi); 0000000245757934 (B.R. Chakravarthi) Hande et al.: Preprint submitted to Elsevier Page 1 of 27arXiv:2108.12177v1  [cs.CL]  27 Aug 2021OÔ¨Äensive Language IdentiÔ¨Åcation 2017; Puranik et al., 2021). Researchers recognised the need to detect and remove oÔ¨Äensive content from social media platformsfor along period. However,there werea fewchallengesfaced inthis Ô¨Åeld. Thoughautomating thisprocess with the help of supervised machine learning models gave better accuracy than human moderators (Zampieri et al., 2020),thelatterwerepreferredastheycouldjustifytheirdecisioninremovingthecomment/postfromtheplatform (Rischetal.,2020). Secondly,mostofthecommentsandpostsmadewereincodemixedunderresourcedlanguages (Chakravarthi et al., 2020a). There was an absence of enough datasets and tools to produce stateoftheart results to be implementedintheseplatforms. OurpaperpresentssomeuniqueapproachestogiveexcellentF1scoresforcodemixed Dravidian languages, mainly Tamil, Malayalam, and Kannada. Social mediacreates awhole newopportunity in theÔ¨Åeld ofresearch. NonEnglish speakerstend touse phonetic typing,Romanscripts,transliteration,codemixingandmixingseverallanguagesinsteadofUnicode1. Codemixed sentences for Dravidian languages can be Intersentential which consists of pure Dravidian languages written in Latin script, Codeswitching at a morphological level when it is written in both Latin and the Dravidian language and an IntrasententialmixofEnglishandtheDravidianlanguagewritteninLatinscript(Yasaswinietal.,2021). Theforemost stepinanalysingcodemixedorcodeswitcheddataincludeslanguagetagging,which,ifnotaccurate,canaÔ¨Äectthe results of other tasks. Language tagging has evolved over the years but is not yet satisfactory for analysing codemixed data(MandalandSingh,2018). Thepastyearshaveseenenormousencouragementandresearchincodemixingof underresourcedlanguagesduetooverÔ¨Åtting. Oneofthemainissuesofdealingwithcodemixedlanguagesarethe lackofannotateddatasetsandlanguagesmodelsbeingpretrainedoncodemixedtexts. Thelackofcodemixeddata resultedinconstriction ofdatacrisis,which aÔ¨Äectedtheperformanceofvarious tasks. Transliterationofcodemixed data can increase the size of the input dataset. Transliteration refers to converting a word from one language to another while protecting the semantic meaning of the utterance and obeying the syntactic structure in the target language. The pronunciation of the source word is maintained as much as possible. While trying to get the critical features from a text or translating from one language to another, some language pairs like English/Spanish might not encounter any issue as Porfavor iswrittenas Porfavor inEnglishastheysharethesameLatinscript. However,performingsuchtaskson Dravidianlanguagesmightposeproblems, andtransliterationcansolvethemtoanextent(KnightandGraehl,1997). Supervised learning on small datasets or languages with limited resources can be complex. Thus, pseudolabeling (Lee, 2013) can be employed to increase the performance considerably. In pseudolabeling, the model is trained on labeled data to predict labels for a batch of unlabeled data. The predictions are then fed into the model as pseudo labelled data. 1.1. Research Questions In this paper, we attempt to address the following research questions: 1.What architectures can be employed for eÔ¨Äective crosslingual knowledge transfer among codemixed languages for oÔ¨Äensive language identiÔ¨Åcation? We evaluate several recent approaches for oÔ¨Äensive language identiÔ¨Åcation, primarily focusing on crosslingual transfer due to the persistence of codemixed instances in the dataset. We have also used several stateoftheart pretrained multilingual language models for three languages, Kannada, Malayalam, and Tamil. 2.How do we break the curse of the lack of data for underresourced Dravidian languages? To overcome the barrier of lack of data, we revisit pseudolabeling. We transliterate the dataset to the respective Dravidian language for our multilingual dataset and generate labels by using approaches such as pseudolabeling. We combine the two datasets to form a larger dataset. 1.2. Contribution 1.WeproposeanapproachtoimproveoÔ¨ÄensivelanguageidentiÔ¨Åcationbyemphasisingmoreonconstructinga biggerdataset,generatingthepseudolabelsonthetransliterateddataset,andcombiningthelatterwiththeformer to have extensive amounts of data for training. 2.We experiment withmultilingual languages models separately on theprimarydataset, the transliterated dataset, and the newly constructed combination of both the datasets to examine if an increase in training data would improve the overall performance of the language models. We observe that this approach yields the bestweighted F1Scores on all three languages concerning its counterparts. 3.Wehaveshownthatourmethodworksforthreeunderresourcedlanguages,namelyKannada,Malayalamand Tamil, in a codemixed setting. We also have compared our approaches to all other models that have been benchmarked on the datasets. 1https://amitavadas.com/CodeMixing.html Hande et al.: Preprint submitted to Elsevier Page 2 of 27OÔ¨Äensive Language IdentiÔ¨Åcation The rest of our work is organised as follows. Section 2 talks about the related work on oÔ¨Äensive language identiÔ¨Åcation, while Section 3 entails a discussion about the Dravidian languages and their histories. Section 4 introduces the dataset usedforthetaskathand. Section5discussestheseveralmodelsandapproachestotesttheirÔ¨Ådelityontheoriginalcode mixeddataset,pseudolabelsprocuredforthetransliterateddataandthecombinationoftheboth. Section6comprisesa detailedanalysisconcerningthebehaviourandresultsofthepretrainedmodelswhenÔ¨Ånetunedoncodemixedand transliterated data, and the results are compared with other approaches (Chakravarthi et al., 2021a) from the shared task conducted by DravidianLangTech20212at EACL 2021. We also perform the error analysis on the Kannada and Tamil predictions. Finally,Section7concludesourworkandtalksaboutpotentialdirectionsforfutureworkonOÔ¨Äensive Language IdentiÔ¨Åcation in Dravidian languages. 2. Related Work "
274,Detecting changes in dynamic social networks using multiply-labeled movement data.txt,"The social structure of an animal population can often influence movement and
inform researchers on a species' behavioral tendencies. Animal social networks
can be studied through movement data; however, modern sources of data can have
identification issues that result in multiply-labeled individuals. Since all
available social movement models rely on unique labels, we extend an existing
Bayesian hierarchical movement model in a way that makes use of a latent social
network and accommodates multiply-labeled movement data (MLMD). We apply our
model to drone-measured movement data from Risso's dolphins (Grampus griseus)
and estimate the effects of sonar exposure on the dolphins' social structure.
Our proposed framework can be applied to MLMD for various social movement
applications.","For many species, analyzing social networks is necessary for understanding an imal behavior (e.g., Couzin et al., 2005). The underlying social network of an animal population can strongly in uence movement (Scharf et al., 2016, 2018) and provide researchers with information on the behavioral characteristics of a species. Such information is benecial to various ecological applications, includ ing determining the eects of anthropogenic activities, landscape fragmentation, or pollutants on the collective behavior of an animal population. Many marine mammals, including Risso's dolphins ( Grampus griseus ), are highly social with closeknit relationships (Lusseau David, 2004; Hartman et al., 2008; Weiss et al., 2021; Whitehead and Rendell, 2021). With such an anity for social interactions, they may be expected to exhibit a collective behavioral re sponse when exposed to changes in their environment. For example, G. Griseus 1arXiv:2204.00542v2  [stat.ME]  1 Nov 2022Z. Boulil et al./Dynamic social networks using multiplylabeled movement data 2 are commonly exposed to midfrequency active sonar due to their high abun dance in nearcoastal navy training areas (Carretta et al., 2019; Rice et al., 2020). Nonetheless, there is a sparsity of data on how dolphins respond to sonar and uncertainty about the consequences of exposure (Durban et al., 2022). Thus, our motivating application is studying G. Griseus exposed to sonar o Catalina Island in Southern California in a controlled exposure experiment (CEE), de signed to ll these key data gaps. Currently available social movement models that account for social interac tions between individuals require unique identication of animals and assume a closed, fully observed population throughout the study period (e.g., Langrock et al., 2014; Scharf et al., 2016, 2018; Niu et al., 2020; Scharf and Buderman, 2020; Milner et al., 2021). Movement data of terrestrial and marine animals have been conventionally obtained by tracking individuals singularly, often with telemetry tags. However, drones now oer the ability to track multiple individ uals simultaneously (Durban et al., 2022). While this new source of movement data can achieve a high spatial precision (Dawson et al., 2017; Durban et al., 2015, 2022), it sometimes introduces complex labeling issues due to the obser vation process. When animals that cannot be uniquely identied disappear out of a drone's active eld of view for sustained intervals of time, they are typically assigned a new label upon reappearance. The lack of identication leads to prob lematic \multilabeling"" as well as varying numbers of animals in view at a given time point. Thus, new methods are needed that allow for social network infer ence from multiplylabeled movement data (MLMD) such as those obtained via drones. We extend existing methodology for animal movement to estimate the eects of sonar exposure on an unobserved social network of dolphins through the use of dronemeasured movement data. Our modeling approach allows us to infer social connections between dolphins directly from the dronemeasured movement data. We adopt a Bayesian approach through the implementation of a discrete time continuousspace Gaussian Markov Random Field (GMRF; Rue and Held, 2005) with an underlying dynamic social network. The two main behavioral components motivating the model are attraction and alignment, both of which are directly related to social structure. Attraction represents an individual's inclination toward the mean position of connected individuals, while alignment represents an individual's tendency to move in parallel with the trajectories of connected individuals (Bode et al., 2012). The model also allows for repulsive and/or antialigning behavior by allowing negative values of relevant parameters, however, such behaviors are unlikely to occur in our motivating application. Thus, we limit our framework to modeling the positive behaviors of attraction and alignment. As social structure is expected to play a role in the movement of animals in relation to one another, the attraction and alignment mechanisms are motivating elements in uencing animal movement. Additionally, the model captures the overall stability of the social network and measures the density of social connections as it varies with time. We incorporate generalized linear models (GLMs) on certain parameters of interest as an extension to the model introduced in Scharf et al. (2016). TheZ. Boulil et al./Dynamic social networks using multiplylabeled movement data 3 GLMs provide information on the direct impact of external sources on the be havioral characteristics of a population, which allow for assessing the eect of environmental covariates on social structure. By including GLMs in the mod eling framework, we provide a deeper understanding of how external sources might in uence the collective movement of a population. While the closely related model from Scharf et al. (2016) provides a tractable likelihood for individuals with unique labels, the multiplylabeled nature of MLMD makes calculating the likelihood infeasible. To accommodate the in tractability of the exact likelihood, we develop an implementation scheme based on a proxy likelihood, which we evaluate through a simulation study. We detail our inference of social networks when analyzing MLMD in Section 2 and 3 and validate our approach using an approximate likelihood through a simulation study in Section 4. We apply our methodology to dronemeasured movement data following many Risso's dolphins simultaneously as they are ex posed to sonar in Section 5. We found that during periods of sonar exposure, dolphins are more likely to exhibit an inclination towards the mean position of connected individuals, less likely to move in parallel with connected individuals, and tend to have fewer social connections. We close with potential for future directions in Section 6 including applying our model to subsets of a population when analyzing data sets with a large number of individuals. 2. Methods "
275,Generative Knowledge Transfer for Neural Language Models.txt,"In this paper, we propose a generative knowledge transfer technique that
trains an RNN based language model (student network) using text and output
probabilities generated from a previously trained RNN (teacher network). The
text generation can be conducted by either the teacher or the student network.
We can also improve the performance by taking the ensemble of soft labels
obtained from multiple teacher networks. This method can be used for privacy
conscious language model adaptation because no user data is directly used for
training. Especially, when the soft labels of multiple devices are aggregated
via a trusted third party, we can expect very strong privacy protection.","Neural network based language models (LMs) are used in many Ô¨Åelds such as speech recognition, chatbot, sentence completion and machine translation (Mikolov et al., 2010; Serban et al., 2015; Spithourakis et al., 2016; Mirowski & Vlachos, 2015; Cho et al., 2014). Training such LMs re quires a large amount of training data. A straightforward way of gathering a large amount of training data is to col lect user data through mobile or the internet connected de vices. However, since device users are increasingly reluc tant to leak their privacy, it becomes important to collect data while protecting the privacy. Even after training the LM once, it needs to be updated for the purpose of user adaptation or adding new expression. However, it is not desired to use the user data directly. Instead of collecting sensitive user data, the model pa rameters of a neural network adapted to a user can be used for training a new model by the knowledge transfer method (Hinton et al., 2014). However, this approach can also cause an unwanted privacy violation by an adversary 1Department of Electrical and Computer Engineering, Seoul National University, Seoul, 08826 Korea. Correspondence to: Sungho Shin <sungho.develop@gmail.com >, Wonyong Sung <wysung@snu.ac.kr >.through a machine learning model attack. If the adversary can access the machine learning model, the output of the model can be used to restore the face of the individual used in the training (Fredrikson et al., 2015). In the case of text data, similar attacks are possible because an LM can be used as a text generator for generating the sensi tive user data used for training the model (Sutskever et al., 2011; Graves, 2013). Therefore, even the model parame ters trained with sensitive data should not allow direct ac cess to the adversary. Ensemble of knowledge aggregation can increase the se curity of personal data. Ensemble methods combine the results of multiple classiÔ¨Åers to improve the performance of machine learning algorithms (Dietterich, 2000). Sev eral studies trained private classiÔ¨Åers to produce a Ô¨Ånal distributable classiÔ¨Åer in various ways, such as averaging the model parameters of teacher networks (Pathak et al., 2010), training hard labels by voting the ensemble of all the teachers (Papernot et al., 2016), or using soft labels (Hamm et al., 2016). In this process, the Ô¨Ånal classiÔ¨Åer mixes ran dom noise, such as Laplacian or Gaussian noise, to hide information about a speciÔ¨Åc person and achieve strong dif ferential privacy (Dwork et al., 2006; 2014). In this paper, we propose a method that efÔ¨Åciently trans fers personal text data information for training a recurrent neural network (RNN) based LM while minimizing privacy infringement. This method sends the soft labels generated by the teacher networks instead of sending personal data or model parameters, and the soft outputs obtained from the individual users are aggregated for training a student net work by a reliable third party. The proposed GKT trains the student network using only the generated data and la bels without the original training data, by operating RNN LMs as a generative model. The text generation can be conducted by either the teacher or the student network. This paper is composed as follows. Section 2 introduces the related work and Section 3 describes the proposed GKT. Section 4 describes the GKT by ensemble of multiple LMs, Section 5 shows the experimental results, and Section 6 concludes this paper.arXiv:1608.04077v3  [cs.LG]  28 Feb 2017Generative Knowledge Transfer for Neural Language Models (a) Teacherdriven generative knowledge transfer (TDGKT) (b) Studentdriven generative knowledge transfer (SDGKT) Figure 1. Two different schemes of GKT. Text sequence genera tion (green lines) can be produced by the teacher (TDGKT) or the student network (SDGKT). 2. Related Work on Knowledge Transfer "
276,Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution.txt,"Crowd sourcing has become a widely adopted scheme to collect ground truth
labels. However, it is a well-known problem that these labels can be very
noisy. In this paper, we demonstrate how to learn a deep convolutional neural
network (DCNN) from noisy labels, using facial expression recognition as an
example. More specifically, we have 10 taggers to label each input image, and
compare four different approaches to utilizing the multiple labels: majority
voting, multi-label learning, probabilistic label drawing, and cross-entropy
loss. We show that the traditional majority voting scheme does not perform as
well as the last two approaches that fully leverage the label distribution. An
enhanced FER+ data set with multiple labels for each face image will also be
shared with the research community.","Understanding the unspoken words from facial and body cues is a fundamental human trait, and such aptitude is vi tal in our daily communications and social interactions. In research communities such as human computer interaction (HCI), neuroscience and computer vision, scientists have conducted extensive research to understand human emo tions. Such studies would allow creating computers that can understand human emotions as well as ourselves, and lead to seamless interactions between human and computers. Among many inputs that can be used to derive emotions, facial expression is by far the most popular. One of the pi oneer works by Paul Ekman [10] identied 6 emotions that are universal across dierent cultures. Later, Ekman [11] developed the Facial Action Coding System (FACS), which became the standard scheme for facial expression research. Facial expression analysis can thus be conducted by analyz ing facial action units for each of the facial parts (eyes, nose, mouth corners, etc.), and map them into FACS codes [30]. Unfortunately, FACS coding requires professionally trained coders to annotate, and there are very few existing data sets that are available for learning FACS based facial expressions, in particular for unconstrained realworld images.With the latest advances in machine learning, it is more and more popular to recognize facial expressions directly from input images. Such appearancebased approaches have the advantage that the ground truth labels may be abun dantly obtained through crowdsourcing platforms [1]. The cost of tagging a holistic facial emotion is often on the or der of 12 US cents, which is orders of magnitude cheaper than FACS coding. On the other hand, crowdsourced la bels are usually much noisier than FACS codes annotated by specially trained coders. This can be attributed to two main reasons. First, emotions are very subjective, and it is very common that two people have diametrically dier ent opinions on the same face image. Second, the workers in crowdsourcing platforms are paid very low, and their incen tive is more on getting more work done rather than ensuring the tagging quality. Consequently, crowdsourced labels on emotions exhibit only 65 5% accuracy, as reported for the original FER data set [12]. In this paper, we adopt the latest deep convolutional neu ral networks (DCNN) architecture, and evaluate the eec tiveness of four dierent schemes to train emotion recog nition on crowdsourced labels. In order to overcome the noisy label issue, we asked 10 crowd taggers to relabel each image in the FER data set, resulting in a new data set named FER+[2]. Then, we change the cost function of the DCNN based on dierent schemes using the distribution of tags: majority voting, multilabel learning, probabilistic label drawing, and crossentropy loss. We compare the per formance of the trained classiers and found the last two schemes to be the most eective to train emotion recogni tion classiers based on noisy labels. The rest of the paper is organized as follows. Related works are discussed in Section 2 and a description of the FER+ data set is introduced in Section 3. Then, the four schemes for DCNN training are presented in Section 4 while experimental results and conclusions are given in Section 5 and 6, respectively. 2. RELATED WORK "
277,LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification.txt,"Weakly supervised text classification methods typically train a deep neural
classifier based on pseudo-labels. The quality of pseudo-labels is crucial to
final performance but they are inevitably noisy due to their heuristic nature,
so selecting the correct ones has a huge potential for performance boost. One
straightforward solution is to select samples based on the softmax probability
scores in the neural classifier corresponding to their pseudo-labels. However,
we show through our experiments that such solutions are ineffective and
unstable due to the erroneously high-confidence predictions from poorly
calibrated models. Recent studies on the memorization effects of deep neural
models suggest that these models first memorize training samples with clean
labels and then those with noisy labels. Inspired by this observation, we
propose a novel pseudo-label selection method LOPS that takes learning order of
samples into consideration. We hypothesize that the learning order reflects the
probability of wrong annotation in terms of ranking, and therefore, propose to
select the samples that are learnt earlier. LOPS can be viewed as a strong
performance-boost plug-in to most of existing weakly-supervised text
classification methods, as confirmed in extensive experiments on four
real-world datasets.","Weakly supervised text classiÔ¨Åcation meth ods (Agichtein and Gravano, 2000; Riloff et al., 2003; Tao et al., 2015; Meng et al., 2018; Mekala and Shang, 2020; Mekala et al., 2020, 2021) typi cally start with generating pseudolabels, and train a deep neural classiÔ¨Åer to learn the mapping be tween documents and classes. There is no doubt that the quality of pseudolabels plays a fundamen tal role in the Ô¨Ånal classiÔ¨Åcation accuracy, how ever, they are inevitably noisy due to their heuristic ¬òJingbo Shang is the corresponding author. 0.10.20.30.40.50.60.70.80.91.0 Probability0.00.20.40.60.81.0Proportion of SamplesPrediction Probability Distribution Correct Wrong(a) 1 2 3 4 Never Epoch0.00.20.40.60.81.0Proportion of Samples learntProportion of Samples vs Learnt Epoch Correct Wrong (b) Figure 1: Distributions of correct and wrong instances using different pseudolabel selection strategies on the NYTCoarse dataset for its initial pseudolabels. The base classiÔ¨Åer is BERT. (a) is based on the softmax probability of samples‚Äô pseudolabels and (b) is based on the earliest epochs at which samples are learnt. nature. Pseudolabels are typically generated by some heuristic, for example, through string match ing between the documents and userprovided seed words (Mekala and Shang, 2020). Deep neural net works (DNNs) trained on such noisy labels have a high risk of making erroneous predictions. More importantly, when selftraining is employed, such error can be further ampliÔ¨Åed upon boostrapping. To address this problem, in this paper, we study the pseudolabel selection in weakly supervised text classiÔ¨Åcation, aiming to select a high quality subset of the pseudolabeled documents (in every iteration when using selftraining) that can poten tially achieve a higher classiÔ¨Åcation accuracy. A straightforward solution is to Ô¨Årst train a deep neural classiÔ¨Åer based on the pseudolabeled doc uments and then threshold the documents by the predicted probability scores corresponding to their pseudolabels. However, DNNs usually have a poor calibration and generate overconÔ¨Ådent predicted probability scores (Guo et al., 2017). For exam ple, on New York Times (NYT) coarsegrained dataset, as shown in Figure 1(a), 60% of wrong instances in the pseudolabeled documents have a predicted probability by BERT greater than 0:9for their wrong pseudolabels. Recent studies on the memorization effects of DNNs show that they memorize easy and clean inarXiv:2205.12528v2  [cs.CL]  25 Oct 2022Unlabeled Documents‚Ä¶TextLabel ‚ùì ‚ùì ‚ùì ‚ñ™Tampa bay won NFL championship‚ñ™He was banned by football federation..‚ñ™‚Äúcitizen kane‚Äù film music is composed by ‚Ä¶ Label Surface NamesSeed WordsWeak SupervisionMoviesCinematographerSongMusicSoccerGoalPenaltyFootballNFLMovieMusicSoccerFootballPseudolabeled Documents‚Ä¶Text‚ñ™Tampa bay won NFL championship‚ñ™He was banned by football federation..‚ñ™‚Äúcitizen kane‚Äù film music is composed by ‚Ä¶SoccerMusic ‚úÖ ‚ùå Football ‚úÖ LabelDeep Neural Classifier Learning Orderbased Label Selection123123123123MovieMusicSoccerFootball50%Selected Pseudolabeled Documents‚Ä¶Text‚ñ™Tampa bay won NFL championship‚ñ™He was banned by football federation..Soccer ‚úÖ Football ‚úÖ LabelProbing Classifier TrainInputSelftraining LOPS (Our proposed method)TrainBootstrap TrainFigure 2: An overview of our proposed LOPS and how it plugs into selftraining frameworks to replace the conven tional training step. Given pseudolabeled samples, LOPS trains a probing classiÔ¨Åer to obtain their learning order and we stop the training when at least %of samples corresponding to each class are learnt and select the learnt samples. The numbers shown are learnt epochs and the samples in the shaded part are selected. A text classiÔ¨Åer is trained on selected pseudolabeled documents that is further used for inference and bootstrapping. stances Ô¨Årst, and gradually learn hard instances and eventually memorize the wrong annotations (Arpit et al., 2017; Geifman et al., 2019; Zhang et al., 2021). We have conÔ¨Årmed this in our experiments for different classiÔ¨Åers. For example, as shown in Figure 1(b), BERT classiÔ¨Åer learns most of the clean instances in the Ô¨Årst epoch and learns wrong instances across all epochs. Although it also learns good number of wrong instances in the Ô¨Årst epoch, it is signiÔ¨Åcantly less than the probabilitybased selection in Figure 1(a). Therefore, we deÔ¨Åne the learning order of a pseudolabeled document as the epoch when it is learnt during training i.e. when the training model‚Äôs prediction is the same as its given pseudolabel. Since correct samples are learnt Ô¨Årst, we hypothesize that learning orderbased selection will be able to Ô¨Ålter out wrongly labeled samples. Inspired by our observation, we propose a novel learning order inspired pseudolabel selection method LOPS , as shown in Figure 2. SpeciÔ¨Å cally, LOPS involves training a probing classiÔ¨Åer on pseudolabeled data and tracking the learning order of samples. We deÔ¨Åne a sample is learnt if and only if the classiÔ¨Åer trained on pseudolabels gives the same argmax prediction as its pseudo label at the end of an epoch. We stop the training when at least %of samples corresponding to each class are learnt and select all the learnt samples. Then, we train a text classiÔ¨Åer on these selected pseudolabeled documents that is further used for inference. We empirically show that LOPS can boost the accuracy of various weakly supervisedtext classiÔ¨Åcation methods and it is much more effective and stable than probability scorebased selections. Our contributions are summarized as follows: ‚Ä¢We propose a novel pseudolabel selection method LOPS that takes learning order of sam ples into consideration. ‚Ä¢We show that selection based on learning order is much stable and effective than selection based on probability scores. ‚Ä¢Extensive experiments and case studies on real world datasets with different classiÔ¨Åers and weakly supervised text classiÔ¨Åcation methods demonstrate signiÔ¨Åcant performance gains upon using LOPS . It can be viewed as a solid performanceboost plugin for weak supervision. Reproducibility. We will release the code and datasets on Github1. 2 Related Work "
278,Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition.txt,"This paper describes our approach to the DSTL Satellite Imagery Feature
Detection challenge run by Kaggle. The primary goal of this challenge is
accurate semantic segmentation of different classes in satellite imagery. Our
approach is based on an adaptation of fully convolutional neural network for
multispectral data processing. In addition, we defined several modifications to
the training objective and overall training pipeline, e.g. boundary effect
estimation, also we discuss usage of data augmentation strategies and
reflectance indices. Our solution scored third place out of 419 entries. Its
accuracy is comparable to the first two places, but unlike those solutions, it
doesn't rely on complex ensembling techniques and thus can be easily scaled for
deployment in production as a part of automatic feature labeling systems for
satellite imagery analysis.","The signiÔ¨Åcant increase of satellite imagery has given a radically improved understanding of our planet. Object recognition in aerial imagery enjoys growing interest to day, due to the recent advancements in computer vision and deep learning, along with important improvements in lowcost highperformance GPUs. The possibility of ac curately distinguishing different types of objects in aerial images, such as buildings, roads, vegetation and other cate gories, could greatly help in many applications, such as cre ating and keeping uptodate maps, improving urban plan ning, environment monitoring, and disaster relief. Besides the practical need for accurate aerial image interpretation systems, this domain also offers scientiÔ¨Åc challenges to the computer vision. In this paper, we describe and analyze these challenges for the speciÔ¨Åc satellite imagery dataset from a Kaggle com petition. We explore the challenges faced due to the small size of the dataset, the speciÔ¨Åc character of data, and super vised and unsupervised machine learning algorithms thatare suitable for this kind of problems. Our efforts can be summarized as follows: We adapted fully convolutional network to multispec tral input data and evaluated several data fusion strate gies on semantic segmentation task of satellite images. We introduced joint training objective that properly de Ô¨Ånes desired output for the segmentation task. We analyze local and global boundary effect on overall performance of the segmentation pipeline. 2. Related Work "
279,Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels.txt,"Performing controlled experiments on noisy data is essential in understanding
deep learning across noise levels. Due to the lack of suitable datasets,
previous research has only examined deep learning on controlled synthetic label
noise, and real-world label noise has never been studied in a controlled
setting. This paper makes three contributions. First, we establish the first
benchmark of controlled real-world label noise from the web. This new benchmark
enables us to study the web label noise in a controlled setting for the first
time. The second contribution is a simple but effective method to overcome both
synthetic and real noisy labels. We show that our method achieves the best
result on our dataset as well as on two public benchmarks (CIFAR and
WebVision). Third, we conduct the largest study by far into understanding deep
neural networks trained on noisy labels across different noise levels, noise
types, network architectures, and training settings. The data and code are
released at the following link: http://www.lujiang.info/cnlw.html","Performing experiments on controlled noise is essential in understanding Deep Neural Networks (DNNs) trained on noisy labeled data. Previous work performs controlled experiments by injecting a series of synthetic label noises into a wellannotated dataset such that the dataset‚Äôs noise level can vary, in a controlled manner, to reÔ¨Çect different magnitudes of label corruption in real applications. Through studying controlled synthetic label noise, researchers have discovered theories and methodologies that have greatly fostered the development of this Ô¨Åeld. 1Google Research, Mountain View, United States2Google Cloud AI, Sunnyvale, United States3Cornell University, Ithaca, United States. Correspondence to: Lu Jiang <lu jiang@google.com >. Proceedings of the 37thInternational Conference on Machine Learning , Online, PMLR 119, 2020. Copyright 2020 by the au thor(s).However, due to the lack of suitable datasets, previous work has only examined DNNs on controlled synthetic label noise, and realworld label noise has never been studied in a con trolled setting. This leads to two major issues. First, as synthetic noise is generated from an artiÔ¨Åcial distribution, a tiny change in the distribution may lead to inconsistent or even contradictory Ô¨Åndings. For example, contrary to the common understanding that DNNs trained on synthetic noisy labels generalize poorly (Zhang et al., 2017), Rolnick et al. (2017) showed that DNNs can be robust to massive label noise when the noise distribution is made slightly different. Due to the lack of datasets, these Ô¨Åndings, unfor tunately, have not yet been veriÔ¨Åed beyond synthetic noise in a controlled setting. Second, the vast majority of previous studies prefer to verify robust learning methods on a spec trum of noise levels because the goal of these methods is to overcome a wide range of noise levels. However, current evaluations are limited because they are conducted only on synthetic label noise. Although there do exist datasets of real label noise e.g.WebVision (Li et al., 2017a), Clothing 1M (Xiao et al., 2015), etc, they are not suitable for con trolled evaluation in which a method must be systematically veriÔ¨Åed on multiple different noise levels, because the train ing images in these datasets are not manually labeled and hence their data noise level is Ô¨Åxed and unknown. In this paper, we study a realistic type of label noise in a con trolled setting called web labels. ‚ÄúWeblylabeled‚Äù images are commonly used in the literature (Bootkrajang & Kab ¬¥an, 2012; Li et al., 2017a; Krause et al., 2016; Chen & Gupta, 2015), in which both images and labels are crawled from the web and the noisy labels are automatically determined by matching the images‚Äô surrounding text to a class name during web crawling or equivalently by querying the search index afterward. Unlike synthetic labels, web labels follow a realistic label noise distribution but have not been studied in a controlled setting. We make three contributions in this paper. First, we establish the Ô¨Årst benchmark of controlled web label noise, where each training example is carefully annotated to indicate whether the label is correct or not. SpeciÔ¨Åcally, we auto matically collect images by querying Google Image Search using a set of class names, have each image annotated by 35 workers, and create training sets of ten controlled noise levels. As the primary goal of our annotation is to identifyarXiv:1911.09781v3  [cs.LG]  27 Aug 2020Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels images with incorrect labels, to obtain a sufÔ¨Åcient number of these images we have to collect a total of about 800,000 annotations over 212,588 images. The new benchmark en ables us to go beyond synthetic label noise and study web label noise in a controlled setting. For convenience, we will refer it as web label noise (or red noise ) to distinguish it from synthetic label noise (or blue noise )1. Second, this paper introduces a simple yet highly effective method to overcome both synthetic and realworld noisy labels. It is based on a new idea of minimizing the empirical vicinal risk using curriculum learning. We show that it con sistently outperforms baseline methods on our datasets and achieves stateoftheart performance on two public bench marks of synthetic and realworld noisy labels. Notably, on the challenging benchmark WebVision 1.0 (Li et al., 2017a) that consists of 2.2 million images of realworld noisy labels, it yields a signiÔ¨Åcant improvement of 3%in the top1 accu racy, achieving the bestpublished result under the standard training setting. Finally, we conduct the largest study by far into understand ing DNNs trained on noisy labels across a variety of noise types (blue and red), noise levels, training settings, and net work architectures. Our study conÔ¨Årms the existing Ô¨Åndings of Zhang et al. (2017) and Arpit et al. (2017) on synthetic labels, and brings forward new Ô¨Åndings that may challenge our preconceptions about DNNs trained on noisy labels. See the Ô¨Åndings in Section 5.2. It is worth noting that these Ô¨Ånd ings along with benchmark results are a result of conducting thousands of experiments using tremendous computation power (hundreds of thousands of V100 GPU hours). We hope our (i) benchmark, (ii) new method, and (iii) Ô¨Åndings will facilitate future deep learning research on noisy labeled data. We will release our data and code. 2. Related Work "
280,Dropout can Simulate Exponential Number of Models for Sample Selection Techniques.txt,"Following Coteaching, generally in the literature, two models are used in
sample selection based approaches for training with noisy labels. Meanwhile, it
is also well known that Dropout when present in a network trains an ensemble of
sub-networks. We show how to leverage this property of Dropout to train an
exponential number of shared models, by training a single model with Dropout.
We show how we can modify existing two model-based sample selection
methodologies to use an exponential number of shared models. Not only is it
more convenient to use a single model with Dropout, but this approach also
combines the natural benefits of Dropout with that of training an exponential
number of models, leading to improved results.","Noisy labels are ubiquitous in practice. For example, noise may appear due to disagreement in crowdsourc ing based annotation, Su et al. (2012), or annotations carried out by computer programs on web crawled images, Hu et al. (2017); Ratner et al. (2017). Conse quently, it is necessary to research techniques that are robust to noisy labeling. Multiple techniques have been developed to tackle this issue. Among them, sample selection is the one that we will focus on in this paper. Sample selection can be regarded as a derivative of curriculum learning, Bengio et al. (2009). In sample selection techniques a curriculum is deÔ¨Åned/learnt to select a subset of the data in each iteration of the training. MentorNet, Jiang et al. (2018), used a single network to select a minibatch in each iteration. Self paced MentorNet only considered samples with a loss lower than a certain threshold for training. Coteach ing, Han et al. (2018), upgraded the MentorNet byutilizing two Networks, where the minibatch used for training one network was decided by the loss obtained on the samples using the second network. Further, au thors of Coteachingplus , Yu et al. (2019), argued that thereshouldbedisagreementbetweenthetwonetworks, which can be beneÔ¨Åcial for the learning. Multiple tech niqueshavebeenproposedsincethentoperformsample selection based training. However, one general trend remains among these techniques, two networks are trained for sample selection Li et al. (2020b); Sachdeva et al. (2021); Feng et al. (2021); Wei et al. (2020a). We argue that instead of two networks, an exponential number of shared models can be utilized for sample selection. This can be achieved by utilizing Dropout, Srivastava et al. (2014), in a single network. Dropout when present in a network trains an ensemble of sub networks, thus, it can simulate an exponential number of shared models. We can utilize this property of Dropout to transform existing twomodel based ap proaches to utilize an exponential number of shared models by training just a single model with Dropout. This approach can combine the natural beneÔ¨Åts of Dropout, Srivastava et al. (2014); Gal and Ghahramani (2016) with the beneÔ¨Åts of training an exponential num ber of models for performing sample selection, resulting in improved performance for the existing approaches when transformed to use Dropout. Thus, our main contributions can be listed as follows ‚Ä¢We propose to replace the two modelbased sample selection algorithms found in the literature with algorithms using an exponential number of shared models. ‚Ä¢We show how an exponential number of models can be utilized for sample selection using Dropouts in a single network. ‚Ä¢We provide empirical results by transforming ex isting approaches utilizing two models to use an exponential number of shared models with the help of Dropout. Our results suggest that sucharXiv:2202.13203v1  [cs.LG]  26 Feb 2022Dropout can Simulate Exponential Number of Models for Sample Selection Techniques transformations lead to better results. 2 Related Works "
281,Analysis of an adaptive lead weighted ResNet for multiclass classification of 12-lead ECGs.txt,"Background: Twelve lead ECGs are a core diagnostic tool for cardiovascular
diseases. Here, we describe and analyse an ensemble deep neural network
architecture to classify 24 cardiac abnormalities from 12-lead ECGs.
  Method: We proposed a squeeze and excite ResNet to automatically learn deep
features from 12-lead ECGs, in order to identify 24 cardiac conditions. The
deep features were augmented with age and gender features in the final fully
connected layers. Output thresholds for each class were set using a constrained
grid search. To determine why the model made incorrect predictions, two expert
clinicians independently interpreted a random set of 100 misclassified ECGs
concerning Left Axis Deviation.
  Results: Using the bespoke weighted accuracy metric, we achieved a 5-fold
cross validation score of 0.684, and sensitivity and specificity of 0.758 and
0.969, respectively. We scored 0.520 on the full test data, and ranked 2nd out
of 41 in the official challenge rankings. On a random set of misclassified
ECGs, agreement between two clinicians and training labels was poor (clinician
1: kappa = -0.057, clinician 2: kappa = -0.159). In contrast, agreement between
the clinicians was very high (kappa = 0.92).
  Discussion: The proposed prediction model performed well on the validation
and hidden test data in comparison to models trained on the same data. We also
discovered considerable inconsistency in training labels, which is likely to
hinder development of more accurate models.","The 12lead electrocardiogram (ECG) provides critical information that assists in identifying cardiac abnormalities. The signal from each of the 12 leads corresponds to the heart's electrical activity from a distinct angle that can be mapped to the anatomy of thearXiv:2112.01496v1  [eess.SP]  1 Dec 2021Analysis of an adaptive lead weighted ResNet for multiclass classication of 12lead ECGs 2 heart. A skilled interpreter can therefore use ECG signals from multiple leads to localise the source of a cardiac abnormality. Expert cardiologists can identify abnormalities with high accuracy. A recent systematic review highlighted how the accuracy of human expert interpretation may be as high as 95% in a controlled setting in which the nal diagnosis was known [1]. However, expertlevel human ECG interpretation is limited by the availability of a trained cardiologist and the time required to synthesize information from the 12lead signal (and to document their ndings). In clinical practice, the absence of cardiologists means that other, nonspecialist, clinicians commonly make preliminary interpretations, but are demonstrably less accurate [2]. Computeraided methods for ECG interpretation have been suggested as one approach for circumventing these resource constraints. Historically, the accuracy of these methods has been poorer than humans [3]. For instance, Anh et. al's 2006 reported how 19% of atrial brillation were considered to be false positives when reviewed by a cardiologist [4]. Traditional machine learning approaches, in which salient features of the ECG signal are rst identied, have been successful for some use cases. As far back as 1991, the performance of some methods were almost as accurate as cardiologists, for a limited subset of clinical conditions [5]. However, such methods have frequently struggled to correctly interpret ECG with arrhythmias, conduction disorders and pacemaker rhythms [6]. Modern deep learning methods may be able to improve interpretation accuracy. Until recently, the use of such techniques for 12lead ECGs has been impractical due to the shortage of labelled training data. There remains room for improvement over initial promising results [7]. The public release of a new large labelled data set presents a fresh opportunity to revisit this challenging problem [8]. Here, we consider the task of cardiac abnormality classication from 12lead electrocardiogram (ECG) recordings of varying sampling frequency and duration. We have tackled this problem by developing a deep neural network architecture [9]. Our architecture acknowledges the importance of the spatial relationship between the ECG channels by using a squeezeandexcitation (SE) block. In this extended analysis, we present a deeper investigation into the strengths and weaknesses of this approach, including the use of expert clinical knowledge to determine why examples may be misclassied. 2. Methods "
282,Noisy Label Detection for Speaker Recognition.txt,"The success of deep neural networks requires both high annotation quality and
massive data. However, the size and the quality of a dataset are usually a
trade-off in practice, as data collection and cleaning are expensive and
time-consuming. Therefore, automatic noisy label detection (NLD) techniques are
critical to real-world applications, especially those using crowdsourcing
datasets. As this is an under-explored topic in automatic speaker verification
(ASV), we present a simple but effective solution to the task. First, we
compare the effectiveness of various commonly used metric learning loss
functions under different noise settings. Then, we propose two ranking-based
NLD methods, inter-class inconsistency and intra-class inconsistency ranking.
They leverage the inconsistent nature of noisy labels and show high detection
precision even under a high level of noise. Our solution gives rise to both
efficient and effective cleaning of large-scale speaker recognition datasets.","The success of Deep Neural Networks (DNNs) heavily relies on largescale datasets with highquality annotations. How ever, in practice, the size and the quality of datasets are usu ally a tradeoff, as collecting reliable datasets is laborintensive and timeconsuming. This implies that datasets usually contain some hardtoestimate inaccurate annotations, often referred to as ‚Äùnoisy labels‚Äù. Noisy labels may overparameterize DNNs and lead to performance degradation due to the memorization effect. These pose bottlenecks in training and employing DNNs in realworld scenarios. Various strategies have been proposed to mitigate the neg ative effects of noisy labels, usually referred to as noisylabel learning (NLL) techniques [1]. Loss correction technique is one of the effective ways [2, 3, 4, 5, 6, 7]. It usually involves mod ifying the loss function to make it robust to the label noise, or correcting the loss values according to the label noise. Another approach is to let neural nets correct the labels during training. The correction signal can be computed from an external neural structure [8] or algorithm [9], or internal structure, i.e. a noise layer[10]. Coteaching is another way of label correction. It usually involves training two networks simultaneously through dataset codivide, label coreÔ¨Ånement, and coguessing, in order to Ô¨Ålter out label noise for each other [11, 12, 13]. The majority of such research studies are from the computer vision domain. For the audio and speech domain, much less re search on noisy labels has been conducted. [14] applied semi supervised techniques to learn from soft pseudo labels, and en sembles the predictions to overcome noisy data. [15] explored the extent of noisy labels‚Äô inÔ¨Çuences on trained xvector embeddings [16]. They showed that mislabeled data can severely damage the performance of speaker veriÔ¨Åcation systems on the NIST SRE 2016 dataset [17], and proposed regularization ap proaches to mitigate the damage. Later, [18] also demonstrated that label noise leads to signiÔ¨Åcant performance degradation for both the xvector frontend and PLDA backend on NIST SRE 2016, and proposed a few strategies to resist label noise. Contrary to previous results, [19] experimented with simulating noise on the V oxCeleb2 dataset [20], and showed that even high levels of label noise had only a slight impact on the ASV task, using either GE2E or CE loss functions. However, the noises that they investigate are all closedset noise [21], and never con sider another type of frequently occurring noise, openset noise. The most similar work to this work is [22], which proposed an iterative Ô¨Åltering method to remove noisy labels from the train ing set iteratively in order to improve the speaker recognition performance on the V oxCeleb. However, it requires manually setting a similarity threshold for the Ô¨Åltering and focuses on mitigating the noise effect. In this paper, instead of directly mitigating the effects of la bel noise, we try to address this problem from the root cause: we focus on the detection of noisy labels1. Detecting and Ô¨Åltering noisy labels beneÔ¨Åts not only the model training, but also the crowdsourcing data platforms, e.g. Amazon Mechanical Turk and Scale AI. The Ô¨Åltered noisy labels can be used for relabeling to build higher quality datasets, or identifying the noisy labels providers, i.e. cheaters. SpeciÔ¨Åcally, we investigate noisy label detection (NLD) under the context of speaker recognition, as this has been an underexplored topic, yet it plays an important role in collecting and correcting speaker identity annotations in largescale speech datasets. We are the Ô¨Årst to comprehensively compared various common metric learning loss functions used in the ASV task under a variety of noise settings, openset noise and closedset noise with different noise levels, on a relatively large scale for the Ô¨Årst time. Second, we propose two ranking based NLD methods based on the inconsistent nature of the noisy labels, and achieve high detection precision even under high level of noise. 2. Method "
283,Deep Learning Approach to Diabetic Retinopathy Detection.txt,"Diabetic retinopathy is one of the most threatening complications of diabetes
that leads to permanent blindness if left untreated. One of the essential
challenges is early detection, which is very important for treatment success.
Unfortunately, the exact identification of the diabetic retinopathy stage is
notoriously tricky and requires expert human interpretation of fundus images.
Simplification of the detection step is crucial and can help millions of
people. Convolutional neural networks (CNN) have been successfully applied in
many adjacent subjects, and for diagnosis of diabetic retinopathy itself.
However, the high cost of big labeled datasets, as well as inconsistency
between different doctors, impede the performance of these methods. In this
paper, we propose an automatic deep-learning-based method for stage detection
of diabetic retinopathy by single photography of the human fundus.
Additionally, we propose the multistage approach to transfer learning, which
makes use of similar datasets with different labeling. The presented method can
be used as a screening method for early detection of diabetic retinopathy with
sensitivity and specificity of 0.99 and is ranked 54 of 2943 competing methods
(quadratic weighted kappa score of 0.925466) on APTOS 2019 Blindness Detection
Dataset (13000 images).","Diabetic retinopathy (DR) is one of the most threat ening complications of diabetes in which damage oc curs to the retina and causes blindness. It damages the blood vessels within the retinal tissue, causing them to leak Ô¨Çuid and distort vision. Along with diseases leading to blindness, such as cataracts and glaucoma, DR is one of the most frequent ailments, according to the US, UK, and Singapore statistics (NCHS, 2019; NCBI, 2018; SNEC, 2019). DR progresses with four stages: Mild nonproliferative retinopathy , the earliest stage, where only microaneurysms can occur; Moderate nonproliferative retinopathy , a stage which can be described by losing the blood ves sels‚Äô ability of blood transportation due to their distortion and swelling with the progress of the a https://orcid.org/0000000226787556 b https://orcid.org/0000000199959454 c https://orcid.org/0000000164994575disease; Severe nonproliferative retinopathy results in de prived blood supply to the retina due to the in creased blockage of more blood vessels, hence signaling the retina for the growing of fresh blood vessels; Proliferative diabetic retinopathy is the advanced stage, where the growth features secreted by the retina activate proliferation of the new blood ves sels, growing along inside covering of retina in some vitreous gel, Ô¨Ålling the eye. Each stage has its characteristics and particular properties, so doctors possibly could not take some of them into account, and thus make an incorrect di agnosis. So this leads to the idea of creation of an automatic solution for DR detection. At least 56% of new cases of this disease could be reduced with proper and timely treatment and mon itoring of the eyes (Rohan T, 1989). However, the initial stage of this ailment has no warning signs, and it becomes a real challenge to detect it on the early start. Moreover, welltrained clinicians sometimesarXiv:2003.02261v1  [cs.LG]  3 Mar 2020could not manually examine and evaluate the stage from diagnostic images of a patient‚Äôs fundus (accord ing to Google‚Äôs research (Krause et al., 2017), see Figure 1). At the same time, doctors will most of ten agree when lesions are apparent. Furthermore, existing ways of diagnosing are quite inefÔ¨Åcient due to their duration time, and the number of ophthalmol ogists included in patient‚Äôs problem solution. Such sources of disagreement cause wrong diagnoses and unstable groundtruth for automatic solutions, which were provided to help in the research stage. Figure 1: Google showed that ophtalmologists‚Äô diagnoses differ for same fundus image. Best viewed in color. Thus, algorithms for DR detection began to ap pear. The Ô¨Årst algorithms were based on different classical algorithms from computer vision and set ting thresholds (Michael D. Abrmoff and Quellec, 2010; Christopher E.Hann, 2009; Nathan Silberman and Subramanian, 2010). Nevertheless, in the past few years, deep learning approaches have proved their superiority over other algorithms in tasks of classiÔ¨Å cation and object detection (Harry Pratt, 2016). In particular, convolutional neural networks (CNN) have been successfully applied in many adjacent subjects and for diagnosis of diabetic retinopathy itself (Shao hua Wan, 2018; Harry Pratt, 2016). In 2019, APTOS (Asia PaciÔ¨Åc Tele Ophthalmology Society) and competition ML platform Kaggle challenged ML and DL researchers to develop a Ô¨Åveclass DR automatic diagnosing solu tion (APTOS 2019 Blindness Detection Dataset). In this paper, we propose the transfer learning approach and an automatic method for detection of the stage of diabetic retinopathy by single photography of the human fundus. This approach is able to learn useful features even from a noisy and small dataset and could be used as a DR stages screening method in automatic solutions. Also, this method was ranked 54 of 2943 different methods on APTOS 2019 Blindness Detection Competition and achieved the quadratic weighted kappa score of 0.92546.2 RELATED WORK "
284,Enhancing Certifiable Robustness via a Deep Model Ensemble.txt,"We propose an algorithm to enhance certified robustness of a deep model
ensemble by optimally weighting each base model. Unlike previous works on using
ensembles to empirically improve robustness, our algorithm is based on
optimizing a guaranteed robustness certificate of neural networks. Our proposed
ensemble framework with certified robustness, RobBoost, formulates the optimal
model selection and weighting task as an optimization problem on a lower bound
of classification margin, which can be efficiently solved using coordinate
descent. Experiments show that our algorithm can form a more robust ensemble
than naively averaging all available models using robustly trained MNIST or
CIFAR base models. Additionally, our ensemble typically has better accuracy on
clean (unperturbed) data. RobBoost allows us to further improve certified
robustness and clean accuracy by creating an ensemble of already certified
models.","The lack of robustness in deep neural networks (DNNs) has motivated recent research on verifying and improv ing the robustness of DNN models (Katz et al., 2017; Dvijotham et al., 2018c; Gehr et al., 2018; Singh et al., 2018; Madry et al., 2018; Raghunathan et al., 2018b; Wong and Kolter, 2018; Zhang et al., 2018). Improv ing the robustness of neural networks, or designing the ‚Äúdefense‚Äù to adversarial examples, is a challenging problem. Athalye et al. (2018); Uesato et al. (2018) showed that many proposed defenses are broken and do not signiÔ¨Åcantly increase robustness under adaptive attacks. So far, stateoftheart defense methods in clude adversarial training (Madry et al., 2018; Sinha et al., 2018) and optimizing a certiÔ¨Åed bound on ro bustness (Wong and Kolter, 2018; Raghunathan et al., This is an extended version of ICLR 2019 Safe Machine Learning Workshop paper, ‚ÄúRobBoost: A provable approach to boost the robustness of deep model ensemble‚Äù. May 6, 2019. New Orleans, LA, USA2018a; Wang et al., 2018a; Mirman et al., 2018), but there is still a long way to go to conquer the adversarial example problem. On MNIST at perturbation = 0:3, adversarially trained models (Madry et al., 2018) are resistant to strong adversarial attacks but cannot be eÔ¨Éciently certiÔ¨Åed using existing neural network ver iÔ¨Åcation techniques. On the other hand, certiÔ¨Åable training methods usually suÔ¨Äer from high clean and veriÔ¨Åed error; in (Wong et al., 2018), the best model achieves 43.1% veriÔ¨Åed error and 14.9% clean error, which is much higher than ordinary MNIST models. Mostoftheseexistingdefensemethodsonlyfocusonim proving the robustness of a single model. Traditionally, a model ensemble has been used to improve prediction accuracy of weak models. For example, voting or boot strap aggregating (bagging) can be used to improve prediction accuracy in many occasions. Furthermore, boosting based algorithms, including AdaBoost (Fre und and Schapire, 1997), LogitBoost (Friedman et al., 2000), gradient boosting (Friedman, 2001, 2002) and many other variants, are designed to minimize an up per bound (surrogate loss) of classiÔ¨Åcation error, which provably increases the model‚Äôs accuracy despite the fact that each base model can be very weak. Inspired by the successful story of model ensembles, our question is that if a similar technique can be used to build a robust model with better provable robustness via an ensemble of certiÔ¨Åable base models? Intuitively, attacking an ensemble seems to be harder than attacking a single model, because an adversary must fool all models simultaneously. Some works on using a model ensemble to defend against adversarial examples (Abbasi and Gagn√©, 2017; Strauss et al., 2017; Liu et al., 2018; Pang et al., 2019; Kariyappa and Qureshi, 2019) show promising results that they can indeed increase the required adversarial distortion for a successful attack and improve robustness. However, none of these works attempt to propose a provable (orcertiÔ¨Åable ) method to improve model robustness via an ensemble, so there is no guarantee that these methods work in all situations. For example, He et al. (2017) reported that attacking a specialist ensemble only increases the required adversarial distortion by as little as 6% compared to a single model. In this paper, we propose a new algorithm, RobBoost , that can provably enhance the robustness certiÔ¨Åcate ofarXiv:1910.14655v1  [stat.ML]  31 Oct 2019Enhancing CertiÔ¨Åable Robustness via a Deep Model Ensemble a deep model ensemble. First, we consider the setting where a set of pretrained robust models are given, and we aim to Ô¨Ånd the optimal weights for each base classi Ô¨Åer that maximize provable robustness. We select the weight for each base model iteratively, to maximize a lower bound on the classiÔ¨Åcation margin of the neu ral network ensemble classiÔ¨Åer. Given a set of Tbase models which are individually certiÔ¨Åable, we formulate a certiÔ¨Åed robustness bound for the model ensemble and show that solving the optimal ensemble leads to an optimization problem. We propose a coordinate de scent based algorithm to iteratively solve the RobBoost objective. Second, we consider training each base classi Ô¨Åer sequentially from scratch, in a setting more similar to traditional gradient boosting, where each model is sequentially trained to improve the overall robustness of the current ensemble. Our experiments show that RobBoost can select a set of good base classiÔ¨Åers and weight them optimally, outperforming a naive average of all base models; on MNIST with `1perturbation of= 0:3, RobBoost reduces veriÔ¨Åed error from 36% (averaging models) to 34%using 12 certiÔ¨Åable base models trained individually. 2 Related Work "
285,Neighborhood Collective Estimation for Noisy Label Identification and Correction.txt,"Learning with noisy labels (LNL) aims at designing strategies to improve
model performance and generalization by mitigating the effects of model
overfitting to noisy labels. The key success of LNL lies in identifying as many
clean samples as possible from massive noisy data, while rectifying the wrongly
assigned noisy labels. Recent advances employ the predicted label distributions
of individual samples to perform noise verification and noisy label correction,
easily giving rise to confirmation bias. To mitigate this issue, we propose
Neighborhood Collective Estimation, in which the predictive reliability of a
candidate sample is re-estimated by contrasting it against its feature-space
nearest neighbors. Specifically, our method is divided into two steps: 1)
Neighborhood Collective Noise Verification to separate all training samples
into a clean or noisy subset, 2) Neighborhood Collective Label Correction to
relabel noisy samples, and then auxiliary techniques are used to assist further
model optimization. Extensive experiments on four commonly used benchmark
datasets, i.e., CIFAR-10, CIFAR-100, Clothing-1M and Webvision-1.0, demonstrate
that our proposed method considerably outperforms state-of-the-art methods.","Deep neural networks (DNNs) have achieved significant success in computer vi sion tasks, such as image classification [41,1,22,5,18,52], etc. However, they rely heavily on tremendous quantities of highquality manual annotations. To allevi ate the need for extensive human annotations while improving the generalization capability of deep neural networks, learning with noisy labels (LNL) has been proposed to effectively leverage largescale yet poorlyannotated datasets while mitigating the effects of model overfitting to noisy labels. *Corresponding Authors are Guanbin Li and Yizhou Yu.arXiv:2208.03207v1  [cs.CV]  5 Aug 20222 J. Li et al. B CDA O E Fig. 1. An illustration to exemplify our basic idea. Samples distributed within the dotted circle, including the candidate sample, Point O, and its nearest neighbors, i.e., Point A, B, C, D and E are close to each other in the featurespace neighborhood. Dif ferent colors indicate different labels (either predicted label or given groundtruth label). In the noise verification stage, a given label of the candidate (Point O) is considered noisy if there is a huge inconsistency between the label distributions of the candidate and its nearest neighbors; and otherwise, the candidate is considered as a clean sam ple. Likewise, in the noise correction stage, a noisy sample discards the given noisy label and is relabeled through a neighborhood collective estimation process involving its contrastive neighbors To tackle the challenges imposed by LNL, previous works have proposed massive strategies [10,39,19,32,47], including noisy label correction [3,24], noisy label or sample rejection[19,47,15,14], and noisy sample reweighing [42,35,12]. The mainstream pipeline first uses noise verification strategies to separate the original training set into a clean set and a noisy set, which contain training samples with clean labels and noisy labels respectively, in order to diminish the effect of noisy labels during model training. Then, (un)supervised learning or semisupervised learning (SSL) based techniques are adopted to correct noisy labels and further optimize the classification model by regarding the clean set and noisy set as labeled and unlabeled samples respectively. In this scheme, original noisy labels are simply discarded for their high chances to be incorrect, avoiding the negative effect of noisy label memorization in the trained model. In the context of learning with noisy labels, there may exist classes with imbalanced noisy or clean samples, especially in realworld noisy datasets such as Clothing1M [45] and Webvision1.0 [23]. For instance, there might be a relatively high proportion of noisy labels in some hardtoannotate classes; on the other hand, a trained model may produce lowconfident predictions on a relatively high proportion of hardtolearn clean samples in some classes, making existing noise identification algorithms incorrectly identify them as noisy samples. As a result, noise accumulation may take place implicitly in such classes, making the trained model produce unreliable label predictions. The above scenarios could make an LNL algorithm fall into the socalled confirmation bias [40,2], which causes the algorithm to favor incorrect training labels that have been confirmed with predicted labels in earlier training iterations. In this context, relying too much on the potentially biased label predictions for individual training samples would increase the risk of incorrectly identifying noisy labels in the noise verification stage. Moreover, confirmation bias also exists in the subsequent noise correction stage, where SSL or other methods, such as labelguessing [19,30,51] and labelNeighborhood Collective Estimation 3 reassignment [47], construct pseudolabels for unlabeled samples in the noisy set using potentially biased label predictions. Apparently, model training in the optimization stage would strengthen this bias as more confident but incorrect predictions would defy new changes, and subsequently even deteriorate model performance in high noise ratio scenarios. We are inspired by the premise of contrastive learning that samples from the same class should have higher similarity in the feature space than those from dif ferent classes [29,31,9]. Therefore, we approach learning with noisy labels from a different perspective and propose Neighborhood Collective Estimation (NCE), in which we reestimate the predictive reliability of a candidate sample by contrast ing it against its featurespace nearest neighboring samples. Herein, we borrow the concept from contrastive learning, and then name such neighboring samples of the candidate as contrastive neighbors. Leveraging contrastive neighbors en riches the predictive information associated with the candidate and also makes such information relatively unbiased, thereby improving the accuracy of noisy label identification and correction. Fig. 1 displays the basic idea of the proposed method. Specifically, to abide by the mainstream LNL pipeline, we divide our method into two steps: 1) Neighborhood Collective Noise Verification (NCNV) to sep arate all training samples into a clean set and a noisy set, 2) Neighborhood Collective Label Correction (NCLC) to relabel noisy samples. In the NCNV stage, a candidate sample is considered noisy when there is a huge inconsistency between the onehot vector of the given label of the candidate and the label dis tributions of its contrastive neighbors predicted using the trained model. In the NCLC stage, we only relabel noisy samples whose predicted label distribution is sufficiently similar to the given labels of neighboring clean samples, and the cor rected label of a noisy sample is related to a weighted combination of the given labels of neighboring clean samples. Once we have identified clean samples and relabeled noisy ones, we leverage offtheshelf and wellestablished techniques, such as mixup regularization [50] and consistency regularization [36], to perform further SSLbased model training. In summary, the main contributions are as follows. ‚ÄìWe propose Neighborhood Collective Estimation for learning with noisy la bels, which leverages contrastive neighbors to obtain richer and relatively unbiased predictive information for candidate samples and thus mitigates confirmation bias. ‚ÄìConcretely, we design two steps called Neighborhood Collective Noise Verifi cation and Neighborhood Collective Label Correction to identify clean sam ples and relabel noisy ones respectively. ‚ÄìWe evaluate our method on four widely used LNL benchmark datasets, i.e., CIFAR10 [16], CIFAR100 [16], Clothing1M [45] and Webvision1.0 [23], and the results demonstrate that our proposed method considerably outper forms stateoftheart LNL methods.4 J. Li et al. 2 Related Work "
286,Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition.txt,"Facial expression recognition from videos in the wild is a challenging task
due to the lack of abundant labelled training data. Large DNN (deep neural
network) architectures and ensemble methods have resulted in better
performance, but soon reach saturation at some point due to data inadequacy. In
this paper, we use a self-training method that utilizes a combination of a
labelled dataset and an unlabelled dataset (Body Language Dataset - BoLD).
Experimental analysis shows that training a noisy student network iteratively
helps in achieving significantly better results. Additionally, our model
isolates different regions of the face and processes them independently using a
multi-level attention mechanism which further boosts the performance. Our
results show that the proposed method achieves state-of-the-art performance on
benchmark datasets CK+ and AFEW 8.0 when compared to other single models.","Automatic facial expression recognition from images/videos has many applica tions such as humancomputer interaction (HCI), bodily expressed emotions, hu man behaviour understanding, and has thus gained a lot of attention in academia and industry. Although there has been extensive research on this subject, facial expression recognition in the wild remains a challenging problem because of sev eral factors such as occlusion, illumination, motion blur, subjectspecic facial variations, along with the lack of extensive labelled training datasets. Following a similar line of research, our task aims to classify a given video in the wild to one of the seven broad categorical emotions. We propose an ecient model that addresses the challenges posed by videos in the wild while tackling the issue of labelled data inadequacy. The input data used for facial expression recognition can be multimodal, i.e. it may have visual information as well as audio informa tion. However, the scope of this paper is limited to emotion classication using only visual information. ?equal contributionarXiv:2008.02655v2  [cs.CV]  24 Feb 20212 V. Kumar et al. Most of the recent research on the publiclyavailable AFEW 8.0 (Acted Facial Expressions in the Wild) [1] dataset has focused on improving accuracy without regard to computational complexity, architectural complexity, energy & policy considerations, generality, and training eciency. Several stateoftheart meth ods [2,3,4] on this dataset have originated from the EmotiW [5] challenge with no clear computationalcost analysis. Fan et al. [2] achieved the highest validation accuracy based on visual cues, but they used a fusion of ve dierent architec tures with more than 300 million parameters. In contrast, our proposed method uses a single model with approximately 25 million parameters and comparable performance. While previous work focused on improving performance by increasing model capacity, our method focuses on better preprocessing, feature selection, and adequate training. Prior research [6,7,8,9] uses simple aggregation or averaging operation on features from multiple frames to form a xeddimensional feature vector. However, such methods do not account for the fact that a few principal frames in a video can be used to identify the target emotion, while the rest of the frames have a negligible contribution. Frameattention has been used [10] for selectively processing frames in a video, but it can further be coupled with spatialattention which could identify the most discriminative regions in a particular frame. We use a threelevel attention mechanism in our model: a) spatialattention block that helps to selectively process feature maps of a frame, b) channelattention block that focuses on the face regions at a local and a global level, i.e. eyes region (upper face), mouth region (lower face) and whole face, and c) frameattention block that helps to identify the most important frames in a video. AFEW 8.0 [1] has several limitations (Sec. 2) that restricts the generalization capabilities of deep learning models. To overcome these limitations, we use an unlabelled subset of the BoLD dataset [11] for semisupervised learning. Inspired by Xie et al. [12], we use a teacherstudent learning method where the training process is iterated by using the same student again as the teacher. During the training of the student, noise is injected into the student model to force it to generalize better than the teacher. Results show that the student performs better with each iteration, hence improving the overall accuracy on the validation set. The rest of the paper is organized as follows. Sec. 3 explains the datasets (AFEW 8.0 [1], CK+ [13] and BoLD [11]) that are used for training our model along with the preprocessing pipeline used for face detection, alignment and illumination correction. Sec. 4.1 explains the backbone network and covers the three types of attention and its importance in detail. Sec. 4.2 covers the use of the BoLD dataset for iterative training and the experimental results of semi supervised learning. Sec. 5.3 compares the results of our methods to other state oftheart methods on the AFEW 8.0 dataset. Additionally, we use another benchmark dataset CK+ [13] (posed conditions) as well as perform ablation studies (Sec. 5.4) to prove the validity of our model and training procedure.Noisy Student Training Improves Facial Expression Recognition 3 2 Related Work "
287,Active Learning for Noisy Data Streams Using Weak and Strong Labelers.txt,"Labeling data correctly is an expensive and challenging task in machine
learning, especially for on-line data streams. Deep learning models especially
require a large number of clean labeled data that is very difficult to acquire
in real-world problems. Choosing useful data samples to label while minimizing
the cost of labeling is crucial to maintain efficiency in the training process.
When confronted with multiple labelers with different expertise and respective
labeling costs, deciding which labeler to choose is nontrivial. In this paper,
we consider a novel weak and strong labeler problem inspired by humans natural
ability for labeling, in the presence of data streams with noisy labels and
constrained by a limited budget. We propose an on-line active learning
algorithm that consists of four steps: filtering, adding diversity, informative
sample selection, and labeler selection. We aim to filter out the suspicious
noisy samples and spend the budget on the diverse informative data using strong
and weak labelers in a cost-effective manner. We derive a decision function
that measures the information gain by combining the informativeness of
individual samples and model confidence. We evaluate our proposed algorithm on
the well-known image classification datasets CIFAR10 and CIFAR100 with up to
60% noise. Experiments show that by intelligently deciding which labeler to
query, our algorithm maintains the same accuracy compared to the case of having
only one of the labelers available while spending less of the budget.","Obtaining a labeled dataset for training machine learning models is a time consuming and expensive task. The common practise of curating data continuously collects both data and the metadata that serves as labels Hendrycks et al. [2018] from the public domain. This immediately reudcesthe cost of acquiring a high volume of labeled data for deep learning models but introduces the challenge of handling data streams with noisy labels Han et al. [2018], Wang et al. [2018], i.e., data that is annotated with wrong class labels. Human experts Sheng et al. [2008] are sought after to correct labels to enhance the robustness of learning models against label noise. Cognitive science Yan et al. [2014] has shown that humans are better in answering binary questions, such as True/False questions, and are less skilled in multiplechoice questions, e.g., identifying one out of 100 classes in CIFAR100 benchmark Krizhevsky et al. [2009]. It is more expensive to use strong labelers who are skilled in directly pointing out true class labels than weak labelers who can only (dis)agree with the provided labels. To cost effectively correct the noisy labels by experts Sheng et al. [2008], Chang et al. [2017], it is imperative to assign the correct tasks to the labelers according to their skills and the Preprint. Under review.arXiv:2010.14149v1  [cs.LG]  27 Oct 2020difÔ¨Åculty levels of querying tasks. This becomes particularly challenging in streaming data scenarios due to future uncertainty, e.g., how to allocate the correction effort across the learning horizon. Online learning from streaming data corresponds to today‚Äôs data common practise to train machine learning models with a small set of data that is periodically curated from the public domains. Machine learning models thus need to be learned online from the stream data. Moreover, due to privacy or regulation constraints gdp, or storage limits, data turns are available for the limited duration. Moreover, in such scenarios, combating label noise adds to the challenge. The prior art tries to enhance the robustness of the deep model training against noisy labels in the offline scenario by (i) Ô¨Åltering out noisy data through model disagreement Han et al. [2018], Yu et al. [2019], (ii) correcting noisy labels through the estimated noisy corruption matrix Patrini et al. [2017], or (iii) modifying the loss functions Ma et al. [2018], Wang et al. [2019]. Among the related studies, high quality labels from human experts are used to correct labels through the estimate noise corruption matrix, e.g., the trust data set in Distillation Li et al. [2017] and GLC Hendrycks et al. [2018]. The trust data sheds light on how a small percentage of high quality labels can prevent deep models from accuracy degradation due to label noise. However, the focus on the offline datasets renders existing approaches insufÔ¨Åcient for handling noisy data streams only a subset of which can be learnt at a time. Indeed, online learning from noisy labels can lead to a much more severe degradation of accuracy than the typical offline case. Moreover, such a subset of trust data is randomly selected and the cost of acquiring additional data label yet to be modeled. In this paper we address the challenge of training a deep classiÔ¨Åer from noisy labeled data streams that are collected over time and that can only be learnt for a limited time. We propose an active learning framework, DuoLab, which aims to learn a robust classiÔ¨Åer by costeffectively assigning the labelcorrection task to either a strong or a weak labeler within a labeling budget. Extensively querying the strong labeler can easily lead to budget exhaustion, whereas the weak labeler might require multiple queries to achieve the cleansing goal. DuoLab consist of four steps: Ô¨Åltering out the suspicious data, choosing diverse samples, informativeness ranking, and selecting labelers. While training the classiÔ¨Åer, DuoLab leverages the output of its classiÔ¨Åer to Ô¨Ålter suspicious data samples. As for label cleansing, we propose a labeler selection function Q that combines the overall model conÔ¨Ådence and the informativeness of individual samples. Our contributions are the following. First we design a cost and skill aware active learning framework for noisy data streams. Secondly, by leveraging the diversity of the labelers we are able to greatly enhance the robustness of deep models even in challenging online learning scenarios. The proposed Q function can effectively assess the model conÔ¨Ådence and informativeness of data samples and assign the suitable labelers accordingly. 2 Related Work "
288,In Defense of the Triplet Loss Again: Learning Robust Person Re-Identification with Fast Approximated Triplet Loss and Label Distillation.txt,"The comparative losses (typically, triplet loss) are appealing choices for
learning person re-identification (ReID) features. However, the triplet loss is
computationally much more expensive than the (practically more popular)
classification loss, limiting their wider usage in massive datasets. Moreover,
the abundance of label noise and outliers in ReID datasets may also put the
margin-based loss in jeopardy. This work addresses the above two shortcomings
of triplet loss, extending its effectiveness to large-scale ReID datasets with
potentially noisy labels. We propose a fast-approximated triplet (FAT) loss,
which provably converts the point-wise triplet loss into its upper bound form,
consisting of a point-to-set loss term plus cluster compactness regularization.
It preserves the effectiveness of triplet loss, while leading to linear
complexity to the training set size. A label distillation strategy is further
designed to learn refined soft-labels in place of the potentially noisy labels,
from only an identified subset of confident examples, through teacher-student
networks. We conduct extensive experiments on three most popular ReID
benchmarks (Market-1501, DukeMTMC-reID, and MSMT17), and demonstrate that FAT
loss with distilled labels lead to ReID features with remarkable accuracy,
efficiency, robustness, and direct transferability to unseen datasets.","Person reidentiÔ¨Åcation (ReID) has attracted tremendous attention owing to its vast applications in video surveillance, public safety, and so on. Given a person image spotted by one camera, ReID aims to accurately match that probe im age against a large amount of gallery images, taken by other cameras and timestamps. The dramatic visual appearance variations of the same person, as caused by different poses, view angles, illuminations, and backgrounds, constitute se rious challenges for learning robust identity representations. Most existing ReID algorithms use a classiÔ¨Åcation loss to train their feature learning backbones [48, 41, 42, 19, 3, 45]. (a) Triplet Loss (b) FAT Loss  Figure 1: Illustrative comparison of standard triplet loss and FAT loss. The former compares pointtopoint distances, while the lat ter compares pointtoset distances while regularizing all cluster sets to be compact. The solid arrows depict the ‚Äúpush and pull‚Äù ef fect of triplet loss and the pointtoset term of FAT loss. The dash arrows represents the compactness regularization of FAT loss. See details in Section 3. However, ReID is essentially an ‚Äúopenended‚Äù retrieval problem rather than closedset classiÔ¨Åcation, e.g., the train ing and testing sets usually have no overlapped identity classes. The learned feature extractor should be able to gen eralize to matching unseen identities. The testing perfor mance is evaluated by the precision and recall of the match ing instances, rather than classiÔ¨Åcation accuracy. Therefore, the classiÔ¨Åcationdriven learning could be misaligned with the end goal. Instead, the comparative losses [31, 7, 25, 49], which compares the distances between two sample pairs, are naturally better choices, as empirically validated by a handful of works [21, 19, 4, 40, 5]. Among many, the triplet loss [13], which maximizes the margin between the intra class distance and the interclass distance, has been mostly used in ReID, in order to explicitly embed the relative ordersarXiv:1912.07863v2  [cs.CV]  19 Dec 2019between right and wrong matches ( i.e., the correct matches should always be closer to the query than the wrong ones). However, an important downside of triplet loss lies in itscomputational expensiveness , which prohibits its wide usage in the largescale ReID applications. A naive triplet loss that compares every possible pair of training samples will incur cubic complexity w.r.t. the training set size [13]. Also, triplet loss relatively quickly learns to correctly map most trivial triplets, rendering a large fraction of all triplets uninformative. Applying triplet loss with randomly selected triplets can accelerate training but quickly stagnates, or be comes difÔ¨Åcult to converge. Hard sample mining [43, 46] has recently become the standard practice in using triplet loss, to select only ‚Äúinformative‚Äù (a.k.a. hard) pairs rather than all pairs to enforce the loss. However, it runs the risk of causing sample bias [43], and often appears fragile to outliers. The vanilla triplet loss needs to calculate over allPK(K 1)(PK K)possible triplets, where Kde notes average number of images per identity and Pidenti ties in total [13]. The time complexity can be reduced to PK(PK 1) +PK when hard sample mining is used. In this paper, we will propose a new fastapproximated triplet ( FAT) loss to trim down the computational cost of triplet loss without hampering its effectiveness. Viewing all images belonging to the same identity class as a cluster, the proposed FAT loss redeÔ¨Ånes a triplet to include an anchor, its corresponding cluster centroid, and the centroid of an other cluster. The main idea of FAT loss is to replace point topoint distances with pointtocluster distances, through an upper bound relaxation of the triplet form. Such a re laxation simultaneously requires the query to be closest to its groundtruthcluster centroid, and enforces each cluster to have a compact radius. The FAT loss thus has a linear complexity w.r.t. the training set size. Another downside of triplet loss, as well as many other marginbased losses, lies in their fragility to label noise . Unfortunately, ReID datasets are notorious to have many noisy labels and outliers, such as label Ô¨Çipping, mislabel, and multiperson coexistence, due to the tedious manual annotation process. The proposed FAT loss can alleviate the label noise to some extent, by averaging all samples within the same cluster. To provide further improved ro bustness, we consider a distillation network to Ô¨Årst generate soft pseudo labels for each sample, associated with its conÔ¨Å dence. Then we use those soft labels in place of the original labels to feed into the FAT loss, where each individual sam ples contribution to the model update will be reweighted by their label conÔ¨Ådence. In sum, we strive to make triplet loss a more effective, efÔ¨Åcient, and robust choice for ReID, via multifold efforts: We propose a fastapproximated triplet (FAT) loss to remarkably improve the efÔ¨Åciency over the standard triplet loss, with linear complexity to the training setsize. It is derived by relaxing triplet loss to its upper bound form, and operates without hard sample mining. We are the Ô¨Årst to demonstrate that explicitly consid ering and handling label noise can further boost ReID performance. A distillation network is presented to as sign soft labels for samples in place of the original (po tentially noisy) hard labels. Combined with FAT loss, a more robust reID feature can be learned. We conduct extensive experiments on three most pop ular ReID benchmarks, and demonstrate that FAT loss with learned soft labels lead to comparable or supe rior ReID performance than using triplet loss and other stateoftheart baselines, with remarkably higher ef Ô¨Åciency than triplet loss. We also observe improved robustness and direct transferability to unseen data. 2. Related Work "
289,Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation.txt,"Unsupervised domain adaptation (UDA) requires source domain samples with
clean ground truth labels during training. Accurately labeling a large number
of source domain samples is time-consuming and laborious. An alternative is to
utilize samples with noisy labels for training. However, training with noisy
labels can greatly reduce the performance of UDA. In this paper, we address the
problem that learning UDA models only with access to noisy labels and propose a
novel method called robust local preserving and global aligning network
(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One
is learning a classifier by a robust informative-theoretic-based loss function.
The other is constructing two adjacency weight matrices and two negative weight
matrices by the proposed local preserving module to preserve the local topology
structures of input data. We conduct theoretical analysis on the robustness of
the proposed RLPGA and prove that the robust informative-theoretic-based loss
and the local preserving module are beneficial to reduce the empirical risk of
the target domain. A series of empirical studies show the effectiveness of our
proposed RLPGA.","UNSUPERVISED domain adaptation emphasizes [1], [2], [3] the problem of learning a classiÔ¨Åer that can be transferred across two domains. In general, the samples in the source domain are labeled, while the samples in the target domain are unlabeled. The main challenge in this research area is to reduce the difference between the probability distributions of two domains [4], [5], [6], [7]. To this end, the strategy based on discrepancy minimization has attracted much attention. Among them, adversarial learning methods have achieved remarkable performance improvements [8]. The training set of unsupervised adversarial domain adaptation models consists of two parts including the la beled source domain samples and unlabeled target domain samples. However, it is usually very expensive and tedious to accurately label large source domain training samples. An alternative way is to collect labels of samples from some crowdsourcing platforms in which the cost is cheaper and aW. Qiang and J. Li are with the University of Chinese Academy of Sciences, Beijing, China. They are also with the Science & Technology on Integrated Information System Laboratory, Institute of Software Chi nese Academy of Sciences, Beijing, China. Email: a01114115@163.com, Jiangmeng2019@iscas.ac.cn aC. Zheng is with the Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China. Email: changwen@iscas.ac.cn aB. Su is with the Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of ArtiÔ¨Åcial Intelligence, Renmin Uni versity of China, Beijing, 100872, China. Email: subingats@gmail.com aH. Xiong is with the Hong Kong University of Science and Technology (Guangzhou). Email: xionghui@ust.hk. a¬òThey have contributed equally to this work (Corresponding author: Bing Su). a¬©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promo tional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.easier but the obtained labels are always contaminated by noise. As a result, the performance of adversarial domain adaptation models learning from noisy labels will be de creased. One reason is that adversarial domain adaptation models usually learn the classiÔ¨Åer by minimizing the cross entropy loss. The crossentropy loss can be regarded as the distance between the outputs of the classiÔ¨Åer and the labels, so it is sensitive to label noises. That is to say, when a careless annotator tends to label positive class to negative class, then the distancebased loss would force adversarial domain adaptation to learn a classiÔ¨Åer who is more likely to output negative class than to output true label. However, to make the domain adaptive models robust to label noises, it is not enough to only employ robust classiÔ¨Åcation loss to train the models. The main reason is that robust loss can only reduce the impact of noisy labels, but can not completely eliminate it. Actually, for the source domain, learning with noisy labels can reduce the feature discriminability of samples in the latent space. This can lead to that a sample belonging to the one class is easy to be misclassiÔ¨Åed into another class. From a geometrically intuitive point of view, if a sample belonging to class i is incorrectly labeled as class j, then the gradient back propagation operation will force the sample to go from a place surrounded by many samples of the same type to a place surrounded by many different class samples. There fore, besides learning based on a robust classiÔ¨Åcation loss, designing an unsupervised method to maintain the local structure of the data distribution is also very important. To tackle these issues, we propose a novel method for adversarial domain adaptation named Robust Local Preserv ing and Global Aligning Network (RLPGA). RLPGA consists of three parts including a robust loss function for learning a classiÔ¨Åer, a local preserving module, and a global aligning module. Firstly, RLPGA projects samples of both domains into a latent space. Then, a robust informativetheoreticarXiv:2203.04156v1  [cs.CV]  8 Mar 2022SUBMITTED TO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 2 based loss function is minimized to learn the classiÔ¨Åer. The global aligning module minimizes the Wasserstein distance between source domain distribution and target domain dis tribution. The local preserving module constructs two ad jacency weight matrices and two negative weight matrices to encode the local topological relationship among samples and propose an objective function based on the graphs for local preserving. The major contributions of this paper are threefold: aA robust informativetheoreticbased loss function is proposed to measure the performance of the classiÔ¨Åer for adversarial domain adaptation. aTo reduce the effect of label noises from the per spective of the learned feature representation, we propose a new objective function for preserving lo cal neighbor topology based on constructing two adjacency weight matrices and two negative weight matrices. We jointly minimize the Wasserstein dis tance between two domain distributions and the new objective. In this way, the margins between different classes are enlarged and hence the learned features are more discriminative and robust. aWe provide theoretical analysis on the robustness of RLPGA, and prove that the robust loss and the enhanced feature discriminability are beneÔ¨Åcial to reduce the empirical risk in the target domain. 2 R ELATED WORKS "
290,Exploring Uncertainty in Deep Learning for Construction of Prediction Intervals.txt,"Deep learning has achieved impressive performance on many tasks in recent
years. However, it has been found that it is still not enough for deep neural
networks to provide only point estimates. For high-risk tasks, we need to
assess the reliability of the model predictions. This requires us to quantify
the uncertainty of model prediction and construct prediction intervals. In this
paper, We explore the uncertainty in deep learning to construct the prediction
intervals. In general, We comprehensively consider two categories of
uncertainties: aleatory uncertainty and epistemic uncertainty. We design a
special loss function, which enables us to learn uncertainty without
uncertainty label. We only need to supervise the learning of regression task.
We learn the aleatory uncertainty implicitly from the loss function. And that
epistemic uncertainty is accounted for in ensembled form. Our method correlates
the construction of prediction intervals with the uncertainty estimation.
Impressive results on some publicly available datasets show that the
performance of our method is competitive with other state-of-the-art methods.","WITH the rapid development of artiÔ¨Åcial intelligence, deep learning has attracted the interest of many researchers [1]. As a representative tool of deep learning, deep neural networks has achieved impressive performance in many tasks. At present, deep neural networks have important applications in computer vision [2], speech recog nition [3], natural language processing [4], bioinformatics [5] and other Ô¨Åelds. Although deep neural networks have achieved high accuracy for many tasks, they still perform poorly in quantifying the uncertainty of predictions and tend to be overconÔ¨Ådent. For many realworld applications, it is not enough for a model to be accurate in its pre dictions. It must also be able to quantify the uncertainty of each prediction. This is especially importance in tasks where wrong prediction has a great negative impact, such as: machine learning auxiliary medical diagnosis [6], au tomatic driving [7], Ô¨Åance and energy system, etc. These highrisk applications require not only point prediction, but also the precise quantiÔ¨Åcation of the uncertainty. So it can be said that in many application scenarios, uncertainty is very important, more important than precision. Moreover, uncertainty plays an important role in determining when to abandon the prediction of the model. Abandoning the inaccurate predictions of the model can handle exceptions and hand over highrisk decisions to humans [8]. As more and more autonomous systems based on deep learning are deployed in our real life that may cause physical or economic harm, we need to better understand when we can have conÔ¨Ådence in the prediction of deep neural networks and when we should not be so sure. The development of neural networks precise prediction intervals (PIs) [9] is a challenging work to explore the Y. Lai, Y. Shi and Y. Han are with College of Intelligence and Com puting, Tianjin University, Tianjin, China (email: fyuandulai, yucheng, yahong g@tju.edu.cn). Y. Shao, M. Qi and B. Li are with Huawei Noah‚Äôs Ark Lab, Huawei Technologies (email: fshaoyunfeng, qimeiyu, libingshuai g@huawei.com).uncertainty of prediction, which has just aroused the interest of researchers. PIs directly conveys uncertainty, providing a lower bound and an upper bound for prediction [10], but traditional deep neural networks can only provide a point estimation. PIs is of great use in practical applica tions, and can help people make better decisions. However, although there are some researches on uncertainty estima tion [11] [12] [13] [14] [15], there are few researches on PIs. In this paper, We will explore the uncertainty in deep learning to construct the prediction intervals. Uncertainty comes from different sources in vari ous forms, and most literature considers two sources of uncertainty: aleatory uncertainty and epistemic uncer tainty [16] [17] [18] [19] [20] [21]. Aleatory uncertainty describes the irreducible inherent noise of the observed data. This is due to the complexity of the data itself, which can be sensor noise, label noise, class overlap, etc. Aleatory uncertainty is also caused by hidden variables that are not observed or errors in the measurement process, and this type of uncertainty cannot be reduced by collecting more data. Aleatory uncertainty is also known in some literature as data uncertainty, random uncertainty [22], stochastic un certainty [23]. Epistemic uncertainty describes the model er rors due to lack of experience in some regions of the feature space [24]. In some feature spaces, if the training samples do not cover them, the uncertainty will increase in those places. Therefore, the epistemic uncertainty is inversely pro portional to the density of the training samples, which can be reduced by collecting more data in the lowdensity areas. In regions with highdensity training samples, epistemic uncertainty will decrease, and aleatory uncertainty caused by data noise will play a major role. Epistemic uncertainty is also known as model uncertainty [19], because it is caused by the limitations of the model. By analyzing the sources and categories of uncertainty, we can deal with and estimate each kind of uncertainty well in practice.arXiv:2104.12953v1  [cs.LG]  27 Apr 2021MANUSCRIPT SUBMITTED TO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 2 The precise prediction intervals of neural network is a difÔ¨Åcult task to solve, and its development is closely related to uncertainty estimation. The prediction interval directly reÔ¨Çects the uncertainty. Many literatures have proposed that the loss function can be modiÔ¨Åed to implicit learning prediction intervals [9] [10] [12] [25] [26]. These methods have obvious disadvantages, such as loss function can not be optimized by gradient descentbased algorithm, which is a modern machine learning technology. Or the accuracy of PIs is low. Although there are many conÔ¨Ådence interval construction methods with good performance in statistical literature, these methods can not be effectively combined with deep learning. In this paper, we propose a prediction interval construc tion method from the perspective of statistics which can be used effectively only by slight modiÔ¨Åcation of deep neural networks. We modify the last layer of the general deep neural network so that the network no longer outputs only a point estimation, but the upper bound and lower bound of the interval. And a loss function that can be used to train such a network is derived from the perspective of proba bility theory. Because most modern deep neural networks can use our loss function with only a few modiÔ¨Åcations, our method will have more practical use than other methods. The contributions of this work are summarized as fol lows: i) In this paper, the construction of optimal pre diction intervals is associated with uncertainty estimation. We design a novel loss function based on the theory of statistics. We think our work bridges the gap between the construction of prediction intervals and the uncertainty esti mation; ii) Unlike bayesian deep learning, our loss function can be seamlessly integrated with current deep learning frameworks such as TensorFlow andPyTorch . Existing deep learning networks can use our loss function with only minor modiÔ¨Åcations. And it can be optimized efÔ¨Åciently using standard optimization techniques. We believe that our method promotes the development of nonbayesian method to construct prediction interval. iii) Experimental results show that this method can construct compact opti mal prediction intervals. It can sensitively capture the epis temic uncertainty. In the lowdensity data distribution area, the uncertainty increases obviously. Experimental results on some publicly available regression datasets show the effectiveness of the method. The rest of the paper is organized as follows. In the section 2, we summarize the related work of uncertainty estimation and prediction interval. And we introduce the method of constructing prediction interval based on explor ing uncertainty in section 3. In the section 4, we organize some convincing experiments to show the effectiveness of our method. Finally, in the section 5, we make a brief summary of this paper. 2 RELATED WORK "
291,FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity.txt,"Federated noisy label learning (FNLL) is emerging as a promising tool for
privacy-preserving multi-source decentralized learning. Existing research,
relying on the assumption of class-balanced global data, might be incapable to
model complicated label noise, especially in medical scenarios. In this paper,
we first formulate a new and more realistic federated label noise problem where
global data is class-imbalanced and label noise is heterogeneous, and then
propose a two-stage framework named FedNoRo for noise-robust federated
learning. Specifically, in the first stage of FedNoRo, per-class loss
indicators followed by Gaussian Mixture Model are deployed for noisy client
identification. In the second stage, knowledge distillation and a
distance-aware aggregation function are jointly adopted for noise-robust
federated model updating. Experimental results on the widely-used ICH and
ISIC2019 datasets demonstrate the superiority of FedNoRo against the
state-of-the-art FNLL methods for addressing class imbalance and label noise
heterogeneity in real-world FL scenarios.","Federated Learning (FL), allowing individual clients to train a deep learning model collaboratively without data sharing, has been widely studied in privacyconscious occasions, e.g. medical [Kaissis et al. , 2020 ]and Ô¨Ånancial [Zheng et al. , 2021 ]applications. Most existing federated learning frame works are based on the paradigm of fully supervised learning [McMahan et al. , 2017; Li et al. , 2020b ], which implicitly as sumes that each participant‚Äôs data is labeled correctly. How ever, building a completely clean dataset with highquality annotation is costly in realistic medical scenarios, as label ing medical data is timeconsuming and laborintensive re quiring expertise. Consequently, it would unavoidably intro duce noisy labels when hiring nonprofessionals to label or using automatic labeling techniques [Irvin and others, 2019 ]. Corresponding author 1Code is available at https://github.com/wnn2000/FedNoRo.Therefore, it is more common that the labels of some clients are clean while others are not in realworld FL scenarios. Due to the existence of noisy clients, developing a noise robust FL framework is of great importance, where accurately identifying the noisy clients is the Ô¨Årst and the most cru cial step. Existing methods for noisy client detection pro pose to calculate an average indicator ( e.g. loss) over all samples of each client as its feature and Ô¨Ålter out the clients with abnormal features as noisy clients, which assumes clean clients‚Äô features are independent and identically distributed (IID) while noisy clients‚Äô are outliers following the small loss trick [Han et al. , 2018 ]. SpeciÔ¨Åcally, Xu et al. [2022 ] calculated the average LID value [Houle, 2013 ]of each client and identiÔ¨Åed the clients with larger average LID values as noisy clients. Similarly, Wang et al. [2022 ]replaced the LID value with the conÔ¨Ådence score and identiÔ¨Åed noisy clients with smaller conÔ¨Ådence scores. Unfortunately, these indica tors can be less effective to deal with noisy clients in real world FL scenarios, due to the following observations: 1. Data is highly classimbalanced from the global perspec tive [Wang et al. , 2021; Shang et al. , 2022 ]. Under class imbalance, using a global indicator for all classes is highly sensitive to clients‚Äô label distributions which may vary dramatically. 2. Data is heterogeneous across clients [Liet al. , 2020b ]. As each client collects its own data independently under FL, there may exist severe data variations, affecting the calculation of indicators across clients. 3. Label noise is heterogeneous across clients where the heterogeneity of noise varies in both strength and pat tern. The former represents different noise rates across clients, and the latter indicates various forms of label noise related to clients‚Äô local data distributions. Suffering from class imbalance, the federated model in FL can bias to the global majority classes [Kang et al. , 2020; Zhou et al. , 2020; Cao et al. , 2019; Menon et al. , 2021 ], re sulting in large indicator variations across classes and making noisy clients‚Äô indicators less distinguishable. For instance, in clinical scenarios, given one cancerspecialized hospital A and one general hospital B,Awould enroll more malignant cases and Bwill enroll more healthy cases. Therefore, Ais more likely to produce an abnormal clientwise feature ( e.g.,arXiv:2305.05230v1  [cs.LG]  9 May 2023large loss values similar to noisy clients) due to class imbal ance ( i.e., healthymalignant). In addition to data hetero geneity, the clientwise feature can also be affected by het erogeneous label noise, making noisy client detection harder. For instance, in clinical scenarios, given two hospitals Cand Dwith different patient diversity ( i.e., various label distribu tions), one benign case is more likely to be wrongly diag nosed as healthy by Cand malignant by D. Though both la bels are wrong, the loss values ( i.e., indicators for noisy client detection) produced by Cwould be much smaller than D, due to class imbalance ( i.e., healthymalignant). In summary, class imbalance would divert the clientwise feature from be ing only related to whether a client is noisy to being related to both its local data distribution and label noise pattern, mak ing the above identiÔ¨Åcation methods based on global indicator struggle in realistic FL scenarios. To address this, one straightforward way is to combine classbalancing [Kang et al. , 2020; Zhou et al. , 2020; Cao et al. , 2019; Menon et al. , 2021 ]with noisy label learning. However, those methods can only mitigate the bias in class prior probability while the bias in learning difÔ¨Åculty of class speciÔ¨Åc features [Yiet al. , 2022 ]is unsolved. Due to rela tively limited training data, the classspeciÔ¨Åc features of mi nority classes would be more difÔ¨Åcult to learn compared to those of the majority classes, resulting in imbalance in the feature space. When adopting feature learning [Karthik et al., 2021; Yi et al. , 2022 ]to alleviate the bias in learning difÔ¨Åculty, it can be constrained by each client‚Äôs limited lo cal data and computing resources. Till now, how to address class imbalance in federated noisy label learning (FNLL) is underexplored. In this paper, we Ô¨Årst formulate a new FNLL problem to model more realistic FL scenarios under class imbalance, and then propose a twostage framework FedNoRo for noise robust learning. SpeciÔ¨Åcally, in the Ô¨Årst stage, instead of us ing a global indicator, we propose to identify noisy clients according to the clientwise perclass average loss values cal culated by a warmup model trained by FedAvg [McMahan et al. , 2017 ]. As each class is considered independently, this detection method will be less affected by class imbalance and heterogeneity, leading to better detection performance. In the second stage, different learning strategies are employed for clean and noisy clients respectively, where crossentropy loss is adopted for training on clean clients and knowledge dis tillation (KD) [Hinton et al. , 2015 ]is employed to minimize the negative inÔ¨Çuence of noisy labels on noisy clients. In ad dition to clientlevel training, a distanceaware aggregation function is proposed to better balance the importance of clean and noisy clients for global model updating in the server. In the local training phase of both the two stages, logit adjust ment (LA) [Menon et al. , 2021 ]is imposed to Ô¨Åght against data heterogeneity and class imbalance. The main contribu tions are summarized as follows: ‚Ä¢ A new FNLL problem where both classimbalanced global data and heterogeneous label noise are considered to model real FL scenarios. ‚Ä¢ A new label noise generation approach for multisource data, where the synthetic label noise is heterogeneousand instancedependent. ‚Ä¢ A twostage FL framework, named FedNoRo, to address both class imbalance and label noise heterogeneity. In FedNoRo, noisy clients are identiÔ¨Åed based on abnormal perclass loss values, and noiserobust training strategies are used for effective federated model updating. ‚Ä¢ Superior performance against the stateoftheart FNLL approaches on realworld multisource medical datasets. 2 Related Work "
292,Noise-robust classification with hypergraph neural network.txt,"This paper presents a novel version of the hypergraph neural network method.
This method is utilized to solve the noisy label learning problem. First, we
apply the PCA dimensional reduction technique to the feature matrices of the
image datasets in order to reduce the ""noise"" and the redundant features in the
feature matrices of the image datasets and to reduce the runtime constructing
the hypergraph of the hypergraph neural network method. Then, the classic
graph-based semi-supervised learning method, the classic hypergraph based
semi-supervised learning method, the graph neural network, the hypergraph
neural network, and our proposed hypergraph neural network are employed to
solve the noisy label learning problem. The accuracies of these five methods
are evaluated and compared. Experimental results show that the hypergraph
neural network methods achieve the best performance when the noise level
increases. Moreover, the hypergraph neural network methods are at least as good
as the graph neural network.","During the last decade, the deep convolution neural network can be considered the current state of  the art method for various classification tasks such as image recognition [1], speech recognition [2], to name  a few. Recently, to deal with irregular data structures, data scientists have gained many interests in graph  convolution neural network method such as [3]. In this method, the pairwise relationships between objects  (samples) are used. In the other words, in this graph data structure, the edge of the graph can connect only  two vertices.   To overcome the information loss due to only considering the ‚Äúpairwise relationship between  objects ‚Äù of graph data structure [4, 5] have recently proposed the hypergraph neural network approach. In  this hypergraph data structure, an edge (hyperedge) can connect more than two vertices. In the other words,  the hy peredge is the subset of the set of vertices of the hypergraph. Recently, this hypergraph neural  network method has just been employed to solve classification tasks [4, 5] and outperforms the graph neural  network and can be considered the current state of the art method of semisupervised learning approach.  However, this method has also not been utilized to solve the noisy label learning problem.   Inspired from the idea combining the pagerank algorithm with the graph convolution neural network  in [6], in this paper, we propose the novel version of hypergraph neural network method combining the  classic hypergraph based semisupervised learning method [7, 8] with the hypergraph neural network  method [4, 5]. In the other words, we combine the propagation scheme utilizing the hypergraph model with  the hypergraph neural network which is the current state of the art method of semisupervised learnin g        ÔÅ≤  ISSN: 25024752  Indonesian J Elec Eng & Comp Sci, Vol. 21, No. 3, March 2021 :  1465  1 473 1466   approach. We find out that this proposed combination of the propagation scheme and the hypergraph neural  network method significantly improves the accuracy of the hypergraph neural network method alone even  when the noise presents in the labels.   In this paper, our contributions are threefolds:   a) In order to reduce the runtime constructing the graphs and the hypergraphs from the image datasets, we apply the dimensional reduction technique PCA to the image datasets. b) Propose the novel version of hypergraph neural network method combining the classic hypergraph based semisupervised learning method with the hypergraph neural network method. c) Compare the accuracy performance measures of the classic graph based semisupervised learning problem, the classic hypergraph based semisupervised learning problem, the graph neural network method, the hypergraph neural network method, and our proposed hypergraph neural network method when we apply these five methods to solve the noisy label learning problem. We will organize the paper as follows: Section 2 will discuss the related work. Section 3 will  introduce the novel version of hypergraph neural network method. Section 4 will describe the datasets and  present the experimental results. Section 5 will conclude this paper and the future direction of researches will  be discussed.   2. RELATED WORK "
293,CNN-Based Real-Time Parameter Tuning for Optimizing Denoising Filter Performance.txt,"We propose a novel direction to improve the denoising quality of
filtering-based denoising algorithms in real time by predicting the best filter
parameter value using a Convolutional Neural Network (CNN). We take the use
case of BM3D, the state-of-the-art filtering-based denoising algorithm, to
demonstrate and validate our approach. We propose and train a simple, shallow
CNN to predict in real time, the optimum filter parameter value, given the
input noisy image. Each training example consists of a noisy input image
(training data) and the filter parameter value that produces the best output
(training label). Both qualitative and quantitative results using the widely
used PSNR and SSIM metrics on the popular BSD68 dataset show that the
CNN-guided BM3D outperforms the original, unguided BM3D across different noise
levels. Thus, our proposed method is a CNN-based improvement on the original
BM3D which uses a fixed, default parameter value for all images.","Image denoising refers to the process of removing noise from a distorted image to recover the clean image. During acquisition, compression or transmission, images and videos often get corrupted by noise. Thus, when the corruption occurs at a particular stage of the processing pipeline, there is a degradation in quality of output of subsequent steps, ultimately aecting the nal visualization. This necessitates the image denoising [21] step for signal processing and transmission applications. In the real world, accurately predicting the result of noise contamination of a clean signal is dicult, as theoretically, there are innumerable possible noise patterns that can contaminate a clean signal. However, most realworld noise patterns can be approximated by Additive White Gaussian Noise (AWGN), and thus it is commonly discussed in the literature. Consequently, traditional denos ing approaches try to model image priors and solve optimization problems, e.g., nonlocal selfsimilarity (NSS) models [4,9], sparse representations models [13,20] ?Supported by NSERC Discovery Grant and DND Supplement.arXiv:2001.06961v1  [eess.IV]  20 Jan 20202 S. Mukherjee et al. and gradientbased models [26,31]. However, these traditional approaches to denoising are slow due to the optimization process, and thus often unt for realtime applications. Also, complex and diverse scene content often cannot be denoised eectively using such handcrafted image priors. The recent breakthroughs in image denoising come from deep neural networks (DNNs), and especially deep Convolutional Neural Networks (CNNs), which use a discriminative denoising model, e.g., MLP [5], REDNet [21] and DnCNN [34]. Their superior performance in many instances is mainly due to the modeling capability of CNNs and the computational capacity of modern GPUs for training progressively deeper and deeper networks. These discriminative models based on deep learning often demonstrate better performance that the traditional model based methods. However, their performance on unseen data (during inference) often varies depending on the type of data they were trained on. If training data for a particular type of application is not representative enough and the model cannot generalize well enough, the denoising performance will suer, which is an inherent issue with all learningbased approaches. For natural images, if the test image has been signicantly distorted with high noise level, causing most structures and ne details in the original image to get visually obfuscated, the discriminative learning approaches often prove insucient. This paper proposes and validates a \middle ground"" between the above two approaches. It uses the GPUbased implementation of a stateoftheart modelbased approach (namely, the Block Matching 3D lter, BM3D [9]) whose parameter is tuned by our proposed CNN in real time, depending on the charac teristics of the noisy input image. This approach is \bestofbothworlds"" in the sense that its denoising work ow has a wellunderstood theoretical basis and is thus, fully explainable (BM3D algorithm) unlike endtoend trained CNNs. At the same time, it optimizes denoising quality by tuning the model parameter us ing a CNN, which can capture more complex characteristics of the input image than what is possible using traditional handcrafted methods. In this paper, we consider a \nonblind"" denoising scenario like section 5.2.1 of [35], where the noise is assumed to be AWGN with known standard deviation. 1.1 Motivation As discussed earlier, over the last few decades, the various challenges posed by the denoising problem has been analyzed thoroughly by many researchers and a lot of interesting solutions have been proposed. In the nonlearningbased cate gory, one of the greatest and recent breakthroughs was achieved by BM3D. Very recently, researchers have found that BM3D outperforms even deep learning based methods for realworld, nonAWGN noise, e.g. in photographs captured by consumer cameras [24]. Moreover, ecient GPU implementation of BM3D has signicantly improved its time performance [17]. BM3D has a lot of input parameters which need to be tuned, though most published denoising methods (learning and nonlearning based) compare their performance with BM3D using its default parameter values, as mentioned in the original BM3D paper [9].Title Suppressed Due to Excessive Length 3 In recent years, researchers have experimentally proved that BM3D perfor mance is, in fact, sensitive to its parameter settings and further, that changing some parameter values in uence its denoising performance signicantly more than changing values of other parameters [19,2]. We repeated those experiments and came to the same conclusion as the researchers that the 3Dis one of the few parameters which cause signicant dierence in BM3D's denoising performance. In BM3D, after grouping of similar (correlated) image blocks (patches), a 3D decorrelating unitary transform is applied to each 3D stack of grouped similar blocks. Enhanced denoising and image detail preservation can only be ensured by choosing a suitable threshold value ( 3D) for applying a hard thresholding operator on the transform coecients. This explains why the 3Dparameter has signicant in uence on BM3D denoising quality. Recently, researchers have tried to adapt the BM3D parameter 3Dto the statistical characteristics of the input image and noise [14] using the Noise Inval idation Denoising (NIDe) technique [3]. However, the parameter used in NIDe for noise condence interval estimation has been xed to the constant value 3, and the suitability of the method [14] for realtime performance has not been dis cussed. Researchers have also attempted to adaptively set the distance threshold for grouping similar image blocks, based on the ratio of the mean and standard deviation and the estimated noise intensity [11]. Motivated by the observation that the Human Vision System is locally adaptive, in another work [12] re searchers have tried to vary BM3D parameters according to local perceptual im age characteristics in a manner determined by extensive subjective experiments. In yet another work, researchers have tried to incorporate locallyadaptive patch shapes and Principal Component Analysis (PCA) in the 3D transform to improve denoising quality, but at the cost of increasing time complexity manyfold, as well as rendering their algorithm unsuitable for realtime GPU implementation (due to adaptiveshape patches) [10]. Other researchers [2] have used traditional learning algorithms like Naive Bayes, Support Vector Machine (SVM), K Nearest Neighbors (kNN) and Random Forest to train numerous classiers to set the 3D value for each block based on the block's texture. However, blockwise prediction of3Dis expected to increase the BM3D time complexity signicantly. Yet, the authors did not report the time performance of their proposed method. Also, they used 77 sized blocks for classication, but did not report or discuss the possible eects of choosing other block sizes. Lastly, even a very recent attempt at replacing parts of the BM3D pipeline with a CNN did notshow potential for realtime performance, even using the fastest GPUs available in the market [32]. In this work, we design a Convolutional Neural Network (CNN) that can pre dict the3Dparameter value which best denoises a noisy image. We compare the performance of our method by comparing the denoising performance of BM3D (using our CNNestimated parameter value) against the denoising performance of BM3D using the default value for the 3Dparameter, as recommended in the original BM3D paper [9].4 S. Mukherjee et al. 1.2 Our Contribution To the best of our knowledge, we are the rst to propose a simple, shallow CNNbased realtime solution to predict optimum parameter values for a lter ing based denoising algorithm. In this paper, we consider such a stateoftheart algorithm, BM3D as a use case to demonstrate and validate this proposal. We propose a method that is readily implementable on GPUs and (for our use case) enhances the denoising capability of the recent GPUbased BM3D implementa tion without signicantly increasing the overall time complexity. The rest of this paper is organized as follows: Related work is given in Sec tion 2. We present our proposed method in Section 3. Experimental results and analysis are in Section 4. In Section 5, we give the conclusion and future work. 2 Related Work "
294,De-identifying Australian Hospital Discharge Summaries: An End-to-End Framework using Ensemble of Deep Learning Models.txt,"Electronic Medical Records (EMRs) contain clinical narrative text that is of
great potential value to medical researchers. However, this information is
mixed with Personally Identifiable Information (PII) that presents risks to
patient and clinician confidentiality. This paper presents an end-to-end
deidentification framework to automatically remove PII from Australian hospital
discharge summaries. Our corpus included 600 hospital discharge summaries which
were extracted from the EMRs of two principal referral hospitals in Sydney,
Australia. Our end-to-end de-identification framework consists of three
components: 1) Annotation: labelling of PII in the 600 hospital discharge
summaries using five pre-defined categories: person, address, date of birth,
individual identification number, phone/fax number; 2) Modelling: training six
named entity recognition (NER) deep learning base-models on balanced and
imbalanced datasets; and evaluating ensembles that combine all six base-models,
the three base-models with the best F1 scores and the three base-models with
the best recall scores respectively, using token-level majority voting and
stacking methods; and 3) De-identification: removing PII from the hospital
discharge summaries. Our results showed that the ensemble model combined using
the stacking Support Vector Machine (SVM) method on the three base-models with
the best F1 scores achieved excellent results with a F1 score of 99.16% on the
test set of our corpus. We also evaluated the robustness of our modelling
component on the 2014 i2b2 de-identification dataset. Our ensemble model, which
uses the token-level majority voting method on all six basemodels, achieved the
highest F1 score of 96.24% at strict entity matching and the highest F1 score
of 98.64% at binary token-level matching compared to two state-of-the-art
methods.","There is currently intense interest in using narrative free text from Electronic Medical Records (EMRs)  to develop predictive algorithms for patient stratification and personalised health care. However,  sharing of and access to this clinical text for research purposes are severely constrained because it  includes information that may identify specific individuals, known as Personally Identifiable  Information (PII) [1]. Development of robust and scalable yet resourceefficient methods for de identifying clinical narrative text is a priority for making these valuable data more available to  researchers [2] to generate evidence that will deliver benefits to both health care providers and patients  [3].  In Australia, there is no clear definition of the term ‚Äúdeidentified health data‚Äù or agreed standards for  deidentification, even though a strong health privacy regulatory environment is in place [4]. In contrast,  the Health Insurance Portability and Accountability Act (HIPAA) which was passed in United States in  1996 provides two deidentification methods: 1) The ‚ÄúExpert Determination‚Äù method: qualified experts  analyse and determine the risk of reidentifying an individual who is a subject of the information; and  2) The ‚ÄúSafe Harbor‚Äù method: this method requires the removal of 18 types of identifiers (see Table  S1), including names, geographic subdivisions, telephone numbers, medical record numbers and so on  [5].   Manual deidentification of a large clinical corpus is a timeconsuming and challenging task [6, 7].  Some studies have explored the possibility of using Natural Language Processing (NLP) and deep  learning techniques for automatic deidentification relevant to the ‚ÄúSafe Harbor‚Äù method [810]. Three  deidentification challenges, organized by the Informatics for Integrating Biology and the Bedside (i2b2)  organization in 2006 [11] and 2014 [12], and Centres of Excellence in Genomic Science (CEGS) and  Neuropsychiatric GenomeScale and RDOC Individualized Domains (NGRID) in 2016 [13],  facilitated innovation in deidentification methods based on NLP and proved the efficiency of automatic  deidentification. However, these challenges involved building models and comparing results using a  specific annotated dataset. There is a need for endtoend deidentification solutions which can be easily  applied to raw clinical narrative documents, especially within the internet accessrestricted  environments of hospitals.  In this paper, we propose an endtoend deidentification framework with application to Australian  hospital discharge summaries. This paper describes the three components of the endtoend framework:  1) Annotation: a webbased annotation tool to help our human annotators tag the PII entities in the raw  hospital discharge summaries; 2) Modelling: training and evaluating ensembles of different named  entity recognition (NER) deep learning models to identify PII in hospital discharge summaries; 3) De identification: automatic tag and removal of PII entities from hospital discharge summaries.  2. Related work   "
295,Part-based Pseudo Label Refinement for Unsupervised Person Re-identification.txt,"Unsupervised person re-identification (re-ID) aims at learning discriminative
representations for person retrieval from unlabeled data. Recent techniques
accomplish this task by using pseudo-labels, but these labels are inherently
noisy and deteriorate the accuracy. To overcome this problem, several
pseudo-label refinement methods have been proposed, but they neglect the
fine-grained local context essential for person re-ID. In this paper, we
propose a novel Part-based Pseudo Label Refinement (PPLR) framework that
reduces the label noise by employing the complementary relationship between
global and part features. Specifically, we design a cross agreement score as
the similarity of k-nearest neighbors between feature spaces to exploit the
reliable complementary relationship. Based on the cross agreement, we refine
pseudo-labels of global features by ensembling the predictions of part
features, which collectively alleviate the noise in global feature clustering.
We further refine pseudo-labels of part features by applying label smoothing
according to the suitability of given labels for each part. Thanks to the
reliable complementary information provided by the cross agreement score, our
PPLR effectively reduces the influence of noisy labels and learns
discriminative representations with rich local contexts. Extensive experimental
results on Market-1501 and MSMT17 demonstrate the effectiveness of the proposed
method over the state-of-the-art performance. The code is available at
https://github.com/yoonkicho/PPLR.","Person reidentification (reID) aims to retrieve a per son corresponding to a given query across disjoint camera views or different time stamps [59, 69]. Thanks to the dis criminative power of deep neural networks, supervised ap proaches [20‚Äì22,53] have achieved impressive performance in this task. Unfortunately, they require a large amount of labeled data that demands costly annotations, limiting their practicality in largescale realworld reID problems. Dueto this issue, unsupervised methods that learn the discrimi native features for person retrieval from unlabeled data have recently received much attention. Prior works on unsupervised person reID have utilized pseudolabels obtained by knearest neighbor search [25, 47,60] or unsupervised clustering [7,24] for training. These approaches alternate a twostage training scheme: the label generation phase that assigns pseudolabels and the train ing phase that trains a model with generated labels. Among these approaches, the clusteringbased methods [2, 9] have especially demonstrated their effectiveness with stateof theart performance. However, inherent noises in pseudo labels significantly hinder the performance of these unsu pervised methods. To tackle this problem, many efforts have been made to improve the accuracy of pseudolabels by performing robust clustering [9, 62] or pseudolabel refinement [25, 64]. Re cent techniques [8, 63] significantly reduce the label noise through the model ensemble in a peerteaching manner by using predictions from an auxiliary network as refined la bels for the target network. Nevertheless, training multi ple backbones as teacher networks ( e.g., dual ResNet in MMT [8], and single DenseNet, ResNet, and Inceptionv3 in MEBNet [63]) requires high computational costs. Fur thermore, labels refined by these methods consider only global features and neglect the finegrained clues essential to person reID, leading to insufficient performance. To address the aforementioned problems, we propose Partbased Pseudo Label Refinement (PPLR) , a novel unsu pervised reID framework that effectively handles the label noise using part features in a selfteaching manner. Sev eral studies [42,67] demonstrate that the finegrained infor mation from part features improves the reID performance. Our key idea is that this finegrained information can pro vide not only useful cues for better representation learning but also robustness against label noises. In contrast to the globalshape information that has large variations due to significant changes in poses and viewpoints, part features can capture the localtexture information that provides a more crucial clue to reidentifying a person [70]. We argue that the complementary relationship between 1the global and part features can be used to refine the label noise in each of their feature spaces. However, some of the global and part features from the same image capture very different semantic information, and using the comple mentary relationship na ¬®ƒ±vely can result in noisy and even incorrect information. For instance, images may contain ir relevant parts ( e.g., occlusions or backgrounds) that provide unreliable complementary information, and it is desirable to exclude them from the training. Therefore, it is essential to identify whether the information of global and part features are reliable with each other to properly exploit their com plementary relationship. To address this issue, we design a cross agreement score based on the similarity between the knearest neighbors of global and part features. Based on the cross agreement, we propose two pseudolabel refine ment methods ‚Äì partguided label refinement (PGLR) and agreementaware label smoothing (AALS) . PGLR refines the pseudolabels of global features by aggregating the pre dictions of part features, guiding the global features to learn from rich local contexts. AALS refines the pseudolabels of part features by smoothing the label distributions, thus calibrating the predictions of part features. Our contributions can be summarized as follows: ‚Ä¢ We propose a partbased pseudolabel refinement framework that operates in a selfensemble manner without auxiliary networks. To the best of our knowl edge, this is the first work to handle the label noise using the part feature information for person reID. ‚Ä¢ We design a cross agreement score to capture reli able complementary information, which is computed by the similarity between the knearest neighbors of the global and part features. ‚Ä¢ Extensive experimental results with superior perfor mance against the stateoftheart methods demon strate the effectiveness of the proposed method. 2. Related Work "
296,DivideMix: Learning with Noisy Labels as Semi-supervised Learning.txt,"Deep neural networks are known to be annotation-hungry. Numerous efforts have
been devoted to reducing the annotation cost when learning with deep networks.
Two prominent directions include learning with noisy labels and semi-supervised
learning by exploiting unlabeled data. In this work, we propose DivideMix, a
novel framework for learning with noisy labels by leveraging semi-supervised
learning techniques. In particular, DivideMix models the per-sample loss
distribution with a mixture model to dynamically divide the training data into
a labeled set with clean samples and an unlabeled set with noisy samples, and
trains the model on both the labeled and unlabeled data in a semi-supervised
manner. To avoid confirmation bias, we simultaneously train two diverged
networks where each network uses the dataset division from the other network.
During the semi-supervised training phase, we improve the MixMatch strategy by
performing label co-refinement and label co-guessing on labeled and unlabeled
samples, respectively. Experiments on multiple benchmark datasets demonstrate
substantial improvements over state-of-the-art methods. Code is available at
https://github.com/LiJunnan1992/DivideMix .","The remarkable success in training deep neural networks (DNNs) is largely attributed to the collection of large datasets with human annotated labels. However, it is extremely expensive and timeconsuming to label extensive data with highquality annotations. On the other hand, there exist alternative and inexpensive methods for mining largescale data with labels, such as querying commercial search engines (Li et al., 2017a), downloading social media images with tags (Mahajan et al., 2018), leveraging machinegenerated labels (Kuznetsova et al., 2018), or using a single annotator to label each sample (Tanno et al., 2019). These alternative methods inevitably yield samples with noisy labels . A recent study (Zhang et al., 2017) shows that DNNs can easily overÔ¨Åt to noisy labels and results in poor generalization performance. Existing methods on learning with noisy labels (LNL) primarily take a loss correction approach. Some methods estimate the noise transition matrix and use it to correct the loss function (Patrini et al., 2017; Goldberger & BenReuven, 2017). However, correctly estimating the noise transition matrix is challenging. Some methods leverage the predictions from DNNs to correct labels and modify the loss accordingly (Reed et al., 2015; Tanaka et al., 2018). These methods do not perform well under high noise ratio as the predictions from DNNs would dominate training and cause overÔ¨Åtting. To overcome this, Arazo et al. (2019) adopt MixUp (Zhang et al., 2018) augmentation. Another approach selects or reweights samples so that noisy samples contribute less to the loss (Jiang et al., 2018; Ren et al., 2018). A challenging issue is to design a reliable criteria to select clean samples. It has been shown that DNNs tend to learn simple patterns Ô¨Årst before Ô¨Åtting label noise (Arpit et al., 2017). Therefore, many methods treat samples with small loss as clean ones (Jiang et al., 2018; Arazo et al., 2019). Among those methods, Coteaching (Han et al., 2018) and Coteaching +(Yu et al., 2019) train two networks where each network selects smallloss samples in a minibatch to train the other. Another active area of research that also aims to reduce annotation cost is semisupervised learning (SSL). In SSL, the training data consists of unlabeled samples in addition to the labeled samples. SigniÔ¨Åcant progress has been made in leveraging unlabeled samples by enforcing the model to produce 1arXiv:2002.07394v1  [cs.CV]  18 Feb 2020Published as a conference paper at ICLR 2020 low entropy predictions on unlabeled data (Grandvalet & Bengio, 2004) or consistent predictions on perturbed input (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Miyato et al., 2019). Recently, Berthelot et al. (2019) propose MixMatch , which uniÔ¨Åes several dominant SSL approaches in one framework and achieves stateoftheart performance. Despite the individual advances in LNL and SSL, their connection has been underexplored. In this work, we propose DivideMix , which addresses learning with label noise in a semisupervised manner. Different from most existing LNL approaches, DivideMix discards the sample labels that are highly likely to be noisy, and leverages the noisy samples as unlabeled data to regularize the model from overÔ¨Åtting and improve generalization performance. The key contributions of this work are: We propose codivide, which trains two networks simultaneously. For each network, we dynamically Ô¨Åt a Gaussian Mixture Model (GMM) on its persample loss distribution to divide the training samples into a labeled set and an unlabeled set. The divided data is then used to train the other network. Codivide keeps the two networks diverged, so that they can Ô¨Ålter different types of error and avoid conÔ¨Årmation bias in selftraining. During SSL phase, we improve MixMatch with label coreÔ¨Ånement and coguessing to account for label noise. For labeled samples, we reÔ¨Åne their groundtruth labels using the network‚Äôs predictions guided by the GMM for the other network. For unlabeled samples, we use the ensemble of both networks to make reliable guesses for their labels. We experimentally show that DivideMix signiÔ¨Åcantly advances stateoftheart results on multiple benchmarks with different types and levels of label noise. We also provide extensive ablation study and qualitative results to examine the effect of different components. 2 R ELATED WORK "
297,A Novel Self-Supervised Re-labeling Approach for Training with Noisy Labels.txt,"The major driving force behind the immense success of deep learning models is
the availability of large datasets along with their clean labels.
Unfortunately, this is very difficult to obtain, which has motivated research
on the training of deep models in the presence of label noise and ways to avoid
over-fitting on the noisy labels. In this work, we build upon the seminal work
in this area, Co-teaching and propose a simple, yet efficient approach termed
mCT-S2R (modified co-teaching with self-supervision and re-labeling) for this
task. First, to deal with significant amount of noise in the labels, we propose
to use self-supervision to generate robust features without using any labels.
Next, using a parallel network architecture, an estimate of the clean labeled
portion of the data is obtained. Finally, using this data, a portion of the
estimated noisy labeled portion is re-labeled, before resuming the network
training with the augmented data. Extensive experiments on three standard
datasets show the effectiveness of the proposed framework.","The success of deep learning models like Alexnet [15], VGG [32], ResNet [12], etc. for image classiÔ¨Åcation tasks can be attributed to the availability of large, annotated datasets like ImageNet [16]. But, obtaining clean annotations of large datasets is very expensive, and thus recent research has focused on learning from weakly supervised data, where the labels are often noisy , since they have been acquired from web searches [18] or crowdsourcing [34]. The presence of noisy labels can severely degrade the performance of the learned classiÔ¨Åers, since deep neural networks can even overÔ¨Åt on the noisy labels with sufÔ¨Åcient training, due to their memorization capability [37] [3]. Recently, several approaches have been proposed to handle label noise in the training data [37] [11] [35] [13] [20] [10] [28]. One direction to address this problem is to estimate the noise transition matrix itself by utilizing an additional softmax layer [10] for modeling the channel noise. An alternative twostep approach along the same lines is proposed in [28]. However, it is observed that estimation of the noise transition matrix is often hard and * equal contribution. c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:1910.05700v3  [cs.CV]  1 Jan 20202DEVRAJ: SELFSUPERVISED RELABELING METHOD FOR HANDLING NOISY LABELS computationally expensive, especially when a large number of classes are present in the data [11]. A more recent and elegant direction to handle label noise is to utilise the concept of small loss instances [13] [30] [11]. Here, the algorithms estimate which samples are correctly labeled, and uses them to train the network subsequently. MentorNet [13] uses a pretrained network (trained in a selfpaced manner with a curriculum loss) to select clean labeled data to train the Ô¨Ånal classiÔ¨Åcation model. Coteaching [11] trains two networks in parallel and updates the weights of the networks using only the small loss samples. In addition, the gradients of the two networks are switched during the parameter update, which leads to better performance. It is observed [11] that when the training continues for a long time, the two networks generally converge to the same state and start performing similarly, leading to the accumulation of errors. Here, we propose a novel framework based on the Coteaching approach [11], which also uses the concept of smallloss instances along with selfsupervision and relabeling, to signiÔ¨Åcantly improve the training of deep networks with very noisy training data. The proposed approach has four main steps. First, to deal with signiÔ¨Åcant amount of label noise, we utilize selfsupervision as a pretraining step, so that the network can learn robust features without the use of any labels. Second, we use a parallel network architecture similar to the one in [11] to get an estimate of the small loss samples. In the third step, utilizing a portion of the small loss samples, the class means for all the categories are computed, which are then used to relabel the large loss samples. Finally, the network training is resumed by taking all the small loss samples along with a portion of the relabeled large loss samples based on a conÔ¨Ådence measure. The proposed framework is termed as mCTS2R (modiÔ¨Åed coteaching with selfsupervision and relabeling). The main contributions of this work are as follows: We develop a novel approach by utilizing selfsupervision and relabeling of the large loss data using the small loss samples for training deep networks under signiÔ¨Åcant label noise. Our framework uses two parallel networks only to determine the small loss instances. Unlike [11] [20] which requires two networks for the entire training, the Ô¨Ånal training of the proposed framework after relabeling proceeds using a single network. This makes our model computationally much lighter. We propose using a selfsupervised training paradigm like [9] to learn about the data distribution without using the labels, which helps to avoid overÔ¨Åtting on the noisy labels. Extensive experiments on three benchmark datasets show the effectiveness of the pro posed framework. 2 Related Work "
298,Learning to Learn from Noisy Web Videos.txt,"Understanding the simultaneously very diverse and intricately fine-grained
set of possible human actions is a critical open problem in computer vision.
Manually labeling training videos is feasible for some action classes but
doesn't scale to the full long-tailed distribution of actions. A promising way
to address this is to leverage noisy data from web queries to learn new
actions, using semi-supervised or ""webly-supervised"" approaches. However, these
methods typically do not learn domain-specific knowledge, or rely on iterative
hand-tuned data labeling policies. In this work, we instead propose a
reinforcement learning-based formulation for selecting the right examples for
training a classifier from noisy web search results. Our method uses Q-learning
to learn a data labeling policy on a small labeled training dataset, and then
uses this to automatically label noisy web data for new visual concepts.
Experiments on the challenging Sports-1M action recognition benchmark as well
as on additional fine-grained and newly emerging action classes demonstrate
that our method is able to learn good labeling policies for noisy data and use
this to learn accurate visual concept classifiers.","Humans are a central part of many visual scenes, and un derstanding human actions in videos is an important prob lem in computer vision. However, a key challenge in action recognition is scaling to the long tail of actions. In many practical applications, we would like to quickly and cheaply learn classiÔ¨Åers for new target actions where annotations are scarce, e.g. Ô¨Ånegrained, rare or niche classes. Manually annotating data for every new action becomes impossible, so there is a need for methods that can automatically learn from readily available albeit noisy data sources. A promising approach is to leverage noisy data from web Riding a camel Riding a bull Riding animals Feeding animals Feeding camels Feeding cattleTRAINING: Learn to select queries from ‚ÄúYouTube‚Äù Grazing animalsTESTING: Select right queries from ‚ÄúYouTube‚Äù Our model Semi supervised Running horse ?Figure 1: Our model uses a set of annotated data to learn a policy for how to label data for new, unseen classes. This enables learning domainspeciÔ¨Åc knowledge and how to se lect diverse exemplars while avoiding semantic drift. For example, it can learn from training data that human motion cues are important for actions involving animals (e.g. ‚Äúrid ing animals‚Äù) while animal appearance is not. This knowl edge can be applied at test time to label noisy data for new classes such as ‚Äúfeeding animals‚Äù, while traditional semi supervised methods would label based on visual similarity. queries. Training models for new classes using the data re turned by web queries has been proposed as an alternative to expensive manual annotation [7, 8, 19, 28]. Methods for automated labeling of new classes include traditional semisupervised learning approaches [14, 33, 34] as well as weblysupervised approaches [7, 8, 19]. However, these methods typically rely on iterative handtuned data label ing policies. This makes it difÔ¨Åcult to dynamically manage the risk tradeoff between exemplar diversity and semantic drift. Going further, as a result these methods typically can 1arXiv:1706.02884v1  [cs.CV]  9 Jun 2017not learn domainspeciÔ¨Åc knowledge. For example, when learning an action recognition model from a set of videos returned by YouTube queries, videos prominently featuring humans are more likely to be positives while those with out are more likely to be noise; this intuition is difÔ¨Åcult to manually quantify and encode. Even more, when learning an animalrelated action such as ‚Äúfeeding animals‚Äù, videos containing the action with different animals are likely to be useful positives even though their visual appearance may be different (Fig. 1). Such diverse classconditional data selec tion policies are impossible to manually encode. This in tuition inspires our work on learning data selection policies for noisy web search results. Our key insight is that good data labeling policies can be learned from existing manually annotated datasets. In tuitively, a good policy labels noisy data in a way where a classiÔ¨Åer trained on the labels would achieve high classiÔ¨Å cation accuracy on a manually annotated heldout set. Al though data labeling is a nondifferentiable action, this can be naturally achieved in a reinforcement learning setting, where actions correspond to labeling of examples and the reward is the effect on downstream classiÔ¨Åer accuracy. Concretely, we introduce a joint formulation of a Q learning agent [29] and a class recognition model. In con trast to related weblysupervised approaches [7, 19], the data collection and classiÔ¨Åer training steps are not disjoint but rather integrated into a single uniÔ¨Åed framework. The agent selects web search examples to label as positives, which are then used to train the recognition model. A sig niÔ¨Åcant challenge is the choice of the state representation, and we introduce a novel representation based on the distri bution of classiÔ¨Åer scores output by the recognition model. At training time, the model uses a dataset of labeled train ing classes to learn a data labeling policy, and at test time the model can use this policy to label noisy web data for new unseen classes. In summary, our main contribution is a principled formu lation for learning how to label noisy web data, using a re inforcement learning framework. To enable this, we also in troduce a novel state representation in terms of the classiÔ¨Åer score distributions from a jointly trained recognition model. We demonstrate our approach Ô¨Årst in the controlled setting of MNIST, then on the largescale Sports1M video bench mark [16]. Finally, we show that our method can be used for labeling newly emerging and Ô¨Ånegrained categories where annotated data is scarce. 2. Related work "
299,Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings.txt,"Fine-grained entity type classification (FETC) is the task of classifying an
entity mention to a broad set of types. Distant supervision paradigm is
extensively used to generate training data for this task. However, generated
training data assigns same set of labels to every mention of an entity without
considering its local context. Existing FETC systems have two major drawbacks:
assuming training data to be noise free and use of hand crafted features. Our
work overcomes both drawbacks. We propose a neural network model that jointly
learns entity mentions and their context representation to eliminate use of
hand crafted features. Our model treats training data as noisy and uses
non-parametric variant of hinge loss function. Experiments show that the
proposed model outperforms previous state-of-the-art methods on two publicly
available datasets, namely FIGER (GOLD) and BBN with an average relative
improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one
dataset can be transferred to other datasets while using same model or other
FETC systems. These approaches of transferring knowledge further improve the
performance of respective models.","Entity type classiÔ¨Åcation is the task for assigning types or labels such as organization ,location to entity mentions in a document. This classiÔ¨Åca tion is useful for many natural language process ing (NLP) tasks such as relation extraction (Mintz et al., 2009), machine translation (Koehn et al.,2007), question answering (Lin et al., 2012) and knowledge base construction (Dong et al., 2014). There has been considerable amount of work on Named Entity Recognition (NER) (Collins and Singer, 1999; Tjong Kim Sang and De Meul der, 2003; Ratinov and Roth, 2009; Manning et al., 2014), which classiÔ¨Åes entity mentions into a small set of mutually exclusive types, such as Person ,Location ,Organization andMisc . How ever, these types are not enough for some NLP applications such as relation extraction, knowl edge base construction (KBC) and question an swering. In relation extraction and KBC, know ing Ô¨Ånegrained types for entities can signiÔ¨Åcantly increase the performance of the relation extrac tor (Ling and Weld, 2012; Koch et al., 2014; Mitchell et al., 2015) since this helps in Ô¨Åltering out candidate relation types that do not follow the type constrain. Finegrained entity types provide additional information while matching questions to its potential answers and signiÔ¨Åcantly improves performance (Dong et al., 2015). For example, Li and Roth (2002) rank questions based on their ex pected answer types (will the answer be food,ve hicle ordisease ). Typically, FETC systems use over hundred la bels, arranged in a hierarchical structure. An im portant aspect of FETC is that based on local con text, two different mentions of same entity can have different labels. We illustrate this through an example in Figure 1. All three sentences S1,S2, andS3mention same entity Barack Obama . How ever, looking at the context, we can infer that S1 mentions Obama as a person/author ,S2mentions Obama only as a person , and S3mentions Obama as aperson/politician . Available training data for FETC has noisy la bels. Creating manually annotated training data for FETC is time consuming, expensive, and er ror prone. Note that, a human annotator willarXiv:1702.06709v1  [cs.CL]  22 Feb 2017Figure 1: Noise introduced via distant supervision process. S1S3 indicates sentences where only a subset of labels for entity mention (bold typeface) are relevant given context, highlighted in T1T3. have to assign a subset of correct labels from a set of around hundred labels for each entity men tion in the corpus. Existing FETC systems use distant supervision paradigm (Craven and Kum lien, 1999) to automatically generate training data. Distant supervision maps each entity in the cor pus to knowledge bases such as Freebase (Bol lacker et al., 2008), DBpedia (Auer et al., 2007), YAGO (Suchanek et al., 2007). This method as signs same set of labels to all mentions of an entity across the corpus. For example, Barack Obama is a person, politician, lawyer, and author. If a knowledge base has these four matching la bels for Barack Obama, then distant supervision assigns all of them to every mention of Barack Obama. Training data generated with distant su pervision will fail to distinguish between mentions of Barack Obama in sentences S1,S2, and S3. Existing FETC systems have one or both of fol lowing drawbacks: 1. Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016) 2. Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016) We have observed that for real world datasets, more than twenty Ô¨Åve percent of training data has noisy labels. First drawback propagates this noise in training data to the FETC model. To extract hand crafted features various NLP tools are used. Since errors inevitably exist in such tools, the sec ond drawback propagates errors of these tools to FETC model. We propose a neural network based model to overcome the two drawbacks of existing FETC systems. First, we separate training data into clean andnoisy partitions using the same method as in AFET system (Ren et al., 2016). For these partitions, we use simple yet effective nonparametric variant of hinge loss function while training. To avoid use of hand crafted features, we learn repre sentations for given entity mention and its context. Additionally, we investigate effectiveness of us ing transfer learning (Pratt, 1993) for FETC task both at feature and model level. We show that feature level transfer learning can be used to im prove performance of other FETC system such as AFET, by up to 4.5% in microF1 score. Simi larly, model level transfer learning can be used to improve performance of the same model using dif ferent dataset by up to 3.8% in microF1 score. Our contributions can be summarized as fol lows: 1. We propose a simple neural network model that learns representations for entity mention and its context, and incorporate noisy label information using a variant of nonparametric hinge loss function. Experimental results on two publicly available datasets demonstrate the effectiveness of proposed model, with an average relative improvement of 2.69% in microF1 score. 2. We investigate the use of feature level and model level transferlearning strategies in the domain of the FETC task. The proposed transfer learning strategies further improve the stateoftheart on BBN dataset by 3.8% in microF1 score. 2 Related Work "
300,Learning to Bootstrap for Combating Label Noise.txt,"Deep neural networks are powerful tools for representation learning, but can
easily overfit to noisy labels which are prevalent in many real-world
scenarios. Generally, noisy supervision could stem from variation among
labelers, label corruption by adversaries, etc. To combat such label noises,
one popular line of approach is to apply customized weights to the training
instances, so that the corrupted examples contribute less to the model
learning. However, such learning mechanisms potentially erase important
information about the data distribution and therefore yield suboptimal results.
To leverage useful information from the corrupted instances, an alternative is
the bootstrapping loss, which reconstructs new training targets on-the-fly by
incorporating the network's own predictions (i.e., pseudo-labels).
  In this paper, we propose a more generic learnable loss objective which
enables a joint reweighting of instances and labels at once. Specifically, our
method dynamically adjusts the per-sample importance weight between the real
observed labels and pseudo-labels, where the weights are efficiently determined
in a meta process. Compared to the previous instance reweighting methods, our
approach concurrently conducts implicit relabeling, and thereby yield
substantial improvements with almost no extra cost. Extensive experimental
results demonstrated the strengths of our approach over existing methods on
multiple natural and medical image benchmark datasets, including CIFAR-10,
CIFAR-100, ISIC2019 and Clothing 1M. The code is publicly available at
https://github.com/yuyinzhou/L2B.","Recent advances in deep learning have achieved great success on various computer vision applications, where largescale clean datasets are available. However, 1arXiv:2202.04291v1  [cs.CV]  9 Feb 2022noisy labels or intentional label corruption by an adversarial rival could easily cause dramatic performance drop [ 24]. This problem is even more crucial in the medical Ô¨Åeld, given that the annotation quality requires great expertise. Therefore, understanding, modeling, and learning with noisy labels has gained great momentum in recent research eÔ¨Äorts [ 5,23,8,17,20,11,28,40,16,34, 47, 41, 49, 36, 48]. Existing methods of learning with noisy labels primarily take a loss correction strategy. One popular direction is to Ô¨Årst estimate the noise corruption matrix and then use it to correct the loss function [ 26,6]. However, correctly estimating the noise corruption matrix is usually challenging and often involves assumptions about the noise generation process [ 37,21,10]. Other research eÔ¨Äorts focus on selecting clean samples from the noisy data [ 11,7,44,4] by treating samples with small loss as clean ones [ 2]. Instead of directly discarding those ‚Äúunclean‚Äù examples, an extension of this idea is focusing on assigning learnable weights to each example in the noisy training set [ 28,30], where noisy samples have low weights. However, discarding or attending less to a subset of the training data (e.g., noisy samples) can erase important information about the data distribution. To fully exploit the corrupted training samples, another direction is to leverage the network predictions (i.e., pseudolabels [ 14]) to correct or reweight the original labels [ 27,31], so that the holistic data distribution information could be preserved during network training. One representative work is the bootstrapping loss [ 27], which introduces a perceptual consistency term in the learning objective that assigns a weight to the pseudolabels to compensate for the erroneous guiding of noisy samples. While in this strategy, the weight for the pseudolabels is manually selected and remains the same for all training samples, which does not prevent Ô¨Åtting the noisy ones and can even lead to lowquality label correction [ 1]. To tackle this challenge, Arazo et al. [ 1] designed a dynamic bootstrapping strategy to adjusts the label weight by Ô¨Åtting a mixture model. Instead of separately reweighting labels or instances, in this paper, we propose a more generic learning strategy to enable a joint instance and label reweighting. We term our method as Learning to Bootstrap (L2B), where we aim to leverage the learner‚Äôs own predictions to bootstrap itself up for combating label noise from a metalearning perspective. During each training iteration, L2B learns to dynamically rebalance the importance between the real observed labels and pseudolabels, where the per sample weights are determined by the validation performance on a separated clean set in a meta network. Unlike the bootstrapping loss used in [ 27,1,46] which explicitly conducts relabeling by taking a weighted sum of the pseudo and the real label, L2B reweights the two losses associated with the pseudo and the real label instead (where the weights need not be summed as 1). In addition, we theoretically prove that our formulation, which reweights diÔ¨Äerent loss terms, can be reduced to the original bootstrapping loss and therefore conducts an implicit relabeling instead. By learning these weights in a metaprocess, our L2B yields substantial improvement (e.g., +8.9%improvement on CIFAR100 with 50% noise) compared with the instance reweighting baseline with almost no extra cost. We conduct extensive experiments on public natural image datasets 2(i.e., CIFAR10, CIFAR100, and Clothing 1M) and medical image dataset (i.e., ISIC2019), under diÔ¨Äerent types of simulated noise and realworld noise. Our method outperforms various existing explicit label correction and instance reweighting works, demonstrating the strengths of our approach. Our main contributions are as follows: ‚Ä¢We propose a generic learnable loss objective which enables a joint instance and label reweighting, for combating label noise in deep learning models. ‚Ä¢We prove that our new objective is, in fact, a more general form of the bootstrapping loss, and propose L2B to eÔ¨Éciently solve for the weights in a metalearning framework. ‚Ä¢Compared with previous instance reweighting methods, L2B exploits noisy examples more eÔ¨Äectively without discarding them by jointly rebalancing the contribution of real and pseudo labels. ‚Ä¢We show the theoretical convergence guarantees for L2B, and demonstrate its superior results on natural and medical image recognition tasks under both synthetic and realworld noise. 2 Related Works "
301,Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels.txt,"Graph Neural Networks (GNNs) have shown their great ability in modeling graph
structured data. However, real-world graphs usually contain structure noises
and have limited labeled nodes. The performance of GNNs would drop
significantly when trained on such graphs, which hinders the adoption of GNNs
on many applications. Thus, it is important to develop noise-resistant GNNs
with limited labeled nodes. However, the work on this is rather limited.
Therefore, we study a novel problem of developing robust GNNs on noisy graphs
with limited labeled nodes. Our analysis shows that both the noisy edges and
limited labeled nodes could harm the message-passing mechanism of GNNs. To
mitigate these issues, we propose a novel framework which adopts the noisy
edges as supervision to learn a denoised and dense graph, which can down-weight
or eliminate noisy edges and facilitate message passing of GNNs to alleviate
the issue of limited labeled nodes. The generated edges are further used to
regularize the predictions of unlabeled nodes with label smoothness to better
train GNNs. Experimental results on real-world datasets demonstrate the
robustness of the proposed framework on noisy graphs with limited labeled
nodes.","Graph Neural Networks (GNNs) [ 15,22] have made remarkable achievements in modeling graphs from various domains such as social networks [ 15], financial system [ 35], and recommendation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM ‚Äô22, February 21‚Äì25, 2022, Tempe, AZ, USA ¬©2022 Association for Computing Machinery. ACM ISBN 9781450391320/22/02. . . $15.00 https://doi.org/10.1145/3488560.3498408 + Adversarial edge Deleted edge Added edge Noisy edge + L abeled nodes  2  Hop neighbors ? ? D istant nodes ? + ? ? ? ? ? ? ? ? + ? ? ? ?   + W rong predictions   Noisy Graph Learned  Graph  Down  weighted edgeFigure 1: An illustration of downweighting/removing noise edges and densifying the graph for better performance. system [ 36]. The success of GNNs relies on the messagepassing mechanism [ 15,22], where node representations are updated by aggregating the information from neighbors. With this mechanism, the node representations capture node features, information of neighbors and local graph structure, which facilitate various tasks, especially semisupervised node classification. Although GNNs have shown great ability in modeling graphs, their performance can degrade significantly when trained on graphs with noisy edges and/or limited labeled nodes .First, due to the mes sage passing, GNNs are vulnerable to adversarial or noisy edges. For example, as shown in Fig. 1, poisoning attacks [ 46] add/delete care fully chosen edges to the graph. These adversarial edges (shown in red) usually connect nodes of different labels or features, thus con taminating the neighborhoods of nodes, propagating noises/errors to node representations. In addition, inherent edge noises also exist in realworld graphs. For instance, in social networks, bots tend to build connections with normal users to spread misinformation [ 11], which can also harm the performance of GNNs for bot detection. Second , for many applications, graphs are often sparsely labeled such as cell phone network for fraud detection [ 13]. Label sparsity can severely reduce the involvement of unlabeled nodes during mes sage passing, leading to poor performance. Generally, in a ùêælayer GNN, a labeled node aggregates its ùêæhop neighborhood informa tion, thus making many unlabeled nodes in ùêæhop neighborhood participate in the training, which is one major reason that GNNs can leverage unlabeled nodes for semisupervised node classifica tion. However, as verified in our preliminary analysis in Fig. 2a of Sec. 3.3, when the number of labeled nodes decreases, the amount of unlabeled nodes participating in training drops quickly, making message passing less effective. These shortcomings of GNNs hinder the adoption of GNNs for many realworld applications. Thus, it is important to develop robust GNNs that can simultaneously handle noisy graphs with sparse labels.arXiv:2201.00232v1  [cs.LG]  1 Jan 2022However, developing robust GNNs for graphs with noisy edges and limited labeled nodes is challenging. First, the training graph itself is noisy, i.e., noisy edges are mixed with the normal edges. Thus, we need supervision in downweighting or eliminating noisy edges. Second , alleviating the limited label issue requires more la bels, while obtaining more labeled nodes is timeconsuming and expensive. Hence, we need alternative approaches to more effec tively utilize the limited labels. Some initial efforts [ 20,20,33,38] have been taken to alleviate the effects of the adversarial edges such as pruning edges by using node similarity [ 38], and adopting Gaussian distribution as node representations to absorb noises [ 43]. To address the problem of sparsely labeled graphs, some meth ods [ 24,30,32] propose to obtain better representations by training GNNs with selfsupervised learning tasks such as pseudo label pre diction [ 24,32] and global context predictions [ 30]. However, little efforts are taken for robust GNNs that can simultaneously handle noisy edges and label sparsity. Since both the noisy edges and limited labeled nodes harm the message passing of GNNs and message passing is directly related to the graph structure, we argue that learning a denoised and dense graph guided by the raw attributed graph is promising to facilitate message passing for robust GNNs. First, for many graphs such as social networks, nodes with similar features and labels tend to be linked [ 26], while noisy edges would link nodes of dissimilar fea tures [ 38]. Thus, we can use node attributes to predict the links. For existing links, the link predictor will assign small weights to links connecting nodes of dissimilar features while large weights to links connecting nodes of similar features, thus alleviating negative issue of noisy edges during message passing. Second , realworld graphs are usually very sparse, containing many missing edges. With the link predictor, nodes that are potentially to be linked could be iden tified. Densifying the graph by linking similar nodes would induce more unlabeled nodes to become neighbors of labeled nodes with the same labels as shown in Fig. 1, which can alleviate the label sparsity issue. In addition, since adjacent nodes tend to have the same labels, the predicted new links can be used to further regu larize the label predictions of unlabeled nodes. Though promising, the work on downweighting noisy edges and densifying graph for robust GNN on noisy graphs with sparse labels are rather limited. Therefore, in this paper, we investigate a novel problem of de veloping robust noiseresistant GNNs with limited labeled nodes by learning a denoised and densified graph. In essence, we need to solve two challenges: (i) how to effectively learn a link predictor from the noisy graph which can eliminate noisy edges and densify the graph; and (ii) how to simultaneously use the learned graph to learn a structural noiseresistant GNNs with limited labeled nodes. To address these challenges, we propose a novel framework named robust structural noiseresistant GNN (RSGNN)1. RSGNN adopts the node attributes and supervision from the noisy edges to denoise and dense graph, which can alleviate the negative effects of noisy edges and facilitate the message passing between unlabeled nodes and labeled nodes. The learned graph is used as input for learning a GNN. RSGNN also adopts the predicted edges to further explicitly regularize the predictions of unlabeled nodes to alleviate the label sparsity issue. In summary, our main contributions are: 1Codes are available at: https://github.com/EnyanDai/RSGNN‚Ä¢We study a new problem of learning robust noiseresistant GNNs with limited labeled nodes; ‚Ä¢We propose a novel framework RSGNN, which can simultane ously learn a denoised and densified graph and a robust GNN on noisy graphs with limited labeled nodes; and ‚Ä¢We conduct extensive experiments on realworld datasets to demonstrate the robustness of RSGNN on both noisy/clean graphs with limited labeled nodes. 2 RELATED WORK "
302,Fair Evaluation of Graph Markov Neural Networks.txt,"Graph Markov Neural Networks (GMNN) have recently been proposed to improve
regular graph neural networks (GNN) by including label dependencies into the
semi-supervised node classification task. GMNNs do this in a theoretically
principled way and use three kinds of information to predict labels. Just like
ordinary GNNs, they use the node features and the graph structure but they
moreover leverage information from the labels of neighboring nodes to improve
the accuracy of their predictions. In this paper, we introduce a new dataset
named WikiVitals which contains a graph of 48k mutually referred Wikipedia
articles classified into 32 categories and connected by 2.3M edges. Our aim is
to rigorously evaluate the contributions of three distinct sources of
information to the prediction accuracy of GMNN for this dataset: the content of
the articles, their connections with each other and the correlations among
their labels. For this purpose we adapt a method which was recently proposed
for performing fair comparisons of GNN performance using an appropriate
randomization over partitions and a clear separation of model selection and
model assessment.","Graph neural networks (GNN) (Yang et al., 2016; Kipf and Welling, 2017; Defferrard et al., 2016) have become a tool of choice when modeling datasets whose observations are not i.i.d. but are comprised of entities interconnected according to a graph of relations. They can be used either for graph classiÔ¨Åcation, like molecule classiÔ¨Åcation (Dobson and Doig, 2003; Borgwardt et al., 2005), or for node classiÔ¨Åcation, like document classiÔ¨Åca tion in a citation network (Sen et al., 2008). The most common task is certainly semi supervised node classiÔ¨Åcation in which unlabeled nodes of a given subset are to be classiÔ¨Åed using a distinct subset of labeled nodes, the train set, fromthe same graph (Kipf and Welling, 2017; Deffer rard et al., 2016). Inductive classiÔ¨Åcation on the other hand refers to the most common setting in machine learning in which nodes to be labeled are not known ahead of time (Hamilton et al., 2017). A number of architectures have been proposed over the years which deal with speciÔ¨Åc issues oc curring with GNNs. Some combat oversmoothing, (which is the tendency for deep GNNs to predict the same labels for all nodes) (Klicpera et al., 2018), some deal with assortativity or heterophily (which refers to situations in which neighboring nodes are likely to have different labels) (Zhu et al., 2020, 2021; Bo et al., 2021) and others still try to learn the connection weights from data using an appropri ate attention mechanism (Veli Àáckovi ¬¥c et al., 2018). Despite their diversity, these models all have one important shortcoming. Namely they assume that labels can be predicted independently for each node in the graph. In other words they neglect la bel dependencies altogether. More recently Graph Markov Neural Networks (GMNN) (Qu et al., 2019) were introduced as genuine probabilistic models which include label correlations in graphs by combining the strength of GNNs and those of conditional random Ô¨Åelds (CRF) while avoiding their limitations. These are the models we shall focus on in this work. The accuracy of the GMNN model was evalu ated for node classiÔ¨Åcation and link prediction tasks in (Qu et al., 2019) on the classical benchmark datasets Cora, Pubmed and Citeseer (Sen et al., 2008) using the public splits deÔ¨Åned in (Yang et al., 2016). Under these settings a clear improvement was demonstrated when comparing the GMNN model to existing baselines that do not account for label dependencies. However, as a number of recent works (Shchur et al., 2018; Errica et al., 2020) have pointed out, a fair evaluation of the performance of GNNs requires a procedure which performs a systematic randomization over trainarXiv:2304.01235v1  [cs.LG]  3 Apr 2023validationtest set partitions and makes clear sepa ration between model selection and model assess ment. Our aim in this paper is to subject GMNN to such a rigorous performance analysis on a new, rel atively large graph of documents named WikiVitals . In a Ô¨Årst step we shall rigorously evaluate the effect on the accuracy of a classiÔ¨Åer of leveraging the graph structure. This is done by comparing a basic GNN with a graph agnostic baseline such as a MLP. In a second step we shall estimate the increase in accuracy that results from taking into account label correlations using a GMNN on top of a basic GNN. For completeness we also perform the same thor ough analysis on the classical benchmark datasets mentioned above. In summary, our contributions1are: ‚Ä¢We introduce a new dataset of interconnected documents named WikiVitals . Compared to the classical benchmark datasets this is a rela tively large graph comprising 48k nodes clas siÔ¨Åed into 32 categories and connected by 2.3M edges. ‚Ä¢We apply the fair comparison procedure pro posed in (Errica et al., 2020) to a GMNN which is a sophisticated node classiÔ¨Åcation model. So far only graph classiÔ¨Åcation mod els had been evaluated in this manner. ‚Ä¢We evaluate the respective contributions to the accuracy of classifying WikiVitals articles when Ô¨Årst including the graph structure infor mation using a common GNN and next when leveraging the label correlations information using a GMNN model on top of that GNN. 2 Related Work "
303,Amended Cross Entropy Cost: Framework For Explicit Diversity Encouragement.txt,"Cross Entropy (CE) has an important role in machine learning and, in
particular, in neural networks. It is commonly used in neural networks as the
cost between the known distribution of the label and the Softmax/Sigmoid
output. In this paper we present a new cost function called the Amended Cross
Entropy (ACE). Its novelty lies in its affording the capability to train
multiple classifiers while explicitly controlling the diversity between them.
We derived the new cost by mathematical analysis and ""reverse engineering"" of
the way we wish the gradients to behave, and produced a tailor-made, elegant
and intuitive cost function to achieve the desired result. This process is
similar to the way that CE cost is picked as a cost function for the
Softmax/Sigmoid classifiers for obtaining linear derivatives. By choosing the
optimal diversity factor we produce an ensemble which yields better results
than the vanilla one. We demonstrate two potential usages of this outcome, and
present empirical results. Our method works for classification problems
analogously to Negative Correlation Learning (NCL) for regression problems.","It has been shown in several studies, both theoretically and empirically, that training an ensemble of models, i.e. aggregating predictions from multiple models, is superior to training a single model[ 1‚Äì 10]. Many works point out that one of the keys for an ensemble to perform well is to encourage diversity among the models [5, 10‚Äì15]. This property is the main motivation our work. Sigmoid and Softmax are both well known functions which are used for classiÔ¨Åcation (the former for binary and the second for multi label classiÔ¨Åcations). Both are used to generate distribution vectors qY(x) =fq1(x);::;qL(x)gover the labels Y=f1;::;Lg, wherexis a given input. For Deep Neural Networks (DNNs) the framework of applying a Sigmoid/Softmax on top of the network is very popular, where the goal is to estimate the real distribution pY(x) =fp1(x);::;pL(x)g, which might be a 1hot vector for a hard label. Henceforth, we omit xunless it is crucial for some deÔ¨Ånition or proof. We denote p=pY(x);q=qY(x). We optimize qby minimizing the CE cost function H(p;q) =Ep[ logq] = LX i=1pilogqi: (1) The optimization is usually gradient based[ 16,17]. Hence, one of the main motivations for using the CE cost function over Sigmoid/Softmax outputs is the linear structure of the gradient, which is similar to that obtained by applying the Mean Squared Error (MSE) method over a linear regression estimator. Studies show that this property is important for preventing vanishing gradient phenomena [18, 19]. Preprint. Under review.arXiv:2007.08140v1  [cs.LG]  16 Jul 2020Now let us deÔ¨Åne the setting of the ensemble problem. We train KclassiÔ¨Åers, with distribution functionsq1;::;qK, to generate ensemble q=1 KPK k=1qk, which estimates the real distribution p. This setting is very common and the straightforward way to tackle it is by training each model independently using the CE cost function H(p;qk). Encouraging diversity is manifested by using different training samples or different seeds for weight initialization. However, to the best of our knowledge, there is no explicit way to control the ‚Äúamount‚Äù of diversity between the classiÔ¨Åers. In this work we present a novel framework, called Amended Cross Entropy (ACE), which makes it possible for us to train each model and, simultaneously, to achieve diversity between the classiÔ¨Åers. Our main result in this work is the introduction of a new cost function H(p;qk)  K 1X j6=kH(qj;qk); (2) which is applied for the kth classiÔ¨Åer and is not independent of the other classiÔ¨Åers. We see that ACE is built from the vanilla CE between pandqk, minus the average of the CE between qkand the other estimators, factored with  . This result is very intuitive since we wish to minimize the CE of the estimated distribution with the real one, while enlarging the CE of the estimator with the others, i.e. encourage diversity. The hyperparameter  2[0;K 1 K]explicitly controls the diversity, and is Ô¨Ånetuned in order to achieve optimal results. The development of ACE starts from an assumption of the structure we wish the gradient to be in. As we show in this paper, a similar assumption lies at the base of applying CE over Softmax. We develop a variant especially for DNNs, which can be stacked on top of the network instead of the vanilla Softmax layer, and makes it possible to yield superior results without signiÔ¨Åcantly increasing the number of parameters or the computational resources. This work has been inspired by the Negative Correlation Learning (NCL) [ 1,5,14] framework, which is used for regression ensembles. In the next section we will present the NCL framework, its development and its results, in order to explain the analogous approach we used in our work. 2 Related work: Negative Correlation Learning (NCL) "
304,Spatial-context-aware deep neural network for multi-class image classification.txt,"Multi-label image classification is a fundamental but challenging task in
computer vision. Over the past few decades, solutions exploring relationships
between semantic labels have made great progress. However, the underlying
spatial-contextual information of labels is under-exploited. To tackle this
problem, a spatial-context-aware deep neural network is proposed to predict
labels taking into account both semantic and spatial information. This proposed
framework is evaluated on Microsoft COCO and PASCAL VOC, two widely used
benchmark datasets for image multi-labelling. The results show that the
proposed approach is superior to the state-of-the-art solutions on dealing with
the multi-label image classification problem.","With the rapid development of technologies, abundant visual information is constantly received. One of the fundamental but challenging problems for image understanding is to label the objects, locations or attributes in the images, possibly with more than one label. Multilabel image classiÔ¨Åcation problem has attracted the attention of researchers in the past few years. However, the rich semantic information and higherorder la bel cooccurrence are challenging to model [1, 2]. Recently, many deep convolutional neural networks (DC NNs) were developed for singlelabel image classiÔ¨Åcation problem [3‚Äì5], and transforming the multilabel classiÔ¨Åca tion problem into multiple binary classiÔ¨Åcation tasks is one of the common strategies [6] to solve the multilabel image classiÔ¨Åcation problem. However, this kind of method ignores the interdependencies among labels, which have been proved useful [7, 8]. To tackle this problem, researchers developed various DCNNs [7, 9‚Äì14] that can consider all the labels for a given image concurrently. For example, Wang et al. designed a sequentially predict model and used a recurrent This work was supported in part by the National Natural Science Foun dation of China under Grant 72071116, and in part by the Ningbo Municipal Bureau of Science and Technology under Grant 2019B10026. Fig. 1 . An illustrative example. Previous methods cannot detect the tennis racket. By exploiting the label dependencies, spatial and context information, the proposed approach could more accurately detect it. neural network (RNN) to determine label dependencies [7], while Chen et al. and Wang et al. employed graph convolu tional networks to capture global label dependencies [13, 15]. By exploiting labelcorrelation information, these approaches made great progress on image multilabelling. Nevertheless, object spatial information and image context information are not fully exploited in these approaches. Wei et al. [16] and Yang et al. [17] addressed this problem by devising a 2stage pipeline for multilabelling in which the model generates image patches Ô¨Årst and then labels them. However, these methods overemphasize the generated patches, thereby neglecting surrounding context infomation and label dependencies. The idea of object local ization is similar to the attention mechanism that has been successfully applied in many vision tasks [10, 11, 18‚Äì20]. Fig. 1 illustrates the importance of label dependencies, spa tial and context information. Additionally, context has been demonstrated useful in various visual processing tasks, such as recognition and detection [21‚Äì23]. To make good use of these dependencies and information, a twobranch spatialcontextaware DCNN is designed, where one branch is designed to extract the spatial information of the objects and the other aims at capturing the image con text information. The network‚Äôs label predictor follows thearXiv:2111.12296v2  [cs.CV]  20 Feb 2022principle of multilabel image classiÔ¨Åcation and utilizes the dependencies among labels. Moreover, with more contextual information, the proposed model performs well in labelling small objects that other models may not be able to capture. Different from existing spatialcontextaware models ex ploiting the context information between objects [24] or con sidering the receptive Ô¨Åeld of the shallow layers as the context information to the pixels on the deep layers of the network [25], the proposed method exploits the spatial context infor mation directly on the feature maps and utilizes the feature maps around the object as the background context. The exper imental results on two large benchmark datasets, MSCOCO and PASCAL VOC, demonstrate the effectiveness of the pro posed approach compared with stateoftheart approaches. The contributions of this paper are summarized as fol low: 1) To make use of the spatial and context information to the object, a twobranch spatialcontextaware deep neu ral network is proposed for multilabel image classiÔ¨Åcation problem. 2) The proposed imagecontextaware branch could well exploit both spatial and semantic information of objects. 3) The proposed approach signiÔ¨Åcantly outperforms the state oftheart approaches [5, 7, 10, 11, 13‚Äì15, 18, 19, 26] on the MSCOCO dataset [27] and PASCAL VOC [28] dataset. 2. METHODOLOGY "
305,Deep learning for class-generic object detection.txt,"We investigate the use of deep neural networks for the novel task of class
generic object detection. We show that neural networks originally designed for
image recognition can be trained to detect objects within images, regardless of
their class, including objects for which no bounding box labels have been
provided. In addition, we show that bounding box labels yield a 1% performance
increase on the ImageNet recognition challenge.","The task of separating objects from background is funda mental for many computer vision tasks. This has led to much research on localizing and classifying objects by us ing object segmentation, object detection, and region pro posals. Currently, most detectors are trained individually for each object class, which requires a class label and a bounding box for all images. Unfortunately, in this ap proach it is difÔ¨Åcult to transfer information from previously trained detectors to novel classes where bounding box la bels may not be available. This situation is common in current datasets, which often have many class labels but incomplete bounding box labels. In this work, we aim to overcome these challenges by training separately from bounding box labels and class labels, enabling our system to learn even when only one of these labels is available. This approach harnesses the notion of objectness (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013) to build a deep neural network (Krizhevsky et al., 2012) able to detect novel objects where bounding box labels have not been provided. One successful approach to object detection is to train a sin gle detector for each class of objects (for example, the De formable Parts Model (DPM) (Felzenszwalb et al., 2010)). In this approach, one discriminatively trains a set of detec Proceedings of the 31stInternational Conference on Machine Learning , Beijing, China, 2014. JMLR: W&CP volume 32. Copy right 2014 by the author(s).tors on each individual class. This strategy generally has proven useful on the Pascal VOC detection challenge due to the limited number of classes, each of which includes many bounding box labels. In other cases, however, where we may have an abundance of class labels, but few or no bounding box labels, it is not clear how to apply this same strategy. For example, the ImageNet dataset has 14 million class labels but only about 7%are labeled with bounding boxes (Deng et al., 2009). Recently, region proposal algorithms have shown good performance in object detection pipelines by proposing classgeneric locations for further classiÔ¨Åcation (Endres & Hoiem, 2010; Alexe et al., 2012; Girshick et al., 2013; Ui jlings et al., 2013). They attempt to measure objectness within an image by training on all bounding boxes labels, regardless of class, in hopes of building a single detector for all classes. While training from only bounding box labels potentially enables a detector to locate novel classes never seen be fore, it may perform poorly due to having too few train ing examples and failing to exploit the wealth of class la bels available in datasets like ImageNet. We propose to train a detector to localize objects while also exploiting ob ject class labels by separating the recognition and detec tion problems. We show that by pretraining our detector on class labels and then on object locations, we can increase its performance in detecting previously seen objects, while nearly retaining its ability to localize objects for which we have no bounding box labels. 2. Related works "
306,What Do Neural Networks Learn When Trained With Random Labels?.txt,"We study deep neural networks (DNNs) trained on natural image data with
entirely random labels. Despite its popularity in the literature, where it is
often used to study memorization, generalization, and other phenomena, little
is known about what DNNs learn in this setting. In this paper, we show
analytically for convolutional and fully connected networks that an alignment
between the principal components of network parameters and data takes place
when training with random labels. We study this alignment effect by
investigating neural networks pre-trained on randomly labelled image data and
subsequently fine-tuned on disjoint datasets with random or real labels. We
show how this alignment produces a positive transfer: networks pre-trained with
random labels train faster downstream compared to training from scratch even
after accounting for simple effects, such as weight scaling. We analyze how
competing effects, such as specialization at later layers, may hide the
positive transfer. These effects are studied in several network architectures,
including VGG16 and ResNet18, on CIFAR10 and ImageNet.","Overparameterization helps deep neural networks (DNNs) to generalize better in reallife appli cations [ 8,24,30,54], despite providing them with the capacity to Ô¨Åt almost any set of random labels [ 55]. This phenomenon has spawned a growing body of work that aims at identifying funda mental differences between real and random labels, such as in training time [ 4,19,20,56], sharpness of the minima [ 28,40], dimensionality of layer embeddings [ 3,11,35], and sensitivity [ 4,41], among other complexity measures [ 6,7,39,40]. While it is obvious that overparameterization helps DNNs to interpolate any set of random labels, it is not immediately clear what DNNs learn when trained in this setting. The objective of this study is to provide a partial answer to this question. There are at least two reasons why answering this question is of value. First, in order to understand how DNNs work, it is imperative to observe how they behave under ‚Äúextreme‚Äù conditions, such as ?Equal contribution. yWork completed during the Google AI Residency Program. Preprint. Accepted, NeurIPS 2020.arXiv:2006.10455v2  [stat.ML]  11 Nov 20201. Pretraining helps (real labels)2. Pretraining helps (random labels)3. Pretraining hurts (real labels)4. Pretraining hurts (random labels) 50 1500 3000 Training Iterations0.00.20.40.60.81.0Accuracypretrained (train) pretrained (test) from scratch (train) from scratch (test) 50 10000 20000 Training Iterations0.00.20.40.60.81.0Accuracy pretrained (train) from scratch (train) 50 1500 3000 Training Iterations0.00.20.40.60.81.0Accuracypretrained (train) pretrained (test) from scratch (train) from scratch (test) 50 10000 20000 Training Iterations0.00.20.40.60.81.0Accuracy pretrained (train) from scratch (train) Figure 1: Pretraining on random labels may exhibit both positive ( 1 & 2 ) and negative ( 3 & 4 ) effects on the downstream Ô¨Ånetuning depending on the setup. VGG16 models are pretrained on CIFAR10 examples with random labels and subsequently Ô¨Ånetuned on the fresh CIFAR10 examples with either real labels (1 & 3) or 10 random labels (2 & 4) using different hyperparameters. when trained with labels that are entirely random. Since the pioneering work of [ 55], several works have looked into the case of random labels. What distinguishes our work from others is that previous works aimed to demonstrate differences between real and random labels, highlighting the negative side of training on random labels. By contrast, this work provides insights into what properties of the data distribution DNNs learn when trained on random labels. Second, observing DNNs trained on random labels can explain phenomena that have been previously noted, but were poorly understood. In particular, by studying what is learned on random labels, we offer new insights into: (1) why DNNs exhibit critical stages [ 1,17], (2) how earlier layers in DNNs generalize while later layers specialize [ 3,4,10,53], (3) why the Ô¨Ålters learned by DNNs in the Ô¨Årst layer seem to encode some useful structure when trained on random labels [ 4], and (4) why pretraining on random labels can accelerate training in downstream tasks [ 42]. We show that even when controlling for simple explanations like weight scaling (which was not always accounted for previously), such curious observations continue to hold. The main contributions of this work are: ‚Ä¢We investigate DNNs trained with random labels and Ô¨Ånetuned on disjoint image data with real or random labels, demonstrating unexpected positive and negative effects. ‚Ä¢We provide explanations of the observed effects. We show analytically for convolutional and fully connected networks that an alignment between the principal components of the network parameters and the data takes place. We demonstrate experimentally how this effect explains why pretraining on random labels helps. We also show why, under certain conditions, pretraining on random labels can hurt the downstream task due to specialization at the later layers. ‚Ä¢We conduct experiments verifying that these effects are present in several network architectures, including VGG16 [ 46] and ResNet18v2 [ 22], on CIFAR10 [ 31] and ImageNet ILSVRC2012 [ 14], across a range of hyperparameters, such as the learning rate, initialization, number of training iterations, width and depth. In this work, we do not use data augmentation as it provides a (weak) supervisory signal. Moreover, we use the terms ‚Äúpositive‚Äù and ‚Äúnegative‚Äù to describe the impact of what is learned with random labels on the downstream training, such as faster/slower training. The networks reported throughout the paper are taken from a big set of experiments that we conducted using popular network architectures, datasets, and wide hyperparameter ranges. Experimental details are provided in Appendix A and B. We use boldface for random variables, small letters for their values, and capital letters for matrices. 1.1 Motivating example Figure 1 shows learning curves of the VGG16 architecture [ 46] pretrained on 20k CIFAR10 exam ples [ 31] with random labels (upstream) and Ô¨Ånetuned on a disjoint subset of 25k CIFAR10 examples with either random or real labels (downstream). We observe that in this setup, pretraining a neural network on images with random labels accelerates training on a second set of images, both for real and random labels (positive effect). However, in the same setting but with a different initialization scale and number of random classes upstream, a negative effect can be observed downstream: training 2becomes slower. We also observe a lower Ô¨Ånal test accuracy for real labels in both cases, which we are not explicitly investigating in this paper (and which has been observed before, e.g. in [17]). The fact that pretraining on random labels can accelerate training downstream has been observed previously, e.g. in [ 42]. However, there is a ‚Äúsimple‚Äù property that can explain improvements in the downstream task: Because the crossentropy loss is scalesensitive, training the network tends to increase the scale of the weights [ 40], which can increase the effective learning rate of the downstream task (see the gray curve in Figure 5). To eliminate this effect, in all experiments we rescale the weights of the network after pretraining to match their `2norms at initialization. We show that even after this correction, pretraining on random labels positively affects the downstream task. This holds for both VGG16 and ResNet18 trained on CIFAR10 and ImageNet (see Appendix B). We show experimentally that some of the positive transfer is due to the secondorder statistics of the network parameters. We prove that when trained on random labels, the principal components of weights at the Ô¨Årst layer are aligned with the principal components of data. Interestingly, this alignment effect implies that the model parameters learned at the Ô¨Årst layer can be summarized by a onedimensional mapping between the eigenvalues of the data and the eigenvalues of the network parameters. We study these mappings empirically and raise some new open questions. We also analyze how, under certain conditions, a competing effect of specialization at the later layers may hide the positive transfer of pretraining on random labels, which we show to be responsible for the negative effect demonstrated in Figure 1. To the best of our knowledge, the alignment effect has not been established in the literature before. This paper proves the existence of this effect and studies its implications. Note that while these effects are established for training on random labels, we also observe them empirically for real labels. 1.2 Related work "
307,Influential Rank: A New Perspective of Post-training for Robust Model against Noisy Labels.txt,"Deep neural network can easily overfit to even noisy labels due to its high
capacity, which degrades the generalization performance of a model. To overcome
this issue, we propose a new approach for learning from noisy labels (LNL) via
post-training, which can significantly improve the generalization performance
of any pre-trained model on noisy label data. To this end, we rather exploit
the overfitting property of a trained model to identify mislabeled samples.
Specifically, our post-training approach gradually removes samples with high
influence on the decision boundary and refines the decision boundary to improve
generalization performance. Our post-training approach creates great synergies
when combined with the existing LNL methods. Experimental results on various
real-world and synthetic benchmark datasets demonstrate the validity of our
approach in diverse realistic scenarios.","The current deep learning has made a huge breakthrough because of ‚Äòdata‚Äô. Thus, many researchers in both academia and industry endeavor to obtain considerable data. How ever, realworld data inevitably contain some proportion of incorrectly labeled data, owing to perceptual ambiguity, or errors from human or machine annotations. These noisy la bels negatively affect the generalization performance of a trained model since a deep neural network (DNN) can eas ily overÔ¨Åt to even noisy labels due to its high capacity [52]. Therefore, learning from noisy labels (LNL) has received much attention in recent years [13, 49, 38, 57, 28, 5, 16] due to the increasing need to handle noisy labels in practice. To handle noisylabel problem, prior literature aims to distinguish between clean and mislabeled data, and use this information to train a robust classiÔ¨Åer during training. To this end, prior works mainly rely on the assumption that the clean labels are more likely to have smaller losses before the model is overÔ¨Åtted [2]. However, due to the high capacity of deep neural networks (DNNs), DNNs can Ô¨Åt even noisy labels [52]; thus it is challenging to correctly detect mis labeled data during training. Hence, various methods have been proposed to use more robust models before overÔ¨Åtting, CEVolminNet CoteachingELR+DivideMix UNICON6065707580859095AccuracyOriginal w/ RoG w/ OursFigure 1. Test accuracy improvement over various methods on CIFAR10N (Worst). As a posttraining method, our pro posed InÔ¨Çuential Rank can improve various pretrained models by large margin, compared to the postprocessing baseline method, RoG [23]. The used CIFAR10N (Worst) is a humanannotated realworld noisy dataset with about 40% noise rate [46]. such as leveraging the model with early stopping [39, 26], or using multiple networks with cotraining for sample se lection [11, 51, 24]. Here, we introduce a different perspective against the mainstream research. We propose a new posttraining LNL approach, which can synergize with the model trained using prior robust methods, further enhancing the generalization capability of the model. Given a pretrained model, the pro posed posttraining scheme reÔ¨Ånes the model by exploiting the ‚ÄòoverÔ¨Åtting property‚Äô of mislabeled samples. ‚ÄòOverÔ¨Åt ting property‚Äô of mislabeled samples is derived from two following intuitions. (1) Mislabeled samples are more likely to distort the decision boundary than clean samples. Thus removing the mislabeled samples is likely to sway the de cision boundary signiÔ¨Åcantly. (2) The overÔ¨Åtted model pre dicts poorly on unseen data, and the mislabeled sample is usually the main culprit for the model to classify new data with incorrect labels. The details on these intuitions are dis cussed in Section 3.1. These intuitions on overÔ¨Åtting motivate us to propose a novel method named InÔ¨Çuential Rank , which leverages the samples‚Äô inÔ¨Çuence on the decision boundary and on un seen samples to enhance robustness. To this end, we pro pose overÔ¨Åtting score on model (OSM) and overÔ¨Åtting score on data (OSD). OSM measures the inÔ¨Çuence of a trainingarXiv:2106.07217v4  [cs.CV]  19 Apr 2023sample on changes in model parameters, and OSD measures the inconsistency of the sample‚Äôs inÔ¨Çuence on the classi Ô¨Åcation prediction for a small number of clean validation data. Based on OSM and OSD, InÔ¨Çuential Rank updates the trained model by removing high inÔ¨Çuential samples and mitigating their negative inÔ¨Çuence on the classiÔ¨Åer. Since the posttraining provides a new information ( i.e., sample‚Äôs inÔ¨Çuence) to any pretrained models, InÔ¨Çuen tial Rank can effectively improve robustness of existing LNL methods. Through extensive experiments on multiple benchmark data sets, we demonstrate the validity of our method, and show that InÔ¨Çuential Rank can improve the performance of the model consistently whether or not it is pretrained with LNL methods, as shown in Figure 1. Fur thermore, we show that InÔ¨Çuential Rank is useful in two different applications other than LNL. The proposed over Ô¨Åtting scores can be effective for (1) data cleansing that Ô¨Ål ters out erroneous examples in realworld video data and (2)regularization that boosts the classiÔ¨Åcation performance on clean data. Our key contributions can be summarized as follows: ‚Ä¢Posttraining : InÔ¨Çuential Rank is a novel posttraining approach for LNL, which leverages the overÔ¨Åtting scores of training examples on the decision boundary. ‚Ä¢Practicability : InÔ¨Çuential Rank is applicable to any pre trained models and works synergistically with other ex isting LNL methods. ‚Ä¢Extensibility : InÔ¨Çuential Rank can be easily extended to cleansing video dataset and a regularization for reducing overÔ¨Åtting arising from clean but inÔ¨Çuential samples. 2. Related Work "
308,Alternating Loss Correction for Preterm-Birth Prediction from EHR Data with Noisy Labels.txt,"In this paper we are interested in the prediction of preterm birth based on
diagnosis codes from longitudinal EHR. We formulate the prediction problem as a
supervised classification with noisy labels. Our base classifier is a Recurrent
Neural Network with an attention mechanism. We assume the availability of a
data subset with both noisy and clean labels. For the cohort definition, most
of the diagnosis codes on mothers' records related to pregnancy are ambiguous
for the definition of full-term and preterm classes. On the other hand,
diagnosis codes on babies' records provide fine-grained information on
prematurity. Due to data de-identification, the links between mothers and
babies are not available. We developed a heuristic based on admission and
discharge times to match babies to their mothers and hence enrich mothers'
records with additional information on delivery status. The obtained additional
dataset from the matching heuristic has noisy labels and was used to leverage
the training of the deep learning model. We propose an Alternating Loss
Correction (ALC) method to train deep models with both clean and noisy labels.
First, the label corruption matrix is estimated using the data subset with both
noisy and clean labels. Then it is used in the model as a dense output layer to
correct for the label noise. The network is alternately trained on epochs with
the clean dataset with a simple cross-entropy loss and on next epoch with the
noisy dataset and a loss corrected with the estimated corruption matrix. The
experiments for the prediction of preterm birth at 90 days before delivery
showed an improvement in performance compared with baseline and state
of-the-art methods.","The digitization of hospitals, by the adoption of Electroni c Health Record systems, promises to revo lutionize the future of healthcare. Several countries have achieved nearly 100% adoption. Therefore the complexity and size of EHR data is drastically increasin g. This is creating new challenges and opportunities to the research community of machine learnin g for healthcare. In this paper we con sider the clinical application of predicting preterm birth from EHR based on deep learning models. Between 10% to 15% of babies are born before 37 weeks of gestat ion [1]. Preterm birth is the leading cause of mortality and longterm disabilities in ne onates. It is also an important cause of developmental retardation. The cost of preterm deliveries and care exceed 26 billion dollars in the Machine Learning for Health (ML4H) Workshop at NeurIPS 2018 .US [2]. The goal of our application is to predict in advance th e risk for a preterm delivery [3, 4]. Developing such predictive model can be of high value for obs tetricians. The availability of large clinical EHR data should help in building accurate and inter pretable models. 2 Related Work "
309,Unsupervised Domain Adaptation with Random Walks on Target Labelings.txt,"Unsupervised Domain Adaptation (DA) is used to automatize the task of
labeling data: an unlabeled dataset (target) is annotated using a labeled
dataset (source) from a related domain. We cast domain adaptation as the
problem of finding stable labels for target examples. A new definition of label
stability is proposed, motivated by a generalization error bound for large
margin linear classifiers: a target labeling is stable when, with high
probability, a classifier trained on a random subsample of the target with that
labeling yields the same labeling. We find stable labelings using a random walk
on a directed graph with transition probabilities based on labeling stability.
The majority vote of those labelings visited by the walk yields a stable label
for each target example. The resulting domain adaptation algorithm is
strikingly easy to implement and apply: It does not rely on data
transformations, which are in general computational prohibitive in the presence
of many input features, and does not need to access the source data, which is
advantageous when data sharing is restricted. By acting on the original feature
space, our method is able to take full advantage of deep features from external
pre-trained neural networks, as demonstrated by the results of our experiments.","Unsupervised domain adaptation (DA) addresses the prob lem of building a good predictor for a target domain using labeled training data from a related source domain and tar get unlabeled training data. A typical example in visual ob ject recognition involves two different datasets consisti ng of images taken under different cameras or conditions: for in stance, one dataset consists of images taken at home with a digital camera while another dataset contains images taken in a controlled environment with studio lightning conditions . In some cases, the source domain is related to the target one, but predictive features for the target domain may not even be present in the source domain as illustrated in the toy example in the Ô¨Ågure. For instance this phenomenon can hap pen in natural language processing, where different genreshs (a) Source domainht (b) Target domain Figure 1: A simple dataset for DA, the vertical dimension is r elevant for the target domain, but not for the source. often use very different vocabulary to described similar co n cepts. Here the target domain is rotated 45deg compared to the source domain. A linear classiÔ¨Åer hsfor the source do main will have an accuracy of only around 84% on the target domain. If we perform feature selection on the source data, then we lose a feature that is relevant to the target domain and we will not be able to improve the accuracy. However, the two classes are well separated in the target domain, and i t should be possible to Ô¨Ånd a largemargin classiÔ¨Åer separati ng the classes. Just trying to separate the classes in the target domain is no t enough, mainly because this does not tell us which class is which, since no labeled target data are available. For that w e need to use the relation to the source domain. More generally , there is a tradeoff between having a classiÔ¨Åer that separat es the classes in the target domain, and a classiÔ¨Åer that stays close to the knowledge from the source domain. We propose to model such tradeoff by casting domain adaptation as the problem of Ô¨Ånding a ‚Äòstable‚Äô label for each target example. We introduce the notion of labeling stability, motivated by a generalization bound for linear large margin classiÔ¨Åers: a target labeling is stable when, with high expectation, a tar get hypothesis trained on a random subsample of the target data with that labeling yields the same labeling. To Ô¨Ånd stable ta r get labelings we use a formalization based on random walks. We deÔ¨Åne a Markov chain with states equal to labelings from large margin linear classiÔ¨Åers and onestep transition pro ba bilities deÔ¨Åned using the proposed notion of labeling stabi lity. Then we perform a random walk starting at the labeling obtained from the source hypothesis. The walk will be attracte d toward more stable labelings, which will be visited more of ten. The majority vote of the labelings visited by the walk provides our Ô¨Ånal estimated label for each target example. We call the resulting unsupervised adaptation algorithm RWA ( Random Walk based Adaptation). RWA is strikingly simple to implement and apply. It does not rely on data trans formations, which are in general computational prohibitiv e in the presence of many input features. RWA does not need to access the source data. It acts on the original feature spa ce, hence can take full advantage of the use of deep features from external pretrained deep neural networks, as demonstrate d by the results of our experiments. Results of extensive expe r iments on sentiment analysis and image object recognition show stateoftheart performance of RWA across adaptatio n datasets with diverse nature and characteristics. Notably , us ing deep learning features from pretrained deep neural net  works RWA outperforms much more involved endtoend DA methods based on deep learning. Our contributions can be summarized as follows: (1) a new deÔ¨Ånition of stability of a target labeling inspired by a gen er alization bound for linear large margin classiÔ¨Åers; (2) a ne w representation of the DA problem based on random walks; (3) a strikingly simple method for unsupervised DA; (4) a direct and effective way to exploit deep features from pre trained deep neural networks for visual adaptation tasks; ( 5) new stateoftheart results on hard adaptation tasks with im age as well as text data. 2 Related work "
310,Image recognition from raw labels collected without annotators.txt,"Image classification problems are typically addressed by first collecting
examples with candidate labels, second cleaning the candidate labels manually,
and third training a deep neural network on the clean examples. The manual
labeling step is often the most expensive one as it requires workers to label
millions of images. In this paper we propose to work without any explicitly
labeled data by i) directly training the deep neural network on the noisy
candidate labels, and ii) early stopping the training to avoid overfitting.
With this procedure we exploit an intriguing property of standard
overparameterized convolutional neural networks trained with (stochastic)
gradient descent: Clean labels are fitted faster than noisy ones. We consider
two classification problems, a subset of ImageNet and CIFAR-10. For both, we
construct large candidate datasets without any explicit human annotations, that
only contain 10%-50% correctly labeled examples per class. We show that
training on the candidate examples and regularizing through early stopping
gives higher test performance for both problems than when training on the
original, clean data. This is possible because the candidate datasets contain a
huge number of clean examples, and, as we show in this paper, the noise
generated through the label collection process is not nearly as adversarial for
learning as the noise generated by randomly flipping labels.","Much of the recent success in building machine learning systems for image classication can be attributed to training deep neural networks on large, humanly annotated datasets, such as Ima geNet [Den+09] or CIFAR10 [TFF08]. However, assembling such datasets is timeconsuming and expensive: Both ImageNet and CIFAR10 were constructed by rst searching the web for candidate images and second labeling the candidate images by workers to obtain clean labels. The rst step is relatively inexpensive and already yields labeled examples, but the accuracy of those automatically collected, henceforth called candidate labels is low: For example, depending on each class, only about 10%50% of the candidate labels we automatically collected from Flickr (as described later) are correct, and only about half of the candidate labels obtained in the process of constructing the CIFAR10 dataset are correct [TFF08]. The second step in constructing such a dataset is expen sive because it requires either employing expert labelers or asking workers through a crowdsourcing platform for several annotations per image, and aggregating them to a clean label. 1arXiv:1910.09055v3  [cs.LG]  25 Feb 20201.1 Contributions In this paper we propose to train directly on the noisy candidate examples, eectively skipping the expensive human labeling step. To make this work, we exploit an intriguing property of large, overparameterized neural networks: If trained with stochastic gradient descent or variants thereof, neural networks t clean labels signicantly faster than noisy ones. This fact is well known, see for example the experiments by Zhang et al. [Zha+17, Fig. 1a]. What is notwell known is that this eect is suciently strong to enable training on candidate labels only. Our idea is that, if neural networks t clean labels faster than noise, then training them on a set containing clean and wrong labels and early stopping the training resembles training on the clean labels only. Our main nding is that early stopping the training on candidate examples can enable better image classication performance than when trained on clean labels, provided the total number of clean labels in the candidate training set is suciently large. We show this on CIFAR10 and ImageNet, two of the most widelyused image classication benchmarks [Ham17], with candidate training sets that we constructed without any human labeling step. This result questions the expensive practice of building clean humanly labeled training sets, and suggests that it can be better to collect larger, noisier datasets instead. Better ImageNet performance by learning from candidates: We demonstrate that we can achieve stateoftheart ImageNet classication performance on a subset of the original classes, without learning from the original clean labels. Specically, we chose 100 classes for which there are suciently many Flickr search results and for which there are no signicant semantic overlaps between the keywords and/or the search results. For those classes, we constructed a new candidate training set by collecting images from Flickr by keyword search, using the keywords of the ImageNet classes (from the WordNet hierarchy [Mil95]). We choose Flickr to collect candidate images because it enables image search unaltered by learning algorithms, unlike what a search engine like Google yields. Our results show that the performance of ResNet50 [He+16], a standard ImageNet benchmark network, improves signicantly by training and early stopping on the noisy candidate examples, compared to training on the original, high quality ImageNet training set. Specically, for the 100 class problem ResNet50 trained on the original training set achieves a top1 classication error of 15% whereas on our candidate training set, with early stopping, ResNet50 achieves a better error rate of 10.56%. Better CIFAR10 performance by learning from candidates: On CIFAR10, we achieve higher classication performance by training on candidate labels than any standard model achieves when trained on the original training set. The goal of the CIFAR10 classication problem is to classify 32x32 color images in 10 classes. The clean training set consists of 5000 images per class and was obtained by labeling the images from the Tiny Images dataset with expert workers. We constructed a new noisy training set consisting of candidate examples only, by picking the images from the Tiny Images dataset with the labels of the CIFAR10 classes, followed by cleaning to prevent any trivial (same images) or nontrivial (similar images) overlaps with the test set. Only about half of the examples in this candidate dataset are correctly labeled. We trained the best performing networks for CIFAR10, (ResNet [He+16], ShakeShake [Gas17], VGG [SZ15], DenseNet [Hua+17], PyramidNet [HKK17] and PNASNet [Liu+18]), and each model 2achieved signicantly higher performance with early stopping than when trained on the original, clean examples. In numbers, the best performing model trained on the CIFAR10 training set has 7% classi cation error on the CIFAR10.1 test set [Rec+19], while the best performing model trained on our candidate training set achieves 5.85%, with early stopping training of the PyramidNet110 model, lower than any standard model achieves when trained on the original training set. Early stopping is critical: Early stopping is critical to achieve the best performance. By keeping track of the performance of a large clean subset of the noisy training set, we show that the clean labels are tted signicantly faster than the false labels, and a good point to stop is when most of the clean labels are tted well, because at this point most of the false labels have not been tted yet. Limitations of learning from candidate labels: Whether our approach works depends on whether there is signicant overlap of the candidate labels; we demonstrate this by showing that when training on ImageNet classes that have a signicant semantic overlap, without labeling, yields obviously to high prediction errors for such classes. Real label noise is far from adversarial: Finally, we study the dierence between \real"" noise in the data obtained through the data collection process (from sources such as search engines and Flickr) and articially generated noise through randomly  ipping the labels of a clean dataset, which is often used in the literature as a proxy for the former. We nd that \real"" noise is more structured and therefore is easier to t and less harmful to the classication performance in contrast to articially generated noise, which is harder to t and more harmful. These ndings are consistent with those from the recent paper [JHY19], which quanties dierences of synthetic and real noise. 1.2 Related work "
311,Semi-Supervised Learning for Sparsely-Labeled Sequential Data: Application to Healthcare Video Processing.txt,"Labeled data is a critical resource for training and evaluating machine
learning models. However, many real-life datasets are only partially labeled.
We propose a semi-supervised machine learning training strategy to improve
event detection performance on sequential data, such as video recordings, when
only sparse labels are available, such as event start times without their
corresponding end times. Our method uses noisy guesses of the events' end times
to train event detection models. Depending on how conservative these guesses
are, mislabeled samples may be introduced into the training set. We further
propose a mathematical model for explaining and estimating the evolution of the
classification performance for increasingly noisier end time estimates. We show
that neural networks can improve their detection performance by leveraging more
training data with less conservative approximations despite the higher
proportion of incorrect labels. We adapt sequential versions of CIFAR-10 and
MNIST, and use the Berkeley MHAD and HMBD51 video datasets to empirically
evaluate our method, and find that our risk-tolerant strategy outperforms
conservative estimates by 3.5 points of mean average precision for CIFAR, 30
points for MNIST, 3 points for MHAD, and 14 points for HMBD51. Then, we
leverage the proposed training strategy to tackle a real-life application:
processing continuous video recordings of epilepsy patients, and show that our
method outperforms baseline labeling methods by 17 points of average precision,
and reaches a classification performance similar to that of fully supervised
models. We share part of the code for this article.","Labeled image and video datasets are crucial for training and evaluating machine learning models. As a result, com puter vision researchers have compiled a number of labeled benchmark datasets, such as MNIST [20], ImageNet [8], MSCOCO [22], Kinetics [17], CIFAR [18], and Cityscapes [7]. However, many application areas still remain poorly covered, such as medical imaging data, despite recent ini tiatives such as the UK Biobank [28]. Although medical institutions often possess large amounts of data, most of it remains unlabeled and underutilized. For example, for research purposes, some hospitals record hours of videos of patients in intensive care, but those videos remain only poorly labeled in the clinical routine, with at best, the sparse event labels. Weaklysupervised learning aims to leverage datasets with either incomplete or incorrect labels. Zhou et al. [32] identified two subtypes of weak supervision schemes: in complete and inaccurate supervision. Incomplete super vision applies when only a portion of the training sam ples are labeled. For example, semisupervised learning methods are designed to leverage unlabeled samples next to labeled samples. Inaccurate supervision applies when the given labels are not necessarily correct (e.g., crowd sourcing [23, 6]). The works of Hao et al. [12] on mam mograms and Karimi et al. [16] on brain MRIs are also examples of inaccurate supervision with deep learning for medical data. In this work, we propose a method which combines semisupervised learning and inaccurate supervision to leverage sparselylabeled sequential data. The main task is to detect sequences of events, given only sparse training la bels, i.e., the start times of these events. The end times and the duration of these events remain unknown, which pre vents sampling any positives events with certainty (FigurearXiv:2011.14101v5  [cs.CV]  1 Oct 2022Figure 1. Leveraging sparse video labels. During training time, only event start times are annotated. Event end times have to be guessed, which determine the number Nof elements that can be used as positives during training. A conservative model only uses a single element (N= 1) as positive per sparse label, while a risktolerant model uses multiple elements, e.g. N= 3. Higherrisk labeling strategies may result in more incorrect labels during training (negative segments being mislabeled as positives). However, these higherrisk strategies can provide more training data, which may result in better detection performance despite training with incorrect labels. 1). For example, in a cooking videos dataset, sparse training labels could indicate when cooking an ingredient started at time T, but without any information about when the cooking of that ingredient stopped. To address this problem, we propose making a noisy ap proximation of event end times. For each sparse label, we choose a fixed number of consecutive elements in sequence that follow the sparse label, and use them as positive train ing samples (essentially providing a noisy estimate of dura tion). In the above example with cooking videos, we could guess that the cooking of the ingredient lasts one minute, or 1500 frames at 25 frames per second, and use all 1500 frames as positive samples. The longer the estimated guess, the more likely it is that we introduce potential incorrectly labeled samples (false positives in the training set). We further propose a mathematical model for explain ing and estimating the evolution of the classification per formance for increasingly noisier end time estimates. This model include two sigmoidbased components respectively describing the positive and negative impact of additional noisy labeled sequence elements on the performance. We empirically evaluate our method on sparselylabeled sequences of CIFAR10 images, MNIST images, Berke ley MHAD videos, and HMDB51, and show an improve ment of 3.5 points of mean average precision for CIFAR, 30 points for MNIST, 3 points for MHAD, and 14 points for HMDB51 over the baseline method. Finally, we demonstrate our method on a reallife se quential analysis task‚Äîvideo monitoring of epileptic hos pital patients. Electroencephalography (EEG) is a com mon modality for recording brain activity and monitoring patients. Automated methods have been developed to au tomatically detect seizures from EEG activity [9, 24] but can fail to discern seizures from artifacts caused by distur bances in EEG measurement (e.g., patting on the back or rocking neonatal patients can trigger false positive seizure detections). We address EEG artifacts by automatically detecting five artifact‚Äìsuctioning of neonates, chewing, rocking, cares by nurse and patting of neonates‚Äìfrom continuous videorecordings acquired during clinical routine. Those events are annotated with sparse labels (only start times, no end times), which is common practice for the labeling of con tinuous recordings in clinical routine [26]. Our method for learning from sparselylabeled sequences can leverage those sparse labels, outperforming baseline methods by 17 points of mean average precision. We show that our semi supervised model can reach the classification of fully super vised models. We also give insight into estimating the pa rameters of the proposed model in case of merging classes. To summarize, our main contributions are: ‚Ä¢ A training strategy for semisupervised learning with sparselylabeled sequential data. ‚Ä¢ A mathematical model for explaining and estimating the evolution of the classification performance for in creasingly noisier end time estimates. ‚Ä¢ A method that automatically detects events from sparselylabeled continuous video recordings of hos pital neonates. 2. Related works "
312,Multi-Dialect Arabic BERT for Country-Level Dialect Identification.txt,"Arabic dialect identification is a complex problem for a number of inherent
properties of the language itself. In this paper, we present the experiments
conducted, and the models developed by our competing team, Mawdoo3 AI, along
the way to achieving our winning solution to subtask 1 of the Nuanced Arabic
Dialect Identification (NADI) shared task. The dialect identification subtask
provides 21,000 country-level labeled tweets covering all 21 Arab countries. An
unlabeled corpus of 10M tweets from the same domain is also presented by the
competition organizers for optional use. Our winning solution itself came in
the form of an ensemble of different training iterations of our pre-trained
BERT model, which achieved a micro-averaged F1-score of 26.78% on the subtask
at hand. We publicly release the pre-trained language model component of our
winning solution under the name of Multi-dialect-Arabic-BERT model, for any
interested researcher out there.","The term Arabic language is better thought of as an umbrella term, under which it is possible to list hundreds of varieties of the language, some of which are not even mutually comprehensible. Nonetheless, such varieties can be grouped together with varying levels of granularity, all of which correspond to the various ways the geographical extent of the Arab world can be divided, albeit loosely. Despite such diversity, up until recently, such varieties were strictly conÔ¨Åned to the spoken domains, with Modern Standard Arabic (MSA) dominating the written forms of communication all over the Arab world. However, with the advent of social media, an explosion of written content in said varieties have Ô¨Çooded the internet, attracting the attention and interest of the wide Arabic NLP research community in the process. This is evident in the number of held workshops dedicated to the topic in the last few years. In this paper we present and discuss the strategies and experiments we conducted to achieve the Ô¨Årst place in the Nuanced Arabic Dialect IdentiÔ¨Åcation (NADI) Shared Task 1 [ 1], which is dedicated to dialect identiÔ¨Åcation at the country level. In section 2 we discuss related work. This is followed by section 3 in which we discuss the data used to develop our model. Section 4 discusses the most signiÔ¨Åcant models we tested and tried in our experiments. The details and results of said experiments can be found in section 5. The analysis and discussion of the results can be obtained in section 6 followed by our conclusions in section 7. 2 Related Work "
313,Deep Self-Paced Learning for Person Re-Identification.txt,"Person re-identification (Re-ID) usually suffers from noisy samples with
background clutter and mutual occlusion, which makes it extremely difficult to
distinguish different individuals across the disjoint camera views. In this
paper, we propose a novel deep self-paced learning (DSPL) algorithm to
alleviate this problem, in which we apply a self-paced constraint and symmetric
regularization to help the relative distance metric training the deep neural
network, so as to learn the stable and discriminative features for person
Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive
the adaptive weights to samples based on both the training loss and model age.
As a result, the high-confidence fidelity samples will be emphasized and the
low-confidence noisy samples will be suppressed at early stage of the whole
training process. Such a learning regime is naturally implemented under a
self-paced learning (SPL) framework, in which samples weights are adaptively
updated based on both model age and sample loss using an alternative
optimization method. Secondly, we introduce a symmetric regularizer term to
revise the asymmetric gradient back-propagation derived by the relative
distance metric, so as to simultaneously minimize the intra-class distance and
maximize the inter-class distance in each triplet unit. Finally, we build a
part-based deep neural network, in which the features of different body parts
are first discriminately learned in the lower convolutional layers and then
fused in the higher fully connected layers. Experiments on several benchmark
datasets have demonstrated the superior performance of our method as compared
with the state-of-the-art approaches.","Person reidentiÔ¨Åcation (ReID) has become an active re search topic in the Ô¨Åeld of computer vision, because of its wide application in the video surveillance community. Given one single shot or multiple shots of a target, person ReID concerns the problem of matching the same person among a set of gallery candidates captured from the disjoint camera views [1‚Äì4]. It is a very challenging task due to noisy samples with mutual oc clusion and background clutter that makes the large appearance variations across different camera views [5, 6]. Therefore, the Corresponding author: Tel.: +8602983395146; Fax: +8602983395175; Email address: sanpingzhou@stu.xjtu.edu.cn (Sanping Zhou)key to improve the identiÔ¨Åcation performance is to learn the stable and discriminative features for representation. The fundamental person ReID problem is to compare an im age of each interested target seen in a probe camera view to a large number of candidates captured from a gallery camera view which has no overlap with the probe one [7]. If a true match to the probe exists in the gallery, it should have a higher similarity score as compared with the incorrect matches. Pre vious efforts for solving this problem primarily focus on the following two aspects: 1) developing robust feature descriptors to handle the variations in person‚Äôs appearance, and 2) design ing discriminative distance metrics to measure the similarity of person‚Äôs images. For the Ô¨Årst category, different cues are Preprint submitted to Journal of Pattern Recognition October 17, 2017arXiv:1710.05711v1  [cs.CV]  7 Oct 2017employed for the stable and discriminative features. Represen tative descriptors include the Local Binary Pattern (LBP) [8], Ensemble of Local Feature (ELF) [9] and Local Maximal Oc currence (LOMO) [10]. For the second category, labeled im ages are used to train a distance metric, in which the intraclass distance is minimized while the interclass distance is maxi mized. Typical metric learning methods include the Locally Adaptive Decision Function (LADF) [11], Large Margin Near est Neighbor (LMNN) [12] and Information Theoretic Metric Learning (ITML) [13]. Since both line of works regard the feature extraction and metric learning processes as two disjoint steps, their performances are limited. In the past two years, the deep convolutional neural network (CNN) based methods [14‚Äì19] have been proposed to combine the feature extraction and metric learning into an endtoend learning framework, in which a neural network is built to ex tract the stable and discriminative features under the supervi sion of a suitable distance metric. BeneÔ¨Åt from the powerful representation capability of the deep CNN, this line of meth ods have achieved promising results on the benchmark datasets for person ReID. The relative distance metric [20] has been widely used as loss function in the deep learning based methods for visual recognition. Compared with the wellknown softmax loss [21], it is a better choice for the zeroshot recognition prob lem, because of the training set doesn‚Äôt have the same identity with the testing set. The relative distance metric aims to maxi mize the relative distance between the positive pair and negative pair in each triplet unit, which can generate a large number of triplet inputs even using a small number of training samples. Therefore, it is very suitable choice for the person ReID prob lem which not only is a zeroshot problem but also can only provide the smallscale dataset for training. To further improve the identiÔ¨Åcation performance, our ob servation shows that the following three issues should also be addressed in the learning process. Firstly, the order and weight of training samples should be considered, as shown in Fig. 1, otherwise it might be easy to cause the unstable learning due to the noisy samples or outliers with mutual occlusion and back 1 2 3 4 5 6Similarity 1 2 3 4 5 6 1 2 3 4 5 6Training the deep model in SPL Manner Weight = 0.32Weight = 0.29 Weight = 0.96Weight = 0.84 Weight = 0.84 Weight = 0.09Weight = 0.98Weight = 0.87Weight = 0.26 Weight = 0.19Weight = 0.02 Weight = 0.86Anchor AnchorAnchor     Candidates in GalleryFigure 1: Illustration of our SPL motivations in dealing with the noisy training samples or outliers. The left column shows some typical positive candidates to two anchor images, in which the similarity scores of these positive candidates to the anchor vary from large to small with the incensement of indexes. The right column shows the SPL training strategy , in which the derived weighting scheme will adaptively update the sample weights according to the training loss and model age. Therefore, highconÔ¨Ådence Ô¨Ådelity samples will be emphasized and the the lowconÔ¨Ådence noisy samples will be suppressed at early stage of the whole learning process. ground clutter. Secondly, it is unsuitable to directly apply the distance metric to supervise the training process of deep CNN without any regularization to the gradient backpropagation. Because most of the deep learning tools, such as Caffe [22] and TensorÔ¨Çow [23], take the gradient backpropagation algorithm to optimize the deep parameters. Thirdly, the neural network should be relatively small and include the part processing mod ule, due to the person ReID is a Ô¨Ånegrained problem and the dataset for person ReID is usually in small size. As a conse quence, it is very urgent to study the three aspects of problems in the training process. In this paper, we propose a novel deep selfpaced learning (DSPL) algorithm to adaptively update the weights to samples and regularize the gradient backpropagation of relative dis tance metric [20] in the learning process, so as to further im prove the identiÔ¨Åcation performance of deep neural network for person ReID. In order to extract the stable and discrimi native features, we Ô¨Årstly build a partbased deep neural net work, in which the features of different body parts are discrim inately learned in the lower convolutional layers and then fused in the higher fully connected layers. Then, we introduce the 2selfpaced learning (SPL) theory [24] into the training frame work, in which samples can be ranked in a selfpaced manner by applying a novel soft polynomial regularizer term to adap tively update the weights according to both the model age and sample loss in each iteration. Specially, the highconÔ¨Ådence Ô¨Å delity samples will be emphasized and the the lowconÔ¨Ådence noisy samples will be suppressed at early stage of the whole learning process. Therefore, the neural network can be trained in a stable process by gradually involving the faithful samples from easy to hard. In addition, a symmetric regularizer term is introduced to overcome the drawback of relative distance met ric in gradient backpropagation. As a result, the intraclass dis tance is minimized and the interclass distance is maximized by regularizing the asymmetric gradient backpropagation in each triplet unit. Extensive experimental results on several bench mark datasets have shown that our method performs much bet ter than the stateoftheart approaches. In summary, the main contributions of this paper can be high lighted as follows: We propose a novel DSPL algorithm to supervise the learning of deep neural network, in which a soft polyno mial regularizer term is proposed to gradually involve the faithful samples into training process in a selfpaced man ner. We optimize the gradient backpropagation of relative dis tance metric by introducing a symmetric regularizer term, which can convert the backpropagation from the asym metric mode to a symmetric one. We build an effective partbased deep neural network, in which features of different body parts are Ô¨Årst discrimi nately learned in the lower convolutional layers and then fused in the higher fully connected layers. The rest of our paper is organized as follows: Section 2 re views some of the related works. In Section 3, we describe the proposed method, including the DSPL algorithm and deep neu ral network. The experimental results and corresponding anal ysis are presented in Section 4. Conclusion comes in Section 5.2. Related work "
314,Accurate 3D Cell Segmentation using Deep Feature and CRF Refinement.txt,"We consider the problem of accurately identifying cell boundaries and
labeling individual cells in confocal microscopy images, specifically, 3D image
stacks of cells with tagged cell membranes. Precise identification of cell
boundaries, their shapes, and quantifying inter-cellular space leads to a
better understanding of cell morphogenesis. Towards this, we outline a cell
segmentation method that uses a deep neural network architecture to extract a
confidence map of cell boundaries, followed by a 3D watershed algorithm and a
final refinement using a conditional random field. In addition to improving the
accuracy of segmentation compared to other state-of-the-art methods, the
proposed approach also generalizes well to different datasets without the need
to retrain the network for each dataset. Detailed experimental results are
provided, and the source code is available on GitHub.","Accurate cell boundary detection from 3D confocal imagery is critical in many applications including modeling of cell morphogenesis [1] and plant growth [2]. A typical cell seg mentation workÔ¨Çow consists of the following steps: (i) im age preprocessing to enhance the signaltonoise ratio, (ii) a segmentation method that then partitions the cells, and (iii) a Ô¨Ånal reÔ¨Ånement step. Typical preprocessing methods includ ing morphological operations, edge enhancement methods, and denoising are widely used in the recent cell segmenta tion tasks [3, 4, 5, 6]. This preprocessing stage is often data dependent and requires parameter tuning to avoid under or oversegmentation. For membranetagged images, segmen tation is usually carried out with either 3D watershed based methods [2, 4] or 3D level sets [7]. Some methods work on individual 2D images and then fuse the results to get a 3D segmentation [3, 8]. Finally, postprocessing using cell vol ume or shape heuristic is usually applied. For this traditional This work was supported by NSF MCB Grant No. 1121893 to D.B.S. and NSF MCB Grant No. 1715544 to B.S.M. 1https://github.com/UCSBVRL/Purdue3DCellworkÔ¨Çow of cell segmentation, preprocessing is subjective, and its parameters are highly dependent on the data. During the past few years, there has been much interest in the use of deep learning for the cell segmentation problems, and the UNet in particular has been widely explored [9, 10]. UNet is a fully convolutional network which consists of en coder and decoder parts. The encoder part uses context infor mation to encode raw images into feature maps, and the de coder part uses the produced feature maps to localize objects and generate the segmentation. UNet based method gives fairly good cell boundary segmentation accuracy on the sim ilar dataset but its performance drops in generalizing to other datasets. Besides, UNet is not guaranteed to generate closed cell surfaces. A  B Fig. 1 . (A) Intercellular spaces and (B) Protrusion are indi cated by red arrows. Best viewed in color. In this paper, we aim to solve the problem of accurately identifying cell boundaries and labeling individual cells in confocal microscopy image stacks. The goal is to generate closed cell surfaces in the 3D image stack while being able to accurately delineate features of interest such as the inter cellular spaces and protrusions , see Ô¨Åg. 1. The Ô¨Årst step is to generate a membrane probability map where voxels that are likely to belong to a cell membrane have higher values than those that are not on the cell membrane. This is achieved by a 3D UNet architecture which is trained on a large, public data. This is followed by a watershed method that labels individual cells. One challenge for watershed algorithm is automated seeding of the cells, and this is achieved using a simple dis tance transform of the membrane probability map. The water shed based approach is sensitive to the image signal and likelyarXiv:1902.04729v1  [cs.CV]  13 Feb 2019to smooth out the boundaries and also miss smaller features that are signiÔ¨Åcant in modeling cell morphogenesis. For this purpose, we introduce a Ô¨Ånal processing step based on condi tional random Ô¨Åelds (CRFs). The CRF reÔ¨Ånes the watershed boundaries taking into account the probability map and local voxel boundary information. As demonstrated through the ex tensive experiments, this would help the overall approach to be sensitive to local boundary perturbation, including detect ing salient boundary features. The proposed method is vali dated in two datasets, and the experimental results shows that our segmentation method outperforms the current cell seg mentation methods [2, 11, 12] with less computation time. In addition, we also demonstrate that the proposed method gen eralizes well to different datasets without addition retraining or Ô¨Åne tuning the parameters of the neural network. To summarize, the main contributions of this paper in clude (i) a method that is sensitive to local image features, for accurate detection and localization of boundaries in 3D confocal stacks; (ii) and show that the method is capable of generalizing to other datasets without further training. 2. MATERIAL AND METHODS 2.1. Dataset Two 3D confocal image stack datasets of Ô¨Çuorescenttagged plasmamembrane cells are used in this paper. In both datasets, only the plasmamembrane signal is available and is represented by voxels with high intensity values. The Ô¨Årst dataset contains 124 image stacks of cells in the shoot api cal meristem of 6 Arabidopsis thaliana [13]. In each image stack, there are 150 to 300 slices containing of 3 layers of cells: outer layer (L1), middle layer (L2), and deep layer (L3), and the size of each slice is 512512. The available resolution of each image in x and y direction are 0.22 mand in z is about 0.27 m. We should note that as we go from L1 towards L3, the image quality degrades progressively due to scattering of light. This condition makes the segmenta tion problem challenging in L3. In this dataset, the ground truth voxelwise cell labels are provided, and each cell has a unique label. The second dataset [1] consists of a longterm timelapse from A. thaliana‚Äôs leaf epidermal tissue that spans over a 12 hour period with a xyresolution of 0.212 mand 0.5mthick optical sections. There are 20 image stacks in this dataset. In each image stack, there are 15 to 30 slices containing one layer of cell, and the size of each slice is 512512. The ground truth cell labels are not provided in this dataset. 2.2. Method "
315,Contrastive Learning and Self-Training for Unsupervised Domain Adaptation in Semantic Segmentation.txt,"Deep convolutional neural networks have considerably improved
state-of-the-art results for semantic segmentation. Nevertheless, even modern
architectures lack the ability to generalize well to a test dataset that
originates from a different domain. To avoid the costly annotation of training
data for unseen domains, unsupervised domain adaptation (UDA) attempts to
provide efficient knowledge transfer from a labeled source domain to an
unlabeled target domain. Previous work has mainly focused on minimizing the
discrepancy between the two domains by using adversarial training or
self-training. While adversarial training may fail to align the correct
semantic categories as it minimizes the discrepancy between the global
distributions, self-training raises the question of how to provide reliable
pseudo-labels. To align the correct semantic categories across domains, we
propose a contrastive learning approach that adapts category-wise centroids
across domains. Furthermore, we extend our method with self-training, where we
use a memory-efficient temporal ensemble to generate consistent and reliable
pseudo-labels. Although both contrastive learning and self-training (CLST)
through temporal ensembling enable knowledge transfer between two domains, it
is their combination that leads to a symbiotic structure. We validate our
approach on two domain adaptation benchmarks: GTA5 $\rightarrow$ Cityscapes and
SYNTHIA $\rightarrow$ Cityscapes. Our method achieves better or comparable
results than the state-of-the-art. We will make the code publicly available.","The goal in semantic image segmentation is to assign the correct class label to each pixel. This makes it suitable for complex imagebased scene analysis that is required in ap plications like automated driving. However, in order to gen erate a labeled training dataset, pixellevel annotation must Ô¨Årst be performed by humans. Since detailed manual labeling can take 90 minutes per image [8], it is associated with high costs. A potential workaround would be to generate the images and corresponding segmentation maps synthetically using computer game environments like Grand Theft Auto V (GTA5) [29]. However, even current segmentation mod els [2, 34, 1] do not generalize well to data from a different domain. In fact, their segmentation performance decreases drastically when there is a discrepancy between the training and test distribution as in a synthetictoreal scenario. The research Ô¨Åeld of unsupervised domain adaptation (UDA) studies how to transfer knowledge from a labeled source domain to an unlabeled target domain. The aim is to achieve the best possible results in the target domain, whereas the performance in the source domain is not con sidered. Current methods for UDA address the problem by minimizing the distribution discrepancy between the do mains while doing a supervised training on source data. The distribution alignment can take place in the pixel space [15, 23, 36, 44], feature space [16, 17, 41, 45], output space [38, 40, 39, 41, 24], or even in several spaces in parallel. While adversarial training (AT) [10] is commonly used to minimize the distribution discrepancy between domains, it can fail to align the correct semantic categories. This is because adversarial training minimizes the mismatch be tween global distributions rather than classspeciÔ¨Åc ones, which can negatively affect the results [7]. This is also true for discrepancy measures such as the maximum mean dis crepancy [31], which can be minimized without aligning the correct class distributions across domains [19]. To address the problem of misaligned classes across domains, we rely on contrastive learning (CL) [12]. The basic idea of CL is to encourage positive data pairs to be similar and negative data pairs to be apart. To perform domain adaptation and match the features of the correct semantic categories, posi tive pairs consist of features from the same class but differ ent domains while negative pairs are from different classes and possibly from different domains [19, 27]. Due to the lack of labels in the target domain, the class of each target feature must be determined based on the predictions of the model. However, since different classarXiv:2105.02001v1  [cs.CV]  5 May 2021prior probabilities can bias the Ô¨Ånal segmentation layer to wards the source domain, we extend our approach with selftraining (ST), i.e. using target predictions as pseudo labels. We start from the observation that target pixels are predicted with high uncertainty [40]. Moreover, these un certain predictions may also vary between different classes during training and are therefore usually not considered for selftraining. By using a memoryefÔ¨Åcient temporal ensem ble that combines the predictions of a single network over time [21], we obtain the predictive tendency of the model. This allows us to create robust pseudolabels even for the uncertain predictions that have high information content. The temporal ensemble has the additional advantage that pseudolabels are updated directly during training, which reduces the computational complexity compared to a sepa rate stagewise recalculation [45, 26, 22, 49, 44]. We summarize our contributions as follows: First, we ex tend contrastive learning and selftraining using a memory efÔ¨Åcient temporal ensemble to UDA for semantic segmen tation. Second, we empirically show that both approaches are able to transfer knowledge between domains. Further more, we show that combining our contrastive learning and selftraining (CLST) approach leads to a symbiotic setup that yields competitive and superior stateoftheart results for GTA5!Cityscapes and SYNTHIA !Cityscapes, re spectively. 2. Related works "
316,ProbAct: A Probabilistic Activation Function for Deep Neural Networks.txt,"Activation functions play an important role in training artificial neural
networks. The majority of currently used activation functions are deterministic
in nature, with their fixed input-output relationship. In this work, we propose
a novel probabilistic activation function, called ProbAct. ProbAct is
decomposed into a mean and variance and the output value is sampled from the
formed distribution, making ProbAct a stochastic activation function. The
values of mean and variances can be fixed using known functions or trained for
each element. In the trainable ProbAct, the mean and the variance of the
activation distribution is trained within the back-propagation framework
alongside other parameters. We show that the stochastic perturbation induced
through ProbAct acts as a viable generalization technique for feature
augmentation. In our experiments, we compare ProbAct with well-known activation
functions on classification tasks on different modalities: Images(CIFAR-10,
CIFAR-100, and STL-10) and Text (Large Movie Review). We show that ProbAct
increases the classification accuracy by +2-3% compared to ReLU or other
conventional activation functions on both original datasets and when datasets
are reduced to 50% and 25% of the original size. Finally, we show that ProbAct
learns an ensemble of models by itself that can be used to estimate the
uncertainties associated with the prediction and provides robustness to noisy
inputs.","Activation functions add nonlinearity to neural networks making them learn complex functional mappings from data [ 1]. Different activation functions with different characteristics have been proposed. Sigmoid [ 2] and hyperbolic tangent (Tanh) were the popular ones during the early usage of neural networks [ 3] mainly due to their monotonicity, continuity, and bounded properties. In recent times, the RectiÔ¨Åed Linear Unit (ReLU) [ 4] has become an extremely popular activation function for neural networks. Several variants of ReLU have been proposed, e.g., Leaky ReLU [ 5], Parametric ReLU (PReLU) [6], and Exponential Linear Unit (ELU) [7]. However, all of these are deterministic activation functions with Ô¨Åxed inputoutput relationships. In this work, we propose a new activation function, called ProbAct , which is not only trainable but also 1 k_shridhar16@cs.unikl.de |{joonho.lee, hayashi, brian, seokjun.kang, uchida} @human.ait.kyushuu.ac.jp pmehta9@ur.rochester.edu{sheraz.ahmed, andreas.dengel}@dfki.de Preprint. Under review.arXiv:1905.10761v2  [cs.LG]  16 Jun 2020(a) ReLU  (b) ProbAct  (c) Effect of stochastic perturbation Figure 1: Comparison of (a) ReLU and (b) the proposed activation function. (c) is the effect of stochastic perturbation by ProbAct at feature spaces in a neural network. stochastic in nature. The idea of ProbAct is inspired by the stochastic behavior of biological neurons. Noise in neuronal spikes can arise due to uncertain biomechanical effects [ 8]. We try to emulate a similar behavior in the information Ô¨Çow to the neurons by injecting stochastic sampling from a Gaussian distribution to the activations. Consequently, even for the same input value x, the output value from ProbAct varies stochastically ‚Äî a capability conventional activation functions do not offer. The induced perturbations prove to be effective in avoiding overÔ¨Åtting during training, thus yielding better generalizations. Since the operation is a resemblance to feature augmentation, we call it augmentationbyactivation . Furthermore, we show that ProbAct improves the classiÔ¨Åcation accuracy by 23% compared to ReLU or other conventional activation functions on established image datasets and 12% on text datasets. The main contributions of our work are as follows: We introduce a novel activation function, called ProbAct, whose output undergoes stochastic perturbation. We propose a novel method of governing the stochastic perturbation with parameters trained through backpropagation. We show that ProbAct improves the performance on various visual and textual classiÔ¨Åcation tasks while generalizing well on reduced datasets. We also show that the improvement by ProbAct is realized by the augmentationbyactivation, which acts as a stochastic regularizer to prevent overÔ¨Åtting of the network and acts as a feature augmentation method. Finally, we demonstrate that ProbAct learns an ensemble of models by itself, allowing the estimation of predictive uncertainties and robustness to noisy data. 2 Related Work "
317,Membership Inference Attack for Beluga Whales Discrimination.txt,"To efficiently monitor the growth and evolution of a particular wildlife
population, one of the main fundamental challenges to address in animal ecology
is the re-identification of individuals that have been previously encountered
but also the discrimination between known and unknown individuals (the
so-called ""open-set problem""), which is the first step to realize before
re-identification. In particular, in this work, we are interested in the
discrimination within digital photos of beluga whales, which are known to be
among the most challenging marine species to discriminate due to their lack of
distinctive features. To tackle this problem, we propose a novel approach based
on the use of Membership Inference Attacks (MIAs), which are normally used to
assess the privacy risks associated with releasing a particular machine
learning model. More precisely, we demonstrate that the problem of
discriminating between known and unknown individuals can be solved efficiently
using state-of-the-art approaches for MIAs. Extensive experiments on three
benchmark datasets related to whales, two different neural network
architectures, and three MIA clearly demonstrate the performance of the
approach. In addition, we have also designed a novel MIA strategy that we
coined as ensemble MIA, which combines the outputs of different MIAs to
increase the attack accuracy while diminishing the false positive rate.
Overall, one of our main objectives is also to show that the research on
privacy attacks can also be leveraged ""for good"" by helping to address
practical challenges encountered in animal ecology.","In animal ecology, the ability to reidentify (reID) an indi vidual animal across multiple encounters allows for addressing a broad range of questions such as ecosystem function, community, and population dynamics as well as behavioral ecology [2, 27]. In many cases, especially for aquatic species such as marine mammals, reID requires extensive training and practical experience for a human to acquire sucient expertise to be able to accurately recognize a particular indi vidual. To partially circumvent this issue, biologists usually rely on approaches such as tagging and photoidentication (photoID) [59, 27]. While accurate, the tagging approach is intrusive to animals and is often expensive and laborious. In contrast, the photoID approach uses visual identication from camera images ( e.g., handheld camera, camera trap, or drones), which is noninvasive for animals and has a lower cost. Nonetheless, there are some practical and methodolog ical challenges associated with its use. First, even among experienced researchers, there is a nonnegligible chance of human error and bias when reviewing photos [17]. Second, it is also timeconsuming and expensive in terms of human involvement to manually lter through thousands of images. To overcome these limitations, one possible strategy is to rely on computer vision techniques to standardize and au tomatize the animal reID process [54]. To realize this, for decades, \feature engineering"", which can be dened as the process of selecting or transforming raw data into informa tive features, has been the most commonly used technique. Basically, it means that most of the algorithms for animal reID are designed and implemented to focus exclusively on predetermined traits, such as patterns of spots or stripes, to discriminate among individuals. However, feature engineer ing requires programming experience, sucient familiarity with the species considered to identify relevant features. In addition, this approach lacks in generality as once a feature detection algorithm has been designed for one species, it is unlikely to be useful for others [21]. More recently, the last decade has witnessed the emer gence of deep learning systems that make use of large data volumes to automatically learn discriminative features [34]. In particular, Convolutional Neural Networks (CNNs) have achieved stateoftheart results in a variety of uses cases based on the assumption of a closed world ( i.e., a xed number of classes/identities), However, CNNs are known to lack robustness when deployed in realworld classica tion/recognition applications, in which incomplete knowl 1arXiv:2302.14769v1  [cs.CV]  28 Feb 2023edge of the world during training result in unknown classes being submitted to the model during testing. This corre sponds for instance to the situation in which when used in the wild, the model will have to recognize individuals that it has not seen during training. In marine ecology, one of the main challenges related to animal reID, such as wild whales, is the encounter of large populations in which there is frequently the addition of new individual appearing due to birth or migration, therefore creating an \openset"" setting [52] wherein the identity model must deal with \classes"" ( i.e., individuals) unseen during training. Thus, a desirable feature for an animal reID ap proach is to have the ability to identify not only animals that belong to the catalog but also recognize new individuals (i.e., previously unknown animals). To address this issue, we investigate the use of Membership Inference Attacks (MIA), which is a form of privacy leakage in which the objective of the adversary is to decide whether a given data sample was in a machine learning model's training dataset [55, 62, 50, 39, 32, 12]. Knowing that a specic data sample was used to train a particular model may lead to potential privacy breaches if for instance this membership reveals a sensitive characteristic ( e.g., being part of the cohort of patients hav ing a particular disease or being a member of a vulnera ble group). The gist of our approach is we could leverage on a MIA to discriminate whether a new beluga whale was present or not in the training set. Then, this information can be used in the reID pipeline to take the decision to clas sify known individuals or to add a new entry in the catalog for an unknown individual. To summarize, in this paper our main contribution is the proposition of a novel approach for whales discrimina tion through images (photoID), which relies on the use of MIAs. In particular, one of our objective is to show that by drawing on the signicant body of work on MIAs, it is possible to eciently address the \openset"" vs \closed set"" problem. To demonstrate this, extensive experiments have been conducted with three stateoftheart MIAs that leverage dierent information produced by the model ( i.e., prediction condence, predicted and ground truth label, or both of them) as well as dierent attack strategies (neu ral networkbased, metricbased and querybased attacks). More precisely, we have performed a comprehensive mea surement of the success of MIAs to address the openset problem over two model architectures (ResNet50 [19] and DenseNet121 [23]), three benchmark image datasets related to whales species (GREMM [37], Humpback [15, 9] and NOAA [40]) along with three stateoftheart MIAs, namely Yeom et al. [62], Salem et al. [50] and LabelOnly [14], thus building a total of 36 attack scenarios. In addition, previ ous works [55, 50, 14] assume the leak information is more likely for machine learning models on the in uence of over tting, we ensure this assumption by evaluating overtted and nonovertted models while monitoring the false pos itive rate as recommended in [8, 48] for the reliability of the results. Finally, we introduced a novel attack design for whale discrimination, which we coined as ensemble MIAs, which combines the outputs of dierent MIAs to increase the attack accuracy while decreasing the false positive rate. The outline of the paper is as follows. First in Section 2, we review the relevant background on automated photo iden tication systems as well as on membership inference at tack. Then in Section 3, we describe the St. Lawrencebeluga whale reid pipeline from side pictures, the training of the attack model as well as the dierent MIA strategies that we propose to implement the discrimination between known and unknown belugas. Afterwards in Section 4, we present the experimental setting used to evaluate our ap proach, which includes the datasets, the experimental con guration as well as the target and attack models. Finally in Section 5, we report on the performance of the approach under dierent scenarios, before discussing how the attack can generalize to dierent settings as well as the factors in  uencing its success and its robustness before concluding in Section 6. 2 Related Work "
318,Distributed learning of deep neural network over multiple agents.txt,"In domains such as health care and finance, shortage of labeled data and
computational resources is a critical issue while developing machine learning
algorithms. To address the issue of labeled data scarcity in training and
deployment of neural network-based systems, we propose a new technique to train
deep neural networks over several data sources. Our method allows for deep
neural networks to be trained using data from multiple entities in a
distributed fashion. We evaluate our algorithm on existing datasets and show
that it obtains performance which is similar to a regular neural network
trained on a single machine. We further extend it to incorporate
semi-supervised learning when training with few labeled samples, and analyze
any security concerns that may arise. Our algorithm paves the way for
distributed training of deep neural networks in data sensitive applications
when raw data may not be shared directly.","Deep neural networks have become the new state of the art in classication and prediction of high dimensional data such as images, videos and biosensors. Emerging technologies in domains such as biomedicine and health stand to benet from building deep neural networks for prediction and inference by automating the human involvement and reducing the cost of operation. However, training of deep neural nets can be extremely data intensive requiring preparation of large scale datasets collected from multiple entities [1, 2]. A deep neural network typically contains millions of parameters and requires tremendous computing power for training, making it dicult for individual data repositories to train them. Corresponding author Email address: otkrist@mit.edu (Otkrist Gupta) Preprint submitted to Journal of L ATEX Templates October 16, 2018arXiv:1810.06060v1  [cs.LG]  14 Oct 2018Suciently deep neural architectures needing large supercomputing resources and engineering oversight may be required for optimal accuracy in real world applications. Furthermore, application of deep learning to such domains can sometimes be challenging because of privacy and ethical issues associated with sharing of deanonymized data. While a lot of such data entities have vested interest in developing new deep learning algorithms, they might also be obligated to keep their user data private, making it even more challenging to use this data while building machine learning pipelines. In this paper, we attempt to solve these problems by proposing methods that enable training of neural networks using multiple data sources and a single supercomputing resource. 2. Related Work "
319,Smooth Neighbors on Teacher Graphs for Semi-supervised Learning.txt,"The recently proposed self-ensembling methods have achieved promising results
in deep semi-supervised learning, which penalize inconsistent predictions of
unlabeled data under different perturbations. However, they only consider
adding perturbations to each single data point, while ignoring the connections
between data samples. In this paper, we propose a novel method, called Smooth
Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on
the predictions of the teacher model, i.e., the implicit self-ensemble of
models. Then the graph serves as a similarity measure with respect to which the
representations of ""similar"" neighboring points are learned to be smooth on the
low-dimensional manifold. We achieve state-of-the-art results on
semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for
CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,
the improvements are significant when the labels are fewer. For the
non-augmented MNIST with only 20 labels, the error rate is reduced from
previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.","As collecting a fully labeled dataset is often expensive and timeconsuming, semisupervised learning (SSL) has been extensively studied in computer vision to improve general ization performance of the classiÔ¨Åer by leveraging limited labeled data and a large amount of unlabeled data [ 9]. The success of SSL relies on the key smoothness assumption, i.e., data points close to each other are likely to have the same label. It has a special case named cluster orlow density sep aration assumption, which states that the decision boundary should lie in low density regions, not crossing high density regions [ 10]. Based on these assumptions, many traditional methods have been developed [22, 54, 51, 10, 4]. Recently due to the great advances of deep learning [ 25], remarkable results have been achieved on SSL [ 24,35,40, Corresponding author.27]. Among these works, perturbationbased methods [ 37, 2,35,39,27] have demonstrated great promise. Adding noise to the deep model is important to reduce overÔ¨Åtting and learn more robust abstractions, e.g., dropout [ 21] and randomized data augmentation [ 13]. In SSL, perturbation regularization aids by exploring the smoothness assumption. For example, the Manifold Tangent ClassiÔ¨Åer (MTC) [ 37] trains contrastive autoencoders to learn the data manifold and regularizes the predictions to be insensitive to local perturbations along the lowdimensional manifold. Pseudo Ensemble [ 2] and  model in Ladder Network [ 35] evaluate the classiÔ¨Åers with and without perturbations, which act as a‚Äúteacher‚Äù and a ‚Äústudent‚Äù , respectively. The student needs to predict consistently with the targets generated by the teacher on unlabeled data. Following the same principle, temporal ensembling, mean teacher and virtual adversarial training [ 27,46,33] improve the target quality in different ways to form better teachers. All these approaches aim to fuse the inputs into coherent clusters by adding noise and smoothing the mapping function locally [27]. However, these methods only consider the perturbations around each single data point, while ignoring the connec tions between data points, therefore not fully utilizing the information in the unlabeled data structure, such as clusters or manifolds. An extreme situation may happen where the function is smooth in the vicinity of each unlabeled point but not smooth in the vacancy among them. This artifact could be avoided if the unlabeled data structure is taken into consideration. It is known that data points similar to each other ( e.g., in the same class) tend to form clusters ( cluster assumption). Therefore, the connections between similar data points help the fusing of clusters become tighter and more effective (see Fig. 5 for the visualization of real data). Motivated by that, we propose Smooth Neighbors on Teacher Graphs (SNTG) that considers the connections be tween data points to induce smoothness on the data manifold. By learning a teacher graph based on the targets generated by the teacher, our model encourages invariance when some perturbations are added to the neighboring points on the graph. Since deep networks have a hierarchical property,arXiv:1711.00258v2  [cs.LG]  28 Mar 2018yx Teacher Predictions Teacher graphStudenth(x)Supervised lossConsistency Loss 1234Student Predictions h(x) feature Space Batch Data LabelsÀúf(x)f(x) TeacherEnsemble SNTG LossFigure 1: The structure of our model. the top layer maps the inputs into a lowdimensional feature space [ 5,42,29]. Given the teacher graph, SNTG makes the learned features more discriminative by enforcing them to be similar for neighbors and dissimilar for those nonneighbors. The model structure is depicted in Fig. 1. We then propose a doubly stochastic sampling algorithm to reduce the compu tational cost with large minibatch sizes. Our method can be applied with very little engineering effort to existing deep SSL works including both generative and discriminative approaches because SNTG does not introduce any extra net work parameters. We demonstrate signiÔ¨Åcant performance improvements over stateoftheart results while the extra time cost is negligible. 2. Related work "
320,Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification.txt,"Person re-identification (re-ID) aims at identifying the same persons' images
across different cameras. However, domain diversities between different
datasets pose an evident challenge for adapting the re-ID model trained on one
dataset to another one. State-of-the-art unsupervised domain adaptation methods
for person re-ID transferred the learned knowledge from the source domain by
optimizing with pseudo labels created by clustering algorithms on the target
domain. Although they achieved state-of-the-art performances, the inevitable
label noise caused by the clustering procedure was ignored. Such noisy pseudo
labels substantially hinders the model's capability on further improving
feature representations on the target domain. In order to mitigate the effects
of noisy pseudo labels, we propose to softly refine the pseudo labels in the
target domain by proposing an unsupervised framework, Mutual Mean-Teaching
(MMT), to learn better features from the target domain via off-line refined
hard pseudo labels and on-line refined soft pseudo labels in an alternative
training manner. In addition, the common practice is to adopt both the
classification loss and the triplet loss jointly for achieving optimal
performances in person re-ID models. However, conventional triplet loss cannot
work with softly refined labels. To solve this problem, a novel soft
softmax-triplet loss is proposed to support learning with soft pseudo triplet
labels for achieving the optimal domain adaptation performance. The proposed
MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and
16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT
unsupervised domain adaptation tasks. Code is available at
https://github.com/yxgeee/MMT.","Person reidentiÔ¨Åcation (reID) aims at retrieving the same persons‚Äô images from images captured by different cameras. In recent years, person reID datasets with increasing numbers of images were proposed to facilitate the research along this direction. All the datasets require timeconsuming an notations and are keys for reID performance improvements. However, even with such largescale datasets, for person images from a new camera system, the person reID models trained on exist ing datasets generally show evident performance drops because of the domain gaps. Unsupervised Domain Adaptation (UDA) is therefore proposed to adapt the model trained on the source image do main (dataset) with identity labels to the target image domain (dataset) with no identity annotations. Stateoftheart UDA methods (Song et al., 2018; Zhang et al., 2019b; Yang et al., 2019) for person reID group unannotated images with clustering algorithms and train the network with clustering generated pseudo labels. Although the pseudo label generation and feature learning with pseudo labels are conducted alternatively to reÔ¨Åne the pseudo labels to some extent, the training of the neural network is still substantially hindered by the inevitable label noise. The noise derives from the limited transferability of sourcedomain features, the unknown number of targetdomain identities, and the imperfect results of the clustering algorithm. The reÔ¨Ånery of noisy pseudo labels has crucial inÔ¨Çuences to the Ô¨Ånal performance, but is mostly ignored by the clusteringbased UDA methods. 1Code is available at https://github.com/yxgeee/MMT . 1arXiv:2001.01526v2  [cs.CV]  30 Jan 2020Published as a conference paper at ICLR 2020            Net1Net2Noisy Hard Pseudo Labels by ClusteringRobust Soft Pseudo Labels by MMT Figure 1: Person image A1andA2belong to the same identity while Bwith similar appearance is from another person. However, clusteringgenerated pseudo labels in stateoftheart Unsupervised Domain Adaptation (UDA) methods contain much noise that hinders feature learning. We propose pseudo label reÔ¨Ånery with online reÔ¨Åned soft pseudo labels to effectively mitigate the inÔ¨Çuence of noisy pseudo labels and improve UDA performance on person reID. To effectively address the problem of noisy pseudo labels in clusteringbased UDA methods (Song et al., 2018; Zhang et al., 2019b; Yang et al., 2019) (Figure 1), we propose an unsupervised Mutual MeanTeaching (MMT) framework to effectively perform pseudo label reÔ¨Ånery by optimizing the neural networks under the joint supervisions of offline reÔ¨Åned hard pseudo labels and online reÔ¨Åned soft pseudo labels. SpeciÔ¨Åcally, our proposed MMT framework provides robust soft pseudo labels in an online peerteaching manner, which is inspired by the teacherstudent approaches (Tarvainen & Valpola, 2017; Zhang et al., 2018b) to simultaneously train two same networks. The networks gradually capture targetdomain data distributions and thus reÔ¨Åne pseudo labels for better feature learning. To avoid training error ampliÔ¨Åcation, the temporally average model of each network is proposed to produce reliable soft labels for supervising the other network in a collaborative training strategy. By training peernetworks with such online soft pseudo labels on the target domain, the learned feature representations can be iteratively improved to provide more accurate soft pseudo labels, which, in turn, further improves the discriminativeness of learned feature representations. The classiÔ¨Åcation and triplet losses are commonly adopted together to achieve stateoftheart per formances in both fullysupervised (Luo et al., 2019) and unsupervised (Zhang et al., 2019b; Yang et al., 2019) person reID models. However, the conventional triplet loss (Hermans et al., 2017) can not work with such reÔ¨Åned soft labels. To enable using the triplet loss with soft pseudo labels in our MMT framework, we propose a novel soft softmaxtriplet loss so that the network can beneÔ¨Åt from softly reÔ¨Åned triplet labels. The introduction of such soft softmaxtriplet loss is also the key to the superior performance of our proposed framework. Note that the collaborative training strategy on the two networks is only adopted in the training process. Only one network is kept in the inference stage without requiring any additional computational or memory cost. The contributions of this paper could be summarized as threefold. (1) We propose to tackle the label noise problem in stateoftheart clusteringbased UDA methods for person reID, which is mostly ignored by existing methods but is shown to be crucial for achieving superior Ô¨Ånal per formance. The proposed Mutual MeanTeaching (MMT) framework is designed to provide more reliable soft labels. (2) Conventional triplet loss can only work with hard labels. To enable train ing with soft triplet labels for mitigating the pseudo label noise, we propose the soft softmaxtriplet loss to learn more discriminative person features. (3) The MMT framework shows exceptionally strong performances on all UDA tasks of person reID. Compared with stateoftheart methods, it leads to signiÔ¨Åcant improvements of 14.4% ,18.2% ,13.4% ,16.4% mAP on MarkettoDuke, DuketoMarket, MarkettoMSMT, DuketoMSMT reID tasks. 2 R ELATED WORK "
321,Building a Competitive Associative Classifier.txt,"With the huge success of deep learning, other machine learning paradigms have
had to take back seat. Yet other models, particularly rule-based, are more
readable and explainable and can even be competitive when labelled data is not
abundant. However, most of the existing rule-based classifiers suffer from the
production of a large number of classification rules, affecting the model
readability. This hampers the classification accuracy as noisy rules might not
add any useful informationfor classification and also lead to longer
classification time. In this study, we propose SigD2 which uses a novel,
two-stage pruning strategy which prunes most of the noisy, redundant and
uninteresting rules and makes the classification model more accurate and
readable. To make SigDirect more competitive with the most prevalent but
uninterpretable machine learning-based classifiers like neural networks and
support vector machines, we propose bagging and boosting on the ensemble of the
SigDirect classifier. The results of the proposed algorithms are quite
promising and we are able to obtain a minimal set of statistically significant
rules for classification without jeopardizing the classification accuracy. We
use 15 UCI datasets and compare our approach with eight existing systems.The
SigD2 and boosted SigDirect (ACboost) ensemble model outperform various
state-of-the-art classifiers not only in terms of classification accuracy but
also in terms of the number of rules.","Classication is dened as a process of predicting the class label of new data points, given a set of labeled data points in the training set. The association rule mining is a rulebased approach that helps in identifying patterns in the data in the form of rules, by nding the relationships between the items in the dataset. The association rules are in the form X ‚ÜíY, where X is the antecedent and Y is the consequent [1]. Associative classiers combine the concept of association rule mining and classication to build a classication model. In an associative classier, we choose the consequent of the rule to be the class label and the antecedent set is a set of attributevalue pairs for the associated class label. In the literature, various associative classiers have been proposed till now namely,arXiv:2007.01972v1  [cs.LG]  4 Jul 20202 Sood et al. CBA [18], CMAR [17], CPAR [20], ARC [3] etc. Although these classiers are easily understandable,  exible and do not assume independence among the at tributes, they require prior knowledge for choosing appropriate parameter values (support and condence). Furthermore, the rules generated may include noisy and meaningless rules, which might hinder the classication. A rule is said to be noisy if it does not add any new information for prediction and instead misleads the classication model. In other terms, a noisy rule would participate more often in misclassications than in correct classications. The authors in [16] proposed SigDirect, an associative classier which mines statistically signicant rules without the need for the support and condence values. However, in this paper, we propose SigD2 where we introduce a more ef fective two stage pruning strategy to obtain a more accurate classication model. The proposed method reduces the number of rules to be used for classication without compromising on the prediction performance. In fact, the performance is improved. Most of the prevalent supervised classication techniques like Ar ticial Neural Networks (ANN), Support Vector Machines (SVM) etc, although provide very high classication accuracy, they act as a black box. The models produced by such classiers are not straight forwardly explainable. However, the proposed associative classier makes the model more explainable by produc ing only a minimal set of classication association rules (CARs). The proposed technique nds its immense usage in various healthcare related applications, where the explanation of proposed models along with the classication accuracy are highly signicant [22]. In healthcare, incorrect predictions may have catas trophic eect, so doctors nd it hard to trust AI unless they can validate the obtained results. Furthermore, we also propose ACboost, which uses an ensemble of classica tion models obtained from the weak version of SigDirect, for boosting. Our goal is to strengthen the classier using less number of rules for prediction. Since, SigDirect is a strong learner and produces already a lesser number of rules for prediction, we form a weak version of SigDirect called wSigDirect, by further re ducing the number of rules to be used for classication as explained later in Sec tion 3. Moreover, in the proposed approach we use Adaboost [11] based boosting strategy over the ensemble of wSigDirect. The wSigDirect's classication model is learnt by running it multiple times on a reweighted data, thereafter performs voting over the learned classiers. We also propose ACbag which is dened as bagging on an ensemble of wSigDirect classiers. Motivated by the approach proposed by Breiman in [5], we use an ensemble model of wSigDirect classiers trained in parallel over dierent training datasets, and perform a majority vot ing over the ensemble for prediction. With the use of this strategy of combining weak learners, the goal is to decrease the variance in the prediction and improve the classication performance henceforth. It was found that for most of the datasets ACboost performs better than SigD2, ACbag, SVM, or ANN; ANN which performs similarly to deep neural network on these reasonably sized datasets. The main aim of this study is to make associative classiers more competitive and to highlight their signicanceBuilding a Competitive Associative Classier 3 as opposed to the other machine learning based classiers like neural networks which do not produce explainable predictions. Deep Learning has garnered all the attention lately, but the inability to produce transparent explanations for the decisions motivates us towards the domain of explainable articial intelligence using rulebased models which have fallen out of favour of late. Our contribution in this study is as follows: {We propose SigD2, an associative classier, which uses an eective two stage pruning strategy for pruning the rules to be used for classication. Using the proposed approach, the number of rules used for classication are reduced notably, without compromising on the classication performance. {We propose ACbag, an ensemble based classier founded on wSigDirect. {We also propose ACboost, which is boosting the wSigDirect classier, to im prove the classication accuracy with an explainable base model. Therefore, making SigDirect more competitive for classication tasks. The rest of the paper is organized as follows: Section 2 gives a literature review about some previously proposed associative classiers, Section 3 explains the methodologies we have adapted in SigD2, ACbag and ACboost, Section 4 shows the evaluation results of our proposed classier on UCI datasets and lastly, Section 5 gives the conclusion of the work and directions about future investi gations. 2 Related Work "
322,Pick up the PACE: Fast and Simple Domain Adaptation via Ensemble Pseudo-Labeling.txt,"Domain Adaptation (DA) has received widespread attention from deep learning
researchers in recent years because of its potential to improve test accuracy
with out-of-distribution labeled data. Most state-of-the-art DA algorithms
require an extensive amount of hyperparameter tuning and are computationally
intensive due to the large batch sizes required. In this work, we propose a
fast and simple DA method consisting of three stages: (1) domain alignment by
covariance matching, (2) pseudo-labeling, and (3) ensembling. We call this
method $\textbf{PACE}$, for $\textbf{P}$seudo-labels, $\textbf{A}$lignment of
$\textbf{C}$ovariances, and $\textbf{E}$nsembles. PACE is trained on top of
fixed features extracted from an ensemble of modern pretrained backbones. PACE
exceeds previous state-of-the-art by $\textbf{5 - 10 \%}$ on most benchmark
adaptation tasks without training a neural network. PACE reduces training time
and hyperparameter tuning time by $82\%$ and $97\%$, respectively, when
compared to state-of-the-art DA methods. Code is released here:
https://github.com/Chris210634/PACE-Domain-Adaptation","Deep learning is infamous for requiring a large amount of labeled data to achieve stateoftheart results, but in many applications, labeled data is expensive to obtain [ 1,2,3]. In the last few years, new subdisciplines of deep learning have emerged to address this overreliance on labeled data. Selfsupervised learning learns useful representations of data through handcrafted augmentations instead of labels [ 4,5].Semisupervised learning learns from a small amount of labeled data and a large amount of unlabeled data from the same distribution [ 6].Domain Adaptation (DA) is similar to semisupervised learning, but considers the case where the labeled and unlabeled data come from different distributions [ 7,8]. We call the labeled data the source domain and the unlabeled data the target domain. For example, consider the following scenario. A company wants to train a speech recognition system for deployment in a noisy car environment (target domain). A large amount of labeled recordings is available for indoor environments (source domain), but a limited amount of labeled target data is available. DA addresses this problem by minimizing the empirical risk on source data while encouraging features to be domain invariant. We consider both Semisupervised Domain Adaptation (SSDA) [9], where a small amount of labeled target data is available, and Unsupervised Domain Adaptation (UDA) [10], where no labeled target data is available. Many modern DA methods train a feature extractor and linear classiÔ¨Åer to minimize a weighted sum of three losses: (1) cross entropy loss on labeled source data [ 10,9,11,12], (2) divergence loss between source and target domain features [ 10,9,11,13] and (3) consistency loss between augmented views of unlabeled target data [ 14,15,16]. Generally, better performing methods require large batch sizes and a large number of hyperparameters [ 17,15,12,16,14]. For example, CDAC [15], a stateoftheart method in SSDA, requires a batch of labeled data, a batch of unlabeled data, and two augmented batches of unlabeled data. On a small feature extractor, such as ResNet34, Preprint. Under review.arXiv:2205.13508v1  [cs.LG]  26 May 2022Figure 1: Illustration of the PACE framework. A solid arrow indicates a forward pass. A dashed arrow indicates a backward pass. PACE consists of three steps: (1) the source features Xsare transformed such that their covariances match the target feature Xtucovariances, (2) a linear classiÔ¨Åer is trained with logistic loss on source data and pseudolabeled target data, and (3) the predictions from ensemble members are averaged to return an aggregate prediction. tuning and running this method is feasible. However, scaling this type of method to large modern feature extractors would require several GPUs working in parallel [ 18,19]. Computationally efÔ¨Åcient training is important for two reasons: (1) there is signiÔ¨Åcant interest in energyefÔ¨Åcient training on resourceconstrained edge devices [ 20,21], and (2) algorithms which achieve stateoftheart with low computational cost foster democratic research [ 22]. This raises the question: is it possible to leverage high quality features from large modern backbones, using only one GPU, and still achieve competitive DA results? The answer is yes! Present work: In this paper, we propose an efÔ¨Åcient DA method PACE (Pseudolabels, Alignment ofCovariances, and Ensembles), which uses predeep learning DA methods on top of Ô¨Åxed features extracted from an ensemble of ConvNeXt [ 19] and Swin [ 18] backbones pretrained on ImageNet [23]. Our method consists of three stages (see Figure 1): (1) We align the source and target feature distributions by matching their covariances with CORAL [ 24]. (2) We use selftraining with conÔ¨Ådence thresholding to align the classconditional feature distributions between source and target domains. (3) We run the Ô¨Årst two steps with different pretrained backbones and average the predictions. We hypothesize that the features extracted by the collection of pretrained backbones are not perfectly correlated because of differences in pretraining and architecture. Therefore, we should observe a boost in accuracy when combining predictions from the ensemble. Our contributions are: ‚Ä¢We revisit CORAL [ 24], a simple method for domain alignment that is not widely used by stateoftheart DA methods. We show that when combined with selftraining and ensembling, CORAL offers a boost in target accuracy without adding much complexity. ‚Ä¢We propose PACE, which reduces training time and hyperparameter tuning time by 82% and 97%, respectively (see Figure 2) while exceeding stateoftheart SSDA and UDA methods by 510% on most benchmark tasks (see Tables 2, 3 and 4). 2 Related Work "
323,Continuous Adaptation of Multi-Camera Person Identification Models through Sparse Non-redundant Representative Selection.txt,"The problem of image-base person identification/recognition is to provide an
identity to the image of an individual based on learned models that describe
his/her appearance. Most traditional person identification systems rely on
learning a static model on tediously labeled training data. Though labeling
manually is an indispensable part of a supervised framework, for a large scale
identification system labeling huge amount of data is a significant overhead.
For large multi-sensor data as typically encountered in camera networks,
labeling a lot of samples does not always mean more information, as redundant
images are labeled several times. In this work, we propose a convex
optimization based iterative framework that progressively and judiciously
chooses a sparse but informative set of samples for labeling, with minimal
overlap with previously labeled images. We also use a structure preserving
sparse reconstruction based classifier to reduce the training burden typically
seen in discriminative classifiers. The two stage approach leads to a novel
framework for online update of the classifiers involving only the incorporation
of new labeled data rather than any expensive training phase. We demonstrate
the effectiveness of our approach on multi-camera person re-identification
datasets, to demonstrate the feasibility of learning online classification
models in multi-camera big data applications. Using three benchmark datasets,
we validate our approach and demonstrate that our framework achieves superior
performance with significantly less amount of manual labeling.","Person identiÔ¨Åcation/recognition across cameras is an important problem in many surveillance and security applications, and requires the ability to handle very large data volumes. An automated solution of this problem is to use images of persons to pro vide identities to the images based on learned models that describe his/her appearance. Most existing solutions depend on learning a static model on tediously labeled training data. An example of such a task is person reidentiÔ¨Åcation [1, 2, 3, 4] which is the task of identifying and monitoring people moving across a number of nonoverlapping cameras. Considering the time and labor involved in labeling the training data manu ally, scalability to large numbers of persons remains an issue. Also, the learned models being static, cannot adapt to new images that may be available over time. Some recent semisupervised [5] and unsupervised methods [6, 7, 8] tried to ex plore saliency information or weighted features or learning a discriminative null space in a reidentiÔ¨Åcation scenario. However, none of these works assume a continuous learning setting. Moreover, it can be seen that unsupervised methods give signiÔ¨Åcantly lower performance compared to supervised methods [6, 7, 8]. A next alternative is to involve a human in the loop but at the same time efforts should be made to keep the human annotation to a minimum. Active learning [9] is a natural choice for reducing labeling effort by asking for labels only on a few but informative samples (called the active samples), rather than seeking labels either for samples chosen randomly from a set or for the whole set. In this paper, we explore the question of learning person identi Ô¨Åcation models online in a multicamera settings with limited labeling effort. We argue that, in order to truly reduce the labeling cost we need to choose a sparse but informa tive set of samples to be labeled. As only a small part of the whole data is annotated, the annotation effort is reduced considerably compared to annotating the whole dataset. Active learning has been successfully applied to many computer vision problems in 2cluding tracking [10], object detection [11], image [12] and video segmentation [13], image or scene classiÔ¨Åcation [14, 15, 16] and activity recognition [17, 18, 19] How ever, these methods deal with data coming from single source. It is not trivial to extend traditional active learning methods for an application scenario where multisensor data is involved. It is a natural challenge to select a few informative samples yet cover as much appearance variation as possible across multiple cameras in such a scenario. Apart from high cost of labeling the training data, all the data may not also be available at the very outset. A static pretrained model can not adapt to the changing dynamics of the incoming data. In this work, we will address both the abovementioned scenarios a). selection of a manageable set of informative samples for labeling and b). doing so in an online manner where all the training data is not available a priori. For this purpose, we propose an iterative framework which, starting with a pool of unlabeled images, progressively and judiciously selects the most informative set of images  termed as the ‚Äò representative ‚Äô images for labeling with minimal overlap with previously labeled images. Ideally, a set of representative images are ‚Äúrepresentatives‚Äù of a dataset because this set possesses most of the variabilities of the dataset within itself. On the other hand, without any label information, the representative images are some of the most confusing samples in the whole dataset by the same trait. Thus anno tating such representatives enriches the model by injecting valuable information with a reasonable labeling effort. We also use a structure preserving sparse reconstruction based classiÔ¨Åer to reduce the training burden typically seen in discriminative classiÔ¨Åers. The use of a sparse classiÔ¨Åer enables an online update of the identiÔ¨Åcation framework involving only new samples without requiring to train from scratch whenever new batch of data arrives. This pipeline leads to a framework for online update of the classiÔ¨Åers involving only the incorporation of new labeled data rather than any expensive training phase. Identifying and eliminating redundant samples is especially important in such an online scenario since reducing redundancy implies more information gain at the cost of less labeling effort. Thus the proposed work addresses the following question: Is it possible to select a sparse set of nonredundant training images progressively in an online setting for annotation from multisensor data while maintaining good identiÔ¨Å cation performance? 3We demonstrate the effectiveness of our proposed approach on datasets in per son reidentiÔ¨Åcation (although our problem setting is different than the traditional re identiÔ¨Åcation framework). There are many reasons for it. Using the reidentiÔ¨Åcation datasets allows us to demonstrate the effectiveness of the online representative selection framework where due to a multicamera setting, large intraperson variation is preva lent. Also, these datasets represent uncontrolled settings where we are not dependent on the availability of good quality facial shots. 1.1. Motivation behind the Proposed Approach The representatives or samples chosen iteratively for labeling can have two types of redundancies. Firstly, in each iteration, the chosen representatives may have many images of the same person. Secondly, representatives selected in subsequent iterations may also have overlap with the representatives chosen earlier for labeling. The Ô¨Årst type of redundancy is termed as the ‚Äòintraiteration redundancy‚Äô while the second type is termed as the ‚Äòinteriteration redundancy‚Äô. ‚ÄòIntraiteration redundancy‚Äô is restricted by exploiting the fact that redundant samples in any iteration are very close neighbors in the feature space. Without any feedback about the already chosen representatives, any representative selection strategy may select images of the same person as repre sentatives in subsequent iterations. Using a similar redundancy reduction strategy of looking for close neighbors as above, we will be able to Ô¨Ålter out samples redundant to the already labeled ones in previous iterations. However, using such a strategy to reduce ‚Äòinteriteration redundancy‚Äô will prohibit the information gain as images of a person from multiple cameras will be hard to come by. We tackle this situation by en forcing diversity among the selected representatives as information about the already chosen samples in previous iterations are fed back while choosing subsequent samples to be labeled. Variabilities are not only caused by the presence of different people but the same person may appear differently in different cameras. These two different types of variabilities make nonredundant representative selection a challenge in scenarios where there are multiple sources of data as is the case with multicamera person iden tiÔ¨Åcation. The proposed method exploits these variabilities by choosing diverse but small set of representatives from multiple cameras (ref. section 4.3.2) while discarding 4similar images of the same person which primarily comes from the same camera (ref. section 4.3.1). Such a representative selection problem is formulated as a convex optimization that minimizes the cost of representing an unlabeled pool with a sparse set of representa tives as well as one that minimizes the redundancy with the representatives selected earlier. Experiments on three benchmark datasets show that annotating the small but informative set of representative images reduces the labeling effort considerably, main taining reasonable identiÔ¨Åcation performance. Apart from the huge labeling effort, another factor that is a challenge for a scal able solution of the problem is the generally exponential increase of training time with the number of training samples for traditional discriminative classiÔ¨Åers ( e.g., SVM or random forest). These classiÔ¨Åers have to be retrained from scratch after each batch of representative selection and annotation in such repetitive active learning strategy. The generally super linear time complexity of the traditional discriminative classiÔ¨Åers makes them unsuitable for use in such a scenario. Though incremental learning based classiÔ¨Åers [20] can update the model without retraining from scratch, their performance is limited by the condition of knowing the number of classes from the start. Motivated by the recent progress of sparse coding based classiÔ¨Åers [21, 22], we employ a structure preserving sparse dictionary for classiÔ¨Åcation. Such a classiÔ¨Åcation strategy is helpful as updating the model with newly labeled data means simply adding the new samples with labels without making any changes to the existing dictionary elements made of the already labeled samples. This model update strategy not only helps in reducing the training time signiÔ¨Åcantly by avoiding the need for retraining but also enables the operation of the framework without assuming any knowledge of the number of classes. Thus, in summary, the proposed framework uses two convex optimization based strategies to select a few informative but nonredundant samples for labeling and to update a person identiÔ¨Åcation model online . The rest of the paper is organized as follows. Section 2 discusses the related works and our contributions. An overview of the proposed approach is given in section 3 . The details about approach, as nonredundant representative selection, and the use of structure preserving sparse coding based classiÔ¨Åcation are described in section 4. 5Experimental results and comparisons are shown in section 5. Finally, conclusions are drawn in section 6. 2. Related Works and Our Contributions "
324,Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation.txt,"Semantic segmentation with limited annotations, such as weakly supervised
semantic segmentation (WSSS) and semi-supervised semantic segmentation (SSSS),
is a challenging task that has attracted much attention recently. Most leading
WSSS methods employ a sophisticated multi-stage training strategy to estimate
pseudo-labels as precise as possible, but they suffer from high model
complexity. In contrast, there exists another research line that trains a
single network with image-level labels in one training cycle. However, such a
single-stage strategy often performs poorly because of the compounding effect
caused by inaccurate pseudo-label estimation. To address this issue, this paper
presents a Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and
SSSS. The SLRNet uses cross-view self-supervision, that is, it simultaneously
predicts several complementary attentive LR representations from different
views of an image to learn precise pseudo-labels. Specifically, we reformulate
the LR representation learning as a collective matrix factorization problem and
optimize it jointly with the network learning in an end-to-end manner. The
resulting LR representation deprecates noisy information while capturing stable
semantics across different views, making it robust to the input variations,
thereby reducing overfitting to self-supervision errors. The SLRNet can provide
a unified single-stage framework for various label-efficient semantic
segmentation settings: 1) WSSS with image-level labeled data, 2) SSSS with a
few pixel-level labeled data, and 3) SSSS with a few pixel-level labeled data
and many image-level labeled data. Extensive experiments on the Pascal VOC
2012, COCO, and L2ID datasets demonstrate that our SLRNet outperforms both
state-of-the-art WSSS and SSSS methods with a variety of different settings,
proving its good generalizability and efficacy.","Semantic segmentation is a fundamental computer vi sion task that aims to assign a label to each pixel, promoting the development of many downstream tasks, such as scene parsing, autonomous driving, and medi cal image analysis (Chen et al., 2018; Zhou et al., 2019; Havaei et al., 2017). Recently, deep learning based se mantic segmentation models (Long et al., 2015; Chen et al., 2018), trained with largescale data labeled at pixel level, have achieved impressive progress. However, such supervised approaches require intensive manual annotations that are timeconsuming and expensive, which have inspired many investigations about learn ing with lowcost annotations, such as semisupervisedarXiv:2203.10278v1  [cs.CV]  19 Mar 20222 Junwen Panet al. I II IPùëì!(I) PP""P#MVMCCVLRùëì$%&ùëì!!(I)ùëì!""(I)ùëì!(I"")ùëì!(I#)ùëì$%&ùëì'$&ùëì'$&(I) 1StageNet(II) CrossPseudoSeg (III) PseudoSeg(IV) SLRNetùë°(ùë°)ùë°""ùë°# Fig. 1: Overview of pseudo supervision architectures: (I) Single pseudo supervision for WSSS (Araslanov and Roth, 2020), (II) Cross pseudo supervision for SSSS (Chen et al., 2021), (III) PseudoSeg supervision for SSSS (Zou et al., 2021), and (IV) The SLRNet with MVMC and CVLR can mitigate the compounding ef fect of pseudo supervision error. ` !' means the forward operation, ` 99K' means pseudo supervision, `/' on ` !' means stopgradient and pseudolabel generation. tde notes the transformation operator, sandware short for \strong"" and \weak"" respectively. semantic segmentation (SSSS) with limited amounts of labeled data, weakly supervised semantic segmenta tion (WSSS) with bounding boxes (Dai et al., 2015), scribbles (Lin et al., 2016), points (Bearman et al., 2016), and imagelevel labels (Kolesnikov and Lampert, 2016). Nevertheless, there is a considerable gulf between weakly supervised and semisupervised approaches. Most popular imagelevel WSSS methods (Ahn et al., 2019; Dong et al., 2020; Sun et al., 2020) resort to multiple training and renement stages to obtain more accurate pseudolabels while avoiding error accumulation. These methods often start from a weakly supervised localization, such as a class activation map (CAM) (Zhou et al., 2016), which highlights the most discriminative regions in an image. In this approach, diverse enhanced CAMgenerating networks (Lee et al., 2019; Wang et al., 2020b; Sun et al., 2020) and CAMrenement procedures (Ahn and Kwak, 2018; Ahn et al., 2019; Shimoda and Yanai, 2019) have been designed to expand the highlighted area to the entire object or eliminate the wrongly highlighted area. Although these multistage methods can produce more accurate pseudolabels, they suer from the need of a large number of hyperparameters and complex training procedures. Singlestage WSSS methods (Zheng et al., 2015; Papandreou et al., 2015) have received less attention because their segmentation is less accurate than that of multistage methods. Re cently, Araslanov and Roth (2020) proposed a simple singlestage WSSS model that generates pixellevelpseudolabels online as selfsupervision (Fig. 1 (I)). However, its accuracy is still not comparable with that of multistage approaches. In contrast, the simple online pseudosupervision scheme has made promising progress in SSSS (Fig. 1(II) (Chen et al., 2021) and (III) (Zou et al., 2021)). We argue that the cause of the inferior performance of the online pseudo supervised WSSS is the com pounding eect of errors caused by online inaccurate pseudo supervision. Like multistage renements, on line pseudolabel supervision should gradually improve the semantic delity and completeness during the training process. However, this also increases the risk that errors are mimicked and accumulated with the gradient  ows being backpropagated from the top to the lower layers. Consistency learning is widely used as additional supervision to semisupervised learn ing (Ouali et al., 2020; Chen et al., 2021). However, in practice, existing consistencybased methods are not applicable to imagelevel weakly supervised settings. First, they require pixellevel supervision to avoid the collapsing solution (Chen and He, 2021). Second, the dominance of consistency harms the region expansion for WSSS. To this end, we propose the Selfsupervised Low Rank Network (SLRNet) for singlestage WSSS and SSSS. As illustrated in Fig. 1(IV), the SLRNet simultaneously predicts several segmentation masks for various augmented versions of one image, which are jointly calibrated and rened by a multiview mask cal ibration (MVMC) module to generate one pseudomask for selfsupervision. The pseudomask leverages the complementary information from various augmented views, which enforces the crossview consistency on the predictions. To further regularize the network, the SLRNet introduces the LR inductive bias implemented by a crossview lowrank (CVLR) module. The CVLR exploits the collective matrix factorization to jointly decompose the learned representations from dierent views into submatrices while recovering a clean LR signal subspace. Through the dictionary shared over dierent views, a variety of related features from various views can be rened and amplied to eliminate the ambiguities or false predictions. Thereby, the input features of the decoder deprecate noisy information, and this can eectively prevent the network from over tting to the false pseudolabels. Additionally, instead of directly randomly initializing the submatrices, a latent space regularization is designed to improve the optimization eciency. The SLRNet is an ecient and elegant framework that generalizes well to dierent labelecient seg mentation settings without additional training phases.Learning SelfSupervised LowRank Network for SingleStage Weakly and SemiSupervised Semantic Segmentation 3 For instance, to simultaneously utilize imagelevel and pixellevel labels, previous SSSS methods (Lee et al., 2019; Wei et al., 2018) have to generate and rene pseudolabels oine using WSSS model, which are bundled with pixellevel labels to train a network in the next stage. Such a multistage scheme provides a marginal improvement over dedicated SSSS algo rithms (Ouali et al., 2020; Zou et al., 2021) with unlabeled data. In contrast, the SLRNet directly introduces additional pixellevel supervision while combining it with imagelevel data without extra cost. In other words, the online pseudomask generation takes into account both imagelevel and pixellevel labels in a single training phase and is undoubtedly more accurate. To the best of our knowledge, the SLRNet is the rst attempt to bridge these tasks into a unied singlestage scheme, allowing it to maximize exploiting various annotations with a limited budget. In our experiments, we rst validate the perfor mance of SLRNet in an imagelevel WSSS setting on several datasets, including Pascal VOC 2012 (Ev eringham et al., 2010), COCO (Lin et al., 2014), and L2ID (Wei et al., 2020). Extensive experiments demonstrate that the crossview supervision and the CVLR help improve semantic delity and completeness of the generated segmentation masks. Notably, the SLRNet also establishes new stateofthearts for various labelecient semantic segmentation tasks, including 1) WSSS with imagelevel labeled data, 2) SSSS with pixellevel and imagelevel labeled data and 3) SSSS with pixellevel labeled and unlabeled data. Moreover, the SLRNet achieves the best performance at the WSSS Track of CVPR 2021 Learning from Limited and Imperfect Data (L2ID) Challenge (Wei et al., 2020), outperforming other competitors by large margins of9:35% in terms of mIoU. The main contributions of this work are summarized as follows: 1) We propose an eective crossview selfsupervision scheme, incorporating the CVLR module, to allevi ate the compounding eect of selfsupervision errors for the online pseudolabel training. 2) We present a plugandplay collective matrix factor ization method with latent space regularization for multiview LR representation learning, which can be readily embedded into any Siamese networks for endtoend training. 3) The SLRNet provides a unied framework that can be well generalized to learn a segmentation model from dierent limited annotations in various WSSS and SSSS settings. 4) The SLRNet achieves leading performance com pared to a variety of stateoftheart methods onPascal VOC 2012, COCO, and L2ID datasets for both WSSS and SSSS tasks. 2 Related Work "
325,Model Composition: Can Multiple Neural Networks Be Combined into a Single Network Using Only Unlabeled Data?.txt,"The diversity of deep learning applications, datasets, and neural network
architectures necessitates a careful selection of the architecture and data
that match best to a target application. As an attempt to mitigate this
dilemma, this paper investigates the idea of combining multiple trained neural
networks using unlabeled data. In addition, combining multiple models into one
can speed up the inference, result in stronger, more capable models, and allows
us to select efficient device-friendly target network architectures. To this
end, the proposed method makes use of generation, filtering, and aggregation of
reliable pseudo-labels collected from unlabeled data. Our method supports using
an arbitrary number of input models with arbitrary architectures and
categories. Extensive performance evaluations demonstrated that our method is
very effective. For example, for the task of object detection and without using
any ground-truth labels, an EfficientDet-D0 trained on Pascal-VOC and an
EfficientDet-D1 trained on COCO, can be combined to a RetinaNet-ResNet50 model,
with a similar mAP as the supervised training. If fine-tuned in a
semi-supervised setting, the combined model achieves +18.6%, +12.6%, and +8.1%
mAP improvements over supervised training with 1%, 5%, and 10% of labels.","Deep learning has enabled achieving outstanding results on a wide range of applications in computer vision and image processing [5, 27]. However, the diversity of datasets and neural network architectures necessitates a careful selection of model architecture and training data that match best to the target application. Often times, for a same task, many models are available. These models might be trained on different datasets, or might come in different capacities, architectures, or even bit precisions. Motivation: A natural question that arises in this case, is whether we can combine the neural networks so that one combined network can perform the same task as several input networks. Fig. 1 shows an example, where two input object detection models to detect ‚Äòper son‚Äô and ‚Äòvehicle‚Äô are combined in one model. The beneÔ¨Åts of combining models include: a) possible latency improvements due to running one inference as opposed to many, b) in case input models cover partially overlapping or nonoverlapping classes/categories, one can build a stronger model with the union of the classes/categories through model composition ¬© 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2110.10369v1  [cs.LG]  20 Oct 20212 BANITALEBIDEHKORDI, KANG, AND ZHANG: MODEL COMPOSITION Figure 1: An example of the proposed model composi tion approach for object detec tion. Two input models for detecting ‚Äòperson‚Äô and ‚Äòvehicle‚Äô categories are combined to cre ate one model that does both. (i.e. merging models‚Äô skills as in Fig. 1), and c) for applications involving model deploy ment, e.g. for cloud services providers, it can reduce the deployment frequency/load. Challenges: Creating a combined model from several input models is a challenging task. First, depending on the target task, the output model may need to have a speciÔ¨Åc architecture, and not necessarily one that is dictated by the input models. The input models themselves also might have different architectures. Second, in case input models are provided by users of a cloud system, or by different model creators/clients, the individual model owners would likely prefer not to share their training data, labels, not even weights or code. A privacy pre serving model composition approach should rely on only a minimum amount of information from the model creators. Third, input models may have only partially overlapping or disjoint class categories. This imposes a major challenge when combining the individual models. Existing methods: The existing solutions are mostly based on techniques such as knowl edge distillation [6, 16, 34] or ensembling [35], which may be useful when classes/categories are identical and labeled data are available, but not for the case of arbitrary classes/categories with only unlabeled data. More details regarding the existing approaches are provided in sec tion 2. In summary, to the best of our knowledge, the existing methods do not fully address the three challenges mentioned above. Our contributions: In this paper, we propose a simple yet effective method to address the model composition of neural networks. Our method supports combination of an arbitrary number of networks with arbitrary architectures. To train a combined model, we leverage the abundance of unlabeled data and having labels or original training data of the input models is not a requirement. However, if any labeled data are available, the algorithm uses them to further boost the performance of the output model. Furthermore, we put no restrictions on the type and number of object categories of the input models. We demonstrate the effectiveness of our method through an extensive set of experiments for the task of object detection. 2 Related works "
326,Categorizing Items with Short and Noisy Descriptions using Ensembled Transferred Embeddings.txt,"Item categorization is a machine learning task which aims at classifying
e-commerce items, typically represented by textual attributes, to their most
suitable category from a predefined set of categories. An accurate item
categorization system is essential for improving both the user experience and
the operational processes of the company. In this work, we focus on item
categorization settings in which the textual attributes representing items are
noisy and short, and labels (i.e., accurate classification of items into
categories) are not available. In order to cope with such settings, we propose
a novel learning framework, Ensembled Transferred Embeddings (ETE), which
relies on two key ideas: 1) labeling a relatively small sample of the target
dataset, in a semi-automatic process, and 2) leveraging other datasets from
related domains or related tasks that are large-scale and labeled, to extract
""transferable embeddings"". Evaluation of ETE on a large-scale real-world
dataset provided to us by PayPal, shows that it significantly outperforms
traditional as well as state-of-the-art item categorization methods.","Online shopping websites have become extremely popular in recent years. In particular, marketplaces such as eBay.com and Amazon.com accept mil Corresponding author Email addresses: had.yonatan@gmail.com (Yonatan Hadar), shmueli@tau.ac.il (Erez Shmueli) 1arXiv:2110.11431v1  [cs.LG]  21 Oct 2021lions of new items every day (Cevahir & Murakami, 2016). To better cope with the enormous number of items, companies typically organize items into a predened set of categories (Shen et al., 2012). Such categorization of items1 is essential for enhancing user experience, where it allows users to search and navigate more easily between items, receive better recommendations for relevant items, and view customized descriptions of items. Categorization of items is also important for improving operational processes of the company, such as targeted advertising, determining shipping and handling fees, fraud detection, identication of duplicate items, and enforcement of company poli cies (e.g., which items are allowed or forbidden to be sold on the website) (Shen et al., 2012). While many item attributes can only be assigned by humans, the category of a given item can be inferred automatically from other attributes of that item. Automatic categorization of items is important both for saving the costs associated with manual labeling of millions of items every day and for ensuring the consistency of categories, which quickly becomes a serious prob lem when several individuals are involved in the labeling process (Kozareva, 2015). Indeed, many studies have investigated the problem of item categoriza tion as a machine learning text classication task. Earlier studies relied on representing text as a vector in a multidimensional space of identiers (e.g., single index terms or Ngrams) using various weighting schemes (e.g., TFIDF) (Ding et al., 2002; Yu et al., 2012; Mathivanan et al., 2018; Sun et al., 2014). However, these methods share the following limitations: 1) they typically produce training matrices which are very high dimensional and consequently very sparse; 2) the semantic meaning of words and their relationship to other words is typically ignored; and 3) the context of words (e.g., where they appear in the sentence, after which word, etc.) is rarely taken into account. With the growing popularity of deep learning, studies have started to show the applicability of deep learning also in the domain of item catego rization (Das et al., 2016; Ha et al., 2016; Cevahir & Murakami, 2016; Li et al., 2018; Krishnan & Amarthaluri, 2019; Chen et al., 2019). For ex 1While we name this task \item categorization"", it is important to note that a more accurate term would be \item classication"". Nevertheless, we stick with the term \item categorization"" as it is widely used in the literature, for example by Shen et al. (2012); Ha et al. (2016). 2ample, Ha et al. (2016) demonstrated how using multiple Recurrent Neural Network (RNN) layers for encoding textual item attributes can signicantly outperform traditional bagofwords methods. More recently, Krishnan & Amarthaluri (2019) showed how to combine both textual and nontextual item attributes as part of Long Short Term Memory network (LSTM) and Convolutional Neural Network (CNN) layers for improving classication per formance. However, a major limitation of deep learning methods is the need for very large labeled datasets to train on. Recent studies have tried to overcome the aforementioned limitation of deep learning models by using general purpose embeddings such as Word2Vec (Kozareva, 2015) and general purpose pretrained models such as BERT (Za hera & Sherif; Yang et al., 2020) and CamemBERT (Verma et al., 2020; Lee et al., 2020). In both cases, the idea is to train once deep learning models on a very large generalpurpose dataset, usually based on news articles, books, or Wikipedia pages. Then, the obtained embeddings or pretrained models are adjusted to the problem at hand, by using a considerably smaller labeled dataset from that problem's domain. For example, the two bestperforming solutions (Verma et al., 2020; Lee et al., 2020) in the ecommerce item cat egorization challenge as part of SIGIR 2020, used CamemBERT as a text encoder for item descriptions. However, as we demonstrate later in this pa per, the generalpurpose nature of these methods makes them less suitable for domain specic problems. In this paper, we focus on item categorization settings in which: 1) item descriptions are relatively short and noisy, and 2) labeled data for the tar get dataset is unavailable. Such settings entail that deep learning techniques cannot be applied directly on the target dataset, and generalpurpose embed ding might be less appropriate to use since the text distribution of the target dataset may dier greatly from that of the generalpurpose corpus they were trained with. To address such settings, we propose a novel learning framework, Ensem bled Transferred Embedding (ETE), which has four main steps: 1) manually label a small sample dataset; 2) extract embeddings from related largescale labeled datasets; 3) train transferred models using the extracted transferred embeddings and the labels of the sample dataset; and 4) build an ensemble to combine the outputs of the dierent transferred models into a single pre diction. We then show the applicability of the proposed framework to the item categorization task in settings for which item descriptions are noisy and short, and labels are not available. 3Extensive evaluation that we conducted, using a largescale realworld invoice dataset provided to us by PayPal, shows that our method signicantly outperforms all other considered traditional (e.g., TFIDF) as well as state oftheart (e.g., methods based on general purpose pretrained models such as BERT) item categorization methods. The contribution of this paper is threefold: ‚Ä¢We propose the ETE learning framework which relies on labeling a rel atively small sample of the target dataset, in a semiautomatic process, and leveraging other largescale labeled datasets from related domains or related tasks. ‚Ä¢We show the applicability of the proposed framework for the case of item categorization with short and noisy item descriptions. ‚Ä¢An extensive evaluation that we conducted demonstrates the superior ity of our method compared to traditional as well as stateoftheart text classication methods. The rest of this paper is structured as follows: In Section 2, we review the background and related work to this study. In Section 3, we describe the datasets we used in this study. In ection 4, we describe the proposed ETE learning framework and how it can be applied to our item categorization setting. Section 5 discusses the experimental setting and the results. Section 6 summarizes this paper and suggests directions for future work. 2. Related Work "
327,ADMoE: Anomaly Detection with Mixture-of-Experts from Noisy Labels.txt,"Existing works on anomaly detection (AD) rely on clean labels from human
annotators that are expensive to acquire in practice. In this work, we propose
a method to leverage weak/noisy labels (e.g., risk scores generated by machine
rules for detecting malware) that are cheaper to obtain for anomaly detection.
Specifically, we propose ADMoE, the first framework for anomaly detection
algorithms to learn from noisy labels. In a nutshell, ADMoE leverages
mixture-of-experts (MoE) architecture to encourage specialized and scalable
learning from multiple noisy sources. It captures the similarities among noisy
labels by sharing most model parameters, while encouraging specialization by
building ""expert"" sub-networks. To further juice out the signals from noisy
labels, ADMoE uses them as input features to facilitate expert learning.
Extensive results on eight datasets (including a proprietary enterprise
security dataset) demonstrate the effectiveness of ADMoE, where it brings up to
34% performance improvement over not using it. Also, it outperforms a total of
13 leading baselines with equivalent network parameters and FLOPS. Notably,
ADMoE is model-agnostic to enable any neural network-based detection methods to
handle noisy labels, where we showcase its results on both multiple-layer
perceptron (MLP) and the leading AD method DeepSAD.","Anomaly detection (AD), also known as outlier detection, is a crucial learning task with many realworld applications, in cluding malware detection (Nguyen et al. 2019), antimoney laundering (Lee et al. 2020), raredisease detection (Li et al. 2018) and so on. Although there are numerous detection al gorithms (Aggarwal 2013; Pang et al. 2021; Zhao, Rossi, and Akoglu 2021; Liu et al. 2022), existing AD methods as sume the availability of (partial) labels that are clean (i.e. without noise), and cannot learn from weak/noisy labels1. Simply treating noisy labels as (pseudo) clean labels leads to biased and degraded models (Song et al. 2022). Over the years, researchers have developed algorithms for classiÔ¨Åca tion and regression tasks to learn from noisy sources (Ro drigues and Pereira 2018; Guan et al. 2018; Wei et al. 2022), which has shown great success. However, these methods are *The project is primarily done at Microsoft Research. Copyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved. 1We use the terms noisy andweak interchangeably. /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000003/uni00000054/uni00000058/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni0000004b/uni0000004c/uni0000004a/uni0000004b/uni00000048/uni00000055/uni00000012/uni00000046/uni0000004f/uni00000048/uni00000044/uni00000051/uni00000048/uni00000055/uni00000003/uni00000057/uni00000052/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000055/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000000c/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni0000004b/uni0000004c/uni0000004a/uni0000004b/uni00000048/uni00000055/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000045/uni00000048/uni00000057/uni00000057/uni00000048/uni00000055/uni0000000c /uni0000003b/uni0000002a/uni00000025/uni00000032/uni00000027 /uni00000033/uni00000055/uni00000048/uni00000031/uni00000048/uni00000057 /uni00000027/uni00000048/uni00000059/uni00000031/uni00000048/uni00000057 /uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000024/uni00000027/uni0000002f/uni0000002a/uni00000025 /uni00000030/uni0000002f/uni00000033 /uni00000024/uni00000027/uni00000030/uni00000052/uni00000028/uni0000000e/uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000024/uni00000027 /uni00000024/uni00000027/uni00000030/uni00000052/uni00000028/uni0000000e/uni00000030/uni0000002f/uni00000033(a)Comparison with leading AD methods /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000003/uni00000054/uni00000058/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni0000004b/uni0000004c/uni0000004a/uni0000004b/uni00000048/uni00000055/uni00000012/uni00000046/uni0000004f/uni00000048/uni00000044/uni00000051/uni00000048/uni00000055/uni00000003/uni00000057/uni00000052/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000055/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000000c/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni0000004b/uni0000004c/uni0000004a/uni0000004b/uni00000048/uni00000055/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000045/uni00000048/uni00000057/uni00000057/uni00000048/uni00000055/uni0000000c /uni00000036/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000031/uni00000052/uni0000004c/uni00000056/uni0000005c /uni00000030/uni00000044/uni0000004d/uni00000052/uni00000055/uni00000039/uni00000052/uni00000057/uni00000048 /uni0000002b/uni00000028/uni00000042/uni00000024 /uni0000002b/uni00000028/uni00000042/uni00000030/uni00000026/uni00000055/uni00000052/uni0000005a/uni00000047/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000038/uni00000051/uni0000004c/uni00000052/uni00000051/uni00000031/uni00000048/uni00000057 /uni00000024/uni00000027/uni00000030/uni00000052/uni00000028/uni00000010/uni00000030/uni0000002f/uni00000033 (b)Comp. w/ noisy learning methods Figure 1: Performance (ROCAUC) comparison on Yelp (see results on all datasets in ¬ß4.2 and 4.3), where ADMoE outperforms two groups of baselines: (a) SOTA AD meth ods; (b) leading classiÔ¨Åcation methods for learning from multiple noisy sources. ADMoE enhanced DeepSAD and MLP are denoted as ADandAM. not tailored for AD with extreme data imbalance, and exist ing AD methods cannot learn from (multiple) noisy sources. Why is it important to leverage noisy labels in AD appli cations? Taking malware detection as an example, it is im possible to get a large number of clean labels due to the data sensitivity and the cost of annotation. However, often there exists a large number of weak/noisy historical security rules designed for detecting malware from different perspectives, e.g., unauthorized network access and suspicious Ô¨Åle move ment, which have not been used in AD yet. Though not as perfect as human annotations, they are valuable as they en code prior knowledge from past detection experiences. Also, although each noisy source may be insufÔ¨Åcient for difÔ¨Åcult AD tasks, learning them jointly may build competitive mod els as they tend to complement each other. In this work, we propose ADMoE, (to our knowledge) theÔ¨Årst weaklysupervised approach for enabling anomaly detection algorithms to learn from multiple sets of noisy labels . In a nutshell, ADMoE enhances existing neural networkbased AD algorithms by Mixtureofexperts (MoE) network(s) (Jacobs et al. 1991; Shazeer et al. 2017), which has a learnable gating function to activate different sub networks (experts) based on the incoming samples andtheir noisy labels . In this way, the proposed ADMoE can jointly learn from multiple sets of noisy labels with the majority of parameters shared, while providing specialization and scalarXiv:2208.11290v2  [cs.LG]  22 Nov 2022ability via experts. Unlike existing noisy label learning ap proaches, ADMoE does not require explicit mapping from noisy labels to network parameters, providing better scala bility and Ô¨Çexibility. To encourage ADMoE to develop spe cialization based on the noisy sources, we use noisy labels as (part of the) input features with learnable embeddings to make the gating function aware of them. Key Results . Fig. 1 shows that a multiple layer perception (MLP) (Rosenblatt 1958) enhanced by ADMoE can largely outperform both (1a) leading AD algorithms as well as (1b) noisylabel learning methods for classiÔ¨Åcation. Note AD MoE is not strictly another detection algorithm, but a gen eral framework to empower any neuralbased AD methods to leverage multiple sets of weak labels. ¬ß4 shows extensive results on more datasets, and the improvement in enhancing more complex DeepSAD (Ruff et al. 2019) with ADMoE. In summary, the key contributions of this work include: ‚Ä¢Problem formulation, baselines, and datasets . We for mally deÔ¨Åne the crucial problem of using multiple sets of noisy l abels for AD (MNLAD), and release the Ô¨Årst batch of baselines and datasets for future research2. ‚Ä¢The Ô¨Årst AD framework for learning from multiple noisy sources . The proposed ADMoE is a novel method with Mixtureofexperts (MoE) architecture to achieve specialized and scalable learning for MNLAD. ‚Ä¢Modelagnostic design . ADMoE enhances any neural networkbased AD methods, and we show its effective ness on MLP and stateoftheart (SOTA) DeepSAD. ‚Ä¢Effectiveness and realworld deployment . We demon strate ADMoE‚Äôs SOTA performance on seven benchmark datasets and a proprietary enterprise security application, in comparison with two groups of leading baselines (13 in total). It brings on average 14% and up to 34% im provement over not using it, with the equivalent number of learnable parameters and FLOPs as baselines. 2 Related Work "
328,CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation.txt,"Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to a different unlabeled target domain. Most existing
UDA methods focus on learning domain-invariant feature representation, either
from the domain level or category level, using convolution neural networks
(CNNs)-based frameworks. One fundamental problem for the category level based
UDA is the production of pseudo labels for samples in target domain, which are
usually too noisy for accurate domain alignment, inevitably compromising the
UDA performance. With the success of Transformer in various tasks, we find that
the cross-attention in Transformer is robust to the noisy input pairs for
better feature alignment, thus in this paper Transformer is adopted for the
challenging UDA task. Specifically, to generate accurate input pairs, we design
a two-way center-aware labeling algorithm to produce pseudo labels for target
samples. Along with the pseudo labels, a weight-sharing triple-branch
transformer framework is proposed to apply self-attention and cross-attention
for source/target feature learning and source-target domain alignment,
respectively. Such design explicitly enforces the framework to learn
discriminative domain-specific and domain-invariant representations
simultaneously. The proposed method is dubbed CDTrans (cross-domain
transformer), and it provides one of the first attempts to solve UDA tasks with
a pure transformer solution. Experiments show that our proposed method achieves
the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet.
Code and models are available at https://github.com/CDTrans/CDTrans.","Deep neural network have achieved remarkable success in a wide range of application scenar ios (Wang et al., 2022; Qian et al., 2021; Yiqi Jiang, 2022; Tan et al., 2019; Chen et al., 2021b; Jiang et al., 2021; Chen et al., 2017) but it still suffers poor generalization performance to other new domain because of the domain shift problem (Csurka, 2017; Zhao et al., 2020; Zhang et al., 2020; Oza et al., 2021). To handle this issue and avoid the expensive laborious annotations, lots of research efforts (Bousmalis et al., 2017; Kuroki et al., 2019; Wilson & Cook, 2020; VS et al., 2021) are devoted on Unsupervised Domain Adaptation (UDA). The UDA task aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. In UDA, most approaches focus on aligning distributions of source and target domain and learning domain invariant feature representations. One kind of such UDA methods are based on categorylevel alignment (Kang et al., 2019; Zhang et al., 2019; Jiang et al., 2020; Li et al., 2021b), which have achieved promising results on public UDA datasets using deep convolution neural networks (CNNs). The fundamental problems in categorylevel based alignment is the production of pseudo labels for samples in target domain to generate the input sourcetarget pairs. However, the current CNNsbased methods are not robust to the generated noisy pseudo labels for accurate domain alignment (Morerio et al., 2020; Jiang et al., 2020). With the success of Transformer in natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2018) and vision tasks (Dosovitskiy et al., 2020; Han et al., 2020; He et al., 2021; Khan et al., These authors contributed equally to this work. 1arXiv:2109.06165v4  [cs.CV]  19 Mar 2022Published as a conference paper at ICLR 2022 2021), it is found that crossattention in Transformer is good at aligning different distributions, even from different modalities e.g., visiontovision (Li et al., 2021e), visiontotext (Tsai et al., 2019; Hu & Singh, 2021) and texttospeech (Li et al., 2019). And we Ô¨Ånd that it is robust to noise in pseudo labels to some extent. Hence, in this paper, we apply transformers to the UDA task to take advantage of its robustness to noise and super power for feature alignment to deal with the problems as described above in CNNs. In our experiment, we conclude that even with noise in the labeling pair, the crossattention can still work well in aligning two distributions, thanks to the attention mechanism. To obtain more accurate pseudo labels, we designed a twoway centeraware labeling algorithm for samples in the target domain. The pseudo labels are produced based on the crossdomain similarity matrix, and a centeraware matching is involved to weight the matrix and weaken noise into the tolerable range. With the help of pseudo labels, we design the crossdomain transformer (CDTrans) for UDA. It consists of three weightsharing transformer branches, of which two branches are for source and target data respectively and the third one is the feature alignment branch, whose inputs are from sourcetarget pairs. The selfattention is applied in the source/target transformer branches and cross attention is involved in the feature alignment branch to conduct domain alignment. Such design explicitly enforces the framework to learn discriminative domainspeciÔ¨Åc and domaininvariant representations simultaneously. In summary, our contributions are threefold: ‚Ä¢ We propose a weightsharing triplebranch transformer framework, namely, CDTrans, for accurate unsupervised domain adaptation, taking advantage of its robustness to noisy labeling data and great power for feature alignment. ‚Ä¢ To produce pseudo labels with high quality, a twoway centeraware labeling method is proposed, and it boosts the Ô¨Ånal performance in the context of CDTrans. ‚Ä¢ CDTrans achieves the best performance compared to stateofthearts with a large margin on VisDA2017 (Peng et al., 2017) and DomainNet (Peng et al., 2019) datasets. 2 R ELATED WORK "
329,Deep Double Descent via Smooth Interpolation.txt,"The ability of overparameterized deep networks to interpolate noisy data,
while at the same time showing good generalization performance, has been
recently characterized in terms of the double descent curve for the test error.
Common intuition from polynomial regression suggests that overparameterized
networks are able to sharply interpolate noisy data, without considerably
deviating from the ground-truth signal, thus preserving generalization ability.
At present, a precise characterization of the relationship between
interpolation and generalization for deep networks is missing. In this work, we
quantify sharpness of fit of the training data interpolated by neural network
functions, by studying the loss landscape w.r.t. to the input variable locally
to each training point, over volumes around cleanly- and noisily-labelled
training samples, as we systematically increase the number of model parameters
and training epochs. Our findings show that loss sharpness in the input space
follows both model- and epoch-wise double descent, with worse peaks observed
around noisy labels. While small interpolating models sharply fit both clean
and noisy data, large interpolating models express a smooth loss landscape,
where noisy targets are predicted over large volumes around training data
points, in contrast to existing intuition.","The ability of overparameterized deep networks to interpolate noisy data, while at the same time showing good generalization performance (Belkin et al., 2018; Zhang et al., 2018), has been recently characterized in terms of the double descent curve of the test error (Belkin et al., 2019; Geiger et al., 2019). Within this framework, as model size increases, the test error follows the classical biasvariance tradeoÔ¨Ä curve (Geman etal.,1992), peakingasmodelsbecomelargeenoughtoperfectlyinterpolatethetrainingdata, anddecreasing as model size grows further (Belkin et al., 2019). This phenomenon, largely studied in the context of regression (Bartlett et al., 2020; Muthukumar et al., 2020) and random features (Belkin et al., 2020), at present lacks a precise characterization relating interpolation to generalization for deep networks. Current intuition from linear and polynomial regression suggests that, under some hypothesis on the training sample, large overparameterized models are able to perfectly interpolate both cleanly and noisilylabeled samples, without considerably deviating from the groundtruth signal, thus showing good performance de spite overÔ¨Åtting the training data (Muthukumar et al., 2020; Bartlett et al., 2020; Nakkiran et al., 2019a). 1Source code to reproduce our results available at https://github.com/magamba/double_descent 1arXiv:2209.10080v4  [cs.LG]  8 Apr 2023Published in Transactions on Machine Learning Research (04/2023) 0  2 x2 1 012sin(x)+ Prediction Ground Truth Observations (a) 0  2 x2 1 012sin(x)+ Prediction Ground Truth Observations (b) x2 x1LosspathœÄ1 pathœÄ2 pathœÄ3K=1 K=2 K=3 training sample (c) x2 x1Loss (d) Figure 1: Intuition from overparameterized regression. a) Polynomial of large degree, trained with gradient descent to Ô¨Åt noisy scalar data, reproducing the polynomial regression experiment of Nakkiran et al. (2019a), and reÔ¨Çecting common intuition on double descent, suggesting that the generalization ability of large interpolating models is tied to sharply Ô¨Åtting of noisy data, thus resulting in models that do not deviate considerably from the ground truth signal. b) In this work we show that, contrary to intuition, deep networks smoothly interpolate both clean and noisy data, and that improved generalization in the interpolating regime is tied to smoothness of the loss w.r.t. the input variable. Geodesic MC integration. For each base training point, we generate Pgeodesic paths by connecting a sequence of augmentations of increasingstrength,whichweusetocovervolumesofincreasingsizeinthelosslandscapearoundeachtraining point. We compare points that are c) sharply interpolated from those that are d) smoothly interpolated. Figure 1a illustrates this phenomenon, showing a polynomial of large degree that perfectly Ô¨Åts the training data, with predictive function sharply interpolating noisy samples (intuitively corresponding to a spike at each training point), while overall remaining close to the datagenerating function. In this work, we study the emergence of double descent for the test error of deep networks (Nakkiran et al., 2019b) through the lens of smoothness of interpolation of the training data, as model size as well as the number of training epochs vary, for models trained in practice. To quantify smoothness of interpolation, we conduct an empirical exploration of the loss landscape w.r.t. the input variable, by providing explicit measures of sharpness of the loss, focusing on image classiÔ¨Åcation. Due to the inherently noisy nature of Euclidean estimators in pixel space, and following the manifold hy pothesisPope et al. (2020); Bengio (2013); Narayanan & Mitter (2010), postulating that natural data lies on a combination of manifolds of lower dimension than the input data‚Äôs ambient dimension, we constrain our measures to the support of the data distribution, locally to each training point. Our empirical study shows that the polynomial intuition in Figure 1a does not hold in practice for deep networks, which instead smoothly interpolate both clean and noisy data (Figure 1b). SpeciÔ¨Åcally, smooth interpolation ‚Äì emerging both for large overparameterized networks and prolonged training ‚Äì results in large models conÔ¨Ådently predicting the (noisy) training targets over large volumes around each training point. Contributions ‚Ä¢We present the Ô¨Årst systematic empirical study of smoothness of the loss landscape of deep networks in relation to overparameterization and interpolation for natural image datasets. ‚Ä¢Starting from inÔ¨Ånitesimal smoothness measures from prior work, we introduce volumetric measures that capture loss smoothness when moving away from training points. ‚Ä¢We develop a geodesic Monte Carlo integration method for constraining our measures to a local approximation of the data manifold, in proximity of each training point. 2Published in Transactions on Machine Learning Research (04/2023) ‚Ä¢We present an empirical study of modelwise and epochwise double descent for neural networks trained without confounders (explicit regularization, data augmentation, batch normalization), as well as for commonlyfound training settings. By decoupling smoothness from generalization, we empirically show that overparameterization promotes inputspace smoothness of the loss landscape. Particularly, we produce practical examples in which smoothness of the learned function of deep networks does not result in improved generalization, highlighting that the implicit regularization eÔ¨Äectofoverparameterizationshouldbestudiedintermsofreducedvariationofthelearnedfunction. 2 Related work "
330,Audio-Visual Efficient Conformer for Robust Speech Recognition.txt,"End-to-end Automatic Speech Recognition (ASR) systems based on neural
networks have seen large improvements in recent years. The availability of
large scale hand-labeled datasets and sufficient computing resources made it
possible to train powerful deep neural networks, reaching very low Word Error
Rate (WER) on academic benchmarks. However, despite impressive performance on
clean audio samples, a drop of performance is often observed on noisy speech.
In this work, we propose to improve the noise robustness of the recently
proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based
architecture by processing both audio and visual modalities. We improve
previous lip reading methods using an Efficient Conformer back-end on top of a
ResNet-18 visual front-end and by adding intermediate CTC losses between
blocks. We condition intermediate block features on early predictions using
Inter CTC residual modules to relax the conditional independence assumption of
CTC-based models. We also replace the Efficient Conformer grouped attention by
a more efficient and simpler attention mechanism that we call patch attention.
We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip
Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and
visual modalities allows to better recognize speech in the presence of
environmental noise and significantly accelerate training, reaching lower WER
with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC)
model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on
LRS2 and LRS3 test sets. Code and pretrained models are available at
https://github.com/burchim/AVEC.","Endtoend Automatic Speech Recognition based on deep neural networks has become the standard of stateof theart approaches in recent years [25, 47, 18, 16, 17, 31, 7]. The availability of large scale handlabeled datasets and suf Ô¨Åcient computing resources made it possible to train power 40 ms rateVisual Conformer Stage 2 20 ms rate Visual Conformer Stage 1 Visual Frontend   Conv3d + ResNet18  Audio Frontend   STFT + Conv2d  Audio Conformer Stage 1Audio Conformer Stage 2Audio Conformer Stage 3AudioV isual  Fusion ModuleAudioV isual Conformer Stage Visual  BackendAudio   Backend80 ms rateCTC loss 40 ms rate80 ms rate 80 ms rateFigure 1: AudioVisual EfÔ¨Åcient Conformer architec ture. The model is trained endtoend using CTC loss and takes raw audio waveforms and lip movements from the speaker as inputs. ful deep neural networks for ASR, reaching very low WER on academic benchmarks like LibriSpeech [34]. Neural ar chitectures like Recurrent Neural Networks (RNN) [15, 19], Convolution Neural Networks (CNN) [10, 28] and Trans formers [12, 23] have successfully been trained from raw audio waveforms and melspectrograms audio features to transcribe speech to text. Recently, Gulati et al. [16] proposed a convolutionaugmented transformer architec ture (Conformer) to model both local and global dependen cies using convolution and attention to reach better speech recognition performance. Concurrently, Nozaki et al. [33]arXiv:2301.01456v1  [cs.CV]  4 Jan 2023improved CTCbased speech recognition by conditioning intermediate encoder block features on early predictions us ing intermediate CTC losses [14]. Burchi et al. [7] also pro posed an EfÔ¨Åcient Conformer architecture using grouped attention for speech recognition, lowering the amount of computation while achieving better performance. Inspired from computer vision backbones, the EfÔ¨Åcient Conformer encoder is composed of multiple stages where each stage comprises a number of Conformer blocks to progressively downsample and project the audio sequence to wider fea ture dimensions. Yet, even if these audioonly approaches are breaking the stateoftheart, one major pitfall for using them in the realworld is the rapid deterioration of performance in the presence of ambient noise. In parallel to that, Audio Visual Speech Recognition (A VSR) has recently attracted a lot of research attention due to its ability to use image process ing techniques to aid speech recognition systems. Preced ing works have shown that including the visual modality of lip movements could improve the robustness of ASR sys tems with respect to noise while reaching better recognition performance [41, 42, 36, 1, 45, 29]. Xu et al. [45] pro posed a twostage approach to Ô¨Årst separate the target voice from background noise using the speakers lip movements and then transcribe the Ô¨Åltered audio signal with the help of lip movements. Petridis et al. [36] uses a hybrid architec ture, training an LSTMbased sequencetosequence (S2S) model with an auxiliary CTC loss using an early fusion strategy to reach better performance. Ma et al. [29] uses Conformer backend networks with ResNet18 [20] front end networks to improve recognition performance. Other works focus on Visual Speech Recognition (VSR), only using lip movements to transcribe spoken language into text [4, 9, 48, 3, 49, 37, 30]. An important line of research is the use of crossmodal distillation. Afouras et al.[3] and Zhao et al. [49] proposed to improve the lip read ing performance by distilling from an ASR model trained on a largescale audioonly corpus while Ma et al. [30] uses predictionbased auxiliary tasks. Prajwal et al. [37] also proposed to use subwords units instead of characters to transcribe sequences, greatly reducing running time and memory requirements. Also providing a language prior, re ducing the language modelling burden of the model. In this work we focus on the design of a noise robust speech recognition architecture processing both audio and visual modalities. We use the recently proposed CTC based EfÔ¨Åcient Conformer architecture [7] and show that including the visual modality of lip movements can suc cessfully improve noise robustness while signiÔ¨Åcantly ac celerating training. Our AudioVisual EfÔ¨Åcient Conformer (A VEC) reaches lower WER using 4 times less training steps than its audioonly counterpart. Moreover, we are the Ô¨Årst work to apply intermediate CTC losses betweenblocks [27, 33] to improve visual speech recognition perfor mance. We show that conditioning intermediate features on early predictions using Inter CTC residual modules allows to close the gap in WER between autoregressive and non autoregressive A VSR systems based on S2S. This also helps to counter a common failure case which is that audiovisual models tend to ignore the visual modality. In this way, we force prefusion layers to learn spatiotemporal features. Fi nally, we replace the EfÔ¨Åcient Conformer grouped attention by a more efÔ¨Åcient and simpler attention mechanism that we call patch attention. Patch attention reaches similar per formance to grouped attention while having a lower com plexity. The contributions of this work are as follows: ‚Ä¢ We improve the noise robustness of the recently pro posed EfÔ¨Åcient Conformer architecture by processing both audio and visual modalities. ‚Ä¢ We condition intermediate Conformer block features on early predictions using Inter CTC residual modules to relax the conditional independence assumption of CTC models. This allows us to close the gap in WER between autoregressive and nonautoregressive meth ods based on S2S. ‚Ä¢ We propose to replace the EfÔ¨Åcient Conformer grouped attention by a more efÔ¨Åcient and simpler at tention mechanism that we call patch attention. Patch attention reaches similar performance to grouped at tention with a lower complexity. ‚Ä¢ We experiment on publicly available LRS2 and LRS3 datasets and reach stateoftheart results using audio and visual modalities. 2. Method "
331,The Group Loss for Deep Metric Learning.txt,"Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that ""similar objects
should belong to the same group"", the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We show state-of-the-art results on clustering and image
retrieval on several datasets, and show the potential of our method when
combined with other techniques such as ensembles","Measuring object similarity is at the core of many important machine learning problems like clustering and object retrieval. For visual tasks, this means learning a distance function over images. With the rise of deep neural networks, the focus has rather shifted towards learning a feature embedding that is easily separable using a simple distance function, such as the Euclidean distance. In essence, objects of the same class (similar) should be close by in the learned manifold, while objects of a dierent class (dissimilar) should be far away. Historically, the best performing approaches get deep feature embeddings from the socalled siamese networks [4], which are typically trained using the contrastive loss [4] or the triplet loss [41,53]. A clear drawback of these losses is that they only consider pairs or triplets of data points, missing key infor mation about the relationships between all members of the minibatch. On aarXiv:1912.00385v4  [cs.CV]  20 Jul 20202 I. Elezi et al. minibatch of size n, despite that the number of pairwise relations between sam ples isO(n2), contrastive loss uses only O(n=2) pairwise relations, while triplet loss usesO(2n=3) relations. Additionally, these methods consider only the rela tions between objects of the same class (positives) and objects of other classes (negatives), without making any distinction that negatives belong to dierent classes. This leads to not taking into consideration the global structure of the embedding space, and consequently results in lower clustering and retrieval per formance. To compensate for that, researchers rely on other tricks to train neural networks for deep metric learning: intelligent sampling [25], multitask learning [59] or hardnegative mining [40]. Recently, researchers have been increasingly working towards exploiting in a principled way the global structure of the em bedding space [36,5,12,50], typically by designing ranking loss functions instead of following the classic triplet formulations. In a similar spirit, we propose Group Loss , a novel loss function for deep metric learning that considers the similarity between all samples in a minibatch. To create the minibatch, we sample from a xed number of classes, with samples coming from a class forming a group . Thus, each minibatch consists of several randomly chosen groups, and each group has a xed number of samples. An iterative, fullydierentiable label propagation algorithm is then used to build feature embeddings which are similar for samples belonging to the same group, and dissimilar otherwise. At the core of our method lies an iterative process called replicator dynamics [52,9], that renes the local information, given by the softmax layer of a neural network, with the global information of the minibatch given by the similarity between embeddings. The driving rationale is that the more similar two samples are, the more they aect each other in choosing their nal label and tend to be grouped together in the same group, while dissimilar samples do not aect each other on their choices. Neural networks optimized with the Group Loss learn to provide similar features for samples belonging to the same class, making clustering and image retrieval easier. Ourcontribution in this work is fourfold: {We propose a novel loss function to train neural networks for deep metric embedding that takes into account the local information of the samples, as well as their similarity. {We propose a dierentiable labelpropagation iterative model to embed the similarity computation within backpropagation, allowing endtoend training with our new loss function. {We perform a comprehensive robustness analysis showing the stability of our module with respect to the choice of hyperparameters. {We show stateoftheart qualitative and quantitative results in several stan dard clustering and retrieval datasets.The Group Loss for Deep Metric Learning 3 Embedding Embedding Embedding . . . Summer  Tanger  White  Pelican  Black footed  Albatross  Indigo  Bunting CNN  CNN CNN  CNN Shared   Weights  Classes  Prior Group Loss  Similarity Refinement  Procedure  C.E. Loss Shared   Weights  Shared   Weights  = Anchor  Anchor Positive Negative  CNN CNN CNN  Triplet  Loss Shared   Weights Shared   Weights Embedding  Softmax  Softmax  Softmax  Softmax 2 31 CVPR 2020  Fig. 1: A comparison between a neural model trained with the Group Loss (left) and the triplet loss (right). Given a minibatch of images belonging to dierent classes, their embeddings are computed through a convolutional neural network. Such embeddings are then used to generate a similarity matrix that is fed to the Group Loss along with prior distributions of the images on the possible classes. The green contours around some minibatch images refer to anchors . It is worth noting that, dierently from the triplet loss, the Group Loss considers multiple classes and the pairwise relations between all the samples. Numbers from 1 to 3 refer to the Group Loss steps, see Sec 3.1 for the details. 2 Related Work "
332,Adapting Convolutional Neural Networks for Geographical Domain Shift.txt,"We present the winning solution for the Inclusive Images Competition
organized as part of the Conference on Neural Information Processing Systems
(NeurIPS 2018) Competition Track. The competition was organized to study ways
to cope with domain shift in image processing, specifically geographical shift:
the training and two test sets in the competition had different geographical
distributions. Our solution has proven to be relatively straightforward and
simple: it is an ensemble of several CNNs where only the last layer is
fine-tuned with the help of a small labeled set of tuning labels made available
by the organizers. We believe that while domain shift remains a formidable
problem, our approach opens up new possibilities for alleviating this problem
in practice, where small labeled datasets from the target domain are usually
either available or can be obtained and labeled cheaply.","Domain shift is a problem often arising in machine learning, when a model i s trained on a dataset that might be sufÔ¨Åciently large and diverse, but later the model is supposed to be applied to datasets with a different data distribution . One important example of this problem is the geographical domain shift in image processing, when, e.g., the same semantic category of objects can look quite different o n photos taken in different geographical locations (see Fig. 1). Domain shift also often results from dataset bias : e.g., a dataset of human faces heavily shifted towards Cauca sian faces would suffer from this problem when applied in, e.g., Asia. Modern techniques in domain adaptation (see references in S ection 2) usually op erate in conditions where the target domain is completely di fferent from the source domain in some aspects; e.g., the source domain are syntheti c images generated arti Ô¨Åcially and the target domain includes the corresponding re al images. Geographical domain shift is a more subtle problem: in an image classiÔ¨Åcat ion problem with geo graphical shift, some classes will not change at all from the source to target domain, while others might change radically. In this work, we present the winning solution for the Inclusi ve Images Competi tion [ 1] organized as part of the Conference on Neural Information P rocessing Systems 1(NeurIPS 2018) Competition Track. Based on the work [ 25], the challenge was in tended to develop solutions for the geodiversity problem, w ith a training set skewed towards North America and Western Europe and test sets drawn from completely dif ferent geographic distributions that were not revealed to t he competitors. One interesting property of our solution is that it is relati vely straightforward and simple. We did not use any state of the art models for domain ad aptation, and our Ô¨Ånal solution is an ensemble of several CNNs where only the last la yer is Ô¨Ånetuned with the help of a small labeled set of tuning labels (Stage 1 set) t hat was made available by the organizers. It turned out that this set had a geographica l distribution similar enough to the hidden Stage 2 evaluation set, and the very small set of tuning labels (only 1000 examples) proved to sufÔ¨Åce, with proper techniques such as d ata augmentation and ensembling, to adapt the base models to a new domain. The paper is organized as follows. In Section 2, we survey some related work on domain shift and domain adaptation. Section 3introduces the problem of the Inclusive Images Challenge and describes the dataset and evaluation m etrics. Section 4presents our solution in detail, Section 5shows experimental results for both singlemodel so lutions and ensembles, and Section 6concludes the paper. 2 Related Work "
333,Compensation Learning in Semantic Segmentation.txt,"Label noise and ambiguities between similar classes are challenging problems
in developing new models and annotating new data for semantic segmentation. In
this paper, we propose Compensation Learning in Semantic Segmentation, a
framework to identify and compensate ambiguities as well as label noise. More
specifically, we add a ground truth depending and globally learned bias to the
classification logits and introduce a novel uncertainty branch for neural
networks to induce the compensation bias only to relevant regions. Our method
is employed into state-of-the-art segmentation frameworks and several
experiments demonstrate that our proposed compensation learns inter-class
relations that allow global identification of challenging ambiguities as well
as the exact localization of subsequent label noise. Additionally, it enlarges
robustness against label noise during training and allows target-oriented
manipulation during inference. We evaluate the proposed method on %the widely
used datasets Cityscapes, KITTI-STEP, ADE20k, and COCO-stuff10k.","Semantic segmentation is a wellknown and challeng ing task in computer vision [6, 34, 50]. Thanks to the large investment of time and resources, the research community published a large number of elaborately curated datasets to train and evaluate methods for semantic segmentation [16, 37, 53, 60, 79, 92]. Nevertheless, the industry needs an in creasing amount of accurately annotated data and spends billion dollars to curate them [17]. Unfortunately, the an notation task stays challenging for humans even with ad vanced semiautomated annotation frameworks [1, 10, 72], because ambiguous image elements often can be assigned to multiple classes. Thus, annotated data is often noisy, with the consequence that the optimization of stochastic meth ods like neural networks is corrupted and the evaluation is distorted. Even the ground truth of widely used research benchmarks, which form the basis of this and many other papers, are subject to noise, as lamented by [42]. Semi automated annotation without incorporating label noise is therefore a serious problem in semantic segmentation. 1arXiv:2304.13428v1  [cs.CV]  26 Apr 2023While tackling the impact of noisy labels is a well known research topic [7, 19, 31, 63], avoiding noisy labels during labeling is shallow investigated. Because modern semi automated annotation frameworks estimate an initial guess with a pretrained segmentation framework [1,9,10,72], an obvious way to improve the annotation framework is to im prove the segmentation framework. To remove the residual error in the estimate, the human curator is still asked to in spect and correct the entire image. To reduce this effort, un certainty estimation can help to guide the curator to Ô¨Ånd the most likely error regions. Current approaches like Bayesian Neural Networks [52,73], that estimate and incorporate un certainty in semantic segmentation aim to make the training more robust against label noise, but mainly detect bound aries between neighboring segments [4, 7, 73]. Instead of using uncertainty estimation to make training more robust against noise, we aim to utilize robust training methods and uncertainty estimation to avoid new noise dur ing data annotation. Therefore, we present a novel method transferring compensation learning to semantic segmenta tion to compensate noise and ambiguities with endtoend trainable compensation weights. Compensation learning, which adds ground truth depending bias to model predic tions, has been introduced by Yao et al. [85] for image clas siÔ¨Åcation. It allows the lowering of the inÔ¨Çuence of simi lar classes in order to reduce the impact of ambiguities and noise. We induce symmetry to make compensation learning stable during training and introduce an adaptive uncertainty branch that estimates the local importance of compensation. Experiments on the widely used segmentation datasets Cityscapes [16], KITTISTEP [79], ADE20k [92], and COCOstuff10k [37] show that our method learns inter pretable interclass compensations and is able to estimate prediction uncertainties. We present how compensation identiÔ¨Åes challenging class pairs and the uncertainty lo calizes prediction errors very accurately. Besides the in terpretable guidance for data annotation, our method in creases the robustness of training semantic segmentation methods with noisy labels and additionally introduces a use ful method to improve the segmentation accuracy of certain classes. Moreover, we analyze and visualize interclass am biguities for the datasets. In summary , our work contributes a novel framework1 to improve semiautomated annotation that ‚Ä¢ learns humaninterpretable compensation weights of global interclass ambiguities. ‚Ä¢ introduces a novel uncertainty branch to adapt the compensation locally. The branch provides local guid ance to image regions with high risk of errors. ‚Ä¢ improves robustness against noise during training. 1Code available at https://github.com/tntLUH/compensation learning‚Ä¢ allows applicationoriented manipulation of segmenta tion accuracy during inference. 2. Related Work "
334,Enabling Smart Data: Noise filtering in Big Data classification.txt,"In any knowledge discovery process the value of extracted knowledge is
directly related to the quality of the data used. Big Data problems, generated
by massive growth in the scale of data observed in recent years, also follow
the same dictate. A common problem affecting data quality is the presence of
noise, particularly in classification problems, where label noise refers to the
incorrect labeling of training instances, and is known to be a very disruptive
feature of data. However, in this Big Data era, the massive growth in the scale
of the data poses a challenge to traditional proposals created to tackle noise,
as they have difficulties coping with such a large amount of data. New
algorithms need to be proposed to treat the noise in Big Data problems,
providing high quality and clean data, also known as Smart Data. In this paper,
two Big Data preprocessing approaches to remove noisy examples are proposed: an
homogeneous ensemble and an heterogeneous ensemble filter, with special
emphasis in their scalability and performance traits. The obtained results show
that these proposals enable the practitioner to efficiently obtain a Smart
Dataset from any Big Data classification problem.","Vast amounts of information surround us today. Technologies such as the Internet generate data at an exponential rate thanks to the aordability and great development of storage and network resources. It is predicted that by 2020, the digital universe will be 10 times as big as it was in 2013, totaling an astonishing 44 zettabytes [22]. The current volume of data has exceeded the processing capabilities of classical data mining systems [50] and have created a need for new frameworks for storing and processing this data. It is widely Corresponding author Email addresses: djgarcia@decsai.ugr.es (Diego Garc aGil), julianlm@decsai.ugr.es (Juli an Luengo), salvagl@decsai.ugr.es (Salvador Garc a), herrera@decsai.ugr.es (Francisco Herrera) Preprint submitted to Journal of L ATEX Templates July 31, 2017arXiv:1704.01770v2  [cs.DB]  28 Jul 2017accepted that we have entered the Big Data era [31]. Big Data is the set of technologies that make processing such large amounts of data possible [7], while most of the classic knowledge extraction methods cannot work in a Big Data environment because they were not conceived for it. Big Data as concept is dened around ve aspects: data volume, data veloc ity, data variety, data veracity and data value [24]. While the volume, variety and velocity aspects refer to the data generation process and how to capture and store the data, veracity and value aspects deal with the quality and the useful ness of the data. These two last aspects become crucial in any Big Data process, where the extraction of useful and valuable knowledge is strongly in uenced by the quality of the used data. In Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases. The lack of ecient and aordable preprocessing tech niques implies that the problems in the data will aect the models extracted. Among all the problems that may appear in the data, the presence of noise in the dataset is one of the most frequent. Noise can be dened as the partial or complete alteration of the information gathered for a data item, caused by an exogenous factor not related to the distribution that generates the data. Learn ing from noisy data is an important topic in machine learning, data mining and pattern recognition, as real world data sets may suer from imperfections in data acquisition, transmission, storage, integration and categorization. Noise will lead to excessively complex models with deteriorated performance [49], re sulting in even larger computing times for less value. The impact of noise in Big Data, among other pernicious traits, has not been disregarded. Recently, Smart Data (focusing on veracity and value) has been in troduced, aiming to lter out the noise and to highlight the valuable data, which can be eectively used by companies and governments for planning, operation, monitoring, control, and intelligent decision making. Three key attributes are needed for data to be smart, it must be accurate, actionable and agile: Accurate: data must be what it says it is with enough precision to drive value. Data quality matters. Actionable: data must drive an immediate scalable action in a way that maximizes a business objective like media reach across platforms. Scalable action matters. Agile: data must be available in realtime and ready to adapt to the changing business environment. Flexibility matters. Advanced Big Data modeling and analytics are indispensable for discov ering the underlying structure from retrieved data in order to acquire Smart Data. In this paper we provide several preprocessing techniques for Big Data, transforming raw, corrupted datasets into Smart Data. We focus our interest on classication tasks, where two types of noise are distinguished: class noise , when it aects the class label of the instances, and attribute noise , when it aects 2the rest of attributes. The former is known to be the most disruptive [39, 54]. Consequently, many recent works, including this contribution, have been de voted to resolving this problem or at least to minimize its eects (see [15] for a comprehensive and updated survey). While some architectural designs are already proposed in the literature[52], there is no particular algorithm which deals with noise in Big Data classication, nor a comparison of its eect on model generalization abilities or computing times. Thereby we propose a framework for Big Data under Apache Spark for removing noisy examples composed of two algorithms based on ensembles of classiers. The rst one is an homogeneous ensemble, named Homogeneous Ensembe for Big Data (HMEBD), which uses a single base classier (Random Forest [4]) over a partitioning of the training set. The second ensemble is an heterogeneous ensemble, namely Heterogeneous Ensembe for BigData (HTE BD), that uses dierent classiers to identify noisy instances: Random Forest, Logistic Regression and KNearest Neighbors (KNN) as base classiers. For the sake of a more complete comparison, we have also considered a simple ltering approach based on similarities between instances, named Edited Nearest Neigh bor for Big Data (ENNBD). ENNBD examines the nearest neighbors of every example in the training set and eliminates those whose majority of neighbors belong to a dierent class. All these techniques have been implemented under the Apache Spark framework [20, 40] and can be downloaded from the Spark's community repository1. To show the performance of the three proposed algorithms, we have carried out an experimental evaluation with four large datasets, namely SUSY ,HIGGS , Epsilon and ECBDL14 . We have induced several levels of class noise to eval uate the eects of applying such framework and the improvements obtained in terms of classication accuracy for two classiers: a decision tree and the KNN technique. Decision trees with pruning are known to be tolerant to noise, while KNN is a noise sensitive algorithm when the number of selected neighbors is low. These dierences allow us to better compare the eect of the framework in classiers which behave dierently towards noise. We also show that, for the Big Data problems considered, the classiers also benet from applying the noise treatment even when no additional noise is induced, since Big Data problems contain implicit noise due to incidental homogeneity, spurious correlations and the accumulation of noisy examples [12]. The results obtained indicate that the framework proposed can successfully deal with noise. In particular, the ho mogeneous ensemble is a suitable technique for dealing with noise in Big Data problems, with low computing times and enabling the classier to achieve better accuracy. The remainder of this paper is organized as follows: Section 2 presents the concepts of noise, MapReduce and Smart Data. Section 3 explains the pro posed framework. Section 4 describes the experiments carried out to check the performance of the framework. Finally, Section 5 concludes the paper. 1https://sparkpackages.org/package/djgarcia/NoiseFramework 32. Related work "
335,Transfer Learning for Named-Entity Recognition with Neural Networks.txt,"Recent approaches based on artificial neural networks (ANNs) have shown
promising results for named-entity recognition (NER). In order to achieve high
performances, ANNs need to be trained on a large labeled dataset. However,
labels might be difficult to obtain for the dataset on which the user wants to
perform NER: label scarcity is particularly pronounced for patient note
de-identification, which is an instance of NER. In this work, we analyze to
what extent transfer learning may address this issue. In particular, we
demonstrate that transferring an ANN model trained on a large labeled dataset
to another dataset with a limited number of labels improves upon the
state-of-the-art results on two different datasets for patient note
de-identification.","Electronic health records (EHRs) have been widely adopted in some countries such as the United States and represent gold mines of infor mation for medical research. The majority of EHR data exist in unstructured form such as patient notes (Murdoch and Detsky, 2013). Applying nat ural language processing on patient notes can im prove the phenotyping of patients (Ananthakrish nan et al., 2013; Pivovarov and Elhadad, 2015; Halpern et al., 2016), which has many down stream applications such as the understanding of diseases (Liao et al., 2015). However, before patient notes can be shared with medical investigators, some types of infor mation, referred to as protected health informa tion (PHI), must be removed in order to preserve These authors contributed equally to this work.patient conÔ¨Ådentiality. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) (OfÔ¨Åce for Civil Rights, 2002) de Ô¨Ånes 18 different types of PHI, ranging from pa tient names and ID numbers to addresses and phone numbers. The task of removing PHI from a patient note is referred to as deidentiÔ¨Åcation . The essence of deidentiÔ¨Åcation is recognizing PHI in patient notes, which is a form of namedentity recognition (NER). Existing deidentiÔ¨Åcation systems are often rulebased approaches or featurebased machine learning approaches. However, these techniques require additional lead time for developing and Ô¨Ånetuning the rules or features speciÔ¨Åc to each new dataset. Meanwhile, recent work using ANNs have yielded stateoftheart performances with out using any manual features (Dernoncourt et al., 2016). Compared to the previous systems, ANNs have a competitive advantage that the model can be Ô¨Ånetuned on a new dataset without the over head of manual feature development, as long as some labels for the dataset are available. However, it may still be inefÔ¨Åcient to mass de ploy ANNbased deidentiÔ¨Åcation system in prac tical settings, since creating annotations for pa tient notes is especially difÔ¨Åcult. This is due to the fact that only a restricted set of individuals is authorized to access original patient notes; the annotation task cannot be crowdsourced, mak ing it slow and expensive to obtain a large anno tated corpus. Medical professionals are therefore wary to explore patient notes because of this de identiÔ¨Åcation barrier, which considerably hampers medical research. In this paper, we analyze to what extent trans fer learning may improve deidentiÔ¨Åcation perfor mances on datasets with a limited number of la bels. By training an ANN model on a large dataset (MIMIC) and transferring it to smaller datasetsarXiv:1705.06273v1  [cs.CL]  17 May 2017(i2b2 2014 and i2b2 2016), we demonstrate that transfer learning allows to outperform the stateof theart results. 2 Related Work "
336,Iterative fully convolutional neural networks for automatic vertebra segmentation and identification.txt,"Precise segmentation and anatomical identification of the vertebrae provides
the basis for automatic analysis of the spine, such as detection of vertebral
compression fractures or other abnormalities. Most dedicated spine CT and MR
scans as well as scans of the chest, abdomen or neck cover only part of the
spine. Segmentation and identification should therefore not rely on the
visibility of certain vertebrae or a certain number of vertebrae. We propose an
iterative instance segmentation approach that uses a fully convolutional neural
network to segment and label vertebrae one after the other, independently of
the number of visible vertebrae. This instance-by-instance segmentation is
enabled by combining the network with a memory component that retains
information about already segmented vertebrae. The network iteratively analyzes
image patches, using information from both image and memory to search for the
next vertebra. To efficiently traverse the image, we include the prior
knowledge that the vertebrae are always located next to each other, which is
used to follow the vertebral column. This method was evaluated with five
diverse datasets, including multiple modalities (CT and MR), various fields of
view and coverages of different sections of the spine, and a particularly
challenging set of low-dose chest CT scans. The proposed iterative segmentation
method compares favorably with state-of-the-art methods and is fast, flexible
and generalizable.","Segmentation and identiÔ¨Åcation of the vertebrae is often a prerequisite for automatic analysis of the spine, such as detection of vertebral fractures (Yao et al., 2012), assessment of spinal deformities (Forsberg et al., 2013), or computerassisted surgical interventions (Knez et al., 2016). Automatic spine analysis can be performed with a large variety of tomographic scans, includ ing dedicated spine scans but also scans of the neck, chest or abdomen that incidentally cover part of the spine. A generic vertebra segmentation algorithm therefore needs to be robust with respect to di erent image resolutions and di erent cover ages of the spine. This especially means that no assumptions should be made about the number of visible vertebrae and their anatomical identity, i.e., to which section of the spine they be long. Vertebra segmentation is therefore essentially an instance segmentation problem with an a priori unknown number of in stances ( i.e.vertebrae). However, in contrast to generic instance segmentation the individual instances are not independent of each other. The instances are known to be located in close prox imity to each other in the image, forming together the vertebral column. We propose to approach vertebra segmentation with aninstance segmentation algorithm that explicitly incorporates this prior knowledge to locate instances, but that makes no further assumptions. Approaching vertebra segmentation as an instance segmen tation problem entails treating all vertebrae as instances of the same class of objects. However, an anatomical identiÔ¨Åcation of the segmented vertebrae is often also needed, for instance, for further analysis steps or for reporting purposes. Especially in images originally not intended for spine imaging, anatomical labeling of the vertebrae can be challenging due to variations in the Ô¨Åeld of view. These variations lead to variable coverage of the spine and also of structures that provide anatomical cues for identiÔ¨Åcation of the vertebrae, such as the ribs or the sacrum. Additionally, neighboring vertebrae often have similar shape and appearance so that independent labeling of each vertebra may result in mistakes. Vertebra identiÔ¨Åcation therefore requires a global rather than a perinstance approach to ensure an overall plausible, anatomically correct labeling. Another challenge inherent to an instance segmentation ap proach is the identiÔ¨Åcation of partially visible instances. While occlusion is a typical problem in twodimensional but not in threedimensional images, some vertebrae may be only partially Preprint submitted to Medical Image Analysis January 19, 2019arXiv:1804.04383v3  [cs.CV]  11 Feb 2019visible due to the limited Ô¨Åeld of view of the scan. If these in completely visible vertebrae are included in subsequent analyses that are based on the obtained vertebra segmentations, such as measurement of vertebral heights for detection and classiÔ¨Åca tion of vertebral compression fractures (Grigoryan et al., 2003), their results may be unreliable. Therefore, incompletely visi ble instances need to be either ignored or explicitly identiÔ¨Åed as incomplete so that they can be excluded from subsequent analyses. In this paper, we propose an iterative instancebyinstance segmentation approach for vertebra segmentation based on a fully convolutional neural network. This network performs vertebra detection, segmentation, anatomical identiÔ¨Åcation and classiÔ¨Åcation of their completeness concurrently and therefore presents an entirely supervised approach that can be trained end toend. While we propose to attempt a perinstance identiÔ¨Åcation of the individual vertebrae together with the segmentation, the labeling is subsequently adjusted taking all segmented vertebrae into account. In contrast to previous approaches, the presented method can be used for any imaging modality, any Ô¨Åeld of view and any number and type (cervical, thoracic, lumbar) of visi ble vertebrae because it avoids explicit modeling of shape and appearance of the vertebrae and the vertebral column. We evalu ate these claims using a diverse selection of datasets, including scans from di erent modalities (CT and MR), various Ô¨Åelds of view, cases with severe compression fractures and a particularly challenging set of lowdose chest CT. 2. Related work "
337,The DKU-DukeECE System for the Self-Supervision Speaker Verification Task of the 2021 VoxCeleb Speaker Recognition Challenge.txt,"This report describes the submission of the DKU-DukeECE team to the
self-supervision speaker verification task of the 2021 VoxCeleb Speaker
Recognition Challenge (VoxSRC). Our method employs an iterative labeling
framework to learn self-supervised speaker representation based on a deep
neural network (DNN). The framework starts with training a self-supervision
speaker embedding network by maximizing agreement between different segments
within an utterance via a contrastive loss. Taking advantage of DNN's ability
to learn from data with label noise, we propose to cluster the speaker
embedding obtained from the previous speaker network and use the subsequent
class assignments as pseudo labels to train a new DNN. Moreover, we iteratively
train the speaker network with pseudo labels generated from the previous step
to bootstrap the discriminative power of a DNN. Also, visual modal data is
incorporated in this self-labeling framework. The visual pseudo label and the
audio pseudo label are fused with a cluster ensemble algorithm to generate a
robust supervisory signal for representation learning. Our submission achieves
an equal error rate (EER) of 5.58% and 5.59% on the challenge development and
test set, respectively.","This report describes the submission of the DKUDukeECE team to the selfsupervision speaker veriÔ¨Åcation task of the 2021 V oxCeleb Speaker Recognition Challenge (V oxSRC). In our previous work on selfsupervised speaker represen tation learning [1], we proposed a twostage iterative labeling framework. In the Ô¨Årst stage, contrastive selfsupervised learn ing is used to pretraining the speaker embedding network. This allows the network to learn a meaningful feature representa tion for the Ô¨Årst round of clustering instead of random initial ization. In the second stage, a clustering algorithm iteratively generates pseudo labels of the training data with the learned representation, and the network is trained with these labels in a supervised manner. The clustering algorithm can discover the intrinsic structure of the representation of the unlabeled data, providing meaningful supervisory signals comparing to con trastive learning which draws negative samples uniformly from the training data without label information. The idea behind the proposed framework is to take advantage of the DNN‚Äôs ability to learn from data with label noise and bootstrap its discrimina tive power. In this work, we extend this iterative labeling framework to multimodal audiovisual data, considering that complemen tary information from different modalities can help the cluster ing algorithm generate more meaningful supervisory signals.SpeciÔ¨Åcally, we train a visual representation network to en code face information using the pseudo labels generated by au dio data. With the resulted visual representations, clustering is performed to generate pseudo labels for visual data. Then, we employ a cluster ensemble algorithm to fuse pseudolabels gen erated by different modalities. This fused pseudolabel is then used to train speaker and face representation networks. With the clustering ensemble algorithm, information in one modal ity can Ô¨Çow to the other modality, providing more robust and faulttolerant supervisory signals. 2. Methods "
338,Stochastic parameterization identification using ensemble Kalman filtering combined with expectation-maximization and Newton-Raphson maximum likelihood methods.txt,"For modelling geophysical systems, large-scale processes are described
through a set of coarse-grained dynamical equations while small-scale processes
are represented via parameterizations. This work proposes a method for
identifying the best possible stochastic parameterization from noisy data.
State-the-art sequential estimation methods such as Kalman and particle filters
do not achieve this goal succesfully because both suffer from the collapse of
the parameter posterior distribution. To overcome this intrinsic limitation, we
propose two statistical learning methods. They are based on the combination of
two methodologies: the maximization of the likelihood via
Expectation-Maximization (EM) and Newton-Raphson (NR) algorithms which are
mainly applied in the statistic and machine learning communities, and the
ensemble Kalman filter (EnKF). The methods are derived using a Bayesian
approach for a hidden Markov model. They are applied to infer deterministic and
stochastic physical parameters from noisy observations in coarse-grained
dynamical models. Numerical experiments are conducted using the Lorenz-96
dynamical system with one and two scales as a proof-of-concept. The imperfect
coarse-grained model is modelled through a one-scale Lorenz-96 system in which
a stochastic parameterization is incorpored to represent the small-scale
dynamics. The algorithms are able to identify an optimal stochastic
parameterization with a good accuracy under moderate observational noise. The
proposed EnKF-EM and EnKF-NR are promising statistical learning methods for
developing stochastic parameterizations in high-dimensional geophysical models.","The statistical combination of observations of a dynamical model with a priori information of physical laws allows the estimation of the full state of the model even when it is only ‚ãÜCorresponding author. email: pulido@unne.edu.arpartially observed. This is the main aim of data assimilatio n (Kalnay, 2002). One common challenge of evolving multisca le systems in applications ranging from meteorology, oceanog ra phy, hydrology and space physics to biochemistry and biolog  ical systems is the presence of parameters that do not rely on known physical constants so that their values are unknown an d unconstrained. Data assimilation techniques can also be fo r c/circlecopyrt0000 Tellus2 mulated to estimate these model parameters from observatio ns (Jazwinski, 1970; Wikle and Berliner, 2007). There are several multiscale physical systems which are modelled through coarsegrained equations. The most parad ig matic cases being climate models (Stensrud, 2009), largee ddy simulations of turbulent Ô¨Çows (Mason and Thomson, 1992), and electron Ô¨Çuxes in the radiation belts (Kondrashov et al. , 2011). These imperfect models need to include subgridscal e effects through physical parameterizations (Nicolis, 200 4). In the last years, stochastic physical parameterizations hav e been incorporated in weather forecast and climate models (Palme r, 2001; Shutts, 2015; Christensen et al., 2015). They are call ed stochastic parameterizations because they represent stoc hasti cally a process that is not explicitly resolved in the model, even when the unresolved process may not be itself stochastic. Th e forecast skill of ensemble forecast systems has been shown t o improve with these stochastic parameterizations (Ibid.). Deter ministic integrations with models that include these param eter izations have also been shown to improve climate features (s ee e.g. Lott et al. 2012). In general, stochastic parameteriza tions are expected to improve coarsegrained models of multisca le physical systems (Katsoulakis et al., 2003; Majda and Gersh  gorin, 2011). However, the functional form of the schemes an d their parameters, which represents smallscale effects, a re un known and must be inferred from observations. The develop ment of automatic statistical learning techniques to ident ify an optimal stochastic parameterization and estimate its para meters is, therefore, highly desirable. One standard methodology to estimate physical model pa rameters from observations in data assimilation technique s, such as the traditional Kalman Ô¨Ålter, is to augment the state space with the parameters (Jazwinski, 1970). This methodol  ogy has also been implemented in the ensemblebased Kalman Ô¨Ålter (see e.g. Anderson 2001). The parameters are constrai ned through their correlations with the observed variables. The collapse of the parameter posterior distribution found in both ensemble Kalman Ô¨Ålters (Delsole and Yang, 2010; Ruiz et al., 2013a;b; Santitissadeekorn and Jones, 2015) and par ti cle Ô¨Ålters (West and Liu, 2001) is a major contention point when one is interested in estimating stochastic parameters of nonlinear dynamical models. Hereinafter, we refer as stoch as tic parameters to those that deÔ¨Åne the covariance of a Gaussi an stochastic process (Delsole and Yang, 2010). In other words , the sequential Ô¨Ålters are, in principle, able to estimate deter ministic physical parameters, the mean of the parameter posterior di stribution, through the augmented statespace procedure, but t hey are unable to estimate stochastic parameters of the model, b e cause of the collapse of the corresponding posterior distri bution. Using the Kalman Ô¨Ålter with the augmentation method, Delsol e and Yang (2010) proved analytically the collapse of the para m eter covariance in a Ô¨Årstorder autoregressive model. They pro posed a generalized maximum likelihood estimation using an approximate sequential method to estimate stochastic para m eters. Carrassi and Vannitsem (2011) derived the evolution of the augmented error covariance in the extended Kalman Ô¨Ålter using a quadratic in time approximation that mitigates the c ol lapse of the parameter error covariance. Santitissadeekor n and Jones (2015) proposed a particle Ô¨Ålter blended with an ensem ble Kalman Ô¨Ålter and use a random walk model for the parameters. This technique was able to estimate stochastic parameters i n the Ô¨Årstorder autoregressive model, but a tunable parameter i n the random walk model needs to be introduced. The ExpectationMaximization (EM) algorithm (Dempster et al., 1977; Bishop, 2006) is a widely used methodology to maximize the likelihood function in a broad spectrum of ap plications. One of the advantages of the EM algorithm is that its implementation is rather straigthforward. Wu (1983) sh owed that if the likelihood is smooth and unimodal, the EM algorit hm converges to the unique maximum likelihood estimate. Accel  erations of the EM algorithm have been proposed for its use in machine learning (Neal and Hinton, 1999). Recently, it wa s used in an application with a highly nonlinear observation o p erator (Tandeo et al., 2015). The EM algorithm was able to es timate subgridscale parameters with good accuracy while s tan dard ensemble Kalman Ô¨Ålter techniques failed. It has also be en applied to the Lorenz63 system to estimate model error cova ri ance (Dreano et al., 2017). In this work, we combine for stochastic parameterization identiÔ¨Åcation these two independent methodologies: the en sem ble Kalman Ô¨Ålter (Evensen, 1994; 2003) for the stateestima te with maximum likelihood estimators, the EM (Dempster et al. , 1977; Bishop, 2006) and the NewtonRaphson (NR) algorithms (Capp¬¥ e et al., 2005). The derivation of the technique is ex plained in detail and simple terms so that readers that are not from those communities can understand the basis of the methodologies, how they can be combined, and hopefully fore  see potential applications in other geophysical systems. T he learning statistical techniques are suitable to infer the f unctional form and the parameter values of stochastic parameterizati ons in chaotic spatiotemporal dynamical systems. They are eva lSTOCHASTIC PARAMETERIZATION IDENTIFICATION USING MAXIMU M LIKELIHOOD METHODS 3 uated here on a twoscale spatially extended chaotic dynam ical system (Lorenz, 1996) to estimate deterministic physi cal parameters, together with additive and multiplicative sto chastic parameters. Pulido et al. (2016) evaluated methods based on the EnKF alone to estimate subgridscale parameters in a twosc ale system: they showed that an ofÔ¨Çine estimation method is able to recover the functional form of the subgridscale parameter iza tion, but none of the methods was able to estimate the stochas tic component of the subgridscale effects. In the present work , the results show that the NR and EM techniques are able to uncover the functional form of the subgridscale parameterization while succesfully determining the stochastic parameters of the r epre sentation of subgridscale effects. This work is organized as follows. Section 2 brieÔ¨Çy intro duces the EM algorithm and derives the marginal likelihood o f the data using a Bayesian perspective. The implementation o f the EM and NR likehood maximization algorithms in the con text of data assimilation using the ensemble Kalman Ô¨Ålter is also discussed. Section 3 describes the experiments which a re based on the one and twoscale Lorenz96 systems. The forme r includes simple deterministic and stochastic parameteriz ations to represent the effects of the smaller scale to mimic the two  scale Lorenz96 system. Section 4 focuses on the results: Se c tion 4.1 discusses the experiments for the estimation of mod el noise. Section 4.2 shows the results of the estimation of det er ministic and stochastic parameters in a perfectmodel scen ario. Section 4.3 shows the estimation experiments for an imperfe ct model. The conclusions are drawn in Section 5. 2. Methodology "
339,Jo-SRC: A Contrastive Approach for Combating Noisy Labels.txt,"Due to the memorization effect in Deep Neural Networks (DNNs), training with
noisy labels usually results in inferior model performance. Existing
state-of-the-art methods primarily adopt a sample selection strategy, which
selects small-loss samples for subsequent training. However, prior literature
tends to perform sample selection within each mini-batch, neglecting the
imbalance of noise ratios in different mini-batches. Moreover, valuable
knowledge within high-loss samples is wasted. To this end, we propose a
noise-robust approach named Jo-SRC (Joint Sample Selection and Model
Regularization based on Consistency). Specifically, we train the network in a
contrastive learning manner. Predictions from two different views of each
sample are used to estimate its ""likelihood"" of being clean or
out-of-distribution. Furthermore, we propose a joint loss to advance the model
generalization performance by introducing consistency regularization. Extensive
experiments have validated the superiority of our approach over existing
state-of-the-art methods.","DNNs have recently lead to tremendous progress in var ious computer vision tasks [14, 28, 42, 25, 40, 21]. These successes largely attribute to largescale datasets with re liable annotations ( e.g., ImageNet [4]). However, col lecting wellannotated datasets is extremely laborintensive and timeconsuming, especially in domains where expert knowledge is required ( e.g., Ô¨Ånegrained categorization [37, 36]). The high cost of acquiring largescale well labeled data poses a bottleneck in employing DNNs in real world scenarios. As an alternative, employing web images to train DNNs has received increasing attention recently [20, 41, 43, 34, CleansamplesCleansamplesUncleansamples CleansamplesUncleansamples IDsamplesOODsamplesCleansamplesIDsamplesOODsamplesminibatchiminibatchjminibatchiminibatchjfixedrater‚Ä¶‚Ä¶‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶(a) (b) ‚Ä¶‚Ä¶Figure 1. Existing smallloss based sample selection methods (upper ) tend to regard a humandeÔ¨Åned proportion of samples within each minibatch as clean ones. They ignore the Ô¨Çuctu ation of noise ratios in different minibatches. On the contrary, our proposed method ( bottom ) selects clean samples in a global manner. Moreover, indistribution (ID) noisy samples and outof distribution (OOD) ones are also selected and leveraged for en hancing the model generalization performance. 46, 45, 52, 53, 32]. Unfortunately, whereas web images are cheaper and easier to obtain via image search engines [5, 29, 47, 44], they usually yield inevitable noisy labels due to the errorprone automatic tagging system or nonexpert annotations [23, 32, 46, 48]. A recent study has suggested that samples with noisy labels would be unavoidably over Ô¨Åtted by DNNs and consequently cause performance degra dation [15, 51]. To alleviate this issue, many methods have been pro posed for learning with noisy labels. Early approaches primarily attempt to correct losses during training. Some methods correct losses by introducing a noise transition ma trix [31, 24, 6, 11]. However, estimating the noise transition matrix is challenging, requiring either prior knowledge or a subset of welllabeled data. Some methods design noisearXiv:2103.13029v1  [cs.CV]  24 Mar 2021robust loss functions which correct losses according to pre dictions of DNNs [26, 55, 34]. However, these methods are prone to fail when the noise ratio is high. Another active research direction in mitigating the nega tive effect of noisy labels is training DNNs with selected or reweighted training samples [12, 27, 22, 8, 50, 38, 32]. The challenge is to design a proper criterion for identifying clean samples. It has been recently observed that DNNs have a memorization effect and tend to learn clean and simple pat terns before overÔ¨Åtting noisy labels [15, 51]. Thus, stateof theart methods ( e.g., Coteaching [50], Coteaching+ [50], and JoCoR [38]) propose to select a humandeÔ¨Åned propor tion of smallloss samples as clean ones. Although promis ing performance gains have been witnessed by employ ing the smallloss sample selection strategy, these meth ods tend to assume that noise ratios are identical among all minibatches. Hence, they perform sample selection within each minibatch based on an estimated noise rate. How ever, this assumption may not hold true in realworld cases, and the noise rate is also challenging to estimate accurately (e.g., Clothing1M [39]). Furthermore, existing literature mainly focuses on closedset scenarios, in which only in distribution (ID) noisy samples are considered. In openset cases ( i.e., realworld cases), both indistribution (ID) and outofdistribution (OOD) noisy samples exist. Highloss samples do not necessarily have noisy labels. In fact, hard samples, ID noisy ones, and OOD noisy ones all produce large loss values, but the former two are potentially beneÔ¨Å cial for making DNNs more robust [32]. Motivated by the selfsupervised contrastive learning [3, 7], we propose a simple yet effective approach named JoSRC ( Joint Sample Selection and Model Regularization based on Consistency) to address aforementioned issues. SpeciÔ¨Åcally, we Ô¨Årst feed two different views of an im age into a backbone network and predict two corresponding softmax probabilities accordingly. Then we divide samples based on two likelihood metrics. We measure the likelihood of a sample being clean using the JensenShannon diver gence between its predicted probability distribution and its label distribution. We measure the likelihood of a sample being OOD based on the prediction disagreement between its two views. Subsequently, clean samples are trained con ventionally to Ô¨Åt their given labels. ID and OOD noisy sam ples are relabeled by a meanteacher model before they are backpropagated for updating network parameters. Finally, we propose a joint loss, including a classiÔ¨Åcation term and a consistency regularization term, to further advance model performance. A comparison between JoSRC and existing sample selection methods is provided in Figure 1. The ma jor contributions of this work are: (1) We propose a simple yet effective contrastive ap proach named JoSRC to alleviate the negative effect of noisy labels. JoSRC trains the network with a joint loss,including a crossentropy term and a consistency term, to obtain higher classiÔ¨Åcation and generalization performance. (2) Our proposed JoSRC selects clean samples globally by adopting the JensenShannon divergence to measure the likelihood of each sample being clean. We also propose to distinguish ID noisy samples and OOD noisy ones based on the prediction consistency between samples‚Äô different views. ID and OOD noisy samples are relabeled by a mean teacher network before being used for network update. (3) By providing comprehensive experimental results, we show that JoSRC signiÔ¨Åcantly outperforms stateof theart methods on both synthetic and realworld noisy datasets. Furthermore, extensive ablation studies are con ducted to validate the effectiveness of our approach. 2. Related Works "
340,Combining Multi-level Contexts of Superpixel using Convolutional Neural Networks to perform Natural Scene Labeling.txt,"Modern deep learning algorithms have triggered various image segmentation
approaches. However most of them deal with pixel based segmentation. However,
superpixels provide a certain degree of contextual information while reducing
computation cost. In our approach, we have performed superpixel level semantic
segmentation considering 3 various levels as neighbours for semantic contexts.
Furthermore, we have enlisted a number of ensemble approaches like max-voting
and weighted-average. We have also used the Dempster-Shafer theory of
uncertainty to analyze confusion among various classes. Our method has proved
to be superior to a number of different modern approaches on the same dataset.","Deep Learning has brought a new era in machine learning. Being able to learn more complex features from images, problems such as classication, localiza tion, segmentation has seen remarkable progress especially for natural images. Previously most signicant research in the domain of natural image process ing was performed using some sort of pattern recognition over pixels [5,9,3]. The problem that has been dealt in this paper is semantic image segmentation. Image segmentation goes beyond tasks like object recognition or localization. In this problem we are mainly interested in precise segments which semanti cally separates one object from another. While pixel level algorithms [12,8,10] provide very ne level segmentation, superpixels [18] provide much lesser com putational complexity while not compromising performance. Superpixels refer to small patches of adjacent similar pixels grouped together. We have used these su perpixels for our algorithms thus providing realtime performance. ConvolutionalarXiv:1803.05200v1  [cs.CV]  14 Mar 20182 Aritra Das et al. neural networks(CNNs) have showed tremendous performance in the eld of natural image processing as well as segmentation. In our approach we have im plemented multiple convolutional neural networks to obtain results. Any classi cation problem can be associated with uncertainty in the decision process. We have used some ensemble methods as well as DempsterShafer Theory to han dle such uncertainty. The next section will give a brief review of some related works. Section 3 will explain the methodologies. In section 4 and 5 will cover the experimentations and discussions regarding obtained results. 2 Related Works "
341,Label Aware Speech Representation Learning For Language Identification.txt,"Speech representation learning approaches for non-semantic tasks such as
language recognition have either explored supervised embedding extraction
methods using a classifier model or self-supervised representation learning
approaches using raw data. In this paper, we propose a novel framework of
combining self-supervised representation learning with the language label
information for the pre-training task. This framework, termed as Label Aware
Speech Representation (LASR) learning, uses a triplet based objective function
to incorporate language labels along with the self-supervised loss function.
The speech representations are further fine-tuned for the downstream task. The
language recognition experiments are performed on two public datasets - FLEURS
and Dhwani. In these experiments, we illustrate that the proposed LASR
framework improves over the state-of-the-art systems on language
identification. We also report an analysis of the robustness of LASR approach
to noisy/missing labels as well as its application to multi-lingual speech
recognition tasks.","The conventional approach for deriving speech representations for nonsemantic speech tasks, such as speaker and language recognition, involved the use of training deep neural models with a statistics pooling layer. Some of the popular methods in this direction include dvectors [1] and xvectors [2], where a deep neural model is trained to classify the speaker/language labels on a large corpus of supervised data. However, recent trends in speech processing has seen a paradigm shift towards selfsupervision based representation learning, mirroring the ef forts in computer vision [3] and natural language processing [4]. Some popular examples of such approaches include contrastive predictive coding (CPC) [5], wav2vec family of models [6, 7], and hidden unit BERT (HuBERT) [8]. These methods primar ily rely on learning speech representations at the framelevel with its impact reported on semantic tasks such as lowresource speech recognition [8, 9] or zero resource spoken language modeling [10]. These representations have also been investi gated for speaker and language recognition tasks [11] through various benchmarks such as SUPERB [12] and NOSS [13]. In many learning paradigms, it is plausible to have portions of pretraining data along with the corresponding metadata. In the broad spectrum of representation learning, where supervised and selfsupervised frameworks constitute the twoends of the spectrum, we hypothesize that a combination of supervision and selfsupervision based methods may be more optimal than ei ther of the two frameworks in isolation, for scenarios whereparts of the pretraining have additional metadata in the form of labels. In this paper, we propose a framework for Label Aware Speech Representation learning (LASR) for such scenarios. To the best of our knowledge, this is the first attempt to combine label information with a selfsupervision loss for nonsemantic speech tasks. The contributions from this work are as follows. 1. We propose LASR, a framework for incorporating label in formation in selfsupervised speech representation learning. 2. We demonstrate the effectiveness of LASR for language identification task and establish its efficacy even with miss ing and noisy labels. 3. Our findings demonstrate that inclusion of language infor mation in the pretraining phase results in stateofartresults on the FLEURS dataset [14]. 2. Related Work "
342,A Baseline for Multi-Label Image Classification Using An Ensemble of Deep Convolutional Neural Networks.txt,"Recent studies on multi-label image classification have focused on designing
more complex architectures of deep neural networks such as the use of attention
mechanisms and region proposal networks. Although performance gains have been
reported, the backbone deep models of the proposed approaches and the
evaluation metrics employed in different works vary, making it difficult to
compare each fairly. Moreover, due to the lack of properly investigated
baselines, the advantage introduced by the proposed techniques are often
ambiguous. To address these issues, we make a thorough investigation of the
mainstream deep convolutional neural network architectures for multi-label
image classification and present a strong baseline. With the use of proper data
augmentation techniques and model ensembles, the basic deep architectures can
achieve better performance than many existing more complex ones on three
benchmark datasets, providing great insight for the future studies on
multi-label image classification.","Multilabel image classiÔ¨Åcation has been a hot topic in com puter vision community. Its extensive applications include but are not limited to image retrieval, automatic image anno tation, web image search and image tagging [1, 2, 3, 4, 5]. The abundant labelled data (e.g. ImageNet [6]) and ad vanced computational hardware have promoted the devel opment of deep convolutional neural network (CNN) based methods on singlelabel image classiÔ¨Åcation [7, 8]. Recently, such successful models have been extended to multilabel classiÔ¨Åcation tasks with promising performance reported by [9, 10, 11, 12, 13, 14, 15], proving that CNN models are ca pable of handling this challenging and more general problem. However, due to the varying backbones [15, 16] employed in the deep models, the achieved performance cannot be di rectly compared with each other. In addition, the lack of thoroughly investigated baselines of these deep CNN models hinders an explicit evaluation of the beneÔ¨Åt brought by ad vanced frameworks specially designed for multilabel image classiÔ¨Åcation.To address the aforementioned issues, we present a thor ough investigation on different baseline deep CNN models for multilabel image classiÔ¨Åcation. We focus on two state oftheart deep CNN architectures (i.e., VGG16 [17] and ResNet101[8]) as they have been widely employed in multi label image classiÔ¨Åcation [12, 15]. We evaluate the models by taking advantage of varying data augmentation techniques and model ensemble, surprisingly achieving comparable or superior performance on three benchmark datasets than the stateoftheart results achieved by more complex models. The contributions of this work are summarized as follows: We investigate the impacts of varying image sizes and data augmentation techniques including ‚Äúmixup‚Äù which has not been employed in multilabel image classiÔ¨Åcation. We use score level fusion to investigate the complemen tarity of different models and point out possible direc tions for future model design. We present a strong baseline for multilabel image clas siÔ¨Åcation with performance comparable with stateof theart on three benchmark datasets. 2. RELATED WORK "
343,Robust Inference via Generative Classifiers for Handling Noisy Labels.txt,"Large-scale datasets may contain significant proportions of noisy (incorrect)
class labels, and it is well-known that modern deep neural networks (DNNs)
poorly generalize from such noisy training datasets. To mitigate the issue, we
propose a novel inference method, termed Robust Generative classifier (RoG),
applicable to any discriminative (e.g., softmax) neural classifier pre-trained
on noisy datasets. In particular, we induce a generative classifier on top of
hidden feature spaces of the pre-trained DNNs, for obtaining a more robust
decision boundary. By estimating the parameters of generative classifier using
the minimum covariance determinant estimator, we significantly improve the
classification accuracy with neither re-training of the deep model nor changing
its architectures. With the assumption of Gaussian distribution for features,
we prove that RoG generalizes better than baselines under noisy labels.
Finally, we propose the ensemble version of RoG to improve its performance by
investigating the layer-wise characteristics of DNNs. Our extensive
experimental results demonstrate the superiority of RoG given different
learning models optimized by several training techniques to handle diverse
scenarios of noisy labels.","Deep neural networks (DNNs) tend to generalize well when they are trained on largescale datasets with groundtruth label annotations. For example, DNNs have achieved state oftheart performance on many classiÔ¨Åcation tasks, e.g., image classiÔ¨Åcation (He et al., 2016), object detection (Gir shick, 2015), and speech recognition (Amodei et al., 2016). However, as the scale of training dataset increases, it be comes infeasible to obtain all groundtruth class labels from domain experts. A common practice is collecting the class labels from data mining on social media (Mahajan et al., 1KAIST2University of Michingan Ann Arbor3Google Brain 4University of Illinois at Urbana Champaign5AItrics. Correspon dence to: Kimin Lee <kiminlee@kaist.ac.kr >. Proceedings of the 36thInternational Conference on Machine Learning , Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).2018) or web data (Krause et al., 2016). Machinegenerated labels are often used; e.g., the Open Images Dataset V4 con tains such 70 million labels for training images (Kuznetsova et al., 2018). However, they may contain incorrect labels, and recent studies have shown that modern deep architec tures may generalize poorly from the noisy datasets (Zhang et al., 2017) (e.g., see the black line of Figure 1(a)). To address the poor generalization issue of DNNs with noisy labels, many training strategies have been investigated (Reed et al., 2014; Patrini et al., 2017; Ma et al., 2018; Han et al., 2018b; Hendrycks et al., 2018; Goldberger & BenReuven, 2017; Jiang et al., 2018; Ren et al., 2018; Zhang & Sabuncu, 2018; Malach & ShalevShwartz, 2017; Han et al., 2018a). However, using such training methods may incurs expensive backandforth costs (e.g., additional time and hyperparam eter tuning) and suffer from the reproducibility issue. This motivates our approach of developing a more plausible in ference method applicable to any pretrained deep model. Hence, our direction is complementary to the prior works: one can combine ours and a prior training method for the best performance (see Tables 3, 4, & 5 in Section 4). The key contribution of our work is to develop such an infer ence method, Robust Generative classiÔ¨Åer (RoG) , which is applicable to any discriminative (e.g., softmax) neural classi Ô¨Åer pretrained on noisy datasets (without retraining). Our main idea is inducing a better posterior distribution from the pretrained (noisy, though) feature representation by uti lizing a robust generative classiÔ¨Åer. Here, our belief is that the softmax DNNs can learn meaningful feature patterns shared by multiple training examples even under datasets with noisy labels, e.g., see (Arpit et al., 2017). To motivate our approach, we Ô¨Årst observe that training samples with noisy labels (red circles) are distributed like outliers when their hidden features are projected in a 2 dimensional space using tSNE (Maaten & Hinton, 2008) (see Figure 1(b)). In other words, this phenomena implies that DNN representations even when trained with noisy la bels may still exhibit clustering properties (i.e., the DNN learns embedding that tend to group clean examples of the same class into the clusters while pushing away the exam ples with corrupt labels outside these clusters). The obser vation inspires us to induce a generative classiÔ¨Åer on the pretrained hidden features since it can model joint data distributions P(x;y)for inputxand its label yfor outlierarXiv:1901.11300v2  [stat.ML]  13 May 2019Robust Inference via Generative ClassiÔ¨Åers for Handling Noisy Labels Softmax Generative (sample mean on noisy labels) Generative (MCD on noisy labels) Generative (MCD on noisy labels) + ensemble Generative (sample mean on clean labels) [Oracle]Test set accuracy (%)405060708090100 Noise fraction0 0.2 0.4 0.6 (a) Test set accuracy comparison Samples with clean labels Samples with noisy labels (b) Penultimate features by tSNE MCD estimatorStandard sample estimator (c) An illustration of the MCD estimator Figure 1. Experimental results under DenseNet100 and CIFAR10 with uniform noise, i.e., the labels of a given proportion of training samples are Ô¨Çipped to other labels uniformly at random. (a) Test set accuracy of softmax and generative classiÔ¨Åers with various parameter estimations. (b) Visualization of features on the penultimate layer using tSNE from training samples when the noise fraction is 20%. (c) An illustration of the MCD estimator: it is more robust against outliers by Ô¨Ånding a subset with minimum covariance determinant. detection and thus produce robust posterior P(yjx)for pre diction. Here, one may suggest to train a deep generative classiÔ¨Åer from scratch. However, such a fully generative approach is expensive and has been not popular for recent stateofart classiÔ¨Åcation. We instead postprocess a light generative classiÔ¨Åer only for inference. In particular, we propose to induce the generative classiÔ¨Åer under linear discriminant analysis (LDA) assumption and choose its parameters by the minimum covariance determi nant (MCD) (Rousseeuw, 1984) estimator which calculates more robust parameters. We provide a theoretical support on the generalization property (Durrant & Kab ¬¥an, 2010) of RoG based on MCD: it has the smaller errors on parame ter estimations provably under some Gaussian assumptions. To improve RoG further, we observe that RoG built from lowlevel features can be often more effective since DNNs tend to have similar hidden features, regardless of whether they are trained with clean or noisy labels at early layers (Arpit et al., 2017; Morcos et al., 2018). Under the obser vations, we Ô¨Ånally propose an ensemble version of RoG to incorporate all effects of low and high layers. We demonstrate the effectiveness of RoG using modern neural architectures on image classiÔ¨Åcation and natural lan guage processing tasks. In all tested cases, our methods (e.g., see green and blue lines in Figure 1(a)) signiÔ¨Åcantly outperform the softmax classiÔ¨Åer, although they use the same feature representations trained by the noisy dataset. In particular, we show that RoG can be used to further improve various prior training methods (Reed et al., 2014; Patrini et al., 2017; Ma et al., 2018; Han et al., 2018b; Hendrycks et al., 2018) which are specialized to handle the noisy en vironment. For example, we improve the test accuracy of the stateoftheart training method (Han et al., 2018b) on CIFAR100 dataset with 45% noisy labels from 33.34% to 43.02%. Finally, RoG is shown to be working properly against more semantic noisy labels (generated from a ma chine labeler) and openset noisy labels (Wang et al., 2018).2. Related work "
344,Multi-Label Text Classification using Attention-based Graph Neural Network.txt,"In Multi-Label Text Classification (MLTC), one sample can belong to more than
one class. It is observed that most MLTC tasks, there are dependencies or
correlations among labels. Existing methods tend to ignore the relationship
among labels. In this paper, a graph attention network-based model is proposed
to capture the attentive dependency structure among the labels. The graph
attention network uses a feature matrix and a correlation matrix to capture and
explore the crucial dependencies between the labels and generate classifiers
for the task. The generated classifiers are applied to sentence feature vectors
obtained from the text feature extraction network (BiLSTM) to enable end-to-end
training. Attention allows the system to assign different weights to neighbor
nodes per label, thus allowing it to learn the dependencies among labels
implicitly. The results of the proposed model are validated on five real-world
MLTC datasets. The proposed model achieves similar or better performance
compared to the previous state-of-the-art models.","MultiLabel Text ClassiÔ¨Åcation (MLTC) is the task of assigning one or more labels to each input sample in the corpus. This makes it both a challenging and essential task in Natural Language Processing(NLP). We have a set of labelled training data f(xi;yi)gn i=1; where xi2RDare the input features with Ddimen sion for each data instances and yi2f0;1gare the targets. The vector yihas one in the jth coordinate if the ith data point belongs to jth class. We need to learn a mapping (prediction rule) between the features and the labels, such that we can predict the class label vector yof a new data point xcorrectly. MLTC has many realworld applications, such as text categorization (Schapire and Singer, 2000), tag recommendation (Katakis et al., 2008), information retrieval (Gopal and Yang, 2010), and so on. Before deep learning, the solution to the MLTC task used to focus on traditional machine learning algorithms. In Proceedings of the 12th International Conference on Agents and ArtiÔ¨Åcial Intelligence (ICAART 2020) DOI: 10.5220/0008940304940505 ISBN: 9789897583957 Copyright¬©2020 by SCITEPRESS Science and Technol ogy Publications, Lda. All rights reservedDifferent techniques have been proposed in the literature for treating multilabel classiÔ¨Åcation prob lems. In some of them, multiple singlelabel classi Ô¨Åers are combined to emulate MLTC problems. Other techniques involve modifying singlelabel classiÔ¨Åers by changing their algorithms to allow their use in multilabel problems. The most popular traditional method for solv ing MLTC is Binary Relevance (BR) (Zhang et al., 2018). BR emulates the MLTC task into multiple independent binary classiÔ¨Åcation problems. How ever, it ignores the correlation or the dependencies among labels (Luaces et al., 2012). Binary Rele vance has stimulated research for Ô¨Ånding approaches to capture and explore the label correlations in various ways. Some methods, including Deep Neural Net work (DNN) based and probabilistic based models, have been introduced to model dependencies among labels, such as Hierarchical Text ClassiÔ¨Åcation. (Sun and Lim, 2001), (Xue et al., 2008), (Gopal et al., 2012) and (Peng et al., 2019). Recently Graphbased Neural Networks (Wu et al., 2019) e.g. Graph Convo lution Network (Kipf and Welling, 2016), Graph At tention Networks (Velickovic et al., 2018) and Graph Embeddings (Cai et al., 2017) have received consid erable research attention. This is due to the fact that many realworld problems in complex systems, sucharXiv:2003.11644v1  [cs.CL]  22 Mar 2020as recommendation systems (Ying et al., 2018), so cial networks and biological networks (Fout et al., 2017) etc, can be modelled as machine learning tasks over large networks. Graph Convolutional Network (GCN) was proposed to deal with graph structures. The GCN beneÔ¨Åts from the advantage of the Convo lutional Neural Network(CNN) architecture: it per forms predictions with high accuracy, but a relatively low computational cost by utilizing fewer parame ters compared to a fully connected multilayer per ceptron (MLP) model. It can also capture essential sentence features that determine node properties by analyzing relations between neighboring nodes. De spite the advantages as mentioned above, we suspect that the GCN is still missing an essential structural feature to capture better correlation or dependencies between nodes. One possible approach to improve the GCN per formance is to add adaptive attention weights depend ing on the feature matrix to graph convolutions. To capture the correlation between the labels bet ter, we propose a novel deep learning architecture based on graph attention networks. The proposed model with graph attention allows us to capture the dependency structure among labels for MLTC tasks. As a result, the correlation between labels can be au tomatically learned based on the feature matrix. We propose to learn interdependent sentence classiÔ¨Åers from prior label representations (e.g. word embed dings) via an attentionbased function. We name the proposed method Multilabel Text classiÔ¨Åcation us ingAttention based Graph Neural NET work (MAG NET). It uses a multihead attention mechanism to extract the correlation between labels for the MLTC task. SpeciÔ¨Åcally, these are the following contribu tions: ‚Ä¢ The drawbacks of current models for the MLTC task are analyzed. ‚Ä¢ A novel endtoend trainable deep network is pro posed for MLTC. The model employs Graph At tention Network (GAT) to Ô¨Ånd the correlation be tween labels. ‚Ä¢ It shows that the proposed method achieves sim ilar or better performance compared to previous Stateoftheart(SoTA) models across two MLTC metrics and Ô¨Åve MLTC datasets. 2 RELATED WORK "
345,Walk in Wild: An Ensemble Approach for Hostility Detection in Hindi Posts.txt,"As the reach of the internet increases, pejorative terms started flooding
over social media platforms. This leads to the necessity of identifying hostile
content on social media platforms. Identification of hostile contents on
low-resource languages like Hindi poses different challenges due to its diverse
syntactic structure compared to English. In this paper, we develop a simple
ensemble based model on pre-trained mBERT and popular classification algorithms
like Artificial Neural Network (ANN) and XGBoost for hostility detection in
Hindi posts. We formulated this problem as binary classification (hostile and
non-hostile class) and multi-label multi-class classification problem (for more
fine-grained hostile classes). We received third overall rank in the
competition and weighted F1-scores of ~0.969 and ~0.61 on the binary and
multi-label multi-class classification tasks respectively.","During coronavirus lockdown, number of active internet users across the globe has increased rapidly. The government enforced lockdown, which pushed people to stay indoors and thus, increased the engagement with social media platforms like Facebook, Twitter, Instagram, Whatsapp, etc. This led to increased hostile posts over social media, including cyberbullying, trolling, spreading hate, death threat, etc. A major challenge for the common users in the digital space is to identify misinformation (aka fake news) in online content. In addition to that, according to a recent survey2, on Twitter, there has been a 900% increase in hate speech directed towards Chinese people and 200% increase in the trac to Hate sites and posts written against the Asian community. It is also found3that the percentage of nonEnglish tweets in India has jumped up by 50%. This inspires a necessity of research in hostility detection in posts written in low resource but widelyused languages like Hindi. 1https://competitions.codalab.org/competitions/26654 2https://bit.ly/38BBrTu 3http://bit.ly/38Eo7gYarXiv:2101.06004v1  [cs.CL]  15 Jan 20212 Chander et al. As billions of posts appear each day on social media and antisocial elements can get full anonymity while expressing hostile behavior over the internet, iden tication of authorized information should have a reliable automation system. Even though Hindi is the third most spoken language globally, it is considered a lowresource language due to the unavailability of accurate tools and suitable datasets for various tasks in Hindi. This motivates us to take the task of hostility detection of Hindi posts on Social media. We view the hostility detection as a twostage process. First, a Coarsegrained classication is done to mark a post as hostile or nonhostile. If the post is de tected as hostile, then the second stage performs a more negrained classica tion of hostile classes. We brie y dene these two terms: 1.Coarsegrained classication : It is a binary classication problem in which each post is categorised as hostile or nonhostile. 2.Finegrained classication : It is a multilabel multiclass classication of the hostile classes. Each hostile post belongs to one or more of the following categories: fake news, hate speech, oensive and defamation. In our proposed approach, we leverage the pretrained multilingual BERT (mBERT) [6]4for input post representation and further these representations are used as input for Articial Neural Network (aka ANN) and other ML learning models for binary and multilabel multiclass classication problems. The base architecture of mBERT is the same as BERT. BERT and mBERT have proven as the state of the art models across multiple NLU and NLG tasks. The rest of this paper is organized in the following way: Section 2 presents related work; Section 3 gives an indepth explanation of our model. Section 4 presents the experimental setup. Section 5 provides results and our analysis. Finally, we provide conclusions and directions for future work in Section 6. 2 Related Work "
346,TabGSL: Graph Structure Learning for Tabular Data Prediction.txt,"This work presents a novel approach to tabular data prediction leveraging
graph structure learning and graph neural networks. Despite the prevalence of
tabular data in real-world applications, traditional deep learning methods
often overlook the potentially valuable associations between data instances.
Such associations can offer beneficial insights for classification tasks, as
instances may exhibit similar patterns of correlations among features and
target labels. This information can be exploited by graph neural networks,
necessitating robust graph structures. However, existing studies primarily
focus on improving graph structure from noisy data, largely neglecting the
possibility of deriving graph structures from tabular data. We present a novel
solution, Tabular Graph Structure Learning (TabGSL), to enhance tabular data
prediction by simultaneously learning instance correlation and feature
interaction within a unified framework. This is achieved through a proposed
graph contrastive learning module, along with transformer-based feature
extractor and graph neural network. Comprehensive experiments conducted on 30
benchmark tabular datasets demonstrate that TabGSL markedly outperforms both
tree-based models and recent deep learning-based tabular models. Visualizations
of the learned instance embeddings further substantiate the effectiveness of
TabGSL.","Tabular data is a common data type in the real world [ 5]. Among various machine learning (ML) algorithms, gradient boosted decision trees (GBDTs) have been one type of the most competitive ML to handle tabular data for many years [ 18,27,19]. Recently, many studies also tried applying deep learning (DL) methods to tabular data to improve prediction performance, which has become popular in academics and the industry [ 18,24]. Studies found that DL methods designed for other domains are also helpful for tabular data, such as FiGNN [ 36], Table2Graph [ 62], and T2GFormer [ 57]. In these methods, feature interactions are taken into account. However, associations among instances are usually ignored in these approaches. Some instances may share similar patterns of features correlated with the prediction target, which can be modeled by a graph. Recently, studies on graphs have flourished in various domains [ 55]. Due to its powerful capability to learn latent representations from relational structure, DL methods are applied to modeling graphs and lead to a thriving research topic, graph neural networks (GNNs) [ 55,37]. GNNs learn latent representations for each node by aggregating information from the node‚Äôs neighbors on the given graph structure. However, graph structures are usually generated from complex systems and thus are inevitably noisy [ 63]. Specifically, there might be redundant information in a graph structure. Some connections might be missing or even incorrect [ 9]. Without a doubt, these noises on the graph structure are harmful to GNN‚Äôs performance considerably. Besides, GNN is not available when modeling data without an apparent graph structure, such as tabular data, which is very common in the real world [ 37]. Such issues lead to studies focusing on graph structure learning (GSL), which aims Preprint. Under review.arXiv:2305.15843v1  [cs.LG]  25 May 2023to learn a reliable graph structure with less noise for GNNs. Although there have been GSL methods proposed, they mainly focused on refining an existing graph structure. Not many discussed learning graph structures purely from tabular data, where there is no available graph initially [ 9,37,63]. GNNs can model the latent associations among instances, which cannot be captured by existing treebased models. To this end, learning a credible graph structure from the given tabular dataset is crucial. Most of the existing DL methods modeling tabular data consider feature interactions. Feature reconstruction, such as VIME [ 58] and TabNet [ 2], and contrastive learning, such as SubTab [ 49] and SCARF [ 3], are welladopted approaches. However, the latent associations among instances are almost ignored. Thus, patterns of features and the response variable that some similar instances (i.e., neighbors) share cannot be identified and exploited in tabular data learning. Data instances can be correlated with each other in terms of their features. For example, users with similar profiles or online behaviors tend to have similar preferences for ads or items [ 44,23,15]. Patients with similar clinical data or symptoms have a higher potential to suffer from similar diseases [ 10,26,6]. To better represent instances for downstream tasks, rather than solely employing each instance‚Äôs self features, it is crucial to model the correlation between instances. The key idea is to exploit such correlation to learn higherquality feature representations of instances, i.e., instances with similar labels are close to one another while those with different labels are pushed away from each other in the embedding space. With proper learning of the graph structure that depicts the relationships between instances, GNN would be a good fit to let instances learn to represent each other. That said, we require both GSL and GNNs to model instance association. Regarding the learning of graph structure for tabular data prediction, three challenges need to be tackled. First, existing GSL methods require a noisy graph as the input for refinement and adjustment, but tabular data contains no graph topology in essence. We need to learn the graph structure from scratch. Second, both modeling feature interactions and instance associations are essential in tabular data learning. How to simultaneously consider them both for tabular data in a unified framework is unclear. Third, in addition to learning the graph structure from tabular data, we further need to jointly train a graph neural network to obtain feature representations of instances for final predictions. In this work, we propose a novel graph machine learning model, TabularGraph Structure Learning (TabGSL ), for tabular data prediction. The key idea is to learn the graph structure from a given tabular dataset so that the latent correlation between instances can be modeled. We propose a novel contrastive learning mechanism to learn the graph structure from tabular data. The intuition is to construct a teacher, which possesses confident knowledge about label knowledge among instances, to guide a student graph learner to continuously adjust the graph structure. Besides, to further capture the interactions between features, we adopt the transformer with tokenized features to produce feature embeddings for GSL. The learning of tabular graph structure is jointly trained with a GNN module to generate final representations of instances. We summarize the contributions of this work as follows. (a)We revisit the tabular data prediction task from the perspective of graph structure learning and graph neural networks, and highlight the potential of modeling the associations between instances and capturing the interactions between features. (b)We present a new graphbased solution, Tabular Graph Structure Learning (TabGSL), to better perform tabular data prediction by simultaneously learning instance correlation and feature interaction in a united framework. A novel graph contrastive learning module is devised to fulfill the goal. (c)Experiments conducted on 30 benchmark tabular datasets exhibit that the proposed TabGSL significantly and consistently outperforms treebased models and recent deep learningbased tabular models. Visualization plots also show the effectiveness of the learned instance embeddings. This paper is organized as below. We review relevant studies in Section 2, followed by the description of the problem statement in Section 3. The methods and tasks involved in learning graph structures for tabular data prediction are presented in Section 4. We give the evaluation plan and discuss the experimental results in Section 5. Section 6 concludes this work. 2 Related Work "
347,No Regret Sample Selection with Noisy Labels.txt,"Deep neural networks (DNNs) suffer from noisy-labeled data because of the
risk of overfitting. To avoid the risk, in this paper, we propose a novel DNN
training method with sample selection based on adaptive k-set selection, which
selects k (< n) clean sample candidates from the whole n noisy training samples
at each epoch. It has a strong advantage of guaranteeing the performance of the
selection theoretically. Roughly speaking, a regret, which is defined by the
difference between the actual selection and the best selection, of the proposed
method is theoretically bounded, even though the best selection is unknown
until the end of all epochs. The experimental results on multiple noisy-labeled
datasets demonstrate that our sample selection strategy works effectively in
the DNN training; in fact, the proposed method achieved the best or the
second-best performance among state-of-the-art methods, while requiring a
significantly lower computational cost. The code is available at
https://github.com/songheony/TAkS.","Deep neural networks (DNNs) require a large number of ‚Äúcorrectlylabeled‚Äù samples to achieve highperformance classiÔ¨Åcation [43]. However, it is practically difÔ¨Åcult for many CV/PRrelated datasets to guarantee the correctness of the attached labels [6, 29]. For example, datasets an notated via crowdsourcing [37, 42] often become noisy labeled samples, which contain a certain number of sam ples with incorrect labels. Beyer et al. [2] point out that Im ageNet contains several incorrectly labeled samples, even though each sample in ImageNet is labeled by a careful majority voting scheme among at least 10 workers [5]. Datasets created by automatic data collection and annota tion, such as Clothing1M [36], will also contain a large number of incorrectly labeled samples. A possible remedy for noisylabeled samples is sample selection , which is a method to select a clean subset of train ing samples (i.e., correctly labeled training samples) from the whole sample set. A typical strategy is cotraining, which compares the results from two DNNs to detect theincorrectly labeled samples, as shown in Fig. 1 (a). This strategy has achieved stateoftheart performance so far but requires extra computations due to its coupledDNN struc ture. Another drawback is that it has no theoretical guaran tee on its performance. We can consider another sample selection strategy by using the idea of kset, which is a subset of ksamples. Fig. 1 (b) shows the most naive ksetbased sample selec tion strategy; after generating all n k ksets exhaustively from the whole training set with nsamples, n k DNNs are trained independently with individual ksets. If there are more thankclean samples, some DNNs are trained only by clean samples and thus avoid the degradation by incorrectly labeled samples. However, this naive and exhaustive kset strategy is obviously intractable in practical scenarios with largernandk. This paper proposes a method where kset selection and DNN training by the selected ksamples are performed al ternatively. Let dtdenote thendimensional kset vec tor showing the selection at epoch tandt 1denote the ndimensional noiserisk vector whoseith element is the noiserisk of the ith training sample, which is assessed at t 1. The noiserisk is the likelihood that the sample is incorrectly labeled. In the ideal case when tis highly re liable, we can select the ksetdtthat minimizes dtt 1. However, this greedy strategy does not work because of the difÔ¨Åculty of having a reliable tduring the training process. We, therefore, design our method to suppress the total selection risk , which is deÔ¨Åned asPT t=1dtt, as shown in Fig. 1 (c). By achieving a smaller total selection risk at T, we can believe that DNN is trained with less incorrectly labeled samples on average during the entire training pro cess. Due to the online (i.e., nonpredictable) nature of the training process, one may imagine that it is difÔ¨Åcult to de termine dtattwhile guaranteeing a smaller total selec tion risk. We prove that it is still possible to achieve a total selection risk smaller than a theoretical bound, based on the theory of adaptivekset selection [33] with Follow thePerturbed Leader (FPL) algorithm, as discussed later. Hereafter we call the proposed method training DNN with sample selection based on adaptive kset selection (TAkS).arXiv:2003.03179v5  [cs.LG]  4 Apr 2021(b) exhaustive ùëòset all ùëòsets CNN CNN CNN ‚ãØ ‚ãØ CNN1 CNN2 (a) cotraining (SOTA) ‚ãÆ ‚ãÆ ùëëùë°‚àôùúÉùë°ùëá ùë°=1 (c) total selection risk ùúÉùë° ùëëùë°ùúÉ1 ùëë1 ùúÉùëá ùëëùëá‚ãÆ ‚ãÆ (d) proposed (TAkS)  ùúÉùë° ‚ãÆ followtheperturbedleader CNNFigure 1: (a) Cotraining. (b) The most naive and computationallyintractable kset selection method. (c) Total selection risk. (d) The proposed method based on adaptive kset selection. The vectors tanddtare the noiserisk vector and the khot vector at epoch t, respectively. In (d), the perturbation mechanism in the kset selection is omitted for simplicity. Fig. 1 (d) shows TA kS, which has the following three ad vantages. The Ô¨Årst and most important advantage is its theo retical support. SpeciÔ¨Åcally, its performance is theoretically guaranteed in terms of regret . Regret is a performance mea sure and is often used in theoretical machine learning re search. In our kset selection task, the regret RTis deÔ¨Åned as the difference between the actual total selection risk and the minimum total selection risk with the best ksetd. (See Eq. (2) for the formal deÔ¨Ånition of RT.) Although dis un known until T,RTof TA kS is upperbounded; this means that thekset selection by TA kS is not far from d, even under the uncertain online nature of the selection process. The second advantage is its computational efÔ¨Åciency. TAkS selects an appropriate ksetdtfrom all the n k kset candidates with just O(n)time complexity at each t. In ad dition, TA kS trains a single DNN, whereas the stateofthe art sample selection methods (i.e., the cotraining methods) need to train coupledDNNs as noted above. Finally, TA kS achieves the best or nearbest classiÔ¨Åca tion accuracy, as shown by our experimental results on sev eral benchmark datasets, such as MNIST [15], CIFAR10, CIFAR100 [14], and Clothing1M [36], which have often been used in the past studies of noisylabeled samples. Un der various noise settings, TA kS achieves the best or the secondbest accuracy and time efÔ¨Åciency among various stateoftheart sample selection methods [7, 34, 40]. The summary of our main contributions are as follows: ‚Ä¢ For learning with noisylabels, we propose a novel and efÔ¨Åcient method, TA kS, which tries to suppress the to tal selection risk based on the theory of adaptive kset selection. To the best of our knowledge, TA kS is the Ô¨Årst method that utilizes the idea of adaptive kset se lection for noisylabeled samples. ‚Ä¢ We discuss how the theoretical support of TA kS is use ful for selecting a promising kset from noisylabeled samples. We also prove how the performance of TA kS is guaranteed in terms of the total selection risk. ‚Ä¢ The experimental results on multiple noisylabeleddatasets show that TA kS achieved the best or the secondbest performance in not only classiÔ¨Åcation ac curacy but also time efÔ¨Åciency among stateoftheart methods. We also conÔ¨Årmed that TA kS selects more clean samples than the other comparative methods. 2. Related Work "
348,The Devil is in the Labels: Noisy Label Correction for Robust Scene Graph Generation.txt,"Unbiased SGG has achieved significant progress over recent years. However,
almost all existing SGG models have overlooked the ground-truth annotation
qualities of prevailing SGG datasets, i.e., they always assume: 1) all the
manually annotated positive samples are equally correct; 2) all the
un-annotated negative samples are absolutely background. In this paper, we
argue that both assumptions are inapplicable to SGG: there are numerous ""noisy""
groundtruth predicate labels that break these two assumptions, and these noisy
samples actually harm the training of unbiased SGG models. To this end, we
propose a novel model-agnostic NoIsy label CorrEction strategy for SGG: NICE.
NICE can not only detect noisy samples but also reassign more high-quality
predicate labels to them. After the NICE training, we can obtain a cleaner
version of SGG dataset for model training. Specifically, NICE consists of three
components: negative Noisy Sample Detection (Neg-NSD), positive NSD (Pos-NSD),
and Noisy Sample Correction (NSC). Firstly, in Neg-NSD, we formulate this task
as an out-of-distribution detection problem, and assign pseudo labels to all
detected noisy negative samples. Then, in Pos-NSD, we use a clustering-based
algorithm to divide all positive samples into multiple sets, and treat the
samples in the noisiest set as noisy positive samples. Lastly, in NSC, we use a
simple but effective weighted KNN to reassign new predicate labels to noisy
positive samples. Extensive results on different backbones and tasks have
attested to the effectiveness and generalization abilities of each component of
NICE.","Scene Graph Generation (SGG), i.e., detecting all object instances and their pairwise visual relations, is a crucial steparXiv:2206.03014v1  [cs.CV]  7 Jun 2022towards comprehensive visual scene understanding. In gen eral, each scene graph is a visuallygrounded graph, where each node and edge refer to an object and visual relation, re spectively. Recently, with the release of several largescale SGG benchmarks ( e.g., Visual Genome (VG) [15]) and ad vanced object detectors [28, 1, 35], SGG has received un precedented attention [7]. However, due to the composi tional nature of pairwise visual relations, the number distri butions of different triplets in SGG datasets are much more imbalanced ( i.e., longtailed) than other recognition tasks. Accordingly, the performance of many stateoftheart SGG models [42, 2, 32, 23] degrades signiÔ¨Åcantly on the tailcat egories1compared to the head categories counterparts. Currently, the mainstream solutions to mitigate the long tailed problem in SGG can be coarsely categorized into two types: 1) Rebalancing strategy : It utilizes classaware sam ple resampling or loss reweighting to balance the propor tions of different predicate categories in the network train ing. The former attempts to balance the number of training samples in instancelevel2or imagelevel [17], and the latter leverages prior commonsense knowledge ( e.g., frequency of predicates [22], predicate correlations [39], or rulebased predicate priority [23, 14]) to reweight the contributions of different categories in loss calculations. 2) Biasedmodel based strategy : It inferences debiased predictions from pre trained biased SGG models. For instance, using counterfac tual causality to disentangle frequency biases [31], deriving more balanced loss weights for different predicates [41], or adjusting the probabilities of predicate predictions [4]. Although these methods have dominated performance on debiasing metrics ( e.g., mean Recall@K), it is worth noting that almost all existing models have taken two plausible as sumptions about the groundtruth annotations for granted: Assumption 1: All the manually annotated positive sam ples are equally correct. Assumption 2: All the unannotated negative samples are absolutely background. For the Ô¨Årst assumption, by ‚Äúequally‚Äù, we mean that the conÔ¨Ådence (or quality) of annotated groundtruth predicate label for each positive sample2is the same as others, i.e., all positive predicate labels are of high quality. Unfortunately, unlike other closeset classiÔ¨Åcation tasks where each sample has only a unique groundtruth label, a subjectobject pair in SGG sometimes has multiple reasonable predicates. This phenomenon has led to two inevitable annotation charac teristics in SGG datasets: 1) Commonprone : When these reasonable relations are in different semantic granularities, 1For brevity, we directly use ‚Äútail‚Äù, ‚Äúbody‚Äù, and ‚Äúhead‚Äù categories to represent the predicate categories in the tail, body, and head parts of the number distributions of different predicates in SGG datasets, respectively. 2We use ‚Äúinstance‚Äù to denote an instance of visual relation triplet, and we also use ‚Äúsample‚Äù to represent the triplet instance interchangeably.the annotators tend to select the most common predicate (or coarsegrained) as groundtruth. As shown in Figure 1(a), bothriding andonare ‚Äúreasonable‚Äù for man andbike , but the annotated groundtruth predicate is less informative oninstead of more convincing riding . And this charac teristic is very common in SGG datasets (more examples in Figure 1(a)). 2) Synonymrandom : When these reasonable relations are synonymous for the subjectobject pair, the an notators usually randomly choose one predicate as ground truth, i.e., the annotations for some similar visual patterns are inconsistent. For example, in Figure 1(b), both has and with denote ‚Äúbe dressed in‚Äù for man/woman andshirt , but the groundtruth annotations are inconsistent even in the same image. We further visualize thousands of sampled in stances ofhmanhas /with shirtiin VG, and these in stances are all randomly distributed in the feature space (cf. Figure 1(b)). Thus, we argue that all the positive samples are NOT equally correct, i.e., a part of positive samples are not highquality ‚Äî their labels can be more Ô¨Ånegrained (cf. commonprone) or more consistent (cf. synonymrandom). For the second assumption, although all SGG works have agreed that visual relations in existing datasets are always sparsely identiÔ¨Åed and annotated [25] (Figure 1(c)), almost all of them still train their models by regarding all the un annotated pairs as background ,i.e., there is no visual re lation between the subject and object. In contrast, we argue that all negative samples are NOT absolutely background, i.e., a part of negative samples are not highquality ‚Äî they are actually foreground with missing annotations. In this paper, we try to get rid of these two questionable assumptions, and reformulate SGG as a noisy label learning problem. To the best of our knowledge, we are the Ô¨Årst work to take a deep dive into the groundtruth annotation qualities of both positive and negative samples in SGG. SpeciÔ¨Åcally, we propose a novel modelagnostic NoIsy label CorrEction strategy, dubbed as NICE . NICE can not only detect numer ousnoisy samples, but also reassign more highquality pred icate labels to them. By ‚Äúnoisy‚Äù, we mean that these sam ples break these two assumptions. After the NICE training, we can obtain a cleaner version of dataset for SGG training. Particularly, we can: 1) increase the number of Ô¨Ånegrained predicates ( commonprone ); 2) decrease annotation incon sistency among similar visual patterns ( synonymrandom ); 3) increase the number of positive samples ( assumption 2 ). NICE consists of three components: negative noisy sam ple detection (NegNSD), positive noisy sample detection (PosNSD), and noisy sample correction (NSC). Firstly, in NegNSD, we reformulate the negative NSD as an outof distribution (OOD) detection problem, i.e., regarding all the positive samples as indistribution (ID) training data, and all the unannotated negative samples as OOD test data. In this way, we can detect the missing annotated (ID) samples with pseudo labels. Then, in PosNSD, we use a clusteringbasedalgorithm to divide all positive samples (including the out puts of NegNSD) into multiple sets, and regard samples in the noisiest set as noisy positive samples. The clustering re sults are based on the local density of each sample. Lastly, in NSC, we use a simple but effective weighted KNN to reassign new predicate labels to all noisy positive samples. We evaluate NICE on the most prevalent SGG bench mark: VG [15]. Since NICE only focuses on reÔ¨Åning noisy annotations of the dataset, it can be seamlessly incorporated into any SGG architecture to boost their performance. Ex tensive ablations have attested to the effectiveness and gen eralization abilities of each component of NICE. In summary, we make three contributions in this paper: 1. We are the Ô¨Årst to reformulate SGG as a noisy label learn ing problem, and point out the two plausible assumptions are not applicable for SGG, i.e., the devil is in the labels. 2. We propose a novel modelagnostic strategy NICE. Ex tensive ablations on several baselines, tasks, and metrics have demonstrated its excellent generalization abilities. 3. Each part of NICE can serve as an independent plugand play module to improve SGG annotation qualities3. 2. Related Work "
349,Multi-Representational Learning for Offline Signature Verification using Multi-Loss Snapshot Ensemble of CNNs.txt,"Offline Signature Verification (OSV) is a challenging pattern recognition
task, especially in presence of skilled forgeries that are not available during
training. This study aims to tackle its challenges and meet the substantial
need for generalization for OSV by examining different loss functions for
Convolutional Neural Network (CNN). We adopt our new approach to OSV by asking
two questions: 1. which classification loss provides more generalization for
feature learning in OSV? , and 2. How integration of different losses into a
unified multi-loss function lead to an improved learning framework? These
questions are studied based on analysis of three loss functions, including
cross entropy, Cauchy-Schwarz divergence, and hinge loss. According to
complementary features of these losses, we combine them into a dynamic
multi-loss function and propose a novel ensemble framework for simultaneous use
of them in CNN. Our proposed Multi-Loss Snapshot Ensemble (MLSE) consists of
several sequential trials. In each trial, a dominant loss function is selected
from the multi-loss set, and the remaining losses act as a regularizer.
Different trials learn diverse representations for each input based on
signature identification task. This multi-representation set is then employed
for the verification task. An ensemble of SVMs is trained on these
representations, and their decisions are finally combined according to the
selection of most generalizable SVM for each user. We conducted two sets of
experiments based on two different protocols of OSV, i.e., writer-dependent and
writer-independent on three signature datasets: GPDS-Synthetic, MCYT, and
UT-SIG. Based on the writer-dependent OSV protocol, we achieved substantial
improvements over the best EERs in the literature. The results of the second
set of experiments also confirmed the robustness to the arrival of new users
enrolled in the OSV system.","Despite impressive advances in computer vision and pattern recognition, several classic  problems such as  Offline Signature Verification (OSV) remains challenging (Hafemann, Sabourin, & Oliveira, 2017b; E. Zois,  Tsourounis, Theodorakopoulos, Kesidis, & Economou, 2019) . The OSV system aims to distinguish whether  a given signature image is produced by the claimed author (genuine) or by an impostor (forgery). There are  three types of forgeries, including random, simple and skilled  forgeries. Random forgeries refer to the ca ses  when forger signs without information about the user. In the case of simple forgeries, forger knows the  user‚Äôs name while the user‚Äôs signature is not available. In the worst case, forger  has ac cess to  both the  user‚Äôs name and his signature and carefull y attempts to forge the signature. In this case, skilled forgeries  show high similarity with the user‚Äôs signature, and thus are harder to verify. The problem of OSV becomes  challenging in the presence of skilled forgeries. This challenge is further aggrava ted in the real scenario of OSV when a few genuine signatures and no skilled forgeries are available for training (Hafemann et al.,  2017b) .  Recently, howe ver, automatic feature learning by deep Convolutional Neural Networks (CNNs) has shown  its potential to tackle the challenges of OSV (Dey et al., 2017; Hafemann, Sabourin, & Oliveira, 2017a) . Due  to the small  training size, the power  of Deep CNNs may not be effectively leveraged  in the problem of OSV  unless seriously considering regularization . Choosing a suitable loss function is one of most  effective  learning parameters in the generalization  of CNNs  (Janocha & Czarnecki, 2017) . Substantial need for the  generalization in OSV lead researchers to examine different loss functions for CNNs to find which one  promotes more generalization.   Two main categories of loss functions were studied  for OSV, including metric learning losses and  classification (identification) losses. In the first category, several studies  (Berkay Yilmaz & Ozturk, 2018; Dey  et al., 2017; Rantzsch, Yang, & Meinel, 2016; Soleimani, Araabi, & Fouladi, 2016; Xing, Yin, Wu, & L iu, 2018)   suggested several metric learning losses for feature learning in CNNs and a threshold based verification  was then used to classify whether an input signature is genuine or forgery. While the second category  includes the studies (Hafemann, Sabourin, & Oliveira, 2016b; Hafemann et al., 2017b; Souza, Oliveira, &  Sabourin, 2018)  employing classification loss function for feature learning in CNNs and verificati on is then  performed using SVM as a binary classifier.  These two categories competed to achieve better accuracy,  while the works in the second category mostly obtained more accuracy compared to the first one.   In addition to OSV, these two categories of loss functions were also studied  for different classification tasks,  and the similar comparative results were reported  (Horiguchi, Ikami, & Aizawa, 2016; Janocha & Czarnecki,  2017) . The work (Horiguchi et al., 2016)  confirmed this result by performing a fair comparison  between  Cross Entropy  (CE) loss as a classification loss and several state oftheart metric learning losses.  Furthermore, another study (Janocha & Czarnecki, 2017)  obtained similar  results, however, proposed that  other classification losses rather than CE also have their merits. Due to the superiority  of classification  losses, we follow the approach of the second  category in this research. In OSV literature, a few studies  employed CE loss function for feature learning in CNNs. However, as the best of our knowledge, an  interesting question on ‚Äú which classification loss leads to m ore generalization in OSV? ‚Äù has not been yet  studied in OSV.   From another point  of view, different loss functions have complementary advantages and limitations  (Janocha & Czarnecki, 2017; C. Xu et al., 2016) . Ensemble learning (Masoudnia & Ebrahimpour, 2012)  can  provid e a framework for combining the  advantages of different loss functions. Moreover, the ensemble   framework brings more diversity and regularization  and thus more generalization  for OSV(Berkay Yilmaz &  Ozturk, 2018; Yƒ±lmaz & Yanƒ±koƒülu, 2016) . However, why  diversity in terms of  different loss functions in the  ensemble can also improve  the OSV?   The OSV can benefit from employing different loss functions by which diverse feature sets are learn ed from  signature images. Thanks to these diverse repre sentations , the discrimination  power of the OSV  can be  improved. This is because features that discriminate between genuine  signature s of a user and his/her skilled forgeries are different from  the features required to discriminate genuine  and skilled forgeries  of  another user.   The diverse representations learn ed by different classification losses also provide another advantage for  the OSV. The methods in the second category employ a CNN trained with a classification loss on the  signature identification task. H owever, the learn ed representation is then used  for signature  verification.  In the signature application, the identification is simpler  than the verification, while the learn ed features  should be transferred  to the harder  task. These learned features do no t necessarily include fine features  required for the verification of genuine vs. skilled forgery signatures. Using diverse multi representation  set learned by different loss functions may address this issue since it extends the feature set for the  verifica tion. Moreover, the learn ed diverse feature sets may capture infrequent features (Xie, Deng, & Xing,  2015) .  The need for diversity and regularization in the OSV lead us to ensemble approaches. However, the  ensemble o f CNNs in which each network is independently trained  on a different loss function brings heavy   computational burden. We attempt to address this limitation using a multi loss function which eliminate s  the need for learning each of loss functions from scrat ch.   In this study, we aim to fill the gap in the literature by examining different loss functions and proposing a  novel framework for using a dynamic multi loss function in CNNs. We adopt  a new approach to the OSV  problem by asking two questions: 1. which classification loss function provides more generalization for  feature learning in OSV? , and  2. How  may an integration of different loss functions into a dynamic multi  loss function  lead to an improved learning framework?   These questions are studied in the remaining of this paper as follows: The related works on the literature   of OSV are reviewed  in the following section. Our approach to the first question is presented  in section 3.  Section 4 and 5 explains our proposed mul tiloss ensemble approach for feature learning and the  verification method in OSV, respectively. The experimental results are provided  in section 6. We finally  conclude the results and propose future research directions in the last section.      2. Related Works   "
350,Improving Robustness and Generality of NLP Models Using Disentangled Representations.txt,"Supervised neural networks, which first map an input $x$ to a single
representation $z$, and then map $z$ to the output label $y$, have achieved
remarkable success in a wide range of natural language processing (NLP) tasks.
Despite their success, neural models lack for both robustness and generality:
small perturbations to inputs can result in absolutely different outputs; the
performance of a model trained on one domain drops drastically when tested on
another domain.
  In this paper, we present methods to improve robustness and generality of NLP
models from the standpoint of disentangled representation learning. Instead of
mapping $x$ to a single representation $z$, the proposed strategy maps $x$ to a
set of representations $\{z_1,z_2,...,z_K\}$ while forcing them to be
disentangled. These representations are then mapped to different logits $l$s,
the ensemble of which is used to make the final prediction $y$. We propose
different methods to incorporate this idea into currently widely-used models,
including adding an $L$2 regularizer on $z$s or adding Total Correlation (TC)
under the framework of variational information bottleneck (VIB). We show that
models trained with the proposed criteria provide better robustness and domain
adaptation ability in a wide range of supervised learning tasks.","Supervised neural networks have achieved remark able success in a wide range of NLP tasks, such as language modeling (Xie et al., 2017; Devlin et al., 2018a; Liu et al., 2019; Joshi et al., 2020; Meng et al., 2019b), machine reading comprehen sion (Seo et al., 2016; Yu et al., 2018), and machine translation (Sutskever et al., 2014; Vaswani et al., 2017b; Meng et al., 2019a). Despite the success,neural models lack for both robustness and gener ality and are extremely fragile: the output label can be changed with a minor change of a single pixel (Szegedy et al., 2013; Goodfellow et al., 2014b; Nguyen et al., 2015; Papernot et al., 2017; Yuan et al., 2019) in an image or a token in a document (Li et al., 2016; Papernot et al., 2016; Jia and Liang, 2017; Zhao et al., 2017; Ebrahimi et al., 2017; Jia et al., 2019b); The model lacks for domain adapta tion abilities (Mou et al., 2016; Daum√© III, 2009): a model trained on one domain can hardly generalize to new test distributions (Fisch et al., 2019; Levy et al., 2017). Despite that different avenues have been proposed to address model robustness such as augmenting the training data using rulebased lexical substitutions (Liang et al., 2017; Ribeiro et al., 2018) or paraphrase models (Iyyer et al., 2018), building robust and domainadaptive neural models remains a challenge. In a standard supervised learning setup, a neural network model Ô¨Årst maps an input xto a single vectorz=f(x).zcan be viewed as the hidden feature to represent x, and is transformed to its logitlfollowed by a softmax operator to output the target label y. At training time, parameters in volved in mapping from x2Xtozthen toyare learned. At test time, the pretrained model makes a prediction when presented with a new instance x02X0. This methodology works well if Xand X0come from exactly the same distribution, but signiÔ¨Åcantly suffers if not. This is because the implicit representation learned through supervised signals can easily and overÔ¨Åt to the training do mainX, and the mapping function f(x), which is trained only based on X, can be confused with outofdomain features in x0, such as a lexical, prag matic, and syntactic variation not seen in the train ing set (Ettinger et al., 2017). We can also interpret the weakness of this methodology from a domainarXiv:2009.09587v1  [cs.CL]  21 Sep 2020adaptation point of view (Daume III and Marcu, 2006; Daum√© III, 2009; Tan et al., 2009; Patel et al., 2014): it is crucial to separate sourcespeciÔ¨Åc fea tures, targetspeciÔ¨Åc features and general features (features shared by sources and targets). One of the most naive strategies for domain adaptation is to ask the model to only use general features for test. In the standard x!z!ysetup, all features, including sourcespeciÔ¨Åc, targetspeciÔ¨Åc and gen eral features, are entangled in z. Due to the lack of interpretability (Li et al., 2015; Linzen et al., 2016; Lei et al., 2016; Koh and Liang, 2017) of neural models, it is impossible to disentangle them. Inspired by recent work in disentangled representa tion learning (Bengio et al., 2013; Kim and Mnih, 2018; Hjelm et al., 2018; Kumar et al., 2018; Lo catello et al., 2019), we propose to improve robust ness and generality of NLP models using disen tangled representations. Different from mapping xto a single representation zand then to y, the proposed strategy Ô¨Årst maps xto a set of distinct representations Z=fz1;;zKg, which are then individually projected to logits l1;;lK.ls are ensembled to make the Ô¨Ånal prediction of y. In this setup, we wish to make zs orls to be disentangled from each other as much as possible, which poten tially improves both robustness and generality: For the former, the decision of yis more immune to small changes in xsince even though small changes lead to signiÔ¨Åcant changes in some zs orls, others may remain invariant. The ultimate inÔ¨Çuence on ycan be further regulated when ls are combined. For the latter, different ls have the potential to dis entangle or partially disentangle sourcespeciÔ¨Åc, targetspeciÔ¨Åc and general features. Practically, we propose two ways to disentangle representations: adding an L2 regularizer or adding Total Correlation (TC) (Cover and Thomas, 2012; Ver Steeg and Galstyan, 2015; Steeg, 2017; Gao et al., 2018; Chen et al., 2018) under the frame work of variational information bottleneck (VIB). We show that models trained with the proposed criteria provide better robustness and domain adap tation ability in a wide range of NLP tasks, with tiny or nonsigniÔ¨Åcant sacriÔ¨Åce on taskspeciÔ¨Åc ac curacies. In summary, the contributions of this paper are: We present two methods to improve the ro bustness and generality of NLP models in theview of disentangled representation learning and the information bottleneck theory. Extensive experiments on domain adaptation and defense against adversarial attacks show that the proposed methods are able to provide better robustness compared with conventional taskspeciÔ¨Åc models, which indicates the ef fectiveness of the theory of information bottle neck and disentangled representation learning for NLP tasks. The rest of this paper is organized as follows: we present related work in Section 2. Models are de tailed in Section 3 and Section 4. We present exper imental results and analysis in Section 5, followed by a brief conclusion in Section 6. 2 Related Work "
351,Self-Supervised Noisy Label Learning for Source-Free Unsupervised Domain Adaptation.txt,"It is a strong prerequisite to access source data freely in many existing
unsupervised domain adaptation approaches. However, source data is agnostic in
many practical scenarios due to the constraints of expensive data transmission
and data privacy protection. Usually, the given source domain pre-trained model
is expected to optimize with only unlabeled target data, which is termed as
source-free unsupervised domain adaptation. In this paper, we solve this
problem from the perspective of noisy label learning, since the given
pre-trained model can pre-generate noisy label for unlabeled target data via
directly network inference. Under this problem modeling, incorporating
self-supervised learning, we propose a novel Self-Supervised Noisy Label
Learning method, which can effectively fine-tune the pre-trained model with
pre-generated label as well as selfgenerated label on the fly. Extensive
experiments had been conducted to validate its effectiveness. Our method can
easily achieve state-of-the-art results and surpass other methods by a very
large margin. Code will be released.","In practical applications, a deep model trained on source domain is usually deployed in edge devices to test unlabeled images from unknown target domain. The data distribu tion of target domain is rather different from source domain due to agnostic domain shift, such as diverse illumination, complex weather, etc. This is the main factor of model performance degradation in realworld scenarios. Recently, there are more and more researchers delving into unsuper *Equal contribution1Zhejiang University, Hangzhou, China 2Hikvision Research Institute, Hangzhou, China3Fuzhou Uni versity, Fuzhou, China. Correspondence to: Yueting Zhuang <yzhuang@zju.edu.cn >. Copyright 2021 by the author(s). Figure 1. Sourcefree UDA can be viewed as pseudo label de noising. Here (Xs;Ys)denote the source data and the annotated label.Xtis the unlabeled target data. The pregenerated pseudo labelYtis denoised into a cleaner one Y0 tafter sourcefree UDA. vised domain adaptation (UDA) to address this problem. Most of previous UDA methods aim to align the labeled source data and unlabeled target data in a common represen tation space, so that the classiÔ¨Åer trained on source domain can be well generalized to target domain (Long et al., 2015; Sun & Saenko, 2016; Haeusser et al., 2017). These vanilla UDA methods always assume that source data is accessible and thus can be used with target data for domain transferring. However, it is illsuited in some practical applications, e:g:, source data is inaccessible and only the model pretrained on source domain is available due to the expensive data transmission and data privacy protection. Such situation is termed as sourcefree UDA, where only unlabeled tar get data is provided for model optimization (Li et al., 2020; Liang et al., 2020; Li et al., 2021). Note that we only discuss image classiÔ¨Åcation task in this paper. How to solve sourcefree UDA? Although the pretrained model performs not so well on target domain, it still contains informative cues of the task. Naturally, it can be exploited to pregenerate pseudo labels for target data via networkarXiv:2102.11614v1  [cs.CV]  23 Feb 2021SelfSupervised Noisy Label Learning for SourceFree Unsupervised Domain Adaptation inference. Inevitably, the pregenerated labels are not ex actly correct, where the falselabeled ones can be viewed as noisy labels. In this way, as shown in Fig.1, source free UDA can also be regarded as another form of noisy label learning. From this viewpoint, we propose a simple yet effective approach named SelfSupervised Noisy La bel Learning (SSNLL) to address this problem, which is strongly inspired by unsupervised image classiÔ¨Åcation and noisy label learning. First of all, we walk from Unsupervised Image ClassiÔ¨Åcation (UIC) method (Chen et al., 2020b), which is an unsuper vised technique to train the classiÔ¨Åcation network by self generating pseudo label. Similar to other selfsupervised learning methods (He et al., 2020; Chen et al., 2020a; Caron et al., 2018), a critical step is to avoid model collapse of classifying all images into one category. Besides, the clas siÔ¨Åcation results achieved in a totally unsupervised way cannot be directly used in downstream tasks. Hence, we consider to incorporate noisy label learning to eliminate these problems. Since the pregenerated noisy label contains informative cues of the task, it can regularize UIC towards a taskspeciÔ¨Åc optimization direction. Considering what if we can split tar get data into a truelabeled part and a falselabeled part? The truelabeled part trained with pregenerated labels can regu larize the falselabeled part trained with selfgenerated la bels. Only when the selfgenerated label in falselabeled part matches to the pregenerated pseudo label in truelabeled part with consistent semantic information can it achieve optimal solution. To achieve this objective, inspired by the smallloss trick used in noisy label learning (Jiang et al., 2018; Shu et al., 2019; Han et al., 2018), we split the target data Xtinto a cleaner partXclwith smaller loss and a noisier part Xnowith greater loss with respect to the pregenerated label. To avoid the aforementioned model collapse problem, we further de velop it into a labelwise dataset splitting method, which ensures no empty classes in the cleaner part Xcl. After that, we sample images from XclandXnouniformly to train the network with pregenerated label and selfgenerated label, respectively. As the training goes, the loss with respect to the Ô¨Åxed pregenerated label will get smaller in truelabeled samples and get larger in falselabeled samples. To fully exploit this positive feedback, the dataset splitting opera tion and network training operation are alternated epoch by epoch so as to progressively boost the performance. Actually, UIC and noisy label learning in our approach are promoted by each other. The former one can help reÔ¨Åne the pregenerated noisy label, whilst the latter one can regularize selfgenerated label and prevent UIC from model collapsing and class mismatching. Besides, in order to initially reduce the noise ratio, we also introduce two label denoising tricksduring the process of pregenerating pseudo labels, includ ing Adaptive Batch Normalization (AdaBN) (Li et al., 2016) and Deep Transfer Clustering (DTC) (Kai et al., 2019). Our method can wellsolve the sourcefree UDA problem. Extensive experiments had been carried out on several pop ular UDA benchmarks, which show our method can easily achieve stateoftheart results on these benchmarks. It sur passes other methods even the source databased ones by a very large margin. For instance, on VisDAC(Peng et al., 2017), one of the most challenging datasets in UDA, our method can achieve 85.8% accuracy and surpass the second place more than 3% accuracy. 2. Related Work "
352,Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions.txt,"This paper presents a neural relation extraction method to deal with the
noisy training data generated by distant supervision. Previous studies mainly
focus on sentence-level de-noising by designing neural networks with intra-bag
attentions. In this paper, both intra-bag and inter-bag attentions are
considered in order to deal with the noise at sentence-level and bag-level
respectively. First, relation-aware bag representations are calculated by
weighting sentence embeddings using intra-bag attentions. Here, each possible
relation is utilized as the query for attention calculation instead of only
using the target relation in conventional methods. Furthermore, the
representation of a group of bags in the training set which share the same
relation label is calculated by weighting bag representations using a
similarity-based inter-bag attention module. Finally, a bag group is utilized
as a training sample when building our relation extractor. Experimental results
on the New York Times dataset demonstrate the effectiveness of our proposed
intra-bag and inter-bag attention modules. Our method also achieves better
relation extraction accuracy than state-of-the-art methods on this dataset.","Relation Extraction is a fundamental task in nat ural language processing (NLP), which aims to extract semantic relations between entities. For example, sentence ‚Äú[ Barack Obama ]e1was born in [Hawaii ]e2‚Äù expresses the relation BornIn be tween entity pair Barack Obama andHawaii . Conventional relation extraction methods, such as (Zelenko et al., 2002; Culotta and Sorensen, 2004; Mooney and Bunescu, 2006), adopted su pervised training and suffered from the lack of 1The code is available at https://github.com/ZhixiuYe/ IntraBagandInterBagAttentions .bag sentence correct? B1S1. Barack Obama was born in the United States .Yes S2.Barack Obama was the 44th pres ident of the United StatesNo B2S3. Kyle Busch , aLas Vegas res ident who ran second to Johnson last year, Ô¨Ånished third, followed by Kasey Kahne, Jeff Gordon and mark martin .No S4. Hendrick drivers Ô¨Ånished in three of the top four spots at Las Vegas , in cluding Kyle Busch in second and ...No Table 1: Examples of sentences with relation place ofbirth annotated by distant supervision, where ‚ÄúYes‚Äù and ‚ÄúNo‚Äù indicate whether or not each sentence actually expresses this relation. largescale manually labeled data. To address this issue, the distant supervision method (Mintz et al., 2009) was proposed, which generated the data for training relation extraction models automatically. The distant supervision assumption says that if two entities participate in a relation, allsentences that mention these two entities express that rela tion. It is inevitable that there exists noise in the data labeled by distant supervision. For example, the precision of aligning the relations in Freebase to the New York Times corpus was only about 70% (Riedel et al., 2010). Thus, the relation extraction method proposed in (Riedel et al., 2010) argued that the distant su pervision assumption was too strong and relaxed it toexpressedatleastonce assumption. This as sumption says that if two entities participate in a relation, at least one sentence that mentions these two entities might express that relation. An ex ample is shown by sentences S1 and S2 in Table 1. This relation extraction method Ô¨Årst divided the training data given by distant supervision into bags where each bag was a set of sentences con taining the same entity pair. Then, bag representa tions were derived by weighting sentences withinarXiv:1904.00143v1  [cs.CL]  30 Mar 2019each bag. It was expected that the weights of the sentences with incorrect labels were reduced and the bag representations were calculated mainly us ing the sentences with correct labels. Finally, bags were utilized as the samples for training relation extraction models instead of sentences. In recent years, many relation extraction meth ods using neural networks with attention mecha nism (Lin et al., 2016; Ji et al., 2017; Jat et al., 2018) have been proposed to alleviate the inÔ¨Çu ence of noisy training data under the expressedat leastonce assumption. However, these methods still have two deÔ¨Åciencies. First, only the target re lation of each bag is used to calculate the attention weights for deriving bag representations from sen tence embeddings at training stage. Here we argue that the bag representations should be calculated in a relationaware way. For example, the bag B1 in Table 1 contains two sentences S1 and S2. When this bag is classiÔ¨Åed to relation BornIn , the sen tence S1 should have higher weight than S2, but when classiÔ¨Åed to relation PresidentOf , the weight of S2 should be higher. Second, the expressed atleastonce assumption ignores the noisy bag problem which means that all sentences in one bag are incorrectly labeled. An example is shown by bag B2 in Table 1. In order to deal with these two deÔ¨Åciencies of previous methods, this paper proposes a neu ral network with multilevel attentions for dis tant supervision relation extraction. At the instance/sentencelevel, i.e., intrabag level, all possible relations are employed as queries to cal culate the relationaware bag representations in stead of only using the target relation of each bag. To address the noisy bag problem, a bag group is adopted as a training sample instead of a single bag. Here, a bag group is composed of bags in the training set which share the same relation label. The representation of a bag group is calculated by weighting bag representations using a similarity based interbag attention module. The contributions of this paper are threefold. First, an improved intrabag attention mechanism is proposed to derive relationaware bag represen tations for relation extraction. Second, an inter bag attention module is introduced to deal with the noisy bag problem which is ignored by the expressedatleastonce assumption. Third, our methods achieve better extraction accuracy than stateoftheart models on the widely used NewYork Times (NYT) dataset (Riedel et al., 2010). 2 Related Work "
353,Establishment of Neural Networks Robust to Label Noise.txt,"Label noise is a significant obstacle in deep learning model training. It can
have a considerable impact on the performance of image classification models,
particularly deep neural networks, which are especially susceptible because
they have a strong propensity to memorise noisy labels. In this paper, we have
examined the fundamental concept underlying related label noise approaches. A
transition matrix estimator has been created, and its effectiveness against the
actual transition matrix has been demonstrated. In addition, we examined the
label noise robustness of two convolutional neural network classifiers with
LeNet and AlexNet designs. The two FashionMINIST datasets have revealed the
robustness of both models. We are not efficiently able to demonstrate the
influence of the transition matrix noise correction on robustness enhancements
due to our inability to correctly tune the complex convolutional neural network
model due to time and computing resource constraints. There is a need for
additional effort to fine-tune the neural network model and explore the
precision of the estimated transition model in future research.","Deep learning has led to signiÔ¨Åcant breakthroughs in computer vision and image processing and has been applied to a variety of applications, including handwriting recognition [4], satellite image classiÔ¨Åcation [12], IoT crowdsourcing [24], and medical picture classiÔ¨Åcation for illness detection [27]. In the majority of applications, the existence of label noise in the training datasets is one of the primary obstacles to training these deep learning models. Label noise is the presence of labels for classiÔ¨Åcation data in which the labels do not precisely reÔ¨Çect the instance‚Äôs content. Because generating large labelled datasets can be resource heavy and costly, dataset labelling is frequently performed by nonprofessional labellers or automated systems with minimal or no expert supervision [8] [28]. Additionally, in some subject categories, constructing labelled datasets is intrinsically challenging due to the lack of assurance in the image content. For instance, collections of medical photographs frequently exhibit signiÔ¨Åcant observer variability and, consequently, substantial instancedependent label noise [10], as images pose a genuine diagnostic difÔ¨Åculty for specialists. In general, there are three types of label noise: classindependent (sometimes known as ‚Äùuniform‚Äù) noise, classdependent noise, and class and instancedependent noise [6]. This research examines classdependent label noise, in which labels are Ô¨Çipped randomly with a probability dependent on the class. This class noise rate will be referred to as the ‚ÄùÔ¨Çip rate‚Äù in the study. Label noise can dramatically impair classiÔ¨Åcation model performance [5]. Deep neural networks can be more susceptible to label noise because they have a larger propensity to memorise noisy labels, which can have a negative impact on their capacity to generalize [26] [1]. Due to the difÔ¨Åculty of 1arXiv:2211.15279v3  [cs.LG]  24 Apr 2023noisy labels, learning from datasets with noisy labels has been the focus of extensive research in recent years. In Section 2 of this research, we Ô¨Årst summarise and compare a variety of label noise techniques described in the literature. In section 3, we will describe the structure of the neural networks we will employ in our own experimental work, and in Section 4, we will provide our own experimental strategy for addressing label noise. Using backwards noise correction and a transition matrix, we analyse the label noise robustness of two distinct neural networks (LeNet and AlexNet) and compare their performance using this method. The experiment is conducted using three datasets; the Ô¨Årst two are grayscale photos from FashionMINIST, each with distinct Ô¨Çip rate and a transition matrix. The third dataset consists of images with unknown Ô¨Çip rates from CIFAR. We independently estimate the transition matrix for the second dataset. There are three label classes in all three datasets. In Section 5, we give a conclusion of the whole research. 2 Related work "
354,Transductive CLIP with Class-Conditional Contrastive Learning.txt,"Inspired by the remarkable zero-shot generalization capacity of
vision-language pre-trained model, we seek to leverage the supervision from
CLIP model to alleviate the burden of data labeling. However, such supervision
inevitably contains the label noise, which significantly degrades the
discriminative power of the classification model. In this work, we propose
Transductive CLIP, a novel framework for learning a classification network with
noisy labels from scratch. Firstly, a class-conditional contrastive learning
mechanism is proposed to mitigate the reliance on pseudo labels and boost the
tolerance to noisy labels. Secondly, ensemble labels is adopted as a pseudo
label updating strategy to stabilize the training of deep neural networks with
noisy labels. This framework can reduce the impact of noisy labels from CLIP
model effectively by combining both techniques. Experiments on multiple
benchmark datasets demonstrate the substantial improvements over other
state-of-the-art methods.","The revolutionized successes of deep neural networks in a variety of computer vision applications are conferred by large databases with accurate annotation [1, 2, 3]. In many real world scenarios, data labeling is very costly in terms of re source and time consumption. Several efforts had been made for unsupervised model optimization in the past [4, 5, 6, 7, 8]. Recently, Contrastive LanguageImage Pretraining (CLIP) [9] has emerged as a promising alternative for generalizing vision tasks. To alleviate the burden of data labeling, there is a strong motivation to leverage the unlabeled data supervised from the CLIP model in a transductive learning manner. How ever, it cannot achieve satisfactory performance by directly learning from the pseudo labels predicted by CLIP model since the classiÔ¨Åcation model is prone to Ô¨Åt and memorize the label noise [10], leading to the performance degeneration. * This work is done when Junchu Huang was an intern in Hikvision Research Institute.  Corresponding Authors 0.61 0.51 0.27 0.21 0.04 0.85 0.64 0.04 0.26 0.030.00 0.13 0.09 0.00 0.00 0.01 0.00 0.00 0.00 0.000.00 0.01 0.02 0.00 0.00 0.01 0.01 0.00 0.00 0.000.02 0.12 0.06 0.55 0.00 0.01 0.02 0.00 0.12 0.010.00 0.00 0.00 0.02 0.07 0.00 0.02 0.00 0.02 0.000.00 0.01 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.000.27 0.08 0.17 0.04 0.00 0.04 0.07 0.00 0.05 0.780.08 0.04 0.20 0.16 0.88 0.04 0.23 0.95 0.16 0.020.00 0.04 0.13 0.01 0.00 0.03 0.01 0.00 0.38 0.020.01 0.07 0.05 0.00 0.00 0.01 0.01 0.00 0.02 0.15 SeaLakeAnnualCrop Forest HerbaceousVegetation Highway Industrial Pasture PermanentCrop Residential River SeaLake 00.10.20.30.40.50.60.70.80.9 AnnualCropForest HerbaceousVegetationHighway Industrial Pasture PermanentCrop ResidentialRiverFig. 1 . The realistic noise matrix in the EuroSAT dataset [11] based on the CLIP model. The label noise is signiÔ¨Åcantly unbalanced among different categories, where the accuracy of 6th category (‚ÄúPasture‚Äù) is only 0.01 while the accuracy of 8th category (‚ÄúResidential‚Äù) is up to 0.95. To explore robust learning from noisy labels, a series of studies have been conducted, which can be roughly divided into three categories: 1) label correction methods, 2) loss correction methods, and 3) reÔ¨Åned training strategies. Label correction methods focus on rectifying noisy labels with the help of complex noise models, i.e., directed graphical mod els [12] and conditional random Ô¨Åelds [13]. However, in or der to obtain the noise model, the support from extra clean data is indispensable. The idea of loss correction methods [14, 15] is to modify the objective functions for training deep neural networks robustly. It holds the belief that assigning importance weights to examples increases the robustness of the training objective. The methods of reÔ¨Åned training strate gies [16, 17, 18] promote the robustness of deep neural net works via modifying the training paradigms. For instance, Coteaching [19] is proposed to maintain two peer networks simultaneously during training, in which one network is back propagated by the selected conÔ¨Ådent samples from another network to alleviate the accumulated error. To make noisy label learning more trackable, it is re quired to leverage the intrinsic information of the unlabeled data. For example, the consistency regularization from the semisupervised learning holds the assumption that the pre diction of an instance should not be too different from its perturbation one [21]. Under this inspiration, recent works explore to handle the label noise by integrating the wisdom of semisupervised learning technology. In order to achievearXiv:2206.06177v1  [cs.CV]  13 Jun 2022Feature extractor  ClassifierLinear LinearBatchNormReLU Output elephant dolphin ... watch snoopy A photo of  a {object} ... ... ...... CLIP Model Label Text ...+ ++ Lkl LccyÀúi,0  yÀúi,t yÀúi,t‚àí1yÀúi,t+1  Classification NetworkFig. 2 . The pipeline of the proposed method. Here CLIP model is merely required to obtain the label text of each category to generate text embedding so as to annotate the unlabeled images. Modules in blue is trained from scratch with the initial supervision from CLIP model and the training labels are updated in an iterative learning strategy to Ô¨Ålter out the label noise. SpeciÔ¨Åcally, top right is the ensemble labels module while bottom is the noisy label learning process regularized by class conditional contrastive learning loss. In this paper, we use VITB/32 [20] as the backbone of CLIP model for zeroshot pseudo labeling and use ResNet50 as the backbone of the classiÔ¨Åcation network for transductive learning. this goal, DivideMix [22] designs two diverged networks: one network uses the dataset division from another one alter nately to separate the clean data (considered as labeled data) and the noisy data (considered as unlabeled data). Then the semisupervised training are conducted with the improved MixMatch [23] to perform label coreÔ¨Ånement and label coguessing on the labeled and unlabeled data, respectively. Despite the remarkable empirical results achieved by the above mentioned methods, they hold the same assumption: the noisy label is simulated and balanced with known noise rate of each category. As shown in Figure 1, the noisy labels generated from CLIP model violate the above assumption. As a result, an obvious obstacle in the existing methods is the conÔ¨Årmation bias [24]: the performance is restricted when learning from severely inaccurate pseudo labels. To escape from the dilemma, we propose in this paper a novel design of Transductive CLIP. SpeciÔ¨Åcally, this paper has the following contributions: 1) This paper proposes to leverage the unla beled data that supervised from CLIP model in a transductive learning manner to alleviate the burden of data labeling. 2) To tackle conÔ¨Årmation bias problem, a classconditional con trastive learning (C3L) mechanism is proposed to mitigate the reliance on pseudo labels and boost the tolerance to noisy labels. 3) To stabilize the training of deep neural networks, anensemble labels scheme is utilized to update the incorrect pseudo labels in an iterative learning strategy. 2. METHODOLOGY "
355,Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data.txt,"Manually labeled corpora are expensive to create and often not available for
low-resource languages or domains. Automatic labeling approaches are an
alternative way to obtain labeled data in a quicker and cheaper way. However,
these labels often contain more errors which can deteriorate a classifier's
performance when trained on this data. We propose a noise layer that is added
to a neural network architecture. This allows modeling the noise and train on a
combination of clean and noisy data. We show that in a low-resource NER task we
can improve performance by up to 35% by using additional, noisy data and
handling the noise.","For training statistical models in a supervised way, labeled datasets are required. For many natural language processing tasks like partofspeech tag ging (POS) or named entity recognition (NER), every word in a corpus needs to be annotated. While the large effort of manual annotation is reg ularly done for English, for other languages this is often not the case. And even for English, the corpora are usually limited to certain domains like newspaper articles. For tasks in lowresource ar eas there tend to be no or only few labeled words available. Distant supervision and automatic labeling ap proaches are an alternative to manually creating labels. These exploit the fact that frequently large amounts of unannotated texts do exist in the tar geted domain, e.g. from web crawls. The la bels are then assigned using techniques like trans ferring information from highresource languages (Das and Petrov, 2011) or simple lookups inknowledge bases or gazetteers (Dembowski et al., 2017). Once such an automatic labeling system is set up, the amount of text to annotate becomes nearly irrelevant, especially in comparison to man ual annotation. Also, it is often rather easy to ap ply the system to different settings, e.g. by using a knowledge base in a different language. However, while easily obtainable in large amounts, the automatically annotated data usu ally contains more errors than the manually an notated. When training a machine learning algo rithm on such noisy training data, this can result in a low performance. Furthermore, the combination of noisy and clean training instances can perform even worse than just using clean data. In this work, we present an approach to training a neural network with a combination of a small amount of clean data and a larger set of automat ically annotated, noisy instances. We model the noise explicitly using a noise layer that is added to the network architecture. This allows us to di rectly optimize the network weights using stan dard techniques. After training, the noise layer is not needed anymore, removing any added com plexity. This technique is applicable to different classi Ô¨Åcation scenarios and in this work, we apply it to an NER task. To obtain a nonsynthetic, realistic source of noise, we use lookups from gazetteers for automatically annotating the data. In the low resource setting, we show the performance boost obtained from training with both clean and noisy instances and from handling the noise in the data. We also compare to another recent neural network noisehandling approach and we give some more insight into the impact of using additional noisy data and into the learned noise model.arXiv:1807.00745v2  [cs.LG]  22 Jul 20182 Related Work "
356,Network-based Biased Tree Ensembles (NetBiTE) for Drug Sensitivity Prediction and Drug Sensitivity Biomarker Identification in Cancer.txt,"We present the Network-based Biased Tree Ensembles (NetBiTE) method for drug
sensitivity prediction and drug sensitivity biomarker identification in cancer
using a combination of prior knowledge and gene expression data. Our devised
method consists of a biased tree ensemble that is built according to a
probabilistic bias weight distribution. The bias weight distribution is
obtained from the assignment of high weights to the drug targets and
propagating the assigned weights over a protein-protein interaction network
such as STRING. The propagation of weights, defines neighborhoods of influence
around the drug targets and as such simulates the spread of perturbations
within the cell, following drug administration. Using a synthetic dataset, we
showcase how application of biased tree ensembles (BiTE) results in significant
accuracy gains at a much lower computational cost compared to the unbiased
random forests (RF) algorithm. We then apply NetBiTE to the Genomics of Drug
Sensitivity in Cancer (GDSC) dataset and demonstrate that NetBiTE outperforms
RF in predicting IC50 drug sensitivity, only for drugs that target membrane
receptor pathways (MRPs): RTK, EGFR and IGFR signaling pathways. We propose
based on the NetBiTE results, that for drugs that inhibit MRPs, the expression
of target genes prior to drug administration is a biomarker for IC50 drug
sensitivity following drug administration. We further verify and reinforce this
proposition through control studies on, PI3K/MTOR signaling pathway inhibitors,
a drug category that does not target MRPs, and through assignment of dummy
targets to MRP inhibiting drugs and investigating the variation in NetBiTE
accuracy.","There is strong evidence that the tumor‚Äôs genetic makeup can influence the outcome of anticancer drug treatments (1, 2), resulting in heterogeneity in patient clinical response to therapeutic drugs (3). This varied clinical response has led to the promise of personalized (or precision) medicine in cancer, where molecular biomarkers, e.g. gene expression, obtained from a patient‚Äôs tumor profiling may be used to design a personalized course of treatment. Targeted treatments have been shown to improve survival rates, for instance, in treating chronic myeloid leukemia (BCR‚ÄìABL) and malignant melanoma (BRAF) (4,5). Despite these success stories, variability in drug response still remains an open challenge and the link between genetic and epigenetic alterations and drug response is not appropriately characterized for a large number of cancer drugs (6,7). As large datasets emerge containing genetic profiles of tumors and their associated drug sensitivity, there is a need for computational methods that can effectively harness the available data and link genetic profiles with drug sensitivity through identification of important biomarkers (8‚Äì13). The Sanger Institute‚Äôs Genomics of Drug Sensitivity in Cancer (GDSC) database is a vast resource of over 200 cancer compounds screened with over a thousand genetically profiled pancancer cell lines (6). The dataset has been of particular interest for drug sensitivity prediction and biomarker identification efforts (3,8,14‚Äì18). These include a number of works employing quantitative, statistical and machine learning methods such as : Cell linesimilarity and drugsimilarity based models (19) multilevel mixed effect models using all drugcell line combinations (20), quantitative structureactivity relationship (QSAR) analysis using kernelized Bayesian matrix factorization (21), lasso and elastic net models for drug sensitivity prediction and target identification (8,22,23), as well as logic models for predictor identification (24).3In this work, we introduce a novel machine learning method that enables us to predict IC50 values and identify informative predictors for drug sensitivity using the GDSC dataset. Our approach is based on constructing a biased tree ensemble, where bias is elaborately designed to recapitulate the prior knowledge of drug targets and their highconfidence biomolecular interactions extracted from the STRING database of molecular interactions (25). Tree ensemble methods (26,27) such as the popular random forests (RF) (28) algorithm consist of an aggregation of decision trees and are suitable for dealing with highdimensionality (29) (i.e. small number of samples and large number of features) that is often encountered in biomolecular datasets. In addition, unlike regularized linear methods such as lasso and elastic net (30), regression trees can capture nonlinear relationships. Furthermore, tree ensemble methods are robust and have few tuning parameters (number of trees, mtry , and tree depth) and as such are easy to train. Due to these favorable attributes, tree ensembles, and in particular the random forests algorithm, have been used extensively for the analysis of biomolecular data (31‚Äì36). In this paper, we first introduce the Biased Tree Ensembles (BiTE) approach, where the classification and regression trees (CART) (37) are constructed according to prior knowledge. Unlike random tree ensembles (i.e., RF) in which all features have an equal probability of being selected as split variables in a tree (figure 1A), in BiTE, features that are more important or informative according to the available prior knowledge are given a higher probability (figure 1B). We demonstrate that BiTE is a more transparent and interpretable algorithm compared to RF, as it is immediately clear which set of features contributed the most to the model performance. For instance, if a set of features results in BiTE‚Äôs loss of accuracy, it can be deduced that the features were uninformative predictors; conversely, an improved accuracy can be attributed to the set of features towards which we biased the model. In this manner, BiTE may be used to examine the predictive power of various features in a transparent and controllable manner.4Building upon BiTE, we propose the Networkbased Biased Tree Ensembles (NetBiTE) algorithm, where two layers of prior knowledge ‚Äì instead of one in BiTE ‚Äì are fed into the model. First, drug target proteins are determined from drug databases and the literature and are assigned an initial bias weight. Second, the initial bias weights are propagated over STRING, a network of proteinproteininteraction (PPI) comprising the entire gene set. A number of networkbased methods have been previously put forward that take advantage of PPI networks in combination with biomolecular profiles of cells, in order to identify subnetworks that represent a pathway or a functional complex (38,39). Network propagation (or diffusion) over PPI networks has been previously used to identify pathways, subnetworks or associations that represent a disease, a tumor type or a patient (40‚Äì42). Network propagation in essence defines a ‚Äúneighborhood of influence‚Äù surrounding an entity of interest, for instance a mutated gene (42). NetBiTE utilizes network diffusion over STRING PPI network in order to establish a neighborhood of influence surrounding the drug target proteins and construct tree ensembles that are biased towards this neighborhood. Even though several modified random forests or tree ensembles algorithms have been previously proposed (43‚Äì46), to the best of our knowledge, NetBiTE is the first algorithm in which multiple layers of prior knowledge are quantitatively and systematically combined and utilized in constructing biased tree ensembles. In the following sections, we demonstrate that BiTE and NetBiTE outperform RF in predicting IC50 drug sensitivity using both a synthetic dataset and the GDSC dataset. In addition, we showcase how NetBiTE in conjunction with the GDSC dataset and the STRING PPI network can identify important biomarkers for drug sensitivity. The organization of this paper is as follows. In section 2.1, we compare BiTE versus RF using a synthetic dataset. We showcase that BiTE can achieve a superior performance and stability at a significantly lower computational cost. In section 2.2, we apply NetBiTE to the GDSC data for a panel of 50 cancer drugs5and compare the predictive performance with that of RF. We demonstrate that NetBiTE achieves significant accuracy gains over RF for drugs than inhibit membrane receptor pathways (MRPs), suggesting that the expression of their reported target genes is an informative biomarker for drug sensitivity. We further investigate this hypothesis by studying all drugs within the GDSC database that target MRPs and by performing two control experiments. In section 3, we discuss the possible reasons behind our observations in the context of prior findings related to the role of oncogenes in cancer development as well as drug sensitivity and resistance. Figure 1. Working principles of biased tree ensembles (BiTE) compared against random forests (RF). A, B) diagrams describe the standard random forests (RF) algorithm and our devised biased tree ensembles (BiTE) approach. At each node of the tree, both algorithms draw a subset of features (di) from which they select a split feature through optimization of a loss function. In RF, the feature subset is selected at random while BiTE biases the selection towards more informative features according to prior knowledge. C, D) Tuning parameters for RF and BiTE algorithms. There are three tuning parameters for RF: the number of trees, ntree, the target partition size ( TPS) ‚Äì the minimum number of samples in the leaf nodes of the tree, and the number of features to consider when looking for the best split, mtry . In BiTE, there is an additional tuning parameter, the bias weight distribution that controls the probability of each 6feature being included in the feature subset ( di) at each split of the tree. If weights are assigned according to informative prior knowledge, BiTE results in significant performance gains at a lower computational cost. 2. Methods and Materials "
357,Fully automatic scoring of handwritten descriptive answers in Japanese language tests.txt,"This paper presents an experiment of automatically scoring handwritten
descriptive answers in the trial tests for the new Japanese university entrance
examination, which were made for about 120,000 examinees in 2017 and 2018.
There are about 400,000 answers with more than 20 million characters. Although
all answers have been scored by human examiners, handwritten characters are not
labelled. We present our attempt to adapt deep neural network-based handwriting
recognizers trained on a labelled handwriting dataset into this unlabeled
answer set. Our proposed method combines different training strategies,
ensembles multiple recognizers, and uses a language model built from a large
general corpus to avoid overfitting into specific data. In our experiment, the
proposed method records character accuracy of over 97% using about 2,000
verified labelled answers that account for less than 0.5% of the dataset. Then,
the recognized answers are fed into a pre-trained automatic scoring system
based on the BERT model without correcting misrecognized characters and
providing rubric annotations. The automatic scoring system achieves from 0.84
to 0.98 of Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents
acceptable similarity of scoring between the automatic scoring system and the
human examiners. These results are promising for further research on end-to-end
automatic scoring of descriptive answers.","Descriptive answers are better to evaluate learner‚Äôs understanding and problem solving ability.  They encourage learners to think rather than select. However, scoring them requires large work  and time. In recent years, it was proposed to add descriptive questions in the new university  entrance common examinations in Japan as well as the current multiplechoice questions [1], but  given up due to the short period of scoring handwritten answers and the anxiety about reliable  scoring.   One approach is to score handwritten descriptive answers automatically and feedback scores to  examinees and examiners to correct scoring errors. Another approach is to apply automatic scoring  or clustering them for human examiners to score them efficiently and reliably [2], [3]. For both of  them, handwritten answers need to be recognized and scored.   A few datasets storing handwritten answers have been published and used in research on  handwriting recognition, such as SCUTEPT (Chinese handwritten answers) [4] and DsetMix,  which is artificially prepared by synthesized handwritten math answers [5]. Note that these  datasets are all fully labelled, ideally suited to train handwriting recognizers based on deep neural  networks.  The National Center for University Entrance Examinations (NCUEE) conducted trial tests for the  new university entrance common exams with 64,518 and 67,745 examinees in 2017 and 2018,  respectively. Three descriptive questions were included in the Japanese language test for each trial  test. Their handwritten answers were scored by human examiners. The scanned images and scores  are used here.  However, the offline images are only raw images and have not been segmented or labelled. It is  infeasible to label a whole set of scanned handwritten answers. On the other hand, automatic  pattern recognition methods, especially wellknown deep neural networks require largescale  labelled data for training. In this paper, we present normalization, segmentation, and handwriting  recognition from this handwritten answer set and their automatic scoring. In particular, we focus  mainly on training the handwriting recognizer to adapt to the actual data from the examinee‚Äôs  answers. In addition, we also incorporated the language model to rerank the predicted candidates  so that the ambiguous patterns are corrected by linguistic context. In summary, the main  contributions are as follows:  ‚Ä¢ We present an ensemble deep neural networkbased recognizer for offline Japanese  handwritten answer recognition.  ‚Ä¢ We propose a training procedure with multiple steps (pretraining, finetuning and  ensemble learning) to adapt the ensembled handwriting recognizer to real patterns.  ‚Ä¢ We evaluate the handwriting recognizer in combination with the latest automatic scoring  system.  ‚Ä¢ The combined architecture without correction of misrecognized characters and rubric  annotations scores handwritten answers as almost the same as human examiners.  2 Related works  "
358,Panoptic Lifting for 3D Scene Understanding with Neural Fields.txt,"We propose Panoptic Lifting, a novel approach for learning panoptic 3D
volumetric representations from images of in-the-wild scenes. Once trained, our
model can render color images together with 3D-consistent panoptic segmentation
from novel viewpoints.
  Unlike existing approaches which use 3D input directly or indirectly, our
method requires only machine-generated 2D panoptic segmentation masks inferred
from a pre-trained network. Our core contribution is a panoptic lifting scheme
based on a neural field representation that generates a unified and multi-view
consistent, 3D panoptic representation of the scene. To account for
inconsistencies of 2D instance identifiers across views, we solve a linear
assignment with a cost based on the model's current predictions and the
machine-generated segmentation masks, thus enabling us to lift 2D instances to
3D in a consistent way. We further propose and ablate contributions that make
our method more robust to noisy, machine-generated labels, including test-time
augmentations for confidence estimates, segment consistency loss, bounded
segmentation fields, and gradient stopping.
  Experimental results validate our approach on the challenging Hypersim,
Replica, and ScanNet datasets, improving by 8.4, 13.8, and 10.6% in scene-level
PQ over state of the art.","Robust panoptic 3D scene understanding models are key to enabling applications such as VR, robot navigation, or selfdriving, and more. Panoptic image understanding ‚Äì the task of segmenting a 2D image into categorical ‚Äústuff‚Äù areas and individual ‚Äúthing‚Äù instances ‚Äì has experienced tremen dous progress over the past years. These advances can be at tributed to continuously improved model architectures and the availability of largescale labeled 2D datasets, leading to stateoftheart 2D panoptic segmentation models [6,21,45] that generalize well to unseen images captured in the wild. Singleimage panoptic segmentation, unfortunately, is still insufÔ¨Åcient for tasks requiring coherency and consis tency across multiple views. In fact, panoptic masks often contain viewspeciÔ¨Åc imperfections and inconsistent clas siÔ¨Åcations, and singleimage 2D models naturally lack the ability to track unique object identities across views (see Fig. 2). Ideally, such consistency would stem from a full, 3D understanding of the environment, but lifting machine generated 2D panoptic segmentations into a coherent 3D panoptic scene representation remains a challenging task. Project page: nihalsid.github.io/panopticlifting/ 1arXiv:2212.09802v1  [cs.CV]  19 Dec 2022Recent works [11,19,42,47] have addressed panoptic 3D scene understanding from 2D images by leveraging Neu ral Radiance Fields (NeRFs) [24], gathering semantic scene data from multiple sources. Some works [11, 42] rely on ground truth 2D and 3D labels, which are expensive and timeconsuming to acquire. The work of Kundu et al. [19] instead exploits machinegenerated 3D bounding box detec tion and tracking together with 2D semantic segmentation, both computed using offtheshelf models. However, this approach is limited by the fact that 3D detection models, when compared to 2D panoptic segmentation ones, strug gle to generalize beyond the data they were trained on. This is in large part due to the large difference in scale between 2D and 3D training datasets. Furthermore, dependence on multiple pretrained models increases complexity and intro duces potentially compounding sources of error. In this work we introduce Panoptic Lifting, a novel for mulation which represents a static 3D scene as a panoptic radiance Ô¨Åeld (see Sec. 3.2). Panoptic Lifting supports ap plications like novel panoptic view synthesis and scene edit ing, while maintaining robustness to a variety of diverse in put data. Our model is trained from only 2D posed images and corresponding, machinegenerated panoptic segmenta tion masks, and can render color, depth, semantics, and 3D consistent instance information for novel views of the scene. Starting from a TensoRF [4] architecture that encodes density and viewdependent color information, we intro duce lightweight output heads for learning semantic and instance Ô¨Åelds. The semantic Ô¨Åeld, represented as a small MLP, is directly supervised with the machinegenerated 2D labels. An additional segment consistency loss guides this supervision to avoid optima that fragment objects in the presence of label inconsistencies. The 3D instance Ô¨Åeld is modelled by a separate MLP, holding a Ô¨Åxed number of classagnostic, 3Dconsistent surrogate object identiÔ¨Åers. Losses for both the Ô¨Åelds are weighted by conÔ¨Ådence esti mates obtained by testtime augmentation on the 2D panop tic segmentation model. Finally, we discuss speciÔ¨Åc tech niques, e.g. bounded segmentation Ô¨Åelds and stopping semanticstogeometry gradients (see Sec. 3.3), to further limit inconsistent segmentations. In summary, our contributions are: ‚Ä¢ A novel approach to panoptic radiance Ô¨Åeld represen tation that models the radiance, semantic class and in stance id for each point in the space for a scene by directly lifting machinegenerated 2D panoptic labels. ‚Ä¢ A robust formulation to handle inherent noise and in consistencies in machinegenerated labels, resulting in a clean, coherent and viewconsistent panoptic seg mentations from novel views, across diverse data. Figure 2. Predictions from stateoftheart 2D panoptic segmen tation methods such as Mask2Former [6] are typically noisy and inconsistent when compared across views of the same scene. Typ ical failure modes include conÔ¨Çicting labels (e.g. sofa predicted as a bed above) and segmentations (e.g. labeled void above). Fur thermore, instance identities are not preserved across frames (rep resented as different colors). 2. Related Work "
359,Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators.txt,"Multi-label learning is an emerging extension of the multi-class
classification where an image contains multiple labels. Not only acquiring a
clean and fully labeled dataset in multi-label learning is extremely expensive,
but also many of the actual labels are corrupted or missing due to the
automated or non-expert annotation techniques. Noisy label data decrease the
prediction performance drastically. In this paper, we propose a novel Gold
Asymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that
operates robust against noisy labels. GALC-SLR estimates the noise confusion
matrix using single-label samples, then constructs an asymmetric loss
correction via estimated confusion matrix to avoid overfitting to the noisy
labels. Empirical results show that our method outperforms the state-of-the-art
original asymmetric loss multi-label classifier under all corruption levels,
showing mean average precision improvement up to 28.67% on a real world dataset
of MS-COCO, yielding a better generalization of the unseen data and increased
prediction performance.","Realworld images naturally contain multiple objects corresponding to diverse labels, which elevates deep learning models for multilabel learning. Multilabel classiÔ¨Åcation is an extension of multiclass classiÔ¨Åcation where the input is not assigned only a single label, but multiple ones. It is extremely time consuming and expensive to collect high quality labels for singlelabel images. Even longstanding and highly curated datasets, e.g. CIFAR [Krizhevsky et al., 2009], contain wrong labels [Chen et al., 2019a]. Multilabel learning, where each image has multiple possible label combinations, exacerbates this problem [Zhao and Gomes, 2021]. For instance, [Veit et al., 2017] shows that the Open Images dataset [Krasin et al., 2016], which is widely used for multilabel and multiclass image classiÔ¨Åcation, contains 26.6% false positives among the training label set. In singlelabel classiÔ¨Åcation, label noise has been widely studied [Algan and Ulusoy, 2021]. Due to the memorization effect Deep Neural Networks (DNNs) can overÔ¨Åt to noise degrading signiÔ¨Åcantly their performance [Zhang et al., 2017]. Several techniques have been proposed to counter the effect of wrong labels [Hendrycks et al., 2018, Han et al., 2018a, Patrini et al., 2017, Yao et al., 2019]. However, multilabel classiÔ¨Åcation is a more complex problem. As depicted in Fig. 1(a), each image comes with multiple labels including some wrong and some clean ones. This has a negative impact on the performance of DNNs. According to [Song et al., 2020], existing noise resilient methods for singlelabel are not able to learn the correlation among multiple labels. Even so little attention has been given to evaluating multilabel classiÔ¨Åers with noisy labels [Zhao and Gomes, 2021].arXiv:2108.02032v1  [cs.CV]  4 Aug 2021MultiLabel Gold Asymmetric Loss Correction with SingleLabel Regulators ImageReferenceLabelsSettreebicyclecanalperson treecarcanalcat MultiLabel with Wrong Labels  (a) Example of image with wrong labels 0.00.10.20.30.40.50.6 Noise fraction3040506070mAP74.91 69.55 64.71 58.89 52.18 45.24 36.81 (b) Impact of increasing wrong label ratios Figure 1: Wrong labels and their impact in multilabel classiÔ¨Åcation. To Ô¨Åll the gap in noiseresilient multilabel classiÔ¨Åers, we propose Gold Asymmetric Loss Correction with SingleLabel Regulators ( GALCSLR ).GALCSLR assumes that a small subset of the training samples can be trusted. We use this additional information to accurately estimate the noise corruption matrix. Due to class imbalance and label correlations, learning the noise in realworld multilabel datasets is more complicated than in realworld singlelabel datasets. Hence, we introduce a novel method that uses single label regulators to rebalance the predictions towards a targeted label. This leads to accurate noise estimations used to correct the wrong labels during training making the model robust to label noise even in the more challenging multilabel setting. In comparison to the stateoftheart Asymmetric Loss (ASL) multilabel classiÔ¨Åer [Baruch et al., 2020] GALCSLR is signiÔ¨Åcantly more accurate under label noise. ASL balances the probabilities of different samples by treating positive and negative samples differently, i.e. asymmetrically. In empirical evaluation on the MSCOCO dataset [Lin et al., 2014] GALCSLR outperforms ASL under all tested noise ratios from 0% to 60%. GALCSLR improves the mean Average Precision (mAP) over ASL on average by 13.81% and up to 28.67%. The contributions of this paper are summarized as follows: ‚Ä¢We design a noise estimation technique that uses trusted multilabel and singlelabel data in order to calculate the corruption matrix. ‚Ä¢Using our noise estimation, we design a robust multilabel classiÔ¨Åer, GALCSLR , based on a loss correction approach. ‚Ä¢We compare GALCSLR against a stateoftheart classiÔ¨Åer under noisy labels and study the behaviour of GALCSLR in target ablation study experiments. 1.1 Motivation example Our motivation stems from the detrimental effects label noise in training data can have on the model performance. We demonstrate this using the stateoftheart ASL [Baruch et al., 2020] method to train a TResNetM [Ridnik et al., 2021a] network on the MSCOCO dataset [Lin et al., 2014]. ASL applied on TResNet ranks top on the leader board for MLC on MSCOCO1. We inject symmetric label noise (details in Section 4.1) at various corruption levels, from 0% to 60%, and report the mean average precision to asses the impact of wrong labels. mAP is considered by many recent works [Lanchantin et al., 2020, Chen et al., 2019b] as important metric for performance evaluation in multilabel classiÔ¨Åcation, since it takes into consideration both falsenegative and falsepositive rates [Baruch et al., 2020]. Fig. 1(b) shows the results: each additional 10% noisy labels leads to a 5%8% reduction in mAP score. Since it is hard and costly to avoid label noise [Zhao and Gomes, 2021], it is vital to develop robust classiÔ¨Åers that can avoid overÔ¨Åtting to the label noise in the training data. 2 Related Work "
360,Deep Miner: A Deep and Multi-branch Network which Mines Rich and Diverse Features for Person Re-identification.txt,"Most recent person re-identification approaches are based on the use of deep
convolutional neural networks (CNNs). These networks, although effective in
multiple tasks such as classification or object detection, tend to focus on the
most discriminative part of an object rather than retrieving all its relevant
features. This behavior penalizes the performance of a CNN for the
re-identification task, since it should identify diverse and fine grained
features. It is then essential to make the network learn a wide variety of
finer characteristics in order to make the re-identification process of people
effective and robust to finer changes. In this article, we introduce Deep
Miner, a method that allows CNNs to ""mine"" richer and more diverse features
about people for their re-identification. Deep Miner is specifically composed
of three types of branches: a Global branch (G-branch), a Local branch
(L-branch) and an Input-Erased branch (IE-branch). G-branch corresponds to the
initial backbone which predicts global characteristics, while L-branch
retrieves part level resolution features. The IE-branch for its part, receives
partially suppressed feature maps as input thereby allowing the network to
""mine"" new features (those ignored by G-branch) as output. For this special
purpose, a dedicated suppression procedure for identifying and removing
features within a given CNN is introduced. This suppression procedure has the
major benefit of being simple, while it produces a model that significantly
outperforms state-of-the-art (SOTA) re-identification methods. Specifically, we
conduct experiments on four standard person re-identification benchmarks and
witness an absolute performance gain up to 6.5% mAP compared to SOTA.","In recent years, person reidentiÔ¨Åcation (ReID) has at tracted major interest due to its important role in various computer vision applications: video surveillance, human authentication, humanmachine interaction etc. The main ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥ ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥ Mined  Features ùëìùëìùëíùëí2 Global  Features ùëìùëìùëîùëî Erasing Operatorùëåùëå1ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥ ùëåùëå2ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥ùëåùëå2ùëåùëå3 ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥ùëåùëå3ùëåùëå4ùëåùëå3ùëíùëí2 ùëåùëå4ùëíùëí2 ùëåùëå2ùëíùëí1ùëåùëå3ùëíùëí1ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥(ùëåùëå3ùëíùëí1)ùëåùëå4ùëíùëí1ùëåùëå4ùëôùëôLocal  Features ùëìùëìùëôùëô Mined  Features ùëìùëìùëíùëí1 ùê¥ùê¥ùê¥ùê¥ùê¥ùê¥AttentionModule Figure 1: Deep Miner Model Architecture. Given a stan dard CNN backbone, several branches are created to en rich and diversify the features for the purpose of person re identiÔ¨Åcation. The proposed Deep Miner model is made of three types of branches: (i)The main branch G (in or ange ) is the original backbone and predicts the standard global features fg;(ii)Several InputErased (IE) branches (ingreen ) that takes as input erased feature maps and pre dictmined features fe1andfe2;(iii)The local branch (in blue) that outputs local features fland in which a uniform partition strategy is employed for part level feature reso lution as proposed by [38]. In the global branch, and the bottom IE Branch, attention modules are used in order to improve their feature representation. objective of person ReID is to determine whether a given person has already appeared over a network of cameras, which technically implies a robust modelling of the global appearance of individuals. The ReID task is particularly challenging because of signiÔ¨Åcant appearance changes ‚Äì of ten caused by variations in the background, the lightening conditions, the body pose and the subject orientation w.r.t. the recording camera. In order to overcome these issues, one of the main goals of person ReID models is to produce rich representations of any input image for person matching. Notably, CNNs are known to be robust to appearance changes and spatial loca tion variations, as their global features are invariant to such 1arXiv:2102.09321v1  [cs.CV]  18 Feb 2021Input  Global/tildelow IE 1  IE 2  Local Figure 2: Feature visualization for three examples. Warmer color denotes higher value. The global branch (second column) focuses only on some features but ignores other important ones. Thanks to the Input Erased branches (third and fourth columns), Deep Miner discovers new im portant features (localized by the black boxes). For in stance, in the Ô¨Årst row, the IEbranches are more attentive to the person pant. In the second row, they discover some patterns on the coat and get attentive to its cap. In the third row, they Ô¨Ånd out the plastic bag. The local branch (last column) helps the network to focus on local features such as the shoes of the subject or the object handled by the sub ject in the second row. transformations. Nevertheless, the aforementioned global features are prone to ignore detailed and potentially relevant information for identifying speciÔ¨Åc person representations. To enforce the learning of detailed features, attention mech anisms and aggregating global partbased representations were introduced in the literature, yielding very promising results [5, 7, 31]. SpeciÔ¨Åcally, attention mechanisms allow to reduce the inÔ¨Çuence of background noise and to focus on relevant features, while partbased models divide feature maps into spatial horizontal parts thereby allowing the net work to focus on Ô¨Ånegrained and local features. Despite their observed effectiveness in various tasks, these approaches do not provide ways to enrich and diver sify an individual‚Äôs representation. In fact, deep learning models are shown to exhibit a biased learning behavior [4, 3, 32]; in the sense that they retrieve sufÔ¨Åciently partial attributes concepts which contribute to reduce the training loss over the seen classes, rather than learning allsideddetails and concepts. Basically, deep networks tend to focus on surface statistical regularities rather than more general abstract concepts. This behavior is problematic in the context of reidentiÔ¨Åcation, since the network is required to provide the richest and most diverse possible representations. In this paper, we propose to address this problem by adding Input Erased Branches (IEBranch) into a standard backbone. Precisely, an IEBranch takes partially removed feature maps as input in the aim of producing (that is ‚Äúmining‚Äù) more diversiÔ¨Åed features as output (as depicted in Figure 2). In particular, the removed regions correspond quite intuitively to areas where the network has strong activations and are determined by a simple suppression operation (see subsection 3.2). The proposed Deep Miner model is therefore made as the combination of IEbranches with local and global branches. The multibranch architec ture of Deep Miner is depicted on Figure 1. The main contributions brought by this work may be summarized in the following items: (i)We provide a multi branch model allowing the mining of rich and diverse fea tures for people reidentiÔ¨Åcation. The proposed model in cludes three types of branches: a Global branch (Gbranch), a Local branch (Lbranch) and an InputErased Branch (IE Branch); the latter being responsible of mining extra fea tures; (ii)IEBranches are constructed by adding an erase operation on the global branch feature maps, thereby allow ing the network to discover new relevant features; (iii)Ex tensive experiments were conducted on Market1501 [44], DukeMTMCReID [24], CUHK03 [17] and MSMT17[37]. We demonstrate that our model signiÔ¨Åcantly outperforms the existing SOTA methods on all benchmarks. 2. Related Work "
361,3D Point Cloud Denoising via Deep Neural Network based Local Surface Estimation.txt,"We present a neural-network-based architecture for 3D point cloud denoising
called neural projection denoising (NPD). In our previous work, we proposed a
two-stage denoising algorithm, which first estimates reference planes and
follows by projecting noisy points to estimated reference planes. Since the
estimated reference planes are inevitably noisy, multi-projection is applied to
stabilize the denoising performance. NPD algorithm uses a neural network to
estimate reference planes for points in noisy point clouds. With more accurate
estimations of reference planes, we are able to achieve better denoising
performances with only one-time projection. To the best of our knowledge, NPD
is the first work to denoise 3D point clouds with deep learning techniques. To
conduct the experiments, we sample 40000 point clouds from the 3D data in
ShapeNet to train a network and sample 350 point clouds from the 3D data in
ModelNet10 to test. Experimental results show that our algorithm can estimate
normal vectors of points in noisy point clouds. Comparing to five competitive
methods, the proposed algorithm achieves better denoising performance and
produces much smaller variances.","The rapid development of 3D sensing techniques and the emerging Ô¨Åeld of 2D imagebased 3D reconstruction make it possible to sample or generate millions of 3D points from surfaces of objects [1‚Äì3]. 3D point clouds are discrete rep resentations of continuous surfaces and are widely used in robotics, virtual reality, and computeraided shape design. 3D point clouds sampled by 3D scanners are generally noisy due to measurement noise, especially around edges and cor ners [4]. 3D point clouds reconstructed from multiview images contain noise since the reconstructed algorithms fail to manage matching ambiguities [5, 6]. The inevitable noise in 3D point clouds undermines the performance of surfacereconstruction algorithms and impairs further geometry pro cessing tasks since the Ô¨Åne details are lost and the underlying manifold structures are prone to be deformed [7]. However, 3D point clouds denoising or processing is chal lenging because 3D point clouds are permutation invariant and the neighboring points representing the local topology interact without any explicit connecting information. To de noise 3D point clouds, we aim to estimate the continuous sur face localized around each 3D point and remove noise by pro jecting noisy points to the corresponding local surfaces. The intuition is that noiseless points are sampled from surfaces. To estimate local surfaces, we parameterize them by 2D ref erence planes. By projecting noisy points to the estimated reference planes, we ensure that all the denoised points come from the underlying surfaces. Deep neural networks have shown groundbreaking per formances in various domains, such as speech processing and image processing [8]. Recently, several deeplearning archi tectures have been proposed to deal with 3D point clouds in tasks such as classiÔ¨Åcation, segmentation, and upsam pling [9‚Äì11]. In this work, we learn reference planes from noisy point clouds and further reduce noise with deep learn ing; we name the proposed algorithm as neural projection de noising (NPD). Estimated reference planes are represented by normal vectors and interceptions. The reason for using deep learning is that previous algorithms are not robust enough to noise intensity, sampling density, and curvature variations. These algorithms need to deÔ¨Åne neighboring points to capture local structures. However, it is difÔ¨Åcult to choose the neigh boring points adaptively according to the sampling density or curvature variation. In our experiments, the point clouds used for training are sampled from the 3D dataset ShapeNet and the point clouds used for testing are sampled from the 3D dataset Mod elNet10 [12, 13]. The experimental results show that NPD outperforms four of Ô¨Åve other denoising algorithms in all seven categories and achieves the lowest variance in both evaluation metrics. Contributions. 1) To the best of our knowledge, NPD is the Ô¨Årst to directly deal with 3D point clouds for denoisarXiv:1904.04427v1  [cs.CV]  9 Apr 2019ing tasks with deep learning techniques; 2) NPD can estimate normal vector for each point with both local and global infor mation and is less affected by noise intensity and curvature variation; 3) NPD can denoise noisy point clouds without deÔ¨Åning neighboring points for noisy point clouds or calcu lating the eigendecomposition to estimate local geometries; 4) NPD provides the possibility of 3D point cloud parameter ization with the combination of local and global information. 2. RELATED WORK "
362,Decoupled Multi-task Learning with Cyclical Self-Regulation for Face Parsing.txt,"This paper probes intrinsic factors behind typical failure cases (e.g.
spatial inconsistency and boundary confusion) produced by the existing
state-of-the-art method in face parsing. To tackle these problems, we propose a
novel Decoupled Multi-task Learning with Cyclical Self-Regulation (DML-CSR) for
face parsing. Specifically, DML-CSR designs a multi-task model which comprises
face parsing, binary edge, and category edge detection. These tasks only share
low-level encoder weights without high-level interactions between each other,
enabling to decouple auxiliary modules from the whole network at the inference
stage. To address spatial inconsistency, we develop a dynamic dual graph
convolutional network to capture global contextual information without using
any extra pooling operation. To handle boundary confusion in both single and
multiple face scenarios, we exploit binary and category edge detection to
jointly obtain generic geometric structure and fine-grained semantic clues of
human faces. Besides, to prevent noisy labels from degrading model
generalization during training, cyclical self-regulation is proposed to
self-ensemble several model instances to get a new model and the resulting
model then is used to self-distill subsequent models, through alternating
iterations. Experiments show that our method achieves the new state-of-the-art
performance on the Helen, CelebAMask-HQ, and Lapa datasets. The source code is
available at
https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr.","Face parsing, as a finegrained semantic segmentation task, intends to assign a pixelwise label for each facial component, e.g., eyes, nose, and mouth. The detailed anal ysis of semantic facial parts is essential in many highlevel applications, such as face swapping [28], face editing [15], This work is done when Qingping Zheng is an intern at Huawei. Image Image GT GT EAGRNet Image Ours GT Figure 1. The first three rows show typical failure cases of spatial inconsistency and boundary confusion when applying EARGNet [36] to face parsing. The last row displays noisy labels on the training datasets. and facial makeup [29]. Benefit from the learning capac ity of deep Convolutional Neural Networks (CNNs) and the labor effort put in pixellevel annotations [15, 21, 35], methods based on Fully Convolutional Networks (FCNs) [7, 10, 18‚Äì20, 23, 36, 47, 48] have achieved a promising per formance on the fully supervised face parsing. Neverthe less, the local characteristic of the convolutional kernel pre vents FCNs from capturing global contextual information [25], which is crucial for semantically parsing facial com ponents in an image. To address this issue, most of the regionbased face pars ing methods [10,20,47] integrate CNN features into variant CRFs to learn global information. However, these methods do not consider the correlation among various objects. To this end, Te et al. [36] proposes the EAGRNet method to model a regionlevel graph representation over a face image by propagating information across all vertices on the graph. Even though EAGRNet enables reasoning over nonlocalregions to get global dependencies between distant facial components and achieves stateoftheart performance, it still faces the problems of spatial inconsistency and bound ary confusion. In EAGRNet, PSP module [45] adopts an av erage pooling layer [22] to capture the global context prior, leading to an inconsistent spatial topology. Moreover, EA GRNet integrates additional clues of binary edges into con text embedding to improve the parsing results. However, it is hard for EAGRNet to handle boundaries between highly irregular facial parts ( e.g. hair and cloth in Figure 1) and dis tinguish clear boundaries between different face instances in the crowded scenarios (multifaces in Figure 1). Besides, learning a reliable model for face parsing re quires accurate pixellevel annotations. Nonetheless, there inevitably exist careless manual labeling errors on the train ing dataset as shown in the last row of Figure 1. Te et al. [36] employ the traditional fully supervised learning scheme to train EAGRNet, failing to locate label noise be cause all pixels in the ground truth are processed equally. Notably, overlooking such incomplete annotations restricts the model generalization and prevents the performance from increasing to a higher level. In this paper, we propose an endtoend face parsing method, which is based on Decoupled Multitask Learn ing with Cyclical SelfRegulation (DMLCSR). Specifi cally, given an input of facial image, the ResNet101 [8] pretrained on ImageNet is taken as the backbone to extract features from different levels. Afterwards, our multitask model consists of three tasks, namely face parsing, binary edge detection, and category edge detection. These tasks share lowlevel weights from the backbone but do not have highlevel interactions. Therefore, our multitask learning approach can detach additional edge detection tasks from face parsing at the inference stage. To tackle spatial incon sistency raised by the pooling operation, we develop a Dy namic Dual Graph Convolutional Network (DDGCN) in the face parsing branch to capture longrange contextual infor mation. The proposed DDGCN contains no extra pooling operation and it can dynamically fuse the global context ex tracted from GCNs in both spatial and feature spaces. To solve the boundary confusion in both singleface and multi face scenarios, the proposed categoryaware edge detection module exploits more semantic information than the binary edge detection module used in EARGNet [36]. To address the problem caused by noisy labels in train ing datasets, we introduce a cyclically learning scheduler inspired by selftraining [3, 16, 34, 41, 42, 42, 49] to achieve advanced cyclical selfregulation. The proposed CSR con tains a selfensemble strategy that can aggregate a set of his torical models to obtain a new reliable model and another selfdistillation method that exploits the soft labels gener ated by the aggregated model to guide the successive model learning. Finally, the proposed CSR iteration alternates between these two procedures, correcting the noisy labels dur ing training and promoting the model generalization. The proposed CSR can significantly promote the reliability of the model and labels in a cyclical training scheduler with out introducing extra computation costs. To summarize, our main contributions are as follows: ‚Ä¢ We propose a decoupled multitask network includ ing face parsing, binary edge detection, and category edge detection. The face parsing branch introduces a DDGCN without any extra pooling operation to solve the problem of spatial inconsistency, and an additional category edge detection branch is designed to handle the boundary confusion. ‚Ä¢ We introduce a cyclical selfregulation mechanism during training. The iteration alternates between one selfensemble procedure, boosting model generaliza tion progressively, and another selfdistillation pro cessing, regulating noisy labels. ‚Ä¢ Our method establishes new stateoftheart perfor mance on the Helen [35] (93.8% overall F1 score), LaPa [21] (92.4% mean F1) and CelebAMaskHQ [15] (86.1% mean F1) datasets. Compared to EARGNet [36], our method utilizes fewer computation resources as the edge prediction modules can be decoupled from the whole network, decreasing the inference time from 89ms to 31ms but achieving much better performance. 2. Related Work "
363,Prototypical Classifier for Robust Class-Imbalanced Learning.txt,"Deep neural networks have been shown to be very powerful methods for many
supervised learning tasks. However, they can also easily overfit to training
set biases, i.e., label noise and class imbalance. While both learning with
noisy labels and class-imbalanced learning have received tremendous attention,
existing works mainly focus on one of these two training set biases. To fill
the gap, we propose \textit{Prototypical Classifier}, which does not require
fitting additional parameters given the embedding network. Unlike conventional
classifiers that are biased towards head classes, Prototypical Classifier
produces balanced and comparable predictions for all classes even though the
training set is class-imbalanced. By leveraging this appealing property, we can
easily detect noisy labels by thresholding the confidence scores predicted by
Prototypical Classifier, where the threshold is dynamically adjusted through
the iteration. A sample reweghting strategy is then applied to mitigate the
influence of noisy labels. We test our method on CIFAR-10-LT, CIFAR-100-LT and
Webvision datasets, observing that Prototypical Classifier obtains substaintial
improvements compared with state of the arts.","Deep neural networks (DNNs) have been widely used for machine learning applications. Despite of their success, it has been shown that the training of DNNs requires largescale labeled and unbiased data. However, in many realworld applications, training set biases are prevalent [21, 27, 28, 9], which typically have two types: i) classimbalanced data distribution; and ii) noisy labels. For example, in autonomous driving, the vast majority of the training data is composed of standard vehicles but models also need to recognize rarely seen classes such as emergency vehicles or animals with very high accuracy. This will sometime lead to biased training models that do not perform well in practice. Moreover, largescale highquality data annotations are expensive and timeconsuming to obtain. Although coarse labels are cheap and of high availability, the presence of noise will hurt the model performance. Therefore, it is desirable to develop machine learning algorithms that can accommodate not only classimbalanced training set, but also the presence of label noise. Both learning with noisy labels and classimbalanced learning (a.k.a. longtailed learning) have been studied for many years. When dealing with label noise, the most popular approach is sample selection where correctlylabeled examples are identiÔ¨Åed by capturing the training dynamics of DNNs [ 11,29]. When dealing with class imbalance, many existing works propose to reweight examples or design unbiased loss functions by taking into account the class distribution of training set [26, 3, 8]. However, most existing methods focus on only one of these two training set biases. In this paper, we address both training set biases simultaneously. As shown in Figure 1a, it is known that the classiÔ¨Åer directly learned on classimbalanced data is biased towards head classes [ 8,32] which results in poor generalization on tail classes. Moreover, using sample loss/conÔ¨Ådence produced by biased classiÔ¨Åers fails to detect label noise,arXiv:2110.11553v1  [cs.CV]  22 Oct 2021Prototypical ClassiÔ¨Åer for Robust ClassImbalanced Learning A P REPRINT ùë•ùë•1 (a) Normal ClassiÔ¨Åer ùëêùëê2ùëêùëê1 ùëêùëê3ùë•ùë•2 (b) Prototypical 1NN ùëêùëê2ùëêùëê1 ùëêùëê3ùë•ùë•2ùë•ùë•1 (c) Prototypical ClassiÔ¨Åer Figure 1: Illustration of normal classiÔ¨Åer and Prototypical ClassiÔ¨Åer. because both clean and noisy samples of tail classes have large loss and low conÔ¨Ådence. To solve this problem, we propose to use Prototypical ClassiÔ¨Åer which is demonstrated to produce balanced predictions even through the training set is classimbalanced. Our basic idea is that there exists an embedding in which examples cluster around a single prototype representation for each class. In order to do this, we learn a nonlinear mapping of the input into an embedding space using a neural network and take a class‚Äôs prototype to be the normalized mean vector of examples in the embedding space. ClassiÔ¨Åcation is then performed for an embedded test example by simply Ô¨Ånding the nearest class prototype. Notably, Prototypical ClassiÔ¨Åer does not need additional learnable parameters given embedding of examples. Unfortunately, it is easy to observe that simply using prototypes for classiÔ¨Åcation may lead to many wrong predictions for samples of head classes as shown in Figure 1b. The reason is that the representations are supposed to be modiÔ¨Åed when the classiÔ¨Åcation boundaries of tail classes expand. We therefore train the neural networks to pull together embedding of examples and the prototype of their class, while pushing apart examples from prototypes of other classes. By doing this, it can avoid many misclassiÔ¨Åcations for samples of head classes, as shown in Figure 1c. Subsequently, we Ô¨Ånd that the conÔ¨Ådence scores produced by Prototypical ClassiÔ¨Åer is balanced and comparable across classes. By leveraging this property, we can simply detect noisy labels via thresholding where the threshold is dynamically adjusted, followed by a sample reweighting strategy. In summary, our key contributions of this work are: ‚Ä¢ We propose to learn from training set with mixed biases, which is practical but has been understudied; ‚Ä¢Our approach, Prototype ClassiÔ¨Åer, is simple yet powerful. It produces more balanced predictions over all classes than normal classiÔ¨Åers even when the training set is classimbalanced. This property further beneÔ¨Åts the detection of label noise. ‚Ä¢On both simulated datasets and a realworld dataset Webvision with label noise, Prototype ClassiÔ¨Åer achieves substaintial performance improvement. 2 Related Work "
364,Asymmetric Tri-training for Unsupervised Domain Adaptation.txt,"Deep-layered models trained on a large number of labeled samples boost the
accuracy of many tasks. It is important to apply such models to different
domains because collecting many labeled samples in various domains is
expensive. In unsupervised domain adaptation, one needs to train a classifier
that works well on a target domain when provided with labeled source samples
and unlabeled target samples. Although many methods aim to match the
distributions of source and target samples, simply matching the distribution
cannot ensure accuracy on the target domain. To learn discriminative
representations for the target domain, we assume that artificially labeling
target samples can result in a good representation. Tri-training leverages
three classifiers equally to give pseudo-labels to unlabeled samples, but the
method does not assume labeling samples generated from a different domain.In
this paper, we propose an asymmetric tri-training method for unsupervised
domain adaptation, where we assign pseudo-labels to unlabeled samples and train
neural networks as if they are true labels. In our work, we use three networks
asymmetrically. By asymmetric, we mean that two networks are used to label
unlabeled target samples and one network is trained by the samples to obtain
target-discriminative representations. We evaluate our method on digit
recognition and sentiment analysis datasets. Our proposed method achieves
state-of-the-art performance on the benchmark digit recognition datasets of
domain adaptation.","With the development of deep neural networks in cluding deep convolutional neural networks (CNN) 1The University of Tokyo, Tokyo, Japan. Correspon dence to: Kuniaki Saito <ksaito@mi.t.utokyo.ac.jp >, Yoshi taka Ushiku <ushiku@mi.t.utokyo.ac.jp >, Tatsuya Harada <harada@mi.t.utokyo.ac.jp >.(Krizhevsky et al. ,2012 ), the recognition abilities of im ages and languages have improved dramatically. Train ing deeplayered networks with a large number of la beled samples enables us to correctly categorize samples in diverse domains. In addition, the transfer learning of CNN is utilized in many studies. For object detection or segmentation, we can transfer the knowledge of a CNN trained with a largescale dataset by Ô¨Ånetuning it on a relatively small dataset ( Girshick et al. ,2014 ;Long et al. , 2015a ). Moreover, features from a CNN trained on Ima geNet ( Deng et al. ,2009 ) are useful for multimodal learn ing tasks including image captioning ( Vinyals et al. ,2015 ) and visual question answering ( Antol et al. ,2015 ). One of the problems of neural networks is that although they perform well on the samples generated from the same distribution as the training samples, they may Ô¨Ånd it difÔ¨Å cult to correctly recognize samples from different distrib u tions at the test time. One example is images collected from the Internet, which may come in abundance and be fully la beled. They have a distribution different from the images taken from a camera. Thus, a classiÔ¨Åer that performs well on various domains is important for practical use. To real ize this, it is necessary to learn domaininvariantly discr im inative representations. However, acquiring such represe n tations is not easy because it is often difÔ¨Åcult to collect a large number of labeled samples and because samples from different domains have domainspeciÔ¨Åc characteristics. In unsupervised domain adaptation, we try to train a clas siÔ¨Åer that works well on a target domain on the condi tion that we are provided labeled source samples and un labeled target samples during training. Most of the pre vious deep domain adaptation methods have been pro posed mainly under the assumption that the adaptation can be realized by matching the distribution of features from different domains. These methods aimed to ob tain domaininvariant features by minimizing the diver gence between domains as well as a category loss on the source domain ( Ganin & Lempitsky ,2014 ;Long et al. , 2015b ;2016 ). However, as shown in ( BenDavid et al. , 2010 ), theoretically, if a classiÔ¨Åer that works well on both the source and the target domains does not exist, we can not expect a discriminative classiÔ¨Åer for the target domain . That is, even if the distributions are matched on the non discriminative representations, the classiÔ¨Åer may not wor kAsymmetric Tritraining for Unsupervised Domain Adaptati on !""#$$%&%'()*!""#$$%&%'(+* ,#'""'./$01(2'/$#34""'$ *!""#$$%&%'() *!""#$$%&%'(+ * 56""#'""'./7#(8'7/$#34""'$ * !""#$$/9 *!""#$$/9 *!""#$$%&%'(/7* :$'1.0;""#'""'./7#(8'7/$#34""'$ * Figure 1. Outline of our model. We assign pseudolabels to unla beled target samples based on the predictions from two class iÔ¨Åers trained on source samples. well on the target domain. Since directly learning discrimi  native representations for the target domain, in the absenc e of target labels, is considered very difÔ¨Åcult, we propose to assign pseudolabels to target samples and train target speciÔ¨Åc networks as if they were true labels. Cotraining and tritraining ( Zhou & Li ,2005 ) leverage multiple classiÔ¨Åers to artiÔ¨Åcially label unlabeled sample s and retrain the classiÔ¨Åers. However, the methods do not assume labeling samples from different domains. Since our goal is to classify unlabeled target samples that have different characteristics from labeled source samples, we propose asymmetric tritraining for unsupervised domain adaptation. By asymmetric , we mean that we assign differ ent roles to three classiÔ¨Åers. In this paper, we propose a novel tritraining method for unsupervised domain adaptation, where we assign pseudo labels to unlabeled samples and train neural networks uti lizing the samples. As described in Fig. 1, two networks are used to label unlabeled target samples and the remain ing network is trained by the pseudolabeled target sam ples. Our method does not need any special implementa tions. We evaluate our method on the digit classiÔ¨Åcation task, trafÔ¨Åc sign classiÔ¨Åcation task and sentiment analysi s task using the Amazon Review dataset, and demonstrate stateoftheart performance in nearly all experiments. I n particular, in the adaptation scenario, MNIST ‚ÜíSVHN, our method outperformed other methods by more than 10%. 2. Related Work "
365,NoisyActions2M: A Multimedia Dataset for Video Understanding from Noisy Labels.txt,"Deep learning has shown remarkable progress in a wide range of problems.
However, efficient training of such models requires large-scale datasets, and
getting annotations for such datasets can be challenging and costly. In this
work, we explore the use of user-generated freely available labels from web
videos for video understanding. We create a benchmark dataset consisting of
around 2 million videos with associated user-generated annotations and other
meta information. We utilize the collected dataset for action classification
and demonstrate its usefulness with existing small-scale annotated datasets,
UCF101 and HMDB51. We study different loss functions and two pretraining
strategies, simple and self-supervised learning. We also show how a network
pretrained on the proposed dataset can help against video corruption and label
noise in downstream datasets. We present this as a benchmark dataset in noisy
learning for video understanding. The dataset, code, and trained models will be
publicly available for future research.","The ImageNet dataset [ 5] has been one of the catalysts behind the exponential growth in Deep Learning [ 19] and large scale machine learning research, along with transfer learning adoption to adapt large trained networks on problems with little data. This has led to many largescale datasets targeting various tasks such as classifica tion, detection, segmentation, etc., and a rising interest in training bigger networks to capture more variations and transfer well. Ima geNet [ 5], and Youtube8M [ 1] are enormous datasets in terms of size and annotations. Still, it is not always possible to construct such massive annotated datasets due to logistical and time constraints. Collecting data from the web is getting much popularity due to its availability on several social media platforms (e.g., Webvision [23], and Clothing1M [ 43]). Along with these datasets, many other works [ 7] [4] have shown how learning from web data dramatically increases performance in related domains, despite labels which are mostly inferred from surrounding meta data and not manually verified. Moreover, the meta data itself acts as a rich source of information about the data point for tasks like image captioning, video understanding, etc. As an active research area, there is a dire need to set a standard benchmark for efficient learning from noisy web data. With this objective in mind, we construct such a dataset with a primary focus on video modality. Our dataset consists of raw videos collected from Flickr, with surrounding meta data such as title, description, comments, etc. It has been collected using class labels from popu lar video classification benchmarks as search queries since these datasets have already established useful labels based on various criteria. We first look at various statistics of our dataset to set its importance and show some preliminary results for video action classification. A major challenge that one encounters while learning from web collected data is its heavily imbalanced multilabel nature. Similar works [ 9] randomly select one label from the list of given multi labels. We compare various multilabel learning strategies in literature while pretraining on our dataset and also look at the setting of simple pretraining or combining it with selfsupervised learning at various stages, hoping that this will set a benchmark for the research community. Finally, we also obtain some surprisingarXiv:2110.06827v1  [cs.MM]  13 Oct 2021Gold Coast ‚Äô21, December 01‚Äì03, 2021, Gold Coast, Australia Mohit Sharma, Raj Patra, Harshal Desai, Shruti Vyas, Yogesh Rawat, and Rajiv Ratn Shah results around how models pretrained on the proposed noisy dataset provide some robustness against label noise and video corruption with just simple finetuning and no modification to the training pipeline. We first talk about related work in Section 2. Next, in Section 3, we discuss our dataset construction and statistics. We describe our methodology in Section 4and our experimental setup in Section 5. We finally present our results and a discussion around them in Section 6. We end with Section 7, discussing how this work can be further improved and new research directions from our proposed dataset. 2 RELATED WORK "
366,A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model.txt,"In this work, we explore Parameter-Efficient-Learning (PEL) techniques to
repurpose a General-Purpose-Speech (GSM) model for Arabic dialect
identification (ADI). Specifically, we investigate different setups to
incorporate trainable features into a multi-layer encoder-decoder GSM
formulation under frozen pre-trained settings. Our architecture includes
residual adapter and model reprogramming (input-prompting). We design a
token-level label mapping to condition the GSM for Arabic Dialect
Identification (ADI). This is challenging due to the high variation in
vocabulary and pronunciation among the numerous regional dialects. We achieve
new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We
further reduce the training budgets with the PEL method, which performs within
1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable
parameters. Our study demonstrates how to identify Arabic dialects using a
small dataset and limited computation with open source code and pre-trained
models.","Dialect identiÔ¨Åcation [1, 2] (DI) amounts to identifying similar dialects belonging to the same language family. It is a speciÔ¨Åc case of language identiÔ¨Åcation [3] (LID) task. However, DI is more challenging than LID owing to the fact that dialects share similar acoustic and linguistic characteristics compared to dif ferent languages. Very minute differences in pronunciation [4] and accent are used as cues to identify dialects. Moreover, DI does not share the advantage of publicly available speech recog nition models pretrained on large speech data corpora for net work initialization. Despite these challenges, DI remains rela tively unexplored compared to LID. In this study, we leverage upon a recent openaccess and generalpurpose speech recognition architecture, Whisper [5], pretrained on a large speech corpus from OpenAI, to address DI in resourceconstrained and datalimited conditions. We use ParameterEfÔ¨Åcient Learning [6, 7] (PEL) to adapt a large pre trained model by training small additive modules embedded into the frozen pretrained model. By doing so, we require less training time and computing resources to Ô¨Ånetune the model for DI. Figure 1 is a schematic of the proposed parameter efÔ¨Åcient learning framework. We choose to perform DI in Arabic owing to its substantial regional variations and the widespread use of Arabic as an ofÔ¨Åcial language in over 22 countries [8]. Notably, signiÔ¨Åcant differences exist between the standard written form, referred to as Modern Standard Arabic, and the local colloquial dialects spoken in each region. Interestingly, not all dialects are Figure 1: Overview of proposed parameterefÔ¨Åcient learn ing framework for Arabic dialect identiÔ¨Åcation building upon parameterefÔ¨Åcient learning [7] and label mapping [10]. mutually intelligible. In this study, we present several contributions: (1) Firstly, we introduce the novel use of ParameterEfÔ¨ÅcientLearning (PEL) for this task, marking the Ô¨Årst application of this ap proach to Arabic dialect identiÔ¨Åcation [8]. (2) We investigate different designs to incorporate trainable features into a multi layer encoderdecoder frozen model. (3) We achieve new state oftheart accuracy on the ofÔ¨Åcial testing and development sets of ADI17 [9] dataset using only 30.95% of the training data. (4) Lastly, we demonstrate that our PEL method achieves equiv alent performance to full Ô¨Ånetuning using only 2.5% of (extra) network parameters. 2. Related work "
367,How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?.txt,"Incorrectly labeled examples, or label noise, is common in real-world
computer vision datasets. While the impact of label noise on learning in deep
neural networks has been studied in prior work, these studies have exclusively
focused on homogeneous label noise, i.e., the degree of label noise is the same
across all categories. However, in the real-world, label noise is often
heterogeneous, with some categories being affected to a greater extent than
others. Here, we address this gap in the literature. We hypothesized that
heterogeneous label noise would only affect the classes that had label noise
unless there was transfer from those classes to the classes without label
noise. To test this hypothesis, we designed a series of computer vision studies
using MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous
label noise during the training of multi-class, multi-task, and multi-label
systems. Our results provide evidence in support of our hypothesis: label noise
only affects the class affected by it unless there is transfer.","Supervised deep learning models have been successful in various tasks such as large scale image classiÔ¨Åcation, object detection, semantic segmentation, and many more [16,24,26]. One of the signiÔ¨Åcant contributions behind the success of supervised deep learning is the availability of welllabeled large datasets. However, such welllabeled datasets are only available for a handful of problems [8,19]. Often tools like Amazon Mechanical Turk [6] and Computer Vision Annotation Tool (CV AT) [3] are used to la bel them. The problem with these tools is that they are expensive and require signiÔ¨Åcant time and human effort to label. To circumvent that, many datasets in the real world are either incompletely labeled or extracted from sources that inherently contain label noise [9]. Label noise is detrimental to the training of any deep learning model as it directly impacts the model‚Äôs learning ability [37]. Vision tasks learned with noisy labels don‚Äôt generalize well, resulting in poor test performance[33]. It is essential to thoroughly study the impact of noisy labels to understand how they are associated with poor per formance. This knowledge can be used to improve the current methods that learn witharXiv:2106.15475v3  [cs.CV]  26 Sep 20212 B. Khanal et al. Fig. 1: An example of how classdependent heterogeneous label noise is introduced by corrupting the labels of CIFAR10 and MNIST Dataset. We investigated the impact of noisy labels (red) on the model‚Äôs performance on clean labels (green). noisy labels [18]. However, to the best of our knowledge, most of the works studied up to now have mainly focused on examining the performance of the deep learning model under the inÔ¨Çuence of homogeneous noisy labels imposed by corrupting all the true labels with the same degree [4,27]. We know that the noise may not always be homogeneous and can depend on various heterogeneous sources [35]. Some of the labels might be affected to a greater extent than others because of which the label noise is heterogeneous in nature. The previous studies have not thoroughly investigated the heterogeneous case in supervised vision tasks. Therefore, some open questions still exist. For example: what is the impact of heterogeneous noisy labels of certain classes on the performance of a class with clean labels (as shown in Fig 1) when they are trained together in a naive classiÔ¨Åcation setting? We want to examine to what extent the noisefree class is affected by classdependent noisy labels. We further extended the question to study the impact in other classiÔ¨Åcation settings, such as multitask and multilabel learning. Multitask learning[1] is an approach where a single network is trained to perform two or more tasks. While training a multitask network, the tasks could positively or negatively interfere resulting in positive or negative transfer respectively. Positive trans fer improves the performance of another task, while negative transfer impacts its perfor mance. We hypothesized that if there is a positive transfer between two or more tasks, then training with noisy tasks should impact the performance of clean tasks. The trans ferred beneÔ¨Åt obtained by training tasks together should drop with an increase in label noise in helping tasks. In this work, we veriÔ¨Åed our hypothesis with experiments. Finally, we also investigated the impact of labeldependent noisy labels in multi label learning, in which noise is present only in a certain group of labels, and examined the impact on the group with clean labels. In summary, the key contributions of our work are:How Does Heterogeneous Label Noise Impact Generalization in Neural Nets? 3 Using the popular vision datasets: MNIST, CIFAR10, CIFAR100, and MSCOCO dataset, we assessed the impact of classdependent, taskdependent, and labeldependent heterogeneous noisy labels on multiclass classiÔ¨Åcation, multitask learning, and multi label learning settings, respectively with an attempt to the Ô¨Åll gap that previous studies didn‚Äôt cover. By investigating taskdependent heterogeneous noisy labels, we showed that if there is a positive transfer from one task to another, inducing label noise in helping task should also impact the performance of other tasks that have clean labels. The drop in task transfer beneÔ¨Åt is proportional to the number of noisy labels in helping tasks, i.e., the higher the noisy labels, the higher the transfer drop. 2 Related Works "
368,Opportunities and Challenges of Deep Learning Methods for Electrocardiogram Data: A Systematic Review.txt,"Background:The electrocardiogram (ECG) is one of the most commonly used
diagnostic tools in medicine and healthcare. Deep learning methods have
achieved promising results on predictive healthcare tasks using ECG signals.
Objective:This paper presents a systematic review of deep learning methods for
ECG data from both modeling and application perspectives. Methods:We extracted
papers that applied deep learning (deep neural network) models to ECG data that
were published between Jan. 1st of 2010 and Feb. 29th of 2020 from Google
Scholar, PubMed, and the DBLP. We then analyzed each article according to three
factors: tasks, models, and data. Finally, we discuss open challenges and
unsolved problems in this area. Results: The total number of papers extracted
was 191. Among these papers, 108 were published after 2019. Different deep
learning architectures have been used in various ECG analytics tasks, such as
disease detection/classification, annotation/localization, sleep staging,
biometric human identification, and denoising. Conclusion: The number of works
on deep learning for ECG data has grown explosively in recent years. Such works
have achieved accuracy comparable to that of traditional feature-based
approaches and ensembles of multiple approaches can achieve even better
results. Specifically, we found that a hybrid architecture of a convolutional
neural network and recurrent neural network ensemble using expert features
yields the best results. However, there are some new challenges and problems
related to interpretability, scalability, and efficiency that must be
addressed. Furthermore, it is also worth investigating new applications from
the perspectives of datasets and methods. Significance: This paper summarizes
existing deep learning research using ECG data from multiple perspectives and
highlights existing challenges and problems to identify potential future
research directions.","The electrocardiogram (ECG/EKG) is one of the most commonly used noninvasive diagnostic tools for recording thephysiologicalactivitiesoftheheartoveraperiodoftime. ECG data can aid in the diagnosis of many cardiovascular abnormalities, such as premature contractions of the atria (PAC) or ventricles (PVC), atrial Ô¨Åbrillation (AF), myocar dial infarction (MI), and congestive heart failure (CHF). In recent years, we have witnessed the rapid development of portableECGmonitorsinthemedicalÔ¨Åeld,suchastheHolter monitor [129], and wearable devices in various healthcare areas, such as the Apple Watch. As a result, the amount of ECG data requiring analysis has grown too rapidly for human cardiologists to keep up. Therefore, analyzing ECG dataautomaticallyandaccuratelyhasbecomeahotresearch topic. Additionally, many emerging applications, such as biometrichumanidentiÔ¨Åcationandsleepstaging,canbeim plemented based on ECG data. sdhong1503@gmail.com (S. Hong); joy_yuxi@pku.edu.cn (Y. Zhou); sjy1203@pku.edu.cn (J. Shang); cao.xiao@iqvia.com (C. Xiao); jimeng.sun@gmail.com (J. Sun) ORCID(s):Traditionally, automatic ECG analysis has relied on di agnostic golden rules. As shown in the top of Figure 1, this is a twostage method that requires human experts to engi neer useful features based on raw ECG data, which are re ferredtoas‚Äúexpertfeatures‚Äù,andthendeploydecisionrules or other machine learning methods to generate Ô¨Ånal results. Expert features can be categorized [69] into statistical fea tures(suchasheartratevariability[19],sampleentropy[3], and coeÔ¨Écients of variation and density histograms [172]), frequencydomainfeatures[151,106],andtimedomainfea tures (such as the Philips 12 lead ECG Algorithm [139]). Inpractice,expertfeaturesareautomaticallyextractedusing computerbased algorithms. However, they are still insuf Ô¨Åcient because they are limited by data quality and human expert knowledge [157, 161, 50]. Recently, deep learning methods have achieved promis ingresultsinmanyapplicationareas,suchasspeechrecogni tion,computervision,andnaturallanguageprocessing[89]. The main advantage of deep learning methods is that they do not require an explicit feature extraction step using hu man experts, as shown in the bottom of Figure 1. Instead, feature extraction is performed automatically and implicitly bydeeplearningmodelsbasedontheirpowerfuldatalearn Shenda Hong et al.: Preprint submitted to Elsevier Page 1 of 16arXiv:2001.01550v3  [eess.SP]  30 Apr 2020Opportunities and Challenges of Deep Learning for ECG Data Results ResultsExpert FeaturesTraditionalMethodsDeep LearningMethodsExpertMachine Learning Models Deep Neural Networks ECG	Data Figure 1: Comparisons between traditional methods and deep learning methods. ing capabilities and Ô¨Çexible processing architectures. Some studieshaveexperimentallydemonstratedthatdeeplearning features are more informative than expert features for ECG data [67, 69]. The performance of deep learning methods isalsosuperiortothatoftraditionalmethodsonmanyECG analysistasks,suchasdiseasedetection[30]andsleepstag ing [42]. Although some papers have reviewed machine learning methodsforECGdata[121](2019),cardiacarrhythmiade tection using deep learning [136] (2019), and deep learn ing methods for ECG data [135] (2018), there have no sys tematic reviews focusing on deep learning methods, which weconsidertobepromisingmethodsforminingECGdata. Therefore, we believe it is crucial to conduct a systematic reviewofexistingdeeplearningmethodsforECGdatafrom theperspectivesofmodelarchitecturesandapplicationtasks. Challengesandproblemsrelatedtothecurrentresearchsta tus are discussed, which should provide inspiration and in sights for future work. 2. Method "
369,Guided Labeling using Convolutional Neural Networks.txt,"Over the last couple of years, deep learning and especially convolutional
neural networks have become one of the work horses of computer vision. One
limiting factor for the applicability of supervised deep learning to more areas
is the need for large, manually labeled datasets. In this paper we propose an
easy to implement method we call guided labeling, which automatically
determines which samples from an unlabeled dataset should be labeled. We show
that using this procedure, the amount of samples that need to be labeled is
reduced considerably in comparison to labeling images arbitrarily.","Deep learning has gained a lot of interest over the last few years because the methods perform very well on a wide range of machine learning tasks. One class of especially successful deep learning methods are convolutional neural networks (CNNs) for image classiÔ¨Åcation. Unfortunately, CNNs need a large amount of labeled training data to perform well. In many cases, this label ing is performed by humans. A common approach is to use some form of crowd based labeling. For example, Amazon Mechanical Turk [20] was used for labeling the ImageNet dataset [3]. Data can also be obtained as a side effect of some human interaction with an online system. For exam ple, CAPTCHA [21] challenges to prevent bots using online services can be set up to produce labeled data as a side effect of the veriÔ¨Åcation procedure [5]. Alas, simply labeling all available samples is a very in efÔ¨Åcient use of human labor, since not all samples will be of equal value. On the one hand, adding a sample which is similar to samples already in the dataset will not be very usefull. On the other hand, in most cases, not all classes will have the same difÔ¨Åculty and it might make sense to add more samples of the difÔ¨Åcult classes to the dataset. There fore, it would be advantageous to label a sample whichwould maximize the classiÔ¨Åcation accuracy of a system. Unfortunately, how much the quality of a dataset would in crease by adding a speciÔ¨Åc labeled sample can only be de termined after labeling and training on the resulting dataset. This is obviously not useful if we want to decide which sam ples should be labeled in the Ô¨Årst place. We propose to use the classiÔ¨Åcation conÔ¨Ådence of a CNN while trying to predict the class of unlabeled images to de cide what to label next. The proposed procedure, together with extensive data augmentation, will be evaluated for two small neural networks on the MNIST [10] and CIFAR10 [9] dataset. In practice we propose the following workÔ¨Çow: A small, labeled dataset is used to train a neural network that is used to select a batch of the most confusing images from a set of unlabeled data. This batch is given to human workers who label the images, after which they are added to the training dataset and the process repeats. We call this pro cedure guided labeling . Our hypothesis is that, using this procedure, we are able to trade human for computational resources. 2. Related Work "
370,Star algorithm for NN ensembling.txt,"Neural network ensembling is a common and robust way to increase model
efficiency. In this paper, we propose a new neural network ensemble algorithm
based on Audibert's empirical star algorithm. We provide optimal theoretical
minimax bound on the excess squared risk. Additionally, we empirically study
this algorithm on regression and classification tasks and compare it to most
popular ensembling methods.","Deep learning has been successfully applied to many types of problems and has reached the state oftheart performance. Deep learning models have shown good results in regression analysis and time series forecasting [Qiu+14], computer vision [He+16], as well as in natural language processing [OMK20] and other areas. In many complex problems, such as the Imagenet competition [Den+09], the best results are achieved by ensembles of neural networks, that is, it is often useful to combine the predictions of multiple neural networks to create a new one. The easiest way to ensemble multiple neural networks is to average their predictions [Dru+94]. As shown in work [Kaw16], the number of local minima grows exponentially with the number of parameters. And since modern neural network training methods are based on stochastic optimization, two identical architectures optimized with different initializations will probably converge to different solutions. Such a technique for obtaining neural networks with subsequent construction of an ensemble by majority voting or averaging is used, for example, in article [Car+04]. In addition to the fact that the class of deep neural networks has a huge number of local minima, it is also nonconvex. It was shown in work [LM09] that for the procedure of minimizing the empirical risk in a nonconvex class of functions, the order of convergence is not optimal. In fact, most modern neural network training methods do just that: they minimize the mean value of some error function on the training set. J.Y . Audibert proposed the star procedure method, which has optimal rate of convergence of excess squared risk [Aud07]. Motivated by this observation and the huge success of ensembles of neural networks, we propose a modiÔ¨Åcation of the star procedure that will combine the advantages of both methods. In short, the procedure we propose can be described as follows: we run dindependent learning processes of neural networks, obtaining empirical risk minimizers bg1; :::;bgd, freeze their weights, then we initialize a new model and connect all d+ 1models with a layer of convex coefÔ¨Åcients, after that we start the process of optimizing all nonfrozen parameters. This whole procedure can be viewed as a search for an empirical minimizer in all possible ddimensional simplices spanned by dminimizers and a class of neural networks. As is known, the minimization of the empirical risk with respect to the convex hull is not optimal in the same way as with respect to the original class of functions. Our method, however, minimizes over some set intermediate between the original class of functions and its convex hull, allowing us to combine the advantages of model ensembling and the star procedure. One can look at this procedure as a new way to train one large neural network with a block architecture, as well as a new way of aggregating models. In this work, we carry out a theoretical analysis of the Preprint. Under review.arXiv:2206.00255v1  [cs.LG]  1 Jun 2022behavior of the proposed algorithm for solving the regression problem with a class of sparse neural networks, and also check the operation of the algorithm in numerical experiments on classiÔ¨Åcation and regression problems. In addition to this, we take into account that it is impossible to achieve a global minimum in the class of neural networks, and we consider the situation of imprecise minimization. Themain results of our work can be formulated as follows: ‚Ä¢ A multidimensional modiÔ¨Åcation of the star procedure is proposed. ‚Ä¢We prove that the resulting estimate satisÔ¨Åes the exact oracle inequality. It follows from this estimate that the order of convergence of the algorithm (in terms of sample size n) for a Ô¨Åxed neural network architecture is optimal. Our results improve over the imprecise oracle inequality in [Sch20]. ‚Ä¢We give an upper bound on the generalization error for the case of approximate empirical risk minimizers, which implies the stability of our algorithm against minimization errors. ‚Ä¢Based on our algorithm, we propose a new method for training block architecture neural networks, which is quite universal in terms of procedures. We also propose a new way to solve the aggregation problem. ‚Ä¢We illustrate the efÔ¨Åciency of our approach with numerical experiments on realworld datasets. The rest of this paper is organized as follows. In Section 2, we make an overview of neural network ensembling methods and brieÔ¨Çy discuss the advantages of the star algorithm. In Section 3, following the SchmidtHieber notation [Sch20], we deÔ¨Åne a class of sparse fully connected neural networks and formulate a number of statements from which it follows that the algorithm we proposed has a fast rate of convergence. All proofs are attached in additional materials. In Section 4, we discuss the implementation of our algorithm, point out a number of possible problems, and suggest several modiÔ¨Åcations to Ô¨Åx them. It also describes the conditions for conducting numerical experiments and presents some of their results. At the end, we offer two possible views on our procedure: a new way to train block neural networks and a fairly Ô¨Çexible model aggregation procedure. 2 Related work "
371,Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification.txt,"This paper considers the problem of unsupervised person re-identification
(re-ID), which aims to learn discriminative models with unlabeled data. One
popular method is to obtain pseudo-label by clustering and use them to optimize
the model. Although this kind of approach has shown promising accuracy, it is
hampered by 1) noisy labels produced by clustering and 2) feature variations
caused by camera shift. The former will lead to incorrect optimization and thus
hinders the model accuracy. The latter will result in assigning the intra-class
samples of different cameras to different pseudo-label, making the model
sensitive to camera variations. In this paper, we propose a unified framework
to solve both problems. Concretely, we propose a Dynamic and Symmetric
Cross-Entropy loss (DSCE) to deal with noisy samples and a camera-aware
meta-learning algorithm (MetaCam) to adapt camera shift. DSCE can alleviate the
negative effects of noisy samples and accommodate the change of clusters after
each clustering step. MetaCam simulates cross-camera constraint by splitting
the training data into meta-train and meta-test based on camera IDs. With the
interacted gradient from meta-train and meta-test, the model is enforced to
learn camera-invariant features. Extensive experiments on three re-ID
benchmarks show the effectiveness and the complementary of the proposed DSCE
and MetaCam. Our method outperforms the state-of-the-art methods on both fully
unsupervised re-ID and unsupervised domain adaptive re-ID.","Person reidentiÔ¨Åcation (reID) attempts to Ô¨Ånd matched pedestrians of a query in a nonoverlapping camera sys *Equal contribution: yangfx@stu.xmu.edu.cn ‚Ä†Corresponding author: fzhiming.luo, szligg@xmu.edu.cn (a)InitialState(c)w/MetaCam(b)w/oMetaCam(1)(2)(3)(4)Figure 1. Illustration of camera variations in person reID (a) and the comparison between methods trained without or with the pro posed MetaCam ((b) and (c), respectively). Different colors rep resent different identities and different shapes indicate different camera IDs. At the initial state, samples under different cameras may suffer from appearance changes of viewpoints ((1) & (2)), il lumination ((3) & (4)), and other factors. Without considering this factor, the trained model may be sensitive to camera variations and may wrongly split intraclass features to different centers. Our proposed MetaCam enables the model to learn camerainvariant features by explicitly considering crosscamera constraint. tem. Recent CNNbased works [31, 35] have achieved impressive accuracies, but their success is largely depen dent on sufÔ¨Åcient annotated data that require a lot of label ing cost. In contrast, it is relatively easy to obtain a large collection of unlabeled person images, fostering the study of unsupervised reID. Commonly, unsupervised reID can be divided into two categories depending on whether us ing an extra labeled data, i.e., unsupervised domain adap tation (UDA) [37, 49, 7] and fully unsupervised reID (FU) [19, 20, 42]. In UDA, we are given a labeled source domain and an unlabeled target domain. The data of two domains have different distributions and are used to train a model that generalizes well on the target domain. The fully unsupervised reID is more challenging since only un labeled images are provided to train a deep model. In this study, we will mainly focus on this setting, and call it as unsupervised reID for simplicity. 1arXiv:2103.04618v1  [cs.CV]  8 Mar 2021Recent popular unsupervised reID methods [19, 34, 20, 42] mainly adopt clustering to generate pseudolabel for un labeled samples, enabling the training of the model in a su pervised manner. Pseudolabel generation and model train ing are applied iteratively to train an accurate deep model. Despite their effectiveness, existing methods often ignore two important factors during this process. (1) Noisy labels brought by clustering . The clustering algorithm cannot en sure intrasamples to be assigned with the same identity, which inevitably will introduce noisy labels in the labeling step. The errors of noisy labels will be accumulated during training, thereby hindering the model accuracy. (2) Feature variations caused by camera shifts . As shown in Fig. 1, intraclass samples under different cameras may suffer from the changes of viewpoint ( e.g., (1) and (2) in Fig. 1), illu mination ( e.g., (3) and (4) in Fig. 1), and other environmen tal factors. At the start of unsupervised learning (‚Äúinitial state‚Äù in Fig. 1), these signiÔ¨Åcant variations will cause large gaps between the intraclass features of different cameras. In such a situation, it is difÔ¨Åcult for the clustering algorithm to cluster samples with the same identity from all cameras into the same cluster. Consequently, training with the sam ples mined by the clustering will lead to unexpected sep aration for intraclass samples (‚Äúw/o MetaCam‚Äù in Fig. 1) and the model might be sensitive to camera variations dur ing testing. In this paper, we attempt to solve the above two crucial problems for robust unsupervised reID. For the Ô¨Årst issue , we try to adopt the technique of learn ing with noisy labels (LNL) for robust training. LNL is wellstudied in image classiÔ¨Åcation, however, most of the existing methods cannot be directly applied to our sce nario. This is because the centers and pseudolabel will change after each clustering step. To overcome this difÔ¨Å culty, this paper proposes a dynamic and symmetric cross entropy loss (DSCE) for unsupervised reID. We maintain a feature memory to store all image features, which enables us to dynamically build new class centers and thus to be adaptable to the change of clusters. With the dynamic cen ters, a robust loss function is proposed for mitigating the negative effects caused by noisy samples. For the second issue , we attempt to explicitly consider camerainvariant constraint during training. Indeed, person reID is a crosscamera retrieval process, aiming to learn a model that can well discriminate samples under different cameras. If a model trained with samples from some of the cameras can also generalize to distinguish samples from the rest of the cameras, then, we could obtain a model that can extract the intrinsic feature without cameraspeciÔ¨Åc bias and is robust to camera changes. Inspired by this, this paper in troduces a cameraaware metalearning (MetaCam), which aims to learn camerainvariant representations by simulat ing the crosscamera reidentiÔ¨Åcation process during train ing. SpeciÔ¨Åcally, MetaCam separates the training data intometatrain and metatest, ensuring that they belong to en tirely different cameras. We then enforce the model to learn camerainvariant features under both camera settings by up dating the model with metatrain and validating the updated model with metatest. Along with learning from different meta divisions, the model is gradually optimized to gener alize well under all cameras. As shown in Fig. 1, Meta Cam gathers intraclass features of different cameras into the same cluster, which is beneÔ¨Åcial for mining pseudo label and learning camerainvariant features. In summary, our main contributions can be summarized in three aspects: ‚Ä¢ We propose a dynamic and symmetric loss (DSCE), which enables the model to be robust to noisy labels during training in the context of changes of clusters and thus promotes the model performance. ‚Ä¢ We propose a cameraaware metalearning algorithm (MetaCam) for adapting the shifts caused by cameras. By simulating the crosscamera searching process dur ing training, MetaCam can effectively improve the ro bustness of the model to camera variations. ‚Ä¢ We introduce a uniÔ¨Åed framework that can jointly take advantage of the proposed DSCE and MetaCam, en abling us to learn a more robust reID model. Extensive experiments on three largescale datasets demonstrate the advantages of our DSCE and MetaCam for the fully unsupervised reID. Besides, further experiments on the UDA setting show that our method can also achieve state of the art. 2. Related Work "
372,Anomaly Detection in Retinal Images using Multi-Scale Deep Feature Sparse Coding.txt,"Convolutional Neural Network models have successfully detected retinal
illness from optical coherence tomography (OCT) and fundus images. These CNN
models frequently rely on vast amounts of labeled data for training, difficult
to obtain, especially for rare diseases. Furthermore, a deep learning system
trained on a data set with only one or a few diseases cannot detect other
diseases, limiting the system's practical use in disease identification. We
have introduced an unsupervised approach for detecting anomalies in retinal
images to overcome this issue. We have proposed a simple, memory efficient,
easy to train method which followed a multi-step training technique that
incorporated autoencoder training and Multi-Scale Deep Feature Sparse Coding
(MDFSC), an extended version of normal sparse coding, to accommodate diverse
types of retinal datasets. We achieve relative AUC score improvement of 7.8\%,
6.7\% and 12.1\% over state-of-the-art SPADE on Eye-Q, IDRiD and OCTID datasets
respectively.","Anomaly detection is a widely discussed topic in the Ma chine learning community. Ocular illnesses such as diabetic retinopathy (DR), agerelated macular degeneration (AMD), Macular Hole, Central Serous Retinopathy, and glaucoma im pact more than 270 million people globally. Anomaly detec tion in retinal data is a signiÔ¨Åcant problem that is useful in identifying any abnormalities in the patients [1]. Supervised classiÔ¨Åcation algorithms [2, 3, 4] are often used to classify normal and anomalous data. However, training supervised classiÔ¨Åers require a good amount of annotated data which is often hard to obtain in the Ô¨Åeld of retinal imaging. Even when annotated data is available, the classiÔ¨Åers might suffer from the class imbalance as the prevalence of normal samples is frequently higher than that of abnormal samples. Also, some lesions are uncommon, and the presence of speciÔ¨Åc lesions is unknown until the diagnosis is made. Moreover, the labels collected from different clinical experts may be different, andhence the annotation process can produce noisy or biased la bels [5]. In this context, anomaly detection in an unsupervised manner can be helpful. In our proposed method, we initially train an autoencoder to reconstruct image patches from normal images to learn domainspeciÔ¨Åc Ô¨Ånegrained features. This step will help learn more relevant micro and macrolevel features unique to normal samples in local and global contexts. Once the au toencoder is trained, we utilize multiscale features extracted from the encoder‚Äôs multiple layers from various depths as an input vector for sparse coding. With multiscale deep fea tures, it will capture more global context from the image to detect anomalies with different scales or sizes. Our method has outperformed stateoftheart unsupervised algorithms on three different retinal datasets. Our main contributions in this paper are It is simple, mem ory efÔ¨Åcient, and easy to train, unlike other methods, We have proposed a multistep training strategy that combines autoen coder training and MultiScale Deep Feature Sparse Coding (MDFSC) for anomaly detection to adopt a different type of datasets, We extended MultiScale Deep Feature Sparse Cod ing (MDFSC) from sparse coding for various types of retinal datasets. 2. RELATED WORK "
373,ForamViT-GAN: Exploring New Paradigms in Deep Learning for Micropaleontological Image Analysis.txt,"Micropaleontology in geosciences focuses on studying the evolution of
microfossils (e.g., foraminifera) through geological records to reconstruct
past environmental and climatic conditions. This field heavily relies on visual
recognition of microfossil features, making it suitable for computer vision
technology, specifically deep convolutional neural networks (CNNs), to automate
and optimize microfossil identification and classification. However, the
application of deep learning in micropaleontology is hindered by limited
availability of high-quality, high-resolution labeled fossil images and the
significant manual labeling effort required by experts. To address these
challenges, we propose a novel deep learning workflow combining hierarchical
vision transformers with style-based generative adversarial network algorithms
to efficiently acquire and synthetically generate realistic high-resolution
labeled datasets of micropaleontology in large volumes. Our study shows that
this workflow can generate high-resolution images with a high signal-to-noise
ratio (39.1 dB) and realistic synthetic images with a Frechet inception
distance similarity score of 14.88. Additionally, our workflow provides a large
volume of self-labeled datasets for model benchmarking and various downstream
visual tasks, including fossil classification and segmentation. For the first
time, we performed few-shot semantic segmentation of different foraminifera
chambers on both generated and synthetic images with high accuracy. This novel
meta-learning approach is only possible with the availability of
high-resolution, high-volume labeled datasets. Our deep learning-based workflow
shows promise in advancing and optimizing micropaleontological research and
other visual-dependent geological analyses.","  Throughout geological history,  foraminifera represent  one of the most exceptionally diverse  groups of marine microfossils, with an estimated number of current species between 8,966  and an estimated number of 40,888 fossil species in the geological record1.  This accounts  for approximately 2% of all animal species from the Cambrian to the present.  (Sen, 2003) .  The size of  fossil foraminifera is very diverse  ranging from  less than  100 microns to 20  centimeters and their shell can be made up of diverse compositions, such as calcite, aragonite ,  agglutinated particles , and other organic compounds.  In foraminifera , factors such as their  cosmopolitan nature and evolutionary diversification make them of particular interest to  provide a paleontological and stratigraphic record, whi ch is of significant  value to carry out                                                              1 https://www.marinespecies.org/foraminifera/  2   biostratigraphic correlations and paleoenvironmental interpretations (Jones, 2014; Sen,  2003) .     In both ancient and modern environments , the relative  abundance of specific species and their  corresponding  morphometric characteristics are used as a proxy for (paleo) temperature,  (paleo) oxygen concentration , and (paleo) oceanic salinity  and paleoproductivity . In addition,  foraminifera are often used as the building block to define biofacies for paleo bathym etric  studies, as an aid in the characterization of sedimentary subenvironments. With the  progressive  change of different macro  and microfossils throughout the history of the Earth,  especially planktonic foram inifera  being  utilized as markers on the geological time scale and  the occurrence of specific events  in the stratigraphic record   (Jones, 2014; Marchant et al.,  2020; Sen, 2003) .    Detailed  identification  of both species and morphotypes and  producing high quality  photomicrographs  of foraminifera have been primarily dependent  on the  availability of high  end equipment  such as advanced stereomicroscopes and high resolution scanning systems.  Although some techniques have become staple in researc h institutes, some high end  equipment for digitizing specimens at high resolution s not widely accessible to  the  geoscientific community . This is exacerbated by the expertise needed to perform species and  genera classification. All of this leads to an issu e with standardization across various  laboratories and institutions, which severely limits the reproducibility of such classification  and its accessibility to non experts. As a result, there is an urgent need to develop an efficient,  automated approach or workflow for  improving the resolution of microfossil images and  obtaining labeled datasets without the need for high end equipment. Furthermore,  the widespread implementation of robust deep learning models for foraminifera classification  and morphological diversity distillation using advanced computer vision technology,  particularly deep convolutional neural networks, has yet to be  fully  investigated.      In the current literature , there are numerous works that leverage  computer vision  technology  to classify and segment microfossil specimens  (Beaufort & Dollfus, 2004; Marchant et al.,  2020) . From seminal implementations in which the algorithms used did not achieve  human  accuracy (Beaufort & Dollfus, 2004; Culverhouse et al., 1996; S. Liu et al., 1994)  to recent  works in which the algorithms exceed human accuracy and speed when classifying  microfossils (Marchant et al., 2020; Pedraza et al., 2017) . Recently, the use of Deep  Convolutional Neural Networks (CNNs) has  been notable in this research corpus , with CNNs  having  several advantages for these tasks  given th e reduced need for feature engineering, the  scalability to larger datasets, and exceptional  ability to  process grid like data (Carvalho et al.,  2020; Ho et al., 2023; Johansen et al., 2021; Koeshidayatullah et al., 2020; Marchant et al.,  2020; Mitra et al., 2019) . This is further supported by the accessibility  to powerful pretrained  architectures as backbones for a starting point during the training of new models ( Ho et al.,  2023; Koeshidayatullah et al., 2020; Koeshidayatullah et al., 2022; Pires de Lima & Duarte,  2021; Shoji et al., 2018) .     In geosciences, r ecent advances  of Generative Adversarial Networks (GANs) enable   geoscientists to generate additional synthetic data as an additional augmentation technique to 3   conventional ones, assisting in the improvement  of machine learning mode l performance   (Ferreira et al., 2022; Wang & Perez, 2017) . In addition, GANs  are effec tive at balancing  the  distribution of data within a particular  geological dataset, ensuring a better representation,  and reducing the risk of bias  during training (Ferreira et al., 2022; Koeshidayatullah, 2022;  Abdellatif et al., 2022) .    In recent research trends, Vision Transformers (ViT) have emerged from the Natural  Language Processing (NLP) corpus as a powerful technique for tackling visual tasks  (Vaswani et al 2017; Dosovitskiy  et al, 2020) . Unlike traditional CNNs, ViTs employ a  transformer ar chitecture to address both global and local relationships in an image, resulting  in more effective feature extraction and representation. Image Super Resolution (SR) is a  field of Computer Vision that focuses on enhancing low resolution images, making them   more visually appealing and informative. Recent improvements in CNN and ViT based  architectures have become the state oftheart for upscaling and restoring images  (Liang et  al, 2021; Lin et al, 2022) . This application could prove useful in preserving fin egrained  details and textural information for micropaleontological image analysis.     The primary goals of this research were to create and suggest new approaches to traditional  microfossil image scaling methods, and to investigate the application of end toend deep  learning for enhancing the quality and accurately representing the morphological diversity of  foraminifera images. Ferreira et al. (2022) successfully showcased this in their study using  petrographic datasets (Fig. 1a and b). Such tools have the potential to expand the variety of  micropaleontological datasets and provide synthetic digital counterparts for confidential data.     Figure 1 : Examples of the use of deep learning architectures in petrography (a) super  resolution imaging of sandstones (Y. Liu et al., 2020)  (b) generation of thin sections (Ferreira   et al.,  2022) .      2. Methodology   "
374,SyReNets: Symbolic Residual Neural Networks.txt,"Despite successful seminal works on passive systems in the literature,
learning free-form physical laws for controlled dynamical systems given
experimental data is still an open problem. For decades, symbolic mathematical
equations and system identification were the golden standards. Unfortunately, a
set of assumptions about the properties of the underlying system is required,
which makes the model very rigid and unable to adapt to unforeseen changes in
the physical system. Neural networks, on the other hand, are known universal
function approximators but are prone to over-fit, limited accuracy, and bias
problems, which makes them alone unreliable candidates for such tasks. In this
paper, we propose SyReNets, an approach that leverages neural networks for
learning symbolic relations to accurately describe dynamic physical systems
from data. It explores a sequence of symbolic layers that build, in a residual
manner, mathematical relations that describes a given desired output from input
variables. We apply it to learn the symbolic equation that describes the
Lagrangian of a given physical system. We do this by only observing random
samples of position, velocity, and acceleration as input and torque as output.
Therefore, using the Lagrangian as a latent representation from which we derive
torque using the Euler-Lagrange equations. The approach is evaluated using a
simulated controlled double pendulum and compared with neural networks, genetic
programming, and traditional system identification. The results demonstrate
that, compared to neural networks and genetic programming, SyReNets converges
to representations that are more accurate and precise throughout the state
space. Despite having slower convergence than traditional system
identification, similar to neural networks, the approach remains flexible
enough to adapt to an unforeseen change in the physical system structure.","Neural networks have successfully been applied in a complex range of hard to solve problems, e.g. convolutional neural networks [ 1], generative adversarial networks [ 2] and transformers [ 3], respectively changed their subÔ¨Åelds of research. Yet, there exist areas that are still barely inÔ¨Çuenced by neural architectures. In this paper, we are particularly interested in modeling dynamics of physical systems. The importance of having a reliable dynamic model is clear for feedforward motion generation, for motion planning, and when a swift dynamic response is required. It also allows for more accurate simulations of the underlying system, which in turn permits longer horizon of motion predictions. Additionally, the controller would be able to work with smaller gains to achieve a given state, which leads to less stiff actuators that are safer to interact with [4, 5]. For many decades, the standard approach for modeling the dynamics of a given physical system is done by measuring or estimating the actuation positions, velocities, accelerations and Ô¨Ånding relations Preprint. Under review.arXiv:2105.14396v1  [cs.LG]  30 May 2021between the commanded inputs (torque for example). Those relations would then be formulated by a set of mathematical equations that describes the motion of the system [ 6]. As an example, Equation 1 represents the inverse dynamics of a serialchain rigidbody robotic system: =M(q)q+C(q;_q)_q+G(q) (1) It describes the relation between torque ( ) and position, velocity, and acceleration ( q;_q;q) at jointlevel.M,CandGare preformulated matrix functions that mathematically describe inertial forces, centrifugal and Coriolis forces, and gravitational forces, respectively, in terms of mass, center of mass, and inertial matrix of each rigidbody. Traditional system identiÔ¨Åcation methods aim to approximate them and, therefore, is still used due to its mathematical stability in the whole observable state space of the agent. The disadvantage is that the equations have Ô¨Åxed terms and even if some of the numerical parameters can be changed over time it limits the physical system to a given set of assumptions (e.g. shape and actuation type), making it complex to optimize for systems made out of hard to model materials (e.g. rubber) or that might be subject to unforeseen changes in structure for different applications. Neural networks are generally known to be universal function approximators [ 7] but they are prone to overÔ¨Åt to the training data and have biased decisions when trained with an imbalanced or incomplete dataset [ 8‚Äì10]. Therefore, they, alone, are not reliable enough to be a surrogate model. However, for most dynamical systems belonging to our speciÔ¨Åc use case, there exists an analytical exact mathematical solution that can accurately describe the dynamics. For those, a different class of algorithms can be applied. For a lack of better term, we denote them as universal exact function estimators since instead of approximating a function they seek to estimate the exact underlying one that solves the problem. The most well known representative of this class is genetic programming [11], which is an evolutionary strategy for evolving symbolic tree representations. Each tree encodes a symbolic function. There, several generations of candidate symbolic solutions are tested and combined until a convergence criterion is reached. Despite suffering the same problems as neural networks, since those methods are mostly not dependent on gradient, if an underlying function exists it can eventually be found due to the random nature of the evolutionary process, given enough time and reinitializations. However, the search is highly inefÔ¨Åcient since the process is fundamentally dependent on randomness, leading to repetitive and ""unnecessary"" evaluations of candidate solutions. Therefore, those approaches tend to not scale well with the increase of dimensionality of the problem [12]. The truth is that exact symbolic estimation is inherently an NPhard problem, which explains why dynamic model learning is still an open problem to solve. We argue that a hybrid approach is a reasonable way to obtain the best of both worlds, namely the expressiveness of neural network architectures and the stability potential of symbolic representations. This would allow a general symbolic learning architecture to be applicable to any physical system that can be expressed analytically, potentially mitigating the disadvantages of ""approximation"" algorithms and the search inefÔ¨Åciency of the exact estimation methods. In this paper, we present Symbolic Residual Neural Networks ( SyReNets ), a neural network archi tecture capable of learning symbolic mathematical relations from streams of data. SyReNets can potentially be applied to estimate a vast number of functions, describing dynamics of many different physical systems. Traditionally, most methods try to learn the inverse dynamics (as denoted by Equation 1), since it conveys the relation between position, velocity and acceleration to applied torques [ 13]. However, each actuator adds one function to the number of equations to learn. Aiming at learning a Ô¨Åx and minimal number of equations we chose to learn the Lagrangian representation of the system: L=T V (2) Where,Lrepresents the Lagrangian, Tis the kinetic energy and Vis the potential energy of the system. By construction, it is always in R, making this a hypercompressive representation that maps from Rn!Rforninputs. From the Lagrangian equation, it is possible to derive the inverse dynamics of the system using the EulerLagrange method: i=d dt@L @_qi @L @qi(3) This operations transform the Lagrangian to mpossible actuator torques, effectively constructing a map from R!Rm. Therefore, the Lagrangian can be considered a highly compact latent representation to a set of noninjective functions. The paper is structured as follows: In Section 2 we will discuss related works that take advantage of energy representations to approximate or estimate physical systems, highlighting what they might be lacking to be of general use. In Section 3, we present our method in detail, followed by Section 4 where we evaluate the performance of the approach on a simulated double pendulum system against 2vanilla neural networks, genetic programming and traditional system identiÔ¨Åcation. Finally, in Section 5 we present our conclusions from the experiments, possible future works and potential societal impacts of our method. 2 Related Works "
375,Identifying Label Errors in Object Detection Datasets by Loss Inspection.txt,"Labeling datasets for supervised object detection is a dull and
time-consuming task. Errors can be easily introduced during annotation and
overlooked during review, yielding inaccurate benchmarks and performance
degradation of deep neural networks trained on noisy labels. In this work, we
for the first time introduce a benchmark for label error detection methods on
object detection datasets as well as a label error detection method and a
number of baselines. We simulate four different types of randomly introduced
label errors on train and test sets of well-labeled object detection datasets.
For our label error detection method we assume a two-stage object detector to
be given and consider the sum of both stages' classification and regression
losses. The losses are computed with respect to the predictions and the noisy
labels including simulated label errors, aiming at detecting the latter. We
compare our method to three baselines: a naive one without deep learning, the
object detector's score and the entropy of the classification softmax
distribution. We outperform all baselines and demonstrate that among the
considered methods, ours is the only one that detects label errors of all four
types efficiently. Furthermore, we detect real label errors a) on commonly used
test datasets in object detection and b) on a proprietary dataset. In both
cases we achieve low false positives rates, i.e., when considering 200
proposals from our method, we detect label errors with a precision for a) of up
to 71.5% and for b) with 97%.","Nowadays, the predominant paradigm in computer vision is to learn models from data. The performance of the model largely depends on the amount of data and its quality, i.e., the diversity of input images and label accuracy (Feng et al. [2020]; Hussain and Zeadally [2018]; Jaeger et al. [2020]; Kaur et al. [2021]; Kuutti et al. [2020]). Deep neural net works (DNNs) are particularly data hungry Sun et al. [2017]. In this work, we focus on the case of object detection where multiple objects per scene belonging to a Ô¨Åxed set of classes Figure 1: Example image from the Pascal VOC 2007 test dataset with two labeled boats marked by the blue boxes and multiple unla beled boats. are annotated via bounding boxes (Everingham et al. [2010]; Riedlinger et al. [2022a]). In many industrial and scientiÔ¨Åc applications, the labeling process consists of an iterative cycle of data acquisition, la beling, quality assessment, and model training. Labeling data is costly, time consuming and error prone, e.g. due to incon sistencies caused by multiple human labelers or a change in label policy over time. Therefore, at least a partial automa tion of the label process is desirable. Research directions that aim at this goal are active learning (Settles [2009]; Brust et al.[2018]; Roy et al. [2018]; Elezi et al. [2021]; Desai et al. [2019]) and automated label error detection (Northcutt et al. [2021a]; Rottmann and Reese [2022]; Dickinson and Meurers [2003]). Active learning alternates between data labeling and model training, where the latter is utilized for selecting new images for labeling such that the model‚Äôs accuracy increases as quickly as possible. These methods often assume that the labels are obtained by an errorfree oracle, which typically does not hold in realworld applications. The extent to which noisy labels affect the model performance is studied by Wu et al.[2018], resulting in the observation that the model is able to tolerate a certain amount of noise in training data with out losing too much performance on test sets. Other methods model label uncertainty (Riedlinger et al. [2022b]; Miller et al.[2018]) or improve robustness w.r.t. noisy labels (Li et al. [2020]; Feng et al. [2021]; Zhang and Wang [2019]). Up to now, in contrast to active learning, automated de tection of label errors has received less attention. There ex ist some works on image classiÔ¨Åcation datasets (Northcutt et al. [2021b,a]; Thyagarajan et al. [2022]) as well as onearXiv:2303.06999v1  [cs.CV]  13 Mar 2023work on semantic segmentation datasets by Rottmann and Reese [2022]. Label errors may affect generalization perfor mance, which makes their detection desirable (Northcutt et al.[2021b]). Furthermore, there is business value interest in improving or accelerating the review process by partial au tomation. Here, we for the Ô¨Årst time study the task of label error de tection in object detection datasets by a) introducing a bench mark and b) developing a detection method and compare it against three canonical baselines. We introduce a benchmark by simulating label errors on the BDD100k (Yu et al. [2020]) and EMNISTDet (Riedlinger et al. [2022a]) dataset. The lat ter is a semisynthetic dataset consisting of EMNIST letters (Cohen et al. [2017]) pasted into COCO images (Lin et al. [2014]) of which we expect to possess highly accurate labels. The types of label errors that we consider are missing labels (drops ), correct localization but wrong classiÔ¨Åcation ( Ô¨Çips), correct classiÔ¨Åcation but inaccurate localization ( shifts ), and labels that actually represent background ( spawns ). We ad dress the detection of these errors by a novel method based on monitoring instancewise object detection loss. We study the effectiveness of our method in comparison to three base lines. Then, we demonstrate for commonly used object detec tion test datasets, such as BDD100k (Yu et al. [2020]), MS COCO (Lin et al. [2014]), Pascal VOC (Everingham et al. [2010]) and Kitti (Geiger et al. [2012]), and also for a pro prietary dataset on car part detection that our method detects label errors by reviewing moderate sample sizes of 200 im ages per dataset. Our contributions can be summarized as follows: ‚Ä¢ We introduce a novel method based on the instancewise loss for detecting label errors in object detection. ‚Ä¢ We introduce a benchmark for identifying four types of label errors on BDD100k and EMNISTDet. ‚Ä¢ We apply our method to detect label errors in commonlyused and proprietary object detection datasets and manually evaluate the error detection performance for moderate sample sizes. To contribute to future development of label error detection methods and potentially cleaning up object detection datasets, we provide an implementation of our benchmark, method and baselines as well as label Ô¨Åles that include simulated label errors and model checkpoints that allow to reproduce of our results, see GitHub . The remainder of this work is structured as follows: sec tion 2 contains a summary of the literature in label error de tection in image classiÔ¨Åcation and semantic segmentation as well as modeling label uncertainty and robust training in ob ject detection. Thereafter, it is summarized how the presented work relates to the literature. In section 3 we introduce the simulation of label errors for benchmarking, followed by im plemented baselines for comparison, our method, the used evaluation metrics and the detection of real label errors. Sec tion 4 Ô¨Årst introduces our experimental setup. Then, we show results on our label error benchmark in section 4.2 and after wards, in section 4.3, we detect real label errors on commonly used object detection test datasets. Finally, we close with a summary of conclusions in section 5.2 Related Work "
376,FMT:Fusing Multi-task Convolutional Neural Network for Person Search.txt,"Person search is to detect all persons and identify the query persons from
detected persons in the image without proposals and bounding boxes, which is
different from person re-identification. In this paper, we propose a fusing
multi-task convolutional neural network(FMT-CNN) to tackle the correlation and
heterogeneity of detection and re-identification with a single convolutional
neural network. We focus on how the interplay of person detection and person
re-identification affects the overall performance. We employ person labels in
region proposal network to produce features for person re-identification and
person detection network, which can improve the accuracy of detection and
re-identification simultaneously. We also use a multiple loss to train our
re-identification network. Experiment results on CUHK-SYSU Person Search
dataset show that the performance of our proposed method is superior to
state-of-the-art approaches in both mAP and top-1.","Person search combines person detection and person reidentiÔ¨Åc ation, which detects all candidate persons in an image and then compares all pos sible pairs of the query persons to identify the target persons, which is diÔ¨Äer ent from per son reidentiÔ¨Åcation. It is a challenging and fastgrowing Ô¨Åeld. It ha s many im portant applications in video surveillance and multimedia, such as pede strian retrieval[1] and crosscamera visual tracking[2]. The recent work [3] proposed an endtoend person search model based on a single convolutiona l neural net work,which adoptsproposedOnline Instance Matching(OIM) lossf unction to 1School of Mathematical Sciences, Anhui University, Hefei 2 30601, China 2School of Computer Science and Technology, Anhui Universit y, Hefei 230601, China Email: sulanzhai@gmail.com21Sulan Zhai et al. Fig. 1: Process of person search. For each query person, we det ect possible query persons in the gallery images, and then compares all possible p airs of the query to identify the target person. train reidentiÔ¨Åcation networks, and built a largescale benchmark dataset for person search (CUHKSYSU). Person search is generally based on twostage search strategy. Firstly, we detect all candidate persons in an ga llery images. Secondly, we reidentify the query persons from the candiate per sons by mak ing a comparison between the query person and all the candidates. See Fig. 1 for demonstration. In general, a single task needs a single convolutional neural networ k to extract the speciÔ¨Åc features. Multitask learning,where a single ne twork is trained to tackle several related tasks, may learn more robust pr esentation and reduce complexity and training time. it has been successfully use d to perform the tasks, such as person reidentiÔ¨Åcation, face alignme nt. The recent work[3] introduced the multitask learning into person search by co mbining the person detection and person reidentiÔ¨Åcation. However, it on ly considers reidentiÔ¨Åcation as expansion of detection task. Detection and re identiÔ¨Åcation are not only interrelated, but also heterogeneous. They are two d iÔ¨Äerent task. Soweshouldconsidertheheterogeneityofdetectionandreident iÔ¨Åcationwhen designing the multitask CNN for person search. In this paper, we propose a fusing multitask convolutional neural network (FMTCNN) to tackle heterogeneity of detection and reidentiÔ¨Åca tion and fo cus on how the interplay of pedestrian detection and person reide ntiÔ¨Åcation aÔ¨Äects the overall performance. Person label is one kind of import ant informa tion that identiÔ¨Åes person which will help to improve the accuracy of p erson reidentiÔ¨Åcation. In a single convolutional neural network, we add person la bels into the RPN and produce more suitable features for reidentiÔ¨Å cation task while keeping the accuracy of detection unchanged. To improv e the per formance of reidentiÔ¨Åcation network, we use the multiple loss and t he more suitablefeaturestotrainreidentiÔ¨Åcationnetwork.Finally,ourpr oposedFMT CNN achieves 77.15% mAP, 79.83% top1 accuracy and 90.90% top5 a ccu racy on CUHKSYSU[3], which shows that FMTCNN can outperform s tate ofthearts in both mAP and top1 evaluation protocols. Our work s can be summarized as the following three aspects.(1) We present an eÔ¨Écien t fusing multitask convolution neural network(FMTCNN) for person sea rch. (2) We add person labels into region proposal network to tackle heteroge neity of deFMT:Fusing Multitask Convolutional Neural Network for Pe rson Search 3 tection and reidentiÔ¨Åcation. (3) We adopt the multiple loss to bette r train reidentiÔ¨Åcation network. 2 Related Works "
377,Contrastive Credibility Propagation for Reliable Semi-Supervised Learning.txt,"Inferencing unlabeled data from labeled data is an error-prone process.
Conventional neural network training is highly sensitive to supervision errors.
These two realities make semi-supervised learning (SSL) troublesome. In
practice, SSL approaches often fail to outperform their fully supervised
baseline. Proposed is a novel framework for deep SSL via transductive
pseudo-label refinement called Contrastive Credibility Propagation (CCP).
Through an iterative process of refining soft pseudo-labels, CCP unifies a
novel contrastive approach for generating pseudo-labels and a powerful
technique to overcome instance-dependent label noise. The result is an SSL
classification framework explicitly designed to overcome inevitable
pseudo-label errors. Using standard text and image benchmark classification
datasets, we show CCP reliably boosts or matches performance over a supervised
baseline in four common real-world SSL scenarios: few-label, open-set,
noisy-label, and class distribution misalignment.","Leveraging massive datasets that are only partially labeled is critical for countless applications (Wang et al., 2020; Zhou et al., 2022; Ding et al., 2022; de Carvalho et al., 2022; Gao et al., 2022; Liu et al., 2022; Lugo et al., 2022). Generating and utilizing pseudolabels for unlabeled data is a powerful but errorprone approach. Common issues often challenge the resulting classiÔ¨Åer‚Äôs ability to reliably perform better than a fully supervised baseline (Oliver et al., 2018). Some of these issues, illustrated in Fig. 1, and how our work addresses them, are described below. A) Error propagation: An incorrect pseudolabel can cause a cascade of future errors. In the original label propa 1AI Research, Palo Alto Networks, Santa Clara, CA. Corre spondence to: Brody Kutt <bkutt@paloaltonetworks.com >. A B CD E F?Figure 1: Six common issues of pseudolabeling algorithms. The color of the interior of each circle indicates the predicted class. The outline of each circle indicates the true class and black outlines indicate a labeled sample. Black arrows indicate label propagation inÔ¨Çuence. In C, repulsive (dark red) and attractive (green) arrows indicate gradient inÔ¨Çuence. InE, the background color indicates a decision surface. gation algorithm (LPA) (Zhu & Ghahramani, 2002), upon which several more recent techniques are based (Malhotra & Chug, 2021; Xie et al., 2011; Li et al., 2021a; Chin & Ratnavelu, 2017; Li et al., 2020; Dong et al., 2020), label information iteratively diffuses across unlabeled samples whether the initial propagation is correct or not. Our ap proach eliminates error propagation at the batch level by focusing exclusively on Ô¨Årstorder, i.e. singlehop, simi larities. Highorder propagation is made possible through multiple iterations of Algorithm 1. B) Ambiguity: The true class of unlabeled samples is sometimes unknowable e.g. if it is highly similar to two dif ferent classes. In a similar fashion to (Dong et al., 2020), our approach uses what we call credibility vectors (see Sec. 3.1), as opposed to traditional label vectors, to help reÔ¨Çect low prediction weight in this scenario. The output credibility vector for such a sample, qi, will feature nearzero scores for the equally similar classes and <0scores for every other class. qivalues of0induce no effect throughout the framework and nearzero values induce a minimal effect. In general, credibility adjustments help softly differentiate uncontested estimated similarities by giving them higher weight.arXiv:2211.09929v2  [cs.LG]  27 Jan 2023Contrastive Credibility Propagation for Reliable SemiSupervised Learning C) Inappropriate similarity metrics: Learned similarity metrics for SSL often suffer from conÔ¨Årmation bias (Arazo et al., 2020). This occurs when a network is optimized to produce similarities corresponding to its label predictions. To combat conÔ¨Årmation bias, the noisy results of the prop agation mechanism (Algorithm 2) do not affect our loss during the pseudolabel assignment. A single CCP itera tion consists of many epochs while minimizing our Soft Supervised Contrastive (SSC) loss (denoted LSSC) which is a generalization of Supervised Contrastive (SupCon) loss (Khosla et al., 2020) and its selfsupervised counterpart, SimCLR loss (Chen et al., 2020a;b). Only after the itera tion has concluded do we update our soft pseudolabels by averaging all predictions across epochs. Averaged predic tions capture network oscillations and inconsistencies across batches. Unlabeled samples inÔ¨Çuence LSSCand Algorithm 2 only via their averaged pseudolabels. This process, CCP‚Äôs outermost iteration, is modeled after an algorithm devel oped to correct instancedependent label noise called SEAL (Chen et al., 2021). We Ô¨Ånd incorrect pseudolabels correct themselves across iterations in a similar fashion (Appendix C). D) Forced propagation: Pseudolabels are often forced to carry a certain total weight, usually through the use of a softmax function (Chen et al., 2022). This eliminates a prop agation mechanism‚Äôs ability to abstain from prediction even if the similarity to all classes is weak. Adjacency matrix row normalization in graphbased label propagation produces the same effect (Zhu & Ghahramani, 2002; Kamnitsas et al., 2018). In Algorithm 2, class similarities are all computed independently. Credibility adjustments do not enforce the pseudolabel to sum to any speciÔ¨Åc value. An unlabeled sample that has nearzero similarity to all classes will have a credibility vector of nearzero values. Through the use of weighted arithmetic means in LSSCand Algorithm 2, as well as a weighted classiÔ¨Åcation loss, these samples will have minimal inÔ¨Çuence on the framework. This is especially relevant in the presence of unlabeled data that belongs to no class (called openset SSL) which can cripple existing approaches (Oliver et al., 2018). The ability to abstain from prediction enables CCP to safely leverage all pseudolabels instead of simply discarding a percentage of the weakest pseudolabels as is common practice (Sohn et al., 2020; Li et al., 2021b; Zheng et al., 2022). E) ClassiÔ¨Åcation sensitivity to label errors: Deep learn ing classiÔ¨Åers with conventional crossentropy loss are sen sitive to label errors (Song et al., 2020). ClassiÔ¨Åers often directly inÔ¨Çuence pseudolabel generation while simultane ously learning from noisy pseudolabels (Yang et al., 2021; Lee et al., 2013; Zheng et al., 2022) which can cause an error cascade. Recent work has explored why contrastive representations boost robustness to label noise (Ghosh &Lan, 2021). In our approach, only softly supervised con trastive representations are used for pseudolabel generation. Further, subsampling pseudolabels between iterations and after the conclusion of CCP eliminates many erroneous or weakly predicted samples. We propose an unsupervised, principled subsampling approach in Sec. 3.3. F) Propagation sensitivity to similarity metrics: Pseudolabel accuracy is often at the mercy of your similarity metric. Similar to (Berthelot et al., 2019b), our approach averages predicted credibility vectors across transformed views of data for increased error resistance. Unlike other work, if two different predictions occur for two transformed views of a sample, even when each has high conÔ¨Ådence, the average credibilityadjusted pseudolabel will consist of all nearzero values. Unlike (Berthelot et al., 2019b), no sharpening takes place on the average vector to prevent forcing a pseudolabel with weak evidence. The evaluation of SSL algorithms often only explores the fewlabel learning scenario i.e. balanced reduction of the number of given labels. Our work expands upon this with three other common realworld SSL scenarios. These are openset (some unlabeled data belongs to no class), noisy label (some of the given labels are incorrect), and class distribution misalignment (labeled and unlabeled data have different class frequency distributions). Our contributions are as follows. To the best of our knowledge, CCP is the Ô¨Årst SSL algorithm to 1) Make use of credibility vectors as we deÔ¨Åne them to properly represent uncertainty. 2) Imbue a pseudolabeling strategy with an approach to overcome instancedependent label noise. 3) Introduce a generalizable, unsupervised strategy to choose a dynamic pseudolabel subsampling rate. 4) Demonstrate a reliable performance boost over a supervised baseline in four realworld SSL data scenarios with a single solution. 2. Related Work "
378,A Competitive Method for Dog Nose-print Re-identification.txt,"Vision-based pattern identification (such as face, fingerprint, iris etc.)
has been successfully applied in human biometrics for a long history. However,
dog nose-print authentication is a challenging problem since the lack of a
large amount of labeled data. For that, this paper presents our proposed
methods for dog nose-print authentication (Re-ID) task in CVPR 2022 pet
biometric challenge. First, considering the problem that each class only with
few samples in the training set, we propose an automatic offline data
augmentation strategy. Then, for the difference in sample styles between the
training and test datasets, we employ joint cross-entropy, triplet and
pair-wise circle losses function for network optimization. Finally, with
multiple models ensembled adopted, our methods achieve 86.67\% AUC on the test
set. Codes are available at https://github.com/muzishen/Pet-ReID-IMAG.","More and more families choose to keep some pets to ac company them in recent years. According to the GMI re port, global pet care market size surpassed 232 billion in 2020. With the rapid growth of pet economy, pet identiÔ¨Å cation is a challenging problem in many scenarios such as pet management, trading, insurance, medical treatment etc., unfortunately, there is no solution balanced accuracy, cost and usability well for this challenge up to now. In human biometrics, person/vehicle reidentiÔ¨Åcation (ReID) [4, 5, 8, 9, 11‚Äì14] methods based on deep learning have made a signiÔ¨Åcant process in recent years. Pet biomet ric challenge1is a workshop in the ECCV2020 conference. The challenge focuses on obtaining high area under curve (AUC) on a dog noseprint dataset. It is very challenging for dog noseprint reidentiÔ¨Åcation due to the adverse inÔ¨Çu 1https://www.vislab.ucr.edu/Biometrics2022/index.php Figure 1. The example of training data. Each column represents the same ID. ence of the sample class imbalance and lacking of labeled data, as shown in 1. However, we Ô¨Ånd that 1 vs 1 pet iden tity veriÔ¨Åcation by dog noseprint images is very similar to the pedestrian ReID task. The two tasks all need to train a model to extract features for each identity, and then com pare the extracted features to judge id information. Based on the pipeline of pedestrian ReID methods, we designed the framework of 1 vs 1 pet identity veriÔ¨Åcation. The rest of the paper is organized as follows. In Section 2, the proposed methods is introduced. The experimental results are presented in Section 3. And Ô¨Ånally Section 4 concludes the paper. 2. Methods "
379,Multi-label Learning Based Deep Transfer Neural Network for Facial Attribute Classification.txt,"Deep Neural Network (DNN) has recently achieved outstanding performance in a
variety of computer vision tasks, including facial attribute classification.
The great success of classifying facial attributes with DNN often relies on a
massive amount of labelled data. However, in real-world applications, labelled
data are only provided for some commonly used attributes (such as age, gender);
whereas, unlabelled data are available for other attributes (such as
attraction, hairline). To address the above problem, we propose a novel deep
transfer neural network method based on multi-label learning for facial
attribute classification, termed FMTNet, which consists of three sub-networks:
the Face detection Network (FNet), the Multi-label learning Network (MNet) and
the Transfer learning Network (TNet). Firstly, based on the Faster Region-based
Convolutional Neural Network (Faster R-CNN), FNet is fine-tuned for face
detection. Then, MNet is fine-tuned by FNet to predict multiple attributes with
labelled data, where an effective loss weight scheme is developed to explicitly
exploit the correlation between facial attributes based on attribute grouping.
Finally, based on MNet, TNet is trained by taking advantage of unsupervised
domain adaptation for unlabelled facial attribute classification. The three
sub-networks are tightly coupled to perform effective facial attribute
classification. A distinguishing characteristic of the proposed FMTNet method
is that the three sub-networks (FNet, MNet and TNet) are constructed in a
similar network structure. Extensive experimental results on challenging face
datasets demonstrate the effectiveness of our proposed method compared with
several state-of-the-art methods.","Facial attribute classiÔ¨Åcation is an important and fundamen tal research area in computer vision and pattern recognition. The task of facial attribute classiÔ¨Åcation is to predict the at tributes of a facial image, including gender, attraction, race, etc. Recently, facial attribute classiÔ¨Åcation has received increasing attention with a wide range of applications, such as face ver iÔ¨Åcation [1, 2, 3], face recognition [4, 5, 6], face retrieval [7]. However, it remains a challenging problem, because of the large facial appearance variations caused by pose, illumination and occlusion, etc. Early works on facial attribute classiÔ¨Åcation usually charac terize the facial attributes based on the histogram representation [2, 3, 8]. For example, Kumar et al. [2] propose to Ô¨Årstly ex tract the lowlevel features from di erent regions of a face, and then predict facial attributes with the Support Vector Machine (SVM) for face veriÔ¨Åcation. Cherniavsky et al. [8] develop a generative facial feature representation method based on the Haarlike features and investigate a semisupervised method to predict facial attributes with SVM. Corresponding author. Tel.: +865922580063 Email address: yanyan@xmu.edu.cn (Yan Yan)Recent research mainly focuses on using the Deep Neural Network (DNN) to predict facial attributes. Luo et al. [9] combine discriminative decision trees with the deep Sum Product Network (SPN) for facial attribute classiÔ¨Åcation. In [10, 11, 12], the authors Ô¨Årstly extract facial features using DNN and then classify facial attributes with SVM. Ehrlich et al. [13] learn the shared feature representation for facial attributes by directly operating on faces and facial landmark points. Rudd et al. [14] address the problem of imbalanced data to predict multiple facial attributes. Generally speaking, methods for facial attribute classiÔ¨Åca tion can be divided into two categories: singlelabel learn ing based methods [2, 8, 10, 11, 12] and multilabel learning based methods [13, 14]. The singlelabel learning based meth ods predict facial attributes separately and thus do not con sider the correlation between facial attributes. In contrast, the multilabel learning based methods, which attempt to predict facial attributes simultaneously by using labelled data, have drawn increasing attention. However, in realworld applica tions, only some commonly used attributes are provided with labelled information, while the other attributes have unlabelled data. Therefore, these methods [13, 14] fail to deal with the fa cial attribute classiÔ¨Åcation problem when unlabelled informa tion is available (recall that these methods are based on superarXiv:1805.01282v1  [cs.CV]  3 May 2018vised learning). Motivated by the above observations, we propose a novel facial attribute classiÔ¨Åcation method, which performs transfer learning based on multilabel learning. More speciÔ¨Åcally, we take advantage of the transfer DNN technique to predict facial attributes that do not have labelled information in the target domain. To e ectively exploit the labelled data in the source domain, we use the multilabel learning technique to predict multiple facial attributes simultaneously, considering the cor relation between facial attributes. Fig. 1 shows an illustration of the correlation between di erent facial attributes. For di er ent learning problems, some carefully designed networks are used, where these networks share the same structure at the for mer layers of the networks and they only di er at the latter lay ers. Therefore, the networks can be e ectively trained via Ô¨Åne tuning. In this paper, we propose an e ective deep transfer neu ral network method, termed FMTNet, which consists of three subnetworks for facial attribute classiÔ¨Åcation. The Ô¨Årst sub network is the Face detection Network (FNet) for face detec tion. FNet is initialized by using the model learned from a large scale ImageNet dataset [15], and then is Ô¨Ånetuned by using the facial images. The second subnetwork is the Multilabel learning Network (MNet) for facial attribute classiÔ¨Åcation with supervised learning, where multiple attributes are predicted si multaneously. Based on FNet, MNet is Ô¨Ånetuned by using labelled attributes in the source domain. The network struc tures at the former layers of both MNet and FNet are the same, whereas the main di erence is that multiple fullyconnected layers are independently constructed in MNet. The third sub network is the Transfer learning Network (TNet) for facial at tribute classiÔ¨Åcation, when labelled information is not available in the target domain. Based on MNet, TNet makes use of un supervised domain adaptation to improve the performance of facial attribute classiÔ¨Åcation. The main contributions of this paper are summarized as fol lows: (1) Instead of using singlelabel learning for each attribute [4, 7], the proposed method e ectively performs facial attribute classiÔ¨Åcation based on multilabel learning for the labelled at tributes in the source domain. Especially, we propose an ef fective loss weight scheme to explicitly exploit the correlation between facial attributes based on attribute grouping, which can signiÔ¨Åcantly improve the generalization performance of the proposed method. (2) Based on multilabel learning, the proposed method leverages transfer learning to predict facial attributes for the unlabelled attributes in the target domain. The transfer neural network successfully transfers the features from the source do main (with labelled information) to the target domain (without labelled information), even when the probability distributions between the two domains are signiÔ¨Åcantly di erent. Therefore, the proposed method alleviates the dependency on fully labelled training data, especially in the absence of labelled information for some attributes. The remainder of the paper is organized as follows: In Sec tion 2, some related work is discussed. In Section 3, the detailsof the proposed FMTNet method for facial attribute classiÔ¨Åca tion are described. In Section 4, the experimental results are reported. In Section 5, the conclusions are presented. 2. Related Work "
380,Boosting Active Learning for Speech Recognition with Noisy Pseudo-labeled Samples.txt,"The cost of annotating transcriptions for large speech corpora becomes a
bottleneck to maximally enjoy the potential capacity of deep neural
network-based automatic speech recognition models. In this paper, we present a
new training pipeline boosting the conventional active learning approach
targeting label-efficient learning to resolve the mentioned problem. Existing
active learning methods only focus on selecting a set of informative samples
under a labeling budget. One step further, we suggest that the training
efficiency can be further improved by utilizing the unlabeled samples,
exceeding the labeling budget, by introducing sophisticatedly configured
unsupervised loss complementing supervised loss effectively. We propose new
unsupervised loss based on consistency regularization, and we configure
appropriate augmentation techniques for utterances to adopt consistency
regularization in the automatic speech recognition task. From the qualitative
and quantitative experiments on the real-world dataset and under real-usage
scenarios, we show that the proposed training pipeline can boost the efficacy
of active learning approaches, thus successfully reducing a sustainable amount
of human labeling cost.","EndtoEnd Automatic Speech Recognition (E2EASR) mod els [1, 2, 3, 4] have achieved impressive improvements in Large V ocabulary Automatic Speech Recognition (LV ASR). However, although they achieve stateoftheart performance [5, 6], the methods require more number of samples, decreasing the economical efÔ¨Åciency considering the high labeling cost of the speech data. The cost to annotate labels might be more troublesome in ASR because the cost to transcribe utterances *Authors contributed equally to this research. The authors are sorted by alphabetical order. This work is done while Heesu Kim did internship at Clova AI Research, NA VER Corp.is more expensive due to its errorprone property compared to simple classiÔ¨Åcation problems such as object class for image classiÔ¨Åcation. The reason E2EASR models require enormous data stems from the fact that they are trained in endtoend without strong inductive bias such as explicit acoustic and language models while having lots of model parameters [5]. Therefore, maximizing the training efÔ¨Åciency in labeling cost is necessary for the stateoftheart E2EASR model. Active Learning (AL) approach, has been studied to re duce the labeling cost by selecting samples most effective for a model training from many unlabeled candidates.The selected samples are annotated by human experts, so Human Labeled Samples (HLS) become the important anchors in training the model. However, the number of HLS is restricted due to the labeling budget, so we usually cannot get sufÔ¨Åcient amount of the labeled data for model capacity. Furthermore, even the deÔ¨Ånitions of effectiveness are different among AL studies, the consensus is that the effective samples for training are in most case unfamiliar and uncertain ones to the current model. Therefore, even HLS complements the model to han dle unfamiliar samples, it cannot fully exploit the potential of E2EASR models because of constrained labeling budget and bais existing in the selected HLS. To mitigate such problems in AL without additional label ing cost, we propose to utilize the unlabeled samples which are not selected for HLS. Inspired by SemiSupervised Learn ing (SSL) , we use the unlabeled samples, relatively familiar and conÔ¨Ådent in view of the current model contrary to HLS, by generating their pseudolabels ( PseudoLabeled Samples (PLS) ) and appending the samples to the training dataset. Unfortunately, simply introducing PLS would not lead to the improvement of model performance mainly because of the two reasons. One is that if PLS are selected conserva tively, PLS are too familiar to model, so they do not incur any effective variation in model parameters after training. the other is that if PLS are selected speculatively, they are likely to have noisy pseudolabels, consequently hurting model per formance. Therefore, in this paper, we propose a training pipeline toarXiv:2006.11021v2  [eess.AS]  5 Nov 2020‚Ä¶certainAL PLSHLSSSLFig. 1 : An overview of the training pipeline proposed in this paper. overcome the limitations of both AL and SSL.To this end, we introduce Consistency Regularization (CR) [7, 8, 9] technique which regularizes the sideeffect of noisy pseudolabels by forcing a model to predict consistently on both of genuine and distorted versions of a sample. The experimental results suggest that our new training pipeline can fully utilize PLS in training. The overview of the training pipeline proposed in this paper is depicted in Figure 1. CR is mostly applied in computer vision tasks [7, 8, 9] and CR has not been actively considered in ASR task because of its incompatibility to distortions (i.e., augmentations), which has been reported to be effective for vision tasks. However, we show that appropriate augmentations, which do not corrupt essential linguistic information, can enable CR on utterances for ASR. By introducing loss for CR to train objective with the carefully conÔ¨Ågured augmentations, our training pipeline restores the degradation of performance caused by restricted labeling budget in AL without any additional labeling cost. Consequently, it achieves a signiÔ¨Åcant reduction of the label ing cost, as well as minimizing the performance degradation. To validate the proposed training pipeline, we conduct ex tensive experiments on the realworld samples acquired from services deployed to endusers, which provides voice search and voice control for IoTs. The amount of collected samples is about 500 hours of utterances recorded from various de vices and users. Comparing with conventional AL on such a dataset, our proposed training pipeline boosts the perfor mance of the model by 12.76% and 4.02% in Characterlevel Error Rate (CER) when the labeling budget is 1/3 and 1/10 of unlabeled samples, respectively. In summary, our contributions to achieving such an objec tive can be summarized as threefold: 1) this work adopts con sistency regularization on samples with noisy pseudolabels in E2EASR model training to boost the effect of active learn ing for labelefÔ¨Åciency. 2) we conÔ¨Ågure the feasible augmen tations for utterances to adapt consistency regularization for ASR, and 3) we verify and analyze the efÔ¨Åcacy of the pro posed training pipeline including consistency regularization with extensive realworld data and realistic environments.2. RELATED WORKS "
381,DeepPFCN: Deep Parallel Feature Consensus Network For Person Re-Identification.txt,"Person re-identification aims to associate images of the same person over
multiple non-overlapping camera views at different times. Depending on the
human operator, manual re-identification in large camera networks is highly
time consuming and erroneous. Automated person re-identification is required
due to the extensive quantity of visual data produced by rapid inflation of
large scale distributed multi-camera systems. The state-of-the-art works focus
on learning and factorize person appearance features into latent discriminative
factors at multiple semantic levels. We propose Deep Parallel Feature Consensus
Network (DeepPFCN), a novel network architecture that learns multi-scale person
appearance features using convolutional neural networks. This model factorizes
the visual appearance of a person into latent discriminative factors at
multiple semantic levels. Finally consensus is built. The feature
representations learned by DeepPFCN are more robust for the person
re-identification task, as we learn discriminative scale-specific features and
maximize multi-scale feature fusion selections in multi-scale image inputs. We
further exploit average and max pooling in separate scale for person-specific
task to discriminate features globally and locally. We demonstrate the
re-identification advantages of the proposed DeepPFCN model over the
state-of-the-art re-identification methods on three benchmark datasets:
Market1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%,
64.3%, and 52.6% respectively on these benchmark datasets.","Person reidentiÔ¨Åcation detects whether a person of interest has been observed in another place (time) by a different camera [ 1]. In many scenarios, people appearing in one camera do not necessarily appear in another camera and sometimes the camera view may include people that have never appeared in any other camera as well. Therefore, it is better to treat person reidentiÔ¨Åcation as a veriÔ¨Åcation problem [ 2]. Person reidentiÔ¨Åcation is a naturally challenging task because correctly matching two images of the same person is difÔ¨Åcult under extensive appearance changes, such as human pose, illumination, occlusion, background clutter, (non)uniform clothing, and camera viewangle [ 2]. It has applications in tracking a particular person across these cameras, tracking the trajectory of a person, real time surveillance, and forensic and security applications. In this paper, we propose Deep Parallel Feature Consensus Net (DeepPFCN) a novel network architecture that learns multiscale person appearance features using convolutional neural networks (CNN) [ 3] and factorizes the visual appearance of a person into latent discriminative factors at multiple semantic levels [ 4]. DeepPFCN is a combination of the above mentioned two architectures, which are orthogonal to each other as mentioned in [ 4]. DeepPFCN focuses on fusing both automated discovery of latent appearance factors and fusing image resolutions. DeepPFCN deploys a multiloss concurrent supervision mechanism. This allows enforcing and improving scalespeciÔ¨Åc feature learning.arXiv:1911.07776v1  [cs.CV]  18 Nov 2019APREPRINT  NOVEMBER 19, 2019 DeepPFCN is evaluated on three person reidentiÔ¨Åcation benchmark datasets  Market1501 [ 1], DukeMTMCreID [ 5], CUHK03 [ 6]. Extensive experiments and ablation study have been conducted on these datasets. In particular, we achieve the mAP scores of 75.8%, 64.3%, and 52.6% on the above mentioned benchmark datasets, which is observed to be better than the stateoftheart methods by 1.5%, 1.5% and 3.4%, respectively. This paper is organized as follows. In section 3, we describe the components of DeepPFCN architecture  (a) modiÔ¨Åed multilevel factor net as base model and (b) multiscale consensus learning with backpropagation. We combine these methods, which achieves the stateoftheart performance for person reidentiÔ¨Åcation. In section 4, we work with the following datasets: Market1501 [ 1], DukeMTMCreID [ 5] and CUHK03 [ 6] and explain the evaluation metrics, data augmentation, training and evaluation in detail. In section 5, we describe the experiments and present the ablation study conducted on these datasets. Finally, section 6 concludes the paper. 2 Related Work "
382,Named Entity Recognition in the Legal Domain using a Pointer Generator Network.txt,"Named Entity Recognition (NER) is the task of identifying and classifying
named entities in unstructured text. In the legal domain, named entities of
interest may include the case parties, judges, names of courts, case numbers,
references to laws etc. We study the problem of legal NER with noisy text
extracted from PDF files of filed court cases from US courts. The ""gold
standard"" training data for NER systems provide annotation for each token of
the text with the corresponding entity or non-entity label. We work with only
partially complete training data, which differ from the gold standard NER data
in that the exact location of the entities in the text is unknown and the
entities may contain typos and/or OCR mistakes. To overcome the challenges of
our noisy training data, e.g. text extraction errors and/or typos and unknown
label indices, we formulate the NER task as a text-to-text sequence generation
task and train a pointer generator network to generate the entities in the
document rather than label them. We show that the pointer generator can be
effective for NER in the absence of gold standard data and outperforms the
common NER neural network architectures in long legal documents.","Named Entity Recognition (NER) is the task of identifying the span and the class of a Named Entity (NE) in unstructured text. NEs typically include but are not limited to persons, companies, dates, and geographical locations (Sang and De Meulder, 2003). Legal NER is a central task in language process ing of legal documents, especially for extracting key information such as the name of the parties in a case, the court name or the case number, or references to laws or judgements, to name a few. The extracted NEs could be integrated in legal re search workÔ¨Çows for functionalities such as search,document anonymization or case summarization thereby enabling and expediting insights for legal professionals (Zhong et al., 2020). NER is commonly formalized as a sequence la beling task: each token of the document is assigned a single label that indicates whether the token be longs to an entity from a predeÔ¨Åned set of cate gories (Li et al., 2018). To create a training dataset in such a format the annotator is required to manu ally label each token in a sentence with the respec tive category. In this format, both the NE and the location of the NE in the source text are known. This format of training data is what we refer to hereafter as ‚Äúgold standard‚Äù data. Obtaining the re quired voluminous gold standard data to train such models is, therefore, a laborious and costly task. In this paper, we perform NER in Ô¨Åled lawsuits in US courts. SpeciÔ¨Åcally, we aim to identify the party names in each case, i.e. the names of the plaintiffs and the defendants, in a large collection of publicly available cases from more than 200 courts in different US jurisdictions. The party names have been identiÔ¨Åed by legal annotators but their exact location in the text is unknown. In this respect, we do not have access to ‚Äúgold standard‚Äù training data even though the target NEs are available. This feature of our dataset introduces a key difference of our task to most NER tasks. One solution to this problem is to generate the ‚Äúgold standard‚Äù training data by searching for the locations of the known NEs in the source text . By performing this additional transformation to our data, we would be able to train sequence labeling NER models. For the following reasons, this solu tion is nontrivial. First, as our source text is also extracted from scanned PDF Ô¨Åles (‚Äùimageonly‚Äù PDFs), it contains Optical Character Recognition (OCR) mistakes and/or typos which may not be present in the target NEs. Second, besides the po tential OCR errors at the character level, the closelyarXiv:2012.09936v1  [cs.CL]  17 Dec 2020spaced, twocolumn page layouts that can be often found as headers in the Ô¨Åled cases, represent an additional challenge for the OCR, which tends to concatenate the text across columns (Figure 1). In such cases, the tokens that make up the NEs in the source text may be intertwined with other words and/or sentences. Third, variations of the names may be also present in the source text and in our humangenerated labels, such as presence of Ô¨Årst and/or middle names whole or as initials and, to a lesser extent, typos. To address some of the challenges imposed by the format of our training data and inspired by the work in the Ô¨Åeld of abstractive summarization, we propose to reformulate the NER task, not as a se quence labeling problem, but as a texttotext se quence generation problem with the use of a pointer generator network (Gu et al., 2016; See et al., 2017; Gehrmann et al., 2018). With this reformulation, in contrast to sequence labeling, we do not require knowledge of the NE‚Äôs locations in the text as train ing labels. A recent study by Li et al. (2020) pro posed a different formulation of the NER task as a question answering task and achieved stateof theart performance in a number of published NER datasets (Li et al., 2020). In this study, we adopt a hybrid extractiveabstractive architecture, based on recurrent neural networks coupled with global (i.e. the entire input document) attention and copy ing (or pointing) attention mechanisms (Gehrmann et al., 2018). The proposed architecture can be suc cessfully used for abstractive summarization since it can copy words from the source text via point ing and can deal effectively with outofvocabulary (OOV) words ‚Äì words that have not been seen dur ing training. Our approach is conceptually sim ple but empirically powerful and we show that the pointer generator outperforms the typical NER ar chitectures in the case of noisy and lengthy inputs where the NE‚Äôs location in the text is not known. In addition, we examine how our approach can be used for the related NER task of case number extraction. The case number is a unique combi nation of letters, numbers and special characters as a single token and are, therefore, particularly challenging for NER models as they are often dealt with as OOV words by the model. As in the party names task discussed above, in the case number task we do not have ‚Äúgold standard‚Äù labels of the case number‚Äôs location in the text. We show that a character level sequence generation network candramatically increase our ability to extract case numbers from the source text, compared to a word level sequence generation network. The rest of the paper is organized as follows. In Section 2, we discuss related work in the Ô¨Åeld of NER in the legal domain. In Section 3, we describe our proposal of NER as a texttotext sequence gen eration task in the absence of gold standard data and formulate the task in two ways: (i) as a combi nation of automatically labeling the NE‚Äôs location and then using the conventional sequence labeling method for NER, and (ii) as a texttotext sequence generation task where the NEs are directly gener ated as text. Section 4 presents our experimental design, results and analysis. Section 5 presents the case number case study. Finally, we conclude and discuss directions for future work. 2 Related Work "
383,Seq-UPS: Sequential Uncertainty-aware Pseudo-label Selection for Semi-Supervised Text Recognition.txt,"This paper looks at semi-supervised learning (SSL) for image-based text
recognition. One of the most popular SSL approaches is pseudo-labeling (PL). PL
approaches assign labels to unlabeled data before re-training the model with a
combination of labeled and pseudo-labeled data. However, PL methods are
severely degraded by noise and are prone to over-fitting to noisy labels, due
to the inclusion of erroneous high confidence pseudo-labels generated from
poorly calibrated models, thus, rendering threshold-based selection
ineffective. Moreover, the combinatorial complexity of the hypothesis space and
the error accumulation due to multiple incorrect autoregressive steps posit
pseudo-labeling challenging for sequence models. To this end, we propose a
pseudo-label generation and an uncertainty-based data selection framework for
semi-supervised text recognition. We first use Beam-Search inference to yield
highly probable hypotheses to assign pseudo-labels to the unlabelled examples.
Then we adopt an ensemble of models, sampled by applying dropout, to obtain a
robust estimate of the uncertainty associated with the prediction, considering
both the character-level and word-level predictive distribution to select good
quality pseudo-labels. Extensive experiments on several benchmark handwriting
and scene-text datasets show that our method outperforms the baseline
approaches and the previous state-of-the-art semi-supervised text-recognition
methods.","Text recognition has garnered a great deal of attention in recent times [ 38], owing primarily to its commercial applica tions. Since the introduction of deep learning, great strides [6,9,14,33,36,37,53,54,63,64,71] have been made in recognition accuracy on various publicly available bench mark datasets [ 28,29,31,39,44,45,49,50,65]. These models, however, are heavily reliant on a large amount of labeled data with complete charactersequence as labels, which is laborious to obtain. Aside from fullysupervised text recognition, very few attempts have been made to utilize unlabelled data samples to improve the model‚Äôs performance [1, 27, 76]. n r u w[s] n r u w n r u w n r u w n r u w H i g h L o wFigure 1: An overview of BeamSearch inference (beam width = 2) on recognizing an unlabeled textimage and pop ulating the hypotheses set. The framework considers all the accumulated hypotheses to approximate the total uncertainty (Utotal) by importancesampling. Semisupervised learning paradigms have been developed to address the preceding issues, and primarily pseudolabel based semisupervised learning (PLSSL) methods have caught much attention. In a PLSSL configuration, a smaller labeled set is used to train an initial seed model and then applied to a larger amount of unlabeled data to generate hypotheses. Furthermore, the unlabeled data along with its most reliable hypothesis as the label is combined with the training data for retraining, this methodology utilizes both the labeled and unlabelled datapoints to retrain the com plete model, allowing the entire network to exploit the latent knowledge from unlabelled datapoints as well. However, on the other hand, PLSSL is sensitive to the quality of the selected pseudolabels and suffers due to the inclusion of erroneous highly confident pseudolabels generated from poorly calibrated models, resulting in noisy training [ 51]. Moreover, for imagebased text recognition, that requires predictions of characters at each timestep for each input image, pseudolabeling is much more challenging due to the combinatorial vastness of the hypotheses space and the fact that a single incorrect character prediction renders the entirearXiv:2209.00641v2  [cs.CV]  6 Oct 2022predicted sequence false. Additionally, in the PLSSL setup, handling erroneous predictions from the model trained with a relatively small amount of labeled data, and being able to exclude them in the beginning of the training cycle is highly essential. Therefore, correct hypotheses generation and se lection are of massive importance in such a framework. This work proposes a reliable hypotheses generation and selection method for PLSSL for imagebased text recog nition. We suggest a way to estimate the uncertainty as sociated with the prediction of character sequences for an input image that gives a firm estimate of the reliability of the pseudolabel for an unlabelled data sample and then based on the estimated uncertainty, select the examples which have a low likelihood of an incorrectly assigned pseduolabel. Our methodology stems from the two primary observations that suggest (a) for pseudolabeling based SSL schemes, choos ing predictions with low uncertainty reduces the effect of poor calibration, thus improving generalization [ 51] and (b) a high positive correlation exists between the predictive un certainty and the token error rate, for a deep neural network based language model, suggesting if the model produces a high uncertainty for an input image, its highly likely that the prediction used as the pseudolabel is incorrect [62]. Nevertheless, the majority of the unsupervised uncertainty estimation methods have concentrated on conventional un structured prediction tasks, such as image classification and segmentation. Uncertainty estimation of an input image for text recognition, inherently a sequence prediction task, is highly nontrivial and poses various challenges [ 42]: (a) Recognition models do not directly generate a distribution over an infinite set of variablelength sequences, and (b) an autoregressive sequence prediction task, such as text recognition, does not have a fixed hypotheses set; therefore, debarring expectation computation on the same. To circumvent these challenges we use BeamSearch in ference (Figure 1) to yield high probability hypotheses for each unlabelled datapoint on the seed model trained with the given labeled data. Moreover, the hypotheses obtained are used to approximate the predictive distribution and obtain expectations over the set. We term this process as deter ministic inference , which generates definite and a distinct hypotheses set for each image that aid in approximating the predictive distribution. Furthermore, to compute the uncer tainty associated with an input, we take a Bayesian approach to ensembles as it produces an elegant, probabilistic and interpretable uncertainty estimates [42]. We use MonteCarloDropout (MCDropout) [ 20], which alleviates the need to train multiple models simultaneously and allows us to utilize Dropout to virtually generate multiple models (with different neurons dropped out of the original model) as MonteCarlo samples and perform inferences on the sampled models, on each of the sequences in the hypothe ses set by teacherforcing [67], terming it as stochastic infer ence. Our motivation to utilize teacherforcing in the pseudo labeling phase is to enforce prediction consistency across allthe sampled models in the ensemble such that we can esti mate the predictive distribution of every hypothesis obtained with deterministicinference . Finally, the predictive posterior for each hypothesis is obtained by taking the expectation over all the sampled models. Furthermore, the obtained pre dictive posterior is used to compute an informationtheoretic estimate of the uncertainty, which estimates the total uncer tainty [22], considering both the characterlevel and word level predictive posteriors and serves as a robust selection criterion for the pseudolabels. Figure 1 shows an intuitive idea behind BeamSearch inference to generate multiple hypotheses for a normalized uncertainty estimate. Finally, we test our method on several handwriting and scenetext datasets, comparing its performance to stateoftheart text recognition methods in semisupervised setting. Moreover, we demonstrate the robustness of our uncertainty estimation using prediction rejection curves [41,43] based on the Word Error Rate (WER). In sum the keypoints are: (a) We propose an uncertainty aware pseudolabelbased semisupervised learning frame work, that utilizes BeamSearch inference for pseudolabel assignment, and a character and sequence aware uncertainty estimate for sample selection. (b) We utilize teacherforcing [67], primarily employed to train sequencemodels, in the pseudolabeling phase to enforce prediction consistency across all the sampled models in the ensemble, to estimate the predictive distribution.(c) Finally, the methods are eval uated on several challenging handwriting and scenetext datasets in the SSL setting. 2. Related Work "
384,Improving Label Ranking Ensembles using Boosting Techniques.txt,"Label ranking is a prediction task which deals with learning a mapping
between an instance and a ranking (i.e., order) of labels from a finite set,
representing their relevance to the instance. Boosting is a well-known and
reliable ensemble technique that was shown to often outperform other learning
algorithms. While boosting algorithms were developed for a multitude of machine
learning tasks, label ranking tasks were overlooked. In this paper, we propose
a boosting algorithm which was specifically designed for label ranking tasks.
Extensive evaluation of the proposed algorithm on 24 semi-synthetic and
real-world label ranking datasets shows that it significantly outperforms
existing state-of-the-art label ranking algorithms.","Label ranking is a prediction task which deals with learn ing a mapping between an instance and a ranking (i.e., or der) of labels from a Ô¨Ånite set, representing their relevance to the instance [Zhou et al. , 2014 ]. Due to its wide applica bility, label ranking has attracted a lot of focus from the ar tiÔ¨Åcial intelligence community in recent years [H¬®ullermeier et al. , 2008; Cheng et al. , 2009; Aiguzhinov et al. , 2010; Cheng et al. , 2013; Zhou et al. , 2014; Gurrieri et al. , 2014; Destercke et al. , 2015; Aledo et al. , 2017; S ¬¥aet al. , 2017; Zhou and Qiu, 2018 ]. Applications of label ranking include for example, text classiÔ¨Åcation, where a news article may belong to multiple topics, and the goal of the label ranking algorithm is to rank the topics according to their relevance to the document. In pattern recognition, objects can be ordered according to their relevance to the image [Yang et al. , 2016 ]. In metalearning, a label ranking model can provide a list of algorithms to a given problem, ranked according to their Ô¨Åt to the problem, based on the characteristics of the problem at hand [Brazdil et al. , 2003 ]. Label ranking tasks must not be confused with multilabel tasks [Tsoumakas et al. , 2009 ]nor with learning to rank tasks [Cohen et al. , 1998 ]. In label ranking tasks, the target at tribute of each instance contains a ranking of labels, repre senting their relative relevance to the instance. In contrast, inmultilabel tasks, the target attribute is a nonranked subset of relevant labels, and in learning to rank tasks, the goal is to produce a ranked list of the instances themselves. Boosting is a wellknown and reliable ensemble technique [Schapire, 2003 ]that was shown to often outperform other learning algorithms. AdaBoost [Freund and Schapire, 1997 ] is one of the most widely used classiÔ¨Åcation boosting tech niques. Variations of AdaBoost were developed for multi class tasks [Freund and Schapire, 1997 ], multilabel tasks [Schapire and Singer, 2000 ], regression tasks [Drucker, 1997; Solomatine and Shrestha, 2004 ], and learning to rank tasks [Cohen et al. , 1998; Xu and Li, 2007; Wu et al. , 2010 ]. How ever, to the best of our knowledge, no boosting algorithm was suggested for label ranking tasks. In this paper, we propose a novel boosting algorithm, Ad aBoost.LR, which was speciÔ¨Åcally designed for label rank ing tasks. An extensive evaluation of AdaBoost.LR over 24 semisynthetic and realworld datasets shows that it signiÔ¨Å cantly outperforms existing stateoftheart label ranking al gorithms. The rest of this paper is organized as follows: we Ô¨Årst dis cuss label ranking ensembles in section 2. Thereafter, we de scribe our proposed method in section 3. This is followed by an overview of our experimental setting in section 4, a de scription of our extensive evaluation in section 5, and a sum mary and suggestions for future work in section 6. 2 Related Work "
385,Virus-MNIST: A Benchmark Malware Dataset.txt,"The short note presents an image classification dataset consisting of 10
executable code varieties and approximately 50,000 virus examples. The
malicious classes include 9 families of computer viruses and one benign set.
The image formatting for the first 1024 bytes of the Portable Executable (PE)
mirrors the familiar MNIST handwriting dataset, such that most of the
previously explored algorithmic methods can transfer with minor modifications.
The designation of 9 virus families for malware derives from unsupervised
learning of class labels; we discover the families with KMeans clustering that
excludes the non-malicious examples. As a benchmark using deep learning methods
(MobileNetV2), we find an overall 80% accuracy for virus identification by
families when beneware is included. We also find that once a positive malware
detection occurs (by signature or heuristics), the projection of the first 1024
bytes into a thumbnail image can classify with 87% accuracy the type of virus.
The work generalizes what other malware investigators have demonstrated as
promising convolutional neural networks originally developed to solve image
problems but applied to a new abstract domain in pixel bytes from executable
files. The dataset is available on Kaggle and Github.","For classifying handwriting, t he popularity of the Modified National Institute of Standards and Technology   dataset (MNIST)  contin ues to dominate the early exploration of new algorithms  [13]. Its extensions to  other domains have included foreign languages  [48], medical dia gnoses  [9], overhead  imagery  [10], and  retail objects  [11].  With modern deep learning an d convolutional neur al networks, t he accuracy for multi ple  classification  challenges typically exceed 90% ac ross all classes  [12]. Recent interest  in applying the same  techniques to anti virus and malware detectors  [1319] motivates the present work to sco re a similar  formatted problem  and compare the algorithmic perfo rmance  with existing me thods . Intel Labs and  Microsoft  Threat Protection Intelligence T eam recently launched t heir static malware collaboration called  STAMINA: S calable Deep Learning  Appro ach for Malware Classific ation  [20]. The contribution  of this short note  is to reformulate  the malware image  problem as a familiar MNIST variant ,  to generate the 9 virus  clusters based  on byte similarities,  and then to  identify the  virus famil y based on a  greyscale thumbnail image (32 x 32) .  Figure 1 shows the abstract images  derived for each of the 10 classes, with  ‚Äú0‚Äù as the only one that is non  malicious.       Figure 1 Virus MNIST showing 10 classes . The ‚Äú0‚Äù class represents non  malicious examples. The  other 9 virus families were clustered using a K  means method to match with  the standard MNIST format  and multi class  solut ions. 2. METHODS   "
386,Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification.txt,"Hierarchical multi-granularity classification (HMC) assigns hierarchical
multi-granularity labels to each object and focuses on encoding the label
hierarchy, e.g., [""Albatross"", ""Laysan Albatross""] from coarse-to-fine levels.
However, the definition of what is fine-grained is subjective, and the image
quality may affect the identification. Thus, samples could be observed at any
level of the hierarchy, e.g., [""Albatross""] or [""Albatross"", ""Laysan
Albatross""], and examples discerned at coarse categories are often neglected in
the conventional setting of HMC. In this paper, we study the HMC problem in
which objects are labeled at any level of the hierarchy. The essential designs
of the proposed method are derived from two motivations: (1) learning with
objects labeled at various levels should transfer hierarchical knowledge
between levels; (2) lower-level classes should inherit attributes related to
upper-level superclasses. The proposed combinatorial loss maximizes the
marginal probability of the observed ground truth label by aggregating
information from related labels defined in the tree hierarchy. If the observed
label is at the leaf level, the combinatorial loss further imposes the
multi-class cross-entropy loss to increase the weight of fine-grained
classification loss. Considering the hierarchical feature interaction, we
propose a hierarchical residual network (HRN), in which granularity-specific
features from parent levels acting as residual connections are added to
features of children levels. Experiments on three commonly used datasets
demonstrate the effectiveness of our approach compared to the state-of-the-art
HMC approaches and fine-grained visual classification (FGVC) methods exploiting
the label hierarchy.","Traditional singlegranularity classiÔ¨Åcation usually as signs a single label to a given object from a set of mu *Corresponding author (a) Differences in domain knowledge and interference from the image occlusion. (b) Large variations of image resolutions. Figure 1. Different objects can be discerned at various levels in the label hierarchy due to differences in domain knowledge or image quality such as occlusion or resolution. tually exclusive class labels. For instance, FGVC aims at distinguishing objects from different subordinatelevel cat egories within a given object category, e.g., subcategories of birds [32], cars [16], aircraft [20]. However, the deÔ¨Ånition of what is Ô¨Ånegrained is subjective, and the image quality may affect the identiÔ¨Åcation, as illustrated in Fig. 1. A bird can be discerned as Albatross or Laysan Albatross due to differences in domain knowledge. Moreover, a bird expert recognizes a bird as Albatross rather than Blackfooted Al batross because of the occlusion of key parts. Airborne or satellite image resolutions often have large variations, caus ing objects to be recognized at different levels. These chal lenges increase the difÔ¨Åculty of constructing a dataset for singlegranularity classiÔ¨Åcation, while images annotated as coarse categories are also overlooked. Compared to singlegranularity classiÔ¨Åcation, a more preferable solution is to employ hierarchical multi granularity labels to describe an object, which provides more Ô¨Çexible options for annotators with different knowl edge backgrounds [4]. HMC [15] aims to exploit hierarchi cal multigranularity labels and embeds the label hierarchy in loss function or network architecture. Whereas conven 1arXiv:2201.03194v2  [cs.CV]  11 Jan 2022tional HMC usually evaluates each sample with complete hierarchical labels from the coarsest to the Ô¨Ånest granular ity. A more robust HMC model should effectively utilize examples observed at various levels in the hierarchy, e.g., making use of bird images annotated as [‚ÄúAlbatross‚Äù] and [‚ÄúAlbatross‚Äù, ‚ÄúLaysan Albatross‚Äù]. In this paper, we study the HMC problem in which sam ples are labeled at any level of the hierarchy. We factor ize this problem into two aspects: (1) how to effectively use instances labeled at different levels; (2) how to perform hierarchical feature interaction in the network architecture. For the Ô¨Årst problem, we adopt a tree hierarchy that deÔ¨Ånes two kinds of semantic relationships between labels: parent child correlations between levels and mutual exclusion at the same level. Inspired by the work of [7], if an instance is discerned at a label in the hierarchy, we maximize its marginal probability in the probability space constrained by the tree hierarchy. Such marginalization enjoys two bene Ô¨Åts: learning with the coarselevel label could impact de cisions of Ô¨Ånegrained subclasses while learning with the Ô¨Ånelevel label aids the prediction of coarsegrained super classes. Moreover, if the ground truth label is observed at the leaf level, we further impose the multiclass cross entropy loss to enhance the discriminative power among Ô¨Ånegrained categories. Another critical issue is to design appropriate hierarchi cal feature interaction that reÔ¨Çects the label hierarchy. A distinct characteristic of hierarchical categories is that from coarsetoÔ¨Åne levels, Ô¨Ånelevel classes not only have unique attributes but also inherit attributes related to coarselevel superclasses. Based on this property, we propose a hierar chical residual network (HRN) illustrated in Fig. 2. We Ô¨Årst set up granularityspeciÔ¨Åc layers to disentangle hierarchical features from the trunk network. Then, these hierarchical features interact via residual connections [11‚Äì14,19,30,34], i.e., features from parent levels acting as skip connections are added to features of children levels. Experiments on three commonly used FGVC datasets demonstrate the ef fectiveness of our approach compared to the stateoftheart HMC approaches and FGVC methods exploiting hierarchi cal knowledge under two evaluation metrics [31]. 2. Related Work "
387,CAGNN: Cluster-Aware Graph Neural Networks for Unsupervised Graph Representation Learning.txt,"Unsupervised graph representation learning aims to learn low-dimensional node
embeddings without supervision while preserving graph topological structures
and node attributive features. Previous graph neural networks (GNN) require a
large number of labeled nodes, which may not be accessible in real-world graph
data. In this paper, we present a novel cluster-aware graph neural network
(CAGNN) model for unsupervised graph representation learning using
self-supervised techniques. In CAGNN, we perform clustering on the node
embeddings and update the model parameters by predicting the cluster
assignments. Moreover, we observe that graphs often contain inter-class edges,
which mislead the GNN model to aggregate noisy information from neighborhood
nodes. We further refine the graph topology by strengthening intra-class edges
and reducing node connections between different classes based on cluster
labels, which better preserves cluster structures in the embedding space. We
conduct comprehensive experiments on two benchmark tasks using real-world
datasets. The results demonstrate the superior performance of the proposed
model over existing baseline methods. Notably, our model gains over 7%
improvements in terms of accuracy on node clustering over state-of-the-arts.","Unsupervised graph representation learning aims to learn lowdimensional node embeddings without supervision. The learned node embeddings preserve useful topological structures and node attributive features extracted from graphs. Traditional graph representation learning algorithms originate in the skipgram model for distributed language representation [ 17]. The pioneering work DeepWalk [ 21] constructs node sequences by performing random walks over the graph. Then, on top of these sequences, the node embeddings can be learned using the skipgram model. Following this line of development, various network embedding methods have been proposed, such as node2vec [9] and LINE [26]. Recently, the graph neural network (GNN), a generalized form of convolutional networks in the graph domain, has attracted a lot of attention. Compared with conventional graph embedding methods, GNN shows superior expressive power and has achieved promising performance in many tasks [ 15,29,35,37]. However, most existing GNN models are established on a semisupervised setting [ 15,28]. Training an accurate GNN model requires a number of highquality node labels, which might not be accessible. Then, a natural question is that can we leverage the expressive power of GNN models and produce node embeddings in an unsupervised manner ? In the real world, graphs can be derived from business data in quantity, which can help facilitate analytical tasks and provide valuable insights for business. Take an ecommerce website for example, structured data derived from everyday user purchases along with their relationship with items is produced in a million scale. Given the purchase data, we can better classify users and items if we further leverage the derived interaction graphs. Since obtaining labels is a laborintensive and timeconsuming process, if we can efficiently train a GNN model in an unsupervised manner, it would be greatly beneficial to facilitate downstream analytical tasks. There has been a surge of research interest in unsupervised visual representation learning to avoid expensive data annotations. As a subfield of unsupervised learning, selfsupervised methods have achieved great success [ 18‚Äì20]. Among them, a series of work performs clustering on embeddings and regards the clustering assignments as the pseudolabels to replace human annotations. Then, the classification objective can be used to train the model. The work DeepCluster [ 5] uses kMeans to compute pseudolabels from raw image data, enabling largescale visual representation learning in an unsupervised manner. Following this work, Asano et al . [2]further propose a selflabeling scheme to regularize the cluster size and avoid degenerate solutions, which has become the stateoftheart method on computer vision benchmarks. Although there is a proliferation of studies in selfsupervised visual representation learning, little attention is paid to graph representation learning using a selfsupervised manner. Hu et al . [12] propose a pretraining GNN model that is trained using several network measures such as betweenness and closeness. However, these statistical measures require domain knowledge and are sensitive to noise in graphs. On the contrary, as a natural characteristic of graph data, clusters group vertices that share similar functionalities in a graph and thus can be used as a good supervisory signal for training the GNN model. Moreover, we observe that graphs often contain noisy edges, which connect nodes belonging to different classes. Such edges may mislead GNN training and further confine the model from learning useful class information. In a graph with many interclass edges, when performing graph convolution through neighborhood aggregation, i.e. taking the average over neighbor nodes, the resulting node embeddings tend to be indistinguishable from different classes. Thus, we argue that a key to improving the quality of embeddings is to alleviate the impact of potentially noisy edges and strengthen edges between nodes of the same class, which will help preserve the cluster structures and obtain betterseparated node embeddings. In summary, considering the existence of noisy interclass edges, it is crucial to mitigate the impact of these ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: August 2019.CAGNN: ClusterAware Graph Neural Networks for Unsupervised Graph Representation Learning 111:3 Graph Neural Networks ClusterlabelsSelfsupervisorysignalsClusteringTopologyrefiningRefined adjacency matrix Fig. 1. The pipeline of the proposed CAGNN model. The CAGNN model alternates between node representa tion learning and clustering. We first obtain node embeddings using graph neural networks (GNN). Then, we perform clustering and use the cluster labels as the selfsupervisory signals. Following that, we use a novel clusteraware topology refining mechanism which reduces intercluster edges and strengthens intraclass connections to mitigate the impact of noisy edges. edges during training, which is usually neglected by previous work that merely leverages network measures as selfsupervision. Motivated by the aforementioned observations, we propose a novel clusteraware graph neu ral network model for selfsupervised graph representation learning in this paper. We term the model CAGNN for brevity. As illustrated in Fig. 1, our CAGNN model consists of three stages. At the first stage, we perform graph convolutions to obtain node embeddings. Then, the model conducts clustering on the node embeddings and updates the model parameters by predicting the corresponding cluster assignments. To avoid degenerate solutions, we use a balanced cluster strategy, which formulates the crossentropy minimization as an optimal transport problem. Finally, to alleviate the impact of noisy edges and better preserve cluster structures in the embedding space, we propose a novel graph topology refining scheme based on cluster assignments. The proposed refining process strengthens intraclass edges and weakens potentially noisy edges by isolating neighborhood nodes of different clusters. The core contribution of this paper is threefold. Firstly, we propose a novel selfsupervised graph neural network for unsupervised graph representation learning, which needs no supervision from labels. Secondly, unlike other GNN models, CAGNN further proposes a topology refining scheme which reduces intercluster connections of neighbor nodes to alleviate the impact of noisy edges. Thirdly, extensive experiments conducted on benchmark datasets demonstrate the superiority over existing baseline methods. It is worth mentioning that the proposed method gains over 7% performance improvement in terms of accuracy on node clustering over stateofthearts. The organization of the remaining of the paper is summarized below. We first review prior arts in relevant domains in Section 2. Then, in Section 3, we introduce our proposed clusteraware graph neural networks in detail. After that, we present empirical studies in Section 4. Finally, we conclude the paper and point out future research directions in Section 5. ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: August 2019.111:4 Zhu et al. 2 RELATED WORK "
388,Learning Boolean Circuits with Neural Networks.txt,"While on some natural distributions, neural-networks are trained efficiently
using gradient-based algorithms, it is known that learning them is
computationally hard in the worst-case. To separate hard from easy to learn
distributions, we observe the property of local correlation: correlation
between local patterns of the input and the target label. We focus on learning
deep neural-networks using a gradient-based algorithm, when the target function
is a tree-structured Boolean circuit. We show that in this case, the existence
of correlation between the gates of the circuit and the target label determines
whether the optimization succeeds or fails. Using this result, we show that
neural-networks can learn the (log n)-parity problem for most product
distributions. These results hint that local correlation may play an important
role in separating easy/hard to learn distributions. We also obtain a novel
depth separation result, in which we show that a shallow network cannot express
some functions, while there exists an efficient gradient-based algorithm that
can learn the very same functions using a deep network. The negative
expressivity result for shallow networks is obtained by a reduction from
results in communication complexity, that may be of independent interest.","It is well known (e.g. Livni et al. (2014)) that while deep neuralnetworks can express any function that can be run efÔ¨Åciently on a computer, in the general case, learning neuralnetworks is compu tationally hard. Despite this theoretic pessimism, in practice, deep neural networks are successfully trained on real world datasets. Bridging this theoreticalpractical gap seems to be the holy grail of theoretical machine learning nowadays. Maybe the most natural direction to bridge this gap is to Ô¨Ånd a property of data distributions that determines whether learning them is computationally easy or hard. The goal of this paper is to propose such a property. To motivate this, we Ô¨Årst recall the kparity problem: the input is nbits, there is a subset of k relevant bits (which are unknown to the learner), and the output should be 1if the number of 1‚Äôs among the relevant bits is even and  1otherwise. It is well known (e.g. ShalevShwartz et al. (2017)) that the parity problem can be expressed by a fully connected two layer network or by a depth log(n)locally connected1network. We observe the behavior of a one hiddenlayer neural network trained on the kparity problem, in two different instances: Ô¨Årst, when the underlying distribution is the uniform distribution (i.e. the probability to see every bit is1 2); and second, when the underlying distribution is a slightly biased product distribution (the probability for every bit to be 1is0:6). As can be seen in Ô¨Ågure 1, adding a slight bias to the probability of each bit 1. i.e. every two adjacent neurons are only connected to one neuron in the upper layer. c E. Malach & S. ShalevShwartz.arXiv:1910.11923v2  [cs.LG]  18 Jan 2020MALACH SHALEV SHWARTZ dramatically affects the behavior of the network: while on the uniform distribution the training process completely fails, in the biased case it converges to a perfect solution. 0 0:5 1 1040:50:60:70:80:91 iterationaccuracybiased uniform Figure 1: Trainig depthtwo ReLU networks of size 128 with Adam, on both instances of the kParity problem ( k= 5;n= 128 ). The Ô¨Ågure shows the accuracy on a test set.This simple experiment shows that a small change in the underly ing distribution can cause a dramatic change in the trainability of neural networks. A key property that dif ferentiates the uniform from the bi ased distribution is the correlation between input bits and the target la bel. While in the uniform distribu tion, the correlation between each bit and the label is zero, in the biased case every bit of the kbits in the par ity has a nonnegligible correlation to the label (we show this formally in section 5). So, local correlations be tween bits of the input and the target label seems to be a promising prop erty which separates easy and hard distributions. In this paper, we analyze the problem of learning treestructured Boolean circuits with neural networks. The key property that we assume is having sufÔ¨Åcient correlation between every inÔ¨Çu encing gate in the circuit and the label. We show a gradientbased algorithm that can efÔ¨Åciently learn such circuits for distributions that satisÔ¨Åes the Local Correlation Assumption (LCA). On the other hand, without LCA, anygradientbased algorithm fails to learn these circuits efÔ¨Åciently. We discuss speciÔ¨Åc target functions and distributions that satisfy LCA. We show that for most product distributions, our gradientbased algorithm learns the (logn)parity problem (parity on lognbits of an input with dimension n). We further show that for every circuit with AND/OR/NOT gates, there exists a generative distribution, such that our algorithm recovers the Boolean circuit exactly. While our primary motivation for studying treestructured Boolean circuits is as an extension to the family of kParities, we show that this family of functions has interesting properties in it self. Building on known results in communication complexity, we show that there exist tree structured AND/OR circuits which cannot be expressed by any depthtwo neuralnetwork with bounded weights, unless an exponential number of units is used. Along with our positive results on learning treestructured AND/OR circuits, this gives a family of distributions that: a) cannot be efÔ¨Åciently expressed by a shallow network and b) can be learned efÔ¨Åciently by a gradientbased algorithm using a deep network. To the best of our knowledge, this is the Ô¨Årst result showing both depth separation and computationally efÔ¨Åcient learnability for some family of distributions. As far as we are aware, all prior results on learning neuralnetworks with gradientbased algorithms as sume a target function that can be expressed by a twolayer neuralnetwork. Therefore, our result is the Ô¨Årst theoretical result that motivates learning a deep network over a shallow one. 2LEARNING CIRCUITS WITH NEURAL NETWORKS 2. Related Work "
389,Sequential Bayesian Neural Subnetwork Ensembles.txt,"Deep neural network ensembles that appeal to model diversity have been used
successfully to improve predictive performance and model robustness in several
applications. Whereas, it has recently been shown that sparse subnetworks of
dense models can match the performance of their dense counterparts and increase
their robustness while effectively decreasing the model complexity. However,
most ensembling techniques require multiple parallel and costly evaluations and
have been proposed primarily with deterministic models, whereas sparsity
induction has been mostly done through ad-hoc pruning. We propose sequential
ensembling of dynamic Bayesian neural subnetworks that systematically reduce
model complexity through sparsity-inducing priors and generate diverse
ensembles in a single forward pass of the model. The ensembling strategy
consists of an exploration phase that finds high-performing regions of the
parameter space and multiple exploitation phases that effectively exploit the
compactness of the sparse model to quickly converge to different minima in the
energy landscape corresponding to high-performing subnetworks yielding diverse
ensembles. We empirically demonstrate that our proposed approach surpasses the
baselines of the dense frequentist and Bayesian ensemble models in prediction
accuracy, uncertainty estimation, and out-of-distribution (OoD) robustness on
CIFAR10, CIFAR100 datasets, and their out-of-distribution variants: CIFAR10-C,
CIFAR100-C induced by corruptions. Furthermore, we found that our approach
produced the most diverse ensembles compared to the approaches with a single
forward pass and even compared to the approaches with multiple forward passes
in some cases.","Deep learning has been the engine that powers stateoftheart performance in a wide array of machine learning tasks [ 1]. However, deep learning models still suffer from many fundamental issues from the perspective of statistical modeling, which are crucial for Ô¨Åelds such as autonomous driving, healthcare, and science [ 2]. One of the major challenges is their ability to reliably model uncer tainty while capturing complex data dependencies and being computationally tractable. Probabilistic machine learning and, especially, the Bayesian framework provides an exciting avenue to address these challenges. In addition to superior uncertainty quantiÔ¨Åcation, Bayesian models have also been shown to have improved robustness to noise and adversarial perturbations [ 3] due to probabilistic prediction capabilities. Bayesian neural networks (BNNs) have pushed the envelope of probabilistic machine learning through the combination of deep neural network architecture and Bayesian infer ence. However, due to the enormous number of parameters, BNNs adopt approximate inference techniques such as variational inference with a fully factorized approximating family [ 4]. Although Preprint. Under review.arXiv:2206.00794v1  [stat.ML]  1 Jun 2022this approximation is crucial for computational tractability, they could lead to underutilization of BNN‚Äôs true potential [5]. Recently, ensemble of neural networks [ 6] has been proposed to account for the parameter/model uncertainty, which has been shown to be analogous to the Bayesian model averaging and sampling from the parameter posteriors in the Bayesian context to estimate the posterior predictive distribution [7]. In this spirit, the diversity of the ensemble has been shown to be a key to improving the predictions, uncertainty, and robustness of the model. To this end, diverse ensembles can mitigate some of the shortcomings introduced by approximate Bayesian inference techniques without compromising computational tractability. Several different diversityinducing techniques have been explored in the literature. The approaches range from using a speciÔ¨Åc learning rate schedule [ 8], to introducing kernalized repulsion terms among the ensembles in the loss function at train time [ 9], mixture of approximate posteriors to capture multiple posterior modes [ 10], appealing to sparsity (albeit adhoc) as a mechanism for diversity [ 11,12] and Ô¨Ånally appealing to diversity in model architectures through neural architecture and hyperparameter searches [13, 14]. However, most approaches prescribe parallel ensembles, with each individual model part of an ensemble starting with a different initialization, which can be expensive in terms of computation as each of the ensembles has to train longer to reach the highperforming neighborhood of the parameter space. Although the aspect of ensemble diversity has taken center stage, the cost of training these ensembles has not received much attention. However, given that the size of models is only growing as we advance in deep learning, it is crucial to reduce the training cost of multiple individual models forming an ensemble in addition to increasing their diversity. To this end, sequential ensembling techniques offer an elegant solution to reduce the cost of obtaining multiple ensembles, whose origin can be traced all the way back to [ 15,16], wherein ensembles are created by combining epochs in the learning trajectory. [ 17,18] use intermediate stages of model training to obtain the ensembles. [ 19] used boosting to generate ensembles. In contrast, recent works by [8,20,12] force the model to visit multiple local minima by cyclic learning rate annealing and collect ensembles only when the model reaches a local minimum. Notably, the aforementioned sequential ensembling techniques in the literature have been proposed in the context of deterministic machine learning models. Extending the sequential ensembling technique to Bayesian neural networks is attractive because we can potentially get highperforming ensembles without the need to train from scratch, analogous to sampling with a Markov chain Monte Carlo sampler that extracts samples from the posterior distribution. Furthermore, sequential ensembling is complementary to the parallel ensembling strategy, where, if the models and computational resources permit, each parallel ensemble can generate multiple sequential ensembles, leading to an overall increase in the total number of diverse models in an ensemble. A new frontier to improve the computational tractability and robustness of neural networks is spar sity [ 21]. Famously, the lottery ticket hypothesis [ 22] established the existence of sparse subnetworks that can match the performance of the dense model. Studies also showed that such subnetworks tend to be inherently diverse as they correspond to different neural connectivity [ 11,12]. However, most sparsityinducing techniques proposed have been in the context of deterministic networks and use adhoc and posthoc pruning to achieve sparsity [ 23,24]. Moreover, the weight pruning methods have been shown to provide inefÔ¨Åcient computational gains owing to the unstructured sparse subnetworks [25]. In Bayesian learning, the prior distribution provides a systematic approach to incorporate inductive bias and expert knowledge directly into the modeling framework [ 26]. First, the automatic datadriven sparsity learning in Bayesian neural networks is achieved using sparsityinducing priors [27]. Second, the use of group sparsity priors [ 28‚Äì30] provides structural sparsity in Bayesian neural networks leading to signiÔ¨Åcant computational gains. We leverage the automated structural sparsity learning using spikeandslab priors similar to [ 30] in our approach to sequentially generate multiple Bayesian neural subnetworks with varying sparse connectivities which when combined yields highly diverse ensemble. To this end, we propose Sequential Bayesian Neural Subnetwork Ensembles ( SeBayS ) with the following major contributions: ‚Ä¢We propose a sequential ensembling strategy for Bayesian neural networks (BNNs) which learns multiple subnetworks in a single forwardpass. The approach involves a single exploration phase with a large (constant) learning rate to Ô¨Ånd highperforming sparse network connectivity yielding structurally compact network. This is followed by multiple exploitation 2phases with sequential perturbation of variational mean parameters using corresponding variational standard deviations together with piecewiseconstant cyclic learning rates. ‚Ä¢We combine the strengths of the automated sparsityinducing spikeandslab prior that allows dynamic pruning during training, which produces structurally sparse BNNs, and the proposed sequential ensembling strategy to efÔ¨Åciently generate diverse and sparse Bayesian neural networks, which we refer to as Bayesian neural subnetworks . 2 Related Work "
390,Releasing Graph Neural Networks with Differential Privacy Guarantees.txt,"With the increasing popularity of Graph Neural Networks (GNNs) in several
sensitive applications like healthcare and medicine, concerns have been raised
over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to
privacy attacks, such as membership inference attacks, even if only blackbox
access to the trained model is granted. To build defenses, differential privacy
has emerged as a mechanism to disguise the sensitive data in training datasets.
Following the strategy of Private Aggregation of Teacher Ensembles (PATE),
recent methods leverage a large ensemble of teacher models. These teachers are
trained on disjoint subsets of private data and are employed to transfer
knowledge to a student model, which is then released with privacy guarantees.
However, splitting graph data into many disjoint training sets may destroy the
structural information and adversely affect accuracy. We propose a new
graph-specific scheme of releasing a student GNN, which avoids splitting
private training data altogether. The student GNN is trained using public data,
partly labeled privately using the teacher GNN models trained exclusively for
each query node. We theoretically analyze our approach in the R\`{e}nyi
differential privacy framework and provide privacy guarantees. Besides, we show
the solid experimental performance of our method compared to several baselines,
including the PATE baseline adapted for graph-structured data. Our anonymized
code is available.","In the past few years, Graph Neural Networks (GNNs) have gained much attention due to their superior performance in a wide range of applications, such as social networks [ 8], biology [ 13], medicine [ 2], Woodstock ‚Äô22, June 03‚Äì05, 2022, Woodstock, NY ¬©2022 Association for Computing Machinery. This is the author‚Äôs version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Woodstock ‚Äô22: ACM Symposium on Neural Gaze Detection, June 03‚Äì05, 2022, Woodstock, NY , https://doi.org/10.1145/1122445.1122456.and molecular chemistry [ 15]. Specifically, GNNs achieved stateof theart results in various graphbased learning tasks, such as node classification, link prediction, and community detection. Realworld graphs, such as medical and economic networks, are associated with sensitive information about individuals and their activities and can not always be made public. Releasing pretrained models provides an opportunity for using the private knowledge beyond company boundaries [ 2]. However, recent works have shown that GNNs are vulnerable to membership inference attacks [ 9,18]. Specifically, membership inference attacks allow to identify which data points have been used for training the model. In general, GNNs are more vulnerable to such attacks as compared to traditional knowledge due to their encoding of the graph structure within the model itself [18]. In addition, the current legal data protection policies to pre serve user privacy highlights a compelling need to develop privacy preserving GNNs . In this work, we propose our framework PrivGnn , which builds on the rigid guarantees of differential privacy (DP), allowing us to protect the sensitive data while releasing the trained GNN model. Differential privacy [ 5] is one of the most popular approaches for releasing data statistics or the trained model while concealing the information about individuals present in the dataset. Roughly speak ing, the key idea of DP is that if we query a dataset containing ùëÅ individuals, the query‚Äôs result will be almost indistinguishable (in a probabilistic sense) from the result of querying a neighboring dataset with one less or one more individual. Hence, each indi vidual‚Äôs privacy is guaranteed with a specific probability. Such probabilistic indistinguishability is usually achieved by adding a sufficient amount of noise to the query result. The seminal work of Abadi et al . [1]proposed differential private stochastic gradient descent (DPSGD) algorithm to achieve differen tial privacy guarantees for deep learning models. Specifically, in each step of the training, DPSGD adds appropriate noise to the ‚Ñì2clipped gradients during the stochastic gradient descent opti mization. The incurred privacy budget ùúÄfor training is computed using the moment‚Äôs accountant technique that keeps track of the privacy loss across multiple invocations of the noise addition mech anism applied to random subsets of the input dataset [1]. Besides the slow training process of DPSGD, the injected noise is proportional to the number of training epochs, which further degrades performance. More importantly, the privacy guarantee for DPSGD does not trivially hold for graph data and GNN mod els [11]. While DPSGD is designed for independent and identically distributed data (i.i.d.), the nodes in graph data are related. GNNs use a messagepassing algorithm to exchange information among connected nodes [ 8]. Therefore, the privacy guarantee of DPSGD,arXiv:2109.08907v1  [cs.LG]  18 Sep 2021Woodstock ‚Äô22, June 03‚Äì05, 2022, Woodstock, NY Iyiola E. Olatunji, Thorben Funke, and Megha Khosla Private GNNPublic GNN+  NoiseRelease ModelQueryPrivate DataPoisson SamplingKNN from Poisson SubsampleTrain PseudolabelPublic Data TrainPRIV ATE PUBLIC PUBLIC Figure 1: Workflow of PrivGnn . We are given two corresponding datasets, labeled private data and unlabeled public data . PrivGnn starts by sampling the private data using Poisson sampling, Def. 7, to retrieve a subset of the private data. We then obtain the Knearest neighbor nodes based on the features of the public query node. The teacher GNN model is trained on the graph induced on Knearest neighbors. We obtain a pseudolabel for the query node by adding independent noise to the output posterior. The pseudolabel and data from the public graph are used in training the student model, which is then released. which requires a set of i.i.d. examples to form batches and lots, does not hold for GNNs and graph data [11]. To work around DPSGD‚Äôs dependency of the training proce dure, such as the number of epochs, Papernot et al . [19] proposed Private Aggregation of Teacher Ensembles (PATE). PATE leverages a large ensemble of teacher models trained on disjoint subsets of private data to transfer knowledge to a student model, which is then released with privacy guarantees. However, splitting graph data into many disjoint training sets destroys the structural information and adversely affects accuracy. Since existing DP methods are not directly applicable to GNNs, we propose a privacypreserving framework, PrivGnn , for releas ing GNN models with differential privacy guarantees. Similar to PATE‚Äôs assumptions, we are given two graphs: a labeled private graph and an unlabeled public graph. PrivGnn leverages the par adigm of knowledge distillation. The knowledge of the teacher model trained on the private graph is transferred to the student model trained only on the public graph in a differential privacy manner. PrivGnn achieves practical privacy guarantees by com bining the studentteacher training with two noise mechanisms: random subsampling using Poisson sampling andnoisy la beling mechanism to obtain pseudolabels for public nodes. In particular, we release the student GNN model, which is trained using a small number of public nodes labeled using the teacher GNN models developed exclusively for each public query node. We present a R√®nyi differential privacy (RDP) analysis of our approach and provide tight bounds on incurred privacy budget or privacy loss. Figure 1 shows an overview of our PrivGnn approach. To summarize, our key contributions are as follows. (1)We propose PrivGnn , a novel privacypreserving frame work for releasing GNN models via differential privacy. By leveraging the studentteacher training paradigm, PrivGnn is robust to attacks on GNN models, including MI attack and model stealing attacks. (2)We derive tight privacy guarantees employing the theoreti cal results of RDP for Poisson subsampled mechanisms and advanced composition theorem for RDP. Ours is the first work utilizing the RDP framework for analyzing the privacy of GNNs.(3)We experimentally show that PrivGnn achieves close to optimal accuracy of the nonprivate version with practical privacy guarantees (singledigit ùúÄ). 2 RELATED WORKS "
391,Attentive Prototypes for Source-free Unsupervised Domain Adaptive 3D Object Detection.txt,"3D object detection networks tend to be biased towards the data they are
trained on. Evaluation on datasets captured in different locations, conditions
or sensors than that of the training (source) data results in a drop in model
performance due to the gap in distribution with the test (or target) data.
Current methods for domain adaptation either assume access to source data
during training, which may not be available due to privacy or memory concerns,
or require a sequence of lidar frames as an input. We propose a single-frame
approach for source-free, unsupervised domain adaptation of lidar-based 3D
object detectors that uses class prototypes to mitigate the effect pseudo-label
noise. Addressing the limitations of traditional feature aggregation methods
for prototype computation in the presence of noisy labels, we utilize a
transformer module to identify outlier ROI's that correspond to incorrect,
over-confident annotations, and compute an attentive class prototype. Under an
iterative training strategy, the losses associated with noisy pseudo labels are
down-weighed and thus refined in the process of self-training. To validate the
effectiveness of our proposed approach, we examine the domain shift associated
with networks trained on large, label-rich datasets (such as the Waymo Open
Dataset and nuScenes) and evaluate on smaller, label-poor datasets (such as
KITTI) and vice-versa. We demonstrate our approach on two recent object
detectors and achieve results that out-perform the other domain adaptation
works.","The localization and categorization of objects in a 3D scene is a crucial component of perception systems in Ô¨Åelds like robotics and autonomous driving. In recent years, datadriven approaches using deep neural networks have achieved superior performance in various versions of this task [22, 27‚Äì29, 38, 43, 45], facilitated in part by the release Class  ROI featuresMisclassified ROI features  Outlier removal  Cluster centroid / class prototype  Class prototype from average of ROI features  Class prototype from weighted average    of attentive ROI features   Class boundary   Comparison with SOT A Average 3D mAP   Domain shift  Figure 1. Top row: Visual representations of prototype compu tation. On the left hand side is a depiction of a standard feature aggregation approach for prototype computation. In the case of noisy labels, features corresponding to mislabeled regions that are not discarded by outlier removal contribute to the class prototype. On the right is the proposed method of entropyweighted average of attentive region features which considers only salient regions for prototype computation. The opacity of the features represents the attention weights, and the width of the connecting lines repre sents the combination weights for computing the average. Bottom row: Comparison of our results on SECONDiou against recent stateoftheart methods for three domain shift scenarios. of numerous datasets and benchmarks [1, 5, 8, 10, 20, 31]. In practical scenarios, it is important for these object de tection frameworks to perform consistently well in different domain scenarios. However, deep neural networks tend to learn not only the valuable features that aid in performing the task at hand, but also the biases present in the data it is trained on. In the case of lidar datasets, the weather condi tions and the location of capture lead to biases in the dataset due to the speciÔ¨Åc dimensions of roads, vehicles, and the driving conventions of the area. Additionally, different li dar sensors possess different rates of return and producearXiv:2111.15656v2  [cs.CV]  1 Dec 2021(a) All predictions (b) thresh = 0.7   Figure 2. 3D bounding box predictions of the object detector [28] trained on Waymo [31] data and tested on KITTI [8]. Ground truth annotations are in green and predictions are in red. Thresholding (b) fails to remove all false positives present in (a). Selftraining with these pseudolabels leads to the enforcement of errors. pointclouds with varying densities, leading to another set of inherent biases. This leads to a distribution gap among various pointcloud datasets. Thus, an object detector trained on a particular dataset will drop in performance when eval uated on samples from a dataset with a different distribu tion. We call the training and test datasets in this scenario as the source and target domain datasets, respectively. One may argue that making use of a large, diverse source do main could solve this problem, however there will always be samples from an unseen distribution, and collecting ev ery possible type of lidar scene is impractical at best. Unsupervised domain adaptation (UDA) refers to the process of bridging this gap to improve the performance of sourcetrained networks on unlabelled target samples. There have been several recent works addressing this prob lem, for both 2D [12,19,25] and 3D [3,18,26,37,40] object detection. However, source samples are often unavailable during training due to limited memory capabilities or pri vacy reasons. This requires a sourcefree domain adapta tion approach, where only the sourcetrained model and un labelled target samples are used for adaptation. There exist several such approaches for various computer vision tasks on images [15, 17, 41]. Only SFUDA3D[26] attempts this setting for 3D object detection, but relies on a sequence of lidar frames as an input to the network. Self trainingbased methods have been successful in unsupervised and semisupervised domain adaptive works [3,36,46], but rely on conÔ¨Ådence thresholding to Ô¨Ålter noisy pseudolabels. As illustrated in the example from Figure 2, the use of high thresholds (as is general practice) results in training the model on easy samples and incorrect labels of high conÔ¨Ådence that contribute to the enforcement of errors during adaptation. We propose an unsupervised, sourcefree domain adap tation framework for 3D object detection that addresses the issue of incorrect, overconÔ¨Ådent pseudo labels during self training through the use of class prototypes. In the pres ence of label noise, standard feature aggregation methods of prototype computation [11,13,39,44] are insufÔ¨Åcient, since features corresponding to incorrectly labeled regions couldcontribute to the Ô¨Ånal prototype (see Figure 1, top row). Inspired by the high representative power of selfattention and recent works that make use of transformers to focus on salient inputs [7, 33], we calculate an attentive class proto type by using a transformer to identify salient regionsof interest and combine their associated feature vectors using prediction entropy weights that represent the uncertainty of the classiÔ¨Åcation branch for each sample. Class predictions corresponding to incorrect pseudo labels, which are identi Ô¨Åed by calculating the similarity with the class prototype, are downweighed to prevent reinforcing errors during self training. We demonstrate our result on several domain shift scenarios (see Figure 1). Our contributions are as follows ‚Ä¢ We propose the attentive prototype for learning repre sentative class features in the presence of label noise by leveraging selfattention through a transformer block and perform sourcefree unsupervised domain adaptation of 3D object detection networks that mitigates the effect of label noise during self training by Ô¨Åltering incorrect an notations. ‚Ä¢ We demonstrate our method on two recent object detec tors, SECONDiou [38], and PointRCNN [28] for six do main shift scenarios and outperform recent domain adap tation works. 2. Related Works "
392,Self-supervised learning for infant cry analysis.txt,"In this paper, we explore self-supervised learning (SSL) for analyzing a
first-of-its-kind database of cry recordings containing clinical indications of
more than a thousand newborns. Specifically, we target cry-based detection of
neurological injury as well as identification of cry triggers such as pain,
hunger, and discomfort. Annotating a large database in the medical setting is
expensive and time-consuming, typically requiring the collaboration of several
experts over years. Leveraging large amounts of unlabeled audio data to learn
useful representations can lower the cost of building robust models and,
ultimately, clinical solutions. In this work, we experiment with
self-supervised pre-training of a convolutional neural network on large audio
datasets. We show that pre-training with SSL contrastive loss (SimCLR) performs
significantly better than supervised pre-training for both neuro injury and cry
triggers. In addition, we demonstrate further performance gains through
SSL-based domain adaptation using unlabeled infant cries. We also show that
using such SSL-based pre-training for adaptation to cry sounds decreases the
need for labeled data of the overall system.","Crying is the primary means by which babies communicate with the world. Researchers have been interested in infant cry analysis since the early 1960s [1]. Cry characteristics may help us to under stand basic baby needs (hunger, pain, etc.) and, more importantly, can be analyzed for the early and noninvasive detection of various diseases [2]. For example, clinical research has reported that cer tain infant cry characteristics are correlated with birth asphyxia [3]. This multicausal condition frequently leads to severe health prob lems, including neurological injury and even death. Various methods based on signal processing [4], statistical modeling [5, 6] and deep learning [7‚Äì10] have been explored for Ô¨Ånding clinical and other in sights using cry recordings. One of the main challenges in baby cry analysis is data acqui sition. Today, cry sounds are not part of routine medical records, so obtaining a database requires targeted efforts such as a clinical study. These are expensive to conduct and typically require the col laboration of several hospital staff over the years. Most machine learning (ML) research on pathology detection from cry sounds was done using the Baby Chillanto [11] database, which contains only six patients diagnosed with birth asphyxia. From an ML problem point of view, cry classiÔ¨Åcation is anal ogous to general audio classiÔ¨Åcation, where deep convolutional neural networks (CNNs) have excelled as the stateoftheart. Re cently, [12] demonstrated that Pretrained Audio Neural Networks(PANNs)  large CNNs pretrained on generic audio  transferred to a wide range of audio pattern recognition tasks outperformed sev eral previous stateoftheart systems. Since then, PANNs have been widely adopted for various audio tasks, including emotion recogni tion from speech [13] and COVID19 detection from cough [14]. Another popular paradigm in audio classiÔ¨Åcation stateoftheart is selfsupervised learning (SSL)  a method to obtain highquality representations by training on unlabeled data. SSL has revolution ized the Ô¨Åelds of Natural Language Processing and Computer Vi sion and is currently widely adopted in audio processing [15]. A neural network (encoder) pretrained with SSL can be seen as a non linear mapping of an audio sequence to a hidden representation  an embedding. The embeddings can be used as input to a classi Ô¨Åer trained on a speciÔ¨Åc task with a supervised objective (using la beled data and conventional crossentropy loss). This approach is common for benchmarking various SSL models on multiple diverse audio tasks [16, 17]. Recently, a similaritybased contrastive learn ing method called SimCLR introduced in Computer Vision [18, 19] demonstrated good performance in multiple audio tasks [17, 20], in cluding music analysis [21, 22]. SimCLR maximizes the similarity between modiÔ¨Åed (distorted) views of the same object. For audio, such distortion can be done, for example, by mixing random audio samples [17], spectrogram masking [23] in [20], or/and reverbera tion, pitch shifting, etc [21]. In this paper, we experiment with PANNs using both super vised and selfsupervised pretraining to learn representations for two downstream tasks. The Ô¨Årst task is classifying brain injury (re sulting from birth asphyxia), and the second is predicting cry trig gers (pain, hunger, discomfort). The methods are tested on a unique clinical database of newborn cries collected by Ubenwa Health in collaboration with hospitals across three countries [24]. In addition, we evaluate the impact of SimCLRbased adap tation of PANNs using unlabeled cries inspired by selfsupervised domain adaptation in Speech [25] and Natural Language Process ing [26]. It should be noted that speech and audio SSL stateof theart frequently uses transformers instead of CNNs and relies on different learning objectives [15]. However, our preliminary exper iments with some popular pretrained speech and audio transform ers (speciÔ¨Åcally, Wav2Vec2.0 [27], HuBERT [28], WavLM [29] and SSAST [30]) have not shown sufÔ¨Åcient improvements but generally required many parameters to be adapted and hyperparameters tuned. We, therefore, focus on CNN and SimCLR, which demonstrated a good balance of accuracy and adaptation complexity. 2. METHODOLOGY "
393,Learn to Propagate Reliably on Noisy Affinity Graphs.txt,"Recent works have shown that exploiting unlabeled data through label
propagation can substantially reduce the labeling cost, which has been a
critical issue in developing visual recognition models. Yet, how to propagate
labels reliably, especially on a dataset with unknown outliers, remains an open
question. Conventional methods such as linear diffusion lack the capability of
handling complex graph structures and may perform poorly when the seeds are
sparse. Latest methods based on graph neural networks would face difficulties
on performance drop as they scale out to noisy graphs. To overcome these
difficulties, we propose a new framework that allows labels to be propagated
reliably on large-scale real-world data. This framework incorporates (1) a
local graph neural network to predict accurately on varying local structures
while maintaining high scalability, and (2) a confidence-based path scheduler
that identifies outliers and moves forward the propagation frontier in a
prudent way. Experiments on both ImageNet and Ms-Celeb-1M show that our
confidence guided framework can significantly improve the overall accuracies of
the propagated labels, especially when the graph is very noisy.","The remarkable advances in visual recognition [6,34,33,11,12,46,7,41,13,17,15,42,29,41,16,28,40] are built on top of largescale annotated training data. However, the ever increas ing demand on annotated data has resulted in prohibitive annotation cost. Trans ductive learning, which aims to propagate labeled information to unlabeled sam ples, is a promising way to tackle this issue. Recent studies [50,26,21,38,25,18] show that transductive methods with an appropriate design can infer unknown labels accurately while dramatically reducing the annotation eorts. Many transductive methods adopt graphbased propagation [49,26,21,38] as a core component. Generally, these methods construct a graph among all samples, propagating labels or other relevant information from labeled sam ples to unlabeled ones. Early methods [49,47,1] often resort to a linear diusion paradigm, where the class probabilities for each unlabeled sample are predictedarXiv:2007.08802v1  [cs.CV]  17 Jul 20202 L. Yang and Q. Huang and H. Huang and L. Xu and D. Lin Geodesic distance to labeled verticesAccuracy0.0 1.00.80.60.40.2Most confident set ‚Ä¶ Most unconfident set ‚Ä¶ConfNet Calibrated confidenceInitial confidence Confidence Fig. 1: In this paper, we propose a framework for transductive learning on noisy graphs, which contain a large number of outliers, e.g.outofclass samples. The framework con sists of a local predictor and a condencebased path scheduler. The predictor updates local patches sequentially following a path driven by the estimated condences. The path scheduler leverages both the condent and uncondent samples from the predic tor to further calibrate the estimated condence. The uncondent samples are usually images with low quality( e.g.the leftmost image is a clock with only top part), hard examples( e.g.the middle image is a spoon mixed with the background) or outofclass samples( e.g.the rightmost image is a lamp but none of the labeled samples belong to this class). The lower left gure experimentally shows that the proposed method improves the reliability of propagation. When the distance from unlabeled samples to labeled ones increases, our method surpasses stateoftheart by a signicant margin as a linear combination of those for its neighbors. Relying on simplistic assump tions restricts their capability of dealing with complicated graph structures in realworld datasets. Recently, graph convolutional networks [21,38,39] have re vealed its strong expressive power to process complex graph structures. Despite obtaining encouraging results, these GCNbased methods remain limited in an important aspect, namely the capability of coping with outliers in the graph. In realworld applications, unlabeled samples do not necessarily share the same classes with the labeled ones, leading to a large portion of outofclass samples, which becomes the main source of outliers. Existing methods ignore the fact that the condences of predictions on dierent samples can vary signicantly, which may adversely in uence the reliability of the predictions. In this paper, we aim to explore a new framework that can propagate labels over noisy unlabeled data reliably . This framework is designed based on three principles: 1) Local update: each updating step can be carried out within a local part of the graph, such that the algorithm can be easily scaled out to a largescale graph with millions of vertices. 2) Learnable: the graph structures over a real world dataset are complex, and thus it is dicult to prescribe a rule that works well for all cases, especially for various unknown outliers. Hence, it is desirable toLearn to Propagate Reliably on Noisy Anity Graphs 3 have a core operator with strong expressive power that can be learned from real data. 3) Reliable path: graphbased propagation is sensitive to noises { a noisy prediction can mislead other predictions downstream. To propagate reliably, it is crucial to choose a path such that most inferences are based on reliable sources. Specically, we propose a framework comprised of two learnable components, namely, a local predictor and a path scheduler. The local predictor is a light weight graph neural network operating on local subgraphs, which we refer to asgraph patches , to predict the labels of unknown vertices. The path scheduler is driven by condence estimates, ensuring that labels are gradually propagated from highly condent parts to the rest. The key challenge in designing the path scheduler is how to estimate the condences eectively. We tackle this problem via a twostage design. First, we adopt a multiview strategy by exploiting the fact that a vertex is usually covered by multiple graph patches , where each patch may project a dierent prediction on it. Then the condence can be evaluated on how consistent and certain the predictions are. Second, with the estimated condence, we construct a candidate set by selecting the most condent samples and the most uncondent ones. As illustrated in Fig. 1, we devise a ConfNet to learn from the candidate set and calibrate the condence estimated from the rst stage. Highly condent samples are assumed to be labeled and used in later propagation, while highly uncondent samples are assumed to be outliers and excluded in later propagation. Both components work closely together to drive the propagation process. On one hand, the local predictor follows the scheduled path to update predictions; on the other hand, the path scheduler estimates condences based on local predictions. Note that the training algorithm also follows the same coupled procedure, where the parameters of the local predictor and condence estimator are learned endtoend. Our main contributions lie in three aspects: (1) A learnable framework that involves a local predictor and a path scheduler to drive propagation reliably on noisy largescale graphs. (2) A novel scheme of exploiting both condent and uncondent samples for condence estimation. (3) Experiments on ImageNet [6] and MsCeleb1M [9] show that our proposed approach outperforms previous algorithms, especially when the graphs are noisy and the initial seeds are sparse. 2 Related Work "
394,Disparity Between Batches as a Signal for Early Stopping.txt,"We propose a metric for evaluating the generalization ability of deep neural
networks trained with mini-batch gradient descent. Our metric, called gradient
disparity, is the $\ell_2$ norm distance between the gradient vectors of two
mini-batches drawn from the training set. It is derived from a probabilistic
upper bound on the difference between the classification errors over a given
mini-batch, when the network is trained on this mini-batch and when the network
is trained on another mini-batch of points sampled from the same dataset. We
empirically show that gradient disparity is a very promising early-stopping
criterion (i) when data is limited, as it uses all the samples for training and
(ii) when available data has noisy labels, as it signals overfitting better
than the validation data. Furthermore, we show in a wide range of experimental
settings that gradient disparity is strongly related to the generalization
error between the training and test sets, and that it is also very informative
about the level of label noise.","Earlystopping using a separate validation set is one of the most popular techniques used to avoid under/over Ô¨Åtting deep neural networks trained with iterative methods, such as gradient descent [15, 42, 56]. The optimization is stopped when the performance of the model on a validation set starts to diverge from its performance on the training set. Early stopping requires an accurately labeled validation set, separated from the training set, to act as an unbiased proxy on the unseen test error. Obtaining such a reliable validation set can be expensive in many realworld applications as data collection is a timeconsuming process that might require domain expertise. Furthermore, deep learning is becoming popular in applications for which there is simply not enough available data [22, 46]. Finally, inexperienced label collectors, complex tasks (e.g., distinguishing a guinea pig from a hamster), and corrupted labels due for instance to adversarial attacks result in datasets that contain noisy labels [12]. Deep neural networks have the unfortunate ability to overÔ¨Åt to such small and/or noisy labeled datasets, an issue that cannot be completely solved by popular regularization techniques [59]. A signal of overÔ¨Åtting during training is therefore particularly useful, if it does notneed a separate, accurately labeled validation set, which is the purpose of this paper. LetS1andS2be two minibatches of points sampled from the available (training) dataset. Suppose that S1 is selected for an iteration (step) of the minibatch gradient descent (SGD), at the end of which the parameter vector is updated to w1. The average loss over S1(denoted by LS1(hw1)) is in principle reduced, given a suÔ¨Éciently small learning rate. The average loss LS2(hw1)over the other minibatch S2is not as likely to be reduced. It is more likely to remain larger than the loss LS2(hw2)computed over S2, if it wasS2instead of S1that had been selected for this iteration. The diÔ¨Äerence R2=LS2(hw1) LS2(hw2)is the penalty that we pay for choosing S1overS2(and similarly,R1is the penalty that we would pay for choosing S2overS1). R2is illustrated in Fig. 1 for a hypothetical nonconvex loss as a function of a one dimensional parameter w. The expected penalty measures how much, in an iteration, a model updated on one minibatch ( S1) is able to generalize on average to another minibatch ( S2) from the dataset. Hence, we call Rthegeneralization penalty . We establish a probabilistic upper bound on the sum of the expected penalties E[R1]+E[R2]by adapting the PACBayesian framework [33 ‚Äì35], given a pair of minibatches S1andS2sampled from the dataset (Theorem 1). Interestingly, under some mild assumptions, this upper bound is essentially a simple expression 1arXiv:2107.06665v1  [cs.LG]  14 Jul 2021Table 1: Test loss and area under the receiver operating characteristic curve (AUC score) of the MRNet dataset [4] when using 5fold crossvalidation (5fold CV) and gradient disparity (GD) as early stopping criteria for detecting the presence of abnormally, ACL tears, and meniscal tears from the sagittal plane MRI scans. The corresponding curves during training are shown in Fig. 15 (see Appendix F.3 for more details). The results of early stopping are given, both when the metric (GD or validation loss) has increased for 5 epochs from the beginning of training and between parenthesis when the metric has increased for 5 consecutive epochs. Using GD outperforms 5fold CV with either choice of the early stopping threshold. The standard deviations are obtained from 5 runs. Task Method Test Loss Test AUC Score (in percentage) Abnormal5fold CV 0:2840:016(0:3070:057) 71:0163:66(87:441:35) GD 0:2740:004(0:2750:053) 72:673:85(88:120:35) ACL5fold CV 0:9730:111(1:2460:142) 79:801:23(89:321:47) GD 0:8420:101(1:1360:121) 81:811:64(91:520:09) Meniscal5fold CV 0:7580:04(1:1630:127) 73:531:30(72:140:74) GD 0:7260:019(1:140:323) 74:080:79(73:800:24) driven bykg1 g2k2, whereg1andg2are the gradient vectors over the two minibatches S1andS2, respectively. We call it gradient disparity : it measures how much a small gradient step on one minibatch negatively aÔ¨Äects the performance on the other one. Figure 1: An illustration of the penalty term R2, where the yaxis is the loss, and the xaxis indicates the pa rameter of the model. LS1andLS2are the average losses over minibatches S1andS2, respectively. w(t)is the parameter at iteration tandw(t+1) iis the parameter at iteration t+ 1if batchSiwas selected for the update step at iteration t, withi2f1;2g.We propose gradient disparity as an eÔ¨Äective early stopping criterion, because of its computational tractability that makes it simple to use during the course of training, and because of its strong link with generalization error, as evidenced in the experiments that we run on stateoftheart conÔ¨Ågurations. Gra dient disparity is particularly well suited when the available dataset has limited labeled data, because it does not require splitting the available dataset into training and validation sets: all the available data can be used during training, unlike for instance kfold crossvalidation. We observe that using gradient dis parity, instead of an unbiased validation set, results in a predictive improvement of at least 1%for clas siÔ¨Åcation tasks with limited and very costly available data, such as the MRNet dataset, which is a small size imageclassiÔ¨Åcation dataset used for detecting knee injuries (Table 1). Moreover, we Ô¨Ånd that gradient disparity is a more accurate early stopping criterion than validation loss when the available dataset contains noisy labels. Gradient disparity reÔ¨Çects the label noise level quite well throughout the training process, especially at early stages of training. Finally, we observe that gradient disparity has a strong positive correlation with the test error across experimental settings that diÔ¨Äer in training set size, batch size, and network width. Code to reproduce our results is available at https://github.com/mahf93/disparity_early_stopping . 2 Related Work "
395,ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks.txt,"To train robust deep neural networks (DNNs), we systematically study several
target modification approaches, which include output regularisation, self and
non-self label correction (LC). Two key issues are discovered: (1) Self LC is
the most appealing as it exploits its own knowledge and requires no extra
models. However, how to automatically decide the trust degree of a learner as
training goes is not well answered in the literature? (2) Some methods penalise
while the others reward low-entropy predictions, prompting us to ask which one
is better?
  To resolve the first issue, taking two well-accepted propositions--deep
neural networks learn meaningful patterns before fitting noise [3] and minimum
entropy regularisation principle [10]--we propose a novel end-to-end method
named ProSelfLC, which is designed according to learning time and entropy.
Specifically, given a data point, we progressively increase trust in its
predicted label distribution versus its annotated one if a model has been
trained for enough time and the prediction is of low entropy (high confidence).
For the second issue, according to ProSelfLC, we empirically prove that it is
better to redefine a meaningful low-entropy status and optimise the learner
toward it. This serves as a defence of entropy minimisation.
  We demonstrate the effectiveness of ProSelfLC through extensive experiments
in both clean and noisy settings. The source code is available at
https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021.
  Keywords: entropy minimisation, maximum entropy, confidence penalty, self
knowledge distillation, label correction, label noise, semi-supervised
learning, output regularisation","There exist many target (label) modiÔ¨Åcation approaches. They can be roughly divided into two groups: (1) Output regularisation (OR), which is proposed to penalise over conÔ¨Ådent predictions for regularising deep neural networks. It includes label smoothing (LS) [42, 29] and conÔ¨Ådence penalty (CP) [33]; (2) Label correction (LC). On the one *Prof. David A. Clifton was supported by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC).hand, LC regularises neural networks by adding the simi larity structure information over training classes into one hot label distributions so that the learning targets become structured and soft . On the other hand, it can correct the semantic classes of noisy label distributions. LC can be further divided into two subgroups: Nonself LC and Self LC. The former requires extra learners, while the latter re lies on the model itself. A typical approach of Nonself LC is knowledge distillation (KD), which exploits the pre dictions of other model(s), usually termed teacher(s) [17]. Self LC methods include PseudoLabel [23], bootstrapping (Bootsoft and Boothard) [35], Joint Optimisation (Joint soft and Jointhard) [43], and TfKD self[55]. According to an overview in Figure 1 (detailed derivation is in Section 3 and Table 1), in label modiÔ¨Åcation, the output target of a data point is deÔ¨Åned by combining a onehot label distribu tion and its corresponding prediction or a predeÔ¨Åned label distribution . Firstly, we present the drawbacks of existing approaches: (1) OR methods naively penalise conÔ¨Ådent outputs without leveraging easily accessible knowledge from other learners or itself (Figure 1a); (2) Nonself LC relies on accurate aux iliary models to generate predictions (Figure 1b). (3) Self LC is the most appealing because it exploits its own knowl edge and requires no extra learners. However, there is a core question that is not well answered: In Self LC, how much should we trust a learner to leverage its knowledge? As shown in Figure 1b, in Self LC, for a data point, we have two labels: a predeÔ¨Åned onehot qand a predicted structured p. Its learning target is (1 )q+p, i.e., a tradeoff between qandp, wheredeÔ¨Ånes the trust score of a learner. In existing methods, is Ô¨Åxed without consid ering that a model‚Äôs knowledge grows as the training pro gresses. For example, in bootstrapping, is Ô¨Åxed through out the training process. Joint Optimisation stagewisely trains a model. It fully trusts predicted labels and uses them to replace old ones when a stage ends, i.e., = 1. Tf KDselftrains a model by two stages: = 0in the Ô¨Årst one whileis tuned for the second stage. Note that pis genarXiv:2005.03788v6  [cs.LG]  2 Jun 2021LS CP 1/3 1/3 1/31 0 0 1/2 1/3 1/61 0 0 0 0 1   1  (a) OR includes LS [42] and CP [33]. LS softens a target by adding a uniform label distribution. CP changes the probability 1 to a smaller value 1 in the onehot target. The doubleended arrow means factual equivalence, because an output is deÔ¨Ånitely nonnegative after a softmax layer. 1/2 1/3 1/61 0 0 1/2 1/6 1/31 0 0 Self LC Nonself LC  target  learner  target  learner extra  learner  1   1  (b) LC contains Self LC [23, 35, 43, 55] and Nonself LC [17]. The parameter deÔ¨Ånes how much a predicted label distribution is trusted. Figure 1: Target modiÔ¨Åcation includes OR (LS and CP), and LC (Self LC and Nonself LC). Assume there are three training classes. qis the onehot target. uis a uniform label distribution. pdenotes a predicted label distribution. The target combination parameter is 2[0;1]. erated by a precedingstage model in stagewise training, which requires signiÔ¨Åcant human intervention and is time consuming in practice. To improve Self LC, we propose a novel method named Progressive Self Label Correction (ProSelfLC), which is endtoend trainable and needs negligible extra cost. Most importantly, ProSelfLC modiÔ¨Åes the target progressively and adaptively as training goes. Two design principles of ProSelfLC are : (1) When a model learns from scratch, hu man annotations are more reliable than its own predictions in the early phase, during which the model is learning sim ple meaningful patterns before Ô¨Åtting noise, even when se vere label noise exists in human annotations [3]. (2) As a learner attains conÔ¨Ådent knowledge as time progresses, we leverage it to revise annotated labels. This is surrounded by minimum entropy regularisation, which is widely evaluated in unsupervised and semisupervised scenarios [9, 10]. Secondly, note that OR methods penalise low entropy while LC rewards it, intuitively leading to a second vital question: Should we penalise a lowentropy status or reward it? Entropy minimisation is the most widely used principle in machine learning [14, 38, 9, 10, 22]. In standard classiÔ¨Åca tion, minimising categorical cross entropy (CCE) optimises a model towards a lowentropy status deÔ¨Åned by human an notations, which contain noise in very largescale machine learning. As a result, conÔ¨Ådence penalty becomes popular for reducing noisy Ô¨Åtting. In contrast, we prove that it is better to reward a meaningful lowentropy status redeÔ¨Åned by our ProSelfLC. Therefore, our work offers a defence ofentropy minimisation against the recent conÔ¨Ådence penalty practice [42, 29, 33, 6]. Finally, we summarise our main contributions: ‚Ä¢ We provide a theoretical study on popular target mod iÔ¨Åcation methods through entropy and KL divergence [21]. Accordingly, we reveal their drawbacks and pro pose ProSelfLC as a solution. ProSelfLC can: (1) en hance the similarity structure information over training classes; (2) correct the semantic classes of noisy label distributions. ProSelfLC is the Ô¨Årst method to trust self knowledge progressively and adaptively. ‚Ä¢ Our extensive experiments: (1) defend the entropy minimisation principle; (2) demonstrate the effective ness of ProSelfLC in both clean and noisy settings. 2. Related Work "
396,Enhancing Diversity in Teacher-Student Networks via Asymmetric branches for Unsupervised Person Re-identification.txt,"The objective of unsupervised person re-identification (Re-ID) is to learn
discriminative features without labor-intensive identity annotations.
State-of-the-art unsupervised Re-ID methods assign pseudo labels to unlabeled
images in the target domain and learn from these noisy pseudo labels. Recently
introduced Mean Teacher Model is a promising way to mitigate the label noise.
However, during the training, self-ensembled teacher-student networks quickly
converge to a consensus which leads to a local minimum. We explore the
possibility of using an asymmetric structure inside neural network to address
this problem. First, asymmetric branches are proposed to extract features in
different manners, which enhances the feature diversity in appearance
signatures. Then, our proposed cross-branch supervision allows one branch to
get supervision from the other branch, which transfers distinct knowledge and
enhances the weight diversity between teacher and student networks. Extensive
experiments show that our proposed method can significantly surpass the
performance of previous work on both unsupervised domain adaptation and fully
unsupervised Re-ID tasks.","Person reidentiÔ¨Åcation (ReID) targets at retrieving a person of interest across nonoverlapping cameras. Since there are domain gaps resulting from illumination condi tion, camera property and viewpoint variation, a ReID model trained on a source domain usually shows a huge per formance drop on other domains. Unsupervised domain adaptation (UDA) targets at shift ing the model trained from a source domain with identity annotation to a target domain via learning from unlabeled target images. In the real world, unlabeled images in a target domain can be easily recorded, which is almost laborfree. It is intuitive to use these images to adapt a pretrained Re ID model to the desired domain. Fully unsupervised ReID 1Code at https://github.com/chenhao2345/ABMT .further minimises the supervision by removing pretraining on the labelled source domain. Stateoftheart UDA Person ReID methods [8, 27] and unsupervised methods [17] assign pseudo labels to unla beled target images. The generated pseudo labels are gener ally very noisy. The noise is mainly from several inevitable factors, such as the strong domain gaps and the imperfection of clustering. In this way, an unsupervised ReID problem is naturally transferred into Generating pseudo labels and Learning from noisy labels problems. To generate pseudo labels, the most intuitive way is to use a clustering algorithm, which gives a good starting point for clustering based UDA ReID [29, 6]. Recently, Ge et al. [8] propose to add a Mean Teacher [23] model as online soft pseudo label generator, which effectively reduces the error ampliÔ¨Åcation during the training with noisy labels. In this paper, we also use both clusteringbased hard labels and teacherbased soft labels in our baseline. To handle noisy labels, one of the most popular ap proaches is to train paired networks so that each network helps to correct its peer, e.g., twostudent networks in Co teaching [9] and twoteachertwostudent networks in MMT [8]. However, these paired models with identical struc ture are prone to converge to each other and get stuck in a local minimum. There are several attempts to alleviate this problem, such as Coteaching+ [28], ACT [27] and MMT [8]. These attempts of keeping divergence between paired models are mainly based on either different train ing sample selection [28, 27] or different initialization and data augmentation[8]. In this paper, we propose a strong alternative by designing asymmetric neural network struc ture in the Mean Teacher Model. We use two independent branches with different depth and global pooling methods as last layers of a neural network. Features extracted from both branches are concatenated as the appearance signa ture, which enhances the feature diversity in the appearance signature and allows to get better clusteringbased hard la bels. Each branch gets supervision from its peer branch of different structure, which enhances the divergence between paired teacherstudent networks. Our proposed decoupling method does not rely on different source domain initializaarXiv:2011.13776v1  [cs.CV]  27 Nov 2020tions, which makes it more effective in the fully unsuper vised scenario where the source domain is not available. In summary, our contributions are: 1. We propose to enhance the feature diversity inside person ReID appearance signatures by splitting last layers of a backbone network into two asymmetric branches, which increases the quality of clustering based hard labels. 2. We propose a novel decoupling method where asym metric branches get crossbranch supervision, which avoids weights in paired teacherstudent networks con verging to each other and increases the quality of teacherbased soft labels. 3. Extensive experiments and ablation study are con ducted to validate the effectiveness of each proposed component and the whole framework. 2. Related Work "
397,Semi-supervised learning of deep metrics for stereo reconstruction.txt,"Deep-learning metrics have recently demonstrated extremely good performance
to match image patches for stereo reconstruction. However, training such
metrics requires large amount of labeled stereo images, which can be difficult
or costly to collect for certain applications. The main contribution of our
work is a new semi-supervised method for learning deep metrics from unlabeled
stereo images, given coarse information about the scenes and the optical
system. Our method alternatively optimizes the metric with a standard
stochastic gradient descent, and applies stereo constraints to regularize its
prediction. Experiments on reference data-sets show that, for a given network
architecture, training with this new method without ground-truth produces a
metric with performance as good as state-of-the-art baselines trained with the
said ground-truth. This work has three practical implications. Firstly, it
helps to overcome limitations of training sets, in particular noisy ground
truth. Secondly it allows to use much more training data during learning.
Thirdly, it allows to tune deep metric for a particular stereo system, even if
ground truth is not available.","The stereo reconstruction problem consists in estimat ing a depth map from two images taken from differ ent viewpoints. The problem has many practical appli cations in robotics [ 34], remote sensing [ 43], and 3D graphics[ 47]. It has been heavily investigated for several decades [ 40], and recent developments focused on designing highorder, regionbased and objectspeciÔ¨Åc priors [60,10,55,17,24,29,52,51], and improving ‚àóstepan.tulyakov@epÔ¨Ç.chefÔ¨Åciency of large scale stereo [ 36,25,16,7]. Perhaps the most signiÔ¨Åcant recent breakthrough was to use deep metrics [ 12,58]. It led to considerable gains in processing speed and reconstruction accuracy (see Tables4,5, and6). Our work improvesupon this line ofresearch. 2 Related work "
398,Meta Clustering Learning for Large-scale Unsupervised Person Re-identification.txt,"Unsupervised Person Re-identification (U-ReID) with pseudo labeling recently
reaches a competitive performance compared to fully-supervised ReID methods
based on modern clustering algorithms. However, such clustering-based scheme
becomes computationally prohibitive for large-scale datasets. How to
efficiently leverage endless unlabeled data with limited computing resources
for better U-ReID is under-explored. In this paper, we make the first attempt
to the large-scale U-ReID and propose a ""small data for big task"" paradigm
dubbed Meta Clustering Learning (MCL). MCL only pseudo-labels a subset of the
entire unlabeled data via clustering to save computing for the first-phase
training. After that, the learned cluster centroids, termed as meta-prototypes
in our MCL, are regarded as a proxy annotator to softly annotate the rest
unlabeled data for further polishing the model. To alleviate the potential
noisy labeling issue in the polishment phase, we enforce two well-designed loss
constraints to promise intra-identity consistency and inter-identity strong
correlation. For multiple widely-used U-ReID benchmarks, our method
significantly saves computational cost while achieving a comparable or even
better performance compared to prior works.","Ubiquitous cameras generate innumerable pedestrian data every day. Due to the growing demands on person reidentification (ReID) and its expensive labeling cost, unsupervised person ReID (UReID) [ 9, 12,14,22,30,31,33,40,49,63,65,67,78] has attracted increasing attention recently. There are mainly two categories in UReID. One is unsupervised domain adaptive (UDA) person ReID, which first pretrains a model on the labeled source dataset, and then finetunes the model on the unlabeled target dataset to reduce domain gap [ 11,35,55,58, 65,75,76]. Albeit effective, UDA ReID branch typically suffers from a complex adaptation process, and its success also relies on an assumption that the discrepancy between source and target domain is not significant. This motivates the exploration on the other branch, the clusteringbased unsupervised ReID [ 9,14,18,22, 36,54]. As the ‚ÄúPrevious work‚Äù shown in Figure 1, the works of this branch tend to perform an iterative optimization process of feature extraction‚Äìclustering‚Äìtrain . In this way, all unlabeled data can be explicitly leveraged with the pseudo labels generated by clustering. The focus of the recent clusteringbased methods lies in creating more reliable clusters and efficiently using them to learn discriminative representations, e.g., with the help of selfsimilarity grouping [ 18], hybrid memory bank with contrastive loss [ 22] or clusterlevel memory bank [ 9], multilabel classification [ 54], and online hierarchical cluster dynamics [66, 71]. However, these methods all neglect an important fact in prac tice: the clustering process costs an intolerable computational re sources due to its pairwise similarity calculation and neighboring samples searching. Taking the most common clustering algorithm DBScan [ 13] in UReID as example, its worst time complexity and space complexity are both ùëÇ(ùëõ2). When the size of unlabeled data is very large (as shown in Figure 1(a)), both of the memory and timearXiv:2111.10032v4  [cs.CV]  6 Aug 2022MM ‚Äô22, October 10‚Äì14, 2022, Lisboa, Portugal Xin Jin, Tianyu He, Xu Shen, Tongliang Liu, Xinchao Wang, Jianqiang Huang, Zhibo Chen, and XianSheng Hua Epoch0             10            20            30            40            50Ration of labeling errors (%) 0.0        0.2        0.4       0.6        0.8     Memory cost (MB) 0       5000    10000   15000   20000 0            15k          30k           45k          60k          75k Number of training imagesTime cost (s) 500     400       300       200       100         0 PersonX : 9.8k imgs 410IDs 449MB 33sMarket1501: 12.9k imgs 751IDs 723MB 43sMSMT17: 32.6k imgs 1041 IDs 4117MB 137sLaST : 71.2k imgs 5000IDs 22399MB 495s(a) (b)  (2) ClusteringPrevious  workOur work (2) Meta  Clustering Problems Figure 1: Motivation illustration for clusteringbased unsu pervised ReID: as the size of unlabeled data increases, the clustering process in previous works will (a) cost an intoler able computational resources in terms of memory and time costs, and (b) be more prone to be affected by pseudo label noise. Our work introduces a new metaclustering learning to achieve a satisfactory UReID performance while simul taneously tackling these two challenges. cost of clustering will rapidly increase. For example, performing clustering once on the LaST [ 45] (71.2k images) on the GPU follow ing previous works [ 9,22,30] will take a memory usage up to 22GB , which can not run on a 16GB Tesla V100. One may ask why not use the offline clustering (on CPU) or batchwise local clustering ( e.g., Kmeans) to avoid a large memory and time cost. This is due to the specificity of ReID: (1) the clusteringbased ReID needs iteratively perform the feature extraction and clustering in feature domain (on GPU) [ 9,22,54]; (2) the batchwise local clustering for ReID is suboptimal, which hinders the exploration and utilization of global relationship among largescale person data. In this paper, we attempt to achieve a largescale unsupervised ReID framework while taking the computational cost into account, which is challenging but valuable and meaningful to bridge the gap between ReID algorithms and practical applications. To this end, we propose a ‚Äúsmall data for big task‚Äù paradigm dubbed Meta Clustering Learning (MCL). Inspired by the other concept of Meta Learning [ 51‚Äì53] that are designed for ‚Äòlearning to learn‚Äô with the assistance of meta knowledge, our MCL first obtain the meta knowl edge on a part of the unlabeled person data and then softly extend the knowledge to the rest unlabeled ones. Therefore, it naturally avoids clustering the full/whole dataset before each training epoch and thus reduces the computation overhead. In addition, during the knowledge extension process, MCL further leverages a clustering free polishing step to enhance the discriminative representation learning while alleviating noisy label issue for ReID model. As illustrated in Figure 1, MCL consists of two phases of meta prototype optimization and prototypereferenced polishment (see Sec. 3.1, 3.2 for details). In the first phase, the features of the partialunlabeled images are extracted. This ratio can be flexibly deter mined according to the computing power of practical environment, as a byproduct of MCL. Then, a clustering algorithm, like DB Scan [ 13], is used to cluster features and generate pseudo ID labels. Based on them, the ReID model is trained with a memorybased optimization strategy [ 9,22,54]. Meanwhile, the clustered centroids (termed as metaprototypes) are stored in the memory and updated on the fly in a momentum manner [24]. The second prototypereferenced polishment is based on the learned metaprototypes in the previous phase, which are taken as a proxy annotator to mine the potential label information for the rest unlabeled data . For each unused person image, we get a soft real valued label likelihood vector by comparing it with metaprototypes reference. Based on such clusteringfree pseudo label, we further polish model by mining the relative comparative characteristic in person images. The reason why we call this phase as ‚Äúpolishment‚Äù is because ‚Äúpolish‚Äù has the meaning of try to perfect one‚Äôs skill, like here we promote the discriminative feature learning for ReID model with the rest unlabeled data . Another point should be noticed is that, the pseudo labeling it self no matter of clusteringbased or referencebased may generate wrong label predictions [ 20,30,54,60]. As shown in Figure 1(b), the larger size of unlabeled dataset, the more possible of generating noisy labels. To alleviate it, we further leverage two loss constraints for label denoising in MCL. One loss enforces instancelevel consis tency to reduce intraidentity variance and the other constructs a softweighted triplet constraint to promise interidentity correla tion. In this way, MCL could better investigate the discriminative information of data even with noisy pseudo labels. We summarize our main contributions as follows: ‚Ä¢To our best knowledge, this paper is the first to achieve the unsupervised largescale ReID training while considering the computational cost savings. A ‚Äúsmall data for big task‚Äù paradigm dubbed Meta Clustering Learning (MCL) is proposed. MCL per forms clusteringbased ReID training on partial unlabeled data, saving computing resources. ‚Ä¢To further leverage the rest unlabeled data, we take the learned prototypes from partial data as proxy annotator to pseudolabel them, and then polish model based on such pseudo labels with two welldesigned losses (as a minor contribution) to promise intraidentity consistency and interidentity strong correlation, which helps alleviate the noisy label issue. ‚Ä¢As the first attempt to handle the largescale unsupervised ReID, extensive experiments on multiple benchmarks show that MCL could significantly save computational cost while achieving a stateoftheart performance. In particular, MCL achieves ReID performance improvements of 4.8%, 2.9% in mAP on the large scale MSMT17 [ 58], LaST [ 45], but saves 71.8%/87.9% memory costs and 73.7%/85.7% time costs compared to the baselines. 2 RELATED WORK "
399,DST: Data Selection and joint Training for Learning with Noisy Labels.txt,"Training a deep neural network heavily relies on a large amount of training
data with accurate annotations. To alleviate this problem, various methods have
been proposed to annotate the data automatically. However, automatically
generating annotations will inevitably yields noisy labels. In this paper, we
propose a Data Selection and joint Training (DST) method to automatically
select training samples with accurate annotations. Specifically, DST fits a
mixture model according to the original annotation as well as the predicted
label for each training sample, and the mixture model is utilized to
dynamically divide the training dataset into a correctly labeled dataset, a
correctly predicted set and a wrong dataset. Then, DST is trained with these
datasets in a supervised manner. Due to confirmation bias problem, we train the
two networks alternately, and each network is tasked to establish the data
division to teach another network. For each iteration, the correctly labeled
and predicted labels are reweighted respectively by the probabilities from the
mixture model, and a uniform distribution is used to generate the probabilities
of the wrong samples. Experiments on CIFAR-10, CIFAR-100 and Clothing1M
demonstrate that DST is the comparable or superior to the state-of-the-art
methods.","The remarkable success on training deep neural networks (DNNs) in various tasks relies on a largescale dataset with the correctly labels. However, labeling large amounts of data with highquality annotations is expensive and time consuming. Although there are some alternative and inex pensive methods such as crowdsourcing [ 36,39], online queries [ 4] and labelling samples with the annotator [ 27] that can annotate the largescale datasets easily to alleviate this problem, the samples with noisy labels are yielded by these alternative methods. A recent study [ 40] shows that a dataset with noisy labels can be overÔ¨Åtted by DNNs and *Corresponding author. (a) Warmup for 15 epochs  (b) DST for 50 epochs Figure 1. Distributions of the normalized loss on CIFAR10 with 80% symmetric noise. We use GMM to select samples for small loss and DST. Top: smallloss; bottom: DST. (a) 3007 samples (2816 correct samples) selected by smallloss and 7771 samples (6964 correct samples) selected by DST; (b) 10503 samples (9889 correct samples) selected by smallloss and 17897 samples (17054 correct samples) selected by DST. leads to poor generalization performance of the model. As this problem generally exists in the neural network training process and makes models get poor generalization, there are many algorithms developed for Learning with Noisy Labels(LNL). Some of methods attempt to estimate the latent noise transition matrix to express noisy labels and correct the loss function [ 8,19,21]. However, how to cor rectly construct noise transition matrix is challenging. Some researches modify labels correctly by predictions of models for improving the model performance [ 23,26]. Because of training labels from the DNN, the model would easily lead to overÔ¨Åtting under a high noise ratio. The recent research [ 1] adopts MixUp [ 41] to address this problem. Another ap proach reduces the inÔ¨Çuence of noise on the training process by selecting or weighting samples [ 24]. Many methods se lect clean samples with small loss [ 1,12]. Coteaching [ 9], 1arXiv:2103.00813v1  [cs.CV]  1 Mar 2021Coteaching +[38] and JoCoR [ 34] use two networks to select smallloss samples to train each other. Despite smallloss is a good method to choose correct samples from the noise samples, the samples which is pre dicted correctly by the models are ignored in training pro cess. In this work, we propose DST (Data Selection and joint Training), which can leverage correctly predicted samples and avoid overÔ¨Åtting new labels chosen by the model under a high level of the noise ratio. Compared with other methods using small loss [ 1,9,38], we propose another method based on two kinds of the loss on each sample to distinguish sam ples with correctly labels for training networks. We provide experiments to demonstrate the feasibility of our approach, which is superior to many related approaches. Our main contributions are as follows: ‚Ä¢We propose two kinds of the sample loss (1. loss of the label from the dataset; 2. loss of the label predicted by model.), which can be used to distinguish correctly la beled and predicted samples. We Ô¨Åt a Gaussian Mixture Model (GMM) dynamically on dataset loss distribu tion to divide the dataset into correctly labeled samples, correctly predicted samples and wrong samples with wrong labels and predictions. ‚Ä¢We train two networks to generate losses of samples. For each network, we use GMM to get correct samples, which is then used to train another network. This can Ô¨Ålter different types of error and avoid conÔ¨Årmation bias in selftraining [16]. 2. Related Work "
400,Embedded Ensembles: Infinite Width Limit and Operating Regimes.txt,"A memory efficient approach to ensembling neural networks is to share most
weights among the ensembled models by means of a single reference network. We
refer to this strategy as Embedded Ensembling (EE); its particular examples are
BatchEnsembles and Monte-Carlo dropout ensembles. In this paper we perform a
systematic theoretical and empirical analysis of embedded ensembles with
different number of models. Theoretically, we use a Neural-Tangent-Kernel-based
approach to derive the wide network limit of the gradient descent dynamics. In
this limit, we identify two ensemble regimes - independent and collective -
depending on the architecture and initialization strategy of ensemble models.
We prove that in the independent regime the embedded ensemble behaves as an
ensemble of independent models. We confirm our theoretical prediction with a
wide range of experiments with finite networks, and further study empirically
various effects such as transition between the two regimes, scaling of ensemble
performance with the network width and number of models, and dependence of
performance on a number of architecture and hyperparameter choices.","A common strategy of improving accuracy of predic tive models is model ensembling [Dietterich, 2000]. In Proceedings of the 25thInternational Conference on Arti cial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022 by the au thor(s).its simplest form, several models are constructed inde pendently, and their outputs are averaged. Despite its simplicity, this strategy is very reliable and ecient, almost invariably improving the accuracy and robust ness of predictions [Dusenberry et al., 2020]. However, a major downside of this strategy is a sub stantial increase of resources required for model train ing and execution: training time, inference time, and required storage scale linearly with the number of models in the ensemble. This downside is especially acute for deep neural networks (DNNs) since they are already complex { as a result, DNN ensembles be come challenging or even infeasible in many applica tions [Schwenk and Bengio, 2000,Huang et al., 2017]. Recently signicant attention was paid to the con struction of \lightweight"" ensembles that mitigate this issue [Wen et al., 2019, Havasi et al., 2020, Ram e et al., 2021, Wenzel et al., 2020, Zhang et al., 2021]. A lightweight ensemble attempts to retain the accu racy gain from ensembling while relaxing requirements for a particular resource. For example, snapshot en sembles [Huang et al., 2017, Garipov et al., 2018] re duce the ensemble training time (without signicantly aecting the storage and inference time). Lightweight ensembles typically have a lower accuracy than the standard independent ensembles of the same size, be cause of a lower diversity of their members. In this work we address what we call Embedded Ensem bles(EE). Their common idea is to construct dierent models by some kind of perturbation of a single refer ence neural network. Examples of EE include Monte Carlo (MC) dropout ensembles [Gal and Ghahramani, 2016] and BatchEnsembles [Wen et al., 2019]. Most weights in an embedded ensemble are just the shared reference network weights, so this ensemble requires much less storage than a respective ensemble of inde pendent reference networks. Furthermore, if the per turbation is restricted to the last layers, then network computations can be eciently reused among ensemarXiv:2202.12297v1  [stat.ML]  24 Feb 2022Embedded Ensembles: Innite Width Limit and Operating Regimes bled models making computation time comparable to that of a single model. The price one pays for this eciency is the lower ac curacy of embedded ensembles. In fact, while the ac curacy of the usual independent ensembles only in creases with additional models, it was observed em pirically [Havasi et al., 2020] that the accuracy of em bedded ensembles can degrade when the number of ensemble members is large. The primary purpose of this work is to systemati cally explore how performance of embedded ensembles scales with the number of models. An additional im portant factor that we consider is the size of the ref erence network. Intuitively, larger reference networks can accommodate more uncorrelated models and so provide higher ensemble accuracy. We conrm this intuition, both theoretically and empirically. Our contribution. We perform an extensive theo retical and empirical study of Embedded Ensembles. ‚Ä¢We describe the behaviour of Embedded Ensem bles in the limit of innite reference network width. Particularly, we derive dynamic equation of EE model outputs describing their evolution under gradient descent. Also, we characterize at initialization the distribution of ensemble outputs and Neural Tangent Kernel of the ensemble. ‚Ä¢In the innite width limit we identify indepen dent and collective operating regimes of Embed ded Ensembles. In the independent regime we show that EE is fully identical to the ensemble of independent reference networks. Also, we propose to use dierent gradient scalings for independent and collective regimes to ensure proper behavior of EEs with large number of ensemble models. We show that the operating regime of Embedded En semble is determined by the structure of individ ual parameters and their initialization strategy. ‚Ä¢We perform extensive experiments with embed ded ensembles on the CIFAR100 data set. We empirically observe the collective and independent regimes and demonstrate the transition between them. We observe that nitewidth EEs in the independent regime have an optimal number of models at which the highest accuracy is achieved; in agreement with our theory this optimal number increases with the network width. We further ex plore, both empirically and theoretically, a num ber of architecture modications and the scaling of the learning rates.   model 1 model 2 model 3 common Figure 1: All models in the BatchEnsemble have com mon fullyconnected (or convolutional) weights (col ored black), and a small number of pre and post ac tivation modulations (colored red, green or blue) that dier for each model. For each model, only the respec tive family of modulations (i.e., red, green or blue) is active on a forward pass. 2 Related Work "
401,Positive-Unlabeled Learning with Uncertainty-aware Pseudo-label Selection.txt,"Positive-unlabeled (PU) learning aims at learning a binary classifier from
only positive and unlabeled training data. Recent approaches addressed this
problem via cost-sensitive learning by developing unbiased loss functions, and
their performance was later improved by iterative pseudo-labeling solutions.
However, such two-step procedures are vulnerable to incorrectly estimated
pseudo-labels, as errors are propagated in later iterations when a new model is
trained on erroneous predictions. To prevent such confirmation bias, we propose
PUUPL, a novel loss-agnostic training procedure for PU learning that
incorporates epistemic uncertainty in pseudo-label selection. By using an
ensemble of neural networks and assigning pseudo-labels based on
low-uncertainty predictions, we show that PUUPL improves the reliability of
pseudo-labels, increasing the predictive performance of our method and leading
to new state-of-the-art results in self-training for PU learning. With
extensive experiments, we show the effectiveness of our method over different
datasets, modalities, and learning tasks, as well as improved calibration,
robustness over prior misspecifications, biased positive data, and imbalanced
datasets.","Many realworld applications involve positive unlabeled (PU) datasets [ 1,2,3] in which only small parts of the data is labeled positive while the major ity is unlabeled. Learning from PU data can reduce development costs in many deep learning applicationsthat otherwise require annotations from experts such as medical image diagnosis [ 1], protein function predic tion [ 2], and it can even enable applications in settings where the measurement technology itself can not detect negative examples [3]. Approaches such as unbiased PU [ 4] and nonnegative PU [5] formulate this problem as costsensitive learn ing and sparked a stream of works on risk estima tors for PU learning (PUL). Others approach PUL as a twostep procedure, rst identifying and labeling some reliable negative examples, and then retraining the model based on this newly constructed labeled dataset [ 6]. These approaches show similarities with pseudolabeling in semisupervised classication set tings [ 7] and are generally able to reach higher perfor mance compared to other methods that use a single training iteration. Such pseudolabeling techniques are however especially vulnerable to incorrectly as signed labels of the selected examples as such errors will propagate and magnify in the retrained model, resulting in a negative feedback loop. This erroneous selection of unreliable pseudolabels occurs when wrong model predictions are associated with excessive model condence, which is accompanied by a distortion of the signal for the pseudolabel selection [ 8]. In recent literature on pseudolabeling, this problem, also re ferred to as conrmation bias [9], is recognized and successfully addressed by explicitly estimating the pre diction uncertainty [ 10,11]. While this is the case for semisupervised classication, recent selftraining ap proaches for PUL do not explore the use of uncertainty quantication for pseudolabeling in this context. Contributions: Motivated by this, we propose a 1arXiv:2201.13192v2  [stat.ML]  31 Aug 2022Train anensemblePredict with uncertainty Select with uncertainty PositiveunlabeledPseudolabeled FPRTPR(e) Ranking quality Network AUC: 0.55 Ensemble AUC: 0.81Figure 1: PUUPL is a pseudolabeling framework for PU learning that uses the epistemic uncertainty of an ensemble to select condent examples to pseudolabel. The ensemble can be trained with any PU loss for PU data while minimizing the crossentropy loss on the previously assigned pseudolabels. In a toy example, a single network is not very condent on most of the unlabeled data (a), resulting in many highcondence incorrect predictions and many lowcondence correct ones (c). The epistemic uncertainty of an ensemble is, on the other hand, very low on most of the unlabeled data (b), resulting in most correct predictions having low uncertainty and most incorrect predictions having high uncertainty (d). Thus, the estimated uncertainty by ensemble can be used more reliably to rank predictions and select correct ones (e). Retraining the model with an increased number of labeled samples will result in a slightly more accurate model, than can be used to predict new pseudolabels, which will further improve the model's performance, etc. novel framework for PUL that leverages uncertainty quantication to identify reliable examples to pseudo label (Fig. 1). In particular, 1.We introduce PUUPL (Positive Unlabeled, Uncer tainty aware PseudoLabeling), a novel framework that successfully overcomes the issue of conrma tion bias in PUL. 2.We evaluate our methods on a wide range of bench marks and PU datasets, achieving stateoftheart results in selftraining for PUL both with and without known positive class prior =p(y= 1). 3.Through extensive experiments we show that PU UPL results in very wellcalibrated predictions, is applicable to dierent data modalities such as im ages and text, can use any risk estimator for PUL and improve thereupon, and is robust to prior misspecication and class imbalance. These results show our framework to be highly reliable, extensible, and applicable in a wide range of real world scenarios.2 Related work "
402,Hand gesture detection in tests performed by older adults.txt,"Our team are developing a new online test that analyses hand movement
features associated with ageing that can be completed remotely from the
research centre. To obtain hand movement features, participants will be asked
to perform a variety of hand gestures using their own computer cameras.
However, it is challenging to collect high quality hand movement video data,
especially for older participants, many of whom have no IT background. During
the data collection process, one of the key steps is to detect whether the
participants are following the test instructions correctly and also to detect
similar gestures from different devices. Furthermore, we need this process to
be automated and accurate as we expect many thousands of participants to
complete the test. We have implemented a hand gesture detector to detect the
gestures in the hand movement tests and our detection mAP is 0.782 which is
better than the state-of-the-art. In this research, we have processed 20,000
images collected from hand movement tests and labelled 6,450 images to detect
different hand gestures in the hand movement tests. This paper has the
following three contributions. Firstly, we compared and analysed the
performance of different network structures for hand gesture detection.
Secondly, we have made many attempts to improve the accuracy of the model and
have succeeded in improving the classification accuracy for similar gestures by
implementing attention layers. Thirdly, we have created two datasets and
included 20 percent of blurred images in the dataset to investigate how
different network structures were impacted by noisy data, our experiments have
also shown our network has better performance on the noisy dataset.","There is increasing interest in developing computer tests that can completed remotely, away from the research centre. This provides convenience for participants, facilitates involvement in research for participants who live remotely and allows research studies to continue when restrictions on travel occur, such as during the COIVD 2019 pandemic. Our team are developing a new online test that analyses hand movement features associated with ageing that can be completed remotely from the research centre. Our test has included two different tests, the alternating hand movement test involves opening and closing the whole hand and the alternating Ô¨Ångertapping test involves opening and closing the Ô¨Ångers of each hand repeatedly. Both tests are considered an important test for evaluating motor function. Figure 1 shows how the Ô¨Ångertapping test is performed, the participants will be instructed to switch between ‚ÄòGesture 1‚Äô and ‚ÄòGesture 2‚Äô quickly and repeatedly. Since the video tests will request participants to open and close their Ô¨Ångers as fastarXiv:2110.14461v2  [cs.CV]  29 Oct 2021APREPRINT  NOVEMBER 1, 2021 as possible using relatively low fps laptop cameras, motion blur may be a signiÔ¨Åcant problem during the data collection process. Meanwhile, getting the participants to follow the instruction and recording the correct gestures are essential for our study as most participants will complete the tests unsupervised in their own homes. To obtain highquality video data in the hand movement tests, it is important to detect if users are following the instructions with the correct gestures. (a) Gesture 1  (b) Gesture 2 Figure 1: Finger tapping test demonstration Since the hand movement tests will request participants to open and close their Ô¨Ångers as fast as possible using relatively low fps laptop cameras, motion blur may be a signiÔ¨Åcant problem during the data collection process. Meanwhile, getting the participants to follow the instruction and recording the correct gestures are essential for the following study as most participants will complete the tests unsupervised in their own homes. To obtain highquality video data in the hand movement tests, it is important to detect if users are following the instructions with the correct gestures. In this case, hand gesture recognition will apply important implications in guiding participants to assist data collection for TAS Test hand movement tests. This paper has the following three contributions: 1.We have processed and labelled 7071 images, then built a hand gestures classiÔ¨Åcation dataset for dementia related gesture classiÔ¨Åcation and motion blur study 2. We have compared the performance of different network structures on the hand gesture classiÔ¨Åcation task 3.We proposed a novel approach and used the attention technique to increase the classiÔ¨Åcation performance on similar gestures. Our model is better than stateoftheart. 4.We have demonstrated how the quality of images will impact the performance on hand gesture classiÔ¨Åcation through two sets of experiments and have proven our network structure has better result than stateoftheart on noisy dataset. 2 Related Work "
403,Speaker attribution with voice profiles by graph-based semi-supervised learning.txt,"Speaker attribution is required in many real-world applications, such as
meeting transcription, where speaker identity is assigned to each utterance
according to speaker voice profiles. In this paper, we propose to solve the
speaker attribution problem by using graph-based semi-supervised learning
methods. A graph of speech segments is built for each session, on which
segments from voice profiles are represented by labeled nodes while segments
from test utterances are unlabeled nodes. The weight of edges between nodes is
evaluated by the similarities between the pretrained speaker embeddings of
speech segments. Speaker attribution then becomes a semi-supervised learning
problem on graphs, on which two graph-based methods are applied: label
propagation (LP) and graph neural networks (GNNs). The proposed approaches are
able to utilize the structural information of the graph to improve speaker
attribution performance. Experimental results on real meeting data show that
the graph based approaches reduce speaker attribution error by up to 68%
compared to a baseline speaker identification approach that processes each
utterance independently.","Speaker diarization is the problem of ‚Äúwho spoke when‚Äù, i.e., grouping the segments of a long audio recording into speaker homogeneous clusters. The conventional speaker diarization task assumes no prior knowledge of speakers‚Äô identities, so it is basically a clustering problem without speaker identiÔ¨Åcation. However, there are scenarios, such as meeting transcription, where voice proÔ¨Åles of speakers are available and the identi Ô¨Åcation of speakers is required. This task is called ‚Äòspeaker attribution‚Äô in this paper. A straightforward approach is to build a multiclass classiÔ¨Åer from the speaker proÔ¨Åles, and then clas sify the test segments onebyone. A drawback of this approach is treating test segments independently without considering the context when making predictions. For example, those test seg ments that are similar to each other should be assigned to the same speaker. Instead of predicting the speaker label for each speech segment independently, we propose to use graphbased semi supervised learning methods that use the structural information among speech segments within a session. Each speech seg ment, either from proÔ¨Åle audio or test audio, is represented as a node on a graph for each session. The feature of each *Work done by the Ô¨Årst author during internship at Microsoft. (a) Extract dvectors  (b) Build a graph (c) Add proÔ¨Åle segments  (d) Label prediction Figure 1: Overview of the proposed method: (a) extract d vectors of audio segments with a pretrained speaker embed ding model; (b, c) build a graph of audio segments based on pairwise similarities of the corresponding dvectors, using both proÔ¨Åle and test audio segments; (d) predict labels for test audio segments by graphbased semisupervised learning meth ods. node is represented by a Ô¨Åxeddimensional speaker embedding, e.g., dvectors [1, 2, 3, 4, 5, 6, 7], extracted from the corre sponding speech segment. Segments from the proÔ¨Åle audio are treated as labeled nodes while those from the test audio are un labeled nodes. The speaker attribution task can then be solved as a graphbased semisupervised learning problem, which can now utilize the structural information of the graph in order to improve the accuracy of classifying the test nodes. The intu ition is that if two nodes are similar to each other and share common neighbors on the graph, they are likely to have the same speaker label. Recently, graphbased methods have also been successfully applied on the conventional speaker diariza tion problem [8]. An overview of the proposed method is shown in Figure 1. First, we apply a pretrained speaker embedding model to ex tract dvectors for speech segments, which are obtained by uni formly segmenting the audio of one session after applying voice activity detection (V AD). Then a graph is built with both speech segments from proÔ¨Åle audio and test audio as shown in Fig ure 1(c), on which each node is a speech segment whose average dvector is used as the feature vector of the node. The weight of each edge represents the similarity between the correspondarXiv:2102.03634v1  [eess.AS]  6 Feb 2021ing segment pair. The proÔ¨Åle segments are labeled nodes on the graph, while test segments are unlabeled. To classify the unla beled nodes, we apply two graphbased semisupervised learn ing methods: a graph Laplacian regularizationbased approach (label propagation) and a graph embeddingbased approach by GNNs. Experiments show that both of these two methods sig niÔ¨Åcantly outperform the classiÔ¨Åcationbased methods on real multiparty meetings and present great potential for realworld applications. Our contributions can be summarized as: ‚Ä¢ we propose the Ô¨Årst solution to speaker attribution with speaker proÔ¨Åles through graphbased semisupervised learning methods; ‚Ä¢ we study two graphbased methods ‚Äì label propagation and GNNbased ‚Äì and their applications on a speaker attribution pipeline; and ‚Ä¢ we evaluate the proposed methods on real meeting data. Results show that the graphbased methods signiÔ¨Åcantly outperform the baseline method and present great poten tial for realworld applications. 2. Related work "
404,Learning to Align Multi-Camera Domains using Part-Aware Clustering for Unsupervised Video Person Re-Identification.txt,"Most video person re-identification (re-ID) methods are mainly based on
supervised learning, which requires cross-camera ID labeling. Since the cost of
labeling increases dramatically as the number of cameras increases, it is
difficult to apply the re-identification algorithm to a large camera network.
In this paper, we address the scalability issue by presenting deep
representation learning without ID information across multiple cameras.
Technically, we train neural networks to generate both ID-discriminative and
camera-invariant features. To achieve the ID discrimination ability of the
embedding features, we maximize feature distances between different person IDs
within a camera by using a metric learning approach. At the same time,
considering each camera as a different domain, we apply adversarial learning
across multiple camera domains for generating camera-invariant features. We
also propose a part-aware adaptation module, which effectively performs
multi-camera domain invariant feature learning in different spatial regions. We
carry out comprehensive experiments on three public re-ID datasets (i.e.,
PRID-2011, iLIDS-VID, and MARS). Our method outperforms state-of-the-art
methods by a large margin of about 20\% in terms of rank-1 accuracy on the
large-scale MARS dataset.","Person reidentiÔ¨Åcation (reID) [55, 27, 46, 5, 44, 35, 51, 39, 4] aims to match IDs of a personofinterest across multiple distinct camera views. These days videobased re ID [9, 43, 24, 56, 49, 12, 45] has been extensively stud ied in video surveillance systems for public safety. Among the reID methods, supervised learning approaches lead to substantial performance improvement. However, anno tating person IDs across multiple cameras entails signiÔ¨Å cantly high labor costs. This timeconsuming labeling work makes reidentiÔ¨Åcation systems hard to be applied in real world situations since the cost increases dramatically as the Figure 1: We map the features of people extracted from Imagenet pretrained neural networks to a 2D space using tSNE [30]. The numbers in different colors denote cam era domains. It shows that a large domain gap induces the cluster within a camera, which implies that controlling the camera domain gap is important to improve the ID discrim ination ability of reidentiÔ¨Åcation systems. number of cameras increases. Therefore, a line of work [33, 41, 6, 22, 47, 48, 46] focuses on unsupervised reID approaches that propose to learn IDdiscriminative feature representations without crosscamera person ID labeling. The major obstacle of unsupervised person re identiÔ¨Åcation is camera domain discrepancy generated by the difference of viewpoint and background as shown in Fig. 1. Recent studies focus on training a model from an additional labeled source dataset and transfer the knowl 1arXiv:1909.13248v4  [cs.CV]  13 May 2020edge to unlabeled target camera domains [5, 33, 42] or iteratively updating a training set using reliable top Ksam ples across multiplecamera domains [27, 47, 46, 23, 48]. However, performance degradation occurs while transfer ring knowledge between different domains. Also, adopting topKsampling is hard to select reliable samples when the camera domain gap is large. Therefore, even though the aforementioned two groups show promising performance in small camera systems [43, 13], the nature of imperfect translation and domain locality degrades the performance of large camera systems [53, 54]. The main motivation of this paper is to construct a feature space that considers the relationship of all cam eras without information transfer between camera domains. To this end, we propose a simple yet effective approach, named PartsAware camera Domain Alignment Learning (PADAL). With a carefully designed architecture, we Ô¨Årst maximize the distance between different ID features at each camera domain by using a metric learning approach. While maintaining a feature discrimination ability, we obtain the domaininvariant features by adopting the concept of do main adversarial learning [16, 37]. Note that our work dif fers from conventional domain adaptation methods in the point that every camera domain has a different number of IDs. This is because not all people move through all cam eras in the system. For example, considering a fourcamera system in a real application, it is possible that a person moves through cameras 1, 2, and 4 but does not appear in the camera 3. To further improve the effect of multicamera domain alignment, we propose a Partaware Adaptation Module (PAM). Most images used in person reidentiÔ¨Åcation are obtained by the detection algorithm [7] or hand labeling. Therefore, it is natural that each body part is located in a similar area in the reID images. For example, head ,torso , andlegsusually appear on the top, middle, and bottom part of images, respectively. We leverage this prior informa tion to reduce camera domain discrepancy. Technically, we make the clusters of the similar spatial features from the embedding feature maps by using the unsupervised cluster ing algorithm (i.e., Kmeans). According to the locations of clustered spatial features, we assign a specialized part domain discriminator for each cluster. We Ô¨Ånd that aligning part features more effectively generates camerainvariant features than aligning global features. To sum up, our main contributions can be summarized as follows: 1) We propose a novel learning scheme for un supervised video person reidentiÔ¨Åcation, named PADAL, which aims to construct the embedding space that repre sents camerainvariant and IDdiscriminative features. 2) To enhance the effectiveness of PADAL, we present a Part aware Adaptation Module (PAM) to effectively minimize the discrepancy from multiple camera domains by aligning partlevel distributions. 3) We conduct extensive exper iments on three public videobased person reID datasets (PRID2011 [13], iLIDSVID [43], and MARS [53]). Our experimental results show that PADAL is more effective on a large camera setting than other methods, which is an advantage to a realworld application. The proposed method improves video reID performance on the large scale MARS dataset up to 20% in rank1 accuracy. 2. Related Work "
405,CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images.txt,"We present a simple yet efficient approach capable of training deep neural
networks on large-scale weakly-supervised web images, which are crawled raw
from the Internet by using text queries, without any human annotation. We
develop a principled learning strategy by leveraging curriculum learning, with
the goal of handling a massive amount of noisy labels and data imbalance
effectively. We design a new learning curriculum by measuring the complexity of
data using its distribution density in a feature space, and rank the complexity
in an unsupervised manner. This allows for an efficient implementation of
curriculum learning on large-scale web images, resulting in a high-performance
CNN model, where the negative impact of noisy labels is reduced substantially.
Importantly, we show by experiments that those images with highly noisy labels
can surprisingly improve the generalization capability of the model, by serving
as a manner of regularization. Our approaches obtain state-of-the-art
performance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101.
With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on
the WebVision challenge for 1000-category classification. This result was the
top performance by a wide margin, outperforming second place by a nearly 50%
relative error rate. Code and models are available at:
https://github.com/MalongTech/CurriculumNet .","Deep convolutional networks have rapidly advanced numerous computer vision tasks, providing stateoftheart performance on image classication [9,31,34,14,37,8], object detection [28,27,22,20], sematic segmentation [23,11,4,10], etc. They pro duce strong visual features by training the networks in a fullysupervised manner using largescale manually annotated datasets, such as ImageNet [5], MSCOCO [21] and PASCAL VOC [6]. Full and clean human annotations are of crucial ?Weilin Huang is the corresponding author (email:whuang@malong.com).arXiv:1808.01097v4  [cs.CV]  18 Oct 20182 S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R. Scott, D. Huang Fig. 1. Image samples of the WebVision dataset [19] from the categories of Carton , Dog,Taxi andBanana . The dataset was collected from the Internet by using text queries generated from the 1, 000 semantic concepts of the ImageNet benchmark [5]. Each category includes a number of mislabeled images as shown on the right. importance to achieving a highperformance model, and better results can be reasonably expected if a larger dataset is provided with noisefree annotations. However, obtaining massive and clean annotations are extremely expensive and timeconsuming, rendering the capability of deep models unscalable to the size of collected data. Furthermore, it is particularly hard to collect clean annotations for tasks where expert knowledge is required, and labels provided by dierent annotators are possibly inconsistent. An alternative solution is to use the web as a source of data and supervision, where a large amount of web images can be collected automatically from the Internet by using input queries, such as text information. These queries can be considered as natural annotations of the images, providing weak supervision of the collected data, which is a cheap way to increase the scale of the dataset nearinnitely. However, such annotations are highly unreliable, and often in clude a massive amount of noisy labels. Past work has shown that these noisy labels could signicantly aect the performance of deep neural networks on im age classication [39]. To address this problem, recent approaches have been developed by proposing robust algorithms against noisy labels [30]. Another so lution is to develop noisecleaning methods that aim to remove or correct the mislabelled examples in training data [32]. However, the noisecleansing meth ods often suer from the main diculty in distinguishing mislabeled samples from hard samples, which are critical to improving model capability. Besides, semisupervised methods have also been introduced by using a small subset of manuallylabeled images, and then the models trained on this subset are gen eralized to a larger dataset with unlabelled or weaklylabelled data [36]. UnlikeCurriculumNet: Weakly Supervised Learning from LargeScale Web Images 3 these approaches, we do not aim to propose a noisecleaning, noiserobust or semisupervised algorithm. Instead, we investigate improving model capability of standard neural networks by introducing a new training strategy. In this work, we study the problem of learning convolutional networks from largescale images with a massive amount of noisy labels, such as the WebVision challenge [18], which is a 1000category image classication task having the same categories as ImageNet [5]. The labels are provided by simply using the queries text generated from the 1,000 semantic concepts of ImageNet [5], without any manual annotation . Several image samples are presented in Fig. 1. Our goal is to provide a solution able to handle massive noisy labels and data imbalance eectively. We design a series of experiments to investigate the impact of noisy labels on the performance of deep networks, when the amount of training im ages is suciently large. We develop a simple but surprisingly ecient training strategy that allows for improving model generalization and overall capability of the standard deep networks, by leveraging highly noisy labels. We observe that training a CNN from scratch using both clean and noisy data is more eective than just using the clean one. The contributions of this work are threefold: { We propose CurriculumNet by developing an ecient learning strategy with curriculum learning. This allows us to train highperformance CNN models from largescale web images with massive noisy labels, which are obtained without any human annotation. { We design a new learning curriculum by ranking data complexity using dis tribution density in an unsupervised manner. This allows for an ecient implementation of curriculum learning tailored for this task, by directly ex ploring highly noisy labels. { We conduct extensive experiments on a number of benchmarks, including WebVision [19], ImageNet [5], Clothing1M [39] and Food101 [2], where the proposed CurriculumNet obtains stateoftheart performance. The Curricu lumNet, with an ensemble of multiple models, archived the top performance with a top5 error rate of 5.2%, on the WebVision Challenge at CVPR 2017, outperforming the other results by a large margin. 2 Related work "
406,A generic ensemble based deep convolutional neural network for semi-supervised medical image segmentation.txt,"Deep learning based image segmentation has achieved the state-of-the-art
performance in many medical applications such as lesion quantification, organ
detection, etc. However, most of the methods rely on supervised learning, which
require a large set of high-quality labeled data. Data annotation is generally
an extremely time-consuming process. To address this problem, we propose a
generic semi-supervised learning framework for image segmentation based on a
deep convolutional neural network (DCNN). An encoder-decoder based DCNN is
initially trained using a few annotated training samples. This initially
trained model is then copied into sub-models and improved iteratively using
random subsets of unlabeled data with pseudo labels generated from models
trained in the previous iteration. The number of sub-models is gradually
decreased to one in the final iteration. We evaluate the proposed method on a
public grand-challenge dataset for skin lesion segmentation. Our method is able
to significantly improve beyond fully supervised model learning by
incorporating unlabeled data.","Image semantic segmentation is a basic and crucial step for many biomedical image analysis tasks (e.g. tumour quan tiÔ¨Åcation). Nowadays, many encoderdecoder based deep convolutional neural networks (DCNNs) such as UNet [1] achieve stateoftheart performance for image segmentation using fullysupervised learning. However, data annotation is extremely time consuming especially for medical imaging where highly skilled expertise is required. Several methods have been proposed to address this chal lenge. Data augmentation is commonly used as an effective solution. A few studies show that geometric transformations and intensity shifts to increase the number of annotated data can achieve better performance than only using the original 1https://github.com/ruizhel/semisegmentationlabeled data [2]. Moreover, semisupervised learning meth ods that use both labeled and unlabeled data for model train ing have also been intensively studied. In 2015, Rasmus et al. [3] adapted the Ladder Network for simultaneously min imizing the sum of supervised and unsupervised cost func tions by backpropagation for image classiÔ¨Åcation. Baur et al. [4] proposed a semisupervised learning method based on fully convolutional networks (FCNs) and random feature embedding for image segmentation. More recently, Lahiri et al. [5] used the discriminator in generative adversarial net works (GANs) to both segment images and discriminate be tween real and generated fake images. Labeled images help to improve the segmentation accuracy, and unlabeled images are used to increase the discrimination power. However, this method only extracts the global information of images to im prove the ability to discriminate true and false samples but does not pay attention to the extraction of segmentation in formation. More closely related to our proposed method is [6]. They developed a networkbased semisupervised learn ing framework using selflearning techniques. A fully super vised model is Ô¨Årstly trained on labeled data, then pseudo la bels are generated for unlabeled data using this model and reÔ¨Åned by a fullyconnected conditional random Ô¨Åeld (CRF). Subsequently, both labeled data and pseudo labeled data are used to reÔ¨Åne the initial model. This process is repeated un til convergence. However, the quality of the pseudo labels is highly dependent on the initial model and has a high impact on the Ô¨Ånal performance. Similar to [6], in this paper, we also train an initial model using labeled data and subsequently reÔ¨Åne the model using unlabeled data with pseudo labels. Different from [6], we use an ensemble technique to reduce the negative inÔ¨Çuence of poor quality pseudo labels. Ensemble learning is a widely used technique in machine learning [7]. It trains multiple weak classiÔ¨Åers and then combines these weak decisions to generate a Ô¨Ånal solution. Several classical classiÔ¨Åers (e.g. random forests [8]) are based on this idea to achieve state oftheart performance prior to the arrival of deep learning methods. Due to heavy computational load, very few studies have integrated ensemble learning with deep learning mod els. In 2017, Dolz et al [9] proposed a suggestive annotation model for infant brain MRI segmentation in a fully supervisedarXiv:2004.07995v1  [cs.CV]  16 Apr 2020Fig. 1 . Overview of the proposed framework. manner. It achieves the stateoftheart performance by com bining 10 CNNs. To the best of our knowledge, no previous studies have reported the use of ensemble learning and DCNN to achieve semisupervised learning. In this paper, we propose a DCNN based semisupervised learning framework that aims to learn a generic model that only uses a few annotated data with some unlabeled data for model training. We evaluate our framework based on a public skin lesion segmentation dataset, showing it outperforms both fully supervised learning method using only labeled data and Bai‚Äôs method [6] by a large margin. 2. METHODOLOGY "
407,UnibucKernel: Geolocating Swiss German Jodels Using Ensemble Learning.txt,"In this work, we describe our approach addressing the Social Media Variety
Geolocation task featured in the 2021 VarDial Evaluation Campaign. We focus on
the second subtask, which is based on a data set formed of approximately 30
thousand Swiss German Jodels. The dialect identification task is about
accurately predicting the latitude and longitude of test samples. We frame the
task as a double regression problem, employing an XGBoost meta-learner with the
combined power of a variety of machine learning approaches to predict both
latitude and longitude. The models included in our ensemble range from simple
regression techniques, such as Support Vector Regression, to deep neural
models, such as a hybrid neural network and a neural transformer. To minimize
the prediction error, we approach the problem from a few different perspectives
and consider various types of features, from low-level character n-grams to
high-level BERT embeddings. The XGBoost ensemble resulted from combining the
power of the aforementioned methods achieves a median distance of 23.6 km on
the test data, which places us on the third place in the ranking, at a
difference of 6.05 km and 2.9 km from the submissions on the first and second
places, respectively.","The Social Media Variety Geolocation (SMG) task was proposed, for the second year consecutively, in the 2021 edition of the VarDial Evaluation Cam paign (Chakravarthi et al., 2021). This task is aimed at geolocation prediction based on short text mes sages exchanged by the users of social media plat forms such as Twitter or Jodel. The location from where a short text was posted on a certain social media platform is expressed by two components: the latitude and the longitude. Naturally, the ge olocation task is formulated as a double regressionproblem. Twitter and Jodel are the platforms used for data collection, and similar to the previous spin of SMG at VarDial 2020 (G Àòaman et al., 2020), the task is divided into three subtasks, by language area, namely: ‚Ä¢Standard German Jodels (DEAT)  which tar gets conversations initiated in Germany and Austria in regional dialectal forms (Hovy and Purschke, 2018). ‚Ä¢Swiss German Jodels (CH)  containing a smaller number of Jodel conversations from the German speaking half of Switzerland (Hovy and Purschke, 2018). ‚Ä¢BCMS Tweets  from the area of Bosnia and Herzegovina, Croatia, Montenegro and Ser bia where the macrolanguage is BCMS, with both similarities and a fair share of variation among the component languages (Ljube Àási¬¥c et al., 2016). The focus of our work falls only on the second subtask, SMGCH, tackled via a variety of hand crafted and deep learning models. We propose a single ensemble model joining the power of sev eral individual models through metalearning based on Extreme Gradient Boosting (XGBoost) (Chen and Guestrin, 2016). We trained two independent ensemble models, each predicting one of the com ponents that form the geographical coordinates (lat itude and longitude). The Ô¨Årst model plugged into our metalearner is a Support Vector Regression (SVR) model (Chang and Lin, 2002) based on string kernels. Previous usage in dialect identiÔ¨Åcation has proved the ef Ô¨Åciency of this technique in the task of interest (Butnaru and Ionescu, 2018b; G Àòaman and Ionescu, 2020; Ionescu and Butnaru, 2017; Ionescu and Popescu, 2016).arXiv:2102.09379v3  [cs.CL]  26 Feb 2021The second model included in the ensemble is a hybrid convolutional neural network (CNN) (Liang et al., 2017) that combines, in the same architecture, characterlevel (Zhang et al., 2015) and wordlevel representations. The ability of capturing morpho logical relationships at the character level and using them as features for CNNs is also known to give promising results in dialect identiÔ¨Åcation (Butnaru and Ionescu, 2019; Tudoreanu, 2019). Different from works using solely characterlevel CNNs for dialect identiÔ¨Åcation (Butnaru and Ionescu, 2019; Tudoreanu, 2019), we believe that the addition of words might bring the beneÔ¨Åt of learning dialect speciÔ¨Åc multiword expressions that are hard to capture at the character level (Dhingra et al., 2016; Ling et al., 2015). Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is a top performing technique used in recent years for solv ing mainstream NLP problems. Thus, it seems Ô¨Åt to also include the outputs of a Ô¨Ånetuned German version of BERT in our XGBoost metalearner. We conducted experiments on the development set provided by the shared task organizers (Hovy and Purschke, 2018) in order to decide which model to choose as our submission for the SMG CH subtask. Our results indicate that the ensemble model attains the best performance. With median distances that are 56km higher, all the other mod els, tested individually on the development set, pro vide slightly worse predictions. The remainder of this paper is organized as fol lows. We present related work on dialect identiÔ¨Å cation and geolocation of short texts in Section 2. Our approach is described in detail in Section 3. We present the experiments and empirical results in Section 4. Finally, our conclusions are drawn in Section 5. 2 Related Work "
408,Anomaly Detection with Inexact Labels.txt,"We propose a supervised anomaly detection method for data with inexact
anomaly labels, where each label, which is assigned to a set of instances,
indicates that at least one instance in the set is anomalous. Although many
anomaly detection methods have been proposed, they cannot handle inexact
anomaly labels. To measure the performance with inexact anomaly labels, we
define the inexact AUC, which is our extension of the area under the ROC curve
(AUC) for inexact labels. The proposed method trains an anomaly score function
so that the smooth approximation of the inexact AUC increases while anomaly
scores for non-anomalous instances become low. We model the anomaly score
function by a neural network-based unsupervised anomaly detection method, e.g.,
autoencoders. The proposed method performs well even when only a small number
of inexact labels are available by incorporating an unsupervised anomaly
detection mechanism with inexact AUC maximization. Using various datasets, we
experimentally demonstrate that our proposed method improves the anomaly
detection performance with inexact anomaly labels, and outperforms existing
unsupervised and supervised anomaly detection and multiple instance learning
methods.","Anomaly detection is an important machine learning task, which is a task to nd the anomalous instances in a dataset. Anomaly detection has been used in a wide variety of applications (Chandola et al., 2009; Patcha and Park, 2007; Hodge and Austin, 2004), such as network intrusion detection for cybersecurity (Dokas et al., 2002; Yamanishi et al., 2004), fraud detection for credit cards (Aleskerov et al., 1997), defect detection in industrial machines (Fujimaki et al., 2005; Id e and Kashima, 2004) and disease outbreak detection (Wong et al., 2003). Many unsupervised anomaly detection methods have been proposed (Breunig et al., 2000; Sch olkopf et al., 2001; Liu et al., 2008; Sakurada and Yairi, 2014). When anomaly labels, which indicate whether each instance is anomalous, are given, the anomaly detection performance can be improved (Singh and Silakari, 2009; Mukkamala et al., 2005; Rapaka et al., 2003; Nadeem et al., 2016; Gao et al., 2006; Das et al., 2016, 2017). However, it is dicult to attach exact anomaly labels in some situations. Consider such example in server system failure detection. System operators often do not know the exact time of failures; they only know that a failure occurred within a certain period of time. In this case, anomaly labels can be attached to instances in a certain period of time, in which nonanomalous instances might be included. Another example is detecting anomalous IoT devices connected to a server. Even when the server displays unusual behavior, we sometimes cannot identify which IoT devices are anomalous. In these situations, only inexact anomaly labels are available. In this paper, we propose a supervised anomaly detection method for data with inexact anomaly labels. An inexact anomaly label is attached to a set of instances, indicating that at least one instance in the set is anomalous. We call this set an inexact anomaly set. First, we dene an extension of the area under the ROC curve (AUC) for performance measurement with inexact labels, which we call aninexact AUC . Then we develop an anomaly detection method that maximizes the inexact AUC. With the proposed method, a function, which outputs an anomaly score given an instance, is modeled by the reconstruction error with autoencoders, which are a successfully used neural networkbased 1arXiv:1909.04807v1  [stat.ML]  11 Sep 2019Figure 1: Example of anomalous (circle) and nonanomalous (triangle) instances, and inexact anomaly sets (red or blue) in an instance space. Instances with identical color (red or blue) are contained in the same inexact anomaly set. White circles are test anomalous instances. unsupervised anomaly detection method (Sakurada and Yairi, 2014; Sabokrou et al., 2016; Chong and Tay, 2017; Zhou and Paenroth, 2017). Note that the proposed method can use any unsupervised anomaly detection methods with learnable parameters instead of autoencoders, such as variational autoencoders (Kingma and Wellniga, 2014), energybased models (Zhai et al., 2016), and isolation forests (Liu et al., 2008). The parameters of the anomaly score function are trained so that the anomaly scores for nonanomalous instances become low while the smooth approximation of the inexact AUC becomes high. Since our objective function is dierentiable, the anomaly score function can be estimated eciently using stochastic gradientbased optimization methods. The proposed method performs well even when only a few inexact labels are given since it incor porates an unsupervised anomaly detection mechanism, which works without label information. In addition, the proposed method is robust to class imbalance since our proposed inexact AUC maximiza tion is related to AUC maximization, which achieved high performance on imbalanced data classication tasks (Cortes and Mohri, 2004). Class imbalance robustness is important for anomaly detection since anomalous instances occur more rarely than nonanomalous instances. Figure 1 shows an example of anomalous and nonanomalous instances, and inexact anomaly sets in a twodimensional instance space. For unsupervised methods, it is dicult to detect test anoma lous instance `A' since some instances are located around it. Unsupervised methods consider that an instance is anomalous when there are few instances around it. Since supervised methods can use label information, they can correctly detect test anomalous instance `A'. However, they might not detect test anomalous instance `B' since there are no labeled instances near it. In addition, with supervised methods that consider all the instances in inexact anomaly sets are anomaly, nonanomalous instances around training instances in the inexact anomaly sets (colored triangles in Figure 1) would be misclassi ed as anomaly. On the other hand, the proposed method detects `A' using label information and `B' by incorporating an unsupervised anomaly detection mechanism. Also, it would not detect nonanomalous instances as anomaly since it can handle inexact information. The remainder of the paper is organized as follows. In Section 2, we brie y review related work. In Section 3, we introduce AUC, which is the basis of the inexact AUC. In Section 4, we present the inexact AUC, dene our task, and propose our method for supervised anomaly detection using inexact labels. In Section 5, we experimentally demonstrate the eectiveness of our proposed method using various datasets by comparing with existing anomaly detection and multiple instance learning methods. Finally, we present concluding remarks and discuss future work in Section 6. 2 Related work "
409,Kinship Identification through Joint Learning Using Kinship Verification Ensembles.txt,"Kinship verification is a well-explored task: identifying whether or not two
persons are kin. In contrast, kinship identification has been largely ignored
so far. Kinship identification aims to further identify the particular type of
kinship. An extension to kinship verification run short to properly obtain
identification, because existing verification networks are individually trained
on specific kinships and do not consider the context between different kinship
types. Also, existing kinship verification datasets have biased
positive-negative distributions which are different than real-world
distributions. To this end, we propose a novel kinship identification approach
based on joint training of kinship verification ensembles and classification
modules. We propose to rebalance the training dataset to become more realistic.
Large scale experiments demonstrate the appealing performance on kinship
identification. The experiments further show significant performance
improvement of kinship verification when trained on the same dataset with more
realistic distributions.","Kinship is the relationship between people who are biologically related with over lapping genes [17,18], such as parentchildren, siblingsibling, and grandparent grandchildren [1,20,21,28]. Imagebased kinship identication is used in a variety of applications including missing children searching [28], family album organiza tion, forensic investigation [21], automatic image annotation [17], social media analysis [34,6,3], social behavior analysis [14,35,19,11], historical and genealogi cal research [15,6], and crime scene investigation [16]. While kinship verication is a wellexplored task, identifying whether or not persons are kin, kinship identication, which is the task to further identify the particular type of kinship, has been largely ignored so far. Existing kinship veri cation methods usually train and test each type of kinship model independently [24,20,28] and hence do not fully exploit the complementary information amongarXiv:2004.06382v4  [cs.CV]  24 Aug 20202 W. Wang et al. Is	Kin/not	Kin	? Which	type	? father  daughterfather  sonmother  daughtermother  son YesYesYesYes ?father daughter (a)	Outputs	from	individual verification	models	of	attention Network(Y an2019)Is	Kin/not	Kin	? Which	type	?kinship veriÔ¨Åcation model with Attention Network structure (Y an2019) binary output of each veriÔ¨Åcation model without softmax multiclassiÔ¨Åcation result of joint learning approach (b)	Output	from	proposed	joint learning	approach(JLNet) MINBasicfeature extraction Module father  daughterfather  sonmother  daughtermother  son+ binary output of each veriÔ¨Åcation model with softmaxkinship veriÔ¨Åcation model with Attention Network structure (Y an2019) Fig. 1: Identication of kinship relationships using verication ensembles. (a) Ex isting verication networks are trained independently resulting in contradictory outputs. (b) The output of our proposed joint training dierent kin types. Moreover, existing datasets have unrealistic positivenegative sample distributions. This leads to signicant limitations in real world applica tions. When conducting kinship identication, since there is no prior knowledge of the distribution of images, all independently trained models are used to de termine the kinship type of a specic image pair. Fig. 1 shows an example of providing an image pair to four individually trained verication networks based on a recent stateoftheart method by Yan et al. [33]. The network generates contradictory outputs showing that the test subjects are simultaneously father daughter, fatherson, motherson and motherdaughter. In this paper, a new identication method is proposed to learn the identica tion and verication labels jointly i.e. combining the kinship identication and verication tasks. Specically, all kinshiptype verication models are ensembled by combining the binary output of each verication model to form a multiclass output while training. The binary and multiclass models are leveraged in a multitasklearning way during the training process to enhance generalization capabilities. Also, we propose a baseline multiclassication neural network for comparison. We test our proposed kinship identication method on the KinfaceWI and KinfaceWII datasets and demonstrate stateoftheart performance for kinship identication. We also show that the proposed method signicantly improves the performance of kinship verication when trained on the same unbiased dataset. To summarize, the contributions of our work are: {We propose a theoretical analysis in metric space of relationships between kinship identication and kinship verication. {We propose a joint learnt network that simultaneously optimizes the perfor mance of kinship verication and kinship identication. {The proposed method outperforms existing methods for both kinship iden tication and unbiased kinship verication.Kinship Identication through Joint Learning 3 2 Related Work "
410,Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers.txt,"Fine-grained classification is a challenging task that involves identifying
subtle differences between objects within the same category. This task is
particularly challenging in scenarios where data is scarce. Visual transformers
(ViT) have recently emerged as a powerful tool for image classification, due to
their ability to learn highly expressive representations of visual data using
self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine
tuned using semi-supervised learning techniques, suitable for situations where
we have lack of annotated data. This is particularly common in e-commerce,
where images are readily available but labels are noisy, nonexistent, or
expensive to obtain. Our results demonstrate that Semi-ViT outperforms
traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned
with limited annotated data. These findings indicate that Semi-ViTs hold
significant promise for applications that require precise and fine-grained
classification of visual data.","In recent years, the development of deep neural networks has led to signiÔ¨Åcant advancements in the Ô¨Åeld of com puter vision [16]. One such architecture is the Visual Trans former (ViT) [5], which utilizes the selfattention mech anism to model longrange dependencies between image features. Unlike traditional convolutional neural networks (CNN) [7, 10, 26], which rely on handcrafted hierarchical feature extraction, visual transformers can learn global spa tial relationships among image features in a more efÔ¨Åcient and effective manner. This has enabled them to outper form stateoftheart methods on various visual recognition tasks [18]. However, in realworld scenarios labeled data can be scarce and expensive to obtain. Therefore, semi * Joint Ô¨Årst authors.supervised learning (SSL) [40] has emerged as a powerful technique for leveraging unlabeled data to improve the per formance of deep neural networks. CNN methods have sig niÔ¨Åcantly advanced the Ô¨Åeld [1, 3, 15, 27, 32] while ViT ar chitectures have only recently demonstrated promising re sults [2, 33] with SSL. In this paper, we investigate the effectiveness of SSL when used with ViT architectures. SpeciÔ¨Åcally, we utilize the SemiViT architecture [2] to conduct transfer learning for Ô¨Ånegrained classiÔ¨Åcation of ecommerce data. The use of ecommerce data presents a unique advantage for SSL as unlabeled images are readily available. However, the labels associated are often noisy or absent altogether. Tradition ally, this issue has been addressed through the use of manual curators, which can be costly and predominantly accessible for established marketplaces. In emerging markets, such as in Latin America, the scarcity of reliable labelled data poses an even greater challenge. We collect three datasets from ecommerce data contain ing labeled and unlabeled images. We perform Ô¨Ånegrained classiÔ¨Åcation on the neck style of a vest ( Vest Neck Style) , the pattern of a phone case ( Phone Case Pattern) , and the pattern of aprons and food bibs ( Apron Food Bib Pattern) . Each of the datasets contains 29K, 30K, and 33K labeled images, and 227K, 287K, 284K unlabeled images, respec tively. Labels were gathered using crowdsourced methods. We Ô¨Åne tune three different models, the wellknown ResNet architecture [10], a ViT, and a SemiViT architecture; all of them pretrained on ImageNet [4]. For the ViT and Semi ViT architectures, we additionally set different labeled data regimes where they are additionally Ô¨Ånetuned using 25%, 50%, and 75% of the labeled data for each of the datasets. In total, we train 9 different models for each task. 2. Related Work "
411,Unsupervised Domain Adaptation in Person re-ID via k-Reciprocal Clustering and Large-Scale Heterogeneous Environment Synthesis.txt,"An ongoing major challenge in computer vision is the task of person
re-identification, where the goal is to match individuals across different,
non-overlapping camera views. While recent success has been achieved via
supervised learning using deep neural networks, such methods have limited
widespread adoption due to the need for large-scale, customized data
annotation. As such, there has been a recent focus on unsupervised learning
approaches to mitigate the data annotation issue; however, current approaches
in literature have limited performance compared to supervised learning
approaches as well as limited applicability for adoption in new environments.
In this paper, we address the aforementioned challenges faced in person
re-identification for real-world, practical scenarios by introducing a novel,
unsupervised domain adaptation approach for person re-identification. This is
accomplished through the introduction of: i) k-reciprocal tracklet Clustering
for Unsupervised Domain Adaptation (ktCUDA) (for pseudo-label generation on
target domain), and ii) Synthesized Heterogeneous RE-id Domain (SHRED) composed
of large-scale heterogeneous independent source environments (for improving
robustness and adaptability to a wide diversity of target environments).
Experimental results across four different image and video benchmark datasets
show that the proposed ktCUDA and SHRED approach achieves an average
improvement of +5.7 mAP in re-identification performance when compared to
existing state-of-the-art methods, as well as demonstrate better adaptability
to different types of environments.","Person reidentiÔ¨Åcation (reID) attempts to match an individual from one camera view across other, non overlapping camera views [15]. The most successful meth ods [45, 50, 21] leverage deep learning via a supervised learning approach. Such supervised learning driven ap Figure 1. Iterative adaptation to unlabelled target domain using the proposed ktCUDA approach. Result on test set after each iteration of adaptation on the unlabelled training set. Starting with direct knowledge transfer from the proposed SHRED source domain on the Ô¨Årst row. Query on the left and the top5 search result with green for correct match and blue for incorrect match. Image from Market1501 [52] (left) and DukeMTMCreID [16] (right). proaches assume the availability of a large, manually labelled dataset of individuals across multiple cameras in the deployment environment (referred as the target domain). This assumption inherently limits the widespread adoption of person reID because of the cost and logistics needed for manually annotating data from the target domain, which is not practical in many realworld scenarios. To overcome the reliance on a large, manuallylabelled dataset from the target domain, two approaches have been proposed in recent literature: a pure unsupervised ap proach [27], and the more popular unsupervised domain adaptation approach [2, 61, 11, 31, 49]. Both approaches rely on an unlabelled dataset from the target domain which is easily obtained by running tracking on the target domain. Furthermore, the unsupervised domain adaptation approach assumes the availability of a manuallylabelled dataset from an independent source domain [11, 31, 49], whereas the pure unsupervised approach does not require a manually labelled source domain dataset. Without an independent source domain, the pure unsu pervised approaches cannot function at all in a new target domain until they have learned the new environment. FromarXiv:2001.04928v1  [cs.CV]  14 Jan 2020a practical point of view, this is undesirable as the system is not able to function at all upon deployment. The unsuper vised domain adaption methods on the other hand are pre trained on an independent source domain and can function upon deployment by directly transferring models learned on the source domain (we refer to this as direct transfer). Start ing from the direct transfer results, the system simply gets better as it adapts to the target domain (Fig. 1). The abil ity for immediate usage upon deployment makes such an unsupervised domain transfer approach very attractive from a practical point of view, but only if direct transfer perfor mance is good and unsupervised domain adaptation can fur ther improve the performance of the system. There are two key limitations to existing unsupervised domain adaptation approaches [2, 61, 11, 31, 49]. The Ô¨Årst limitation is that the domain adaptation component of ex isting approaches either: i) only considers environmental style transfer between source and target domains [2, 61] and do not explicitly learn suitable features and distance metric for the target domain, or ii) directly transfer distance metric (typically Euclidean distance) [11, 31, 49] learned on the source domain to target domain for obtaining pseudolabels on the target domain. Pseudolabels are then used to learn suitable features and distance metric on the target domain. However, direct transfer of distance metric is not optimal due to differences in the source domain environment and target domain environment. The second limitation of existing unsupervised domain adaptation approaches is that they rely heavily on a limited realworld source domain. Typically, a single independent environment is used as a source domain [27, 42, 38, 13, 60] which doesn‚Äôt capture enough variations in environments needed for domain adaptation. Some methods have at tempted to augment the source domain with thousands of synthetic data with varying illuminations [2], but other en vironmental variations outside of illumination are not cap tured. Finally, there are few works [49, 2, 34] that com bine few different datasets in the source domain to obtain some variability. However, their performance before and after adaptation is generally noticeably lower as compared to the latest unsupervised person reID techniques [27]. In this work, we address the two aforementioned limita tions of the current domain adaptation methods. First, we explore how to better leverage distance metrics that have been learned on the source domain to the target domain. Recently, kreciprocal reranking [58] has become a pop ular postprocessing step for all supervised reID methods, where the kreciprocal nearest neighbours are ranked higher than neighbours that minimize a distance metric and result in better performance. It was shown in [58] to boost per formance by10% on the mean average precision (mAP). Motivated by the effectiveness of such an approach within the realm of supervised reID, we propose a kreciprocaltracklet Clustering method for Unsupervised Domain Adap tation (ktCUDA), where kreciprocal neighbours are used to assign pseudolabels to the target domain. Second, we investigate the construction of a source do main that captures large environmental variations i.e., large number of identities and environmental conditions to ensure the best results for direct transfer of source domain to tar get domain. To this end, we constructed the Synthesized Heterogeneous REid Domain (SHRED), the largest source domain used in domain adaptation person reID literature. We show that the proposed SHRED performs very well for the direct transfer scenario. When combined with the pro posed ktCUDA, we show that stateoftheart performance can be achieved for unsupervised domain transfer on several test datasets. The main contributions of this paper are: ktCUDA , a novel kreciprocal tracklet clustering algo rithm for obtaining unsupervised pseudolabels on the target domain. SHRED , a synthesized largescale heterogeneous source domain that captures a wide set of environmen tal variations. A comprehensive analysis using both image and video datasets to show the performance of the proposed ktCUDA and SHRED, with full experimental results for direct transfer of knowledge from source domain to target domain as well as experimental results after domain adaptation. 2. Related Works "
412,Aesthetic Image Captioning From Weakly-Labelled Photographs.txt,"Aesthetic image captioning (AIC) refers to the multi-modal task of generating
critical textual feedbacks for photographs. While in natural image captioning
(NIC), deep models are trained in an end-to-end manner using large curated
datasets such as MS-COCO, no such large-scale, clean dataset exists for AIC.
Towards this goal, we propose an automatic cleaning strategy to create a
benchmarking AIC dataset, by exploiting the images and noisy comments easily
available from photography websites. We propose a probabilistic
caption-filtering method for cleaning the noisy web-data, and compile a
large-scale, clean dataset ""AVA-Captions"", (230, 000 images with 5 captions per
image). Additionally, by exploiting the latent associations between aesthetic
attributes, we propose a strategy for training the convolutional neural network
(CNN) based visual feature extractor, the first component of the AIC framework.
The strategy is weakly supervised and can be effectively used to learn rich
aesthetic representations, without requiring expensive ground-truth
annotations. We finally show-case a thorough analysis of the proposed
contributions using automatic metrics and subjective evaluations.","Availability of large curated datasets such as MS COCO [41] ( 100Kimages), Flickr30K [64] ( 30Kimages) or Conceptual Captions [74] ( 3Mimages) made it possi ble to train deep learning models for complex, multimodal tasks such as natural image captioning (NIC) [81] where the goal is to factually describe the image content. Similarly, several other captioning variants such as visual question answering [5], visual storytelling [38], stylized captioning [56] have also been explored. Recently, the PCCD dataset (4200 images) [11] opened up a new area of research of describing images aesthetically. Aesthetic image captioning (AIC) has potential applications in the creative industries such as developing smarter cameras or webbased applica tions, ranking, retrieval of images and videos etc. How ever in [11], only six wellknown photographic/aesthetic attributes such as composition, color, lighting, etc. havebeen used to generate aesthetic captions with a small cu rated dataset. Hence, curating a largescale dataset to facil itate a more comprehensive and generalized understanding of aesthetic attributes remains an open problem. Largescale datasets have always been pivotal for re search advancements in various Ô¨Åelds [15, 41, 64, 67]. However, manually curating such a dataset for AIC is not only time consuming, but also difÔ¨Åcult due to its subjective nature. Moreover, a lack of unanimously agreed ‚Äòstandard‚Äô aesthetic attributes makes this problem even more challeng ing as compared to its NIC counterpart, where deep models are trained with known attributes/labels [41]. In this pa per, we make two contributions. Firstly, we propose an au tomatic cleaning strategy to generate a large scale dataset by utilizing the noisy comments or aesthetic feedback pro vided by users for images on the web. Secondly, for a CNN based visual feature extractor as is typical in NIC pipelines, we propose a weaklysupervised training strategy. By auto matically discovering certain ‚Äòmeaningful and complex aes thetic concepts‚Äô, beyond the classical concepts such as com position, color, lighting, etc., our strategy can be adopted in scenarios where Ô¨Ånding clean groundtruth annotations is difÔ¨Åcult (as in the case of many commercial applications). We elaborate these contributions in the rest of this section. To generate a clean aesthetic captioning dataset, we col lected the raw user comments from the Aesthetic Visual Analysis (A V A) dataset [58]. A V A is a widely used dataset for aesthetic image analysis tasks such as aesthetic rat ing prediction [44, 48], photographic style classiÔ¨Åcation [25, 34]. However, A V A was not created for AIC. In this pa per, we refer to the original A V A with raw user comments as A V A rawcaption. It contains 250;000photographs from dpchallenge.com and the corresponding user comments or feedback for each photograph ( 3billion in total). Typ ically, in Dpchallenge, users ranging from casual hobby ists to expert photographers provide feedback to the images submitted and describe the factors that make a photograph aesthetically pleasing or dull. Even though these captions contain crucial aestheticbased information from images, they cannot be directly used for the task of AIC. Unlike the well instructed and curated datasets [41], A V A rawcaptions are unconstrained usercomments in the wild with typos,arXiv:1908.11310v1  [cs.CV]  29 Aug 2019Training Strategy (a)Noisy Data & Su pervised CNN (NS)i like the angle and the compo sitioni like the colors and the compo sitioni like the composition and the lightingi like the composition and the bw (b)Clean Data & Su pervised CNN (CS)i like the idea , but i think it would have been better if the door was in focus .i like the colors and the water . the water is a little distracting .i like the way the light hits the face and the background .i like this shot . i like the way the lines lead the eye into the photo . (c) Clean Data & Weakly Supervised CNN (CWS)i like the composition , but i think it would have been better if you could have gotten a little more of the buildingi like the composition and the colors . the water is a little too bright .this is a great shot . i love the way the light is coming from the left .i like the composition and the bw conversion . Figure 1. Aesthetic image captions. We show candidates generated by three different frameworks discussed in this paper: (a)For NS, we use an ImageNet trained CNN and LSTM trained on noisy comments (b)For CS, we use an ImageNet trained CNN and LSTM trained on compiled A V ACaptions dataset (c)For CWS, we use a weaklysupervised CNN and LSTM trained on A V ACaptions grammatically inconsistent statements, and also containing a large number of comments occurring frequently without useful information. Previous work in AIC [11] acknowl edges the difÔ¨Åculty of dealing with the highly noisy captions available in A V A. In this work, we propose to clean the raw captions from A V A by proposing a probabilistic ngram based Ô¨Åltering strategy. Based on wordcomposition and frequency of occurrence of ngrams, we propose to assign an informa tiveness score to each comment, where comments with a little or vague information are discarded. Our resulting clean dataset, A V ACaptions contains230;000images and1:5Mcaptions with an average of 5comments per image and can be used to train the Long and Short Term Memory (LSTM) network in the image captioning pipeline in the traditional way. Our subjective study veri Ô¨Åes that the proposed automatic strategy is consistent with human judgement regarding the informativeness of a cap tion. Our quantitative experiments and subjective studies also suggest that models trained on A V ACaptions are more diverse and accurate than those trained on the original noisy A V AComments. It is important to note that our strategy to choose the largescale A V A rawcaption is motivated from the widely used image analysis benchmarking dataset, MS COCO, which is now used as an uniÔ¨Åed benchmark for di verse tasks such as object detection, segmentation, caption ing, etc. We hope that our cleaned dataset will serve as a new benchmarking dataset for various creative studies and aestheticsbased applications such as aesthetics based im age enhancement, smarter photography cameras, etc. Our second contribution in this work is a weakly super vised approach for training a CNN, as an alternative to the standard practice. The standard approach for most image captioning pipelines is to train a CNN on large annotateddatasets e.g. ImageNet [15], where rich and discriminative visual features are extracted corresponding to the physical properties of objects such as cars, dogs etc. These features are provided as input to an LSTM for generating captions. Although trained for classiÔ¨Åcation, these ImageNetbased features have been shown to translate well to other tasks such as segmentation [42], styletransfer [22], NIC. In fact, due to the unavailability of largescale, taskspeciÔ¨Åc CNN annotations, these ImageNet features have been used for other variants of NIC such as aesthetic captioning [11], styl ized captioning [56], product descriptions [82], etc. However, for many commercial/practical applications, availability of such datasets or models is unclear due to copyright restrictions [24, 37, 83]. On the other hand, col lecting taskspeciÔ¨Åc manual annotations for a CNN is ex pensive and time intensive. Thus the question remains open if we can achieve better or at least comparable performance by utilizing easily available weak annotations from the web (as found in A V A) and use them for training the visual fea ture extractor in AIC. To this end, motivated from weakly supervised learning methods [18, 69], we propose a strategy which exploits the large pool of unstructured rawcomments from A V A and discovers latent structures corresponding to meaningful photographic concepts using Latent Dirichlet Allocation (LDA) [10]. We experimentally observe that the weaklysupervised approach is effective and its perfor mance is comparable to the standard ImageNet trained su pervised features. In essence, our contributions are as follows: 1. We propose a caption Ô¨Åltering strategy and compile A V ACaptions, a largescale and clean dataset for aes thetic image captioning (Sec 3). 2. We propose a weaklysupervised approach for trainingthe CNN of a standard CNNLSTM framework (Sec 4) 3. We showcase the analysis of the AIC pipeline based on the standard automated metrics (such as BLEU, CIDEr, SPICE etc. [2, 62, 78]), diversity of captions and subjective evaluations which are publicly available for further explorations (Section 6). 2. Related Work "
413,On Unsupervised Uncertainty-Driven Speech Pseudo-Label Filtering and Model Calibration.txt,"Pseudo-label (PL) filtering forms a crucial part of Self-Training (ST)
methods for unsupervised domain adaptation. Dropout-based Uncertainty-driven
Self-Training (DUST) proceeds by first training a teacher model on source
domain labeled data. Then, the teacher model is used to provide PLs for the
unlabeled target domain data. Finally, we train a student on augmented labeled
and pseudo-labeled data. The process is iterative, where the student becomes
the teacher for the next DUST iteration. A crucial step that precedes the
student model training in each DUST iteration is filtering out noisy PLs that
could lead the student model astray. In DUST, we proposed a simple, effective,
and theoretically sound PL filtering strategy based on the teacher model's
uncertainty about its predictions on unlabeled speech utterances. We estimate
the model's uncertainty by computing disagreement amongst multiple samples
drawn from the teacher model during inference by injecting noise via dropout.
In this work, we show that DUST's PL filtering, as initially used, may fail
under severe source and target domain mismatch. We suggest several approaches
to eliminate or alleviate this issue. Further, we bring insights from the
research in neural network model calibration to DUST and show that a
well-calibrated model correlates strongly with a positive outcome of the DUST
PL filtering step.","In recent years, Automatic Speech Recognition (ASR) has improved dramatically due to the introduction of novel neural network ar chitectures, training frameworks, and large labeled and unlabeled datasets [1‚Äì5]. However, domain generalization remains an un solved problem; an ASR model‚Äôs performance drops signiÔ¨Åcantly when the training and testing (or inference) conditions do not match. Several previous works attempt to address the issue of domain adaptation, such as consistency regularization, domainadversarial training, multidomain selfsupervised pretraining, and the much simpler SelfTraining (ST) method [6‚Äì10]. ST is useful when the target domain data distribution differs signiÔ¨Åcantly from the source domain data distribution (e.g., read speech to YouTube speech). ST proceeds by training a teacher model on a labeled set in the source domain, which generates pseudolabels (PLs) for the unla beled set in the target domain. Then, a student model uses both labeled and pseudolabeled data for training. We can iterate over the Teacher/Student training such that the student from a previous iter ation acts as the teacher for the next ST iteration. Classical ST that This work is supported by DSTA. A part of this work was performed using HPC resources by GENCI‚ÄìIDRIS under allocation AD011012527.uses all PLs might be suboptimal as PLs might be noisy. To address this issue, we previously proposed Dropout Uncertaintydriven Self Training (DUST) [11, 12], which appends the original SelfTraining algorithm with PL Ô¨Åltering step to weed out noisy PLs. Intuitively, the PL Ô¨Åltering stage in DUST computes a proxy for the model‚Äôs conÔ¨Ådence in its predictions on an unlabeled speech ut terance. It is computed using the agreement between a reference prediction and Tsampled predictions. If the model‚Äôs conÔ¨Ådence is below a predeÔ¨Åned threshold, we discard that utterance. The DUST Ô¨Åltering stage assumes that the model‚Äôs conÔ¨Ådence is a reliable es timate of PL quality, i.e., high conÔ¨Ådence implies low Word Error Rate (WER) on accepted utterances. In this work, we found that the DUST algorithm fails when the domain mismatch is severe. Hence, the DUST Ô¨Åltering method could accept noisy PLs. This paper focuses on PseudoLabel (PL) Ô¨Åltering stage in DUST. The goal of this work is to stress test the DUST PL Ô¨Ål tering under severe domain mismatch, and suggest approaches to alleviate the DUST PL Ô¨Åltering issue. The following are the main contributions to this work, (i) We show that the DUST algorithm fails when the domain mismatch is severe. (ii) We propose several approaches to mitigate or eliminate this breakpoint. (iii) We study, for the time, the PL Ô¨Åltering in the context of model calibration for DUST. (iv) We show that there is a strong correlation between the model calibration errors and the quality of the Ô¨Åltered PLs. 2. METHODOLOGY "
414,Noise2Blur: Online Noise Extraction and Denoising.txt,"We propose a new framework called Noise2Blur (N2B) for training robust image
denoising models without pre-collected paired noisy/clean images. The training
of the model requires only some (or even one) noisy images, some random
unpaired clean images, and noise-free but blurred labels obtained by predefined
filtering of the noisy images. The N2B model consists of two parts: a denoising
network and a noise extraction network. First, the noise extraction network
learns to output a noise map using the noise information from the denoising
network under the guidence of the blurred labels. Then, the noise map is added
to a clean image to generate a new ""noisy/clean"" image pair. Using the new
image pair, the denoising network learns to generate clean and high-quality
images from noisy observations. These two networks are trained simultaneously
and mutually aid each other to learn the mappings of noise to clean/blur.
Experiments on several denoising tasks show that the denoising performance of
N2B is close to that of other denoising CNNs trained with pre-collected paired
data.","Image denoising, which aims to restore a highquality image from its degraded observation, is a fundamental problem in image processing. In many imaging systems [11, 20, 34, 15], image noise comes from multiple sources, such as capturing instruments, data transmission media or subsequent postprocessing. Such complicated generation processes makes it difÔ¨Åcult to estimate the latent noise model and recover the clean image from the noisy observation. A large variety of denoising algorithms have been developed to deal with image noise. Traditional denoising methods ( e.g.BM3D [8], WNNM [13]) exploit a property of the noise or image structure to help denoising. These methods require accurate image model deÔ¨Ånitions, thus performance is limited in realworld cases. Modern denoising methods often employ convolutional neural networks (CNNs) to learn the mapping function from noise to clean on a large collection of noisy/clean image pairs. The performance of CNN denoisers are highly dependent on whether the distributions of training noise and test noise are well matched. Since pairs of real noisy/clean images are difÔ¨Åcult to obtain, CNN denoisers are mostly trained on synthesized data. In addition, the real noise degradation process is usually complex or unknown, so that the synthesized noise distribution can deviate severely from the 1arXiv:1912.01158v2  [eess.IV]  14 May 2020(a) SSIM, PSNR j0.981, 39.34dB  (b) 0.976, 38.25dB  (c)0.828, 28.95dB  (d) 0.968, 37.26dB Figure 1. Comparison of different training schemes. (a) Traditionally, the training of denoising network requires a large amount of paired noisy/clean images. (b) Noise2Noise [22] trains the network using pairs of independent noisy measurements of the same target. (c) Noise2V oid [17] uses just individual noisy images as training data. (d) Our Noise2Blur needs some unpaired noise and clean images, as well as blurred counterparts of the noisy images to generate supervision. real noise distribution. As a result, CNN denoisers are easily overÔ¨Åtted to the speciÔ¨Åc synthetic noise (e.g.Gaussian noise, Poisson noise) and generalize poorly to the realworld noisy images. In this paper, we propose Noise2Blur (N2B), a training scheme that overcomes the above problems. The training of N2B model does not need access to estimation of noise and precollected paired data. It only requires some unpaired noisy images and clean images, which is easy to implement in most practical applications. Although the images we have are unpaired, we can extract information from them to generate supervision for the denoising process. Our N2B model consists of two subnetworks, i.e.denoising and noise extraction. The noisy inputs are Ô¨Årst transformed into noisefree but blurred images by general Ô¨Åltering techniques ( e.g. Gaussian Ô¨Ålter, median Ô¨Ålter). We use the noisy/blurred image pair to guide the noise extraction network to roughly extract noise from its input (‚Äúnoiseto blur‚Äù). The denoising network is then trained using a new noisy/clean image pair obtained by adding the extracted noise to a random clean image (‚Äúnoisetoclean‚Äù). With a simple gradient interruption operation, the denoising network eventually converges to the ‚Äúnoisetoclean‚Äù objective, while the noise extraction network learns to Ô¨Ånely extract the noise. On the other hand, the training of N2B model has no requirement on the number of noisy images; even if only one noisy image of size 512512is available, a denosing network with strong generalization can be trained. Through experiments on several datasets, we demonstrate the effectiveness of N2B. Although we train with unpaired data, the denoising performance of the N2B model is close to that of other denoising CNNs trained with precollected paired data. 2. Related work "
415,A Fairness Analysis on Private Aggregation of Teacher Ensembles.txt,"The Private Aggregation of Teacher Ensembles (PATE) is an important private
machine learning framework. It combines multiple learning models used as
teachers for a student model that learns to predict an output chosen by noisy
voting among the teachers. The resulting model satisfies differential privacy
and has been shown effective in learning high-quality private models in
semisupervised settings or when one wishes to protect the data labels.
  This paper asks whether this privacy-preserving framework introduces or
exacerbates bias and unfairness and shows that PATE can introduce accuracy
disparity among individuals and groups of individuals. The paper analyzes which
algorithmic and data properties are responsible for the disproportionate
impacts, why these aspects are affecting different groups disproportionately,
and proposes guidelines to mitigate these effects. The proposed approach is
evaluated on several datasets and settings.","The availability of large datasets and inexpensive computational resources has rendered the use of machine learning (ML) systems instrumental for many critical decisions involving individuals, including criminal assessment, landing, and hiring, all of which have a profound social impact. A key concern for the adoption of these system regards how they handle bias and discrimination and how much information they leak about the individuals whose data is used as input. Dierential Privacy (DP) [ 5] is an algorithmic property that bounds the risks of disclosing sensitive information of individuals participating in a computation. It has become the paradigm of choice in privacypreserving machine learning systems and its deployments are growing at a fast rate. However, it was recently observed that DP systems may induce biased and unfair outcomes for di erent groups of individuals [1, 19, 28]. The resulting outcomes can have signiÔ¨Åcant societal and economic impacts on the involved individuals: classiÔ¨Åcation errors may penalize some groups over others in important determinations including criminal assessment, landing, and hiring [ 1] or can result in disparities regarding the allocation of critical funds and beneÔ¨Åts [ 19].While these surprising observations are becoming increasingly common, their causes are largely understudied and not fully understood. This paper makes a step toward this important quest, and studies the disparate impacts arising when training a model using Private Aggregation of Teacher Ensembles (PATE) [ 17] an important and popular privacypreserving machine learning framework. It combines multiple agnostic learning models used as teachers for a student model that learns to predict an output chosen by noisy voting among the teachers. The resulting model satisÔ¨Åes di erential privacy and has been shown e ective in learning high quality private models in semisupervised settings or when one wishes to protect the data labels. Preprint. Under review.arXiv:2109.08630v1  [cs.LG]  17 Sep 2021The paper analyzes which properties of the algorithm and the data are responsible for the dispro portionate impacts, why these aspects are a ecting di erent individuals or groups of individuals disproportionately, and proposes a solution that may aid mitigating these e ects. In summary, the paper makes the following contributions: 1.It uses a fairness notion that relies on the concept of excessive risk, and measures the direct impact of privacy to the model accuracy for individuals or groups. 2. It analyzes this fairness notion in PATE, a stateoftheart privacypreserving ML framework. 3.It isolates key components of the model parameters and the data properties which are responsible for the observed disparate impacts. 4.It studies when and why these components a ect di erent individuals or groups disproportionately. 5.Finally, based on these Ô¨Åndings, it proposes a method that may aid mitigating these unfairness eects while retaining high accuracy. To the best of the authors knowledge, this work represents a Ô¨Årst e ort toward understanding the reasons of the disparate impacts in privacypreserving ensemble models. 2 Related Work "
416,Robust Training of Graph Neural Networks via Noise Governance.txt,"Graph Neural Networks (GNNs) have become widely-used models for
semi-supervised learning. However, the robustness of GNNs in the presence of
label noise remains a largely under-explored problem. In this paper, we
consider an important yet challenging scenario where labels on nodes of graphs
are not only noisy but also scarce. In this scenario, the performance of GNNs
is prone to degrade due to label noise propagation and insufficient learning.
To address these issues, we propose a novel RTGNN (Robust Training of Graph
Neural Networks via Noise Governance) framework that achieves better robustness
by learning to explicitly govern label noise. More specifically, we introduce
self-reinforcement and consistency regularization as supplemental supervision.
The self-reinforcement supervision is inspired by the memorization effects of
deep neural networks and aims to correct noisy labels. Further, the consistency
regularization prevents GNNs from overfitting to noisy labels via mimicry loss
in both the inter-view and intra-view perspectives. To leverage such
supervisions, we divide labels into clean and noisy types, rectify inaccurate
labels, and further generate pseudo-labels on unlabeled nodes. Supervision for
nodes with different types of labels is then chosen adaptively. This enables
sufficient learning from clean labels while limiting the impact of noisy ones.
We conduct extensive experiments to evaluate the effectiveness of our RTGNN
framework, and the results validate its consistent superior performance over
state-of-the-art methods with two types of label noises and various noise
rates.","In realworld applications, a set of objects and their relationships can often be represented naturally as a graph. The graph data structure is widely employed in various domains such as biology, transportation, and social science. In the past decade, Graph Neural Networks (GNNs) have shown promising capacity in modeling graph data [ 13,20,38]. Typically, GNNs adopt a message passing and aggregation procedure to effectively propagate information via the graph structure. This mechanism makes GNNs very suitable for semisupervised graph learning (e.g., node classification [1, 32]). While GNNs are generally effective, most of the existing ap proaches assume that labels are sufficient and clean. However, in practice, node labels can be both scarce and noisy. For example, consider a graph from social media whose node labels are con tributed by users. It is often the case that only a small fraction of users would participate in label generation, and intentionally or for other reasons, some of the labels do not reflect the truth. Another example is crowdsourcing node labels, e.g., fake news annotation and medical knowledge graph annotation. It is easy to see that the annotation process is laborintensive and expensive, and almost inevitably, label errors are introduced due to subjective judgment. Such cases will lead to graphs with scarce and noisy node labels. It has already been observed that noisy labels could pose a severe threat to the generalization performance of deep learning mod els [2,42]. Therefore, developing labelefficient and noiseresistant GNNs is an important and challenging problem. In the literature, robust deep learning in the presence of noisy labels has been explored mainly in applications with nongraph data such as images. Several strategies have been developed to combat label noises, e.g., sample selection [ 14,16,24], robust lossarXiv:2211.06614v2  [cs.LG]  26 Feb 2023WSDM ‚Äô23, February 27March 3, 2023, Singapore, Singapore. Siyi Qian, et al. functions [ 10,34,44], and loss correction [ 27,29]. However, sim ply incorporating these approaches into GNNs will be insufficient (we will empirically show this in experiments). We observe that one unique characteristic of this problem with GNNs is that the scarcity of labels causes difficulties for nodes to receive sufficient supervision from labeled neighbors. Meanwhile, nodes in graphs are directly influenced by potential noises through message passing. Failing to balance the two would either lead to massive erroneous supervision from noisy labels or end up with insufficient learning. Hence, a main challenge to solving this problem is how to effec tively leverage supervision of clean labels while limiting the impact of noisy ones . Previous studies mainly focused on leveraging supervi sion of labels, but neglected how to limit the impact of noisy labels. For example, recently, NRGNN [ 6] investigated a robust GNN with noisy and sparse labels. It proposed to link unlabeled nodes with labeled nodes and further mine accurate pseudolabels to provide more supervision. However, NRGNN mainly emphasized leverag ing supervision of labels, which mixed up clean and noisy labels in its learning process. Whereas, as explained above, explicitly gov erning noises (i.e., limiting the impact of noisy labels) is necessary in order to further boost the robustness of GNNs. To this end, we propose a novel GNN framework called RTGNN (Robust Training of Graph Neural Networks via Noise Governance) that is capable of conducting robust learning with scarce and noisy node labels. As a distinguished design of our model, we develop a finegrained noise governance strategy in semisupervised learning. Due to the fact that deep neural networks (DNNs) tend to prior itize learning simple patterns first and may overfit to noises [ 2], we first propose an effective and scalable lossbased label division method to identify potentially noisy labels. Further, besides the commonlyused label supervision, we introduce selfreinforcement and consistency regularization as supplemental supervisions. The selfreinforcement supervision is inspired by the memorization effects of DNNs and aims to correct noisy labels. Consistency regu larization includes an interview regularization based on ensembled classifiers capable of filtering different errors, and an intraview regularization to explore the local homogeneity of the graph [39]. Specifically, we propose to train a pair of peer GNNs to enforce adequate supervision as well as govern noise labels. First, we aug ment the raw graph by linking labeled and unlabeled nodes to facilitate efficient message passing, following NRGNN [ 6]. Then, in each epoch, we progressively perform the main division of clean and noisy candidate set by the smallloss criterion [ 14]. Next, a subset of confident nodes which predict a different class from their labels in the noisy candidate set is reinforced by training with their own prediction. Similarly, we extend the label set by adding those confident and consistent unlabeled nodes to the training set. Finally, we apply interview regularization to help two classifiers coopera tively mimic each other‚Äôs soft targets and intraview regularization to enable nodes to learn from their neighbors. In summary, the main contributions of our work are as follows. ‚Ä¢We investigate the robust training problem of GNNs from the noise governance perspective, which has been under explored in previous studies. ‚Ä¢We develop a novel RTGNN model which governs label noises explicitly with selfreinforcement and consistency regularization. RTGNN also enables finegrained learningon relatively clean, potentially noisy, and pseudo labels. This allows effectively leveraging supervision information while limiting the impact of label noises. ‚Ä¢We conduct extensive experiments to evaluate the effective ness of our new approach, and the results validate the consis tent superior performance of RTGNN over stateoftheart methods with two types of noises and various noise rates. 2 RELATED WORK "
417,Towards Robust Neural Networks via Random Self-ensemble.txt,"Recent studies have revealed the vulnerability of deep neural networks: A
small adversarial perturbation that is imperceptible to human can easily make a
well-trained deep neural network misclassify. This makes it unsafe to apply
neural networks in security-critical applications. In this paper, we propose a
new defense algorithm called Random Self-Ensemble (RSE) by combining two
important concepts: {\bf randomness} and {\bf ensemble}. To protect a targeted
model, RSE adds random noise layers to the neural network to prevent the strong
gradient-based attacks, and ensembles the prediction over random noises to
stabilize the performance. We show that our algorithm is equivalent to ensemble
an infinite number of noisy models $f_\epsilon$ without any additional memory
overhead, and the proposed training procedure based on noisy stochastic
gradient descent can ensure the ensemble model has a good predictive
capability. Our algorithm significantly outperforms previous defense techniques
on real data sets. For instance, on CIFAR-10 with VGG network (which has 92\%
accuracy without any attack), under the strong C\&W attack within a certain
distortion tolerance, the accuracy of unprotected model drops to less than
10\%, the best previous defense technique has $48\%$ accuracy, while our method
still has $86\%$ prediction accuracy under the same level of attack. Finally,
our method is simple and easy to integrate into any neural network.","Deep neural networks have demonstrated their success in many machine learning and computer vision applications, including image classication [14,7,35,9,34], object recognition [30] and image captioning [38]. Despite having nearperfect prediction performance, recent studies have revealed the vulnerability of deep neural networks to adversarial examples|given a correctly classied image, a carefully designed perturbation to the image can make a welltrained neural network misclassify. Algorithms crafting these adversarial images, called attack algorithms, are designed to minimize the perturbation, thus making adversarial images hard to be distinguished from natural images. This leads to securityarXiv:1712.00673v2  [cs.LG]  1 Aug 20182 X. Liu, M. Cheng, H. Zhang and CJ. Hsieh concerns, especially when applying deep neural networks to securitysensitive systems such as selfdriving cars and medical imaging. To make deep neural networks more robust to adversarial attacks, several defense algorithms have been proposed recently [23,40,17,16,39]. However, recent studies showed that these defense algorithms can only marginally improve the accuracy under the adversarial attacks [4,5]. In this paper, we propose a new defense algorithm: Random SelfEnsemble (RSE). More specically, we introduce the new \noise layer"" that fuses input vector with randomly generated noise, and then we insert this layer before each convolution layer of a deep network. In the training phase, the gradient is still computed by backpropagation but it will be perturbed by random noise when passing through the noise layer. In the inference phase, we perform several for ward propagations, each time with dierent prediction scores due to the noise layers, and then ensemble the results. We show that RSE makes the network more resistant to adversarial attacks, by virtue of the proposed training and testing scheme. Meanwhile, it will only slightly aect test accuracy when no at tack is performed on natural images. The algorithm is trivial to implement and can be applied to any deep neural networks for the enhancement. Intuitively, RSE works well because of two important concepts: ensemble andrandomness . It is known that ensemble of several trained models can improve the robustness [29], but will also increase the model size by kfolds. In contrast, without any additional memory overhead, RSE can construct innite number of models f, whereis generated randomly, and then ensemble the results to improve robustness. But how to guarantee that the ensemble of these models can achieve good accuracy? After all, if we train the original model without noise, yet only add noise layers at the inference stage, the algorithm is going to perform poorly. This suggests that adding random noise to an pre trained network will only degrade the performance. Instead, we show that if the noise layers are taken into account in the training phase, then the training procedure can be considered as minimizing the upper bound of the loss of model ensemble, and thus our algorithm can achieve good prediction accuracy. The contributions of our paper can be summarized below: {We propose the Random SelfEnsemble (RSE) approach for improving the robustness of deep neural networks. The main idea is to add a \noise layer"" before each convolution layer in both training and prediction phases. The algorithm is equivalent to ensemble an innite number of random models to defense against the attackers. {We explain why RSE can signicantly improve the robustness toward adver sarial attacks and show that adding noise layers is equivalent to training the original network with an extra regularization of Lipschitz constant. {RSE signicantly outperforms existing defense algorithms in all our experi ments. For example, on CIFAR10 data and VGG network (which has 92% accuracy without any attack), under C&W attack the accuracy of unpro tected model drops to less than 10%; the best previous defense technique has 48% accuracy; while RSE still has 86 :1% prediction accuracy under theRSE for Robust Neural Networks 3 same strength of attacks. Moreover, RSE is easy to implement and can be combined with any neural network. 2 Related Work "
418,Language Identification on Massive Datasets of Short Message using an Attention Mechanism CNN.txt,"Language Identification (LID) is a challenging task, especially when the
input texts are short and noisy such as posts and statuses on social media or
chat logs on gaming forums. The task has been tackled by either designing a
feature set for a traditional classifier (e.g. Naive Bayes) or applying a deep
neural network classifier (e.g. Bi-directional Gated Recurrent Unit,
Encoder-Decoder). These methods are usually trained and tested on a huge amount
of private data, then used and evaluated as off-the-shelf packages by other
researchers using their own datasets, and consequently the various results
published are not directly comparable. In this paper, we first create a new
massive labelled dataset based on one year of Twitter data. We use this dataset
to test several existing language identification systems, in order to obtain a
set of coherent benchmarks, and we make our dataset publicly available so that
others can add to this set of benchmarks. Finally, we propose a shallow but
efficient neural LID system, which is a ngram-regional convolution neural
network enhanced with an attention mechanism. Experimental results show that
our architecture is able to predict tens of thousands of samples per second and
surpasses all state-of-the-art systems with an improvement of 5%.","Language IdentiÔ¨Åcation (LID) is the Natural Language Processing (NLP) task of automatically recognizing the language that a document is written in. While this task was called ‚Äùsolved‚Äù by some authors over a decade ago, it has seen a resurgence in recent years thanks to the rise in pop ularity of social media (Jauhiainen et al., 2018; Jaech et al., 2016), and the corresponding daily creation of millions of new messages in dozens of different languages including rare ones that are not often included in language identi Ô¨Åcation systems. Moreover, these messages are typically very short (Twitter messages were until recently limited to 140 characters) and very noisy (including an abundance ofspelling mistakes, nonword tokens like URLs, emoticons, or hashtags, as well as foreignlanguage words in messages of another language), whereas LID was solved using long and clean documents. Indeed, several studies have shown that LID systems trained to a high accuracy on traditional documents suffer signiÔ¨Åcant drops in accuracy when ap plied to short socialmedia texts (Lui & Baldwin, 2012; Carter et al., 2013). Given its massive scale, multilingual nature, and popular ity, Twitter has naturally attracted the attention of the LID research community. Several attempts have been made to construct LID datasets from that resource. However, a ma jor challenge is to assign each tweet in the dataset to the correct language among the more than 70 languages used on the platform. The three commonlyused approaches are to rely on human labeling (Lui & Baldwin, 2014; Tromp & Pechenizkiy, 2011), machine detection (Tromp & Pech enizkiy, 2011; Jurgens et al., 2017), or user geolocation (Carter et al., 2013; Blodgett et al., 2017; Bergsma et al., 2012). Human labeling is an expensive process in terms of workload, and it is thus infeasible to apply it to create a massive dataset and get the full beneÔ¨Åt of Twitter‚Äôs scale. Automated LID labeling of this data creates a noisy and im perfect dataset, which is to be expected since the purpose of these datasets is to create new and better LID algorithms. And user geolocation is based on the assumption that users in a geographic region use the language of that region; an assumption that is not always correct, which is why this technique is usually paired with one of the other two. Our Ô¨Årst contribution in this paper is to propose a new approach to build and automatically label a Twitter LID dataset, and to show that it scales up well by building a dataset of over 18 million labeled tweets. Our hope is that our new Twitter dataset will become a benchmarking standard in the LID literature. Traditional LID models (Lui & Baldwin, 2012; Carter et al., 2013; Gamallo et al., 2014) proposed different ideas to design a set of useful features. This set of features is then passed to traditional machine learning algorithms such as Naive Bayes (NB). The resulting systems are capable of la beling thousands of inputs per second with moderate accu racy. Meanwhile, neural network models (Kocmi & Bojar,arXiv:1910.06748v1  [cs.CL]  15 Oct 2019Language IdentiÔ¨Åcation on Massive Datasets of Short Message using an Attention Mechanism CNN Table 1. Summary of literature results Paper Input Algorithm Metric Results langid.py Tromp & Pechenizkiy (2011) Character ngrams Graph Accuracy 0.975 0.941 Carter et al. (2013) Social network information Prior probabilities Accuracy 0.972 0.886 Gamallo et al. (2014) Words Dictionary F1score 0.733 N/A Jaech et al. (2016) Words LSTM F1score 0.912 0.879 Kocmi & Bojar (2017) Character ngrams GRU Accuracy 0.955 0.912 Jurgens et al. (2017) Character ngrams Encoderdecoder Accuracy 0.982 0.960 2017; Jurgens et al., 2017) approach the problem by design ing a deep and complex architecture like gated recurrent unit (GRU) or encoderdecoder net. These models use the message text itself as input using a sequence of character embeddings, and automatically learn its hidden structure via a deep neural network. Consequently, they obtain better results in the task but with an efÔ¨Åciency tradeoff. To allevi ate these drawbacks, our second contribution in this paper is to propose a shallow but efÔ¨Åcient neural LID algorithm. We followed previous neural LID (Kocmi & Bojar, 2017; Jurgens et al., 2017) in using character embeddings as in puts. However, instead of using a deep neural net, we pro pose to use a shallow ngramregional convolution neural network (CNN) with an attention mechanism to learn input representation. We experimentally prove that the ngram regional CNN is the best choice to tackle the bottleneck problem in neural LID. We also illustrate the behaviour of the attention structure in focusing on the most impor tant features in the text for the task. Compared with other benchmarks on our Twitter datasets, our proposed model consistently achieves new stateoftheart results with an improvement of 5% in accuracy and F1 score and a com petitive inference time. The rest of this paper is structured as follows. After a back ground review in the next section, we will present our Twit ter dataset in Section 3. Our novel LID algorithm will be the topic of Section 4. We will then present and analyze some experiments we conducted with our algorithm in Section 5, along with benchmarking tests of popular and literature LID systems, before drawing some concluding remarks in Section 6. Our Twitter dataset and our LID algorithm‚Äôs source code are publicly available1. 2. Related Work "
419,Semi-Supervised Music Tagging Transformer.txt,"We present Music Tagging Transformer that is trained with a semi-supervised
approach. The proposed model captures local acoustic characteristics in shallow
convolutional layers, then temporally summarizes the sequence of the extracted
features using stacked self-attention layers. Through a careful model
assessment, we first show that the proposed architecture outperforms the
previous state-of-the-art music tagging models that are based on convolutional
neural networks under a supervised scheme.
  The Music Tagging Transformer is further improved by noisy student training,
a semi-supervised approach that leverages both labeled and unlabeled data
combined with data augmentation. To our best knowledge, this is the first
attempt to utilize the entire audio of the million song dataset.","Automatic music tagging is a classiÔ¨Åcation task whose ob jective is computational understanding of music seman tics. From a given audio excerpt, a trained music tagging model predicts relevant tags (e.g., genre, mood, instru ment, decade, region) based on its acoustic characteristics. The task has attracted music information retrieval (MIR) researchers due to its wide pragmatic usages in many ap plications. Especially, there is a strong demand from in dustries that have music recommendation services from largescale music libraries. Thanks to the recent advances in deep learning, mostly convolutional neural networks (CNNs) [1], the performances of music tagging models have been signiÔ¨Åcantly enhanced by leveraging largescale data with various deep architectures [2‚Äì5]. However, there still are two limitations in the current music tagging re search: i) chunkbased prediction and ii) a limited amount of labeled data for supervised learning. Music signals are in the form of sequential data. In this sequence, regarding typical tags, some acoustic character istics may appear locally (e.g., instruments) while some others may span over the sequence (e.g., mood, genre). This means a successful music tagging model needs to be ¬© Minz Won, Keunwoo Choi, and Xavier Serra. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Minz Won, Keunwoo Choi, and Xavier Serra, ‚ÄúSemi Supervised Music Tagging Transformer‚Äù, in Proc. of the 22nd Int. Society for Music Information Retrieval Conf., Online, 2021.able to extract both local and global features. Fully con volutional network [2], one of the very early deep learning models for music tagging, was designed to capture both local and global features by increasing the size of the over all receptive Ô¨Åelds with maxpooling. More recently, how ever, it is shown that training with a smaller hop size with shorter audio chunks is beneÔ¨Åcial for music tagging [6]. This approach has been adopted in many CNNbased mod els [3‚Äì5], where the models are trained with short audio chunks (3 to 5 second long), densely striding maxpooling, and a global pooling layer. To predict music tags of a 3 minute song, for example, the audio is split into multiple short audio chunks, and the model makes predictions on each chunk. Then, the predictions are aggregated through majority vote or global average/maxpooling. This means that on a track level, the current music tagging models are performing like a bagoffeatures model [7] instead of modeling music representation as a sequence. Another limitation of the current music tagging research is a limited amount of labeled data. The Modern deep learning models are datahungry. However, manually la beling music tags is timeconsuming and requires domain expertise. In pursuit of largescale research, the million song dataset (MSD) [8], which literally includes a mil lion songs in it, became popular in music tagging research. Among the million songs, however, only about 24% are labeled with at least one of the top50 music tags. Most of the previous music tagging research has only utilized the labeled data while discarding 76% of the songs in the dataset. This type of setup (i.e., a small labeled dataset along with a large unlabeled dataset) is not limited to the MSD but can be found often in the real world regard less of the domain. To leverage the unlabeled data, self supervised [9‚Äì13] and semisupervised [14‚Äì16] learning have been actively explored in computer vision and natural language processing. In this paper, we present Music Tagging Transformer that is trained with a semisupervised approach. Our main contribution is threefold: ( i) through a careful model as sessment, we show that our Music Tagging Transformer outperforms the previous works, ( ii) we show that we can use unlabeled data to improve music tagging performances via semisupervised learning, ( iii) we provide a new split of the MSD to solve known issues of the previous one. Re producible code is available online.1 1https://github.com/minzwon/semisupervisedmusictagging transformerarXiv:2111.13457v1  [cs.SD]  26 Nov 20212. RELATED WORKS "
420,SemiGNN-PPI: Self-Ensembling Multi-Graph Neural Network for Efficient and Generalizable Protein-Protein Interaction Prediction.txt,"Protein-protein interactions (PPIs) are crucial in various biological
processes and their study has significant implications for drug development and
disease diagnosis. Existing deep learning methods suffer from significant
performance degradation under complex real-world scenarios due to various
factors, e.g., label scarcity and domain shift. In this paper, we propose a
self-ensembling multigraph neural network (SemiGNN-PPI) that can effectively
predict PPIs while being both efficient and generalizable. In SemiGNN-PPI, we
not only model the protein correlations but explore the label dependencies by
constructing and processing multiple graphs from the perspectives of both
features and labels in the graph learning process. We further marry GNN with
Mean Teacher to effectively leverage unlabeled graph-structured PPI data for
self-ensemble graph learning. We also design multiple graph consistency
constraints to align the student and teacher graphs in the feature embedding
space, enabling the student model to better learn from the teacher model by
incorporating more relationships. Extensive experiments on PPI datasets of
different scales with different evaluation settings demonstrate that
SemiGNN-PPI outperforms state-of-the-art PPI prediction methods, particularly
in challenging scenarios such as training with limited annotations and testing
on unseen data.","Proteinprotein Interactions (PPIs) are central to vari ous cellular functions and processes, such as signal transduction, cellcycle progression, and metabolic path ways [Acuner Ozbabacan et al. , 2011 ]. Therefore, the identi Ô¨Åcation and characterization of PPIs are of great importance for understanding protein functions and disease occurrence, which can potentially facilitate therapeutic target identiÔ¨Åca tion[Petta et al. , 2016 ]and the novel drug design [Skrabanek equal contributionet al. , 2008 ]. In past decades, highthroughput experimen tal methods, e.g., yeast twohybrid screens (Y2H) [Fields and Song, 1989 ], and mass spectrometric protein complex identi Ô¨Åcation (MSPCI) [Hoet al. , 2002 ]have been developed to identify PPIs. Nevertheless, genomescale experiments are expensive, tedious, and timeconsuming while suffering from high error rates and low coverage [Luoet al. , 2015 ]. As such, there is an urgent need to establish reliable computational methods to identify PPIs with high quality and accuracy. In recent years, a large variety of highthroughput compu tational approaches for PPI prediction have been proposed, which can be broadly divided into two groups: classic ma chine learning (ML)based methods [Browne et al. , 2007; Lin and Chen, 2013; Guo et al. , 2008; Wong et al. , 2015; Chen and Liu, 2005 ]and deep learning (DL)based meth ods[Sunet al. , 2017; Du et al. , 2017; Hashemifar et al. , 2018; Chen et al. , 2019a; Lv et al. , 2021 ]. Compared to clas sic ML methods, DL algorithms are capable of processing complicated and largescale data and extracting useful fea tures automatically, achieving signiÔ¨Åcant success in a di verse range of bioinformatics applications [Min et al. , 2017; Soleymani et al. , 2022 ], including PPI prediction [Soleymani et al. , 2022 ]. Most existing DLbased methods treat inter actions as independent instances, ignoring protein correla tions. PPI can be naturally formulated as graph networks with proteins and interactions represented as nodes and edges, respectively [Margolin et al. , 2006; Pio et al. , 2020 ]. To improve PPI prediction performance, recent works [Yang et al., 2020; Lv et al. , 2021 ]have been proposed to investi gate the correlations between PPIs using various graph neu ral network (GNN) architectures [Kipf and Welling, 2016; Xuet al. , 2019 ]. However, they are limited by ignoring learning label dependencies for multitype PPI prediction. It has recently become common practice to employ Graph Convolutional Networks (GCNs) to capture label correlation in a wide range of multilabel tasks [Chen et al. , 2019b; Wang et al. , 2020 ]. Nevertheless, multilabel learning uti lizing label graphs predominantly works in the visual domain and has yet to be extended to PPI prediction tasks. In general, a desired PPI prediction framework should be efÔ¨Åcient, transferable, and generalizable, whereas two maarXiv:2305.08316v1  [qbio.MN]  15 May 2023jor bottlenecks deriving from imperfect datasets have hin dered the development of such models. Label scarcity: De spite the tremendous progress in PPI research using various computational and experimental methods, many interactions still need to be annotated from experimental data. Conse quently, only a small portion of labeled samples can be used for model training. It can be a signiÔ¨Åcant bottleneck in ob taining robust and accurate PPI prediction models. Domain shift: Most existing methods are only developed and val idated using indistribution data ( i.e., trainsethomologous testsets), receiving severe performance degradation when be ing deployed to unseen data with different distributions ( i.e., trainsetheterologous testsets). Although [Lvet al. , 2021 ]de sign new evaluations to better reÔ¨Çect model generalization, giving instructive and consistent assessment across datasets, the domain shift issue still needs to be fully explored for PPI prediction. Therefore, how to deal with imperfect data for improving model efÔ¨Åciency and generalization remains a vital issue in PPI prediction. Recent studies [Zhang et al. , 2021; Zhao et al. , 2022 ]show that selfensemble methods with semisupervised learning (SSL) [Laine and Aila, 2017; Tarvainen and Valpola, 2017 ]have demonstrated effective ness in addressing both label scarcity and domain shift. In this work, to tackle the above challenges and limita tions, we propose an efÔ¨Åcient and generalizable PPI predic tion framework, referred to as Selfensembling multiGraph Neural Network ( SemiGNNPPI ). Firstly, we propose lever aging graph structure to model protein correlations and label dependencies for multigraph learning. SpeciÔ¨Åcally, we learn interdependent classiÔ¨Åers to extract information from the la bel graph, which are then applied to the protein representa tions aggregated by neighbors in the protein graph for multi type PPI prediction. Secondly, we propose combining GNN with Mean Teacher [Tarvainen and Valpola, 2017 ], a power ful SSL model, to explore unlabeled data for selfensemble graph learning. In our framework, the student model learns to classify the labeled data accurately and also distills the knowledge beneath unlabeled data from the teacher model with multiple graph consistency constraints for improving the model performance under complex scenarios. To the best of our knowledge, this is the Ô¨Årst study to explore efÔ¨Åcient and generalizable multitype PPI prediction. Precisely, the main contributions of the work can be summarized as follows: ‚Ä¢ For multitype PPI prediction, we Ô¨Årst investigate the limitations and challenges of existing methods under complex but realistic scenarios, and then propose an ef fective Selfensembling multiGraph Neural Network based PPI prediction ( SemiGNNPPI ) framework for improving model efÔ¨Åciency and generalization. ‚Ä¢ In SemiGNNPPI, we construct multiple graphs to learn correlations between proteins and label dependencies si multaneously. We further advance GNN with Mean Teacher to effectively utilize unlabeled data by consis tency regularization with multiple constraints. ‚Ä¢ Extensive experiments on three PPI datasets with dif ferent settings demonstrate that SemiGNNPPI outper forms other stateoftheart methods for multilabel PPI prediction under various challenging scenarios.2 Related Work "
421,Self-Supervised Learning for Group Equivariant Neural Networks.txt,"This paper proposes a method to construct pretext tasks for self-supervised
learning on group equivariant neural networks. Group equivariant neural
networks are the models whose structure is restricted to commute with the
transformations on the input. Therefore, it is important to construct pretext
tasks for self-supervised learning that do not contradict this equivariance. To
ensure that training is consistent with the equivariance, we propose two
concepts for self-supervised tasks: equivariant pretext labels and invariant
contrastive loss. Equivariant pretext labels use a set of labels on which we
can define the transformations that correspond to the input change. Invariant
contrastive loss uses a modified contrastive loss that absorbs the effect of
transformations on each input. Experiments on standard image recognition
benchmarks demonstrate that the equivariant neural networks exploit the
proposed equivariant self-supervised tasks.","Selfsupervised learning is the method by which we deÔ¨Åne pretext tasks based on our prior knowledge about the input data and train the feature extractor using the pretext tasks without supervised labels. Pretext tasks include tasks to solve illposed problems such as image completion, tasks to predict image context, and tasks to make the features of the augmented images from the same image close to each other. Currently, selfsupervised learning demonstrates comparative accuracy to supervised learning and is an effective framework for learning features in an unsupervised manner. Another direction for utilizing prior knowledge is to incorporate the knowledge into the model structure of the feature extractor. For example, the convolutional layer is designed to be robust to the local translation of the object in an image. Group equivariant neural networks are the effective framework for utilizing the knowledge of invariance for the required transformations such as image rotations and image Ô¨Çipping in the neural networks structure. Given the input datax, the transformation on the input Tin(g), and the transformations on the output Tout(g), the group equivariant neural networks fare constructed to satisfy Tout(g)(f(x)) =f(Tin(g)(x))for any transformation parameter g, input datax, and model parameter . Because the structure of group equivariant neural networks are restricted to satisfy equivariance, this restriction regularizes the model, and the learned model generally demonstrates better accuracy than the standard nonequivariant neural networks. Therefore, we expect that we can obtain the effective feature learning method by combining these two ideas to utilize the prior knowledge. When we combine the idea of selfsupervised learning and the group equivariant neural networks, it is important to design the method such that these two components do not adversely affect each other. The functions that the group equivariant neural networks can learn are restricted to the mappings that preserve equivariance. Therefore, when a pretext task requires the mapping to violate the equivariance, it is difÔ¨Åcult to learn the function through this pretext task. In this paper, we propose a selfsupervised task that is suitable for group equivariant neural networks. The idea is to construct a selfsupervised loss that does not change under the transformations on the input data. This invariance guarantees that we can learn the same equivariant model even when we apply the considered transformations to thearXiv:2303.04427v1  [cs.CV]  8 Mar 2023SelfSupervised Learning for Group Equivariant Neural Networks A P REPRINT input. To construct a selfsupervised loss that satisÔ¨Åes this condition, we propose two concepts for selfsupervised loss: equivariant pretext labels and invariant contrastive loss. Equivariant pretext labels are constructed such that they are consistent with the considered transformations on the input. This indicates that when we apply the transformations on the input, the corresponding pretext labels are also changed according to the considered transformations. The invariant contrastive loss is a loss that does not change when we apply transformations to each input data. We extend several existing selfsupervised pretext tasks to satisfy these concepts. We apply the proposed loss to the image classiÔ¨Åcation model on the ImageNet dataset and evaluate the model using several image recognition benchmarks. We demonstrate that the trained group equivariant neural networks demonstrate good classiÔ¨Åcation accuracy when we use the proposed loss. The contributions of the paper are as follows: ‚Ä¢We propose the concepts of equivariant pretext labels and invariant contrastive loss to train the group equivariant neural networks in a selfsupervised manner. ‚Ä¢ We propose an equivariant extension to several existing selfsupervised tasks. ‚Ä¢We apply our method to standard image recognition benchmarks and demonstrate the effectiveness of the proposed loss. 2 Related Work "
422,Adaptive Regularization of Labels.txt,"Recently, a variety of regularization techniques have been widely applied in
deep neural networks, such as dropout, batch normalization, data augmentation,
and so on. These methods mainly focus on the regularization of weight
parameters to prevent overfitting effectively. In addition, label
regularization techniques such as label smoothing and label disturbance have
also been proposed with the motivation of adding a stochastic perturbation to
labels. In this paper, we propose a novel adaptive label regularization method,
which enables the neural network to learn from the erroneous experience and
update the optimal label representation online. On the other hand, compared
with knowledge distillation, which learns the correlation of categories using
teacher network, our proposed method requires only a minuscule increase in
parameters without cumbersome teacher network. Furthermore, we evaluate our
method on CIFAR-10/CIFAR-100/ImageNet datasets for image recognition tasks and
AGNews/Yahoo/Yelp-Full datasets for text classification tasks. The empirical
results show significant improvement under all experimental settings.","In recent years, supervised neural networks have been widely used in a variety of deep learning tasks with backpropagation technology. It is wellknown that the cross entropy loss function shows remarkable high performances in various tasks in practice. In simple terms, the deÔ¨Ånition of the cross entropy loss function is the cross entropy between the predicted output of the neural network and the onehot encoded of labels. The onehot encoded label is a group of bits among which the legal combinations of values are only those with a single 1(represents groundtruth) and all the others 0. This means that the cross entropy loss based on onehot encoded labels only focuses on the correctness of the groundtruth category. Malcolm Forbes once said that failure is success if we learn from it. In our approach, we make neural networks learn from previous erroneous experience and beneÔ¨Åt future learning. SpeciÔ¨Åcally, our proposed adaptive label regularization enables the neural network to focus not only the correctness but also the incorrectness during the training phase. Not coincidentally, previous researches [ 7,20,22,31,30,15,17,1] have suggested that the cross entropy loss based on onehot encoded labels may not be optimal in the classiÔ¨Åcation task. [ 22,26] aimed to regularize the neural network by adding a stochastic perturbation to labels. In addition, [ 7] shows that softening onehot encoded labels could provide more knowledge of the relevance of labels, which refers to as dark knowledge. For example, there exist lots of similarities between the images labeled as ""cat"" and the images labeled as ""dog"". The images of both categories contain four legs and one head, while the images labeled as ""plane"" are signiÔ¨Åcantly different from them. Since onehot corresponding author. Preprint. Under review.arXiv:1908.05474v1  [cs.LG]  15 Aug 2019encoded labels are orthogonal each other, they can not indicate the relevance of the labels (""cat"" and ""dog"" are independent of each other). Therefore, the soft label was proposed to solve this defect. The essence of the soft label is to soften the onehot encoded labels to vectors of category distribution. But it is very challenging to obtain good soft labels since they are difÔ¨Åcult to accurately express using priori knowledge. And another challenge is how to embed soft labels into the training phase of neural networks after getting good soft labels. To address the above challenges, [ 7] indicated that the soft label, i.e.the category distribution, was distilled by a cumbersome teacher network using qi=exp(zi=T)PK j=1exp(zj=T), whereKis the number of categories and Tis a temperature, using a higher value for Tproduces a softer probability. By learning from soft labels, the student network can compress parameters and even improve performance. It is worth noting that the existing researches on knowledge distillation [ 7,31,30,15] just added the soft loss to the original loss after obtaining a good category distribution. SpeciÔ¨Åcally, they used the KL divergence (equivalent to the cross entropy) between the soft labels and the output of the neural network (soft loss) as part of the loss function in order to supplement the cross entropy with onehot encoded labels (hard loss). The form is typically as L= (1 )Lhard+T2Lsoft, where is a tradeoff parameter and Tis a temperature same as above. But if we think deeply about this combination, we will have some interesting Ô¨Åndings. As an example, now given an image classiÔ¨Åcation task which contains three categories such as ""cat"", ""dog"", and ""plane"". Considered a model of this task, the loss function of the model is same as L. Suppose that the soft label of ""cat"" is [0:6;0:3;0:1]and the neural network model outputs a conÔ¨Ådence probability of [0:7;0:2;0:1] for an image labeled as ""cat"" during the training phase. As we know that the soft loss will push the output of the neural network to the soft label. That is, the soft loss Lsoft will decrease the conÔ¨Ådence probability of ""cat"" from 0:7to0:6. But the hard loss Lsoft,i.e.the cross entropy loss based on the onehot encoded label, will increase the conÔ¨Ådence probability of ""cat"" from 0:7to1. Obviously, the optimization goals of the soft loss Lsoftand the hard lossLhard in the loss function L arecontradictory . This unreasonable phenomenon has prompted us to rethink the loss of soft labels, and we will conduct theoretical analysis of it in later sections. In this work, we propose a novel adaptive label regularization method, which can achieve general improvement in supervised learning tasks. In summary, the contributions of our paper include: 1.We deÔ¨Åne the concept of residual correlation matrix andresidual label , and propose a novel loss function named residual loss . Based on these proposed items, our proposed method can not only adaptively regularize the neural network, but also enable the neural network to use erroneous experience in the training phase. 2.We show the consistency of residual labels by visualizing the residual correlation matrix . And we then analyze the reason why our method can prevent the neural network from overÔ¨Åtting based on the consistency of residual labels. 3.We perform comprehensive empirical evaluations on Ô¨Åve benchmark datasets of two general tasks (image recognition task and text classiÔ¨Åcation task). The empirical results show that our proposed method obtains signiÔ¨Åcant improvement under all settings. 2 Related work "
423,Optimal transport meets noisy label robust loss and MixUp regularization for domain adaptation.txt,"It is common in computer vision to be confronted with domain shift: images
which have the same class but different acquisition conditions. In domain
adaptation (DA), one wants to classify unlabeled target images using source
labeled images. Unfortunately, deep neural networks trained on a source
training set perform poorly on target images which do not belong to the
training domain. One strategy to improve these performances is to align the
source and target image distributions in an embedded space using optimal
transport (OT). However OT can cause negative transfer, i.e. aligning samples
with different labels, which leads to overfitting especially in the presence of
label shift between domains. In this work, we mitigate negative alignment by
explaining it as a noisy label assignment to target images. We then mitigate
its effect by appropriate regularization. We propose to couple the MixUp
regularization \citep{zhang2018mixup} with a loss that is robust to noisy
labels in order to improve domain adaptation performance. We show in an
extensive ablation study that a combination of the two techniques is critical
to achieve improved performance. Finally, we evaluate our method, called
\textsc{mixunbot}, on several benchmarks and real-world DA problems.","Deep neural networks have reached stateoftheart performance on classiÔ¨Åcation problems due to their ability to Ô¨Åt complex dataset while also generalizing within a speciÔ¨Åc domain (He et al., 2016). However, in applications like computer vision, it is standard to have sameclass samples coming from different domains, for instance when they have different backgrounds or colorspaces. Unfortunately, the generalization of deep neural networks across different domains is poor, and the object of intense research. Diversifying the domains with new samples in the training dataset is also a challenging task. For instance in medicine, collecting and annotating data is timeconsuming and prone to errors. The problem of domain adaptation , tackles the case where we have access to two domains sharing the same classes where one has labeled data, called the source domain, and the other has unlabeled data, called the target domain. The purpose of domain adaptation problems is to classify the unlabeled target samples using the labeled source samples (Pan & Yang, 2010; Patel et al., 2015). A popular and efÔ¨Åcient method to solve this problem is to use thealignment strategy , where sameclass samples from different domains are aligned in an embedded space. To align the embedded source and target samples, several techniques exist such as adversarial training (Ganin et al., 2016; Long et al., 2018; Chen et al., 2020a) or optimal transport (Courty et al., 2017; Courty et al., 2017; Redko et al., 2017). Optimal Transport (Peyr ¬¥e & Cuturi, 2019) has become one of the most used methods to compare probability distri butions in machine learning. It has been popular for tasks such as generative models (Arjovsky et al., 2017; Genevay et al., 2018; Salimans et al., 2018; Burnel et al., 2021) or supervised learning problems (Frogner et al., 2015; Fatras et al., 2021a). In domain adaptation, it has been used to transport the source domain to the target domains (Courty et al., 2017), or to align the domains in a joint space of data and labels (Courty et al., 2017; Bhushan Damodaran et al., 2018). To reduce the OT cost, Bhushan Damodaran et al. (2018) used a minibatch approximation of OT (Fa tras et al., 2020), which led to nonoptimal connections between domains due to the minibatch samplings and the marginal constraints of exact OT. This phenomenon corresponds to negative transfer in domain adaptation problems. To mitigate the nonoptimal connections of minibatch OT, DEEPJDOT (Bhushan Damodaran et al., 2018) required large batch sizes‚Äîotherwise small batch sizes lead deep neural networks to overÔ¨Åt. Another workaround to mitigate the nonoptimal connections was to use Unbalanced Optimal Transport (UOT), an OT variant with relaxed marginals, as proposed in Fatras et al. (2021b). In this paper, we explain nonoptimal connections as assigning noisy labels to target samples. To avoid overÔ¨Åtting from noisy labels, we propose to regularize the neural networks and to use a noisy label robust loss in the transfer corresponding author: kilian.fatras@mila.quebec 1arXiv:2206.11180v1  [cs.CV]  22 Jun 2022Published at 1st Conference on Lifelong Learning Agents, 2022 term. We propose to couple two techniques: i)regularize the neural networks using the MixUp regularization (Zhang et al., 2018a) on both source and target domains, MixUp interpolates samples from the same distribution uniformly at random as well as their label when available; ii)Use the symmetric crossentropy loss in the transfer term, which has been proven to be robust to label noise (Wang et al., 2019). Our Ô¨Åndings show that it is the combination of the MixUp and symmetric crossentropy which leads to an increase in the performances of models as shown in an extensive ablation study, while the use of the techniques separately does not lead to any increase. It can be used for different optimal transport loss as we show in our experiments. This paper is structured as follows. In Section 2, we review the different methods to solve domain adaptation and we deÔ¨Åne optimal transport as well as its use in domain adaptation problems. In Section 3, we present our method MIXUNBOT as well as the components it is built upon such as DEEPJDOT , MixUp, and the symmetric crossentropy (SCE) loss. In Section 4, we present extensive domain adaptation and partial domain adaptation experiments. 2 R ELATED WORK ON DOMAIN ADAPTATION AND OPTIMAL TRANSPORT "
424,SENT: Sentence-level Distant Relation Extraction via Negative Training.txt,"Distant supervision for relation extraction provides uniform bag labels for
each sentence inside the bag, while accurate sentence labels are important for
downstream applications that need the exact relation type. Directly using bag
labels for sentence-level training will introduce much noise, thus severely
degrading performance. In this work, we propose the use of negative training
(NT), in which a model is trained using complementary labels regarding that
``the instance does not belong to these complementary labels"". Since the
probability of selecting a true label as a complementary label is low, NT
provides less noisy information. Furthermore, the model trained with NT is able
to separate the noisy data from the training data. Based on NT, we propose a
sentence-level framework, SENT, for distant relation extraction. SENT not only
filters the noisy data to construct a cleaner dataset, but also performs a
re-labeling process to transform the noisy data into useful training data, thus
further benefiting the model's performance. Experimental results show the
significant improvement of the proposed method over previous methods on
sentence-level evaluation and de-noise effect.","Relation extraction (RE), which aims to extract the relation between entity pairs from unstructured text, is a fundamental task in natural language processing. The extracted relation facts can beneÔ¨Åt various downstream applications, e.g., knowledge graph completion (Bordes et al., 2013; Wang et al., 2014), information extraction (Wu and Weld, 2010) and question answering (Yao and Van Durme, 2014; Fader et al., 2014). A signiÔ¨Åcant challenge for relation extraction is the lack of largescale labeled data. Thus, distant  Corresponding authors. ? ? Obama is the 44thpresident of the United States .‚ë†Place_of_birth ‚ë°Employee_of ‚ë†Place_of_birth ‚ë°Employee_of Lived_in (unincluded label)The sentence bag of <Obama, United States>Which   label  ? Obama was back to the United States yesterday.ObamaUnited StatesObama was born in the  United States .Bag labels (We need) Sentence labelsFigure 1: Two types of noise exist in baglevel labels: 1) Multilabel noise: the exact label (‚Äúplace ofbirth‚Äù or ‚Äúemployee of‚Äù) for each sentence is unclear; 2) Wronglabel noise: the third sentence inside the bag actually expresses ‚Äúlive in‚Äù which is not included in the bag labels. supervision (Mintz et al., 2009) is proposed to gather training data through automatic alignment between a database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multi instance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classiÔ¨Åed into two categories: 1) the soft denoise methods that leverage soft weights to differentiate the inÔ¨Çuence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard denoise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these baglevel approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentence level relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentencelevel relation ex traction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng et al. (2018)) have made efforts on sentencelevel (or instancelevel)arXiv:2106.11566v1  [cs.CL]  22 Jun 2021distant RE, empirically verifying the deÔ¨Åciency of baglevel methods on sentencelevel evaluation. However, the instance selection approaches of these methods depend on rewards(Feng et al., 2018) or frequent patterns(Jia et al., 2019) determined by baglevel labels, which contain much noise. For one thing, one bag might be assigned to multiple bag labels, leading to difÔ¨Åculties in onetoone mapping between sentences and labels. As shown in Fig.1, we have no access to the exact relation between ‚Äúplace ofbirth‚Äù and ‚Äúemployee of‚Äù for the sentence ‚ÄúObama was born in the United States.‚Äù. For another, the sentences inside a bag might not express the bag relations. In Fig.1, the sentence ‚ÄúObama was back to the United States yesterday‚Äù actually express the relation ‚Äúlive in‚Äù, which is not included in the bag labels. In this work, we propose the use of negative training (NT) (Kim et al., 2019) for distant RE. Different from positive training (PT), NT trains a model by selecting the complementary labels of the given label, regarding that ‚Äúthe input sentence does not belong to this complementary label‚Äù. Since the probability of selecting a true label as a complementary label is low, NT decreases the risk of providing noisy information and prevents the model from overÔ¨Åtting the noisy data. Moreover, the model trained with NT is able to separate the noisy data from the training data (a histogram in Fig.3 shows the separated data distribution during NT). Based on NT, we propose SENT, a sentence level framework for distant RE. During SENT training, the noisy instances are not only Ô¨Åltered with a noiseÔ¨Åltering strategy, but also transformed into useful training data with a relabeling method. We further design an iterative training algorithm to take full advantage of these datareÔ¨Åning processes, which signiÔ¨Åcantly boost performance. Our codes are publicly available at Github1. To summarize the contribution of this work: ‚Ä¢We propose the use of negative training for sentencelevel distant RE, which greatly protects the model from noisy information. ‚Ä¢We present a sentencelevel framework, SENT, which includes a noiseÔ¨Åltering and a relabeling strategy for reÔ¨Åning distant data. ‚Ä¢The proposed method achieves signiÔ¨Åcant improvement over previous methods in terms of both RE performance and denoise effect. 1https://github.com/rtmaww/SENT2 Related Work "
425,Reconstructing a dynamical system and forecasting time series by self-consistent deep learning.txt,"We introduce a self-consistent deep-learning framework which, for a noisy
deterministic time series, provides unsupervised filtering, state-space
reconstruction, identification of the underlying differential equations and
forecasting. Without a priori information on the signal, we embed the time
series in a state space, where deterministic structures, i.e. attractors, are
revealed. Under the assumption that the evolution of solution trajectories is
described by an unknown dynamical system, we filter out stochastic outliers.
The embedding function, the solution trajectories and the dynamical systems are
constructed using deep neural networks, respectively. By exploiting the
differentiability of the neural solution trajectory, the neural dynamical
system is defined locally at each time, mitigating the need for propagating
gradients through numerical solvers. On a chaotic time series masked by
additive Gaussian noise, we demonstrate the filtering ability and the
predictive power of the proposed framework.","Time series analysis and time series forecasting have been studied to extract information about the data and the underlying dynamics, and to predict the future of observables from past measurements. The Ô¨Årst systematic modeling of time series dates back to 1927 when Yule [1927] introduced a linear autoregression model to reveal the dynamics of sunspot numbers. The model which writes as u(k+ 1) =m 1X n=0a(n)u(k n) +e(k); (1) takes the form of a linear difference equation, which states that the future u(k+ 1) is a weighted sum of the past values in the sequence, with the error term e(k). Here,mdenotes the regression order. Since linear equations lead to only exponential or periodic motions, analyzing systems characterized by irregular motions calls for nonlinear autoregression models which advantageously employ neural networks. Retaining the basic form of Eq. (1), neural autoregression models can be expressed as u(k+ 1) =N[u(k);u(k 1);:::;u (k (m 1));] +e(k); (2) where the model parameters of the neural network, denoted by N(), are determined by minimizing the deviation e(k). Since the Ô¨Årst application of neural networks to reconstruct the governing equations underlying a chaotic timearXiv:2108.01862v1  [cs.LG]  4 Aug 2021Deep dynamical system reconstruction & forecasting A P REPRINT series by Lapedes and Farber [1987], various network architectures including multilayer perceptrons [Weigend et al., 1990, Weigend, 1991], time delayed neural networks [Wan, 1994, Saad et al., 1998], convolution neural networks [Borovykh et al., 2017], recurrent neural networks [Gers et al., 2002, Mirowski and LeCun, 2009, Dubois et al., 2020], and more recently transformers [S. Li et al., 2019, Lim et al., 2021], have been explored to model time series arising from physics, engineering, biological science, Ô¨Ånance,... Using neural networks as a nonlinear autoregression model stems from the universal approximation theorem which states that a sufÔ¨Åciently deep neural network can approximate any arbitrary wellbehaved nonlinear function with a Ô¨Ånite set of parameters [Gorban and Wunsch, 1998, Winkler and Le, 2017, Lin and Jegelka, 2018] and from the Takens [1981] theorem in the following manner. Let v(t)be a state vector on the solution manifold and let dv(t) dt=f[v(t)]; (3) be the governing equation. One seldom has access to v(t). Instead, the state of dynamical systems is partially observed and inferred through a sequence of scalar measurements u(k)sampled at discrete times and, frequently, masked by noise. For a sufÔ¨Åciently large dimension m2Z+and an arbitrary delay time 2R+, Takens [1981] theorem afÔ¨Årms the existence of a diffeomorphism between a delay vector ~uand the underlying state v(t)of the dynamical system. This implies that there exists a nonlinear mapping u(k+ 1) =g[~u(k)]which models the time series exactly. In virtue of the universal approximation theorem, neural networks could capture the g()mapping. Going beyond merely Ô¨Ånding the underlying difference equations (2), inspired by recent works in learning differential equations [Chen et al., 2018, Ayed et al., 2019, Raissi et al., 2019, Rackauckas et al., 2020], we propose to reconstruct a latent continuoustime dynamical system du(t) dt=Au(t) +e(t)withA2Rm2; (4) for the evolution of the solution trajectory u(t)in a reconstructed state space u(t)E[u(t)]withE:R!Rm: (5) The discrete measurements are parameterized by a continuous and differentiable function u(t). The aim of this paper is to propose an algorithmic scheme enabling the reconstruction of the latent continuous dynamics. Through a selfconsistent process to be discussed in Sec. 3, the matrix A, the embeddingE(), and the Ô¨Åtting function u(t)are constructed using deep neural networks, which are learned jointly. The proposed scheme is tested on a synthetic time series, which is sampled from the Lorenz attractor [Lorenz, 1963] and masked by additive Gaussian noise, in Sec. 4. Finally, the limitations of the proposed scheme are discussed in Sec. 5, where conclusions are drawn. 2 Related works "
426,Deep Bayesian Active Semi-Supervised Learning.txt,"In many applications the process of generating label information is expensive
and time consuming. We present a new method that combines active and
semi-supervised deep learning to achieve high generalization performance from a
deep convolutional neural network with as few known labels as possible. In a
setting where a small amount of labeled data as well as a large amount of
unlabeled data is available, our method first learns the labeled data set. This
initialization is followed by an expectation maximization algorithm, where
further training reduces classification entropy on the unlabeled data by
targeting a low entropy fit which is consistent with the labeled data. In
addition the algorithm asks at a specified frequency an oracle for labels of
data with entropy above a certain entropy quantile. Using this active learning
component we obtain an agile labeling process that achieves high accuracy, but
requires only a small amount of known labels. For the MNIST dataset we report
an error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These
results are obtained without employing any special network architecture or data
augmentation.","In recent years deep learning has shown great po tential in solving classication and regression tasks of increasing complexity and diculty. For aca demic purposes, several labeled data sets with as sociated tasks are available to support and facilitate research on machine learning. Though in many practical applications (e.g. in industry, medicine and microbiology) where raw data is available in abundance, labeled information is not readily avail able and the process of generating labels can be time consuming and expensive. Therefore, the de velopment of methods that provide strong predic tive models from as few labels as possible is a eld of high interest. The elds of active learning and semisupervised learning address this issue and provide two ap Bergische Universiat at Wuppertal, Fac ulty of Mathematics and Natural Sciences, frottmann,kkahlg@math.uniwuppertal.de , hanno.gottschalk@uniwuppertal.deproaches to obtain strong predictive models using only few labels, see Gal et al. 2016; Hu et al. 2017; Kingma et al. 2014a; Lee 2013; Pitelis et al. 2014; Rasmus et al. 2015; Rifai et al. 2011; Weston et al. 2012. They both assume a situation where the complete set of data is large, but labels are known only for a small fraction of it. The eld of semisupervised learning has a long history. Already in Suddarth et al. 1990 unlabeled data had been injected into the training of neural networks in order to improve generalization perfor mance. Most approaches rely on the Expectation Maximization (cf. Dempster et al. 1977, EM) tech nique which is a clustering algorithm. In the semi supervised context, EM is used to assign unlabeled data to a nite number of clusters which are ini tially dened by the small set of labeled samples. That is, an initial model is trained and then, using the resulting model, labels are assigned to unla beled data, which in turn are used to further train the model. The pseudolabel approach, introduced in Lee 2013, is such an EM technique. It uses the la bels predicted by the neural network itself and can be viewed as well as an auxiliary loss in the training phase which is inserted to reduce classication en tropy on the unlabeled data. It is well known that the EM strategy works well in presence of low den sity class separation. Thus it is unclear if and how this approach is able to adequately classify sam ples with high classication uncertainty. In case a quantitative measure of classication uncertainty can be dened, unlabeled data should only be used for training if their uncertainty is small. However, some samples typically retain high uncertainty in the semisupervised training cycle. This is where active learning comes into play, in that it is most valuable to acquire ground truth labels for samples with high classication uncertainty and add those to training. In this way active and semisupervised learning complement each other naturally. Both learning approaches benet from good un certainty quantication mechanisms. With the ad vent of MonteCarlo (MC) dropout Gal et al. 2016, we have an instrument at hand that makes it fea sible to construct sensitive metrics to monitor clas sication uncertainty. Bayesian inference has been used in an active deep learning approach introduced 1arXiv:1803.01216v1  [cs.LG]  3 Mar 2018in Gal et al. 2017. In recent years, there were also eorts on design ing specialized network architectures that incorpo rate components like denoising autoencoders, see Rasmus et al. 2015. Also deep generative models were used for semisupervised learning, see Kingma et al. 2014b. Combining the active learning and the semi supervised learning track, a method for synthetic aperture radar image recognition has been pub lished in Gao et al. 2017. In this paper, we present a deep Bayesian Active SemiSupervised learning (deepBASS) approach that is based on an EM deep learning approach for classication tasks paired with an active learn ing component and approximate Bayesian uncer tainty. We rst train a Convolutional Neural Net work (CNN) on a small sample of labeled training data. Afterwards we employ the EM technique, i.e., we iteratively predict classes and assign these as pseudolabels to the unlabeled data set. Then, we train one epoch on the pseudolabeled data and the groundtruthlabeled data. While doing so, we make sure that the prediction accuracy on the ground truth remains high. During this process, the algorithm asks an oracle for additional label information where the neural network shows in creased classication uncertainty, e.g., high classi cation entropy. For all predictions and uncertainty estimations we incorporate MC dropout inference. The remainder of this work is structured as fol lows: In section 2 we classify our method with re spect to existing approaches in the literature. Then we introduce our method in detail in section 3 in cluding all necessary notations. Using a simple toy example in section 4 we motivate the combination of active and semisupervised learning. Using the MNIST dataset, we compare two settings in sec tion 5 where on one hand all unlabeled data is present in training from the beginning and where on the other hand unlabeled data is added only in crementally. Both settings are combined with two dierent label acquisition policies. Concluding the experiments we compare our method with other semisupervised and active learning approaches. 2 Related Work "
427,GPRAR: Graph Convolutional Network based Pose Reconstruction and Action Recognition for Human Trajectory Prediction.txt,"Prediction with high accuracy is essential for various applications such as
autonomous driving. Existing prediction models are easily prone to errors in
real-world settings where observations (e.g. human poses and locations) are
often noisy. To address this problem, we introduce GPRAR, a graph convolutional
network based pose reconstruction and action recognition for human trajectory
prediction. The key idea of GPRAR is to generate robust features: human poses
and actions, under noisy scenarios. To this end, we design GPRAR using two
novel sub-networks: PRAR (Pose Reconstruction and Action Recognition) and FA
(Feature Aggregator). PRAR aims to simultaneously reconstruct human poses and
action features from the coherent and structural properties of human skeletons.
It is a network of an encoder and two decoders, each of which comprises
multiple layers of spatiotemporal graph convolutional networks. Moreover, we
propose a Feature Aggregator (FA) to channel-wise aggregate the learned
features: human poses, actions, locations, and camera motion using
encoder-decoder based temporal convolutional neural networks to predict future
locations. Extensive experiments on the commonly used datasets: JAAD [13] and
TITAN [19] show accuracy improvements of GPRAR over state-of-theart models.
Specifically, GPRAR improves the prediction accuracy up to 22% and 50% under
noisy observations on JAAD and TITAN datasets, respectively","Accurate prediction of human trajectory, i.e., forecast ing pedestrians‚Äô future locations given their past (observed) frames in dynamic scenes, is critical for various applications such as autonomous driving [16], robotic navigation sys tems [18], and pedestrian tracking [21]. For the most part, challenges associated with predicting future trajectories are due to the presence of a multitude of features that may in Ô¨Çuence human future paths such as camera motion (ego Recognized action: walkingReconstructed poses and locationsfuture  trajectory PRARegomotion FA noisy human skeletons Figure 1: GPRAR addresses the problem of future trajec tory prediction given noisy pose observations by two novel subnetworks: (1) a human Pose Reconstruction and Action Recognition network (PRAR) to simultaneously reconstruct the noisy poses and recognize human actions, and (2) Fea ture Aggregator (FA) to channelwise aggregate the learned features: reconstructed poses and locations, actions, and egomotion to predict pedestrians‚Äô future locations. motion), human shapes (pose), past locations, and human actions. More importantly, these features are often noisy due to environmental and scene impediments, occlusions for example. This problem has signiÔ¨Åcantly degraded the performance of feature extractors, which in turn degrades the accuracy of the existing prediction models. Recent deeplearningbased methods[23, 25, 5, 1] have shown promising prediction results in ‚Äòperfect‚Äô settings, where ground truth (or complete) observations are given. Using ground truth observations helps model human motion more accurately and may improve the prediction accuracies. However, the ground truth data is unavailable during test time. This limits the potential applicability of these methodsarXiv:2103.14113v1  [cs.CV]  25 Mar 2021in practice. Other methods [20, 26] rely on preprocessing techniques to denoise the observations in advance of test ing. These approaches mainly focus on preprocessing (i.e. reconstructing or denoising) the human skeleton, an impor tant feature for prediction. However, they are easily prone to errors under harsh conditions, such as fast camera motion and occlusions, especially in dynamic scenes. In this work, the following challenges are addressed: (1) reconstruction of human pose, which is a nontrivial task in computer vi sion. To the best of our knowledge, none of existing meth ods successfully reconstruct human skeletons in dynamic video sequences by exploiting the structural properties of human skeletons spatially and temporally. (2) the use of lowlevel human pose features to learn the higherlevel ac tion features. So far, the skeletonbased action features have not been considered for prediction tasks. We design GPRAR to predict human future trajectory under noisy observations in dynamic video scenes by de vising solutions to the above challenges. It consists of two novel subnetworks: (1) a human pose reconstruction and action recognition network (PRAR) and (2) an encoder decoder based Feature Aggregator (FA), shown in Figure 1. The underlying idea of PRAR is to reconstruct human poses and learn action features simultaneously from the noisy pose detections. To best exploit the coherent and structural properties of human skeletons, PRAR is implemented with an encoder and two decoders, where each encoder and de coder is a multilayer spatiotemporal graph convolutional network operating on the naturally connected human joints (or pose graph). Furthermore, we propose an encoder decoder FA to channelwise aggregate the learned features: reconstructed poses and locations, actions, and camera mo tion using temporal convolutional networks (TCNs). The aggregated feature is then used to output the future trajec tory of a target pedestrian. In summary, the contributions of this paper are as follows: ‚Ä¢ We propose an efÔ¨Åcient and robust human trajectory prediction network (GPRAR) under noisy observa tions (Section 3). GPRAR consists of two novel subnetworks: a human pose reconstruction and ac tion recognition network (PRAR) and (2) an encoder decoder based Feature Aggregator (FA). ‚Ä¢ We evaluate our model on two commonly used datasets: TITAN and JAAD, and show that our method outperforms other methods with a large margin under noisy scenarios (Section 4). We also conduct ablation studies to demonstrate the effectiveness of each system component. 2. Related Work "
428,Random Vector Functional Link Neural Network based Ensemble Deep Learning.txt,"In this paper, we propose a deep learning framework based on randomized
neural network. In particular, inspired by the principles of Random Vector
Functional Link (RVFL) network, we present a deep RVFL network (dRVFL) with
stacked layers. The parameters of the hidden layers of the dRVFL are randomly
generated within a suitable range and kept fixed while the output weights are
computed using the closed form solution as in a standard RVFL network. We also
propose an ensemble deep network (edRVFL) that can be regarded as a marriage of
ensemble learning with deep learning. Unlike traditional ensembling approaches
that require training several models independently from scratch, edRVFL is
obtained by training a single dRVFL network once. Both dRVFL and edRVFL
frameworks are generic and can be used with any RVFL variant. To illustrate
this, we integrate the deep learning networks with a recently proposed
sparse-pretrained RVFL (SP-RVFL). Extensive experiments on benchmark datasets
from diverse domains show the superior performance of our proposed deep RVFL
networks.","Deep Learning, also known as representational learning, has sparked a surg ing interest in neural networks amongst the machine learning enthusiasts with the stateoftheart results in diverse applications ranging from image/video classication to segmentation, action recognition and many others. The supe riority of a deep learning model emanates from its potential ability to extract meaningful representations at dierent levels of the hierarchical model while disentangling a complex task into several simpler ones [1]. Deep neural networks typically consist of multiple hidden layers stacked together. Each hidden layer builds an internal representation of the data with the hidden layers closer to the input layer learning simple features such as edges and layers above them learning sophisticated (complex) features [1, 2]. With such stacked layers, deep learning models typically have thousands of model parameters that need to be optimized during the training phase. These networks are typically trained using backpropagation (BP) technique so as to minimize the loss function (crossentropy or mean square error or others depending on the particular task). In addition to be timeconsuming, such models may fail to converge to a global minimum, thus, giving suboptimal performance or lower generalization [3]. Also, such deep learning models require large amount of training data. While the usual image and speech datasets that are commonly used with deep learning models have abundant data, there are datasets from a wide variety of domains, such as agriculture, credit scoring, health outcomes, ecology and others, with very limited data size. The performance of the state oftheart deep learning models on such datasets are far from superior [4]. Apart from the conventional BPtrained neural networks, there has also been a growing interest in the class of randomization based neural networks [5, 6, 7]. Randomization based neural networks with closed form solution avoid the pit falls of conventional BPtrained neural networks [3, 8, 9]. They are faster to train and have demonstrated good learning performance [10, 11]. Among the random ization based methods, Random Vector Functional Link (RVFL) [12] network 2has rapidly gained signicant traction because of its superior performance in several diverse domains ranging from visual tracking [13], classication [14, 15], regression [16], to forecasting [17, 18]. RVFL is a single layer feedforward neu ral network (SLFN) in which the weights and biases of the hidden neurons are randomly generated within a suitable range and kept xed while the output weights are computed via a simple closed form solution [12, 19]. Randomization based neural networks greatly benet from the presence of direct links from the input layer to the output layer as in RVFL network [16, 18, 20]. The original features are reused or propagated to the output layer via the direct links. The direct links act as a regularization for the randomization [21, 22]. It also helps to keep the model complexity low with the RVFL network being thinner and simpler compared to its other counterparts. With the Occam's Razor principle and PAC learning theory [23] advocating for simpler and less complex mod els, this makes the RVFL network attractive to use compared to other similar randomized neural networks. Ensembles of neural networks are known to be much more robust and ac curate than individual networks [20, 24, 25, 26]. Because of the existence of several randomization operations in their training procedure, neural networks are regarded as unstable algorithms whose performance greatly vary even when there is a small perturbation in training set or random seed. It is therefore not surprising that two neural networks with identical architectures optimized with dierent initialization or slightly perturbed training data will converge to dif ferent solutions. This diversity can be exploited through ensembling, in which multiple neural networks are trained with slightly dierent training set or pa rameters and then combined with majority voting or averaging. Ensembling often leads to drastic reductions in error rates. However, this comes with an ob vious trade o: computational cost. While ensembling shallow neural networks doesn't incur great computational cost, the same is not true for the ensembling of deep networks. With the current trend of building deep networks, there have also been sev eral attempts in the literature to build deep or multilayer networks based on 3randomized neural networks [27, 28, 29]. Even though there exist several deep learning models with randomized neural networks, there are limited works in the context of RVFL network. In this paper, we investigate the performance of deep learning and ensemble deep learning models based on RVFL networks. To the best of our knowledge, [28] is one of the pioneering paper to propose multilayer RVFL network. However, the performance of the multilayer RVFL network compared to a shallow RVFL network (with 1 hidden layer) is suboptimal and nonpersuasive. A deep model enriched with complex feature learning capa bilities should achieve good generalization. Thus, in this paper, we propose deep neural networks based on RVFL while maintaining its advantages of lower complexity, training eciency and good generalization. We also propose an en semble of such deep networks without incurring any signicant training costs. Specically, we propose an ensemble deep RVFL network which can be regarded as a marriage of ensemble and deep learning that is simple and straightforward to implement. The key contributions of this paper are summarized as follows: ‚Ä¢We propose a deep RVFL network (dRVFL), an extension of RVFL for representational learning. The dRVFL network consists of several hidden layers stacked on top of each other. The parameters of the hidden lay ers are randomly generated and kept xed while only the output weights need to be computed. Thus, the deep RVFL network emanates from the standard RVFL network. ‚Ä¢We also propose an implicit ensembling approach called ensemble deep RVFL framework (edRVFL), a marriage of ensembling learning with deep learning. Instead of training Lneural networks independently from scratch as in traditional ensembling method, we only train a single deep RVFL network. The ensemble consists of Lmodels equivalent to the number of hidden layers in the single deep RVFL network. The ensemble is trained in such a way that the higher models (equivalent to higher layers in deep RVFL network) utilize both the original features (from direct links as in standard RVFL network) and nonlinearly transformed features from the 4preceding layers. Thus, the framework is consistent with the tenets of both ensemble learning and deep learning at the same time. The training cost of edRVFL is slightly higher than that of a single dRVFL network while it is signicantly lower than that of traditional ensembles. ‚Ä¢The deep learning models proposed in this paper (dRVFL and edRVFL) are generic and are applicable with any RVFL variant. We create deep learning models using both standard RVFL and recently proposed sparse pretrained RVFL (SPRVFL) [30]. ‚Ä¢With extensive experiments on several realworld classication datasets, we show that our proposed deep RVFL models (dRVFL and edRVFL) have superior performance compared to other relevant neural networks. The rest of this paper is structured as follows. Section 2 gives a brief overview of related works on shallow randomized neural networks followed by random ization based multilayer neural network. Section 3 details our proposed deep RVFL method followed by its ensemble. In Section 4, we compare the perfor mance of our proposed methods with other relevant neural networks. Finally, the conclusion is presented in Section 5. 2. Related Works "
429,Co-Seg: An Image Segmentation Framework Against Label Corruption.txt,"Supervised deep learning performance is heavily tied to the availability of
high-quality labels for training. Neural networks can gradually overfit
corrupted labels if directly trained on noisy datasets, leading to severe
performance degradation at test time. In this paper, we propose a novel deep
learning framework, namely Co-Seg, to collaboratively train segmentation
networks on datasets which include low-quality noisy labels. Our approach first
trains two networks simultaneously to sift through all samples and obtain a
subset with reliable labels. Then, an efficient yet easily-implemented label
correction strategy is applied to enrich the reliable subset. Finally, using
the updated dataset, we retrain the segmentation network to finalize its
parameters. Experiments in two noisy labels scenarios demonstrate that our
proposed model can achieve results comparable to those obtained from supervised
learning trained on the noise-free labels. In addition, our framework can be
easily implemented in any segmentation algorithm to increase its robustness to
noisy labels.","Recent years have witnessed an upsurge of interests in biomedical segmentation. Based on fully convolutional net  works, UNet [1] has been emerging as a classic model which concatenates multiscale features from the downsampling layers and the upsampling layers. By stacking two UNet architectures on top of each other, DoubleUnet [2] is an improved version of UNet aiming to achieve higher perfor mance on speciÔ¨Åc tasks. CENet [3] modiÔ¨Åed UNet structure by adopting pretrained ResNet blocks in the feature encodin g step to capture highlevel spatial information. However, t hese fully supervised learning algorithms are vulnerable to lab el noise and their performance may be hugely degenerated by noisy labels. Therefore, under noisy labels, it is importan t to identify and selectively learn from a clean and reliable sub set of samples which mainly include data with clean labels, rather than learning from the whole sample set. How to improve deep learning performance under noisy labels conditions has caught great attention [4, 5, 6, 7, 8, 9 ]. One direction is to estimate the mathematical model of a nois e distribution [6, 7]. In [7], two procedures were proposed fo r loss correction and noise transition matrix estimation. An  other direction is to directly train on clean samples [8, 9]. Co teaching [8] trains two networks simultaneously to pick cle an samples for each one. However, most current approaches fo cus on classiÔ¨Åcation tasks, which cannot be applied to the se g mentation where labels are spatially arranged in a dense man  ner. Finally, samplebased reweighting methods [8, 9, 10] j ust ignore or assign small weights on noisy samples, which can lead to severe overÔ¨Åtting, especially for small datasets. In this paper, we propose a novel deep learning frame work for image segmentation, namely CoSegmentation (Co Seg), to handle noisy labels. Our framework integrates the idea of selective training and label correction. In particu lar, we propose a robust training network to collaboratively lea rn and select samples with reliable labels. Then a label correc  tion scheme is proposed to enrich the reliable dataset and we retrain a new network on the updated dataset. Experimental results using CoSeg on noisy labels show performance com parable to supervised learning on noisefree labels. In sum  mary, this paper has the following contributions: (1) We develop an easilyimplemented yet effective frame work for image segmentation tasks with noisy labels. It can be easily applied to any deep learning segmentation model to increase learning ability under noisy labels. (2) We demonstrate that, in multiple noise settings, our mod el achieves comparable results to supervised training on nois e free datasets. 2. METHODOLOGY "
430,Identification of Rhetorical Roles of Sentences in Indian Legal Judgments.txt,"Automatically understanding the rhetorical roles of sentences in a legal case
judgement is an important problem to solve, since it can help in several
downstream tasks like summarization of legal judgments, legal search, and so
on. The task is challenging since legal case documents are usually not
well-structured, and these rhetorical roles may be subjective (as evident from
variation of opinions between legal experts). In this paper, we address this
task for judgments from the Supreme Court of India. We label sentences in 50
documents using multiple human annotators, and perform an extensive analysis of
the human-assigned labels. We also attempt automatic identification of the
rhetorical roles of sentences. While prior approaches towards this task used
Conditional Random Fields over manually handcrafted features, we explore the
use of deep neural models which do not require hand-crafting of features.
Experiments show that neural models perform much better in this task than
baseline methods which use handcrafted features.","Rhetorical role labelling of sentences in a legal document r efers to understanding what semantic function a sentence is associated with, such as fac ts of the case, arguments of the parties, the Ô¨Ånal judgement of the court, and so on. Ident ifying the rhetorical roles of sentences in a legal case document can help in a variety of d ownstream tasks like se mantic search [1], summarization [2,3], case law analysis [ 4], and so on. However, legal case documents are usually not well structured [5,6], and va rious themes often interleave with each other. For instance, the reason behind the judgmen t (Ratio of the decision) of ten interleaves with Precedents and Statutes. Hence it some times becomes difÔ¨Åcult even for human experts to understand the intricate differences b etween the rhetorical roles. Hence, automating the identiÔ¨Åcation of these rhetorical roles is a challengin g task. For supervised machine learning of the roles, it is importan t to develop a high qual ity gold standard corpus, capturing the rhetorical roles of sentences as accurately as pos sible. Different approaches for the task have constructed t heir own set of annotated doc 1Equal contribution by the Ô¨Årst and second authors. 2Corresponding Author: Paheli Bhattacharya; Email: paheli .cse.iitkgp@gmail.comuments [1, 2, 4], but do not report an extensive analysis on th e annotation process. Apart from InterAnnotator Agreement (IAA) scores, it is useful t o understand issues such as the amount of subjectivity associated to the labels. In this paper, we perform a system atic annotation study and an extensive interannotator stu dy. We show that even legal ex perts Ô¨Ånd it difÔ¨Åcult to distinguish some speciÔ¨Åc pairs of la bels, thus showing that some subjectivity is inherent in these labels. Prior attempts to automate the identiÔ¨Åcation of rhetorical roles of sentences in legal documents [2‚Äì4] rely on handcrafted features (see Section 2 for details) such as lin guistic cue phrases indicative of a particular rhetorical r ole [2, 3, 7], the sequential ar rangement of labels [2], and so on. Some of these features, e. g., indicator cue phrases, arelargely dependent on legalexpert knowledge which is expensive to obtain. Also, the handcrafted features developed in the prior works are ofte n speciÔ¨Åc to one or a few do mains/categories (e.g., Cyber crime and Trade secrets in [4 ]). It has not been explored whether one can devise a set of features that works for docume nts across domains. Recently developed deep learning, neural network models do not require hand engineering features, but are able to automatically learn t he features, given sufÔ¨Åcient amounts of training data. Additionally, such models perfor m better in tasks like classiÔ¨Å cation than methods using handcrafted features. In this paper, we explore two neural network models to automa tically identify the rhetorical roles of sentences in legal documents ‚Äì (i) a Hier archical BiLSTM model, and (ii) a Hierarchical BiLSTMCRF model. Similar models have b een used in the medical domain [8], but to our knowledge, this work is the Ô¨Årst to use t hem in the legal domain. We use these models for supervised classiÔ¨Åcation across seven rhetorical labels (classes) and over documents from Ô¨Åve different legal domains . The Hierarchical BiLSTMCRF model achieves a very good performance (Macro Fscore in the range[0.8‚àí0.9]), out performing baseline methods that use handcrafted feature s. We also analyse the rhetor ical roles predicted by our model, and Ô¨Ånd that the subjectiv ity between certain pairs of labels (e.g., Ratio vs. Precedent) that is present among t he human annotators is also reÔ¨Çected in the predictions by the algorithm. This is the Ô¨Årst paper on identifying rhetorical roles of sen tences in legal documents that brings together (i) an extensive annotation study, and (ii) deep learning models for automating the task.3 2. Related Work "
431,Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing.txt,"In this paper, we introduce a new problem, named audio-visual video parsing,
which aims to parse a video into temporal event segments and label them as
either audible, visible, or both. Such a problem is essential for a complete
understanding of the scene depicted inside a video. To facilitate exploration,
we collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual
video parsing in a weakly-supervised manner. This task can be naturally
formulated as a Multimodal Multiple Instance Learning (MMIL) problem.
Concretely, we propose a novel hybrid attention network to explore unimodal and
cross-modal temporal contexts simultaneously. We develop an attentive MMIL
pooling method to adaptively explore useful audio and visual content from
different temporal extent and modalities. Furthermore, we discover and mitigate
modality bias and noisy label issues with an individual-guided learning
mechanism and label smoothing technique, respectively. Experimental results
show that the challenging audio-visual video parsing can be achieved even with
only video-level weak labels. Our proposed framework can effectively leverage
unimodal and cross-modal temporal contexts and alleviate modality bias and
noisy labels problems.","Human perception involves complex analyses of visual, auditory, tactile, gustatory, olfactory, and other sensory data. Numerous psychological and brain cognitive studies [ 3,20,46,51] show that combining dierent sensory data is crucial for human perception. However, the vast majority of work [ 9,26,48,64] in scene understanding, an essential perception task, focuses on visualonly methods ignoring other sensory modalities. They are inherently limited. For example, when the object of interest is outside of the eldofview (FoV), one would rely on audio cues for localization. While there is little data on tactile, gustatory, or olfactory signals, we do have an abundance of multimodal audiovisual data, e.g., YouTube videos. Utilizing and learning from both auditory and visual modalities is an emerg ing research topic. Recent years have seen progress in learning representa tions [ 1,2,19,23,37,38], separating visually indicated sounds [ 8,11,12,13,65,66,10,70],arXiv:2007.10558v1  [cs.CV]  21 Jul 20202 Y. Tian et al. A: Basketball5s10sDog 4s8sV: BasketballAV: Dog0s10s0s5s Speech2s3sBasketballSpeech2s3sBasketball5s10s0s5sSpeech2s3sBasketballDog2s3s2s 2s5s3s10s4s5s10s Fig. 1: Our audiovisual video parsing model aims to parse a video into dierent audio (audible), visual (visible), and audiovisual (audivisible) events with correct categories and boundaries. A dog in the video visually appears from 2nd second to 5th second and make barking sounds from 4th second to 8th second. So, we have audio event (4s8s), visual event (2s5s), and audiovisual event (4s5s) for theDog event category. spatially localizing visible sound sources [ 37,45,55], and temporally localizing audiovisual synchronized segments [ 27,55,63]. However, past approaches usually assume audio and visual data are always correlated or even temporally aligned. In practice, when we analyze the video scene, many videos have audible sounds, which originate outside of the FoV, leaving no visual correspondences, but still contribute to the overall understanding, such as outofscreen running cars and a narrating person. Such examples are ubiquitous, which leads us to some basic questions: what video events are audible, visible, and \audivisible,"" where and when are these events inside of a video, and how can we eectively detect them? To answer the above questions, we pose and try to tackle a fundamental problem: audiovisual video parsing that recognizes event categories bind to sensory modalities, and meanwhile, nds temporal boundaries of when such an event starts and ends (see Fig. 1). However, learning a fully supervised audiovisual video parsing model requires densely annotated event modality and category labels with corresponding event onsets and osets, which will make the labeling process extremely expensive and timeconsuming. To avoid tedious labeling, we explore weaklysupervised learning for the task, which only requires sparse labeling on the presence or absence of video events. The weak labels are easier to annotate and can be gathered in a large scale from web videos. We formulate the weaklysupervised audiovisual video parsing as a Multi modal Multiple Instance Learning (MMIL) problem and propose a new framework to solve it. Concretely, we use a new hybrid attention network (HAN) for lever aging unimodal and crossmodal temporal contexts simultaneously. We developWeaklySupervised AudioVisual Video Parsing 3 an attentive MMIL pooling method for adaptively aggregating useful audio and visual content from dierent temporal extent and modalities. Furthermore, we discover modality bias and noisy label issues and alleviate them with an individualguided learning mechanism and label smoothing [42], respectively. To facilitate our investigations, we collect a Look, listen, and Parse (LLP) dataset that has 11 ;849 YouTube video clips from 25 event categories. We label them with sparse videolevel event labels for training. For evaluation, we label a set of precise labels, including event modalities, event categories, and their temporal boundaries. Experimental results show that it is tractable to learn audio visual video parsing even with videolevel weak labels. Our proposed HAN model can eectively leverage multimodal temporal contexts. Furthermore, modality bias and noisy label problems can be addressed with the proposed individual learning strategy and label smoothing, respectively. Besides, we make a discussion on the potential applications enabled by audiovisual video parsing. The contributions of our work include: (1) a new audiovisual video parsing task towards a unied multisensory perception; (2) a novel hybrid attention network to leverage unimodal and crossmodal temporal contexts simultaneously; (3) an eective attentive MMIL pooling to aggregate multimodal information adaptively; (4) a new individual guided learning approach to mitigate the modality bias in the MMIL problem and label smoothing to alleviate noisy labels; and (5) a newly collected largescale video dataset, named LLP, for audiovisual video parsing. Dataset, code, and pretrained models are publicly available in https://github.com/YapengTian/AVVPECCV20 . 2 Related Work "
432,Bootstrapping the Relationship Between Images and Their Clean and Noisy Labels.txt,"Many state-of-the-art noisy-label learning methods rely on learning
mechanisms that estimate the samples' clean labels during training and discard
their original noisy labels. However, this approach prevents the learning of
the relationship between images, noisy labels and clean labels, which has been
shown to be useful when dealing with instance-dependent label noise problems.
Furthermore, methods that do aim to learn this relationship require cleanly
annotated subsets of data, as well as distillation or multi-faceted models for
training. In this paper, we propose a new training algorithm that relies on a
simple model to learn the relationship between clean and noisy labels without
the need for a cleanly labelled subset of data. Our algorithm follows a 3-stage
process, namely: 1) self-supervised pre-training followed by an early-stopping
training of the classifier to confidently predict clean labels for a subset of
the training set; 2) use the clean set from stage (1) to bootstrap the
relationship between images, noisy labels and clean labels, which we exploit
for effective relabelling of the remaining training set using semi-supervised
learning; and 3) supervised training of the classifier with all relabelled
samples from stage (2). By learning this relationship, we achieve
state-of-the-art performance in asymmetric and instance-dependent label noise
problems.","Supervised deep learning has had great success gen erating effective classification models from sets of la belled training data [24, 26]. Modern deep learning mod els require largescale datasets to achieve stateofthe art (SOTA) results [38, 39]. However, realworld large 1Supported by Australian Research Council through grants DP180103232 and FT190100525.scale datasets, such as those collected from search en gines or available from hospitals and clinics, tend to have a nonnegligible amount of instancedependent label noise (IDN) [32, 53]. Existing methods often attempt to address instanceindependent label noise (IIN), such as symmetric or asymmetric noise [15, 58, 65]. Handling the IDN present in largescale realworld datasets has become one of the main research problems in the field. When naively trained with noisylabelled data, deep learning models generalise poorly because they can easily overfit the incorrectly labelled samples [62]. Many meth ods have been developed for handling label noise, with SOTA approaches relying on sample relabelling mecha nisms. These strategies are based on techniques to estimate the relationship between images and clean labels, and after relabelling, the old noisy labels are discarded [29, 47, 65]. However, to model how different image features and noisy labels affect the mislabelling process in IDN, we need to estimate the relationship between images, clean labels and noisy labels [17, 65]. Some methods have attempted to model this relationship with noisetransition matrices and corrective layers for asymmetric noise [15, 37, 57] or part dependant noise in place of instancedependant noise [56], but they failed to achieve SOTA results. Rather than the usual noisylabel learning setting, where a set of cleanly annotated samples is not available, some methods assume the existence of a subset of training data containing images, clean labels and noisy labels [17, 21, 52]. By training a model that predicts clean labels from both images and noisy labels (see right of Figure 1), these meth ods are able to learn the relationship between image fea tures, noisy labels and clean labels, allowing them to model IDN and more effectively relabel noisy samples. However, it can be expensive, difficult and timeconsuming to obtain a clean subset of data with noisy labels and clean labels that is representative of the instancedependant noise in the dataset. Furthermore, these methods require distillation to a more standard model (such as the one on the left of Fig ure 1) for evaluation on samples without labels. 1arXiv:2210.08826v1  [cs.CV]  17 Oct 2022Figure 1. On the left, we show a ‚Äònormal‚Äô deep neural network model used for noisy label learning tasks. On the right, we present a ‚Äòmodified‚Äô model that can learn the relationship between images x, noisy labels Àúyand clean labels y, similar to those used by meth ods that have access to a clean set of data [17, 21, 52]. In this paper, we introduce a new algorithm to learn the relationship between images and their clean and noisy la bels without using any cleanlabel set. Our algorithm fol lows a 3stage process (see Fig. 2): 1) Bootstrapping: self supervised pretraining followed by an earlystopping train ing [62] of the classifier that receives images and ‚Äònull‚Äô la bels as input and predicts the noisy labels as output ‚Äì this stage forms a subset of predicted clean labels for the second stage of training; 2) Semisupervised Learning: use this pre dicted clean subset to learn the relationship between images, noisy labels and clean labels, which we exploit for an effec tive, explicit relabelling of the remaining training set; and 3)Final training: supervised training of the classifier using the relabelled samples. The main contributions of this paper are: ‚Ä¢ An effective threestage training algorithm designed to address instancedependent label noise by learning the relationship between images and their clean and noisy labels ‚Äì using a noisetransition sample balanc ing scheme, explicitly relabelling training samples and without requiring a cleanly annotated training set; ‚Ä¢ A method that reaches SOTA asymmetric and instance dependent label noise results using a simple single model architecture, unlike DivideMix [29] (and its derivatives such as [11, 22, 35, 42, 68]) that require a more complex 2model architecture. ‚Ä¢ A ‚Äòlabel dropping‚Äô strategy that removes the need for distillation to a standard model and allows predictions to be made on samples with andwithout noisy labels;2. Related Work "
433,Learning from Long-Tailed Noisy Data with Sample Selection and Balanced Loss.txt,"The success of deep learning depends on large-scale and well-curated training
data, while data in real-world applications are commonly long-tailed and noisy.
Many methods have been proposed to deal with long-tailed data or noisy data,
while a few methods are developed to tackle long-tailed noisy data. To solve
this, we propose a robust method for learning from long-tailed noisy data with
sample selection and balanced loss. Specifically, we separate the noisy
training data into clean labeled set and unlabeled set with sample selection,
and train the deep neural network in a semi-supervised manner with a balanced
loss based on model bias. Extensive experiments on benchmarks demonstrate that
our method outperforms existing state-of-the-art methods.","Deep neural networks have made great successes in machine learning applications [He et al. , 2016; Vaswani et al. , 2017] but require wellcurated data for training. These data, such as ImageNet [Russakovsky et al. , 2015] and MSCOCO [Lin et al. , 2014], are usually artificially balanced across classes with clean labels obtained by manual labeling, which is costly and timeconsuming. However, the data in realworld applications are longtailed and noisy, since data from specific classes are difficult to acquire and labels are usually collected without expert annotations. To take WebVision dataset as an example, it exhibits longtailed distribution, where the sample size of each class varies from 362 (Windsor tie) to 11,129 (ashcan), and contains about 20% noisy labels [Li et al. , 2017]. Thus, developing robust learning methods for longtailed noisy data is a great challenge. Many methods have been proposed for longtailed learning or learning with noisy labels. In terms of longtailed learning, resampling methods [Chawla et al. , 2002; Jeatrakul et al. , 2010], reweighting methods [Cui et al. , 2019; Cao et al. , 2019; Menon et al. , 2021], transfer learning methods [Liu et al. , 2019; Kim et al. , 2020b] and twostage methods [Kang et al. , 2019; Cao et al. , 2019] are included; in terms of learning with noisy labels, designing noiserobust loss functions [Ghosh et al. , 2017; Zhang and Sabuncu, 2018], constructing unbiased loss terms with the transition matrix [Patrini et al. , 2017; Hendrycks et al. , 2018], filtering clean samples based on smallloss criterion [Han et al. , 2018; Li et al. , 2020] and correcting the noisy labels [Tanaka et al. , 2018; Yi and Wu, 2019] are included. Despite learning from longtailed or noisy data has been well studied, these methods cannot tackle longtailed noisy data in realworld applications. A few methods are proposed to deal with longtailed noisy data, which mainly focus on learning a weighting function in a metalearning manner [Shu et al., 2019; Jiang et al. , 2021]. However, these methods simultaneously require additional unbiased data which may be inaccessible in practice. To deal with longtailed noisy data, an intuitive way is to select clean samples with smallloss criterion and then apply longtailed learning methods. For the sample selection process, Gui et al. ‚àóCorresponding author Preprint. Under review.arXiv:2211.10906v3  [cs.LG]  28 May 2023[2021] revealed that the losses of samples with different labels are incomparable and chose each class a threshold for applying the smallloss criterion. For longtailed learning, existing methods are commonly based on label frequency to prevent head classes from dominating the training process. However, the model bias on different classes may not be directly related to label frequency (see Appendix E), and the true label frequency is also unknown under label noise. In this paper, we propose a robust method for learning from longtailed noisy data. Specifically, we separate the noisy training data into clean labeled set and unlabeled set with classaware sample selection and then train the model with a balanced loss based on model bias in a semisupervised manner. Experiments on the longtailed versions of CIFAR10 and CIFAR100 with synthetic noise and the longtailed versions of miniImageNetRed, Clothing1M, Food101N, Animal10N and WebVision with realworld noise demonstrate the superiority of our method. 2 Related Work "
434,Hybrid Contrastive Learning with Cluster Ensemble for Unsupervised Person Re-identification.txt,"Unsupervised person re-identification (ReID) aims to match a query image of a
pedestrian to the images in gallery set without supervision labels. The most
popular approaches to tackle unsupervised person ReID are usually performing a
clustering algorithm to yield pseudo labels at first and then exploit the
pseudo labels to train a deep neural network. However, the pseudo labels are
noisy and sensitive to the hyper-parameter(s) in clustering algorithm. In this
paper, we propose a Hybrid Contrastive Learning (HCL) approach for unsupervised
person ReID, which is based on a hybrid between instance-level and
cluster-level contrastive loss functions. Moreover, we present a
Multi-Granularity Clustering Ensemble based Hybrid Contrastive Learning
(MGCE-HCL) approach, which adopts a multi-granularity clustering ensemble
strategy to mine priority information among the pseudo positive sample pairs
and defines a priority-weighted hybrid contrastive loss for better tolerating
the noises in the pseudo positive samples. We conduct extensive experiments on
two benchmark datasets Market-1501 and DukeMTMC-reID. Experimental results
validate the effectiveness of our proposals.","Person ReidentiÔ¨Åcation (ReID) is a popular and important task in pattern recogni tion and computer vision, aiming to Ô¨Ånd the images of the same pedestrian in gallery to match the given probe image. The common approaches are to sort the gallery im ages according to the similarity between the probe image and the images in the gallery. Early works are usually based on supervised learning, which trains a deep model with a large amount of labeled data. However, the performance of the supervised ReID model will often seriously degenerate when facing the openworld data because the models are usually trained with limited data with supervision information. Thus it is crucial to exploit the hidden guidance information from the images without supervision. In recent years, unsupervised methods for person ReID have attracted a lot of atten tion. In unsupervised setting, the most popular methods [5‚Äì7, 27] are based on training a deep neural network with pseudo labels, which are generated by clustering algorithm (e.g.,kmeans, DBSCAN [3]). For instance, kmeans is used in [5] to generate the pseudo labels for different part of the images and DBSCAN is used in [6, 7, 27].arXiv:2201.11995v2  [cs.CV]  14 Apr 20222 H. Sun et al. The basic assumption behind the pseudo labelsbased unsupervised methods is that the samples in the same cluster are more likely with the same class label. Unlike the groundtruth labels, however, the pseudo labels obtained via a clustering algorithm are unavoidably noisy. Thus it is critic to tackle the noises in pseudo labels. For example, in [6], a mutual learning strategy via a temporal mean net is leveraged; in [5], a multi branch network from [19] is adopted to perform clustering with different part of images. Besides, some works [15, 24] attempt to exploit the neighborhood relationship instead of using traditional clustering methods. More recently, in [7], contrastive learning is introduced to unsupervised person ReID, in which a hybrid memory bank is used to store all the features and a uniÔ¨Åed contrastive loss based on the similarity of inputs and all features is adopted to train a deep neural network. While remarkable improvements in performance are reported, all these methods depend upon performing clustering method with a delicate hyper parameter (e.g., the neighborhood ratio parameter din DBSCAN). Unfortunately, the performance might dramatically degenerate if an improper hyperparameter is used. In this paper, we present a simple yet effective contrastive learningbased frame work for unsupervised person ReID, in which the noisy pseudo labels are used to deÔ¨Åne a hybrid contrastive loss‚Äîwhich aims to ‚Äúattract‚Äù the pseudo positive samples in the current cluster and at the meantime ‚Äúdispel‚Äù all the remaining samples (i.e., the pseudo negative samples) with respect to the current cluster. Moreover, we introduce a cluster ensemble strategy to generate multigranularity clustering information‚Äîwhich is en coded into priority weights, and adopt the priority weights to deÔ¨Åne a weighted hybrid contrastive loss. The cluster ensemble strategy aims to alleviate the sensitivity of using a single hyperparameter in clustering algorithm by using a range of the hyperparameter to perform clustering ensemble instead; whereas the priorityweighting mechanism in the contrastive loss aims to better tolerate the noises in pseudo labels. Paper Contributions. The contributions of the paper can be summarized as follows. ‚ÄìWe propose a novel hybrid contrastive paradigm for unsupervised person ReID, which is able to better exploit the noisy pseudo labels. ‚ÄìWe adopt a multigranularity clustering ensemble strategy to depict the conÔ¨Ådence of positive samples and hence present a priorityweighted hybrid contrastive loss for better tolerating the noises in pseudo positive samples. ‚ÄìWe conduct extensive experiments on two benchmark datasets and the experimental results validate the effectiveness of our proposals. 2 Related works "
435,Limited Gradient Descent: Learning With Noisy Labels.txt,"Label noise may affect the generalization of classifiers, and the effective
learning of main patterns from samples with noisy labels is an important
challenge. Recent studies have shown that deep neural networks tend to
prioritize the learning of simple patterns over the memorization of noise
patterns. This suggests a possible method to search for the best generalization
that learns the main pattern until the noise begins to be memorized.
Traditional approaches often employ a clean validation set to find the best
stop timing of learning, i.e., early stopping. However, the generalization
performance of such methods relies on the quality of validation sets. Further,
in practice, a clean validation set is sometimes difficult to obtain. To solve
this problem, we propose a method that can estimate the optimal stopping timing
without a clean validation set, called limited gradient descent. We modified
the labels of a few samples in a noisy dataset to obtain false labels and to
create a reverse pattern. By monitoring the learning progress of the noisy and
reverse samples, we can determine the stop timing of learning. In this paper,
we also theoretically provide some necessary conditions on learning with noisy
labels. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate
that our approach has a comparable generalization performance to methods
relying on a clean validation set. Thus, on the noisy Clothing-1M dataset, our
approach surpasses methods that rely on a clean validation set.","Noisy labels tend to affect the generalization performance of machine learning. Errors are often inevitable in manual annotation. Moreover, many datasets a re constructed by crawling images and labels from websites, and these often contain a consider able number of noisy labels (e.g., YFCC100M [1], Clothing1M [2]). Therefore, research on lear ning with noisy labels has great im portance. Deep neural networks (DNNs) have been applied to achieve bre akthroughs in many Ô¨Åelds. Many DNNbased methods have been proposed for learning with nois y labels. However, owing to the powerful Ô¨Åtting ability, DNNs may even memorize noise [3], w hich might hamper the generaliza tion of the main pattern (pattern of interest). However, a re cent work [4] further revealed that DNNs prioritize the learning of simple patterns over the memoriz ation of noise. During training, the gener alization performance of the main pattern increases Ô¨Årst an d then decreases. Traditional approaches [2, 5, 6] often employ a clean valida tion set to identify the best stop timing for learning, i.e., early stopping. However, these methods are almost sensitive to the validation sets. The quality of the validation set directly affects the generalization performance. In fact, it is cumbersome to produce a highquality validation set, and in practice applications, clean validation sets are sometimes not readily available.In this work, we focus on the learning of noisy labels without involving clean samples. To estimate the best stop timing of training, we propose a method called l imited gradient descent (LGD) based on the characteristic that a classiÔ¨Åer learns the main patte rn until the noise pattern begins to be memorized. This method hopes to monitor the learning progre sses of the main and noise patterns. Unfortunately, samples of different patterns cannot be ini tially distinguished. Thus, we randomly select a few samples from a noisy training s et as the reverse pattern, which is mutually exclusive to the main pattern. SpeciÔ¨Åcally, we shi ft the labels of the selected samples as reverse labels (as in label+1). It can be proved that the reverse labels are almost false. N ote that the samples of the main pattern are still unknown. We can obta in the training accuracies for the two parts of the samples: the reverse samples and leftover no isy samples. At the early stage of training, the accuracy of the leftover samples increases be cause the main pattern is learned Ô¨Årst, and the accuracy of the reverse samples does not increase (or may even decrease). We could therefore monitor the ratio of the two accuracies to predict the best ge neralization. We evaluate the performance on the CIFAR10, CIFAR100, and Clothing1M datasets. The exper imental results on CIFAR10 and CIFAR100 show that althoug h the accuracies with our method are comparable to those from corresponding traditional met hods, the variances of the experimental results are signiÔ¨Åcantly reduced, which shows that our meth od has better robustness. We further demonstrate stateoftheart performance on the noisy rea lworld Clothing1M dataset. The main contributions of the present study are as follows. F irst, we propose a weakly supervised method called limited gradient descent (LGD) that can learn the main pattern to the maximum extent possible from noisy labels. Second, we prepare a few samples with false labels for training, which no study has attempted thus far to the best of our knowledge. T hird, we theoretically prove some necessary conditions on LGD learning with noisy labels. Las tly, our method is free of models; thus, it can be applied to most DNNs and loss functions based on the s tochastic gradient descent (SGD) optimization. 2 Related Works "
436,Exploiting Class Similarity for Machine Learning with Confidence Labels and Projective Loss Functions.txt,"Class labels used for machine learning are relatable to each other, with
certain class labels being more similar to each other than others (e.g. images
of cats and dogs are more similar to each other than those of cats and cars).
Such similarity among classes is often the cause of poor model performance due
to the models confusing between them. Current labeling techniques fail to
explicitly capture such similarity information. In this paper, we instead
exploit the similarity between classes by capturing the similarity information
with our novel confidence labels. Confidence labels are probabilistic labels
denoting the likelihood of similarity, or confusability, between the classes.
Often even after models are trained to differentiate between classes in the
feature space, the similar classes' latent space still remains clustered. We
view this type of clustering as valuable information and exploit it with our
novel projective loss functions. Our projective loss functions are designed to
work with confidence labels with an ability to relax the loss penalty for
errors that confuse similar classes. We use our approach to train neural
networks with noisy labels, as we believe noisy labels are partly a result of
confusability arising from class similarity. We show improved performance
compared to the use of standard loss functions. We conduct a detailed analysis
using the CIFAR-10 dataset and show our proposed methods' applicability to
larger datasets, such as ImageNet and Food-101N.","Objects are relatable to each other. A cat bears more resemblance to a dog than to a car. Such distinctions that come easily to us humans help us understand even unseen objects, but such similarities can also be the cause of con fusion. Can you distinguish rabbits from hares? Current AI systems are not immune to confusability arising from class similarities. [6] and [1] have shown that the most confusedCrossentropy Loss LogProjection Loss0% noise ratio  80% noise ratio Figure 1: The tSNE plots of trained models on CIFAR10 using Crossentropy and LogProjection loss, with 0% and 80% asym metric noise ratio and trusted set M= 1k. Although crossentropy attempts to learn feature embeddings that differentiate classes, we observe that similar classes still end up relatively closer. This suggests that AI models have a natural tendency to learn fea tures that cluster according to class similarities . A model trained instead with our proposed LogProjection loss exploits rather than Ô¨Åght this natural clustering , irrespective of the noise. classes on the ImageNet dataset follow the WordNet hierar chy, implying that class similarity is a contributing factor to poor model performance. Machine learning systems are usually not provided any supervision on the interclass similarity, as the typical la bels used for machine learning do not explicitly capture such class similarities. This leaves the models to implicitly 1arXiv:2103.13607v1  [cs.CV]  25 Mar 2021Crossentropy Loss LogProjection LossAccuracy (%) Accuracy (%) Epochs Epochs Figure 2: Image classiÔ¨Åcation results on CIFAR10 with 80% asymmetric noise ratio and trusted set M= 1k. Labels: noisy train evaluation. The model trained using conÔ¨Ådence labels and LogProjection loss autocorrects, whereas the crossentropy loss leads to model overÔ¨Åtting on the noisy training data. discern the class similarity information from the training data to distinguish between classes, leading to poor perfor mance. Several works [23, 15], have looked at this problem by treating it as a regularization issue. [23] presented soft labels, a label regularization strategy wherein a Ô¨Åxed value is assigned to all the nontarget classes, but using soft labels has been shown to actually lead to loss of class similarity information [18]. We introduce conÔ¨Ådence labels that explicitly model the interclass similarity relations and thus provide models the supervision to learn such relations. ConÔ¨Ådence labels are probabilistic labels denoting the likelihood of similarity (or confusability) among the classes. We deÔ¨Åne them as a vec tor of (real or pseudo) probabilities of each possible la bel, which is analogous to a neuralnetwork classiÔ¨Åer‚Äôs Ô¨Å nal softmax activations. In the present work, we obtain conÔ¨Ådence labels on a perclass basis, either though heuris tic measures or by using pretrained models. (Future work will explore applying unique conÔ¨Ådence labels per image or even per pixel.) ConÔ¨Ådence labels are an easy way of introducing apriori interclass similarity information into the neural network training, which, when coupled with our novel projective loss functions, encourages both preserving and learning the naturally occurring class distributions. Most of the typical machine learning objective functions try to learn features that semiequally separate all the classes into different clusters in the latent feature space, ideally pulling apart all the clusters to be orthogonal to each other. Implicit in prior approaches is the assumption that classes are not only distinguishable, but that with the right features, each class is equally distinguishable from every other class. However, these systems still end up having tight clusters for similar classes, which is suggestive of a naturallyoccurring distribution arising from interclass similarity [18], leading us to suspect that any attempt at ‚Äúovercoming‚Äù these naturally occurring distributions may be futile. We further sus pect that modifying the label space to be orthogonal to each other may be incorrect, especially when working on open set classes [2]. A model that is trained on a closed set of classes might not appropriately Ô¨Åt an unseen class into its label space when operating under the assumption that every class is distinct. To preserve the naturally occurring class similarity dis tribution, we introduce projective loss functions tailored to work with conÔ¨Ådence labels with an ability to relax the loss penalty for similar classes. Our projective loss functions preserve and reinforce the naturally occurring cluster distri butions, such that similar classes stay closer while still sep arating dissimilar classes. In contrast, typical loss functions (e.g. crossentropy) impartially force all class clusters to move apart (pushing toward orthogonality), without regard to interclass similarity. Figure1, we observe that similar classes ( e.g. cat and dog) end up being relatively closer ir respective of the label distribution trained upon. Similarly, contrastive learning approaches [10] handle interclass similarity by trying to bring the feature embed dings of sameclass instances closer while moving away from feature embeddings of different classes. This approach is predicated on the assumption of interclass dissimilarity, thus ignoring the existence of interclass similarity. We be lieve our approach is the Ô¨Årst to explicitly exploit the inter class similarity. We test our approach to modeling the label space in the challenging setting of noisy labels (mislabelled data). We show that the mere usage of our projective loss combined with our conÔ¨Ådence labels, without yet incorporating many of the other standard trainingenhancement techniques, can achieve comparable performance to the stateoftheart sys tems when dealing with high label noise stemming form in ter class similarity (i.e. asymmetric or semantic noise [13]). Contributions: Our primary novel ideas are that (A) classsimilarity information should be exploited during training, that (B) similar classes need not, and should not, be well separated in latent feature space, and that (C) noisy la bels often occur as a result of class similarity, which would lead naturally occurring label noise to be asymmetrically clustered. Our main technical contributions are (1) Introducing our novel ConÔ¨Ådence Labels as a new way of labelling data based on probabilistic labels that denote the likelihood of similarity. (2) Methods for the creation and generation of conÔ¨Ådence labels that instil class similarity prior to model training. (3) A novel family of Projective loss functions tailored to handle conÔ¨Ådence labels, which reinforce natu rally occurring class similarity distributions/relations. (4) Training mechanisms for working with conÔ¨Ådence labels and projective loss functions. Our simple, but effective, strategy explicitly exploitsclass similarity information for training more robustly with noisy labels. It achieves signiÔ¨Åcant robustness at high asym metric noise levels on CIFAR10 [11] comparable to current stateoftheart methods, while being a straightforward and simple strategy and not increasing training time as com pared to current methods. We also show improvements on largescale datasets, ImageNet [7] and Food101N [12], without yet incorporating many of the ‚Äústandard‚Äù training enhancement techniques such as AutoAugment [5] and Co sine learning rate decay with warm restarts [16]. 2. Related Work "
437,BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter by Combining Deep Learning and Transfer Learning Models.txt,"The outbreak COVID-19 virus caused a significant impact on the health of
people all over the world. Therefore, it is essential to have a piece of
constant and accurate information about the disease with everyone. This paper
describes our prediction system for WNUT-2020 Task 2: Identification of
Informative COVID-19 English Tweets. The dataset for this task contains size
10,000 tweets in English labeled by humans. The ensemble model from our three
transformer and deep learning models is used for the final prediction. The
experimental result indicates that we have achieved F1 for the INFORMATIVE
label on our systems at 88.81% on the test set.","The rapid spread of the coronavirus (COVID19) has caused a global health crisis. This virus is haz ardous to people‚Äôs health and causes a big panic all over the world. Statistics show that each day there are 4 million tweets related to COVID19 on Twitter (Lamsal, 2020). Therefore, it is essential to keep track of the information associated with this disease. Along with the development of many social networking platforms such as Twitter and Facebook. This is the primary way that helps peo ple capture information about COVID19 regularly. However, there is much content appearing daily on these social media platforms. Most of them do not have information about the status of COVID19, such as the number of suspected cases or cases near the user‚Äôs area. In this article, we present our approach at WNUT2020 Task 2 (Nguyen et al., 2020) to iden tify Tweets containing information about COVID 19 on the social networking platform Twitter or not. A Tweet is believed to have information if it includes information such as recovered, suspected, conÔ¨Årmed, and death cases and location or travelhistory of the patients. SpeciÔ¨Åcally, we described the problem as follows. ‚Ä¢Input : Given English Tweets on the social networking platform. ‚Ä¢Output : One of two labels (INFORMATIVE and UNINFORMATIVE) predicted by our system. Several examples are shown in Table 1 Tweet Label A New Rochelle rabbi and a White Plains doctor are among the 18 con Ô¨Årmed coronavirus cases in Westchester. HTTPURL0 Day 5: On a family bike ride to pick up dinner at @USER Broadway, we encountered our preCOVID19 Land Park happy hour crew keeping up the tradition at an appropriate #SocialDis tance.HTTPURL1 Table 1: Several examples in the WNUT2020 Task 2 dataset. 0 and 1 stand for INFORMATIVE and UNIN FORMATIVE, respectively. In this paper, we have two main contributions as follows. ‚Ä¢Firstly, we implemented four different models based on neural networks and transformers such as BiGRUCNN, BERT, RoBERTa, XLNet to solve the WNUT2020 Task 2: IdentiÔ¨Åcation of informative COVID19 English Tweets. ‚Ä¢Secondly, we propose a simple ensemble model by combining multiple deep learning and transformer models. This model gives thearXiv:2009.02671v2  [cs.CL]  1 Apr 2021highest performance compared with the single models with F1 on the test set is 88.81% and on the development set is 90.65%. 2 Related work "
438,Neural Architectures for Nested NER through Linearization.txt,"We propose two neural network architectures for nested named entity
recognition (NER), a setting in which named entities may overlap and also be
labeled with more than one label. We encode the nested labels using a
linearized scheme. In our first proposed approach, the nested labels are
modeled as multilabels corresponding to the Cartesian product of the nested
labels in a standard LSTM-CRF architecture. In the second one, the nested NER
is viewed as a sequence-to-sequence problem, in which the input sequence
consists of the tokens and output sequence of the labels, using hard attention
on the word whose label is being predicted. The proposed methods outperform the
nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and
Czech CNEC. We also enrich our architectures with the recently published
contextual embeddings: ELMo, BERT and Flair, reaching further improvements for
the four nested entity corpora. In addition, we report flat NER
state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003
English.","In nested named entity recognition, entities can be overlapping and labeled with more than one la bel such as in the example ‚ÄúThe Florida Supreme Court‚Äù containing two overlapping named entities ‚ÄúThe Florida Supreme Court‚Äù and‚ÄúFlorida‚Äù .1 Recent publications on nested named entity recognition involve stacked LSTMCRF NE rec ognizer ( Ju et al. ,2018 ), or a construction of a special structure that explicitly captures the nested entities, such as a constituency graph ( Finkel and Manning ,2009 ) or various modiÔ¨Åcations of a di rected hypergraph ( Lu and Roth ,2015 ;Katiyar and Cardie ,2018 ;Wang and Lu ,2018 ). 1Example from ACE2004 ( Doddington et al. ,2004 ), https://catalog.ldc.upenn.edu/LDC2005T09 .We propose two completely neural network ar chitectures for nested nested named entity recog nition which do not explicitly build or model any structure and infer the relationships between nested NEs implicitly: ‚Ä¢In the Ô¨Årst model, we concatenate the nested entity multiple labels into one multilabel, which is then predicted with a standard LSTMCRF ( Lample et al. ,2016 ) model. The advantages of this model are simplicity and effectiveness, because an already exist ing NE pipeline can be reused to model the nested entities. The obvious disadvantage is a large growth of NE classes. ‚Ä¢In the second model, the nested entities are encoded in a sequence and then the task can be viewed as a sequencetosequence (seq2seq) task, in which the input sequence are the tokens (forms) and the output se quence are the labels. The decoder predicts labels for each token, until a special label ""<eow>"" (end of word) is predicted and the decoder moves to the next token. The expressiveness of the models depends on a nonambiguous encoding of the nested entity structure. We use an enhanced BILOU scheme de scribed in Section 4.1. The proposed models surpass the current nested NER state of the art on four nested entity cor pora: ACE2004, ACE2005, GENIA and Czech CNEC. When the recently introduced contextual embeddings ‚Äì ELMo ( Peters et al. ,2018 ), BERT (Devlin et al. ,2018 ) and Flair ( Akbik et al. ,2018 ) ‚Äì are added to the architecture, we reach further improvements for the above mentioned nested en tity corpora and also exceed current state of the art for CoNLL2002 Dutch and Spanish and for CoNLL2003 English.2 Related Work "
439,Automatic Language Identification for Celtic Texts.txt,"Language identification is an important Natural Language Processing task. It
has been thoroughly researched in the literature. However, some issues are
still open. This work addresses the identification of the related low-resource
languages on the example of the Celtic language family.
  This work's main goals were: (1) to collect the dataset of three Celtic
languages; (2) to prepare a method to identify the languages from the Celtic
family, i.e. to train a successful classification model; (3) to evaluate the
influence of different feature extraction methods, and explore the
applicability of the unsupervised models as a feature extraction technique; (4)
to experiment with the unsupervised feature extraction on a reduced annotated
set.
  We collected a new dataset including Irish, Scottish, Welsh and English
records. We tested supervised models such as SVM and neural networks with
traditional statistical features alongside the output of clustering,
autoencoder, and topic modelling methods. The analysis showed that the
unsupervised features could serve as a valuable extension to the n-gram feature
vectors. It led to an improvement in performance for more entangled classes.
The best model achieved a 98\% F1 score and 97\% MCC. The dense neural network
consistently outperformed the SVM model.
  The low-resource languages are also challenging due to the scarcity of
available annotated training data. This work evaluated the performance of the
classifiers using the unsupervised feature extraction on the reduced labelled
dataset to handle this issue. The results uncovered that the unsupervised
feature vectors are more robust to the labelled set reduction. Therefore, they
proved to help achieve comparable classification performance with much less
labelled data.","Language IdentiÔ¨Åcation (LI) approaches the problem of automatic recognition of speciÔ¨Åc natural languages [Jauhiainen et al., 2018]. LI has multiple applications in the modern world. The original use case is routing source documents to languageappropriate Natural Language Processing (NLP) components, such as machine translation and dialogue systems [Zhang et al., 2018]. Most of the other NLP methods assume input text is monolingual; in that case, routing of the input documents is crucial. Moreover, LI is a valuable component of the corpus creation pipelines, especially for the lowresource languages. Text crawled from the web resources usually needs to be separated by language to create a meaningful corpus. LI can be a relatively easy task for the most popular languages with the abundance of available resources. Nevertheless, there are languages with fewer native speakers and less research focus; such languages are considered lowresource. Lowresource languages and related languages continuously pose a problem for language identiÔ¨Åcation [He et al., 2018,arXiv:2203.04831v1  [cs.CL]  9 Mar 2022Automatic Language IdentiÔ¨Åcation for Celtic Texts A P REPRINT Jauhiainen et al., 2018]. This paper focuses on applying language identiÔ¨Åcation methods to a family of lowresource languages on the example of the Celtic language group combining the two issues. The main problem with the lowresource languages is the unavailability of highquality corpora, which is costly to prepare. Language IdentiÔ¨Åcation faces this problem same as other NLP tasks. Thus, in our research, we created a corpus of three Celtic languages. It contains Irish, Scottish, and Welsh texts. Moreover, the corpus is extended with a small proportion of English samples because these languages are usually mixed in the common usage. This work also focuses on the preparation of a language identiÔ¨Åcation model alongside the evaluation and analysis of the different feature extraction methods. In particular, we explored the possibility of the application of the output of the unsupervised learning models as a feature representation for the classiÔ¨Åcation task. The unsupervised approach was promising in the case of scarce annotated data as here. The unsupervised models operate on the unlabelled data and so enhance the classiÔ¨Åcation model‚Äôs performance on a limited labelled dataset. To solve the lack of large corpora for lowresource languages, we proved that the features extracted from unsupervised methods ensure high performance on the reduced labelled set size. Consequently, this study validates that it is possible to save resources on data annotation, determine the language class of lowresource languages more efÔ¨Åciently, and ensure that the data for further processing is of high quality. In this study, our main contributions are: 1. collecting the dataset of three Celtic languages (Irish, Scottish, Welsh) and English (Section 3), 2.preparing a method to identify these languages from the Celtic family and differentiate them from English (Section 4), 3.evaluating the inÔ¨Çuence of different feature extraction methods and exploring the applicability of unsupervised models as a feature extraction technique (Section 6), 4. experimenting with unsupervised feature extraction on a reduced annotated set (Section 6.3). In the following sections, we discussed the related research (Section 2), the dataset creation process (Section 3), the proposed approach (Section 4), the results of the experiments (Section 6), the list of conclusions (Section 7) and directions for future research (Section 8). 2 Related Work "
440,Multi-object tracking with self-supervised associating network.txt,"Multi-Object Tracking (MOT) is the task that has a lot of potential for
development, and there are still many problems to be solved. In the traditional
tracking by detection paradigm, There has been a lot of work on feature based
object re-identification methods. However, this method has a lack of training
data problem. For labeling multi-object tracking dataset, every detection in a
video sequence need its location and IDs. Since assigning consecutive IDs to
each detection in every sequence is a very labor-intensive task, current
multi-object tracking dataset is not sufficient enough to train
re-identification network. So in this paper, we propose a novel self-supervised
learning method using a lot of short videos which has no human labeling, and
improve the tracking performance through the re-identification network trained
in the self-supervised manner to solve the lack of training data problem.
Despite the re-identification network is trained in a self-supervised manner,
it achieves the state-of-the-art performance of MOTA 62.0\% and IDF1 62.6\% on
the MOT17 test benchmark. Furthermore, the performance is improved as much as
learned with a large amount of data, it shows the potential of self-supervised
method.","MultiObject Tracking(MOT) is one of the fundamental challenges of computer vision and is widely used in in dustrial Ô¨Åelds such as surveillance image processing. many MOT studies have been conducted on pedestrians tracking, because the mainly used benchmark of MOT, the MOT chal lenge benchmark (Milan et al. 2016) is focused on pedestri ans tracking. In MOT, the detection by tracking paradigm has been dominated for a long time. It is a method of obtaining de tection results in each frame of video using offtheshelf de tection algorithm such as FasterRCNN (Ren et al. 2015), DPM (Felzenszwalb et al. 2009) , and organically associ ating the detection results of the previous frame with the detection results of the current frame and form tracks. The MOT algorithm can be divided into online method and batch method, which is the difference between whether future in formation is used to form tracks of the current frame. Online method that does not use future information is more suitable Copyright ¬© 2021, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.for practical application, a lot of research has been done with the online method. There are various approaches in MOT algorithms, but the most of them deals with how to associate the track of the previous frame with the detection of the current frame. Among them, we approach by reidentifying detection re sults through appearance model. we train feature extraction network with appearance information of detection patches and associate tracks with detection patches using its feature. However this approach has two drawbacks. First, a lot of computation is required because all detection patches have to pass through the feature extraction network and proceed matching with tracks using those features. Second, the lack of learning data. Labeling MOT dataset is very labor inten sive task. The MOT dataset consists of video sequence and a detection bounding box of each frame and its ID. To label this ID, a detection bounding box with an ID of all video se quence must be tracked and labeled by a human supervision. But each video sequence consists of many frames and also there are so many people per frame, it takes a lot of labor to label them. This is the one of the biggest problems in MOT. Therefore, we try to solve this problem by applying self supervised learning to MOT. Unsupervised learning or self supervised learning has been proposed to solve the lack of training data problem in image classiÔ¨Åcation or segmenta tion (Doersch, Gupta, and Efros 2015; Gidaris, Singh, and Komodakis 2018; V ondrick et al. 2018), but it has rarely been applied to MOT. We closely analyze the MOT to Ô¨Ånd a new pretext which is suitable for the MOT task and pro pose the selfsupervised associating tracker(SSAT) which is a tracking algorithm that train the feature extraction net work without data constraints by a selfsupervised manner, and utilize it directly to reidentify targets to track without a seperate downstream tasks. 2. Related work "
441,The Neurally-Guided Shape Parser: Grammar-based Labeling of 3D Shape Regions with Approximate Inference.txt,"We propose the Neurally-Guided Shape Parser (NGSP), a method that learns how
to assign fine-grained semantic labels to regions of a 3D shape. NGSP solves
this problem via MAP inference, modeling the posterior probability of a label
assignment conditioned on an input shape with a learned likelihood function. To
make this search tractable, NGSP employs a neural guide network that learns to
approximate the posterior. NGSP finds high-probability label assignments by
first sampling proposals with the guide network and then evaluating each
proposal under the full likelihood. We evaluate NGSP on the task of
fine-grained semantic segmentation of manufactured 3D shapes from PartNet,
where shapes have been decomposed into regions that correspond to part instance
over-segmentations. We find that NGSP delivers significant performance
improvements over comparison methods that (i) use regions to group per-point
predictions, (ii) use regions as a self-supervisory signal or (iii) assign
labels to regions under alternative formulations. Further, we show that NGSP
maintains strong performance even with limited labeled data or noisy input
shape regions. Finally, we demonstrate that NGSP can be directly applied to CAD
shapes found in online repositories and validate its effectiveness with a
perceptual study.","The ability to semantically segment 3D shapes is im portant for numerous applications in vision, graphics, and robotics: reverseengineering the part structure of an object to support editing and manipulation; producing training data for structureaware generative shape models [13, 18, 28]; helping autonomous agents understand how to interact with objects in their environment [1]; and more. These appli cations often demand that the parts detected be finescale (e.g. wheels of an office chair) and hierarchicallyorganized (e.g. a cabinet door decomposes into a handle, door, and frame). Producing such segmentations has proved to be achallenging task, as it is expensive to gather large amounts of data at this granularity; PartNet [29] is the only existing largescale dataset of this type. Recent work on 3D shape semantic segmentation has mainly focused on endtoend approaches that operate on shape atoms (e.g. mesh faces, point cloud points, occupancy grid voxels), i.e. the lowestlevel geometric entity in the input representation [15, 33, 34, 44]. While these methods achieve impressive performance on many tasks, they do not often transfer well to domains with finegrained labels or when access to labeled data is limited. We postulate that one reason for this phenomenon is that attempting to label shape atoms directly results in a massive search space, allowing learningbased methods to overfit unless the ratio of labeled shape instances to the label set complexity is high. One way to address this issue is to design systems that make use of shape regions . When the number of shape regions becomes significantly smaller than the number of shape atoms , the label assignment problem becomes easier. Such a framing may allow methods to learn finegrained semantic segmentation when access to labeled data is limited. When shape regions are provided, they can be used in various ways: (i) as a postprocess aggregation on top of shape atom predictions, (ii) to formulate auxiliary selfsupervised objectives, or (iii) as the object to be labeled. Methods that operate within this last paradigm can more directly reason about relationships between regions, which can help improve finegrained segmentation performance by better considering the context of a region within the entire shape. The problem of decomposing a shape into regions useful for semantic segmentation is applicationdependent. For CAD shapes and scenes found in online repositories, this type of region decomposition is often produced as a by product of the modeling process, e.g. each part instance will be made out of one or more connected mesh compo nents [26, 39, 49]. Discovering region decompositions for shapes that do not already provide them is a wellstudied problem within computer vision and graphics. There has been considerable recent effort on unsupervised techniques that approximate 3D shapes with primitives [9,21,31,37,38], 1arXiv:2106.12026v3  [cs.CV]  22 Mar 2022and there is a long history of research on shape segmentation through purely geometric analysis [3, 19, 41]. There is even reason to believe that region decomposition solutions can generalize across shape categories, i.e. the way that shapes (especially manufactured objects) decompose into parts is largely categoryindependent [14, 50]. In this paper, we propose the NeurallyGuided Shape Parser (NGSP), a method that learns to assign finegrained labels from a semantic grammar to regions of a 3D shape. Our approach is based on maximum a posteriori (MAP) in ference in a model of the probability that a label assignment to the shape‚Äôs regions is correct. Our likelihood consists of a mixture of modules that each operate on some regions of the shape. One set of modules evaluates the validity of the implied geometry and spatial layout for each label in the semantic grammar. Another module evaluates groups of re gions formed by the label assignment. As this combinatorial search problem is too complex to solve with exhaustive enu meration, we employ a neural guide network to approximate the posterior. The guide network reasons locally, predicting the label probability for each region independently. Using the perregion probabilities produced by the guide network, NGSP importance samples a set of proposed label assign ments. To choose the best proposal out of this set, each label assignment is evaluated under the full likelihood, and the sample with highest posterior probability is chosen. We compare NGSP against methods that use shape re gions as a postprocess, a selfsupervisory signal, or assign labels to regions with different search strategies and likeli hood formulations. We evaluate each method on the task of finegrained semantic segmentation of manufactured 3D shapes from PartNet, where each method has access to re gions from the annotated part instance oversegmentations (e.g. each semantic part instance may consist of multiple regions). NGSP achieves the best semantic segmentation performance, even in paradigms where access to labeled data is limited or when the input shape regions are noisy. To validate our design decisions, we run an ablation study measuring the effect of each likelihood term and the neural guide network. Finally, we show that NGSP can find good semantic segmentations on ‚Äòin the wild‚Äô CAD shapes found from online repositories, and evaluate its performance with a forced choice perceptual study against comparison methods. Code for our method and experiments can be found at found at https://github.com/rkjones4/NGSP . In summary, our contributions are: (i)We present the NeurallyGuided Shape Parser (NGSP), a method that learns how to assign labels from a seman tic grammar to regions of a 3D shape. NGSP performs approximate MAP inference, using a guide network to find highprobability label assignments under a learned posterior probability of a label assignment conditioned on an input shape.(ii)We demonstrate that NGSP finds better finegrained semantic segmentations for manufactured shapes com pared with methods that use shape regions in alterna tive learning paradigms. 2. Related Work "
442,Improved Regularization and Robustness for Fine-tuning in Neural Networks.txt,"A widely used algorithm for transfer learning is fine-tuning, where a
pre-trained model is fine-tuned on a target task with a small amount of labeled
data. When the capacity of the pre-trained model is much larger than the size
of the target data set, fine-tuning is prone to overfitting and ""memorizing""
the training labels. Hence, an important question is to regularize fine-tuning
and ensure its robustness to noise. To address this question, we begin by
analyzing the generalization properties of fine-tuning. We present a PAC-Bayes
generalization bound that depends on the distance traveled in each layer during
fine-tuning and the noise stability of the fine-tuned model. We empirically
measure these quantities. Based on the analysis, we propose regularized
self-labeling -- the interpolation between regularization and self-labeling
methods, including (i) layer-wise regularization to constrain the distance
traveled in each layer; (ii) self label-correction and label-reweighting to
correct mislabeled data points (that the model is confident) and reweight less
confident data points. We validate our approach on an extensive collection of
image and text data sets using multiple pre-trained model architectures. Our
approach improves baseline methods by 1.76% (on average) for seven image
classification tasks and 0.75% for a few-shot classification task. When the
target data set includes noisy labels, our approach outperforms baseline
methods by 3.56% on average in two noisy settings.","Learning from limited labeled data is a fundamental problem in many realworld applications (Ratner et al., 2016, 2017). A common approach to address this problem is Ô¨Ånetuning a large model that has been pretrained on publicly available labeled data (He et al., 2019). Since Ô¨Ånetuning is typically applied to a target task with limited labels, this algorithm is prone to overÔ¨Åtting or ‚Äúmemorization‚Äù issues (Tan et al., 2018). These issues worsen when the target task contains noisy labels (Zhang et al., 2016). In this paper, we analyze regularization methods for Ô¨Ånetuning from both theoretical and empirical perspectives. Based on the analysis, we propose a regularized selflabeling approach that improves the generalization and robustness properties of Ô¨Ånetuning. Previous works (Li et al., 2018a,b) have proposed regularization methods to constrain the distance between a Ô¨Ånetuned model and the pretrained model in the Euclidean norm. Li et al. (2020) provides extensive study to show that the performance of Ô¨Ånetuning and the beneÔ¨Åt of adding regularization depend on the hyperparameter choices. Salman et al. (2020) empirically Ô¨Ånd that performing adversarial training during the pretraining phase helps learn pretrained models that transfer better to downstream tasks. The work of Gouk et al. (2021) generalizes the above ideas to various norm choices and Ô¨Ånds that projected gradient descent methods perform well for implementing distancebased regularization. Additionally, they derive generalization bounds for Ô¨Ånetuning using Rademacher complexity. These works focus on settings where there is no label noise in the target data set. When 35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2111.04578v1  [cs.LG]  8 Nov 2021label noise is present, for example, due to applying weak supervision techniques (Ratner et al., 2016), an important question is to design methods that are robust to such noise. The problem of learning from noisy labels has a rich history of study in supervised learning (Natarajan et al., 2013). In contrast, little is known in the transfer learning setting. These considerations motivate us to analyze the generalization and robustness properties of Ô¨Ånetuning. In Section 4.1, we begin by conducting a PACBayesian analysis of regularized Ô¨Ånetuning. This is inspired by recent works that have found PACBayesian analysis correlates with empirical perfor mance better than Rademacher complexity (Jiang et al., 2020). We identify two critical measures for analyzing the generalization performance of Ô¨Ånetuning. The Ô¨Årst measure is the `2norm of the distance between the pretrained model (initialization) and the Ô¨Ånetuned model. The second measure is the perturbed loss of the Ô¨Ånetuned model, i.e. its loss after the model weights get perturbed by random noise. First, we observe that the Ô¨Ånetuned weights remain closed to the pretrained model. Moreover, the top layers travel much further away from the pretrained model than the bottom layers. Second, we Ô¨Ånd that Ô¨Ånetuning from a pretrained model implies better noise stability than training from a randomly initialized model. In Section 4.2, we evaluate regularized Ô¨Ånetuning for target tasks with noisy labels. We Ô¨Ånd that Ô¨Ånetuning is prone to ‚Äúmemorizing the noisy labels‚Äù, and regularization helps alleviate such memorization behavior. Moreover, we observe that the neural network has not yet overÔ¨Åtted to the noisy labels during the early phase of Ô¨Ånetuning. Thus, its prediction could be used to relabel the noisy labels. 0 10 20 30 Number of epochs0.00.20.40.6Prediction accuracyFinetuning: Training Finetuning: Test Regularized selflabeling: Training Regularized selflabeling: Test # Corrected training labels 1000130016001900 Number of data points Figure 1: Red: Layerwise regularization closes generalization gap. Magenta: Selflabeling rela bels noisy data points to their correct label.We propose an algorithm that incorporates layer wise regularization and selflabeling for im proved regularization and robustness based on our results. Figure 1 illustrates the two compo nents. First, we encode layerwise distance con straints to regularize the model weights at dif ferent levels. Compared to (vanilla) Ô¨Ånetuning, our algorithm reduces the gap between the train ing and test accuracy, thus alleviating overÔ¨Åtting. Second, we add a selflabeling mechanism that corrects and reweights ‚Äúnoisy labels‚Äù based on the neural network‚Äôs predictions. Figure 1 shows that our algorithm effectively hinders the model from learning the incorrect labels by relabeling them to correct ones. In Section 5, we evaluate our proposed algorithm for both transfer learning and fewshot classiÔ¨Åcation tasks with image and text data sets. First, using ResNet101 (He et al., 2016) as the pretrained model, our algorithm outperforms previous Ô¨Ånetuning methods on seven image classiÔ¨Åcation tasks by1:76% on average and 3:56% when their labels are noisy. Second, we Ô¨Ånd qualitatively similar results for applying our approach to medical image classiÔ¨Åcation tasks (ChestXray14 (Wang et al., 2017; Rajpurkar et al., 2017)) and vision transformers (Dosovitskiy et al., 2020). Finally, we extend our approach to fewshot learning and sentence classiÔ¨Åcation. For these related but different tasks and data modalities, we Ô¨Ånd an improvement of 0:75% and0:46% over previous methods, respectively. In summary, our contributions are threefold. First, we provide a PACBayesian analysis of regularized Ô¨Ånetuning. Our result implies empirical measures that explain the generalization performance of regularized Ô¨Ånetuning. Second, we present a regularized selflabeling approach to enhance the generalization and robustness properties of Ô¨Ånetuning. Third, we validate our approach on an extensive collection of classiÔ¨Åcation tasks and pretrained model architectures. 2 Related work "
443,Leveraging Native Language Speech for Accent Identification using Deep Siamese Networks.txt,"The problem of automatic accent identification is important for several
applications like speaker profiling and recognition as well as for improving
speech recognition systems. The accented nature of speech can be primarily
attributed to the influence of the speaker's native language on the given
speech recording. In this paper, we propose a novel accent identification
system whose training exploits speech in native languages along with the
accented speech. Specifically, we develop a deep Siamese network-based model
which learns the association between accented speech recordings and the native
language speech recordings. The Siamese networks are trained with i-vector
features extracted from the speech recordings using either an unsupervised
Gaussian mixture model (GMM) or a supervised deep neural network (DNN) model.
We perform several accent identification experiments using the CSLU Foreign
Accented English (FAE) corpus. In these experiments, our proposed approach
using deep Siamese networks yield significant relative performance improvements
of 15.4 percent on a 10-class accent identification task, over a baseline
DNN-based classification system that uses GMM i-vectors. Furthermore, we
present a detailed error analysis of the proposed accent identification system.","Over the recent years, many of voicedriven technologies have achieved signiÔ¨Åcant robustness needed for mass deploy ment. This is largely due to signiÔ¨Åcant advances in automatic speech recognition (ASR) technologies and deep learning algorithms. However, the variability in speech accents pose a signiÔ¨Åcant challenge to stateoftheart speech systems. In particular, large sections of the Englishspeaking population in the world face difÔ¨Åculties interacting with voicedriven agents in English due to the mismatch in speech accents This work was carried out with the help of a research grant awarded by Microsoft Research India (MSRI) for the Summer Workshop on ArtiÔ¨Åcial Social Intelligence.seen in the training data. The accented nature of speech can be primarily attributed to the inÔ¨Çuence of the speaker‚Äôs native language. In this work we focus on the problem of accent identiÔ¨Åcation , where the user‚Äôs native language is au tomatically determined from their nonnative speech. This can be viewed as a Ô¨Årst step towards building accentaware voicedriven systems. Accent identiÔ¨Åcation from nonnative speech bears re semblance to the task of language identiÔ¨Åcation [1]. How ever, accent identiÔ¨Åcation is a harder task as many cues about the speaker‚Äôs native language are lost or suppressed in the nonnative speech. Nevertheless, one may expect that the speaker‚Äôs native language is reÔ¨Çected in the acoustics of the individual phones used in nonnative language speech, along with pronunciations of words and grammar. In this work, we focus on the acoustic characteristics of an accent induced by a speaker‚Äôs native language. Our main contributions: We develop a novel deep Siamese network based model which learns the association between accented speech and native language speech. We explore ivector features extracted using both an un supervised Gaussian mixture model (GMM) and a su pervised deep neural network (DNN) model. We present a detailed error analysis of the proposed system which reveals that the confusions among accent predictions are contained within the language family of the corresponding native language. Section 3 outlines the ivector feature extraction process. Section 4 describes our Siamese networkbased model for ac cent identiÔ¨Åcation. Our experimental results are detailed in Section 5 and Section 6 provides an error analysis of our pro posed approach. 2. RELATED WORK "
444,Vehicle Shape and Color Classification Using Convolutional Neural Network.txt,"This paper presents a module of vehicle reidentification based on make/model
and color classification. It could be used by the Automated Vehicular
Surveillance (AVS) or by the fast analysis of video data. Many of problems,
that are related to this topic, had to be addressed. In order to facilitate and
accelerate the progress in this subject, we will present our way to collect and
to label a large scale data set. We used deeper neural networks in our
training. They showed a good classification accuracy. We show the results of
make/model and color classification on controlled and video data set. We
demonstrate with the help of a developed application the re-identification of
vehicles on video images based on make/model and color classification. This
work was partially funded under the grant.","The objective of the vehicle reidentiÔ¨Åcation module based on make/model and color classiÔ¨Åca tion is to recognize a vehicle within a large image or video data set based on its make/model and color attributes. There are a number of challenges related to this task, that need to be addressed: There are more than 150 car manufacturers worldwide with approximately 2.000 mod els. Each model of a make generally has a longer history while model upgrades appear every few years. The appearance of a model of a make varies not only due to its model year but also dif fers strongly depending on the perspective.The same vehicle looks very different from the front than from the rear or from the side view. Video data frequently contains objects at low resolution, this aggravates the classiÔ¨Åcation. Occlusion in case vehicles are close each to other. In this work, we used convolutional neural networks (CNNs) to learn the vehicle‚Äôs make/model and color descriptors. We used TensorÔ¨Çow as framework. Our Training is applied on the detected region of interest (ROI) of the vehicle. 2. Related Works "
445,Signal Combination for Language Identification.txt,"Google's multilingual speech recognition system combines low-level acoustic
signals with language-specific recognizer signals to better predict the
language of an utterance. This paper presents our experience with different
signal combination methods to improve overall language identification accuracy.
We compare the performance of a lattice-based ensemble model and a deep neural
network model to combine signals from recognizers with that of a baseline that
only uses low-level acoustic signals. Experimental results show that the deep
neural network model outperforms the lattice-based ensemble model, and it
reduced the error rate from 5.5% in the baseline to 4.3%, which is a 21.8%
relative reduction.","Multilingual speech recognition is an important feature for modern speech recognition systems allowing users to speak in more than a single, preset language. In Google multilin gual speech recognition service [1] users are allowed to se lect two, or more, languages simultaneously as prior infor mation (Fig. 1). When the microphone is enabled, the sys tem works by running several speech recognziers in paral lel, along with an acoustic language identiÔ¨Åcation (LangID) module [2]. After the system decides the language of the ut terance, the recognition result of the corresponding language will be used and the language decision can be propagated to downstream systems (e.g. a text to speech module). In our previous work [1], the Ô¨Ånal language identiÔ¨Åcation de cision is predominantly taken by the acoustic LangID mod ule, which generates a probability score for each of the lan guage based on the audio. Such approach however, ignores other potentially useful information returned by the individ ual speech recognizers such as the accumulated language or acoustic model score. The author did this work during his internship at Google. Email: shengye@ucsd.edu ‚Ä†Email: fliwan, yyuyy, elnota g@google.com Utterance AudioAcousticLangID Model2+ Speech RecognizersHighlevel LangIDSignal Combinationlangalangblangc‚Ä¶lang ‚ùå lang ‚úÖ lang ‚ùå ‚Ä¶LanguageDecisionLanguageCandidatesFig. 1 .The ‚ÄúLangID‚Äù pipeline. The system takes an audio clip of an utterance and a list of candidate languages, and it predicts the language of the utterance. In this work, we explore a few alternative approaches to improve the language identiÔ¨Åcation accuracy by using signals that can be easily computed by most speech recognition sys tems, including the conÔ¨Ådence, acoustic and language model scores computed by the recognizers (Table 1). Without signal combination, an acoustic LangID model provides a baseline with 5 :5% error rate. Using a lattice based ensemble model [3], we were able to reduce the clas siÔ¨Åcation error rate to 4 :5%, a relative 23 :7% reduction. We continued exploring new methods and found a deep neural network model outperforms the latticebased model: the er ror rate further reduced to 4 :3%, which is a 21 :8% relative reduction from the original baseline. This paper is structured as follows. Section 2 discusses related work in LangID and signal combination. Section 3 presents our method with a latticebased ensemble model and a deep neural networkbased improvement. We also explore methods in deep learning that work well in the signal com bination problem. Section 4 shows the experimental results. Finally, Section 5 concludes the paper. 2. RELATED WORK "
446,The Group Loss++: A deeper look into group loss for deep metric learning.txt,"Deep metric learning has yielded impressive results in tasks such as
clustering and image retrieval by leveraging neural networks to obtain highly
discriminative feature embeddings, which can be used to group samples into
different classes. Much research has been devoted to the design of smart loss
functions or data mining strategies for training such networks. Most methods
consider only pairs or triplets of samples within a mini-batch to compute the
loss function, which is commonly based on the distance between embeddings. We
propose Group Loss, a loss function based on a differentiable label-propagation
method that enforces embedding similarity across all samples of a group while
promoting, at the same time, low-density regions amongst data points belonging
to different groups. Guided by the smoothness assumption that ""similar objects
should belong to the same group"", the proposed loss trains the neural network
for a classification task, enforcing a consistent labelling amongst samples
within a class. We design a set of inference strategies tailored towards our
algorithm, named Group Loss++ that further improve the results of our model. We
show state-of-the-art results on clustering and image retrieval on four
retrieval datasets, and present competitive results on two person
re-identification datasets, providing a unified framework for retrieval and
re-identification.","MEasuring object similarity is at the core of many important machine learning problems like clustering and object retrieval. For visual tasks, this means learning a distance function over images. With the rise of deep neural networks, the focus has rather shifted towards learning a feature embedding that is easily separable using a simple distance function, such as the Euclidean distance. In essence, objects of the same class (similar) should be close by in the learned manifold, while objects of a different class (dissimi lar) should be far away. Historically, the best performing approaches get deep feature embeddings from the socalled siamese networks [1], which are typically trained using the contrastive loss [1] or the triplet loss [2], [3]. A clear drawback of these losses is that they only consider pairs or triplets of data points, missing key information about the relationships between all members of the minibatch. On a minibatch of size n, despite that the number of pairwise relations between sam ples isO(n2), contrastive loss uses only O(n=2)pairwise relations, while triplet loss uses O(2n=3)relations. Addi tionally, these methods consider only the relations between objects of the same class (positives) and objects of other classes (negatives), without making any distinction that negatives belong to different classes. This leads to not taking into consideration the global structure of the embedding space, and consequently results in lower clustering and retrieval performance. To compensate for that, researchers I.E, J.S, L.W and L.L.T are with Dynamic Vision and Learning Group at the Technical University of Munich, S.V, A.T and M.P are at Ca‚Äô Foscari University of Venice Corresponding authors: Ismail Elezi (ismail.elezi@tum.de), Laura Leal Taix¬¥ e (leal.taixe@tum.de) * denotes equal contributionrely on other tricks to train neural networks for deep metric learning: intelligent sampling [4], multitask learning [5] or hardnegative mining [6]. Recently, researchers have been increasingly working towards exploiting in a principled way the global structure of the embedding space [7], [8], [9], [10], typically by designing ranking loss functions instead of following the classic triplet formulations. In a similar spirit, we propose Group Loss , a novel loss function for deep metric learning that considers the sim ilarity between all samples in a minibatch. To create the minibatch, we sample from a Ô¨Åxed number of classes, with samples coming from a class forming a group . Thus, each minibatch consists of several randomly chosen groups, and each group has a Ô¨Åxed number of samples. An iterative, fullydifferentiable label propagation algorithm is then used to build feature embeddings which are similar for samples belonging to the same group, and dissimilar otherwise. At the core of our method lies an iterative process called replicator dynamics [11], [12], that reÔ¨Ånes the local information, given by the softmax layer of a neural network, with the global information of the minibatch given by the similarity between embeddings. The driving rationale is that the more similar two samples are, the more they affect each other in choosing their Ô¨Ånal label and tend to be grouped together in the same group, while dissimilar samples do not affect each other on their choices. We then study the embedding space generated by net works trained with our Group Loss, resulting in a few observations that we exploit by introducing a set of in ference strategies. We call this model, the Group Loss++ and show that reaches signiÔ¨Åcantly better results than the Group Loss , making clustering and image retrieval easier. Finally, we show that our proposed model can be used to train networks in the Ô¨Åeld of person reidentiÔ¨Åcation,arXiv:2204.01509v1  [cs.CV]  4 Apr 20222 thus providing a similarity learning uniÔ¨Åed framework that works both for retrieval and reidentiÔ¨Åcation. Our contribution in this work is Ô¨Åvefold: We propose the Group Loss , a novel loss function to train neural networks for deep metric embedding that takes into account the local information of the samples, as well as their similarity. We propose a differentiable labelpropagation iter ative model to embed the similarity computation within backpropagation, allowing endtoend train ing with our new loss function. We introduce a set of inference strategies, resulting in Group Loss++ that improve the results of our model. We show stateoftheart qualitative and quantita tive results in four standard clustering and retrieval datasets. We show competitive results on two person re identiÔ¨Åcation datasets, thus providing a uniÔ¨Åed framework for similarity learning. 2 R ELATED WORK "
447,Adaptive Neural Network Ensemble Using Frequency Distribution.txt,"Neural network (NN) ensembles can reduce large prediction variance of NN and
improve prediction accuracy. For highly nonlinear problems with insufficient
data set, the prediction accuracy of NN models becomes unstable, resulting in a
decrease in the accuracy of ensembles. Therefore, this study proposes a
frequency distribution-based ensemble that identifies core prediction values,
which are expected to be concentrated near the true prediction value. The
frequency distribution-based ensemble classifies core prediction values
supported by multiple prediction values by conducting statistical analysis with
a frequency distribution, which is based on various prediction values obtained
from a given prediction point. The frequency distribution-based ensemble can
improve predictive performance by excluding prediction values with low accuracy
and coping with the uncertainty of the most frequent value. An adaptive
sampling strategy that sequentially adds samples based on the core prediction
variance calculated as the variance of the core prediction values is proposed
to improve the predictive performance of the frequency distribution-based
ensemble efficiently. Results of various case studies show that the prediction
accuracy of the frequency distribution-based ensemble is higher than that of
Kriging and other existing ensemble methods. In addition, the proposed adaptive
sampling strategy effectively improves the predictive performance of the
frequency distribution-based ensemble compared with the previously developed
space-filling and prediction variance-based strategies.","A surrogate model is built to represent the true model with data obtained from a limited number of simulations  or experiments ; thus, predictions for computationally expensive models can be approximated ( Kang et al., 2019) .  Surrogate modeling focuses on input ‚Äìoutput behavior to find a model that approximates the rel ationship between  input and output as accurately as possible. Neural network (NN) is one of the promising surrogate models and i s  known to be a universal function approximator with a good prediction accuracy  (Pan et al. 2014) . Multiple  artificial neurons that mimic the structure and principle of biological neurons constitute NN ; these  artificial  neurons are interconnected to form a network that derives output for given input data  (Basheer and Hajmeer 2000 ).  NN co mprises  layered structures , in which nodes in each layer are connected to nodes in subsequent layers and  information moves in the forward direction.  NN parameters include weights and biases, and weights are given to  connections between nodes ( Eason and Cremaschi 2014) . The weighted sum of inputs from each node is  transferred to the next node through an activation function , and the parameters can be determined through training .  Once a  given data set is split into training and validation data set, the training data set is used to train the NN  model and  the validation data set is used to estimate the accuracy of the trained model . Applying NN as a surrogate  model in  various engineering d omains , such as mechanical engineering ( Chen et al. 2020; Lin et al. 2020 ; Ktari  et al. 2021 ; Schrader and Schauer 2021 ), structural engineering  (Gomes  et al., 2011; de Santana Gomes 2019;  Yƒ±lmaz et al. 2021 ; Freitag  et al. 202 0), aerospace engineering ( Bouhlel et al. 2020 ; Du et al. 2021; Zhang  et al.  2021 ), biomedical engineering ( Lu et al. 2013; Eskinazi and  Fregly 2015), chemical engineering ( Eason and  Cremaschi 2014 ; Moreno P√©rez et al. 2018 ), civil engineering ( Shaw et al. 2017 ; Garc√≠a Segura et al.  2017;  Thrampoulidis et al. 2021 ), and composite structures ( Papadopoulos et al.  2018 ; Yan et al. 2020) , has been recently  attempted.   However, NN has a problem  in that different models are created in accordance with  the selection of training  data sets, initial  parameters , and training algorithms (Zhang 2007) . This condition  results in various prediction  values ; thus, NNs are referred  to have high prediction variance . Ensembles that combine prediction values  obtained from multiple component models have be en developed  to reduce  the prediction variance of NNs  (Sollich  and Krogh 1996) . The basic premise of ensembles is that the errors of a single model can be compensated by the  other models (Sagi and Rokach 2018). Ensembles are generally constructed through two steps: training multiple  component models and combining prediction values  derived from the multiple component models.  Sufficiently  diverse component  models with high predictive performance  are required  for th e ensemble to have high accuracy   (Deng et al. 2013 ). Various component models can be created depending on the combination of training and validation data  sets and the selection of initial parameters.  If the validation error of the model is small, then the  predictive performance can be estimated to be accurate . The accuracy of the ensemble is highly dependent on the  combination of prediction values . Thus,  various methods , such as averag e ensemble ( Opitz and Shavlik 1996 ),  weigh ted ensemble ( Bishop 1995 ), and mode ensemble (Kourentzes  et al. 2014) , which combin e prediction  values ,  have been developed.  Many studies have shown that the prediction accuracy of ensembles combining multiple  component models is higher than that of a single component  model ( Wolper t 1992; Goodfellow et al. 2016 ).  In the case of real engineering applications , the size of the  data set for creating surrogate  models is usually  small because  simulations or experiments (e.g., finite element analysis  and collision test) are time  consuming and  expensive  (Gaspar et al. 2017; Lee et al. 2020 ). For highly nonlinear problems, the prediction accuracy of NN  decreases, and the situation worsens when the size of the  data set is small.  The deviation of prediction values  increases  as the prediction accuracy of component models becomes unstable , resulting in a decrease in the  accuracy of ensembles.  The mode ensemble that can cope well with outliers is suitable  when the deviation of  prediction values is large . In the mode ensemble, kernel density est imation (KDE)  is performed on t he prediction  values , and the value corresponding to the maximum density is identified as a mode  and used as the final prediction  value (Kourentzes  et al. 2014) . The mode is determined on the  basis of the  most frequent valu e; thus, the mode  ensemble is insensitive to outliers compared with average and weighted ensembles . However,  a biased prediction  value can be obtained  in the presence of  multiple high frequency values . In addition to the most frequent value ,  keep ing the possibility open to various prediction values  is also required  considering the unstable prediction  accurac y.  Therefore, this study proposes a frequency distribution based ensemble that explores the range of core  prediction values, which are expected to be close to the true prediction value.  For a given prediction point, various  prediction values obtained from the component models constitute a prediction value distribution , and the  frequency distribution based ensemble performs statistical analysis with a frequency distribution based on the  prediction value  distribution  to identify the core prediction values.  Prediction values belonging to the range  supported by multiple prediction values can be classified into  core prediction values, and the average of the core  prediction values is determined as the final prediction value.  An adaptive  sampling strategy , which  efficiently  improves the accuracy of the frequency distribu tionbased ensemble according to  the variance of core prediction  values , is also proposed.  Various examples are used to verify the proposed frequency distribution based ensemble  and adaptive  sampling strategy .  The rest of this paper is organized as follows.  Section 2  reviews  previous works related to NNs and ensembles.  Section 3 presents the proposed frequency distribution based ensemble  and t he adaptive sampling strategy , which   efficiently improves the predictive performance of the proposed frequenc y distribution based ensemble . Section  4 demonstrates the predictive accuracy of ensemble methods and compares the proposed adaptive sampling  strategy with other existing sampling strategies  through case studies.  Section 5 finally  provides the conclusion  and future directions.     2. Related works   "
448,Random Occlusion-recovery for Person Re-identification.txt,"As a basic task of multi-camera surveillance system, person re-identification
aims to re-identify a query pedestrian observed from non-overlapping multiple
cameras or across different time with a single camera. Recently, deep
learning-based person re-identification models have achieved great success in
many benchmarks. However, these supervised models require a large amount of
labeled image data, and the process of manual labeling spends much manpower and
time. In this study, we introduce a method to automatically synthesize labeled
person images and adopt them to increase the sample number per identity for
person re-identification datasets. To be specific, we use block rectangles to
randomly occlude pedestrian images. Then, a generative adversarial network
(GAN) model is proposed to use paired occluded and original images to
synthesize the de-occluded images that similar but not identical to the
original image. Afterwards, we annotate the de-occluded images with the same
labels of their corresponding raw images and use them to augment the number of
samples per identity. Finally, we use the augmented datasets to train baseline
model. The experiment results on CUHK03, Market-1501 and DukeMTMC-reID datasets
show that the effectiveness of the proposed method.","Person re identification (ReID ) is an important task in many computer vision systems, such as  behavioral understanding, threat detection and video surveillance. Given a query person image, it aims  to reidentify the person observed by non overlapping multiple cameras or across differen t time with a  single camera. The task has drawn significant attention in computer vision community. So far, it is still a  challenge issue for the appearance of a person may suffer dramatic change under different camera views.  Traditional hand craft methods  address the person ReID issue through either finding discriminative  feature representations [14] or exploiting a suitable dis tance metric function  [57]. When feature  representations are obtained, a distance metric function is applied to estimate whet her the paired inputs  are the same pedestrian or not. Recently, enlightened by the success of deep learning technology, a large   number  of works introduce  the technology to address the person ReID issue and achieve  many promising  performances. Most of recent state oftheart person ReID models are based on deep learning technology.  Both the training processes of the models require a large amount of labeled data. However, existing  available public person ReID dataset s are limited in their scale s, especially for  the number of images per identity. For example, the average numbers of images  per identity for large scale ReID datasets like  CUHK03 [8], Market 1501 [9] and DukeMTMC ReID [10] are only 9.6, 17.2 and 23.5, respectively.  Using such scale datasets to train the deep models may lead to over fitting issue and affect the robustness  of them. Moreover, r ely on human annotating is expensive and time consuming, for one not only needs  to draw a bounding box for pedestrians, but also nee ds to assign each of them an ID . Recently , several  GAN based data augmentation approaches [11, 12] have been introduced to alleviate this issue.  However,  due to the appearances and backgrounds of the generated samples are far away from their original  identities , most of approaches above have to assign the generated sample a new label. Therefore, the  number of images per identity cannot  be increased. In order to solve th e above problem, our motivation  is to generate sample s that similar enough but not identic al to their original images and to annotate them   with their original label information , thus the number of images per identity can be increased.   To this end, as shown in Figure 2, firstly, we use block rectangles to randomly occlude the original  ReID training images. Secondly, we use the paired occluded images and their ground truths to train a de occluding  GAN model. Then the trained GAN model is used to generate the de occluded images with  the same labels of their corresponding ground truths. Finally , these generated images are combined with  the original ReID training images to train the baseline model  together .  In summary, we make the following contributions:   (1) We introduce a data augmentation  method  that can generate person images which similar  enough but not identical to the original pedestrian  images;   (2) We attempt to assign the original annotation information to the generated pedestrian  images,  thus increasing the sample number per identity;   (3) We show that the proposed data augmentation method with original annotation information  improved the person ReID accuracy with the baseline mo del.   2. Related work   "
449,Towards a Robust Differentiable Architecture Search under Label Noise.txt,"Neural Architecture Search (NAS) is the game changer in designing robust
neural architectures. Architectures designed by NAS outperform or compete with
the best manual network designs in terms of accuracy, size, memory footprint
and FLOPs. That said, previous studies focus on developing NAS algorithms for
clean high quality data, a restrictive and somewhat unrealistic assumption. In
this paper, focusing on the differentiable NAS algorithms, we show that vanilla
NAS algorithms suffer from a performance loss if class labels are noisy. To
combat this issue, we make use of the principle of information bottleneck as a
regularizer. This leads us to develop a noise injecting operation that is
included during the learning process, preventing the network from learning from
noisy samples. Our empirical evaluations show that the noise injecting
operation does not degrade the performance of the NAS algorithm if the data is
indeed clean. In contrast, if the data is noisy, the architecture learned by
our algorithm comfortably outperforms algorithms specifically equipped with
sophisticated mechanisms to learn in the presence of label noise. In contrast
to many algorithms designed to work in the presence of noisy labels, prior
knowledge about the properties of the noise and its characteristics are not
required for our algorithm.","To avoid exhausting engineering, Neural Architecture Search (NAS) has emerged as a leading mechanism for automatic design and wiring of neural networks. NAS has been successfully moving forward with diverse ap proaches to achieve a robust automatic architecture search e.g., evolutionbased NAS [11, 30, 31, 37], optimization based NAS [10, 23, 24, 34, 43], and Reinforcement Learning (RL) based NAS [29, 62, 63]. Specifically, a gradientbased method with a continuous architecture space called Differ entiable ARchiTecture Search (DARTS) has attracted sig nificant attention in NAS because of a reduced cost and complexity of searching for high performance architectures.In this paper, we go beyond merely learning with NAS under regular assumptions of clean labels. Supervised learning with neural networks often leads to a performance degradation due to overfitting, especially in the presence of label noise which often emerges due to data corruption and/or human annotation errors especially promi nent in large scale datasets. As a result, neural networks fail to generalize well to previously unseen data and achieve sub optimal classification results. Given the importance of such problems, existing stateoftheart methods are specifically designed to deal with the data noise by correcting labels [42], employing dedicated loss functions [5, 26, 46, 58], reweight ing samples [17, 33], selecting samples [15], and modeling a transition matrix [28, 40, 48]. However, designing robust neural networks that mitigate overfitting due to label noise is still unexplored. To this end, we propose a structural ap proach, that is a method that requires no explicit changes to neither the loss functions nor the input samples nor the final outputs (predictions) but the neural network structure. More over, our approach does not require specific assumptions on the amount or the type of label noise. Our primary focus1is to investigate and advance a robust method to search an architecture in the presence of label noise. To this end, we aim to answer the following questions: ‚Ä¢Motivational Curiosity. As handcrafted neural net works ( e.g., Inception [38]) overfit to noisy labels [55] (even the pretext labeling in selfsupervised UnNAS is imperfect), is the test performance of neural networks constructed by NAS also degraded by the label noise? ‚Ä¢Research Curiosity. Can we design an operator2that is robust to noisy labels and helps existing NAS methods (e.g., DARTS [23]) to perform well in the presence of label noise? The answer to the first question is affirmative as shown in Fig. 1. Firstly, we highlight that the performance of vanilla 1Note that training robust NAS under the label noise is not the same problem as using static network for classification under the label noise. 2An operator is an operation connecting two nodes in NAS e.g., the standard NAS uses conv., max or averagepooling, skip connections, etc. 10% 50% Noise Rate5060708090Accuracy (%)DARTS + our op. Vanilla DARTS94.895.1 85.7 69.0(a) 265457 232852 Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.CountDARTS + our op. Vanilla DARTS Vanilla DARTS 50%Sym. Noise 50%Sym. Noise Clean Labels 60 40 20 060 40 20 060 40 20 0 (b) 265457 232852 Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.CountDARTS + our op. Vanilla DARTS Vanilla DARTS 50%Sym. Noise 50%Sym. Noise Clean Labels 60 40 20 060 40 20 060 40 20 0 (c) 265457 232852 Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.Parameterless  Op.Convolutional  Op.CountDARTS + our op. Vanilla DARTS Vanilla DARTS 50%Sym. Noise 50%Sym. Noise Clean Labels 60 40 20 060 40 20 060 40 20 0 (d) Figure 1: (a) Testing accuracy of vanilla vs.our approach (CIFAR10, clean labels vs.50%symmetric label noise). The histogram of found operators (five runs) of (b) the normal cell of vanilla DARTS under 50%symmetric label noise, (c) the normal cell of DARTS with nConv (our noise injecting operator) searched on CIFAR10 (50%symmetric noisy labels), and (d) the normal cell of vanilla DARTS with clean labels. The normal cell of vanilla DARTS under label noise is constructed poorly due to the large number of parameterless operations. It is apparent the network thus looses the learning capacity in an attempt to prevent overfitting to the noise by selecting parameterless operators. In contrast, DARTS with our proposed operator mitigates such a poor cell design as highlighted by the larger number of convolutional operators being selected in place of a parameterless operator. DARTS [23] suffers when subjected to noisy labels as shown in Fig. 1 (a). Furthermore, the architecture searched by vanilla DARTS under label noise results in a bad architec ture as shown in Fig. 1 (b) while our approach shown in Fig. 1 (c) produces a better designed cell. To answer the second question, we analyze the task of learning deep neural networks under noisy labels using Information Bottleneck. As a result of this analysis, we introduce a variant of the convolutional operation by injecting noise into the pipeline. Upon learning the parameters of the noise from data, we will empirically show that robustness against label corruption can be attained. In essence, we will show later that the noisy convolution regularizes and limits the gradient of the noisy samples during training. In short, our key contributions are: i.We show that the search through noisy labels degrades the classification performance of standard NAS. ii.We provide an information theoretic framework to tackle learning noisy labels. Our proposed noise injection op erator performs implicit regularization during learning preventing overfitting to noisy labels. iii.The proposed operator is included into the NAS search space with the goal of preventing overfitting during train ing with noisy labels. A noise injection on the input of the operator turns activations of hidden units into the socalled stochastic activations. iv.We show experimentally that architectures emerging from the NAS search with our proposed noise injecting operator outperform under fewer parameters the state of the art, especially in the case of no prior knowledge given e.g., the lack of the noise type/its rates.2. Related work "
450,Apparent Age Estimation Using Ensemble of Deep Learning Models.txt,"In this paper, we address the problem of apparent age estimation. Different
from estimating the real age of individuals, in which each face image has a
single age label, in this problem, face images have multiple age labels,
corresponding to the ages perceived by the annotators, when they look at these
images. This provides an intriguing computer vision problem, since in generic
image or object classification tasks, it is typical to have a single ground
truth label per class. To account for multiple labels per image, instead of
using average age of the annotated face image as the class label, we have
grouped the face images that are within a specified age range. Using these age
groups and their age-shifted groupings, we have trained an ensemble of deep
learning models. Before feeding an input face image to a deep learning model,
five facial landmark points are detected and used for 2-D alignment. We have
employed and fine tuned convolutional neural networks (CNNs) that are based on
VGG-16 [24] architecture and pretrained on the IMDB-WIKI dataset [22]. The
outputs of these deep learning models are then combined to produce the final
estimation. Proposed method achieves 0.3668 error in the final ChaLearn LAP
2016 challenge test set [5].","Age estimation from face images is an interesting com puter vision problem. It has various applications, ranging from customer relations to biometrics and entertainment. Although, there are various studies on real age estimation, [7, 6, 26] to name a few, problem of apparent age estimation from face images is a recently introduced topic, which has received signiÔ¨Åcant attention [4, 20, 14, 22, 15]. Besides the common difÔ¨Åculties in facial image process ing and analysis, such as pose and illumination, the main challenges associated with age estimation have been the im Both authors contributed equally to this work. Figure 1. Sample images from the ChaLearn LAP 2016 apparent age estimation challenge dataset. Numbers below the images rep resent their average apparent age and standard deviations. The higher the standard deviation is, the harder it gets to predict appar ent age of a person. Especially, in old age group, standard devia tions are generally large, which makes it difÔ¨Åcult to perform accu rate age prediction from the images of old people. For instance, in LAP 2016 training set, there are 1095 images with standard devia tions less than three, and only 31 of them belong to someone older than 40. pact of ethnicity and subjective factors. The task of apparent age estimation alleviates the challenges posed by subjective factors. Since every person ages differently, real age may not be easy to deduce from face images. However, apparent age estimation is based on annotaters‚Äô perception of subarXiv:1606.02909v1  [cs.CV]  9 Jun 2016jects‚Äô ages, who are displayed in the images. Therefore, the judgments for the age labes are expected to be based on visual appearance cues rather than personal characteristics. On the other hand, these perceived ages are subjective and they depend on the annotators, leading to result in multiple age labels for the same face image. This is a very intrigu ing problem, considering that for object classiÔ¨Åcation prob lems, we normally have a single label per image. Sample images from the ChaLearn LAP 2016 dataset can be seen in Figure 1. In this study, we have proposed a novel approach for ap parent age estimation, which addresses the problem of im precise, uncertain multiple labels by grouping the face im ages that are within a speciÔ¨Åed age range, and by training an ensemble of deep learning models using these age groups and their ageshifted groupings. Convolutional Neural Networks (CNNs) have shown sig niÔ¨Åcant performance improvement in several computer vi sion problems, such as image classiÔ¨Åcation [23], object de tection [23], image segmentation [30], and face recognition [19]. Moreover, all the best performing systems proposed in ChaLearn LAP 2015 [4] were based on CNNs [22, 15, 31]. Due to these reasons, we have opted for CNNs for the pro posed system and have conducted a thorough study to efÔ¨Å ciently transfer already existing models for the problem at hand. In addition, ensemble models are known to increase performance further. To beneÔ¨Åt from this, we have gener ated different age groupings and train multiple CNN mod els. Finally, we have combined the outputs of these CNN models to produce the Ô¨Ånal estimation result. The contributions of this study can be summarized as follows: (i) We proposed an apparent age estimation sys tem that takes into account the imprecise, uncertain multi ple labels available for the task. Instead of using avarage age labels as class labels, we have grouped the face im ages that are within a speciÔ¨Åed age range. An ensemble of CNNs have been trained using these age groups and their ageshifted groupings. (ii) We have conducted an exten sive assessment about transferability of existing CNN mod els for apparent age estimation. (iii) We have analyzed the apparent age estimation problem in detail and pointed the challenges associated to it. The rest of the paper is organized as follows: In Sec tion 2, a brief overview of related work is provided. In Sec tion 3, the problem is stated and the challenges associated to it are pointed. The proposed method is explained in detail in Section 4. Experimental results are presented and discussed in Section 5. Finally, in Section 6, the paper is concluded with a brief summary and discussion. 2. Related Work "
451,NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection.txt,"The labeling cost of large number of bounding boxes is one of the main
challenges for training modern object detectors. To reduce the dependence on
expensive bounding box annotations, we propose a new semi-supervised object
detection formulation, in which a few seed box level annotations and a large
scale of image level annotations are used to train the detector. We adopt a
training-mining framework, which is widely used in weakly supervised object
detection tasks. However, the mining process inherently introduces various
kinds of labelling noises: false negatives, false positives and inaccurate
boundaries, which can be harmful for training the standard object detectors
(e.g. Faster RCNN). We propose a novel NOise Tolerant Ensemble RCNN (NOTE-RCNN)
object detector to handle such noisy labels. Comparing to standard Faster RCNN,
it contains three highlights: an ensemble of two classification heads and a
distillation head to avoid overfitting on noisy labels and improve the mining
precision, masking the negative sample loss in box predictor to avoid the harm
of false negative labels, and training box regression head only on seed
annotations to eliminate the harm from inaccurate boundaries of mined bounding
boxes. We evaluate the methods on ILSVRC 2013 and MSCOCO 2017 dataset; we
observe that the detection accuracy consistently improves as we iterate between
mining and training steps, and state-of-the-art performance is achieved.","With the recent advances in deep learning, modern ob ject detectors, such as Faster RCNN [22], YOLO [21], SSD [20] and RetinaNet [18], are reliable in predicting both ob ject classes and their bounding boxes. However, the appli cation of deep learningbased detectors is still limited by the efforts of collecting bounding box training data. These detectors are trained with huge amount of manually labelled bounding boxes. In real world, each application may require Source detector Boxes | Labels Training Localization  Target detector Box predictions Mined box labels Image labels  Predict  InitUpdate Seed box labels Figure 1. Iterative trainingmining pipeline. us to detect a unique set of the categories. It‚Äôs expensive and timeconsuming to label tens of thousands of object bound ing boxes for each application. To reduce the effort of labelling bounding boxes, re searchers worked on learning object detectors with only imagelevel labels, which are substantially cheaper to an notate, or even free with image search engines; this task is called weakly supervised object detection [4, 26, 30]. Mul tiple Instance Learning (MIL) [6] based trainingmining pipeline [4, 26, 28] is widely used for this task; however, the resulting detectors perform considerably worse than the fully supervised counterparts. We believe the reasons are twofold: First, a detector learned with only imagelevel la bels often performs poorly in localization, it may focus on the object part, but not the whole object ( e.g., in Figure 2, ‚Äúcat‚Äù detector detects cat head); second, without an accurate detector, object instances cannot be mined correctly, espe cially when the scene is complicated. To address the aforementioned problems in weakly su pervised object detection, we propose a semisupervised object detection setting: learning an object detector with a limited amount of labelled bounding boxes (e.g. 10 to 20 images with fully labeled bounding boxes) as well as a large amount of imagelevel labels. SpeciÔ¨Åcally, we want to train an object detector for a set of target categories. For target categories, a small amount of seed bounding box an notations and a large amount of imagelevel annotations are 1arXiv:1812.00124v1  [cs.CV]  1 Dec 2018available for training. We also assume that a pretrained ob ject detector for source categories is available. The source and target categories do not overlap with each other. Given the wide availability of large scale object detection datasets, such as MSCOCO [19] and ILSVRC [23], this assumption is not hard to satisfy in practice. This assumption is not es sential for the formulation either. Note that our formulation is different from previous semisupervised object detection [11, 27], in which the seed bounding box annotations are not considered. The standard trainingmining pipeline [4, 26] in weakly supervised object detection iterates between the following steps: 1. Train object detector with the mined bounding boxes (the initial detector is trained with the whole images and the labels); 2. Mine the bounding boxes with the current object detector. A straightforward way to incorporate the seed bounding boxes is that we use them to train the initial object detector, mine bounding boxes with the initial ob ject detector, train a new detector with both seed and mined bounding boxes, and iterate between mining and training steps. The mining process inherently introduces various types of noise. First, mining process inevitably misses some ob jects, which are treated as negative ( i.e. background) sam ples in training phase; such false negatives are harmful for training the classiÔ¨Åcation head of object detector. Second, the boundaries of the mined bounding boxes are not precise, which is harmful for learning the box regression head of the detector. Third, the class labels of the mined boxes cannot be 100% accurate, leading to some false positives. Some vi sualization examples for the mined labels from the baseline method are shown in Figure 2. Because of these issues, we observe that the detection accuracy usually decreases as we iterate between training and mining steps if standard object detector architecture ( e.g. Faster RCNN) is employed. We propose a novel NOise Tolerant Ensemble RCNN (NOTERCNN) architecture. The NOTERCNN incorpo rates an ensemble of classiÔ¨Åcation heads for both box pre dictor ( i.e. second stage) and region proposal predictor ( i.e. Ô¨Årst stage) to increase the precision of the mined bound ing boxes, i.e., reduce false positives. SpeciÔ¨Åcally, one classiÔ¨Åcation head is only trained with seed bounding box annotations; the other head is trained with both seed and mined box annotations. The consensus of the both heads is employed to determine the conÔ¨Ådence of the classiÔ¨Åca tion. This is similar to recent work that uses ensemble for robust estimation of prediction conÔ¨Ådence [3, 2]. We also utilize the knowledge of the pretrained detector on source categories as weak teachers . SpeciÔ¨Åcally, another classiÔ¨Å cation head is added to distill knowledge [10] from a weak teacher; the distillation process acts as a regularizer to pre vent the network from overÔ¨Åtting on the noisy annotations. The NOTERCNN architecture is also designed to be ro Figure 2. Top: examples of weakly supervised object detection failure cases: poor localization; objects can‚Äôt be discovered in complicated scenes. Bottom: examples of the mined box noises using a standard faster RCNN: 1) false negatives, 2) false pos itives, 3) inaccurate box boundaries; groundtruth boxes are in black, mined boxes are in other colors. bust to false negative labels. For the classiÔ¨Åcation head in the box predictor that uses mined bounding boxes for train ing, we remove the loss of predicting negatives ( i.e. back ground) from its training loss, thus the training is not af fected by the false negatives. Finally, the regression head is only trained with the seed bounding boxes, which avoids it being affected by the inaccurate boundaries of the mined bounding boxes. We evaluated the proposed architecture on MSCOCO [19] and ILSVRC [23] datasets. The experimental results show that the proposed framework increases the precision of mined box annotations and can bring up to 40% improve ment on detection performance by iterative training. Com pared with weakly supervised detection, training with seed annotations using NOTERCNN improves the stateofthe art performance from 36.9% to 43.7%, while using standard Faster RCNN only achieves 38.7%. We also perform a large scale experiment which employs MSCOCO as seed annota tions and Open Image Dataset as imagelevel annotations. We observe the proposed method also leads to consistent performance improvement during the trainingmining pro cess. In summary, our contributions are threefold: Ô¨Årst, we propose a practical semisupervised object detection prob lem, with a limited amount of labelled bounding boxes as well as a large amount of imagelevel labels; second, we identiÔ¨Åed three detrimental types of noise that inherently exists in trainingmining framework ; third, we propose a novel NOTERCNN architecture that is robust to such noise, and achieves stateoftheart performance on bench mark datasets. 2. Related Work "
452,Confidence Adaptive Regularization for Deep Learning with Noisy Labels.txt,"Recent studies on the memorization effects of deep neural networks on noisy
labels show that the networks first fit the correctly-labeled training samples
before memorizing the mislabeled samples. Motivated by this early-learning
phenomenon, we propose a novel method to prevent memorization of the mislabeled
samples. Unlike the existing approaches which use the model output to identify
or ignore the mislabeled samples, we introduce an indicator branch to the
original model and enable the model to produce a confidence value for each
sample. The confidence values are incorporated in our loss function which is
learned to assign large confidence values to correctly-labeled samples and
small confidence values to mislabeled samples. We also propose an auxiliary
regularization term to further improve the robustness of the model. To improve
the performance, we gradually correct the noisy labels with a well-designed
target estimation strategy. We provide the theoretical analysis and conduct the
experiments on synthetic and real-world datasets, demonstrating that our
approach achieves comparable results to the state-of-the-art methods.","With the emergence of highlycurated datasets such as ImageNet [ 1] and CIFAR10 [ 2], deep neural networks have achieved remarkable performance on many classiÔ¨Åcation tasks [ 3‚Äì5]. However, it is extremely timeconsuming and expensive to label a new largescale dataset with highquality annotations. Alternatively, we may obtain the dataset with lower quality annotations efÔ¨Åciently through online keywords queries [ 6] or crowdsourcing [ 7], but noisy labels are inevitably introduced consequently. Previous studies [ 8,9] demonstrate that noisy labels are problematic for overparameter ized neural networks, resulting in overÔ¨Åtting and performance degradation. Therefore, it is essential to develop noiserobust algorithms for deep learning with noisy labels. The authors of [ 8‚Äì11] have observed that deep neural networks learn to correctly predict the true labels for all training samples during early learning stage, and begin to make incorrect predictions in memorization stage as it gradually memorizes the mislabeled samples (in Figure 1 (a) and (b)). In this paper, we introduce a novel regularization approach to prevent the memorization of mislabeled samples (in Figure 1 (c)). Our contributions are summarized as follows: Preprint. Under review.arXiv:2108.08212v2  [cs.LG]  5 Sep 2021(a) Cross Entropywith MultiStep learning rate scheduler(b) Cross Entropywith Cosine Annealing learning rate scheduler(c) ConÔ¨Ådence Adaptive Regularizationwith Cosine Annealing learning rate schedulerFigure 1: We conduct the experiments on the CIFAR10 dataset with 40% symmetric label noise using ResNet34 [ 5]. The top row shows the fraction of samples with clean labels that are predicted correctly (purple) and incorrectly (black). In contrast, the bottom row shows the fraction of samples with false labels that are predicted correctly (purple), memorized (i.e. the prediction equals the false label, shown in blue), and incorrectly predicted as neither the true nor the labeled class (black). For samples with clean labels, all three models predict them correctly with the increasing of epochs. However, for false labels in (a), the model trained with crossentropy loss Ô¨Årst predicts the true labels correctly, but eventually memorizes the false labels. With the cosine annealing learning rate scheduler [12] in (b), the model only slows down the speed of memorizing the false labels. However, our approach shown in (c) effectively prevents memorization, allowing the model to continue learning the correctlylabeled samples to attain high accuracy on samples with both clean and false labels. ‚Ä¢We introduce an indicator branch to estimate the conÔ¨Ådence of model prediction and propose a novel loss function called conÔ¨Ådence adaptive loss (CAL) to exploit the earlylearning phase. A high conÔ¨Ådence value is likely to be associated with a clean sample and a low conÔ¨Ådence value with a mislabeled sample. Then, we add an auxiliary regularization term forming a conÔ¨Ådence adaptive regularization (CAR) to further segregate the mislabeled samples from the clean samples. We also develop a strategy to estimate the target probability instead of using the noisy labels directly, allowing the proposed model to suppress the inÔ¨Çuence of the mislabeled samples successfully. ‚Ä¢We theoretically analyze the gradients of the proposed loss functions and compare them with cross entropy loss. We demonstrate that CAL and CAR have similar effects to existing regularization based approaches. Both neutralize the inÔ¨Çuence of the mislabeled samples on the gradient, and ensure the contribution from correctlylabeled samples to the gradient remains dominant. We also prove the robustness of the auxiliary regularization term to label noise. ‚Ä¢We show that the proposed approach achieves comparable and even better performance to the stateoftheart methods on four benchmarks with different types and levels of label noise. We also perform an ablation study to evaluate the inÔ¨Çuence of different components. 2 Related work "
453,Automatic Semantic Segmentation of the Lumbar Spine: Clinical Applicability in a Multi-parametric and Multi-centre Study on Magnetic Resonance Images.txt,"One of the major difficulties in medical image segmentation is the high
variability of these images, which is caused by their origin (multi-centre),
the acquisition protocols (multi-parametric), as well as the variability of
human anatomy, the severity of the illness, the effect of age and gender, among
others. The problem addressed in this work is the automatic semantic
segmentation of lumbar spine Magnetic Resonance images using convolutional
neural networks. The purpose is to assign a class label to each pixel of an
image. Classes were defined by radiologists and correspond to different
structural elements like vertebrae, intervertebral discs, nerves, blood
vessels, and other tissues. The proposed network topologies are variants of the
U-Net architecture. Several complementary blocks were used to define the
variants: Three types of convolutional blocks, spatial attention models, deep
supervision and multilevel feature extractor. This document describes the
topologies and analyses the results of the neural network designs that obtained
the most accurate segmentations. Several of the proposed designs outperform the
standard U-Net used as baseline, especially when used in ensembles where the
output of multiple neural networks is combined according to different
strategies.","Magnetic Resonance (MR) images are obtained by means of a technique based on magnetic Ô¨Åelds where frequencies in the range of radio waves (8‚Äì130 MHz) are used. This technique ob tains medical images with the maximum level of detail so far. In recent years, MR images became essential to obtain qual ity images from any part of the human body thanks to the fact that MR images provide either functional and morphological information of both anatomy and pathological processes; with a spatial resolution and constrast much higher than the obtained by means of other techniques for medical image acquisition. Concerning lumbar pathologies, MR imaging is the preferred type of images between radiologists and physicians specialized in the lumbar spine and the spine in general. Thanks to MR im ages they can Ô¨Ånd disorders in nerve structures, vertebrae, in tervertebral discs, muscles and ligaments with much more pre cision than ever (Roudsari and Jarvik, 2010). Manual inspection and analysis carried out by human experts (typically radiologists) is the most common methodology to ex tract information from MR images. Visual inspection is carried out slide by slide in order to determine the location, size and ‚àóCorresponding authors: jhonjsaenzg@gmail.com (J.J. S ¬¥aenzGamboa), delaiglesia mar@gva.es (M. IglesiaVay ¬¥a)pattern of multiple clinical Ô¨Åndings in the lumbar structures, that can be either normal or pathological. Manual inspection of slides has a strong dependency on the experience of each ex pert, so that the variability due to di Ô¨Äerent criteria of experts is a challenge that cannot be ignored (Carrino et al., 2009; Berg et al., 2012). Radiologists, even those with a great experience, need a lot of time to perform the visual inspection of images, so this is a very slow task as well as tedious and repetitive. In fact, the excess of information to be processed visually causes fatigue and loss of attention, which leads radiologists to not per ceive some obvious nuances because of the ‚Äútemporary blind ness due to workload excess‚Äù (Konstantinou et al., 2012). Current progress of ArtiÔ¨Åcial Intelligence (AI) and its appli cation to medical imaging is providing new and more sophisti cated algorithms based on Machine Learning (ML) techniques. These new algorithms are complementary to the existing ones in some cases, but in general they perform much better because most of the existing ones are knowledge based (do not learn from data). The new algorithms are much more robust to de tect the lumbar structures (i.e., vertebrae, intervertebral discs, nerves, blood vessels, muscles and other tissues) and repre sent a signiÔ¨Åcant reduction in the workload of radiologists and traumatologists (Coulon et al., 2002; Van Uitert et al., 2005; De Leener et al., 2014, 2015).arXiv:2111.08712v3  [eess.IV]  8 Nov 2022In the context of AI, automatic semantic segmentation is cur rently the most widely used technique (Litjens et al., 2017). This technique classiÔ¨Åes each individual pixel from an image into one of several classes or categories; each class or category corresponds to a type of objects from real world to be detected. In recent years, Convolutional Neural Networks (CNNs) are considered the best ML technique to address semantic segmen tation tasks. However, CNNs require a very large amount of manually annotated images to properly estimate the values of the millions of weights corresponding to all the layers of any CNN topology designed by a Deep Learning (DL) expert. Ro bustness and precision of any classiÔ¨Åer based on CNNs strongly depend on the number of samples available to train the weights of the CNN. So, the challenge in all the projects addressing the task of semantic segmentation is the availability of large enough datasets of medical images. In order to have a minimum of samples to train models, a manual segmentation procedure was designed in this work, where both MR image types T1w and T2w were used to manually adjust the boundaries between structural elements and tissues. Subsection 3.1.2 provides more detail about both MR image types. The main objective of this study is to use a limited dataset of MR images to reach an accurate and e Ô¨Écient segmentation of the structures and tissues from the lumbar region by means of using individually optimized CNNs or ensembles of several CNNs; all the used topologies were based on the original UNet architecture, i.e., they are variants from the UNet. This paper is organised as follows: Section 2 reviews the state of the art and references other works related to the au tomatic semantic segmentation of medical images. Section 3 provides details about the used resources, where Subsection 3.1 describes the dataset used in this work, and Subsection 3.2 pro vides details of the hardware infrastructure and software toolk its. Section 4 describes the block types used in this work to design CNN topologies as variants from the original UNet ar chitecture. Section 5 describes the experiments carried out in this work. Sections 6 and 7 present and discuss the results re spectively. Finally, Section 8 concludes by taking into account the deÔ¨Åned objectives and draws possible future works. 2. Related work "
454,Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes.txt,"How should one combine noisy information from diverse sources to make an
inference about an objective ground truth? This frequently recurring, normative
question lies at the core of statistics, machine learning, policy-making, and
everyday life. It has been called ""combining forecasts"", ""meta-analysis"",
""ensembling"", and the ""MLE approach to voting"", among other names. Past studies
typically assume that noisy votes are identically and independently distributed
(i.i.d.), but this assumption is often unrealistic. Instead, we assume that
votes are independent but not necessarily identically distributed and that our
ensembling algorithm has access to certain auxiliary information related to the
underlying model governing the noise in each vote. In our present work, we: (1)
define our problem and argue that it reflects common and socially relevant real
world scenarios, (2) propose a multi-arm bandit noise model and count-based
auxiliary information set, (3) derive maximum likelihood aggregation rules for
ranked and cardinal votes under our noise model, (4) propose, alternatively, to
learn an aggregation rule using an order-invariant neural network, and (5)
empirically compare our rules to common voting rules and naive
experience-weighted modifications. We find that our rules successfully use
auxiliary information to outperform the naive baselines.","Many collective decision making processes aggregate noisy good faith opinions in order to make an inference about some underlying ground truth. In cooperative policy making, for example, each party advocates for the policy they believe is objectively best. Similarly, in academic peer review, a metareviewer combines good faith reviewer opinions about a submitted paper. Other examples are easy to come by. We refer to this setting as objective social choice, to contrast it with the typical subjective social choice setting [ 35], where the optimal choice is defined in terms of the voter utilities *Code available at https://github.com/spitis/objective_social_choice Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020), B. An, N. YorkeSmith, A. El Fallah Seghrouchni, G. Sukthankar (eds.), May 9‚Äì13, 2020, Auckland, New Zealand .¬©2020 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.rather than a ground truth. Whereas subjective social choice can be viewed as collective compromise , objective social choice can be viewed as collective estimation . Unlike the subjective setting, where it is natural to consider each source or voter equally‚Äîan axiom known as ‚Äúanonymity‚Äù [ 32]‚Äî objective analysis suggests otherwise: diverse and more informed opinions should be valued more. Many sensible, realworld set tings involve asymmetric (nonanonymous) voting, making this a relevant line of analysis. Academic review is one. Another is corporate governance, where different stakeholder classes have varying voting powers, depending on the issue. In such cases, vary ing voter weights are natural, and one can evaluate the quality of social choices via other avenues (e.g., direct evaluation [ 27] or ex post analysis [ 19]). In other settings, such as national elections, the objective approach raises ethical concerns of fairness, and the objective approach may be inappropriate. Although objective social choice has been the subject of numer ous studies in social choice [ 6,10,12,44], forecasting [ 3,9,13], statistics [ 16,18] and machine learning [ 15,37] (Section 2), to our knowledge, no prior work has dealt with the case of noni.i.d. ordi nal feedback (i.e., ranked preferences). Yet this is the case in many practical applications. During peer review, for instance, two of three reviewers might share primary areas of expertise, but being human, cannot share comparable cardinal estimates. Or consider a robot that must aggregate feedback from human principals. Once again, the different principals will draw upon diverse background to form their opinions, which can only be shared as ordinal preferences. In each case, how should the noni.i.d. feedback be aggregated? Our work is intended as a first step toward answering this ques tion. To narrow the scope of our inquiry, we make several modeling assumptions (Section 3), which we hope can be relaxed in future work. In particular, we assume that (1) the underlying ground truth and noise generating process is modeled as a karmed bandit prob lem, where the different arms represent different alternatives, (2) different voters see different pulls of the arms, and (3) the social decision rule sees how many pulls each voter saw (but not their outcomes). We solve for the maximum likelihood social choice in a series of cases (Section 4). As our derived rules rest on strong assumptions about the noise generation process, we also propose to learn a more flexible aggregation rule using an orderinvariant neural network (Section 4.2). We empirically compare our derived and learned rules to classical voting rules (Section 5). Our results confirm the intuition that objective estimation can be improved by upweighing opinions from diverse and more informed sources.arXiv:2001.10092v1  [cs.MA]  27 Jan 2020AAMAS‚Äô20, May 9‚Äì13, 2020, Auckland, New Zealand Silviu Pitis and Michael R. Zhang 2 RELATED WORK "
455,ML-KFHE: Multi-label ensemble classification algorithm exploiting sensor fusion properties of the Kalman filter.txt,"Despite the success of ensemble classification methods in multi-class
classification problems, ensemble methods based on approaches other than
bagging have not been widely explored for multi-label classification problems.
The Kalman Filter-based Heuristic Ensemble (KFHE) is a recent ensemble method
that exploits the sensor fusion properties of the Kalman filter to combine
several classifier models, and that has been shown to be very effective. This
work proposes a multi-label version of KFHE, ML-KFHE, demonstrating the
effectiveness of the KFHE method on multi-label datasets. Two variants are
introduced based on the underlying component classifier algorithm,
ML-KFHE-HOMER, and ML-KFHE-CC which uses HOMER and Classifier Chain (CC) as the
underlying multi-label algorithms respectively. ML-KFHE-HOMER and ML-KFHE-CC
sequentially trains multiple HOMER and CC multi-label classifiers and
aggregates their outputs using the sensor fusion properties of the Kalman
filter. Experiments and detailed analysis performed on thirteen multi-label
datasets and eight other algorithms, including state-of-the-art ensemble
methods, show that for both versions, the ML-KFHE framework improves the
ensembling process significantly with respect to bagging based combinations of
HOMER and CC, thus demonstrating the effectiveness of ML-KFHE. Also, the
ML-KFHE-HOMER variant was found to perform consistently and significantly
better than existing multi-label methods including existing approaches based on
ensembles.","A multiclass classiÔ¨Åcation task assigns an object to at most one class. Real world classiÔ¨Åcation problems exist, however, where an object can be assigned Corresponding author Email addresses: arjun.pakrashi@ucd.ie (Arjun Pakrashi), brian.macnamee@ucd.ie (Brian Mac Namee) Preprint submitted to Information Fusion March 11, 2021arXiv:1904.10552v3  [cs.LG]  10 Mar 2021to more than one class simultaneously [12]. In other words, an object can be labelledwith more than one class at the same time. For example, an image of a landscape may contain mountains, sea and sky and therefore it can be a member of each of the corresponding classes [4]. Similarly, music can be tagged with more than one genre. Such problems are known as multilabel classiÔ¨Åcation problems. Formally, multilabel classiÔ¨Åcation problems can be deÔ¨Åned as follows. Let xibe a datapoint from a ddimensional input space Xof real and/or categorical attributes. Also, let the set of all possible labels for a speciÔ¨Åc multilabel classi Ô¨Åcation problem be L=f1;2;:::;qg, from which a subset of labels, LiL, is applicable to the datapoint xi. Here labels inLiare called the relevant labels, and(L Li)are called the irrelevant labels for xi. Then a typical multilabel dataset is deÔ¨Åned as D=f(xi;yi)j1ing, wherenis the number of data points in the dataset; xi=fxi1;xi2;:::;xidgis addimensional vector indicating theithdatapoint; and yi=fyi1;yi2;:::;yiqgis a binary vector indicating the label assignments Lifor theithdatapoint. Here yij= 1ifj2Li, that is, the jthlabel is relevant to the ithdatapoint, and yij= 0ifj=2Li.The objective of multilabel classiÔ¨Åcation is to learn a model h, which predicts the relevance of every label for a new datapoint d, written as h(d). Multilabel classiÔ¨Åcation algorithms can be categorised as either problem transformation oralgorithm adaptation methods [33]. Problem transforma tion methods‚Äîfor example classiÔ¨Åer chains [26]‚Äîbreak the multilabel prob lem down into smaller multiclass classiÔ¨Åcation problems. Algorithm adaptation methods‚Äîfor example BPMLL [39]‚Äîmodify multiclass algorithms to directly train on multilabel datasets. Ensemble classiÔ¨Åcation methods train multiple component classiÔ¨Åers and aggregate them. Generally, ensemble methods perform better than the single component classiÔ¨Åers [14] and ensemble classiÔ¨Åers based on boosting generally perform better than bagging methods [19]. In the multilabel classiÔ¨Åcation lit erature, several methods have been proposed that combine multiple multilabel models to form an ensemble. These, however, are mostly problem transforma tion methods based on baggingor majority voting based approaches to building ensembles. There are very few boosting approaches in the multilabel classiÔ¨Å cation literature. AdaBoost.MH [29] is probably the most prominent boosting approach in multilabel classiÔ¨Åcation, but does not perform well when compared to other methods [18]. As boosting based methods in multiclass classiÔ¨Åcation perform so much better than approaches based on bagging, this motivates the development of better ensembles for multilabel classiÔ¨Åcation that are based on ideas similar to boosting. Kalman Filterbased Heuristic Ensemble (KFHE) [23] is a recently proposed algorithm that frames ensemble training as a state estimation problem which is solved using a Kalman Ô¨Ålter [13, 17]. Although Kalman Ô¨Ålters are most commonly used to solve problems associated with time series data, this is not the case for KFHE. Rather, this work exploits the data fusion property of the Kalman Ô¨Ålter to combine individual multiclass component classiÔ¨Åer models to construct an ensemble. This can be interpreted as eÔ¨Äectively being in between 2boosting and bagging, and therefore is expected to beneÔ¨Åt from exploiting the advantages of both types of ensembles. By utilising the KFHE framework, this article proposes two new multilabel classiÔ¨Åcation methods, MLKFHEHOMER andMLKFHECC , which are en semble methods for multilabel classiÔ¨Åcation that combine the HOMER [34] and classiÔ¨Åer chains (CC) [26] algorithms respectively with KFHE. In addition, this paper also presents a bagged version of HOMER named EHOMER. The main contributions of this paper are: ‚Ä¢The extension of the KFHE framework to multilabel classiÔ¨Åcation, ML KFHE, and the deÔ¨Ånition of MLKFHEHOMER and MLKFHECC, two MLKFHEvariantsthatcombinemultipleHOMERandmultipleCCmod els respectively. ‚Ä¢Introduction of EHOMER, a simple bagged ensemble version of HOMER. ‚Ä¢An extensive experiment demonstrating the eÔ¨Äectiveness of using the ML KFHE method to combine multilabel classiÔ¨Åers over using bagging based multilabel ensemble algorithms. The remainder of the paper is structured as follows. Section 2 describes existing multilabel ensemble classiÔ¨Åcation algorithms, as well as the relevant aspects of CC and HOMER. Section 3 introduces the proposed MLKFHE HOMER and MLKFHECC methods, by Ô¨Årst introducing KFHE and then MLKFHE. The design of the evaluation experiments performed is described in Section 4, and the results of these experiments are discussed in detail in Section 5. Finally, Section 6 concludes the article and discusses directions for future work. 2. Related Work "
456,LA-HCN: Label-based Attention for Hierarchical Multi-label TextClassification Neural Network.txt,"Hierarchical multi-label text classification (HMTC) has been gaining
popularity in recent years thanks to its applicability to a plethora of
real-world applications. The existing HMTC algorithms largely focus on the
design of classifiers, such as the local, global, or a combination of them.
However, very few studies have focused on hierarchical feature extraction and
explore the association between the hierarchical labels and the text. In this
paper, we propose a Label-based Attention for Hierarchical Mutlti-label Text
Classification Neural Network (LA-HCN), where the novel label-based attention
module is designed to hierarchically extract important information from the
text based on the labels from different hierarchy levels. Besides, hierarchical
information is shared across levels while preserving the hierarchical
label-based information. Separate local and global document embeddings are
obtained and used to facilitate the respective local and global
classifications. In our experiments, LA-HCN outperforms other state-of-the-art
neural network-based HMTC algorithms on four public HMTC datasets. The ablation
study also demonstrates the effectiveness of the proposed label-based attention
module as well as the novel local and global embeddings and classifications. By
visualizing the learned attention (words), we find that LA-HCN is able to
extract meaningful information corresponding to the different labels which
provides explainability that may be helpful for the human analyst.","In recent years, there has been a growing interest in hierarchical multilabel classication (HMC) which can be applied in a wide range of applications such as International Patent Classication (Gomez & Moens, 2014), product annotation (Aly et al., 2019) and advertising recommendation (Agrawal et al., 2013). In the common  atclassication problem, each input sample is only associated with a single label from a set of disjoint labels. However, in HMC problem, the labels are organized in the form of a tree or a Directed Acyclic Graph(DAG) Silla & Freitas (2011) and each input sample is usually associated with multiple labels, which made it more challenging. The most straightforward approach in dealing with HMC problem is to convert it to a  at multilabel classication problem by simply ignoring the rel evance between labels (Li et al., 2018; Hu et al., 2018; Aly et al., 2019). The main disadvantage in doing so is the loss of the useful hierarchical information. Alternatively, the local approach (Koller & Sahami, 1997) is designed to perform multilabel classication, where the classications are carried out at each level of the label hierarchy (e.g., Local Classier per Parent Node, Local Classier per Level and Local Classier per Node). The overall classication results are then generated based on these local predictions. While hierarchical information can be better utilized in local approaches, misclassications are easily propagated to the next levels (Punera & Ghosh, 2008). Global approaches are proposed to learn a single global model for all labels to reduce the model size and consider the entire label hierarchy at once (Kiritchenko et al., 2005, 2006). These global classiers are typically built on  atclassiers with modications made to inte grate the hierarchical information of labels (Wang et al., 2009; Vens et al., 2008) into the model. Recently, more algorithms which combine the local and global 2approaches are proposed (Wehrmann et al., 2018; Mao et al., 2019). All algorithms introduced above only focus on the design of hierarchical clas sier while ignoring the hierarchical features which may be extracted and they are important in HMC as well. Huang et al. (2019) and Rojas et al. (2020) consider hierarchical feature extraction in their work. However, the extraction process is designed and fullled by applying the typical attention mechanism over the whole text. Since in HMC problem the text may be associated with mul tiple labels at each hierarchy level, the features extracted from typical attention may be diluted. We believe it is reasonable to hypothesize that a labelbased attention, where information extraction is performed based on dierent labels at dierent hierarchical levels, would allow the model to be more interpretable and have an overall better performance in accuracy. Given the above moti vations, we propose LAHCN | a HMTC model with a labelbased attention to facilitate labelbased hierarchical feature extraction, where we introduce the concept and mechanism of component which is an intermediate representation that helps bridge the latent association between the words and the labels for labelbased attention. Contribution. Main contributions of this work: ‚Ä¢We propose a novel HMTC model capable of learning disjoint features for each hierarchical level, while sharing the hierarchical information learned across levels. Besides, both local and global classiers with respective embeddings are applied to reduce the impact of errorpropagation across levels. ‚Ä¢We propose a novel mechanism to learn labelbased word attention at each level such that the important features of each document can be captured based on individual labels and prove that such mechanism can provide more interpretable results. ‚Ä¢We evaluate LAHCN against both classical and stateoftheart neural network HMTC baselines on four benchmark datasets and LAHCN out 3performs them on all the datasets. The ablation study shows the eec tiveness of the component mechanism and dierent classiers applied in LAHCN and demonstrates that the learned labelbased attention is able to give reasonable interpretation of the prediction results. 2. Related Work "
457,Memory-based control with recurrent neural networks.txt,"Partially observed control problems are a challenging aspect of reinforcement
learning. We extend two related, model-free algorithms for continuous control
-- deterministic policy gradient and stochastic value gradient -- to solve
partially observed domains using recurrent neural networks trained with
backpropagation through time.
  We demonstrate that this approach, coupled with long-short term memory is
able to solve a variety of physical control problems exhibiting an assortment
of memory requirements. These include the short-term integration of information
from noisy sensors and the identification of system parameters, as well as
long-term memory problems that require preserving information over many time
steps. We also demonstrate success on a combined exploration and memory problem
in the form of a simplified version of the well-known Morris water maze task.
Finally, we show that our approach can deal with high-dimensional observations
by learning directly from pixels.
  We find that recurrent deterministic and stochastic policies are able to
learn similarly good solutions to these tasks, including the water maze where
the agent must learn effective search strategies.","The use of neural networks for solving continuous control problems has a long tradition. Several recent papers successfully apply modelfree, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12]. However, all of this work assumes fully observed state. Many real world control problems are partially observed. Partial observability can arise from dif ferent sources including the need to remember information that is only temporarily available such as a way sign in a navigation task, sensor limitations or noise, unobserved variations of the plant under control (system identiÔ¨Åcation), or statealiasing due to function approximation. Partial ob servability also arises naturally in many tasks that involve control from vision: a static image of a dynamic scene provides no information about velocities, occlusions occur as a consequence of the threedimensional nature of the world, and most vision sensors are bandwidthlimited and only have a restricted Ô¨Åeldofview. Resolution of partial observability is nontrivial. Existing methods can roughly be divided into two broad classes: On the one hand there are approaches that explicitly maintain a belief state that corresponds to the distribution over the world state given the observations so far. This approach has two major disadvantages: The Ô¨Årst is the need for a model, and the second is the computational cost that is typically associated with the update of the belief state [8, 23]. 1arXiv:1512.04455v1  [cs.LG]  14 Dec 2015On the other hand there are model free approaches that learn to form memories based on interactions with the world. This is challenging since it is a priori unknown which features of the observations will be relevant later, and associations may have to be formed over many steps. For this reason, most model free approaches tend to assume the fullyobserved case. In practice, partial observability is often solved by handcrafting a solution such as providing multipleframes at each timestep to allow velocity estimation [16, 14]. In this work we investigate a natural extension of two recent, closely related policy gradient algo rithms for learning continuousaction policies to handle partially observed problems. We primarily consider the Deterministic Policy Gradient algorithm (DPG) [24], which is an offpolicy policy gradient algorithm that has recently produced promising results on a broad range of difÔ¨Åcult, high dimensional continuous control problems, including direct control from pixels [14]. DPG is an actorcritic algorithm that uses a learned approximation of the actionvalue (Q) function to obtain approximate actionvalue gradients. These are then used to update a deterministic policy via the chainrule. We also consider DPG‚Äôs stochastic counterpart, SVG(0) ([6]; SVG stands for ‚ÄúStochastic Value Gradients‚Äù) which similarly updates the policy via backpropagation of actionvalue gradients from an actionvalue critic but learns a stochastic policy. We modify both algorithms to use recurrent networks trained with backpropagation through time. We demonstrate that the resulting algorithms, Recurrent DPG (RDPG) and Recurrent SVG(0) (RSVG(0)), can be applied to a number of partially observed physical control problems with di verse memory requirements. These problems include: shortterm integration of sensor information to estimate the system state (pendulum and cartpole swingup tasks without velocity information); system identiÔ¨Åcation (cart pole swingup with variable and unknown polelength); longterm mem ory (a robot arm that needs to reach out and grab a payload to move it to the position the arm started from); as well as a simpliÔ¨Åed version of the water maze task which requires the agent to learn an exploration strategy to Ô¨Ånd a hidden platform and then remember the platform‚Äôs position in order to return to it subsequently. We also demonstrate successful control directly from pixels. Our results suggest that actorcritic algorithms that rely on bootstrapping for estimating the value function can be a viable option for learning control policies in partially observed domains. We further Ô¨Ånd that, at least in the setup considered here, there is little performance difference between stochastic and deterministic policies, despite the former being typically presumed to be preferable in partially observed domains. 2 Background We model our environment as discretetime, partiallyobserved Markov Decision process (POMDP). A POMDP is described a set of environment states Sand a set of actions A, an initial state distribu tionp0(s0), a transition function p(st+1jst;at)and reward function r(st;at). This underlying MDP is partially observed when the agent is unable to observe the state stdirectly and instead receives observations from the set Owhich are conditioned on the underlying state p(otjst). The agent only indirectly observes the underlying state of the MDP through the observations. An optimal agent may, in principle, require access to the entire history ht= (o1;a1;o2;a2;:::at 1;ot). The goal of the agent is thus to learn a policy (ht)which maps from the history to a distribution over actionsP(A)which maximizes the expected discounted reward (below we consider both stochastic and deterministic policies). For stochastic policies we want to maximise J=E""1X t=1 t 1r(st;at)# ; (1) where the trajectories = (s1;o1;a1;s2;:::)are drawn from the trajectory distribution induced by the policy:p(s1)p(o1js1)(a1jh1)p(s2js1;a1)p(o2js2)(a2jh2):::and wherehtis deÔ¨Åned as above. For deterministic policies we replace with a deterministic function which maps directly from statesSto actionsAand we replace at(jht)withat=(ht). In the algorithms below we make use of the actionvalue function Q. For a fully observed MDP, when we have access to s, the actionvalue function is deÔ¨Åned as the expected future discounted reward when in state stthe agent takes action atand thereafter follows policy . Since we are 2interested in the partially observed case where the agent does not have access to swe instead deÔ¨Åne Qin terms ofh: Q(ht;at) =Estjht[rt(st;at)] +E>tjht;at""1X i=1 ir(st+i;at+i)# (2) where>t= (st+1;ot+1;at+1:::)is the future trajectory and the two expectations are taken with respect to the conditionals p(stjht)andp(>tjht;at)of the trajectory distribution associated with . Note that this equivalent to deÔ¨Åning Qin terms of the belief state since his a sufÔ¨Åcient statistic. Obviously, for most POMDPs of interest, it is not tractable to condition on the entire sequence of observations. A central challenge is to learn how to summarize the past in a scalable way. 3 Algorithms 3.1 Recurrent DPG We extend the Deterministic Policy Gradient (DPG) algorithm for MDPs introduced in [24] to deal with partially observed domains and pixels. The core idea of the DPG algorithm for the fully ob served case is that for a deterministic policy with parameters , and given access to the true actionvalue function associated with the current policy Q, the policy can be updated by backprop agation: @J() @=Es"" @Q(s;a) @a a=(s)@(s) @# ; (3) where the expectation is taken with respect to the (discounted) state visitation distribution induced by the current policy [24]. Similar ideas had previously been exploited in NFQCA [4] and in the ADP [13] community. In practice the exact actionvalue function Qis replaced by an approximate (critic)Q!with parameters !that is differentiable in aand which can be learned e.g. with Q learning. In order to ensure the applicability of our approach to large observation spaces (e.g. from pixels), we use neural networks for all function approximators. These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16]. [14] proposed modiÔ¨Åcations to DPG necessary in order to learn effectively with deep neural networks which we make use of here (cf. sections 3.1.1, 3.1.2). Under partial observability the optimal policy and the associated actionvalue function are both functions of the entire preceding observationaction history ht. The primary change we introduce is the use of recurrent neural networks, rather than feedforward networks, in order to allow the network to learn to preserve (limited) information about the past which is needed in order to solve the POMDP. Thus, writing (h)andQ(h;a)rather than(s)andQ(s;a)we obtain the following policy update: @J() @=E""X t t 1@Q(ht;a) @a a=(ht)@(ht) @# ; (4) where we have written the expectation now explicitly over entire trajectories = (s1;o1;a1;s2;o2;a2;:::)which are drawn from the trajectory distribution induced by the current policy andht= (o1;a1;:::;ot 1;at 1;ot)is the observationaction trajectory preÔ¨Åx at time step t, both as introduced above1. In practice, as in the fully observed case, we replace Qby learned approximation Q!(which is also a recurrent network with parameters !). Thus, rather than di rectly conditioning on the entire observation history, we effectively train recurrent neural networks to summarize this history in their recurrent state using backpropagation through time (BPTT). For 1A discount factor  tappears implicitly in the update which is absorbed in the discounted statevisitation distribution in eq. 3. In practice we ignore this term as is often done in policy gradient implementations in practice (e.g. [26]). 3long episodes or continuing tasks it is possible to use truncated BPTT, although we do not use this here. The full algorithm is given below (Algorithm 1). RDPG is an algorithm for learning deterministic policies. As discussed in the literature [25, 20] it is possible to construct examples where deterministic policies perform poorly under partial ob servability. In RDPG the policy is conditioned on the entire history but since we are using function approximation state aliasing may still occur, especially early in learning. We therefore also inves tigate a recurrent version of the stochastic counterpart to DPG: SVG(0) [6] (DPG can be seen as the deterministic limit of SVG(0)). In addition to learning stochastic policies SVG(0) also admits onpolicy learning whereas DPG is inherently off policy (see below). Similar to DPG, SVG(0) updates the policy by backpropagation @Q=@a from the actionvalue func tion, but does so for stochastic policies. This is enabled through a ‚Äúreparameterization‚Äù (e.g. [10, 19]) of the stochastic policy: The stochastic policy is represented in terms of a Ô¨Åxed, inde pendent noise source and a parameterized deterministic function that transforms a draw from that noise source, i.e., in our case, a=(h;)with()whereis some Ô¨Åxed distribution. For instance, a Gaussian policy (ajh) =N(aj(h);2)can be reparameterized as follows: a=(h;) =(h) +whereN(j0;1). See [6] for more details. The stochastic policy is updated as follows: @J() @=E;2 4X t t 1@Q(ht;a) @a a=(ht;t)@(ht;t) @3 5; (5) withdrawn from the trajectory distribution which is conditioned on IID draws of tfromat each time step. The full algorithm is provided in the supplementary (Algorithm 2). 3.1.1 Offpolicy learning and experience replay DPG is typically used in an offpolicy setting due to the fact that the policy is deterministic but exploration is needed in order to learn the gradient of Qwith respect to the actions. Furthermore, in practice, data efÔ¨Åciency and stability can also be greatly improved by using experience replay (e.g. [4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2). Thus, during learning we store experienced trajectories in a database and then replace the expectation in eq. (4) with trajectories sampled from the database. One consequence of this is a bias in the state distribution in eqs. (3, 5) which no longer corresponds to the state distribution induced by the current policy . With function approximation this can lead to a bias in the learned policy, although this typically ignored in practice. RDPG and RSVG(0) may similarly be affected; in fact since policies (and Q) are not just a function of the state but of an entire actionobservation history (eq. 4) the bias might be more severe. One potential advantage of (R)SVG(0) in this context is that it allows onpolicy learning although we do not explore this possibility here. We found that offpolicy learning with experience replay remained effective in the partially observed case. 3.1.2 Target networks A second algorithmic feature that has been found to greatly improve the stability of neuralnetwork based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Qand of the policy each, with parameters and0, and!and!0respectively. and!are the parameters that are being updated by the algorithm; 0and!0track them with some delay and are used to compute the ‚Äútargets values‚Äù for the Qfunction update. Different authors have explored different approaches to updating 0and!0. In this work we use ‚Äúsoft updates‚Äù as in [14] (see Algorithms 1 and 2 below). 4Algorithm 1 RDPG algorithm Initialize critic network Q!(at;ht)and actor(ht)with parameters !and. Initialize target networks Q!0and0with weights !0 !,0 . Initialize replay buffer R. forepisodes = 1, M do initialize empty history h0 fort = 1, T do receive observation ot ht ht 1;at 1;ot(append observation and previous action to history) select action at=(ht) +(with: exploration noise) end for Store the sequence (o1;a1;r1:::oT;aT;rT)inR Sample a minibatch of Nepisodes (oi 1;ai 1;ri 1;:::oi T;ai T;ri T)i=1;:::;N fromR Construct histories hi t= (oi 1;ai 1;:::ai t 1;oi t) Compute target values for each sample episode (yi 1;:::yi T)using the recurrent target networks yi t=ri t+ Q!0(hi t+1;0(hi t+1)) Compute critic update (using BPTT) !=1 NTX iX t  yi t Q!(hi t;ai t)@Q!(hi t;ai t) @! Compute actor update (using BPTT) =1 NTX iX t@Q!(hi t;(hi t)) @a@(hi t) @ Update actor and critic using Adam [9] Update the target networks !0 !+ (1 )!0 0 + (1 )0 end for 4 Results We tested our algorithms on a variety of partialobserved environments, covering different types of memory problems. Videos of the learned policies for all the domains are included in our sup plementary videos2, we encourage viewing them as these may provide a better intuition for the environments. All physical control problems except the simulated water maze (section 4.3) were simulated in MuJoCo [28]. We tested both standard recurrent networks as well as LSTM networks. 4.1 Sensor integration and system identiÔ¨Åcation Physical control problems with noisy sensors are one of the paradigm examples of partiallyobserved environments. A large amount of research has focused on how to efÔ¨Åciently integrate noisy sensory information over multiple timesteps in order to derive accurate estimates of the system state, or to estimate derivatives of important properties of the system [27]. Here, we consider two simple, standard control problems often used in reinforcement learning, the underactuated pendulum and cartpole swing up. We modify these standard benchmarks tasks such that in both cases the agent receives no direct information of the velocity of any of the components, i.e. for the pendulum swingup task the observation comprises only the angle of the pendulum, and 2Video of all the learned policies is available at https://youtu.be/V4_vb1D5NNQ 5(a)  (b)Figure (1) (a) The reward curve for the partiallyobserved pendulum task. Both RDPG and RSVG(0) are able to learn policies which bring the pendulum to an upright position. (b) The reward curve for the cartpole with no velocity and varying cartpole lengths. RDPG with LSTM, is able to reliably learn a good solution for this task; a purely feedforward agent (DDPG), which will not be able to estimate velocities nor to infer the pole length, is not able to solve the problem. (a)  (b)  (c)  (d) Figure 2: Reward curves for the (a) hidden target reacher task, and (b) return to start gripper task. In both cases the RDPGagents with LSTMs are able to Ô¨Ånd good policies whereas the feedforward agents fail on the memory component. (In both cases the feedforward agents perform clearly better than random which is expected from the setup of the tasks: For instance, as can be seen in the video, the gripper without memory is still able to grab the payload and move it to a ‚Äùdefault‚Äù position.) Example frames from the 3 joint reaching task (c) and the gripper task (d). for cartpole swingup it is limited to the angle of the pole and the position of the cart. Velocity is crucial for solving the task and thus it must be estimated from the history of the system. Figure 1a shows the learning curves for pendulum swingup. Both RDPG and RSVG0 were tested on the pendulum task, and are able to learn good solutions which bring the pole to upright. For the cartpole swingup task, in addition to not providing the agent with velocity information, we also varied the length of the pole from episode to episode. The pole length is invisible to the agent and needs to be inferred from the response of the system. In this task the sensor integration problem is thus paired with the need for system identiÔ¨Åcation. As can be seen in Ô¨Ågure 1b, the RDPG agent with an LSTM network reliably solves this task every time while a simple feedforward agent (DDPG) fails entirely. RDPG with a simple RNN performs considerably less well than the LSTM agent, presumably due to relatively long episodes (T=350 steps) and the failure to backpropagate gradients effectively through the plain RNN. We found that a feedforward agent that does receive velocity information can solve the variablelength swingup task partly but does so less reliably than the recurrent agent as it is unable to identify the relevant system parameters (not shown). 4.2 Memory tasks Another type of partiallyobserved task, which has been less studied in the context of reinforcement learning, involves the need to remember explicit information over a number of steps. We constructed two tasks like this. One was a 3joint reacher which must reach for a randomly positioned target, but the position of the target is only provided to the agent in the initial observation (the entire episode is 80 timesteps). As a harder variant of this task, we constructed a 5 joint gripper which must reach for a (fullyobserved) payload from a randomized initial conÔ¨Åguration and then return the payload to the initial position of its ‚Äùhand‚Äù (T=100). Note that this is a challenging control problem even in the fully observed case. The results for both tasks are shown in Ô¨Ågure 2, RDPG agents with LSTM networks solve both tasks reliably whereas purely feedforward agents fail on the memory components of the task as can be seen in the supplemental video. 6(a) RDPG DDPGRSVG #1 #2 #300.30.60.91.2 RSVG (b)  (c)  (d)  (e) Figure 3: (a) shows the reward curve for different agents performing the water maze task. Both recurrent algorithms are capable of learning good solutions to the problem, while the nonrecurrent agent (DDPG) is not. It is particularly notable that despite learning a deterministic policy, RDPG is able Ô¨Ånd search strategies that allow it to locate the platform. (b) This shows the number of steps the agents take to reach the platform after a reset, normalized by the number of steps taken for the Ô¨Årst attempt. Note that on the 2nd and 3rd attempts the recurrent agents are able to reach the platform much more quickly, indicating they are learning to remember and recall the position of the platform. Example trajectories for the (c) RDPG, (d) RSVG(0) and (e) DDPG agents. Trajectory of the Ô¨Årst attempt is purple, second is blue and third is yellow. 4.3 Water maze The Morris water maze has been used extensively in rodents for the study of memory [3]. We tested our algorithms on a simpliÔ¨Åed version of the task. The agent moves in a 2dimensional circular space where a small region of the space is an invisible ‚Äúplatform‚Äù where the agent receives a positive reward. At the beginning of the episode the agent and platform are randomly positioned in the tank. The platform position is not visible to the agent but it ‚Äúsees‚Äù when it is on platform. The agent needs to search for and stay on the platform to receive reward by controlling its acceleration. After 5 steps on the platform the agent is reset randomly to a new position in the tank but the platform stays in place for the rest of the episode (T=200). The agent needs to remember the position of the platform to return to it quickly. It is sometimes presumed that a stochastic policy is required in order to solve problems like this, which require learning a search strategy. Although there is some variability in the results, we found that both RDPG and RSVG(0) were able to Ô¨Ånd similarly good solutions (Ô¨Ågure 3a), indicating RDPG is able to learn reasonable, deterministic search strategies. Both solutions were able to make use of memory to return to the platform more quickly after discovering it during the initial search (Ô¨Ågure 3b). A nonrecurrent agent (DDPG) is able to learn a limited search strategy but fails to exploit memory to return the platform after having been reset to a random position in the tank. 4.4 Highdimensional observations We also tested our agents, with convolutional networks, on solving tasks directly from high dimensional pixel spaces. We tested on the pendulum task (but now the agent is given only a static rendering of the pendulum at each timestep), and a twochoice reaching task, where the target dis appears after 5 frames (and the agent is not allowed to move during the Ô¨Årst 5 frames to prevent it from encoding the target position in its initial trajectory). We found that RDPG was able to learn effective policies from highdimensional observations which integrate information from multiple timesteps to estimate velocity and remember the visually queued target for the full length of the episode (in the reacher task). Figure 4 shows the results. 7(a)  (b)  (c) Figure 4: RDPG was able to learn good policies directly from highdimensional renderings for pendulum (a), and a two choice reaching task with a disappearing target (b). (c) Example frame from the reaching task. 5 Discussion 5.1 Variants In the experiments presented here, the actor and critic networks are entirely disjoint. However, par ticularly when learning deep, convolutional networks the Ô¨Ålters required in the early layers may be similar between the policy and the actor. Sharing these early layers could improve computational efÔ¨Åciency and learning speed. Similar arguments apply to the recurrent part of the network, which could be shared between the actor and the critic. Such sharing, however, can also result in instabili ties as updates to one network may unknowingly damage or shift the other network. For this reason, we have not used any sharing here, although it is a potential topic for further investigation. 5.2 Related work "
458,Virtual CNN Branching: Efficient Feature Ensemble for Person Re-Identification.txt,"In this paper we introduce an ensemble method for convolutional neural
network (CNN), called ""virtual branching,"" which can be implemented with nearly
no additional parameters and computation on top of standard CNNs. We propose
our method in the context of person re-identification (re-ID). Our CNN model
consists of shared bottom layers, followed by ""virtual"" branches, where neurons
from a block of regular convolutional and fully-connected layers are
partitioned into multiple sets. Each virtual branch is trained with different
data to specialize in different aspects, e.g., a specific body region or pose
orientation. In this way, robust ensemble representations are obtained against
human body misalignment, deformations, or variations in viewing angles, at
nearly no any additional cost. The proposed method achieves competitive
performance on multiple person re-ID benchmark datasets, including Market-1501,
CUHK03, and DukeMTMC-reID.","In person reidentication (reID), the main goal is to nd in a gallery set a matching image with the same identity as a query set. This is a challenging prob lem due to the potentially large dierence between the query and the matching image, such dierence resulting from intrinsic changes, e.g., body position, to extrinsic, e.g., viewing angles. Although a complete person reID system also involves person detection and person tracking, we use the term person reID primarily to refer to the task of person retrieval, which remains an open and heavily studied topic among the computer vision community [1]. In terms of realworld applications, automated person reID has great potential, especially within the elds of security, enabling ecient identication of human subjects from images or video data without human intervention. While motivated by this important problem, the overall framework here proposed is expected to have numerous applications, as it will be clear after fully describing it. We propose a CNN ensemble architecture in Fig. 1 for person reID, to im prove robustness to human body misalignments, deformations, or orientation ?qiang.qiu@duke.eduarXiv:1803.05872v1  [cs.CV]  15 Mar 2018Fig. 1. Proposed model architecture. We adopt the DenseNet architecture with four dense blocks. We share parameters for the rst three blocks, then partition neurons in the last block into virtual branches (purple neurons represent neurons that are selected for the rst branch). With this architecture, no additional parameters are needed. Each branch is learned using dierent data to specialize in dierent aspects (Section 3.2), e.g., according to human landmark information (shown) or pose orientation information. Triplet loss is used as the baseline loss function (Section 3.4). For the example training scheme as shown, we also use the keypoint loss, a.k.a the localizationinducing loss component (Section 3.2). variance (Fig. 2). By training several models, each specialized for a dierent aspect, e.g., a specic body region or pose orientation, we construct a nal ensemble whose predictions are more robust against body misalignment, pose deformations or variations in camera angles. Conventional approaches to ensem ble deep learning are limited by the computational expense of training multiple deep models and longer inference times during testing. Thus, to make our pro posed system usable, an ecient method for implementing ensemble networks is needed that minimizes the number of additional parameters and that can be easily parallelized with limited computational resources, e.g., low GPU memory. To enable our deep model to exhibit ensemble behavior without adding extra parameters, we introduce the method of \virtual branching."" Since lowerlevel features are often similar between deep convolutional neural networks, we con struct a pseudoensemble network by inducing variation only in the top layersof the model, while sharing the lowlevel features in the bottom layers. Whereas in conventional ensemble learning, each member of the ensemble is often treated as a standalone model with information only being shared during the nal pre diction stage, virtual branching assigns a certain subset of the neurons in the original CNN model to each branch partition. Although no physical connections between neurons are broken, we omit activations of neurons not assigned to the same branch (Fig. 1). Compared to Dropout [2], which is used primarily as a regularization technique, our proposed method xes the neurons within each branch, and trains each set of neurons using dierent training data, allowing neurons to develop more specialized features. Fig. 2. Example challenges in person reidentication: body misalignment, pose defor mation, missing parts, dierent viewing angles, and occlusion, etc. Images taken from Market1501 dataset. We illustrate the capability of the proposed ensemble architecture with two example training schemes, tailored to the person reID task. In the rst scheme, we extract keypoint locations of select human landmarks, e.g., shoulders, hips, and ankles, to generate heatmaps, which serve as labels to localize each branch to a specic body region, to improve robustness to body misalignments. In the second scheme, we train each branch using subsets of the original dataset com prised of images of people with dierent pose orientations to improve robustness to dierent viewing angles. Compared to other person reID methods, e.g., [3], [4], and [5], our method does not use a region proposal network and can easily be implemented on existing deep CNN models at little additional computation cost. Our method achieves stateoftheart performance comparable to current methods on the Market1501, CUHK03, and DukeMTMC person reID datasets. The main contributions of this paper can be summarized as follows: {We propose an ensemble deep learning method, called \virtual branching,"" that allows a network to exhibit ensemble behavior without additional pa rameters. Thus, our method does not incur signicantly increased computa tion during both training and testing.{We introduce a localizationinducing loss function component that results in improved alignment of deep features around specic body regions for improved person reID performance. {We demonstrate that the proposed method can also produce deep features that are more robust to dierent pose orientations, i.e., variations in viewing angles. 2 Related Work "
459,Prediction of speech intelligibility with DNN-based performance measures.txt,"This paper presents a speech intelligibility model based on automatic speech
recognition (ASR), combining phoneme probabilities from deep neural networks
(DNN) and a performance measure that estimates the word error rate from these
probabilities. This model does not require the clean speech reference nor the
word labels during testing as the ASR decoding step, which finds the most
likely sequence of words given phoneme posterior probabilities, is omitted. The
model is evaluated via the root-mean-squared error between the predicted and
observed speech reception thresholds from eight normal-hearing listeners. The
recognition task consists of identifying noisy words from a German matrix
sentence test. The speech material was mixed with eight noise maskers covering
different modulation types, from speech-shaped stationary noise to a
single-talker masker. The prediction performance is compared to five
established models and an ASR-model using word labels. Two combinations of
features and networks were tested. Both include temporal information either at
the feature level (amplitude modulation filterbanks and a feed-forward network)
or captured by the architecture (mel-spectrograms and a time-delay deep neural
network, TDNN). The TDNN model is on par with the DNN while reducing the number
of parameters by a factor of 37; this optimization allows parallel streams on
dedicated hearing aid hardware as a forward-pass can be computed within the
10ms of each frame. The proposed model performs almost as well as the
label-based model and produces more accurate predictions than the baseline
models.","The intelligibility of speech is crucial for our social interaction, and it is an essential measure for a diagnosis of hearing decits through speech audiometry and the optimization of speech enhancement algorithms in hearing aids or cochlear implants. Accurate models that predict speech intelligibility (SI) in the presence of dierent masking noises are desirable since they can quantify the outcome of such optimization and, therefore, reduce the requirement of SI measurements that are usually timeconsuming and costly. Several models for SI prediction have been proposed to include the signal processing strategies of the auditory system. A classic model is the speech intelligibility index (SII), which is based on the weighted signaltonoise ratio in separate frequency bands that are combined to estimate SI for longer speech segments (e.g., an utterance) (ANSI, 1997). The extended SII (ESII) (Rhebergen and Versfeld, 2005) uses the same frequency weighting and covers a shorttime analysis of SNRs in each channel, and can take into account the eect of temporal modulations to enable listeninginthedips. The shorttime objective intelligibility (STOI, Taal et al., 2011) quanties the degradation of speech signals by comparing shorttime segments of the original clean speech signal with its degraded counterpart, calculating the correlation of these segments in frequency bands. Ewert and Dau (2000) proposed to include explicit temporal modulation ltering for separate frequency channels, an approach later rened by Jrgensen et al. (2013) by using temporal segments with a variable length that depend on the modulation lter frequency, which resulted in the multi resolution speech envelope power spectrum model (mrsEPSM). Schubotz et al. (2016) compared these models in a study to determine how well they can predict the speech reception threshold (SRT), which is the signaltonoise ratio (SNR) at which 50% of words presented in an acousticallycontrolled environment are correctly recognized. In this com parison, mrsEPSM produced the best average predictions across all maskers 2that varied from stationary speechshaped noise to a single background talker in the timefrequency domain. Additionally, this benchmark provides evidence about the in uence of amplitude modulation, shortterm energy, and informational masking in speech intelligibility. Another model that uses auditory periphery modeling to predict the SI is the hearing aid speech perception index (HASPI, Kates and Arehart, 2014). It calculates the envelope and the temporal ne structure of the noisy speech signal and the clean speech reference. The coherence between the test and reference signals and the correlation of their envelopes results in the HASPI. This index can predict the SI of normalhearing listeners and hearingimpaired listeners by including hearing loss into the auditory model; however, the clean speech material is required for the prediction. An alternative modeling approach combines signal extraction based on auditory principles with pattern matching algorithms borrowed from automatic speech recognition (ASR). Barker and Cooke (2006) introduced a glimpsing model in which the abovethreshold timefrequency patches (glimpses) were used as features for a backend that combines a Gaussian mixture model (GMM) with a hidden Markov model (HMM) to produce a transcript from the input glimpses, which was compared to listener responses. Recent work by Tang et al. (2016) introduced the binaural distortion weighted glimpse proportion metric, building upon the previous glimpsing models of monaural signals ( betterear glimpsing ) and incorporated binaural masking level dierences. J urgens and Brand (2009) combined the output of a model of the internal representation of auditory perception (PEMO, Dau et al., 1997b) with the dynamic time warping algorithm, which produced accurate predictions of phoneme recognition in normalhearing listeners. A GMMHMM approach dubbed Framework for Acoustic Discrimination Experiments (FADE) was proposed by Sch adler et al. (2015). This model produces accurate SRT estimates by retraining a GMMHMM system at dierent SNRs and by selecting the model that produces the lowest SRT when using the same training and test sentences. All previously mentioned models either require separate clean and degraded speech (as in the case of STOI and HASPI) or separate speech and noise signals, either for calculating the frequencydependent SNR as for the SII, by identifying abovenoise speech glimpses as in the model by Barker and Cooke (2006), or by using identical underlying speech utterances for training and testing (FADE, Sch adler et al., 2015). 3The main focus of ASR is to nd the most probable uttered word sequence given the acoustic input. This problem can be expressed probabilistically as nding the candidate word sequence Wwith the highest probability given a set of features Xfrom the observed audio signal. ^W= argmax wP(WjX) (1) Conventionally, the posterior probability is factorized as a product of an acoustic likelihood P(XjW) and a prior model of word sequences P(W) (language model ). The acoustic likelihood can be obtained by multiplying the conditional distributions of acoustic features ( acoustic model ) with those of a phonetic sequence given a word sequence ( lexicon or pronunciation model ). In simpler terms, the acoustic model predicts the probability of the speech sound given a word sequence, and the language model computes the likelihood of that word sequence. Motivated by the success of deep learning in ASR (Hinton et al., 2012), Spille et al. (2018) proposed an ASR model that combines a deep neural network (DNN) trained to estimate isolated phoneme probabilities given the acoustic observation with an HMM to consider transition probabilities between phonemes. The predictive power of this model exceeded the four baseline models mentioned above on the dataset collected by Schubotz et al. (2016). The root meansquare error (RMSE) between measurement and prediction was 1.8 dB on average when using multicondition training and modulation features, which was considerably better than the baseline models RMSEs between 5.6 and 12.5 dB. The training and test sets applied to the model are speakerindependent, marking a step towards referencefree SI models, which could serve in assisted hearing. A use case for such a model is the constant monitoring of speech intelligibility in the current acoustic scene and identifying the optimal speech enhancement algorithm for that scene. However, the previous model uses identical noise signals for training and testing (the eect of disjunct training/testing noises in this model has not yet been explored), and it requires the correct labels of the words in the utterance used as model input. These labels are compared to the transcript produced by the ASR system from which the recognition accuracy is calculated. For online applications of speech intelligibility models, e.g., for realtime comparison and selection of hearing aid and other signal processing algorithms, both the 4identical noises and the label requirement are essential limitations. Further, it is unclear if the computational demands of the model are compatible with mobile listening devices. This paper introduces an SI prediction model that requires neither the speech reference nor the actual labels of the tested utterances. The model follows the DNNbased approach introduced by Spille et al. (2018). The resulting model is not blind concerning the noise signals, for it follows the training/testing procedure suggested by Spille et al. (2018), using identical noise signals. The primary enhancement is that we provide a method for estimating the word error rate (WER) directly from the phoneme posterior probabilities emitted by the DNN instead of computing the WER between the reference transcript and the one decoded by the HMM. The WER estimation algorithm is referred to as mean temporal distance (MTD) and was rst proposed for estimating error rates of automatic recognizers by analyzing the mean distance of phoneme probability vectors obtained from a neural network over a time interval (Hermansky et al., 2013). Intuitively, the higher the MTD value, the more representative the dierence between phonemes contained in the measured time window. In preliminary work, we studied whether MTD was applicable for estimating the WER in SI models, focusing on only four types of masker and one network architecture with comparably high computational demands (Castro Martinez et al., 2020). When considering applications for modelsintheloop in hearing aids, the computational complexity becomes quite relevant. In a previous study, we were able to show that fully connected feedforward DNNs can be used in combination with a hearing aid coprocessor (Castro Martinez et al., 2019). However, the approach is currently limited to run single models in constrained mobile systems due to the computational complexity of these network architectures. This restriction prevents a simultaneous evaluation of two or more processing strategies (e.g., signal enhancement algorithms in the hearing aid) to select the currently best enhancement. We also investigate whether the computational complexity can be reduced by replacing the fullyconnected network with a timedelay deep neural network (TDNN), recently introduced for ecient ASR (Peddinti et al., 2015). This network architecture models temporal context by introducing input dependencies from previous layers ( delays ); because the input is a concatenated sequence of time frames, these delays can be seen as the input at dierent points in time. TDNNs are particularly interesting when modeling speech intelligibility 5as they cover a broader temporal context than traditional fully connected DNNs. Our previous model included temporal context on feature level using amplitude modulation lters (Moritz et al., 2015) and at the input layer by concatenating a given number of frames before and after the current one, thus requiring additional computational resources. Given the number of concatenated features and the input layer size, the overall number of parameters in the network exploded. We explore whether TDNNs can exploit temporal dependencies in the acoustic model more eciently by taking just some specic context frames instead of all of them. Previous research has extensively compared TDNNs with other architec tures for acoustic modeling, including DNNs (Peddinti et al., 2015), CNN embeddings (Rownicka et al., 2018), LSTM and similar architectures that implicitly capture temporal dependencies (Huang et al., 2019), and several other architectures using the same framework as our studies (Georgescu et al., 2019). Moreover, feature engineering combined with DNNs has been the subject of our previous studies (Castro Martinez et al., 2014; Castro Martinez and Sch adler, 2016; Castro Martinez et al., 2019). Phoneme posteriorgram Estimated WER + Matrix sentencesMaskingnoisesNormalhearing listenersTrue SRTEstimatedSRT S1S2S3Hidden Markov model TranscriptWord labelWERModulationfeaturesFully connectedDNNSpectralfeaturesTime delayneural networkMean temporal distance AEFGHBD AGHSpeech CorporaC Figure 1: Building blocks of the modeling approach: Speech intelligibility in noisy sentences is compared to the estimated SRT. To obtain this estimate, a DNN is trained as part of a standard ASR system and subsequently used to measure the degradation of phoneme representations in noise using a performance measure. From this, the WER of the ASR system is estimated, resulting in the predicted SRT. Blue letters link to the corresponding section of the paper. In summary, the main goals of this paper are to explore ASRbased models of speech intelligibility without the need for transcripts or speech reference in a wide range of maskers and to contrast traditional DNNs with more ecient TDNNs for computational eciency. 62. Methods "
460,Unified Robust Training for Graph NeuralNetworks against Label Noise.txt,"Graph neural networks (GNNs) have achieved state-of-the-art performance for
node classification on graphs. The vast majority of existing works assume that
genuine node labels are always provided for training. However, there has been
very little research effort on how to improve the robustness of GNNs in the
presence of label noise. Learning with label noise has been primarily studied
in the context of image classification, but these techniques cannot be directly
applied to graph-structured data, due to two major challenges -- label sparsity
and label dependency -- faced by learning on graphs. In this paper, we propose
a new framework, UnionNET, for learning with noisy labels on graphs under a
semi-supervised setting. Our approach provides a unified solution for robustly
training GNNs and performing label correction simultaneously. The key idea is
to perform label aggregation to estimate node-level class probability
distributions, which are used to guide sample reweighting and label correction.
Compared with existing works, UnionNET has two appealing advantages. First, it
requires no extra clean supervision, or explicit estimation of the noise
transition matrix. Second, a unified learning framework is proposed to robustly
train GNNs in an end-to-end manner. Experimental results show that our proposed
approach: (1) is effective in improving model robustness against different
types and levels of label noise; (2) yields significant improvements over
state-of-the-art baselines.","Nowadays, graphstructured data is being generated across many highimpact applications, ranging from nancial fraud detection in transaction networks to gene interaction analysis, from cyber security in computer networks to social network analysis. To ingest rich information on graph data, it is of paramount importance to learn eective node representations that encode both node at tributes and graph topology. To this end, graph neural networks (GNNs) havearXiv:2103.03414v1  [cs.LG]  5 Mar 20212 Yayong Li, Jie Yin, and Ling Chen been proposed, built upon the success of deep neural networks (DNNs) on grid structured data (e.g., images, etc.). GNNs have abilities to integrate both node attributes and graph topology by recursively aggregating node features across the graph. GNNs have achieved stateoftheart performance on many graph related tasks, such as node classication or link prediction. The core of GNNs is to learn neural network primitives that generate node representations by passing, transforming, and aggregating node features from local neighborhoods [3]. As such, nearby nodes would have similar node rep resentations [20]. By generalizing convolutional neural networks to graph data, graph convolutional networks (GCNs) [10] dene the convolution operation via a neighborhood aggregation function in the Fourier domain. The convolution of GCNs is a special form of Laplacian smoothing on graphs [11], which mixes the features of a node and its nearby neighbors. However, this smoothing operation can be disrupted when the training data is corrupted with label noise. As the training proceeds, GCNs would completely t noisy labels, resulting in degraded performance and poor generalization. Hence, one key challenge is how to improve the robustness of GNNs against label noise. Learning with noisy labels has been extensively studied on image classi cation. Label noise naturally stems from interobserver variability, human an notator's error, and errors in crowdsourced annotations [9]. Existing methods attempt to correct the loss function by directly estimating a noise transition matrix [15,19], or by adding extra layers to model the noise transition ma trix [17,4]. However, it is dicult to accurately estimate the noise transition matrix particularly with a large number of classes. Alternative methods such as MentorNet [8] and Coteaching [6] seek to separate clean samples from noisy samples, and use only the most likely clean samples to update model training. Other methods [2,16] reweight each sample in the gradient update of the loss function, according to model's predicted probabilities. However, they require a large number of labeled samples or an extra clean set for training. Otherwise, reweighting would be unreliable and result in poor performance. The aforementioned learning techniques, however, cannot be directly applied to tackle label noise on graphs. This is attributed to two signicant challenges. (1) Label sparsity : graphs with interconnected nodes are arguably harder to label than individual images. Very often, graphs are sparsely labeled, with only a small set of labeled nodes provided for training. Hence, we cannot simply drop \bad nodes"" with corrupted labels like previous methods using \smallloss trick"" [6,8]. (2)Label dependency : graph nodes exhibit strong label dependency, so nodes with high structural proximity (directly or indirectly connected) tend to have a similar label. This presses a strong need to fully exploit graph topology and sparse node labels when training a robust model against label noise. To tackle these challenges, we propose a novel approach for robustly learning GNN models against noisy labels under semisupervised settings. Our approach provides a uni ed ro bust train ing framework for graph neural net works (Union NET) that performs sample reweighting and label correction simulatenously. The core idea is twofold: (1) leverage random walks to perform label aggregationUnied Robust Training for Graph Neural Networks against Label Noise 3 among nodes with structural proximity. (2) estimate nodelevel class distribu tion to guide sample reweighting and label correction. Intuitively, noisy labels could cause disordered predictions around context nodes, thus its derived node class distribution could in turn re ect the reliability of given labels. This pro vides an eective way to assess the reliability of given labels, guided by which sample reweighting and label correction are expected to weaken unreliable su pervision and encourage label smoothing around context nodes. We verify the eectiveness of our proposed approach through experiments and ablation studies on realworld networks, demonstrating its superiority over competitive baselines. 2 Related Work "
461,Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images.txt,"In the last few years, image denoising has benefited a lot from the fast
development of neural networks. However, the requirement of large amounts of
noisy-clean image pairs for supervision limits the wide use of these models.
Although there have been a few attempts in training an image denoising model
with only single noisy images, existing self-supervised denoising approaches
suffer from inefficient network training, loss of useful information, or
dependence on noise modeling. In this paper, we present a very simple yet
effective method named Neighbor2Neighbor to train an effective image denoising
model with only noisy images. Firstly, a random neighbor sub-sampler is
proposed for the generation of training image pairs. In detail, input and
target used to train a network are images sub-sampled from the same noisy
image, satisfying the requirement that paired pixels of paired images are
neighbors and have very similar appearance with each other. Secondly, a
denoising network is trained on sub-sampled training pairs generated in the
first stage, with a proposed regularizer as additional loss for better
performance. The proposed Neighbor2Neighbor framework is able to enjoy the
progress of state-of-the-art supervised denoising networks in network
architecture design. Moreover, it avoids heavy dependence on the assumption of
the noise distribution. We explain our approach from a theoretical perspective
and further validate it through extensive experiments, including synthetic
experiments with different noise distributions in sRGB space and real-world
experiments on a denoising benchmark dataset in raw-RGB space.","Image denoising is a lowlevel vision task that is funda mental in computer vision, since noise contamination de grades the visual quality of collected images and may ad versely affect subsequent image analysis and processing tasks, such as classiÔ¨Åcation and semantic segmentation [18]. *The work was done in Noah‚Äôs Ark Lab, Huawei Technologies. ‚Ä†Corresponding authorTraditional image denoising methods such as BM3D [7], NLM [4], and WNNM [11], use local or nonlocal struc tures of an input noisy image. These methods are non learningbased without the need for groundtruth images. Recently, convolutional neural networks (CNNs) provide us with powerful tools for image denoising. Numerous CNN based image denoisers, e.g., DnCNN [34], UNet [26], RED [20], MemNet [28], and SGN [10], have superior perfor mance over traditional denoisers. However, CNNbased de noisers depend heavily on a large number of noisyclean image pairs for training. Unfortunately, collecting large amounts of aligned pairwise noisyclean training data is extremely challenging and expensive in realworld photog raphy. Additionally, models trained with synthetic noisy clean image pairs degrade greatly due to the domain gap between synthetic and real noise. To mitigate this problem, a series of unsupervised and selfsupervised methods that do not require any clean im ages for training are proposed. These methods require 1) training the network with multiple independent noisy ob servations per scene [17], 2) designing speciÔ¨Åc blindspot network structures to learn selfsupervised models on only single noisy images [13, 15, 30], and making further im provements by using noise models, e.g., GaussianPoisson models [15, 30], or 3) training the network with noisier noisy pairs, where the noisier image is derived from the noisy one with synthetic noise added [31, 22]. However, these requirements are not practical in realworld denois ing scenarios. Firstly, capturing multiple noisy observa tions per scene remains very challenging, especially for motion scenarios or medical imaging. Secondly, the rel atively low accuracy and heavy computational burden of blindspot networks greatly limit the application. Moreover, selfsupervised methods with noise model assumptions may work well in synthetic experiments when the noise distribu tion is known as a prior. However, these methods degrade sharply when dealing with realworld noisy images where the noise distribution remains unknown. In this work, we propose Neighbor2Neighbor, a novel selfsupervised image denoising framework that overcomesarXiv:2101.02824v3  [eess.IV]  31 Mar 2021the limitations above. Our approach consists of a train ing image pairs generation strategy based on subsampling and a selfsupervised training scheme with a regulariza tion term. SpeciÔ¨Åcally, training input and target are gen erated by random neighbor subsamplers, where two sub sampled paired images are extracted from a single noisy image with each element on the same position of the two images being neighbors in the original noisy image. In this way, if we assume that noise with each pixel is indepen dent conditioned on its pixel value and there is no corre lation between noise in different positions, then these two subsampled paired noisy images are independent given the groundtruth of the original noisy image. Accordingly, in spired by Noise2Noise [17], we use the above training pairs to train a denoising network. Besides, we develop a reg ularization term to address the essential difference of pixel groundtruth values between neighbors on the original noisy image. The proposed selfsupervised framework aims at training denoising networks with only single images avail able, without any modiÔ¨Åcations to the network structure. Any network that performs well in supervised image de noising tasks can be used in our framework. Moreover, our method does not depend on any noise models either. To evaluate the proposed Neighbor2Neighbor, a series of experiments on both synthetic and realworld noisy images are conducted. The extensive experiments show that our Neighbor2Neighbor outperforms traditional denoisers and existing selfsupervised denoising methods learned from only single noisy images. The results demonstrate the ef fectiveness and superiority of the proposed method. The main contributions of our paper are as follows: 1. We propose a novel selfsupervised framework for im age denoising, in which any existing denoising net works can be trained without any clean targets, net work modiÔ¨Åcations, or noise model assumptions. 2. From the theoretical perspective, we provide a sound motivation for the proposed framework. 3. Our method performs very favorably against stateof theart selfsupervised denoising methods especially on realworld datasets, which shows its potential ap plications in realworld scenarios. 2. Related Work "
462,Distilling Effective Supervision from Severe Label Noise.txt,"Collecting large-scale data with clean labels for supervised training of
neural networks is practically challenging. Although noisy labels are usually
cheap to acquire, existing methods suffer a lot from label noise. This paper
targets at the challenge of robust training at high label noise regimes. The
key insight to achieve this goal is to wisely leverage a small trusted set to
estimate exemplar weights and pseudo labels for noisy data in order to reuse
them for supervised training. We present a holistic framework to train deep
neural networks in a way that is highly invulnerable to label noise. Our method
sets the new state of the art on various types of label noise and achieves
excellent performance on large-scale datasets with real-world label noise. For
instance, on CIFAR100 with a $40\%$ uniform noise ratio and only 10 trusted
labeled data per class, our method achieves $80.2{\pm}0.3\%$ classification
accuracy, where the error rate is only $1.4\%$ higher than a neural network
trained without label noise. Moreover, increasing the noise ratio to $80\%$,
our method still maintains a high accuracy of $75.5{\pm}0.2\%$, compared to the
previous best accuracy $48.2\%$.
  Source code available:
https://github.com/google-research/google-research/tree/master/ieg","Training deep neural networks usually requires large scale labeled data. However, the process of data labeling by humans is challenging and expensive in practice, espe cially in domains where expert annotators are needed such as medical imaging. Noisy labels are much cheaper to ac quire (e.g., by crowdsourcing, web search, etc.). Thus, a great number of methods have been proposed to improve neural network training from datasets with noisy labels to take advantage of the cheap labeling practices [48]. How ever, deep neural networks have high capacity for memo rization. When noisy labels become prominent, deep neural networks inevitably overÔ¨Åt noisy labeled data [46, 37]. To overcome this problem, we argue that building the dataset wisely is necessary. Most methods consider the set ting where the entire training dataset is acquired with the 1Source code available: https://github.com/ googleresearch/googleresearch/tree/master/ieg 0.0 0.2 0.4 0.6 0.8 1.0 Noise ratio55606570758085Accuracy (%) Fullysupervised Semisupervised (1000 labels) Noiserobust (Prev. best) Noiserobust (Ours)Ratio 0.85 0.9 0.93 0.95 0.96 0.98 0.99 mean 74.7 70.9 68.8 64.8 62.6 58.4 54.4 Figure 1: Image classiÔ¨Åcation results on CIFAR100. Fully supervised denotes a model trained with all data without label noise. Noiserobust (prev. best) denotes the previ ous best results for noisy labels (50 trusted data per class are used by this method). 10 trusted data per class are available forSemisupervised andNoiserobust (ours) . The bot tom table provides the accuracy of settings over 80% noise ratios. Semisupervised is our improved version of Mix Match [4]. Our method outperforms Semisupervised at up to a 95% noise ratio. The bottom table shows mean ac curacy of three runs. See Section 5.4 for more details. same labeling quality. However, it is often practically fea sible to construct a small dataset with humanveriÔ¨Åed la bels, in addition to a largescale noisy training dataset. If the methods based on this setting can demonstrate high ro bustness to noisy labels, new horizons can be opened in data labeling practices [21, 42]. There are a few recent methods that demonstrate good performance by leverag ing a small trusted dataset while training on a large noisy dataset, including learning weights of training data [17, 33], loss correction [16], and knowledge graph [25]. However, these methods either require a substantially large trusted set or become ineffective at high noise regimes. In contrast, our method maintains superior performance with remark 1arXiv:1910.00701v5  [cs.LG]  12 Jun 2020ably smaller size of the trusted set (e.g., the previous best method [17] uses up to 10% of the total training data while our method achieves superior results with as low as 0.2%). Given a small trusted dataset and large noisy dataset, there are two common machine learning approaches to train neural networks. The Ô¨Årst is noiserobust training, which needs to handle label noise effects as well as distill correct supervision from the large noisy dataset. Considering the possible harmful effects from label noise, the second ap proach is semisupervised learning, which discards noisy labels and treats the noisy dataset as a largescale unlabeled dataset. In Figure 1, we compare methods of the two direc tions under such setting. We can observe that the advanced noiserobust method is inferior to semisupervised meth ods even with a 50% noise ratio (i.e., they cannot utilize the many correct labels from the other data), motivating the necessity for further investigation of noiserobust training. This also raises a practically interesting question: Should we discard noisy labels and opt in semisupervised training at high noise regimes for model deployment? Contributions: In response to this question, we propose a highly effective method for noiserobust training. Our method wisely takes advantage of a small trusted dataset to optimize exemplar weights and labels of mislabeled data in order to distill effective supervision from them for su pervised training. To this end, we generalize a meta re weighting framework and propose a new meta relabeling extension, which incorporates conventional pseudo labeling into meta optimization. We further utilize the probe data as anchors to reconstruct the entire noisy dataset using learned data weights and labels and thereby perform supervised training. Comprehensive experiments show that even with extremely noisy labels, our method demonstrates greatly su perior robustness compared to previous methods (Figure 1). Furthermore, our method is designed to be modelagnostic and generalizable to a variety of label noise types as val idated in experiments. Our method sets new state of the art on CIFAR10 and CIFAR100 by a signiÔ¨Åcant margin and achieves excellent performance on the largescale WebVi sion, Clothing1M, and Food101N datasets with realworld label noise. 2. Related Work "
463,DeepUSPS: Deep Robust Unsupervised Saliency Prediction With Self-Supervision.txt,"Deep neural network (DNN) based salient object detection in images based on
high-quality labels is expensive. Alternative unsupervised approaches rely on
careful selection of multiple handcrafted saliency methods to generate noisy
pseudo-ground-truth labels. In this work, we propose a two-stage mechanism for
robust unsupervised object saliency prediction, where the first stage involves
refinement of the noisy pseudo labels generated from different handcrafted
methods. Each handcrafted method is substituted by a deep network that learns
to generate the pseudo labels. These labels are refined incrementally in
multiple iterations via our proposed self-supervision technique. In the second
stage, the refined labels produced from multiple networks representing multiple
saliency methods are used to train the actual saliency detection network. We
show that this self-learning procedure outperforms all the existing
unsupervised methods over different datasets. Results are even comparable to
those of fully-supervised state-of-the-art approaches. The code is available at
https://tinyurl.com/wtlhgo3 .","Object saliency prediction aims at Ô¨Ånding and segmenting generic objects of interest and help leverage unlabeled information contained in a scene. It can contribute to binary background/foreground segmentation, image caption generation (Show, 2015), semantic segmentation (Long et al., 2015), or object removal in scene editing (Shetty et al., 2018). In semantic segmentation, for example, the network trained on a Ô¨Åxed set of class labels can only identify objects belonging to these classes, while object saliency detection can highlight an unknown object (e.g., ""bear"" crossing a street). Existing techniques on the saliency prediction task primarily fall under supervised and unsupervised settings. The line of work of supervised approaches (Hou et al., 2017; Luo et al., 2017; Zhang et al., 2017b,c; Wang et al., 2017; Li et al., 2016; Wang et al., 2016; Zhao et al., 2015; Jiang et al., 2013b; Zhu et al., 2014) however, requires largescale clean and pixellevel humanannotated datasets, which are expensive and timeconsuming to acquire. Unsupervised saliency methods do not require any human annotations and can work in the wild on arbitrary datasets. These unsupervised methods are further categorized into traditional handcrafted salient object detectors (Jiang et al., 2013b; Zhu et al., 2014; Li et al., 2013; Jiang et al., 2013a; Zou & Komodakis, 2015) and DNNbased detectors (Zhang et al., 2018, 2017a). These traditional methods are based on speciÔ¨Åc priors, such as center priors (Goferman et al., 2011), global contrast prior (Cheng et al., 2014), and background connectivity assumption (Zhu et al., 2014). Despite their simplicity, these methods perform poorly due to the limited coverage of the handpicked priors. DNNbased approaches leverage the noisy pseudolabel outputs of multiple traditional handcrafted saliency models to provide a supervisory signal for training the saliency prediction network. Zhang et al. (2017a) proposes a method (SBF, ‚ÄôSupervision by fusion‚Äô) to fuse multiple saliency models to remove noise from the pseudogroundtruth labels. This method updates the pseudolabels with the predictions of the saliency detection network and yields very noisy saliency predictions, as shown in Fig. 7c. A slightly different approach (USD, ‚ÄôDeep unsupervised saliency detection‚Äô) is taken by Zhang et al. (2018) and introduces an explicit noise modeling module to capture the noise in pseudolabels of different handcrafted methods. The joint optimization, along with the noise module, enables the learning of the saliencyprediction network to generate the pseudonoisefree outputs. It does so by Ô¨Åtting different noise estimates on predicted saliency map, based on different noisy pseudogroundtruth labels. This method produces smooth predictions of salient objects, as seen in Fig. 7c since it employs a noise modeling module to counteract the inÔ¨Çuence of noise in pseudogroundtruth labels from handcrafted saliency models. Both DNNbased methods SBF and USD performs direct pseudo labels fusion on the noisy outputs of handcrafted methods. This implies that the poorquality pseudolabels are directly used for training saliency network. Hence, the Ô¨Ånal performance of the network primarily depends upon the quality of chosen handcrafted methods. On the contrary, a better way is to reÔ¨Åne the poor pseudolabels in isolation in order to maximize the strength of each method. The Ô¨Ånal pseudolabels fusion step 2Figure 3: Overview of the sequence of steps involved in our pipeline. Firstly, the training images are processed through different handcrafted methods to generate coarse pseudolabels. In the second step, which we refer to as interimages consistency , a deep network is learned from the training images and coarse pseudolabels to generate consistent label outputs, as shown in Fig. 2. In the next step, the label outputs are further reÔ¨Åned with our selfsupervision technique in an iterate manner. Lastly, the reÔ¨Åned labels from different handcrafted methods are fused for training the saliency prediction network. Details of the individual components in the pipeline are depicted in Fig. 4. to train a network should be performed on a set of diverse and highquality, reÔ¨Åned pseudolabels instead. More concretely, we propose a systematic curriculum to incrementally reÔ¨Åne the pseudolabels by substituting each handcrafted method with a deep neural network. The handcrafted methods operate on single image priors and do not infer highlevel information such as object shapes and perspective projections. Instead, we learn a function or proxy for the handcrafted saliency method that maps the raw images to pseudolabels. In other words, we train a deep network to generate the pseudolabels which beneÔ¨Åts from learning the representations across a broad set of training images and thus signiÔ¨Åcantly improve the pseudogroundtruth labels as seen in Fig. 2 (we refer this effect as interimages consistency). We further reÔ¨Åne our pseudolabels obtained after the process of interimage consistency to clear the remaining noise in the labels via the selfsupervision technique in an iterative manner. Instead of using pseudolabels from the handcrafted methods directly as Zhang et al. (2018, 2017a), we alleviate the weaknesses of each handcrafted method individually. By doing so, the diversity of pseudolabels from different methods is preserved until the Ô¨Ånal step when all reÔ¨Åned pseudolabels are fused. The large diversity reduces the overÔ¨Åtting of the network to the labels noise and results in better generalization capability. The complete schematic overview of our approach is illustrated in Fig. 3. As seen in the Ô¨Ågure, the training images are Ô¨Årst processed by different handcrafted methods to create coarse pseudolabels. In the second step, we train a deep network to predict the pseudolabels (Fig. 4a) of the corresponding handcrafted method using a imagelevel loss to enforce interimages consistency among the predic tions. As seen in Fig. 2, this step already improves the pseudolabels over handcrafted methods. In the next step, we employ an iterative selfsupervision technique (Fig. 4c) that uses historical moving averages (MV A), which acts as an ensemble of various historical models during training (Fig. 4b) to reÔ¨Åne the generated pseudolabels further incrementally. The described pipeline is performed for each handcrafted method individually. In the Ô¨Ånal step, the saliency prediction network is trained to predict the reÔ¨Åned pseudolabels obtained from multiple saliency methods using a mean imagelevel loss. Our contribution in this work is outlined as follows: we propose a novel systematic mechanism to reÔ¨Åne the pseudogroundtruth labels of handcrafted unsupervised saliency methods iteratively via selfsupervision. Our experiments show that this improved supervisory signal enhances the training process of the saliency prediction network. We show that our approach improves the saliency prediction results, outperforms previous unsupervised methods, and is comparable to supervised methods on multiple datasets. Since we use the reÔ¨Åned pseudolabels, the training behavior of the saliency prediction network largely resembles supervised training. Hence, the network has a more stable training process compared to existing unsupervised learning approaches. 2 Related work "
464,Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise.txt,"The growing importance of massive datasets used for deep learning makes
robustness to label noise a critical property for classifiers to have. Sources
of label noise include automatic labeling, non-expert labeling, and label
corruption by data poisoning adversaries. Numerous previous works assume that
no source of labels can be trusted. We relax this assumption and assume that a
small subset of the training data is trusted. This enables substantial label
corruption robustness performance gains. In addition, particularly severe label
noise can be combated by using a set of trusted data with clean labels. We
utilize trusted data by proposing a loss correction technique that utilizes
trusted examples in a data-efficient manner to mitigate the effects of label
noise on deep neural network classifiers. Across vision and natural language
processing tasks, we experiment with various label noises at several strengths,
and show that our method significantly outperforms existing methods.","Robustness to label noise is set to become an increasingly important property of supervised learning models. With the advent of deep learning, the need for more labeled data makes it inevitable that not all examples will have highquality labels. This is especially true of data sources that admit automatic label extraction, such as web crawling for images, and tasks for which highquality labels are expensive to produce, such as semantic segmentation or parsing. Additionally, label corruption may arise in data poisoning [9, 23]. Both natural and malicious label corruptions tend to sharply degrade the performance of classiÔ¨Åcation systems [29]. Most prior work on label corruption robustness assumes that all training data are potentially corrupted. However, it is usually the case that a number of trusted examples are available. Trusted data are gathered to create validation and test sets. When it is possible to curate trusted data, a small set of trusted data could be created for training. We depart from the assumption that all training data are potentially corrupted by assuming that a subset of the training is trusted. In turn we demonstrate that having some amount of trusted training data enables signiÔ¨Åcant robustness gains. To leverage the additional information from trusted labels, we propose a new loss correction and empirically verify it on a number of vision and natural language datasets with label corruption. SpeciÔ¨Åcally, we demonstrate recovery from extremely high levels of label noise, including the dire case when the untrusted data has a majority of its labels corrupted. Such severe corruption can occur in adversarial situations like data poisoning, or when the number of classes is large. In comparison to Equal contribution. 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr√©al, Canada.arXiv:1802.05300v4  [cs.LG]  28 Jan 2019loss corrections that do not employ trusted data [17], our method is signiÔ¨Åcantly more accurate in problem settings with moderate to severe label noise. Relative to a recent method which also uses trusted data [10], our method is far more dataefÔ¨Åcient and generally more accurate. These results demonstrate that systems can weather label corruption with access only to a small number of gold standard labels. Experiment code is available at https://github.com/mmazeika/glc. 2 Related Work "
465,Scene Labeling using Gated Recurrent Units with Explicit Long Range Conditioning.txt,"Recurrent neural network (RNN), as a powerful contextual dependency modeling
framework, has been widely applied to scene labeling problems. However, this
work shows that directly applying traditional RNN architectures, which unfolds
a 2D lattice grid into a sequence, is not sufficient to model structure
dependencies in images due to the ""impact vanishing"" problem. First, we give an
empirical analysis about the ""impact vanishing"" problem. Then, a new RNN unit
named Recurrent Neural Network with explicit long range conditioning (RNN-ELC)
is designed to alleviate this problem. A novel neural network architecture is
built for scene labeling tasks where one of the variants of the new RNN unit,
Gated Recurrent Unit with Explicit Long-range Conditioning (GRU-ELC), is used
to model multi scale contextual dependencies in images. We validate the use of
GRU-ELC units with state-of-the-art performance on three standard scene
labeling datasets. Comprehensive experiments demonstrate that the new GRU-ELC
unit benefits scene labeling problem a lot as it can encode longer contextual
dependencies in images more effectively than traditional RNN units.","Scene labeling is a fundamental task in computer vision. Its goal is to assign one of many predeÔ¨Åned category la Figure 1: CNNs have challenges in dealing with local tex tures in images as shown in the second row. With the help of Gated Recurrent Units, the model can make globally better prediction. However, GRUs still struggle in modeling Ô¨Åne structures in images due to the ‚Äúimpact vanishing‚Äù problem. Our GRUELC units can effectively model multi scale con textual dependencies in images and thus successfully pre serve local details in predictions, such as the windows and doors (even not annotated in ground truth) in Ô¨Årst row, and the trees and mountains in the second and third row. 1arXiv:1611.07485v2  [cs.CV]  28 Mar 2017bels to each pixel in an image. It is usually formulated as a pixelwise multiclass classiÔ¨Åcation problem. Mod ern scene labeling methods rely heavily on convolutional neural networks (CNNs) [7, 30, 49, 29]. CNNs are capa ble of learning scaleinvariant discriminative features for images. These features have proven more powerful than traditional handcrafted features on many computer vision tasks [20, 35, 30]. Specially designed CNN architectures [7, 30, 1, 48, 49] have shown superior performance on scene labeling by using endtoend training. However, CNNs have challenges in dealing with local textures and Ô¨Åne structures in images and tend to over segment or undersegment objects in images. To accurately segment small components and detect object boundaries, long range contextual dependencies in images are usually desired when designing scene labeling algorithm. Many works have exploited probabilistic graphical models such as conditional random Ô¨Åelds (CRFs) [19] to capture structural dependencies in images [49, 29]. However, CRFs usually require carefully designed potentials and their exact infer ence is usually intractable. In contrast, recurrent neural net works (RNN), as another category of powerful contextual dependency modeling methods, are free from these disad vantages and can learn contextual dependencies in a data driven manner. RNNs have Ô¨Årst proven effective in modeling data de pendencies in domains such as natural language process ing and speech recognition [11, 33]. Recently, there are some attempts at applying RNNs to images [4, 44, 38, 24]. Because the spatial relationship among pixels in 2D image data is fundamentally different from the temporal relation ship in the 1D data in NLP or speech recognition, variants of RNN architectures [17, 38, 44, 42, 12, 25, 41] have been proposed to handle 2D image data, which typically involve unfolding a 2D lattice grid into a 1D sequence. The un folded 1D sequence is usually much longer than the data sequence in NLP or speech recognition. Take a feature map of size 6464for example. Its unfolded 1D sequence is of length 6464 = 4096 . One major Ô¨Çaw of applying exist ing RNN units to sequences of such a long length is that the ‚Äúimpact vanishing‚Äù problem will raise and break the spatial dependencies in images. It is wellknown that long term dependency is hard to learn in RNN units due to the gradient exploding or van ishing problems and RNN units such as Long Short Term Memory (LSTM) or Gated Recurrent Unit (GRU) can ef fectively avoid these problems. However, in this work, we empirically show that even in LSTMs or GRUs, the depen dency in a extremely long range is still hard to capture due to the ‚Äúimpact vanishing‚Äù problem. First, this paper studies the ‚Äúimpact vanishing‚Äù problem when traditional RNNs are applied to image data. Then, we generalize traditional RNN units to RNN units with Explicit Longrange Conditioning (RNNELC) to overcome this problem. SpeciÔ¨Åcally, two variants, GRUELC and LSTMELC, are designed and discussed. Compared with existing works [17, 38, 44, 42, 12, 25, 41], RNNELCs can effectively capture long range con textual dependency in images with the help of their ex plicit long range conditioning architecture. In the RNN ELC units, the present variable is explicitly conditioned on multiple contextually related but sequentially distant vari ables. Intuitively, it is adding skip connection between hid den states. Adding skip connections in RNNs to help learn long term dependency Ô¨Årst appears in [26]. Our method generalizes the idea to model more complex 2D spatial de pendencies. The RNNELC unit is applicable to both raw pixels and image features. It can be naturally integrated in CNNs, thereby enabling joint endtoend training. In order to take the beneÔ¨Åts of the new RNN units for scene labeling tasks, we build a novel scene labeling system using the GRUELC units to model long range multiscale contextual dependencies in image features. In summary, our main contributions include: 1) An em pirical study of the ‚Äúimpact vanishing‚Äù problem, which commonly exists in traditional RNN units when they are applied to images; 2) A new RNN unit with Explicit Longrange Conditioning to alleviate the ‚Äúimpact vanish ing‚Äù problem; 3) A novel scene labeling algorithm based on GRUELCs. There are a few works utilizing GRUs for scene labeling. However, we show that our GRUELC units can actually achieve stateoftheart performances in scene labeling tasks; and 4) Improved performances on several standard scene labeling datasets. 2. Related Work "
466,Digit Image Recognition Using an Ensemble of One-Versus-All Deep Network Classifiers.txt,"In multiclass deep network classifiers, the burden of classifying samples of
different classes is put on a single classifier. As the result the optimum
classification accuracy is not obtained. Also training times are large due to
running the CNN training on single CPU/GPU. However it is known that using
ensembles of classifiers increases the performance. Also, the training times
can be reduced by running each member of the ensemble on a separate processor.
Ensemble learning has been used in the past for traditional methods to a
varying extent and is a hot topic. With the advent of deep learning, ensemble
learning has been applied to the former as well. However, an area which is
unexplored and has potential is One-Versus-All (OVA) deep ensemble learning. In
this paper we explore it and show that by using OVA ensembles of deep networks,
improvements in performance of deep networks can be obtained. As shown in this
paper, the classification capability of deep networks can be further increased
by using an ensemble of binary classification (OVA) deep networks. We implement
a novel technique for the case of digit image recognition and test and evaluate
it on the same. In the proposed approach, a single OVA deep network classifier
is dedicated to each category. Subsequently, OVA deep network ensembles have
been investigated. Every network in an ensemble has been trained by an OVA
training technique using the Stochastic Gradient Descent with Momentum
Algorithm (SGDMA). For classification of a test sample, the sample is presented
to each network in the ensemble. After prediction score voting, the network
with the largest score is assumed to have classified the sample. The
experimentation has been done on the MNIST digit dataset, the USPS+ digit
dataset, and MATLAB digit image dataset. Our proposed technique outperforms the
baseline on digit image recognition for all datasets.","Despite signiÔ¨Åcant success in the area of knowledge discovery, conventional m a chine learning approaches can fail to perform well when they deal with complex data  if it is imbalanced, high dimensional, noisy, etc. This is due to the diÔ¨Éculty of these 2  techni ques in capturing various features and underlying data structure [ 1]. Hence, an  important research topic has evolved in data mining for eÔ¨Äectively building an  eÔ¨Écient knowledge discovery and mining model. Ensemble learning, being one r e search hotspot, aims at integration of fusion, modeling, and mining of data to form a  uniÔ¨Åed model. SpeciÔ¨Åcally, it extracts feature sets along with various transform a tions. Based on the features learnt, various algorithms produce weak predictions.  Then, ensemble learning fuses the information obtained from the same for kno w ledge discovery an d for better prediction with the help of adaptive voting [ 1].  Dong et. al. [ 1] presented an introduction to ensemble learning ap proaches. The  conducted research in [ 2] used weak multiclass ensembles which combine outputs of  different layer based transfer conditions in deep networks. Experiments have shown  that this reduce s the eÔ¨Äects of adverse feature transference of features in image rec ognition tasks [ 1].  The research done in [ 3] has also used weak multiclass ensembles  to reduce the cross domain error in domain adaptation for the task of sentiment ana l ysis. The work in [ 4] has designed an ensemble which uses AdaBoost [ 5] for adjus t ment of weights of source data and target data. It achieved good performance on UCI datasets for insufficient data. Subsequently more work has been done in [ 614] which  usually use ensembles of weak multiclass classifiers which do not give satisfactory results as will be discussed. One important type of ensemble which has potential and is unexplored is  OneVersus All (OVA) ensemble.   We propose to use a deep learning [15] based OVA ensemble approach for digit  image recognition. By doing this, an e ffort has been  made to incr ease the classifica tion acc uracy of deep n eural netwo rks on the classical digit image recognition task by  using them in an ensemble of binary classification deep  netw orks for the purpose  of  multiclass class ification. Handwritten digit recognition is an applied and an interes t ing research area. Many app roaches have been proposed for this, such as Convol u tional neural network based classification techniques [ 1618], reinforcement learning  [19], artificial neural networks [ 20,21], support vector m achines [22,23], etc. In spite  of the fact that many these techniques have achieved decent classification accuracy o n  larger, more complex, and realistic images   issues remain due to issues including  nonstandard writing of circle and hook patterns, e.g. in 4, 5, 7 and 9, and also image  perception issues such as skew, slant, blur, small size, etc. After applying the pr o posed approach to digit image recognition as demonstrated by the experiments, higher classification accuracy has been achieved as compared to that of conventional deep  network classifiers and other state ofart ensemble techniques  on all the datasets used .  Three  digit i mage datasets  namely the  MNIST digit image dataset [24], the USPS+  digit image dataset  [25] and the MATLAB digit image dataset  were used.  In one of  the experi ments which  used a subset of the MNIST  digit image dataset,  the accu racy of Binary  Classi fication Convolutio nal Neural Network  (BCCNN)  Ense mble  was found  to be 98.03%  which was highe r than that found a fter using a con vention al  Multiclass Con volutio nal N eural Netwo rk (MCNN) viz. 97.90%.  After struc tural  modifications  to the  ensemble, and subsequ ently t esting  it as well a s a conven tional  deep net work  on a another  subset  of MNIST,  it was found that the proposed technique   gave an accur acy of 98.50%  which  was higher  than that  of the  conventio nal deep  netwo rk viz. 98.4 875%.  3  The rest of the paper is structured as follows. Related works are discussed in Sec tion 2. Section 3  discusses the proposed a pproach. In Section 4 , experimentation d e tails are discussed. We conclude in Section 5 .    2. Related Works   "
467,Towards Harnessing Feature Embedding for Robust Learning with Noisy Labels.txt,"The memorization effect of deep neural networks (DNNs) plays a pivotal role
in recent label noise learning methods. To exploit this effect, the model
prediction-based methods have been widely adopted, which aim to exploit the
outputs of DNNs in the early stage of learning to correct noisy labels.
However, we observe that the model will make mistakes during label prediction,
resulting in unsatisfactory performance. By contrast, the produced features in
the early stage of learning show better robustness. Inspired by this
observation, in this paper, we propose a novel feature embedding-based method
for deep learning with label noise, termed LabEl NoiseDilution (LEND). To be
specific, we first compute a similarity matrix based on current embedded
features to capture the local structure of training data. Then, the noisy
supervision signals carried by mislabeled data are overwhelmed by nearby
correctly labeled ones (\textit{i.e.}, label noise dilution), of which the
effectiveness is guaranteed by the inherent robustness of feature embedding.
Finally, the training data with diluted labels are further used to train a
robust classifier. Empirically, we conduct extensive experiments on both
synthetic and real-world noisy datasets by comparing our LEND with several
representative robust learning approaches. The results verify the effectiveness
of our LEND.","The philosophy of recent success in deep learning mainly stems from massive highquality labeled data, leading to impressive performance in countless areas, including computer vision [22, 14], natural language processing [38, 7], speech recognition [15, 33], etc. However, the data in real world applications are often associated with label noise, due to human fatigue [13], knowledge limitation [10], or measurement error [35]. These noisy labels might degrade the performance of deep neural networks (DNNs) [1, 49], which raises a great demand for label noiserobust learning algorithms. Therefore, deep learning with label noise has been intensively studied due to its wide applications [25, 27, 23]. Previous results [1] suggest that DNNs /f_irst learn from examples with correct labels, and the noisy data will be /f_itted later. It is wellknown as the memorization eÔ¨Äect of deep learning. In spired by this remarkable /f_inding, a large group of previous works propose to employ the model predictions in the early stage of training to boost the robust learning. For model predictionbased robust methods, existing works can be generally attributed into two categories, namely label correctionbased methods and sample selectionbased methods. Label correctionbased methods [24, 43, 47, 36, 29] take the model predictions of training data as additional supervision signals to correct the potential noisy labels for guiding DNNs‚Äô training. Sample selectionbased meth ods [19, 11, 20, 12, 45] select trustworthy examples with the loss value smaller than prede/f_ined thresholds during training to mitigate the negative eÔ¨Äect of noisy labels. However, in practice, we observe that the model will make mistakes during label prediction and may be unstable in the output [16], resulting in unsatisfactory performance. Fig. 1 provides us a piece of evidence, from which we can see that the model still makes errorprone predictions even if the memorization eÔ¨Äect exists, while the embedded features remain robust. To be speci/f_ic, we have the following three observations: 1) The modelpredicted labels in Fig. 1 (b) are much more reliable than the noisy ones in Fig. 1 (c); 2) Under 30 epochs of training, the model still makes errorprone predictions for some training examples, especially for the cyan, red, and lime points in Fig. 1 (b); 3) The embedded features in the early stage of learning contain strong semantic information, as the data points with the same groundtruth labels are clustered together in Fig. 1 (a). It also indicates that the embedded features induced by the memorization eÔ¨Äect are more robust than model predictions. The reason is that the classi/f_ier output following a neural net work tends to /f_it the noise, while the embedded features are less negatively aÔ¨Äected by the noise [4]. Fig. 4 further provides us an empirical validation, where the accuracy of modelpredicted la bels measures the robustness of model predictions and the accuracy of diluted labels reveals the robustness of the feature embedding. While lots of previous works focus on developing robust models based on model predictions, scarce attention has been paid to the feature embedding. The above observations may provide us a new research insight for deep robust learning with noisy labels by leveraging the intrinsic feature embedding. In this paper, we claim that the embedded features induced by the memorization eÔ¨Äect are more robust than the modelpredicted labels. To instantiate such insight, we further propose a simple yet eÔ¨Äective feature embeddingbased label noise method, termed LabElNoiseDilution (LEND). Therein, we /f_irst compute a similarity matrix based on current embedded features to 2(a) Colored with groundtruth labels       (b) Colored with modelpredicted labels (c) Colored with original noisy labelsFigure 1: The tSNE visualization of training data under 30 epochs when training under asym metric label noise (noise rate: 0.45) on CIFAR10 dataset. ResNet18 is directly used to /f_it training data and the embedded features of the last hidden layer of the neural network are visualized by the tSNE method. Sub/f_igure (a) is colored with ground truth labels, (b) is colored with model predicted labels, and (c) is colored with original noisy training labels. capture the local structure of training data. Then, with the help of the trustworthy feature em bedding, the noisy supervision signals carried by mislabeled data are overwhelmed by nearby correctly labeled ones, Finally, the corrected labels are further employed to train a robust classi /f_ier. To be speci/f_ic, for each example, its nearby examples /f_irst make voting in deciding whether its label is trustworthy, and then this message is propagated among its neighborhoods for further noise dilution. This process is conducted on the embedding space, of which the robustness has been validated before. Finally, the diluted labels are further employed to perform sample selection to train a robust classi/f_ier. Empirically, we conduct extensive experiments on both synthetic and realworld noisy datasets by comparing our method with several representative robust learning approaches, and the results verify the superiority of our LEND. The contributions of this paper can be summarized as threefold: 1). We reveal a new /f_inding that the embedded features induced by the memorization eÔ¨Äect are more robust than the labels predicted by model, which provides us a new perspective for deep robust learning. 2). We propose a novel feature embeddingbased robust learning method, named LEND, which can make full use of the robust feature embedding in the early stage of learning. 3). We evaluate our LEND on both synthetic and realworld noisy datasets, and the experi ments well demonstrate its eÔ¨Äectiveness. The remaining of this paper is organized as follows: we /f_irst review the related works of two diÔ¨Äerent categories of label noise learning methods in Sect. 2. In Sect. 3, we provide some preliminaries of deep label noise learning and detail the memorization eÔ¨Äect of deep learning from the perspective of feature embedding. Further, we propose a simple yet eÔ¨Äective label noise 3learning method in Sect. 4 and conduct extensive experiments on both synthetic and realworld noisy datasets to verify its eÔ¨Äectiveness in Sect. 5. Finally, we conclude our paper in Sect. 6. 2 Related Work "
468,Multiplicative Reweighting for Robust Neural Network Optimization.txt,"Yet, their performance degrades in the presence of noisy labels at train
time. Inspired by the setting of learning with expert advice, where
multiplicative weights (MW) updates were recently shown to be robust to
moderate data corruptions in expert advice, we propose to use MW for
reweighting examples during neural networks optimization. We theoretically
establish the convergence of our method when used with gradient descent and
prove its advantage for label noise in 1d cases. We then validate empirically
our findings for the general case by showing that MW improves neural networks
accuracy in the presence of label noise on CIFAR-10, CIFAR-100 and Clothing1M.
We also show the impact of our approach on adversarial robustness.","Deep neural networks (DNNs) have gained massive popularity due to their success in a variety of applications. Large accurately labeled data are required to train a DNN to achieve good prediction performance. Yet, such data is costly and require a signiÔ¨Åcant amount of human attention. To reduce costs, one may train a network using annotated datasets that were created with lesser eÔ¨Äorts, but these may contain label noise which impedes the training process. We study methods to mitigate the harmful eÔ¨Äect of noise in the training process. Motivated by learning with expert advice, we employ MultiplicativeWeight (MW) updates [ 33,12] for promoting robustness in the training process. In online learning, it was shown theoretically that MW achieves optimal regret in a variety of scenarios, e.g, with losses drawn stochastically [ 42] or adversarially [ 33,12], and also in an intermediate setting with stochastic losses that are moderately corrupted by an adaptive adversary [ 3]. Thus, it is natural to consider this technique in the context of DNN training with noise, such as label noise in train data. To employ MW, we interpret the examples as experts and the predictions induced by the network as theiradvice. Wethenencounteralossfunctionoverthepredictions, whichfacilitatestheuseofMW.Thus, instead of learning by minimizing a uniform average of the losses, we propose to learn a weighted version of thelatterinwhichnotallexamplesaÔ¨Äectthetrainingprocessuniformly. WeemployaMWupdaterulefor reweighting the examples during the learning process, which we denote as multiplicative reweighting(MR). It alternates between SGD steps for optimizing the DNN‚Äôs parameters and MW updates for reweighting. ‚àóBlavatnik School of Computer Science, Tel Aviv University; nogabar@mail.tau.ac.il . ‚Ä†Blavatnik School of Computer Science, Tel Aviv University, and Google Research, Tel Aviv; tkoren@tauex.tau.ac.il . ‚Ä°School of Electrical Engineering, Tel Aviv University; raja@tauex.tau.ac.il . 1arXiv:2102.12192v3  [cs.LG]  29 Nov 20210 20 40 60 80 100 120 140 160 epoch0.00.20.40.60.81.0weighting sumnoisy clean 40% noisy 20% noisyFigure 1. Evolution of the multiplicative weights sum for clean and noisy examples with 20% and 40% artiÔ¨Åcial label noise when trained with CIFAR10. The sum of all weights is 1. Note how the weight of the noisy examples decreases throughout the training and thus they less aÔ¨Äect the network.Fig. 1 shows the weighting evolution during train ing where noisy examples have signiÔ¨Åcantly lower weightsthanthereratiointhetrainingdata. MR is simple, generic and can Ô¨Åt easily in most train ing procedures. We establish the convergence of a simpliÔ¨Åed version of MR under mild conditions and prove the eÔ¨Écacy of MR for 1d input data. Motivated by our theoretical Ô¨Åndings, we show empirically that MR is also beneÔ¨Åcial in the multidimensional case for training with noisy labels. We demonstrate its advantage using two popular DNN optimizers: SGD (with momen tum) and Adam. We evaluate our approach both on artiÔ¨Åcial label noise in CIFAR10 and CIFAR100, and real noise in Clothing1M [ 72]. We compare to common techniques: Mixup [ 78] and labelsmoothing [ 63], which are known to improve accuracy with label noise, and the state oftheart sparse regularization [ 82]. We improve performance when we combine MR with them. We further compare to [4], which also suggested an unsupervised weighting method. Since MW is known to be optimal with worstcase adversarial loss, we also tested our method for adversarial attacks. We show how MR can improve adversarial training, which is known to be one of the best approaches against adversarial attacks. SpeciÔ¨Åcally, we demonstrate the MR advantage with Free Adversarial Training [53] and TRADES [77]. Overall, we show that our method can Ô¨Åt into the deep learning optimization toolbox as an easy to use tool that can help improving network robustness in a variety of scenarios and training pipelines. Our code appears in https://github.com/NogaBar/mr_robust_optim . 2 Related Work "
469,Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery.txt,"The trend towards higher resolution remote sensing imagery facilitates a
transition from land-use classification to object-level scene understanding.
Rather than relying purely on spectral content, appearance-based image features
come into play. In this work, deep convolutional neural networks (CNNs) are
applied to semantic labelling of high-resolution remote sensing data. Recent
advances in fully convolutional networks (FCNs) are adapted to overhead data
and shown to be as effective as in other domains. A full-resolution labelling
is inferred using a deep FCN with no downsampling, obviating the need for
deconvolution or interpolation. To make better use of image features, a
pre-trained CNN is fine-tuned on remote sensing data in a hybrid network
context, resulting in superior results compared to a network trained from
scratch. The proposed approach is applied to the problem of labelling
high-resolution aerial imagery, where fine boundary detail is important. The
dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and
Potsdam benchmark data sets.","Land use classication has been a longstanding research problem in remote sensing, and has historically been applied to coarse resolution multispectral imagery (for example, LANDSAT has 30m x 30m ground sampling distance (GSD), Quickbird 2.2m GSD). More recently, highresolution aerial imagery has become available with a GSD of 510 cm, so that objects such as cars and buildings are distinguishable. Pixel labelling becomes a richer semantic representation, but is more dicult. Now instead of classifying a spectral signature averaged over a large area (one pixel to many objects), individual objects must be segmented (one object to many pixels). Spectral properties alone may not be sucient to distinguish objects (e.g. grass from trees, road from roof), and discriminative appearancebased features are needed. The negrained classication of image pixels is termed semantic labelling . For such highresolution imagery, computer vision techniques for object segmentation and semantic la belling are eminently applicable. Recently deep convolutional neural networks (CNNs) have become the dominant method for visual recognition, achieving stateoftheart results on a number of problems includ ing semantic labelling [16, 3, 17]. CNNs have also been applied to remote sensing data [25], but usually on a patch level. For classication, this involves classifying a cropped out patch of imagery ( e.g.airport, forest). In the case of semantic labelling, the aim is usually to classify the pixel at the centre of the patch, and this classication is applied to overlapping patches densely over the image, resulting in a fully labelled output. In this work we apply the recentlydeveloped fully convolutional network [17] to semantic labelling of aerial imagery, achieving higher accuracy than the patchbased approach. By exploiting the convolutional nature of the CNN, the classier can be treated like a convolutional lter both during training and classication. The result is improved accuracy and computational eciency. However the FCN produces output at a signicantly 1arXiv:1606.02585v1  [cs.CV]  8 Jun 2016lower resolution than the input imagery due to poolinganddownsampling layers in the network. This is undesirable for complete labelling of remote sensing data because the ne boundary details are important. Here a novel nodownsampling network is presented to maintain the full resolution of the imagery at every layer in the FCN. The nodownsampling approach boosts the eective training sample size and achieves higher accuracy than a downsampling FCN, especially when the downsampling factor of the network is large. For aerial imagery, a semantic labelling pixel accuracy of around 75% can be achieved simply using the spectral and elevation information at each individual pixel [25]. To achieve higher accuracy on higher resolution images, the scene appearance must be exploited using discriminative texture features. We use pretrained convolutional features derived from ImageNet data to improve overhead semantic labelling. Pre trained networks only take 3band data as input. To make use of elevation data such as a digital surface model (DSM) we propose a hybrid network that combines the pretrained image features with DSM features that are trained from scratch. The hybrid network improves the labelling accuracy on the highestresolution imagery. The novel contributions of this work are: 1. the use of fullyconvolutional networks to semantically label aerial imagery; 2. a nodownsampling approach to FCNs to preserve output resolution; 3. a hybrid FCN architecture to combine imagery with elevation data; 4. the rst published results on the ISPRS Potsdam benchmark; and 5. stateoftheart semantic labelling accuracy for highresolution aerial imagery. The remainder of the paper is arranged as follows. Related work on semantic labelling using CNNs on remote sensing data and FCNs is reviewed in Section 2. The characteristics of highresolution aerial imagery are discussed in Section 3 and the data sets used for experimentation are introduced. Section 4 explains how FCNs are applied to remote sensing data and compares the approach experimentally to patchbased training. The nodownsampling FCN is introduced in Section 5 and compared with FCNs. Section 6 shows how pretrained image features can be combined with a custom FCN with DSM input in a hybrid architecture to improve segmentation of the veryhighresolution Potsdam data. The paper concludes with Section 7. Details of the experimental parameters are listed in the Appendix. 2 Related Work "
470,Triplet-based Deep Similarity Learning for Person Re-Identification.txt,"In recent years, person re-identification (re-id) catches great attention in
both computer vision community and industry. In this paper, we propose a new
framework for person re-identification with a triplet-based deep similarity
learning using convolutional neural networks (CNNs). The network is trained
with triplet input: two of them have the same class labels and the other one is
different. It aims to learn the deep feature representation, with which the
distance within the same class is decreased, while the distance between the
different classes is increased as much as possible. Moreover, we trained the
model jointly on six different datasets, which differs from common practice -
one model is just trained on one dataset and tested also on the same one.
However, the enormous number of possible triplet data among the large number of
training samples makes the training impossible. To address this challenge, a
double-sampling scheme is proposed to generate triplets of images as effective
as possible. The proposed framework is evaluated on several benchmark datasets.
The experimental results show that, our method is effective for the task of
person re-identification and it is comparable or even outperforms the
state-of-the-art methods.","Recently, person reidentiÔ¨Åcation (reid) catches great at tention in both computer vision community and industry be cause of its potential practical applications, such as surveil lance security [31], person tracking in crosscamera scenes, and retrieval of lost children. The goal is to Ô¨Ånd a query per son among a gallery of people images [34, 11]. InÔ¨Çuenced by illumination condition, widely varying person poses, res olution, partial occlusion, etc., reid is still an open chal lenging and popular task. Since the milestone work [18], deep learning has great achievements in computer vision for different tasks,such as object recognition [26, 12], semantic segmentation [23, 3], artist style transform [10, 16], and the reid task [30, 32, 35,29, 33]. However, it is well known that, a largescale dataset (e.g. ImageNet [18], which have 1.2 million images with 1000 categories) is the prerequisite for sufÔ¨Åciently training a deep learning model [20]. It often lacks of such largescale dataset in many speciÔ¨Åc areas. But many smaller datasets are published by different research groups. Jointly training a deep learning model with all these small datasets is worth trying to alleviate the grate demand of dataset. Furthermore, a dataset which is collected by a research group doesn‚Äôt vary too much because of limited condition of collecting scene and custom of the collector. For example, the CUHK01 [21] (as shown in Fig. 1(a)) and CUHK03 [22] datasets are cap tured in a university, where most of the collected person samples are students. The iLID [36] (Fig. 1(d)) dataset is captured in an airport and the many person are taking lug gages. PRID [14] (Fig. 1(c)) is taken from street views, where crosswalks are the main actors. The image reso lution in VIPeR [13] changes violently with varying cam era views. Combing these data together make the training dataset discrepant a lot, and then the model is trained to learn more general and robust feature representation. Typical person reid framework contains two major com ponents: a feature extractor to describe each sample of the dataset and a metric to measure the distance between the the query image and the gallery images. Many existing works research these two components separately and most of them pay more attention on the Ô¨Årst one [6, 9]. After extracting the features, a standard distance measure such as L2 dis tance [8], Bhattacharyya distance (Bhat) [25], etc. is ap plied to calculate their similarity. Our framework is mainly inspired by [22, 19], which learn features and distance met ric jointly by designing a reasonable loss function. Further more, different from usual way, more than one image can be feed into the system to learn discriminative feature rep resentations simultaneously. In this paper, we propose a triplet CNN framework to learn the deep similarity representation, with which the dis tance within the same person identity is decreased and be tween different persons is increased. Six datasets are com bined together to make the training data vary widely, and 1 arXiv:1802.03254v1  [cs.CV]  9 Feb 2018(a) CUHK1  (b) 3DPES (c) PRID  (d) iLIDS Figure 1: Examples of multiple person reidentiÔ¨Åcation datasets. Each dataset has its certain trait. The green bounding box indicates the query person image and the image in a red box is the corresponding matched person in the gallery. then the framework is able to learn more general and ro bust representation for the task of person reid. While the framework is feed 3 images: two of them have the same per son identity and the 3rd is a different person, there is huge number of such possible combinations in the combined dataset, which makes the training impossible. To address this problem, we introduce a double sampling scheme for training the framework efÔ¨Åciently. An overview of the pro posed framework is depicted in Fig. 2. The main technical contributions of this paper are threefold: First, a double sampling method is proposed to address the challenge of numerous possible combinations of triplet input for training the proposed deep convolution network without loss of gen erality. Second, a triplet ranking loss function for making the distance within the same person samples smaller while the distance between the samples of different persons larger. Third, the model is jointly trained on six different datasets. 2. Related Work "
471,Person re-identification across different datasets with multi-task learning.txt,"This paper presents an approach to tackle the re-identification problem. This
is a challenging problem due to the large variation of pose, illumination or
camera view. More and more datasets are available to train machine learning
models for person re-identification. These datasets vary in conditions: cameras
numbers, camera positions, location, season, in size, i.e. number of images,
number of different identities. Finally in labeling: there are datasets
annotated with attributes while others are not. To deal with this variety of
datasets we present in this paper an approach to take information from
different datasets to build a system which performs well on all of them. Our
model is based on a Convolutional Neural Network (CNN) and trained using
multitask learning. Several losses are used to extract the different
information available in the different datasets. Our main task is learned with
a classification loss. To reduce the intra-class variation we experiment with
the center loss. Our paper ends with a performance evaluation in which we
discuss the influence of the different losses on the global re-identification
performance. We show that with our method, we are able to build a system that
performs well on different datasets and simultaneously extracts attributes. We
also show that our system outperforms recent re-identification works on two
datasets.","In many domains, such as surveillance or digital signage, being able to auto matically recognize a person across diÔ¨Äerent, nonoverlapping cam eras, without the help of a human operator is very valuable. This task is known as pe rson reidentiÔ¨Åcation and can be extremely challenging since great variat ions can oc cur between the diÔ¨Äerent cameras. Figure 1 shows two images, tak en from two diÔ¨Äerent cameras from three academic datasets: VIPeR [2], CUHK0 1 [3] and CUHK03 [4]. Variation can be large between two pictures belonging to t he same dataset such as body pose, luminosity, view angle or background. In many works,person reidentiÔ¨Åcation is based on a similarity score between a pairofimages.If the twoimagesrepresentthe same person,the similarityscore2 Matthieu Ospici, Antoine Cecchi (a) VIPeR  (b) CUHK01  (c) CUHK03 Fig.1: Three reidentiÔ¨Åcation datasets used in our work. is high. Two aspects are usually studied. The Ô¨Årst one consists in ext racting robust invariant features to represent the appearance of a per son [5,6,7]. The second is metric learning [8,9]: it consists in learning the best possible m etric to discriminate between positive and negative samples. Recently, convolutional neural networks demonstrated very hig h eÔ¨Éciency in several computer vision problems such as image segmentation [10] or object recognition [11]. Many research projects have proved that deep n eural networks are also extremely eÔ¨Écient for reidentiÔ¨Åcation [12,13,14,15]. To train such a deep neural network, large datasets are mandato ry. Re cently, reidentiÔ¨Åcation datasets largeenough to train deep mode ls haveemerged [13,4,16]. In many works [17,15], a neural network is trained on a large d ataset and then Ô¨Åne tuned on a smaller one. Consequently, for the perfor mance evalu ation, a speciÔ¨Åc Ô¨Åne tuned model is used to evaluate its correspond ing dataset. For an industrial purpose, having a single model able to perform well on many datasets is extremely important. It means that the model can han dle diÔ¨Äer ent situations, which enables deploying the same model on cameras in stalled in diÔ¨Äerent environments. ReidentiÔ¨Åcation with CNNs is usually performed using features extr acted by the neural network from identities during the training phase. At tributes, that are more highlevel features, like gender, clothes length, ha ndbag may be extremely valuable for reidentiÔ¨Åcation since such features are tr uly robust to viewangle and cameras change. Schumann et al. [15] demonstrated that using only attributes leads to low performance compared to the feature s learned by a CNN from the identities. A good approach is therefore to use a comb ination of attributes and features extracted from identities. To have a system able to make use of attributes, an access to a larg e dataset annotated with attributes is required. Nevertheless, it is diÔ¨Écult to acquire large training datafora set ofattributes sincemanual annotationsis ex tremely expen sive.Thus,only asubsetofreidentiÔ¨Åcationdatasetsis annotate dwith attributes and many of them will remain attributeless. It is therefore a problem to build a general system that performs well on several datasets and ma ke use of atPerson reidentiÔ¨Åcation across diÔ¨Äerent datasets with mul titask learning 3 tributes. To deal with the variation of size and annotation of reide ntiÔ¨Åcation datasets, we present in this paper a multitask learning approach w hich learns the reidentiÔ¨Åcation task from a combination of several datasets . Furthermore, our system is able to take advantage of attribute information in dat aset anno tated with. Two main strategies are used in the reidentiÔ¨Åcation community for t raining deep neural networks. We will describe them in more detail in the nex t section. The Ô¨Årstone[18,19,20]isbasedonsiamesenetworks,contractiveo rtriplet losses. The second one [21,14], used in our work, is based on classiÔ¨Åcation loss es. Since the last layer is a linear classiÔ¨Åer, classiÔ¨Åcation methods ensure that features are linearly separable. Consequently, the distance between featu res belonging to two diÔ¨Äerent classes increases. Nevertheless, with this approa ch, the intra class variation is not controlled. Intuitively, reducing the intraclas s variations can make the features more discriminant and then increase the re identiÔ¨Åcation performance.Inthis work,weaddonetaskinourmultitasklearnin gobjective:a task designed to force the features of same identities to be the clo sest as possible. For the implementation, we employ a method described in [1] which prop oses a loss called center loss . We then evaluate the interest of this center loss for our reidentiÔ¨Åcation system. The contributions of our work are three folds: ‚ÄìWe build a model that learns a generic representation of the person using several datasets for reidentiÔ¨Åcation (CHUK01 [3], CHUK03 [4], MAR S [16], ViPER [2], Market1501with attributes [22]) to build a system that perf orms well on all of these datasets without performing a speciÔ¨Åc Ô¨Åne tunin g. ‚ÄìWe take advantage of attributes available in some reidentiÔ¨Åcation d atasets such as hair length, top/bottom color, clothes length. We have a mu ltitask learning objective: the reidentiÔ¨Åcation task, learned from all the datasets and the attribute classiÔ¨Åcation tasks, learned from a subset of th e available datasets. ‚ÄìWe evaluate an auxiliary task designed to control the intraclass var iation of the reidentiÔ¨Åcation features. This task is based on the center los s described in [1]. 2 Related work "
472,Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification.txt,"Online multi-object tracking is a fundamental problem in time-critical video
analysis applications. A major challenge in the popular tracking-by-detection
framework is how to associate unreliable detection results with existing
tracks. In this paper, we propose to handle unreliable detection by collecting
candidates from outputs of both detection and tracking. The intuition behind
generating redundant candidates is that detection and tracks can complement
each other in different scenarios. Detection results of high confidence prevent
tracking drifts in the long term, and predictions of tracks can handle noisy
detection caused by occlusion. In order to apply optimal selection from a
considerable amount of candidates in real-time, we present a novel scoring
function based on a fully convolutional neural network, that shares most
computations on the entire image. Moreover, we adopt a deeply learned
appearance representation, which is trained on large-scale person
re-identification datasets, to improve the identification ability of our
tracker. Extensive experiments show that our tracker achieves real-time and
state-of-the-art performance on a widely used people tracking benchmark.","Tracking multiple objects in a complex scene is a challenging problem in many video analysis and multimedia applications, such as visual surveillance, sport analysis, and autonomous driving. The objective of multiobject tracking is to estimate trajectories of objects in a speciÔ¨Åc category. Here we tackle the problem of people tracking by taking advantage of person reidentiÔ¨Åcation. Multiobject tracking beneÔ¨Åts a lot from advances in ob ject detection in the past decade. The popular trackingby detection methods apply the detector on each frame, and as sociate detection across frames to generate object trajectories. Both intracategory occlusion and unreliable detection are tremendous challenges in such a tracking framework [1, 2]. Intracategory occlusion and similar appearances of objects 0.70.80.91.0 619 622 625 628Score Frame Detection Track Candidates SelectedFig. 1 : Candidate selection based on uniÔ¨Åed scores. Candi dates from detection and tracks are visualized as blue solid rectangles and red dotted rectangles, respectively. Detection and tracks can complement each other for data association. can result in ambiguities in data association. Multiple cues, including motion, shape and object appearances, are fused to mitigate this problem [3, 4]. On the other hand, detec tion results are not always reliable. Pose variation and occlu sion in crowded scenes often cause detection failures such as false positives, missing detection, and nonaccurate bound ing. Some studies proposed to handle unreliable detection in a batch mode [2, 5, 6]. These methods address detection noises by introducing information from future frames. Detec tion results in whole video frames or a temporal window are employed and linked to trajectories by solving a global op timization problem. Tracking in a batch mode is noncausal and not suitable for timecritical applications. In contrast to these works, we focus on the online multiple people tracking problem, using only the current and past frames. In order to handle unreliable detection in an online mode, our tracking framework optimally selects candidates from outputs of both detection and tracks in each frame (as shownarXiv:1809.04427v1  [cs.CV]  12 Sep 2018in Figure 1). In most of the existing trackingbydetection methods, when talking about data association, candidates to be associated with existing tracks are only made up of de tection results. Yan et al. [4] proposed to treat the tracker and object detector as two independent identities, and keep results of them as candidates. They selected candidates based on handcrafted features, e.g., color histogram, optical Ô¨Çow, and motion features. The intuition behind generating redun dant candidates is that detection and tracks can complement each other in different scenarios. On the one hand, reliable predictions from the tracker can be used for shortterm asso ciation in case of missing detection or nonaccurate bounding. On the other hand, conÔ¨Ådent detection results are essential to prevent tracks drifting to backgrounds in the long term. How to score outputs of both detection and tracks in an uniÔ¨Åed way is still an open question. Recently, deep neural networks, especially convolutional neural networks (CNN), have made great progress in the Ô¨Åeld of computer vision and multimedia. In this paper, we take full advantage of deep neural networks to tackle unreliable detec tion and intracategory occlusion. Our contribution is three fold. First, we handle unreliable detection in online tracking by combining both detection and tracking results as candi dates, and selecting optimal candidates based on deep neu ral networks. Second, we present a hierarchical data associ ation strategy, which utilizes spatial information and deeply learned person reidentiÔ¨Åcation (ReID) features. Third, we demonstrate realtime and stateoftheart performance of our tracker on a widely used people tracking benchmark. 2. RELATED WORK "
473,Improving group robustness under noisy labels using predictive uncertainty.txt,"The standard empirical risk minimization (ERM) can underperform on certain
minority groups (i.e., waterbirds in lands or landbirds in water) due to the
spurious correlation between the input and its label. Several studies have
improved the worst-group accuracy by focusing on the high-loss samples. The
hypothesis behind this is that such high-loss samples are
\textit{spurious-cue-free} (SCF) samples. However, these approaches can be
problematic since the high-loss samples may also be samples with noisy labels
in the real-world scenarios. To resolve this issue, we utilize the predictive
uncertainty of a model to improve the worst-group accuracy under noisy labels.
To motivate this, we theoretically show that the high-uncertainty samples are
the SCF samples in the binary classification problem. This theoretical result
implies that the predictive uncertainty is an adequate indicator to identify
SCF samples in a noisy label setting. Motivated from this, we propose a novel
ENtropy based Debiasing (END) framework that prevents models from learning the
spurious cues while being robust to the noisy labels. In the END framework, we
first train the \textit{identification model} to obtain the SCF samples from a
training set using its predictive uncertainty. Then, another model is trained
on the dataset augmented with an oversampled SCF set. The experimental results
show that our END framework outperforms other strong baselines on several
real-world benchmarks that consider both the noisy labels and the
spurious-cues.","The standard Empirical Risk Minimization (ERM) has shown a high error on speciÔ¨Åc groups of data although it achieves the low test error on the indistribution datasets. One of the reasons accounting for such degradation is the presence of spuriouscues . The spurious cue refers to the feature which is highly correlated with labels on certain training groups‚Äîthus, easy to learn‚Äîbut not correlated with other groups in the test set (Nagarajan et al., 2020; Wiles et al., 2022). This spuriouscue is problematic especially occurs when the model cannot classify the minority samples although the model can correctly classify the majority of the training samples using the spurious cue. In practice, deep neural networks tend to Ô¨Åt easytolearn simple statistical correlations like the spuriouscues (Geirhos et al., 2020). This problem arises in the realworld scenarios due to various factors such as an observation bias and environmental factors (Beery et al., 2018; Wiles et al., 2022). For in stance, an object detection model can predict an identical object differently simply because of the differences in the background(Ribeiro et al., 2016; Dixon et al., 2018; Xiao et al., 2020). In nutshell, there is a low accuracy problem caused by the spuriouscues being present in a certain group of data. In that sense, importance weighting (IW) is one of the classical techniques to resolve this problem. Recently, several deep learning methods related to IW (Sagawa et al., 2019; 2020; Liu et al., 2021; Nam et al., 2020) have shown a remarkable empirical success. The main idea of those IWrelated methods is to train a model with using data oversampled with hard (highloss) samples. The assumption behind such approaches is that the highloss samples are free from the spurious cues because these shortcut features generally reside mostly in the lowloss samples Geirhos et al. (2020). These authors contributed equally 1arXiv:2212.07026v1  [cs.LG]  14 Dec 2022Preprint For instance, JustTrainTwice (JTT) trains a model using an oversampled training set containing the error set generated by the identiÔ¨Åcation model . On the other hand, noisy labels are another factor of performance degradation in the realworld scenario. Noisy labels commonly occur in massivescale human annotation data, biology and chem istry data with inevitable observation noise (Lloyd et al., 2004; Ladbury & Arold, 2012; Zhang et al., 2016). In practice, the proportions of incorrectly labeled samples in the realworld humanannotated image datasets can be up to 40% (Wei et al., 2021). Moreover, the presence of noisy labels can lead to the failure of the highlossbased IW approaches, since a large value of the loss indicates not only that the sample may belong to a minority group but also that the label may be noisy (Ghosh et al., 2017). In practice, we observed that even a relatively small noise ratio (10%) can impair the high lossbased methods on the benchmarks with spuriouscues, such as Waterbirds and CelebA. This is because the high lossbased approaches tend to focus on the noisy samples without focusing on the minority group with spurious cues. Our observation motivates the principal goal of this paper: how can we better select only spurious cuefree (SCF) samples while excluding the noisy samples? As an answer to this question, we pro pose the predictive uncertaintybased sampling as an oversampling criterion, which outperforms the errorsetbased sampling. The predictive uncertainty has been used to discover the minority or unseen samples (Liang et al., 2017; Van Amersfoort et al., 2020). We utilize such uncertainty to detect the SCF samples. In practice, we train the identiÔ¨Åcation model via the noiserobust loss and the Bayesian neural network framework to obtain reliable uncertainty for the minority group samples. By doing so, the proposed identiÔ¨Åcation model is capable of properly identifying the SCF sample while preventing the noisy labels from being focused on. After training the identiÔ¨Åcation model, similar to JTT, the debiased model is trained with the SCF set oversampled dataset. Our novel framework, ENtropybased Debiasing (END), shows an impressive worstgroup accuracy on several benchmarks with various degrees of symmetric label noise. Furthermore, as a theoretical motivation, we demonstrate that the predictive uncertainty (entropy) is a proper indicator for iden tifying the SCF set regardless of the existence of the noisy labels in the simple binary classiÔ¨Åcation problem setting. To summarize, our key contributions are three folds: 1. We propose a novel predictive uncertaintybased oversampling method that effectively selects the SCF samples while minimizing the selection of noisy samples. 2. We rigorously prove that the predictive uncertainty is an appropriate indicator for identifying a SCF set in the presence of the noisy labels, which well supports the proposed method. 3. We propose additional model considerations for realworld applications in both classiÔ¨Åcation and regression tasks. The overall framework shows superior worstgroup accuracy compared to recent strong baselines in various benchmarks. 2 R ELATED WORKS "
474,Beat by Beat: Classifying Cardiac Arrhythmias with Recurrent Neural Networks.txt,"With tens of thousands of electrocardiogram (ECG) records processed by mobile
cardiac event recorders every day, heart rhythm classification algorithms are
an important tool for the continuous monitoring of patients at risk. We utilise
an annotated dataset of 12,186 single-lead ECG recordings to build a diverse
ensemble of recurrent neural networks (RNNs) that is able to distinguish
between normal sinus rhythms, atrial fibrillation, other types of arrhythmia
and signals that are too noisy to interpret. In order to ease learning over the
temporal dimension, we introduce a novel task formulation that harnesses the
natural segmentation of ECG signals into heartbeats to drastically reduce the
number of time steps per sequence. Additionally, we extend our RNNs with an
attention mechanism that enables us to reason about which heartbeats our RNNs
focus on to make their decisions. Through the use of attention, our model
maintains a high degree of interpretability, while also achieving
state-of-the-art classification performance with an average F1 score of 0.79 on
an unseen test set (n=3,658).","Cardiac arrhythmias are a heterogenous group of con ditions that is characterised by heart rhythms that do not follow a normal sinus pattern. One of the most com mon arrhythmias is atrial Ô¨Åbrillation (AF) with an age dependant population prevalence of 2.33.4%[1]. Due to the increased mortality associated with arrhythmias, re  ceiving a timely diagnosis is of paramount importance for patients [1, 2]. To diagnose cardiac arrhythmias, medical professionals typically consider a patient‚Äôs electrocard io gram (ECG) as one of the primary factors [2]. In the past, clinicians recorded these ECGs mainly using multilead clinical monitors or Holter devices. However, the recent advent of mobile cardiac event recorders has given patients the ability to remotely record short ECGs using devices with a single lead.We propose a machinelearning approach based on re current neural networks (RNNs) to differentiate between various types of heart rhythms in this more challenging set ting with just a single lead and short ECG record lengths. To ease learning of dependencies over the temporal dimen sion, we introduce a novel task formulation that harnesses the natural beatwise segmentation of ECG signals. In ad dition to utilising several heartbeat features that have be en shown to be highly discriminative in previous works, we also use stacked denoising autoencoders (SDAE) [3] to capture differences in morphological structure. Further more, we extend our RNNs with a soft attention mech anism [4‚Äì7] that enables us to reason about which ECG segments the RNNs prioritise for their decision making. 2. Methodology "
475,An Entropic Optimal Transport Loss for Learning Deep Neural Networks under Label Noise in Remote Sensing Images.txt,"Deep neural networks have established as a powerful tool for large scale
supervised classification tasks. The state-of-the-art performances of deep
neural networks are conditioned to the availability of large number of
accurately labeled samples. In practice, collecting large scale accurately
labeled datasets is a challenging and tedious task in most scenarios of remote
sensing image analysis, thus cheap surrogate procedures are employed to label
the dataset. Training deep neural networks on such datasets with inaccurate
labels easily overfits to the noisy training labels and degrades the
performance of the classification tasks drastically. To mitigate this effect,
we propose an original solution with entropic optimal transportation. It allows
to learn in an end-to-end fashion deep neural networks that are, to some
extent, robust to inaccurately labeled samples. We empirically demonstrate on
several remote sensing datasets, where both scene and pixel-based hyperspectral
images are considered for classification. Our method proves to be highly
tolerant to significant amounts of label noise and achieves favorable results
against state-of-the-art methods.","Deep learning has been applied with tremendous success on a variety of tasks in remote sensing image analysis. For instance, achievement of stateoftheart performance in scene classiÔ¨Åcation (Cheng et al., 2018; Anwer et al., 2018), pixelwise labeling of both multispectral (Huang et al., 2018; Audebert et al., 2018; Maggiori et al., 2017) and hyperspectral datasets (Zhong et al., 2018; Wang et al., 2017), object detection (Kellenberger et al., 2018) and image retrieval (Zhou et al., 2018; Ye et al., 2017; Li et al., 2018), highlights the recent success of deep learning models in remote sensing. But these phenomenal performances is highly dependant on the availability of large collection of datasets with accurate annotations (labels). If either the size of the dataset or the accuracy of the labels is not sufÔ¨Åcient (i.e, small scale datasets or inaccurate labels), the performance of the deep learning methods could suffer drastically. The former one can be addressed to some Under consideration at Computer Vision and Image Understanding. Preprint. Work in progress.arXiv:1810.01163v1  [cs.CV]  2 Oct 2018degree by data augmentation strategies, however solving the later case of inaccurate labeling is more difÔ¨Åcult. To address the large scale data requirements of deep learning methods, new datasets have been proposed recently in the remote sensing community (Zhou et al., 2018; Huang et al., 2018; Cheng et al., 2017; Kemker et al., 2017; Wang et al., 2016; Xia et al., 2017). This trend will grow con tinuously in the coming years, due for instance to the large constellation of the Earth observation satellites. One of the major challenge in collecting this new large scale data is accurate labeling of the samples. Manual expert labeling of such large collection of samples is often not feasible and not costeffective. Thus, labeling is usually performed by nonexperts through crowd sourcing (Snow et al., 2008; Haklay, 2010), keyword query through search engine in the case of images, open street maps, and outdated classiÔ¨Åcation maps (Kaiser et al., 2017). These cheap surrogate procedures allows scaling the size of labeled datasets, but at the cost of introducing label noise (i.e. inaccurately labeled samples). Even when manual experts are involved in labeling the data samples, they must be provided with sufÔ¨Åcient information; otherwise inaccurate labeling may still occur (for instance, during the Ô¨Åeld survey) (Hickey, 1996). Note that in the some applications, labeling is a subjective task (Smyth et al., 1995) that can again introduce label noise. Furthermore, the label noise could occur due to the misregistration of satellite images. Hence in general, large scale datasets might mostly contain inaccurately labeled samples or affected by label noise. In this case, when deep learning methods are employed with conventional loss functions (for instance, categorical cross en tropy, mean square error), they will not be robust to label noise, and as a result the classiÔ¨Åcation accuracy decreases signiÔ¨Åcantly (Zhang et al., 2017). This calls for robust approaches to mitigate the impact of label noise on the deep learning methods. Recently, it was shown that while training deeper neural networks, models tend to memorize the training data, and this phenomena is more severe when the dataset is affected by the label noise (Zhang et al., 2017). The impact of the label noise in the deep learning models can be partly cir cumvented by regularization techniques such as drop out layers, and weight regularization. These standard procedures make neural networks robust to some extend, but they are still prone to mem orize noisy labels for mediumtolarge noise levels. The problem of learning with noisy labels has been long studied in machine learning (Frenay and Verleysen, 2014; Brooks; Zhu and Wu, 2004; S¬¥aez et al., 2014; Hickey, 1996; Smyth et al., 1995; Natarajan et al., 2013), but still only few works have focused on neural networks. Recently, new approaches have been proposed in the computer vision and machine learning Ô¨Åelds to tackle the label noise by cleaning the noisy labels or designing robust loss functions within the deep learning framework (Jiang et al., 2018; Vahdat, 2017; Patrini et al., 2017). To mitigate the impact of label noise, one category of method relies on estimating the noise transition probability that describes the probability of ithclass label being mislabeled to the jthclass label, and use it to be robust to label noise (Vahdat, 2017; Natarajan et al., 2013; Patrini et al., 2017). Among those, some of them require a small set of clean labels to estimate the noise transition probability (Vahdat, 2017). The other category of methods proposes to use loss functions which are inherently tolerant to the label noise (Natarajan et al., 2013; van Rooyen et al., 2015; MasnadiShirazi, Hamed and Vasconcelos, 2008; Ghosh et al., 2015; Aritra et al., 2017). Though these methods provided satisfactory results, none of them consider the implicit local geometric structure of the underlying data. The primary objective of this paper is to develop a robust approach to tackle the label noise for remote sensing image analysis. The sensitiveness of deep neural networks to label noise has not been well studied in remote sensing image analysis so far as per our knowledge. Hence the Ô¨Årst contribution of this article lies in studying the robustness of deep neural networks to label noise, and also to analyse the efÔ¨Åciency of existing robust loss functions for remote sensing classiÔ¨Åcation tasks. The second contribution of this paper is to propose a novel robust solution to tackle the label noise based on optimal transportation theory (Villani, 2009). Indeed we propose to learn a deep learning model which is robust to label noise by Ô¨Åtting the model to the labelfeatures joint distribution of the dataset with respect to the entropyregularized optimal transport distance. We coin this method as CLEOT for ClassiÔ¨Åcation Loss with Entropic Optimal Transport. One major advantage of our approach compared to existing methods is that our method inherently exploits the geometric structure of the underlying data. A stochastic approximation schemes is proposed to solve the learning problem, and allows the use of our approach within deep learning frameworks. Experiments are conducted on several remote sensing aerial and hyperspectral benchmark datasets, 2and the results demonstrate that our approach is more robust (tolerant) to high level label noise than current stateoftheart methods. The remaining of the paper is organized as follows. Section 2 discusses related works, section 3 deÔ¨Ånes the label noise and describes the problem formulation, and section 4 introduces optimal transport. The proposed method is then presented in section 4.2 while experimental datasets and results are explained in section 5. We Ô¨Ånally draw some conclusions in section 6. 2 Related works "
476,Deep CNNs for Peripheral Blood Cell Classification.txt,"The application of machine learning techniques to the medical domain is
especially challenging due to the required level of precision and the
incurrence of huge risks of minute errors. Employing these techniques to a more
complex subdomain of hematological diagnosis seems quite promising, with
automatic identification of blood cell types, which can help in detection of
hematologic disorders. In this paper, we benchmark 27 popular deep
convolutional neural network architectures on the microscopic peripheral blood
cell images dataset. The dataset is publicly available, with large number of
normal peripheral blood cells acquired using the CellaVision DM96 analyzer and
identified by expert pathologists into eight different cell types. We fine-tune
the state-of-the-art image classification models pre-trained on the ImageNet
dataset for blood cell classification. We exploit data augmentation techniques
during training to avoid overfitting and achieve generalization. An ensemble of
the top performing models obtains significant improvements over past published
works, achieving the state-of-the-art results with a classification accuracy of
99.51%. Our work provides empirical baselines and benchmarks on standard
deep-learning architectures for microscopic peripheral blood cell recognition
task.","Blood carries oxygen and nutrients to living cells in dierent organs and tissues. It carries away the waste for detoxication. It transports hormones to the desired site of action to ght infections and regulates body temperature. The ability to classify blood constituents can be critical in assessing the patient's health. Plasma, which constitutes 55% of blood, is a colored liquid comprising mainly water (about 90%) and other essential substances such as proteins (albumin, clotting factors, antibodies, enzymes, and hormones), glucose, and fats. Rest 45% of blood is composed of white blood cells (WBCs/leukocytes), platelets (thrombocytes), and red blood cells (RBCs/erythrocytes), which  oat in the plasma(Fathima and Syeda, 2017; L., 2005). All these cells are associated with dierent functionalities. RBCs are responsible for transporting gases ( O2; CO 2) from lungs to tissues and maintaining systemic acid/base equilibria. Damage of red cell integrity, dened as hemolysis, has been shown to signicantly contribute to severe pathologies, including endothelial dysfunction(Kuhn et al., 2017). Based on the presence of visible granules in the microscopic view, WBCs can be clas sied into two broad categories: granulocytes and agranulocytes (nongranulocytes). Neu trophils, eosinophils, and basophils belong to the granulocytes category, while lymphocytes ‚àóEqual Contribution ¬©2021 E. Gavas & K. Olpadkar.arXiv:2110.09508v1  [cs.CV]  18 Oct 2021Gavas Olpadkar and monocytes belong to the agranulocytes category (Almezhghwi and Serte, 2020; Acevedo et al., 2019). Various types of WBCs play a role in immune response (L., 2005) and act as a defense mechanism in the body against illnesscausing agents. Immature granulocytes (IG) are underdeveloped WBCs released from the bone marrow into the blood. Except for blood from newborn children or pregnant women, the appearance of IG (promyelocytes, myelocytes, and metamyelocytes) (Acevedo et al., 2019) in the peripheral blood (PB) in dicates an earlystage response to infection, in ammation, or other stimuli of the bone marrow. Similarly, erythroblasts are nucleated immature RBCs or erythroid precursors not seen after the neonatal period. Their appearance in PB of children and adults can signify bone marrow damage, stress, malignant neoplasms, or other potentially serious diseases (Constantino and Cogionis, 2000). Platelets are anucleated cells in blood and get activated at the site of injury to form a blood clot. Besides, they play an important role in innate immunity and regulation of tumor growth and extravasations in the vessel(Holinstat, 2017). They make up less than 1% of blood volume. Usually, the typical percentages of neutrophils in the blood are 06%. Eosinophils constitute 1{3%, basophils 0{1%, lymphocytes 25{33%, and monocytes 3{10% of the leukocytes circulating in the blood(Acevedo et al., 2019). Recognition of various blood cell types can reveal anomalous blood cell populations like immature cells (IG or erythroblasts). On accurate identication, dierential blood cell count can suggest any possible abnormalities in the blood, or help diagnose an infection, in ammation, leukemia(Shaque and Tehsin, 2018; Mathur et al., 2013), or any immune system disorder. Analyzing the blood cell morphology is the outset for the diagnosis of 80% of hematological diseases(Acevedo et al., 2019). Quantitative morphological analysis can thus, help cytologists assess blood samples and conclude about the patient's blood conditions. However, the above processes are complex and timeconsuming, involving a specialist meticulously examining the blood smear under a microscope, subjecting it to human errors. For the past few years, several attempts have been made to automate these processes using image processing techniques and machine learning, making them time and costeective and substantially reducing the workload in laboratories. Convolutional neural networks (CNNs) are known to show excellent results on image recognition tasks, and hence, there has been an extensive research for applying them in the medical domain. In this paper, we explore various deep CNNs for blood cell classication task with peripheral blood cell (PBC) images dataset containing samples of eight dierent cell types: neutrophils, eosinophils, basophils, lymphocytes, monocytes, IG, erythroblasts, and platelets (Acevedo et al., 2020). Our work provides stateoftheart results on the PBC classication task without the need of manual feature extraction or designing complex and hybrid architectures. The main contributions of this paper are as follows: 1. Train and evaluate an endtoend deep learningbased classication system to recog nize eight dierent blood cell types in peripheral blood smear. 2. Explore and benchmark 27 standard deep CNN architectures for blood cell classica tion using transfer learning. 3. Exploit data augmentation and ensembling techniques to further improve the model performance. Our model achieves stateoftheart performance over previously pub lished works. 2Deep CNNs for Peripheral Blood Cell Classification 2. Related Work "
477,Investigating the Effect of Intraclass Variability in Temporal Ensembling.txt,"Temporal Ensembling is a semi-supervised approach that allows training deep
neural network models with a small number of labeled images. In this paper, we
present our preliminary study on the effect of intraclass variability on
temporal ensembling, with a focus on seed size and seed type, respectively.
Through our experiments we find that (a) there is a significant drop in
accuracy with datasets that offer high intraclass variability, (b) more seed
images offer consistently higher accuracy across the datasets, and (c) seed
type indeed has an impact on the overall efficiency, where it produces a
spectrum of accuracy both lower and higher. Additionally, based on our
experiments, we also find KMNIST to be a competitive baseline for temporal
ensembling.","Deep neural networks have seen broad applications across vision speech and language in recent times. Yet this success is contingent on acquiring a large number of labeled datasets, which is expensive and timeconsuming. Further, labeling is mostly manual, done by humans, due to its higher meticulousness. Recently, to address this concern of manual labeling variety of approaches have been designed, including SemiSupervised learning algorithms (Gordon and Hern ¬¥andezLobato, 2017; Laine and Aila, 2017; Lee, 2013), which typically proffer with higher results with a small number of labeled examples (seeds). Notable among this is the Temporal Ensembling (Laine and Aila, 2017), which uses an ensemble of the earlier outputs of a neural network as an unsupervised target label and achieved high accuracy on SVHN and CIFAR10 with just 500 and 4000 labeled samples with both naturally offering lower intraclass variances. Besides, to the best of our knowledge, there is no explicit study of temporal ensembling in the context of datasets with large intraclass variability. As such, in this work, we attempt to investigate this gap by answering the following research questions. ‚ÄìRQ1: Does intraclass variability impact the accuracy of temporal ensembling? Here the intention is to check (a) how accuracy varies and (b) if there is any unique observable behavior with temporal ensembling under different intraclass variances. The assumption here being that the intraclass variability is a spectrum with a range of low to high intraclass variation. To this end, we experiment on datasets of FashionMNIST (Xiao et al., 2017) and KMNIST (Clanuwat et al., 2018) to Ô¨Ånd that there is a sheer drop in performance when using temporal ensembling. ‚ÄìRQ2: Under settings of intraclass variability, how does seed size impact temporal ensembling?. Here we hypothesize and verify that upon increasing seed size, there is an improvement in performance. ‚ÄìRQ3: What‚Äôs the effect of seed selection on temporal ensembling? More speciÔ¨Åcally, we see if the diversity of seeds have an impact on results. The preliminary experimental results show that performance is lower with some category of seeds over others. The rest of the paper is organized as follows. In section 2, we review related research in semisupervised learning. In section 3, we introduce the temporal ensembling approach in brief. In section 4, we present dataset, experimental setup, and answer each of the research questions with analysis in section 5. Finally, we conclude in section 6 with possible implication on future works. Work performed while interning at Hitachi R&D IndiaarXiv:2008.08956v2  [cs.CV]  21 Aug 20202 Related Work "
478,Integration of Autoencoder and Functional Link Artificial Neural Network for Multi-label Classification.txt,"Multi-label (ML) classification is an actively researched topic currently,
which deals with convoluted and overlapping boundaries that arise due to
several labels being active for a particular data instance. We propose a
classifier capable of extracting underlying features and introducing
non-linearity to the data to handle the complex decision boundaries. A novel
neural network model has been developed where the input features are subjected
to two transformations adapted from multi-label functional link artificial
neural network and autoencoders. First, a functional expansion of the original
features are made using basis functions. This is followed by an
autoencoder-aided transformation and reduction on the expanded features. This
network is capable of improving separability for the multi-label data owing to
the two-layer transformation while reducing the expanded feature space to a
more manageable amount. This balances the input dimension which leads to a
better classification performance even for a limited amount of data. The
proposed network has been validated on five ML datasets which shows its
superior performance in comparison with six well-established ML classifiers.
Furthermore, a single-label variation of the proposed network has also been
formulated simultaneously and tested on four relevant datasets against three
existing classifiers to establish its effectiveness.","ClassiÔ¨Åcation is one of the most popular topics in machine learning and is widely performed by various types of supervisedlearning approac hes. When discussing about classiÔ¨Åcation, the traditional approach is indicate d by default. Here, each data instance is associated with a single class, which is thu s termed as singlelabel classiÔ¨Åcation. However, real world scenarios follow a diÔ¨Äerent track of problems where data can be annotated with multiple labels at a time. This gave rise to a subdomain known as multilabel (ML) classiÔ¨Åcation [1]. Reallife examples of multilabel data can be easily seen around us. So cial media posts using various hashtags (labels) for a single image or text , movies categorized under diÔ¨Äerent genres, and so on. In each of these c ases, a set of labels is associated with a particular instance of the dataset which de termines the classes associated with them. However, due to more classes link ed with each data, there is an increase in complexity of decision space. The class b oundaries are much more convoluted and overlapping due to the increased gen erality of ML classiÔ¨Åcation. In a way, ML classiÔ¨Åcation can be seen as a superse t of singlelabel classiÔ¨Åcation where the classiÔ¨Åers need to be able to out put several predictions at once. Keeping the complex nature of the problem in mind, there are a few ap  proaches which researchers have been using to handle ML data. Th e existing classiÔ¨Åers present in literature can be divided into few groups: data transforma tion [2, 3], problem adaptation [4, 5] and ensemble classiÔ¨Åers [3, 6, 7]. Among these, theproblemadaptationtechniquesarethemostconvenien tandarewidely used. Some benchmark problem adaptation methods include the use of support vector machines (SVM) [8], multilayer perceptron (MLP) [9], knear est neigh bours [4], decisiontrees [10] and probabilistic classiÔ¨Åers [6]. While dea ling with the complex nature of ML classiÔ¨Åcation, most of these methods han dle the data as it is. However, it is seen that neural networks (NNs) are quite ca pable of han 2dling complexities without actually creating a bulky model. They are loos ely inspired by the working of a human brain and are one of the most popu lar and widely used tool for machine learning. They inherently bring nonlinea rity by increasing the number of layers in the model which are able to follow dis parate structures in the datasets. Although lot of work has been done in s inglelabel classiÔ¨Åcation using NN, it is to be noted that comparatively very few w orks include the usage of NNs for ML classiÔ¨Åcation. In this current work, the authors have attempted to construct a novel two layer transformation network, adapted from multilabel function al link artiÔ¨Åcial neural network (MLFLANN), previously proposed by the authors in [11], and the wellknown autoencoders (AE). This adaptation of MLFLANN an d AE has beenaptlynamedastheAutoEncoderintegratedMLFLANN(AEML FLANN). This network is capable of overcoming few drawbacks faced in multila bel clas siÔ¨Åcation previously. In the Ô¨Årst layer of our network, we perform functional expansion of features inspired from MLFLANN. The main motivation t o incor porate MLFLANN is its compact structure and complexity handling ca pability which seems suitable for multilabel classiÔ¨Åcation. The input feature s are func tionally expanded to a higher dimension, thus giving rise to a decision sp ace with increased separability. This introduces nonlinearity in the data , similar to that of multilayer perceptrons. It is an attempt to improve the input space, thereby, increasing the convergence. However, the transform ed data helps to improve the multilabel feature space only to some extent. The exp ansion of input space might not always give rise to an optimal representation. There is still some scope of further transforming the data which will lead to m ore im proved performance. Also, the functional expansion leads to the increase in the input dimension, which poses a problem for multilabel data. To handle both these issues, we introduce a second feature transformationcu mreduction layer incorporating autoencoders. The second layer is created from th e encoder sec tion of an AE, which is capable of transforming the features to a com paratively reduced and improved space. Autoencoders are widely known to imp licitly ex tract features for classiÔ¨Åcation tasks, while successfully transf orming the data 3[5]. It is capable of generating a suitable representation through un supervised learning. To concisely describe the network, it can be said that, the input fea tures are functionally expanded in the Ô¨Årst layer. In the second layer, these expanded features are passed through an AE which generates a favourable representation in a reduced feature space. These reduced and transformed fea tures are then mapped to the output layer. This AEMLFLANN model is capable of go od multilabel classiÔ¨Åcation as it can handle the complex decision space be tter by transforming the data through two consecutive layers. The prop osed work has highlighted the importance of the two transformation layers, which can be ex tendedfurthertobuild deepernetworks. Thisliesoutsidethescop eofthis paper and will be explored in future. AEMLFLANN has been experimentally s hown to perform better than six benchmark methods over Ô¨Åve multilabe l datasets. As per the knowledge of the authors, the proposed neural netwo rk model does not exist in literature, hence its application has been explored from m ultilabel domain to traditional singlelabel domain as well. This singlelabel vers ion of the proposed model, named AutoEncoder integrated singlelabel F LANN (AE SLFLANN), has been separately tested on four relevant dataset s to analyse its success. The contribution of this work can be highlighted as follows. ‚Ä¢IntroducinganoveltwolayernetworkbasedonMLFLANNandAEs pecif ically for multilabel classiÔ¨Åcation. ‚Ä¢Improving separability in multilabel data by Ô¨Årst applying functional ex pansion layer, followedby additional transformationby autoencod er layer. ‚Ä¢The increased feature dimension caused by the Ô¨Årst layer is reduce d by consecutive AE in the second layer. This maintains a balance between the feature space and sample size, which leads to a good training of t he classiÔ¨Åer with limited data. ‚Ä¢Introducing the singlelabel variation of this novel twolayer netw ork. 4‚Ä¢Experimental analysis of both singlelabel and multilabel classiÔ¨Åcat ion networks with benchmark datasets. The rest of the paper is organized as follows. In Section 2 related wo rks is discussed. In Section 3 some background on multilabel classiÔ¨Åcatio n followed by the proposed model, termed as AEMLFLANN is described. In Sec tion 4, results are put and Section 5 concludes the paper. 2. Related Works "
479,Bayesian Deep Learning and a Probabilistic Perspective of Generalization.txt,"The key distinguishing property of a Bayesian approach is marginalization,
rather than using a single setting of weights. Bayesian marginalization can
particularly improve the accuracy and calibration of modern deep neural
networks, which are typically underspecified by the data, and can represent
many compelling but different solutions. We show that deep ensembles provide an
effective mechanism for approximate Bayesian marginalization, and propose a
related approach that further improves the predictive distribution by
marginalizing within basins of attraction, without significant overhead. We
also investigate the prior over functions implied by a vague distribution over
neural network weights, explaining the generalization properties of such models
from a probabilistic perspective. From this perspective, we explain results
that have been presented as mysterious and distinct to neural network
generalization, such as the ability to fit images with random labels, and show
that these results can be reproduced with Gaussian processes. We also show that
Bayesian model averaging alleviates double descent, resulting in monotonic
performance improvements with increased flexibility. Finally, we provide a
Bayesian perspective on tempering for calibrating predictive distributions.","Imagine Ô¨Åtting the airline passenger data in Figure 1. Which model would you choose: (1) f1(x) =w0+w1x, (2) f2(x) =P3 j=0wjxj, or (3)f3(x) =P104 j=0wjxj? Put this way, most audiences overwhelmingly favour choices (1) and (2), for fear of overÔ¨Åtting. But of these options, choice (3) most honestly represents our beliefs. Indeed, it is likely that the ground truth explanation for the data is out of class for any of these choices, but there is some setting of the coefÔ¨Åcientsfwjgin choice (3) which provides a better description of reality than could be managed by choices (1) and (2), which are special cases of choice (3). Moreover, our beliefs about the generative processes for our observations, 1949 1951 1953 1955 1957 1959 Year100k200k300k400k500kAirline Passengers Figure 1. Airline passenger numbers recorded monthly. which are often very sophisticated, typically ought to be independent of how many data points we happen to observe. And in modern practice, we are implicitly favouring choice (3): we often use neural networks with millions of param eters to Ô¨Åt datasets with thousands of points. Furthermore, nonparametric methods such as Gaussian processes often involve inÔ¨Ånitely many parameters, enabling the Ô¨Çexibil ity for universal approximation (Rasmussen & Williams, 2006), yet in many cases provide very simple predictive distributions. Indeed, parameter counting is a poor proxy for understanding generalization behaviour. From a probabilistic perspective, we argue that generaliza tion depends largely on twoproperties, the support and the inductive biases of a model. Consider Figure 2(a), where on the horizontal axis we have a conceptualization of all possible datasets, and on the vertical axis the Bayesian ev idence for a model. The evidence, or marginal likelihood, p(DjM ) =R p(DjM;w)p(w)dw, is the probability we would generate a dataset if we were to randomly sample from the prior over functions p(f(x))induced by a prior over parameters p(w). We deÔ¨Åne the support as the range of datasets for which p(DjM )>0. We deÔ¨Åne the inductive biases as the relative prior probabilities of different datasets ‚Äî the distribution of support given byp(DjM ). A similar schematic to Figure 2(a) was used by MacKay (1992) to understand an Occam‚Äôs razor effect in using the evidence for model selection; we believe it can also be used to reason about model construction and generalization.arXiv:2002.08791v4  [cs.LG]  30 Mar 2022Bayesian Deep Learning and a Probabilistic Perspective of Generalization p(D|M ) Corrupted CIFAR10CIFAR10 MNISTDataset Structured Image DatasetsComplex Model Poor Inductive Biases Example: MLPSimple Model Poor Inductive Biases Example: Linear FunctionWellSpeciÔ¨Åed Model Calibrated Inductive Biases Example: CNN (a) True ModelPrior Hypothesis Space Posterior (b) True ModelPrior Hypothesis Space Posterior (c) True ModelPrior Hypothesis Space Posterior (d) Figure 2. A probabilistic perspective of generalization. (a) Ideally, a model supports a wide range of datasets, but with inductive biases that provide high prior probability to a particular class of problems being considered. Here, the CNN is preferred over the linear model and the fullyconnected MLP for CIFAR10 (while we do not consider MLP models to in general have poor inductive biases, here we are considering a hypothetical example involving images and a very large MLP). (b) By representing a large hypothesis space, a model can contract around a true solution, which in the realworld is often very sophisticated. (c) With truncated support, a model will converge to an erroneous solution. (d) Even if the hypothesis space contains the truth, a model will not efÔ¨Åciently contract unless it also has reasonable inductive biases. From this perspective, we want the support of the model to be large so that we can represent any hypothesis we believe to be possible, even if it is unlikely. We would even want the model to be able to represent pure noise, such as noisy CIFAR (Zhang et al., 2016), as long as we honestly believe there is some nonzero, but potentially arbitrarily small, probability that the data are simply noise. Crucially, we also need the inductive biases to carefully represent which hypotheses we believe to be a priori likely for a particular problem class. If we are modelling images, then our model should have statistical properties, such as convolutional structure, which are good descriptions of images. Figure 2(a) illustrates three models. We can imagine the blue curve as a simple linear function, f(x) =w0+w1x, combined with a distribution over parameters p(w0;w1), e.g.,N(0;I), which induces a distribution over functions p(f(x)). Parameters we sample from our prior p(w0;w1) give rise to functions f(x)that correspond to straight lines with different slopes and intercepts. This model thus has truncated support: it cannot even represent a quadratic func tion. But because the marginal likelihood must normal ize over datasetsD, this model assigns much mass to the datasets it does support. The red curve could represent a large fullyconnected MLP. This model is highly Ô¨Çexible, but distributes its support across datasets too evenly to be particularly compelling for many image datasets. The green curve could represent a convolutional neural network, which represents a compelling speciÔ¨Åcation of support and induc tive biases for image recognition: this model has the Ô¨Çexibil ity to represent many solutions, but its structural properties provide particularly good support for many image problems. With large support, we cast a wide enough net that the poste rior can contract around the true solution to a given problemas in Figure 2(b), which in reality we often believe to be very sophisticated. On the other hand, the simple model will have a posterior that contracts around an erroneous solution if it is not contained in the hypothesis space as in Figure 2(c). Moreover, in Figure 2(d), the model has wide support, but does not contract around a good solution because its support is too evenly distributed. Returning to the opening example, we can justify the high order polynomial by wanting large support. But we would still have to carefully choose the prior on the coefÔ¨Åcients to induce a distribution over functions that would have rea sonable inductive biases. Indeed, this Bayesian notion of generalization is not based on a single number, but is a two dimensional concept. From this probabilistic perspective, it is crucial not to conÔ¨Çate the Ô¨Çexibility of a model with thecomplexity of a model class. Indeed Gaussian processes with RBF kernels have large support, and are thus Ô¨Çexible, but have inductive biases towards very simple solutions. We also see that parameter counting has no signiÔ¨Åcance in this perspective of generalization: what matters is how a distri bution over parameters combines with a functional form of a model, to induce a distribution over solutions. Rademacher complexity (Mohri & Rostamizadeh, 2009), VC dimension (Vapnik, 1998), and many conventional metrics, are by con trast one dimensional notions , corresponding roughly to the support of the model, which is why they have been found to provide an incomplete picture of generalization in deep learning (Zhang et al., 2016). In this paper we reason about Bayesian deep learning from a probabilistic perspective of generalization. The key dis tinguishing property of a Bayesian approach is marginaliza tion instead of optimization, where we represent solutions given by all settings of parameters weighted by their posBayesian Deep Learning and a Probabilistic Perspective of Generalization terior probabilities, rather than bet everything on a single setting of parameters. Neural networks are typically under speciÔ¨Åed by the data, and can represent many different but high performing models corresponding to different settings of parameters, which is exactly when marginalization will make the biggest difference for accuracy and calibration. Moreover, we clarify that the recent deep ensembles (Lak shminarayanan et al., 2017) are not a competing approach to Bayesian inference, but can be viewed as a compelling mechanism for Bayesian marginalization. Indeed, we em pirically demonstrate that deep ensembles can provide a better approximation to the Bayesian predictive distribution than standard Bayesian approaches. We further propose a new method, MultiSWAG, inspired by deep ensembles, which marginalizes within basins of attraction ‚Äî achieving signiÔ¨Åcantly improved performance, with a similar training time. We then investigate the properties of priors over functions induced by priors over the weights of neural networks, show ing that they have reasonable inductive biases. We also show that the mysterious generalization properties recently pre sented in Zhang et al. (2016) can be understood by reasoning about prior distributions over functions, and are not speciÔ¨Åc to neural networks. Indeed, we show Gaussian processes can also perfectly Ô¨Åt images with random labels, yet generalize on the noisefree problem. These results are a consequence of large support but reasonable inductive biases for com mon problem settings. We further show that while Bayesian neural networks can Ô¨Åt the noisy datasets, the marginal like lihood has much better support for the noise free datasets, in line with Figure 2. We additionally show that the mul timodal marginalization in MultiSWAG alleviates double descent, so as to achieve monotonic improvements in per formance with model Ô¨Çexibility, in line with our perspective of generalization. MultiSWAG also provides signiÔ¨Åcant im provements in both accuracy and NLL over SGD training and unimodal marginalization. Finally we provide several perspectives on tempering in Bayesian deep learning. In the Appendix we provide several additional experiments and results. We also provide code at https://github. com/izmailovpavel/understandingbdl . 2. Related Work "
480,ECOVNet: An Ensemble of Deep Convolutional Neural Networks Based on EfficientNet to Detect COVID-19 From Chest X-rays.txt,"This paper proposed an ensemble of deep convolutional neural networks (CNN)
based on EfficientNet, named ECOVNet, to detect COVID-19 using a large chest
X-ray data set. At first, the open-access large chest X-ray collection is
augmented, and then ImageNet pre-trained weights for EfficientNet is
transferred with some customized fine-tuning top layers that are trained,
followed by an ensemble of model snapshots to classify chest X-rays
corresponding to COVID-19, normal, and pneumonia. The predictions of the model
snapshots, which are created during a single training, are combined through two
ensemble strategies, i.e., hard ensemble and soft ensemble to ameliorate
classification performance and generalization in the related task of
classifying chest X-rays.","Coronavirus disease 2019 (COVID19) is a contagious disease that was caused by the Severe Acute Respiratory Syndrome Coronavirus 2 (SARSCoV2). The disease was Ô¨Årst detected in Wuhan City, Hubei Province, China in December 2019, and was related to contact with a seafood wholesale market and quickly spread to all parts of the world [1]. The World Health Organization (WHO) promulgated the outbreak of the COVID19 pandemic on March 11, 2020. As of September 20, 2020, this perilous virus has not only overwhelmed the world, but also affected millions of lives. So far, there have been 30;675;675conÔ¨Årmed COVID19 cases and 954;417conÔ¨Årmed deaths [2]. To limit the spread of this infection, all infected countries strive to cover many strategies such as encourage people to maintain social distancing as well as lead hygienic life, enhance the infection screening system through multifunctional testing, seek mass vaccination to reduce the pandemic ahead of time, etc. The reverse transcriptasepolymerase chain reaction (RTPCR) is a modular diagnosis method, however, it has certain limitations, such as the accurate detection of suspect patients causes delay since the testing procedures inevitably preserve the strict necessity of conditions at the clinical laboratory [3] and falsenegative results may lead to greater impact in the prevention and control of the disease [4]. To make up for the shortcomings of RTPCR testing, researchers around the world are seeking to promote a fast and reliable diagnostic method to detect COVID19 infection. The WHO and Wuhan University Zhongnan Hospital respectively issued quick guides [5, 6], suggesting that in addition to detecting clinical symptoms, chest imaging can also be used to evaluate the disease to diagnose and treat COVID19. In [7], the authors have contributed a proliÔ¨Åc guideline for medical practitioners to use chest radiography and computed tomography (CT) to screen and assess the disease progression of COVID19 cases. Although CT scans have higher sensitivity, it also has some drawbacks, such as high cost and the need for high doses of radiation during screening, which exposes pregnant women and children to greater radiation risks [8]. On the other hand, diagnosis based on chest Xray appears to be a propitious solution for COVID19 detection and treatment. In [9], Ng et al. remarked that COVID19 infection pulmonary manifestation is immensely delineated by chest Xray images. Moreover, in the case of an artiÔ¨Åcial intelligence (AI)based disease recognition system, medical practitioners have already emphasized chest Xrays to explore potential symptoms of COVID19 infection, such as opaque patterns in the lungs [10]. The purpose of this study is to ameliorate the accuracy of COVID19 detection system from chest Xray images. In this context, we contemplate a CNNbased architecture since it is illustrious for its topnotch recognition performance in image classiÔ¨Åcation or detection. For medical image analysis, higher detection accuracy along with crucial Ô¨Åndings is a top aspiration, and in current years, CNN based architectures are comprehensively featured the critical Ô¨Åndings related to medical imaging that‚Äôs why we constructed the proposed architecture with CNN. In order to achieve the deÔ¨Åned purpose, this paper presents a novel CNN based architecture called ECOVNet, exploiting the cuttingedge EfÔ¨ÅcientNet [11] family of CNN models together with ensemble strategies. The pipeline of the proposed architecture commences with the data augmentation approach, then optimizes and Ô¨Ånetunes the pretrained EfÔ¨ÅcientNet models, creating respective model‚Äôs snapshots. After that, generated model snapshots are integrated into an ensemble, i.e., soft voting and hard voting, to make predictions. The motivation for using EfÔ¨ÅcientNets is that they are known for their high accuracy, while being smaller and faster than the best existing CNN architectures. Moreover, an ensemble technique has proven to be effective in predicting since it produces a lower error rate compared with the prediction of a single model. Owing to the limited number of COVID19 images currently available, diagnosing COVID19 infection is more challenging, thereby investing with a visual explainable approach is applied for further analysis. In this regard, we use a Gradientbased Class Activation Mapping algorithm, i.e., GradCAM [12], providing explanations of the predictions and identifying relevant features associated with COVID19 infection. The key contributions of this paper are as follows: ‚Ä¢We propose a novel CNN based architecture that includes frontend pretrained EfÔ¨ÅcientNets for feature extraction and model snapshots to detect COVID19 from chest Xrays. ‚Ä¢Taking into account the following assumption, the decisions of multiple radiologists are considered in the Ô¨Ånal prediction, so we propose an ensemble in the proposed architecture to make predictions, thus making a credible and fair evaluation of the system. ‚Ä¢We visualize a class activation map through GradCAM to explain the prediction as well as to identify the critical regions in the chest Xray. ‚Ä¢Finally, we appraise our architecture with stateoftheart architectures through empirical observations to highlight the effectiveness of the proposed architecture in detecting COVID19. The remainder of the paper is arranged as follows: Section 2 discusses related work. Section 3 explains the details of the data set and proposed network architecture, as well as its adjustments to the detection of COVID19 infection. The results of our experimental evaluation is presented in Section 4. Finally, Section 5 concludes paper and highlights the future work. 2APREPRINT  OCTOBER 19, 2020 2 Related Works "
481,Joint Negative and Positive Learning for Noisy Labels.txt,"Training of Convolutional Neural Networks (CNNs) with data with noisy labels
is known to be a challenge. Based on the fact that directly providing the label
to the data (Positive Learning; PL) has a risk of allowing CNNs to memorize the
contaminated labels for the case of noisy data, the indirect learning approach
that uses complementary labels (Negative Learning for Noisy Labels; NLNL) has
proven to be highly effective in preventing overfitting to noisy data as it
reduces the risk of providing faulty target. NLNL further employs a three-stage
pipeline to improve convergence. As a result, filtering noisy data through the
NLNL pipeline is cumbersome, increasing the training cost. In this study, we
propose a novel improvement of NLNL, named Joint Negative and Positive Learning
(JNPL), that unifies the filtering pipeline into a single stage. JNPL trains
CNN via two losses, NL+ and PL+, which are improved upon NL and PL loss
functions, respectively. We analyze the fundamental issue of NL loss function
and develop new NL+ loss function producing gradient that enhances the
convergence of noisy data. Furthermore, PL+ loss function is designed to enable
faster convergence to expected-to-be-clean data. We show that the NL+ and PL+
train CNN simultaneously, significantly simplifying the pipeline, allowing
greater ease of practical use compared to NLNL. With a simple semi-supervised
training technique, our method achieves state-of-the-art accuracy for noisy
data classification based on the superior filtering ability.","Convolutional Neural Networks (CNNs) have led to great improvements in many supervised tasks. However, CNNs‚Äô performance relies heavily on the quality of labels, and accurately labeling a huge amount of data is expen sive and timeconsuming. Furthermore, accurate labeling is done by hand, which can eventually lead to mismatched labeling. Therefore, the robust training of CNNs with noisy data is of great practical importance. There are many ap proaches regarding this issue. For example, there are methods that design noiserobust loss [4, 3, 29, 18], use two neu ral networks to select clean labels [6, 33, 30], and utilize label correction [22, 31]. These existing approaches com monly use the given labels in a direct manner, i.e., ‚Äúinput image belongs to this label‚Äù ( Positive Learning; PL ). This behavior carries the risk of providing faulty information to the CNNs when noisy labels are involved. Motivated by this reason, Negative Learning for Noisy Labels; NLNL [12], which is an indirect learning method for training CNNs, has been proposed recently. Negative Learning (NL) uses randomly chosen complementary la bels and trains the CNN that ‚Äúinput image does not belong to this complementary label,‚Äù reducing the risk of providing the wrong information because of the high chance of not selecting a true label as a complementary label. Addition ally, NLNL proposed threestage pipeline for Ô¨Åltering noisy data from training data (Figure 1 (a)). Each stage is com posed of NL!NL while discarding data of low conÔ¨Ådence (Selective NL; SelNL )!PL while only retaining data of high conÔ¨Ådence ( Selective PL; SelPL ), enabling more con vergence after NL. However, the fundamental problem that NL loss function causes underÔ¨Åtting to the overall training data still remains. This is the reason that NL requires an additional sequential step, SelNL. Furthermore, the three stage pipeline for Ô¨Åltering noisy data is quite inefÔ¨Åcient, ex tending the time for training CNNs. In this study, we propose a novel version of NLNL: Joint Negative Learning and Positive Learning; JNPL which has a uniÔ¨Åed singlestage pipeline for Ô¨Åltering noisy data (Fig ure 1 (b)). JNPL is composed of two losses to train CNN, NL+ and PL+ losses, dedicated to Ô¨Åltering noisy data from training data. Each is developed from NL and PL loss func tions, respectively. Firstly, our paper focuses on analyzing the NL loss function to understand the cause for underÔ¨Åt ting. Then we develop a new loss function NL+ that re solves the issue, which produces a gradient appropriate for convergence on a noisy training dataset. Our study demon strates the effectiveness of NL+, showing improved conver gence across various label noise types and noise rates. Sec ondly, while we utilize PL to aid in training with noisy data, PL+ loss function is also newly designed to enable fasterarXiv:2104.06574v1  [cs.LG]  14 Apr 2021NL selNL selPL JNPL NL selNL selPL JNPL (a) NLNL (b) JNPL Figure 1: Comparison between Negative Learning for Noisy Labels (NLNL) and Joint Negative and Positive Learning (JNPL) for Ô¨Åltering noisy data from training data, demonstrated with histograms showing the distribution of noisy training data . (a): NLNL is a 3stage pipeline (NL !SelNL!SelPL). (b): JNPL is a singlestage pipeline, in which two loss functions (NL+ and PL+) train CNN simultaneously. training with expectedtobeclean data. Our paper shows the effectiveness of the PL+ loss function compared to the previous PL loss function. Finally, as both loss functions of our method (NL+ and PL+) jointly train the model through asingle stage , it is simple and easier to use than NLNL. Our experiments show that JNPL successfully Ô¨Ålters noisy data in a single stage, thereby providing signiÔ¨Åcantly faster train ing of CNN as well as better Ô¨Åltering compared to NLNL. After Ô¨Åltering noisy data from the training data we per form pseudolabeling for noisy data classiÔ¨Åcation. We achieve stateoftheart accuracy across various settings in CIFAR10, CIFAR100 [13], and Clothing1M [31] datasets, proving the superior Ô¨Åltering ability of JNPL. The main contributions of this paper are as follows: We propose an improved version of NLNL, named ‚ÄúJoint Negative and Positive Learning (JNPL), ‚Äù featuring a singlestage pipeline for Ô¨Åltering noisy data, therefore enabling easier usage compared to NLNL. Two novel loss functions are newly designed, each named NL+ loss and PL+ loss. NL+ solves the underÔ¨Åtting problem of the NL loss, and provides better convergence on various types and ratios of label noises in the training data. Moreover, PL+ enables faster training compared to the previous PL loss function. Our method Ô¨Ålters noisy data, more robust across differ ent types and ratios of noise than NLNL. Our method also achieves stateoftheart noisy data classiÔ¨Åcation re sults when used along with pseudolabeling. Prior knowledge of the type or number of noisy data is not required for our method. It does not require any hyperparameter tuning that depend on prior knowledge, allowing our method to be applicable in practice. The remainder of this paper is organized as follows. Sec tion 3 describes NLNL method in depth, which is targeted throughout the whole paper, and discusses the cause of the underÔ¨Åtting problem of the method. Section 4 describes our proposed method, JNPL, and explains in detail on NL+ loss and PL+ loss terms. Section 5 demonstrates the overallcomparison between JNPL and NLNL, showing the distinct advantages of JNPL over NLNL. Section 6 discusses the evaluations of our method in comparison to baseline meth ods. Finally, we summarize and conclude in Section 7. 2. Related works "
482,On Multi-head Ensemble of Smoothed Classifiers for Certified Robustness.txt,"Randomized Smoothing (RS) is a promising technique for certified robustness,
and recently in RS the ensemble of multiple deep neural networks (DNNs) has
shown state-of-the-art performances. However, such an ensemble brings heavy
computation burdens in both training and certification, and yet under-exploits
individual DNNs and their mutual effects, as the communication between these
classifiers is commonly ignored in optimization. In this work, starting from a
single DNN, we augment the network with multiple heads, each of which pertains
a classifier for the ensemble. A novel training strategy, namely Self-PAced
Circular-TEaching (SPACTE), is proposed accordingly. SPACTE enables a circular
communication flow among those augmented heads, i.e., each head teaches its
neighbor with the self-paced learning using smoothed losses, which are
specifically designed in relation to certified robustness. The deployed
multi-head structure and the circular-teaching scheme of SPACTE jointly
contribute to diversify and enhance the classifiers in augmented heads for
ensemble, leading to even stronger certified robustness than ensembling
multiple DNNs (effectiveness) at the cost of much less computational expenses
(efficiency), verified by extensive experiments and discussions.","Deep neural networks (DNNs) have been widely applied in various Ô¨Åelds [1, 2], but at the same time showed devas tating vulnerability to adversarial samples. That is, images crafted with maliciouslydesigned perturbations, namely adversarial examples, can easily mislead welltrained DNNs into wrong predictions [3, 4]. To resist adversarial ex amples, there has been a series of empirical defenses against adversarial attacks, e.g., adversarial training and its variants[5, 6, 7]. Besides, various methods for certiÔ¨Åed robustness [8, 9, 10] have also attracted increasing atten tion in recent years, where the robustness of DNNs for images perturbed by Gaussian noises N(0;2I)is focused. A certiÔ¨Åablyrobust classiÔ¨Åer guarantees that for any input x, the predictions are kept constant within its perturbed neighborhood bounded commonly by the `2norm. Randomized smoothing [11, 12, 13] (RS) is considered as one of the most effective `2norm certiÔ¨Åablyrobust defenses. With RS, any base classiÔ¨Åer (DNN) can be formulated into a smoothed and certiÔ¨Åablyrobust one by giving the most probable prediction over the Gaussian corruptions of the input. Cohen et al. [12] Ô¨Årstly proved a tight robustness guarantee of RS and achieved stateoftheart results on largescale DNNs and complex datasets. Various related works have been developed for a more robust base classiÔ¨Åer with RS. These works can be mainly categorized as two types: extra regularization terms [14, 15, 16] and enhanced data augmentation techniques [17, 18]. Recently, the ensemble of multiple DNNs being the base classiÔ¨Åer in RS has shown great potentials in certiÔ¨Åed robust ness. The weighted logit ensemble of several DNNs trained from different random seeds [19] or maxmargin ensemble of Ô¨Ånetuning multiple pretrained DNNs with extra constraints [20] have achieved signiÔ¨Åcant improvements in higher certiÔ¨Åed accuracy on the predictions for N(x;2I). However, such an ensemble scheme demands considerably higher computational sources to train multiple large DNNs. In particular, the resulting computational burden becomes more pronounced in the certiÔ¨Åcation phase, because certifying each sample xcan even require tens of thousands of samarXiv:2211.10882v1  [cs.LG]  20 Nov 2022(ùê∑ùëí1,ùê∑‚Ñé1) backbone, ùëî head, ‚Ñé1 head, ‚Ñé2 head, ‚Ñé3 head, ‚Ñé4 head, ‚Ñé5forward propagation in a 5head DNN communication flow in our circular teaching (large certified radii) (small certified radii) Head‚Ñéùëò+1istaught by ‚Ñéùëòwith via  selfpaced learning based on smoothed losses . (ùê∑ùëí2,ùê∑‚Ñé2) (ùê∑ùëí3,ùê∑‚Ñé3) (ùê∑ùëí4,ùê∑‚Ñé5) (ùê∑ùëí5,ùê∑‚Ñé5) ùê∑ùëí:easy samples  ùê∑‚Ñé: hard samples  (ùê∑ùëíùëò,ùê∑‚Ñéùëò) top1 top2top1 top2Figure 1: An illustration on the proposed SPACTE on a 5head DNN. In our efÔ¨Åcient structure for the ensemble, a common backbone gis shared by the augmented 5 heads h1;;h5, which are trained to be mutually orthogonal to each other for diversiÔ¨Åed classiÔ¨Åers. In the novel training with circular communication Ô¨Çow between classiÔ¨Åers, the augmented head hk+1are optimized via the circularteaching by its peer classiÔ¨Åer hkwith easy and hard samples in relation to the certiÔ¨Åed radii ( h0:=h5). plings withinN(x;2I)[12], resulting in numerous inferences in forward propagation. In this case, such certiÔ¨Åcation time gets exacerbated approximately by the number of ensembled DNNs. Apart from the sacriÔ¨Åce of efÔ¨Åciency, these ensemble methods also ignore a straightforward and yet crucial fact that the current training of ensembled DNNs underexploits the potentials of each individual DNN at hand and omits the mutual effects of each other in promoting the certiÔ¨Åed robustness. Starting from an individual DNN, we in this paper augments the standard DNN with multiple heads. On such a multihead structure basis we propose a novel ensemble training scheme, namely the SelfPAced CircularTEaching (SPACTE) , where multiple classiÔ¨Åers through mutuallyorthogonal heads get ensembled and trained jointly with the common backbone coupling all heads. In the proposed circular teaching scheme, all heads are simultaneously opti mized and meanwhile communicate with each other, instead of the separate training of individual DNNs in ensem ble [19]. To be speciÔ¨Åc, during the joint training, each augmented head teaches its next neighboring head with selected samples by our modiÔ¨Åed selfpaced learning , which progresses the optimization from ‚Äúeasy‚Äù samples to ‚Äúhard‚Äù ones with speciÔ¨Åc considerations to certiÔ¨Åed robustness. In the following, the proposed SPACTE is elaborated: ‚Ä¢ In the modelling, our mutuallyorthogonal heads leverage the essential idea of inducing diversity among classiÔ¨Åers for variance reduction as in the classic ensemble [21, 22, 23], but manage to greatly alleviate computational burdens both in storage and time expenses. ‚Ä¢ The concept of communication between ensembled classiÔ¨Åers is introduced in a joint optimization, enabling information exchange to exploit potentials of individual classiÔ¨Åers and providing a novel perspective on the interaction in ensemble training. ‚Ä¢ In the optimization, each head selects the minibatch of samples to teach its next neighboring head to update. The sample selection in such a circularteaching scheme is conducted with selfpaced learning [24, 25], which proceeds the optimization from ‚Äúeasy‚Äù samples to ‚Äúhard‚Äù ones, speciÔ¨Åcally deÔ¨Åned in relation to the proposed smoothed loss for certiÔ¨Åed robustness. Rather than the ensemble after the separate training of multiple DNNs, the proposed method instead considers a novel ensemble based training method for a single DNN with augmented heads. On the one hand, the common highlevel features are expected to be exploited by all classiÔ¨Åers in the ensemble; such highlevel feature learning can be reÔ¨Çected in the shared backbone in our multihead structure. On the other hand, classiÔ¨Åers in ensemble methods are designed to be diverse from each other and are capable of learning different subtle features, for the sake of greater reliability and variance reduction [19]; in our method, such diversity is embodied from two folds: (i) the parameters of the augmented heads are imposed to be mutually orthogonal, and (ii) the optimization of each head is conducted via the minibatch samples that are taught by the selfpaced learning of its neighboring head, not by itself. SPACTE is compatible with all singlemodelbased methods, which can be plugged into SPACTE via regularization [14, 15, 16] or data augmentation [17, 18]. Extensive experiments demonstrate that the proposed SPACTE greatly improves the certiÔ¨Åed robustness over the stateoftheart (SOTA) methods with much less computation overhead than the existing 2ensemble methods. SPACTE acts as an efÔ¨Åcient and effective ensemble method, which successfully exempliÔ¨Åes a bridge connecting the methods of improving single models and integrating multiple models for certiÔ¨Åed robustness. In the following, Sec. 2 outlines related works. Section 3 introduces the proposed SPACTE, in terms of the multihead structure and its optimization. Section 4 presents extensive experiments on the certiÔ¨Åed robustness from different aspects. Section 5 brieÔ¨Çy concludes the paper. 2 Related work "
483,Towards Federated Learning against Noisy Labels via Local Self-Regularization.txt,"Federated learning (FL) aims to learn joint knowledge from a large scale of
decentralized devices with labeled data in a privacy-preserving manner.
However, since high-quality labeled data require expensive human intelligence
and efforts, data with incorrect labels (called noisy labels) are ubiquitous in
reality, which inevitably cause performance degradation. Although a lot of
methods are proposed to directly deal with noisy labels, these methods either
require excessive computation overhead or violate the privacy protection
principle of FL. To this end, we focus on this issue in FL with the purpose of
alleviating performance degradation yielded by noisy labels meanwhile
guaranteeing data privacy. Specifically, we propose a Local Self-Regularization
method, which effectively regularizes the local training process via implicitly
hindering the model from memorizing noisy labels and explicitly narrowing the
model output discrepancy between original and augmented instances using self
distillation. Experimental results demonstrate that our proposed method can
achieve notable resistance against noisy labels in various noise levels on
three benchmark datasets. In addition, we integrate our method with existing
state-of-the-arts and achieve superior performance on the real-world dataset
Clothing1M. The code is available at https://github.com/Sprinter1999/FedLSR.","The pervasiveness of large end devices (e.g. personal mobile phones and IoT devices), has contributed to the drastically increasing scale of data generating from distributed clients among the network. Federated learning (FL) is a new distributed learning paradigm that enables a global model to be trained collaboratively by multiple clients while keeping the private data decentralized on devices. As shown in Figure 1, conventional FL [ 42] process mainly con sists of two periods: 1) the selected clients perform local training process on private labeled data after obtaining the distributed global model, and then upload the updated local trained models to the server; 2) the server synchronizes and aggregates these local trained models to obtain the updated global model for next round. This above process continues until the global model converges or within limited communication rounds. Many existing works towards FL have achieved wide success in dealing with four main challenges when deploying a practical FL system, including statistical heterogeneity, systems heterogeneity, communication efficiency, and privacy concerns as referred in [ 35]. But most existing works are based on an important assumption that the raw data owned by clients are perfectly labeled. In practice, it is hard to guarantee the label correctness of collected training data, since highquality annotations requires expensive human intelligence and efforts. Moreover, data are very likely to contain incorrect labels (a.k.a noisy labels) in FL, since labels are usually produced independently by clients with various labelgenerating methods, such as filtering images‚Äô surrounding context [ 34] or exploiting machinegenerated labels [28]. Data with noisy labels inevitably cause the model performance degradation. In detail, Arpit et al. propose the memorization ef fect of deep network [ 1] which indicates that data with correct labels fit before data with incorrect labels, thus the model perfor mance first rises up and then gradually drops during the train ing process. Although learning on data with noisy labels [ 45] has been widely studied in the data centralized setting, most exist ing works cannot be straightforwardly applied to FL, due to un bearable computation burden and exorbitant communication over head for resourceconstrained devices. For example, [ 23,33,47] perform computationheavy procedures for noise cleaning [ 34] , which brings nonnegligible synchronization cost during conduct ing the serverside model aggregation, and thus negatively affects global model convergence. While other methods cannot satisfy some unique characteristics of FL. For example, [ 38] requires di rect access to all data samples in the early learning process, but the server only selects a small fraction of clients to conduct local training on these clients‚Äô private data in FL.arXiv:2208.12807v1  [cs.LG]  25 Aug 2022CIKM ‚Äô22, October 17‚Äì21, 2022, Atlanta, GA, USA. Xuefeng Jiang, Sheng Sun, Yuwei Wang, and Min Liu Cloud Server global modelglobal model local datasetlocal datasetglobal model local datasetlocal trained model local  trained  model local trained model Figure 1: An illustration of the common workflow of federated learning. The global model is updated by aggregating local trained models of a set of selected clients. Note that the local datasets can contain data with noisy labels. To reduce the negative effect caused by noisy labels in FL, most existing researches [ 7,54,65] utilize an auxiliary dataset with per fect labels to identify the noise level of clients or conduct the sample level selection for training. However, since it is hard and impractical to obtain a predefined, fixed and perfectlylabeled auxiliary dataset, these methods fail to tackle the issue of training on data with noisy labels in reality. Fortunately, Yang et al. [ 66] firstly propose an auxiliarydatasetfree FL method, which collects local classwise feature centroids from clients to form global classwise feature cen troids as global supervision to effectually deal with the noisy labels. However, this method can bring about underlying privacy leakage risks, since these centroids transimitted between clients and the server can be inversely utilized to reveal some sensitive informa tion about the private data. Therefore, it is necessary to obtain extra reliable supervision for dealing with noisy labels without compromising data privacy. Data augmentation is a lowcost and widelyutilized technique to effectively obtain extra supervision information, and has been ap plied in many machine learning problems such as supervised learn ing [ 27,64], semisupervised learning [ 4,30,63], selfsupervised contrastive learning [ 2,6,17] and many other domains. Neverthe less, few works have explored the utilization of data augmentation technique in the scope of learning on data with noisy labels [47]. In this work, we focus on the local training process of FL, and uti lize data augmentation technique to obtain extra supervision, so as to promote performance against noisy labels without violating the privacy principle. Considering the presence of data with noisy la bels, our intuition is threefold: 1) The prediction for the augmented sample should be close to the prediction for the original sample. 2) Model predictions can be more close to the ground truth than the corresponding noisy labels. 3) The proposed approach should be privacypreserving to be applied in the practical FL system. Following this intuition, we propose a method named Local Self Regularization to tackle the issue of training on data with noisy labels in FL, which tries to mitigate the performance degradation caused by noisy labels in the premise of protecting data privacy. Specifically, we implicitly regularize local training by enhancing model discrimination confidence to prevent it from memorizing noisy labels, and further utilize self knowledge distillation tech nique to explicitly regularize the model output discrepancy between original and augmented instances. We provide some insights to deal with noisy labels in FL through the following contributions:‚Ä¢We empirically show that in the presence of noisy labels, the memorization effect of deep network proposed in data centralized setting still exists in FL, which brings degradation to the global model performance. ‚Ä¢We propose an auxiliarydatasetfree and privacypreserving FL method named Local SelfRegularization, which implicitly regularizes the model from memorizing noisy labels and explicitly regularizes the model output discrepancy between original and augmented instances. ‚Ä¢We present the effectiveness of our method through exten sive experiments on three benchmark datasets compared to the stateofthearts in various noise levels. In addition, our method has potential to incorporate with other existing methods to further improve performance, and this insight is also verified on the realworld Clothing1M dataset. 2 RELATED WORKS "
484,Webly Supervised Image Classification with Metadata: Automatic Noisy Label Correction via Visual-Semantic Graph.txt,"Webly supervised learning becomes attractive recently for its efficiency in
data expansion without expensive human labeling. However, adopting search
queries or hashtags as web labels of images for training brings massive noise
that degrades the performance of DNNs. Especially, due to the semantic
confusion of query words, the images retrieved by one query may contain
tremendous images belonging to other concepts. For example, searching `tiger
cat' on Flickr will return a dominating number of tiger images rather than the
cat images. These realistic noisy samples usually have clear visual semantic
clusters in the visual space that mislead DNNs from learning accurate semantic
labels. To correct real-world noisy labels, expensive human annotations seem
indispensable. Fortunately, we find that metadata can provide extra knowledge
to discover clean web labels in a labor-free fashion, making it feasible to
automatically provide correct semantic guidance among the massive label-noisy
web data. In this paper, we propose an automatic label corrector VSGraph-LC
based on the visual-semantic graph. VSGraph-LC starts from anchor selection
referring to the semantic similarity between metadata and correct label
concepts, and then propagates correct labels from anchors on a visual graph
using graph neural network (GNN). Experiments on realistic webly supervised
learning datasets Webvision-1000 and NUS-81-Web show the effectiveness and
robustness of VSGraph-LC. Moreover, VSGraph-LC reveals its advantage on the
open-set validation set.","Deep convolutional neural networks (CNNs) are successful by virtue of largescale datasets with human annotation [ 23]. However, human annotation is extremely timeconsuming and expensive, which impedes the further expansion of those big datasets [ 42]. To overcome this limitation, researchers use web crawlers to collect billions of images and annotate them directly using text queries or hashtags [ 20,30]. However, due to the ambiguity or polysemy of the query fed into the search engine, label noise is subsequently introduced. Therefore, webly supervised learning, aiming at using huge scalable webcrawled data directly for networks training by suppressing label noise, has attracted great attention recently [1]. Early exploration in this direction relies on humanverified clean subsets. Representative methods using clean subsets include Men torNet [ 17] and CleanNet [ 24]. However, with the trend of rapid growth in the size of webly datasets, building clean subsets with manual verification becomes more infeasible, especially when the number of categories exceeds ten thousands [ 45]. Therefore, re cent works prefer models without clean subset dependencies, mak ing webly supervised learning fully automatic. To this end, some works strengthen networks‚Äô endurability against label noise using moving average of model predictions [ 40], loss function modifi cation [ 11,31] or regularization [ 34,54] . Coteaching uses two different networks to mutually detect label noise, doubly ensuring the model‚Äôs denoising ability [ 13]. Other works identify noisy sam ples based on some hypotheses including data density [ 12,14] or model confidence [50].arXiv:2010.05864v1  [cs.CV]  12 Oct 2020(a) Colors reflect web labels. Web label ‚ÄôDrumstick‚Äô shows rep resentative images corresponding to 5 regions of interest. Only Region5 corresponds to the correct concept (b) Prediction by Coteaching  (c) Prediction by VSGraphLC Figure 1: TSNE visualization [29] of WebVisionpretrained ResNet50 [15] features on 10 selected categories. Three ob servations are highlighted: (1) CNN models that trained from WebVision can distinguish different semantics within a category, even when semantics mismatch category defini tion. (2) Severe semantic label noise is a realworld problem, as majority images of class ‚Äòdrumsticks‚Äô deviate the true con cept of percussion mallets. (3) Coteaching fails to correct the majority semantic label noise, but our VSGraphLC is able to. Node brightness represents prediction confidence Although the aforementioned methods effectively enhance the model against label noise, especially for outliers, suppressing se mantic label noise is critical but untouched. To clarify, semantic label noise exists due to query‚Äôs polysemy or insufficient semantic resolution. Usually, semantic label noise would be severe in some category, which is composed of a large number of samples reflecting another semantic concept. As webly datasets come from realworld, those offtarget semantics are usually outofdistribution (i.e., devi ate from all semantic labels or out of interests in the test sets). For example, Figure 1a shows that although web label ‚Äòdrum sticks‚Äô has its correct semantic concept of ‚Äòpercussion mallets‚Äô ac cording to the test set, the majority of training samples actually belong to concepts of ‚Äòdrumstick trees/vegetable‚Äô and ‚Äòchicken legs‚Äô due to polysemy. These outofdistribution noisy samples clearly cluster themselves in the visual feature space. As a result, density assumption popularly adopted by previous methods [ 12,14] will re gard ‚Äòchicken legs‚Äô and ‚Äòdrumstick trees/vegetable‚Äô samples falsely as clean data. Figure 1b further shows the ineffectiveness of the rep resentative selftraining method Coteaching, where most samples belong to offtarget semantics are still predicted as positive. As webly dataset is crawled from the Internet, text metadata associated with web images has great potential to provide valuable Label Descripon from WordNet:Drums&ck: a light drums&ck with a rounded head that is used to strike such percussion instruments as chimes, ke8ledrums, marimbas, glockenspiels, etc. ‚úì‚úò‚úòSimilarDissimilarDissimilarFigure 2: Exemplar metadata associated with web images la beled as ‚ÄòDrumstick‚Äô. By comparing metadata with label de scription, images with semantic label noise will be detected information, which, however, has been ignored for a long time. In this paper, we aim to take advantage of extra knowledge provided by metadata to suppress label noise, especially semantic label noise. Figure 2 shows that information in metadata and label description can be used to detect semantic label noise. By using offtheshelf natural language processing (NLP) models [ 52], we convert the comparison between label description and metadata from human cognition level to text feature space, which ensures a fully auto mated process to precisely locate samples with correct semantic concepts, without the need for expensive manual annotations. Hence, we are motivated to build an automatic pipeline for webly supervised learning with metadata. Specifically, we propose a label corrector named VSGraphLC, which first selects anchor samples for each category through matching label description from Word Net [ 33] and metadata of every crawled image using a powerful NLP model XLNet [ 52]. To help those semantically correct anchors propagate their web labels towards more samples, we leverage a graph neural network (GNN) [ 18] training on ùëòNN visual feature graph of the entire training set. The corrected labels substitute the former noisy web labels for finetuning our final model. In summary, our contributions are mainly threefold: ‚Ä¢We explore two understudied but important factors under webly supervised learning setting: semantic label noise and text metadata. ‚Ä¢A humanlaborfree label correcting framework that fully exploits the merits of GNN and CNN is proposed as VSGraph LC, ignited by anchors automatically selected by metadata. ‚Ä¢The proposed framework is shown effective on NUS81Web and WebVision datasets, and reaches the stateoftheart result on WebVision1000. VSGraphLC produces more ap pealing results if the test set contains outofdistribution images1. 1Known as openset recognition task, introduced in [22]2 RELATED WORK "
485,FIRED: a fine-grained robust performance diagnosis framework for cloud applications.txt,"To run a cloud application with the required service quality, operators have
to continuously monitor the cloud application's run-time status, detect
potential performance anomalies, and diagnose the root causes of anomalies.
However, existing models of performance anomaly detection often suffer from low
re-usability and robustness due to the diversity of system-level metrics being
monitored and the lack of high-quality labeled monitoring data for anomalies.
Moreover, the current coarse-grained analysis models make it difficult to
locate system-level root causes of the application performance anomalies for
effective adaptation decisions. We provide a FIne-grained Robust pErformance
Diagnosis (FIRED) framework to tackle those challenges. The framework offers an
ensemble of several well-selected base models for anomaly detection using a
deep neural network, which adopts weakly-supervised learning considering fewer
labels exist in reality. The framework also employs a real-time fine-grained
analysis model to locate dependent system metrics of the anomaly. Our
experiments show that the framework can achieve the best detection accuracy and
algorithm robustness, and it can predict anomalies in four minutes with F1
score higher than 0.8. In addition, the framework can accurately localize the
first root causes, and with an average accuracy higher than 0.7 of locating
first four root causes.","CLOUD environments provide elastic and ondemand resources for developing applications [1]. However, because of the inherent dynamism of clouds, performance anomalies of cloud applications such as degraded response time caused by resource saturation may severely affect the quality of the user experience. In addition, considering complex dependencies and multiple components in cloud applications, it‚Äôs difÔ¨Åcult for operators to detect perfor mance anomalies and identify root causes. Traditionally, operators perform diagnoses for cloud applications man ually, which is complicated and timeconsuming. Data of different monitoring metrics, e.g., CPU and memory usage, can be continuously collected, reÔ¨Çecting the runtime status of cloud applications [2]. Therefore, we could consider a performance diagnosis solution that leverages monitoring data and supports rapid recovery and loss mitigation of cloud applications. For a real cloud application, monitoring data can be identiÔ¨Åed as application and systemlevel data. Application level data such as response time can be used to detect performance anomalies when slow response time is deÔ¨Åned as an anomaly. However, it‚Äôs hard to capture the status of underlying cloud environments and exploit root causes of performance anomalies with singlevariate application level data. Meanwhile, underlying resources affect the per formance of cloud applications heavily. Systemlevel data mainly includes underlying resources, such as CPU, mem The authors are with Multiscale Networked Systems (MNS), University of Amsterdam, the Netherlands. Corresponding authors: Peng Chen (chenpeng@mail.xhu.edu.cn) and Zhiming Zhao (z.zhao@uva.nl). Peng Chen is also with School of Computer and Software Engineering, Xihua University, Chengdu, China. Manuscript received ; .ory, disk, and network. A single resource metric may not precisely reÔ¨Çect the health status of the whole application. Therefore, it‚Äôs reasonable to detect performance anomalies in cloud applications based on all systemlevel monitoring information, which is multivariate time series data. In addi tion, when a performance anomaly occurs, we need to Ô¨Åne grained pinpoint root causes on systemlevel data, which is helpful for the rapid recovery of a cloud application. Performance diagnosis is a process of detecting abnor mal performance phenomena, e.g., degradation, predicting anomalies to forestall future incidents, and localizing the causes of performance anomalies [3]. In recent years, studies for performance diagnosis have been developed and mainly focus on performance anomaly detection and root cause localization. For performance anomaly detection, numerous existing methods [4] [5] target improving detection accuracy, but their performance is inconsistent in cloud environments. For example, scaling of cloud infrastructures will change the data distributions of monitoring data, severely affect ing detection performance. Therefore, robust performance anomaly detection is necessary for performance diagnosis to keep performance consistency. As for root cause local ization, approaches are still developing [6] [7] and most of them are coarsegrained, which focuses on servicelevel or containerlevel faulty [8] [9]. To Ô¨Åll these gaps, we are motivated to develop a performance diagnosis, which can detect performance anomalies with good robustness and identify the root causes with Ô¨Ånegrained. As for the performance diagnosis framework, we iden tify several challenging requirements for performance anomaly detection and root cause localization based on real scenarios. Anomalies need to be detected accurately. The dearXiv:2209.01970v1  [cs.DC]  5 Sep 2022JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 tection should also have good robustness to perform consistently for different patterns in monitoring data. Multistep prediction of future anomalies is neces sary to effectively prevent potential application vio lations. When an anomaly occurs, root causes need to be localized in realtime with Ô¨Ånegrained, which is more helpful for efÔ¨Åcient application maintenance. Furthermore, the development of the diagnosis framework has to handle two data challenges: Monitoring data usually contains only fewer labels or no labels that can be immediately used to train a machine learningbased model because labeling data is often manual and timeconsuming. Collected monitoring data of cloud applications usu ally contains noise, which can inÔ¨Çuence the perfor mance of anomaly detection and root cause localiza tion. To address the two data challenges, we adopt weakly supervised learning [10] and provide methods to Ô¨Ålter data noise. In addition, to satisfy the three requirements, we consider the performance diagnosis framework should have good detection performance, multistep anomaly prediction ability, and Ô¨Ånegrained root cause localization. As for the performance anomaly detection and prediction, we consider integrating existing detection methods based on ensemble learning [11] to improve the detection performance. As for root cause localization, causality inference and graph methods [12] can be used. In this paper, we provide a FIne grained Robust p Erformance Diagnosis (FIRED) frame work. Our contributions can be summarized as follows: We design an integrated framework to implement performance diagnosis effectively by putting metrics selection which can Ô¨Ålter useless monitoring metrics, wellperformed performance anomaly detection and prediction, and Ô¨Ånegrained root cause localization together. We improve performance anomaly detection accu racy and robustness signiÔ¨Åcantly by developing a novel deep ensemble method. The method can also predict anomalies in four minutes with F1 score higher than 0.8. We propose a realtime and Ô¨Ånegrained root cause localization pipeline by building dependency graph and executing random walk automatically. It can accurately localize the Ô¨Årst root cause, and with the average localization accuracy higher than 0.7 of locating Ô¨Årst four root causes. The rest of the paper is organized as follows. In section 2, we review existing research about performance diagnosis and introduce anomaly detection and root cause localization methods. In section 3, a performance diagnosis framework and its main components are introduced in detail. In sec tion 4, experiments and results for each component of our diagnosis framework are provided. Finally, we provide discussion and conclusion in section 5 and section 6.TABLE 1 Unsupervised performance anomaly detection methods Type Algorithm Description DensitybasedLOF [5] Local Outlier Factor LOCI [19] Local Correlation Integral DistanceBasedKNN [20] K Nearest Neighbors LDOF [21] Local Distancebased Outlier Factor Kernelbased OCSVM [22] OneClass Support Vector Machines EnsembleIForest [23] Isolation Forest Feature bagging [24] Subset of features are used Deep learningAutoEncoder [25] Fully connected AutoEncoder VAE [26] Variational AutoEncoder 2 R ELATED WORKS "
486,Learning with Noisy Labels over Imbalanced Subpopulations.txt,"Learning with Noisy Labels (LNL) has attracted significant attention from the
research community. Many recent LNL methods rely on the assumption that clean
samples tend to have ""small loss"". However, this assumption always fails to
generalize to some real-world cases with imbalanced subpopulations, i.e.,
training subpopulations varying in sample size or recognition difficulty.
Therefore, recent LNL methods face the risk of misclassifying those
""informative"" samples (e.g., hard samples or samples in the tail
subpopulations) into noisy samples, leading to poor generalization performance.
  To address the above issue, we propose a novel LNL method to simultaneously
deal with noisy labels and imbalanced subpopulations. It first leverages sample
correlation to estimate samples' clean probabilities for label correction and
then utilizes corrected labels for Distributionally Robust Optimization (DRO)
to further improve the robustness. Specifically, in contrast to previous works
using classification loss as the selection criterion, we introduce a
feature-based metric that takes the sample correlation into account for
estimating samples' clean probabilities. Then, we refurbish the noisy labels
using the estimated clean probabilities and the pseudo-labels from the model's
predictions. With refurbished labels, we use DRO to train the model to be
robust to subpopulation imbalance. Extensive experiments on a wide range of
benchmarks demonstrate that our technique can consistently improve current
state-of-the-art robust learning paradigms against noisy labels, especially
when encountering imbalanced subpopulations.","Deep Neural Networks (DNNs) have achieved remark able progress in various domains, including computer vi sion [19], health care [27], natural language processing [36], etc. In practice, training datasets may contain non negligible label noise caused by human annotators‚Äô errors. *Equal contribution. ‚Ä° Work done during an internship at Tencent AI Lab. yCorresponding authors.Therefore, training against noisy labels becomes a criti cal problem in realworld DNN deployment and has at tracted signiÔ¨Åcant attention from the research communities [1, 3, 39]. In recent years, numerous works aim to develop robust learning paradigms to combat label noise [11,16,20]. Among those, estimated clean probabilities are critical for robust training. For example, Bootstrapping [30] assigns smaller weights to the loss of possible noisy samples. Co teaching [11] maintains two DNN models, wherein one model is only trained by clean samples selected by another. Many StateOfTheArt (SOTA) methods estimate the clean probabilities based on the assumption that correctly labeled samples tend to have ‚Äúsmall loss‚Äù. For example, Dividemix [20] assumes the loss of clean and noisy samples following two Gaussian distributions, while the clean distribution has a smaller mean than the noisy one. Therefore, it utilizes a twocomponent Gaussian Mixture Model (GMM) to model and separate clean and noisy samples. Felidae Canidae Figure 1. A demonstration of the learning with noisy labels over imbalanced subpopulations setup. In the CanidaeFelidae classiÔ¨Å cation problem, the head subpopulations include dog and cat. The tail subpopulations include wolf and tiger. There also exist misla beled samples, e.g., cats mislabeled as Canidae or dogs mislabeled as Felidae. The size of the images indicates the sample size of the corresponding subpopulation. Although models trained with these methods can achieve high average performance on the overall population, they may underperform drastically in some subpopulations.1 The situation could become worse when the training dataset consists of ‚Äúimbalanced‚Äù subpopulations. Here, ‚Äúimbal anced‚Äù means training subpopulations vary in sample size 1We use ‚Äúsubpopulation‚Äù and ‚Äúgroup‚Äù interchangeably. 1arXiv:2211.08722v1  [cs.LG]  16 Nov 2022or recognition difÔ¨Åculty. Taking a CanidaeFelidae binary classiÔ¨Åcation problem as an example: the target task is to classify images into two classes, namely Canidae and Feli dae, where each class consists of different subpopulations. As shown in Fig. 1, there are two subpopulations in each class (the size of the images indicates the sample size of the corresponding subpopulation). There are also some noisy samples that adversely affect the training process. In such a problem, DNNs can easily overÔ¨Åt on samples in the head subpopulation (e.g., dogs) while the tail samples (e.g., wolfs) and the noisy samples (e.g., cats mislabeled into ‚ÄúCanidae‚Äù class) tend to have large classiÔ¨Åcation loss. Therefore, previous LNL methods would face the risk of misclassifying tail subpopulations into noisy samples, ag gravating the damage of subpopulation imbalance. We fur ther perform empirical studies on the corrupted Waterbird dataset to quantitatively demonstrate our point. SpeciÔ¨Åcally, the representative LNL method (green bar), similar to the ERM baseline (dotted black line), performs much worse on tail subpopulations, as shown in Fig. 2(a). We suggest the cause is its bad noise identiÔ¨Åcation performance on tail sub populations as in Fig. 2(b). /uni0000002b/uni00000048/uni00000044/uni00000047 /uni00000037/uni00000044/uni0000004c/uni0000004f /uni00000036/uni00000058/uni00000045/uni00000053/uni00000052/uni00000053/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni00000028/uni00000035/uni00000030/uni00000003/uni00000045/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048 /uni00000036/uni00000032/uni00000037/uni00000024/uni00000003/uni0000002f/uni00000031/uni0000002f/uni00000003/uni00000050/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047 /uni00000032/uni00000058/uni00000055/uni00000056 /uni0000002b/uni00000048/uni00000044/uni00000047 /uni00000037/uni00000044/uni0000004c/uni0000004f /uni00000036/uni00000058/uni00000045/uni00000053/uni00000052/uni00000053/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000024/uni00000038/uni00000026/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000053/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni00000036/uni00000032/uni00000037/uni00000024/uni00000003/uni0000002f/uni00000031/uni0000002f/uni00000003/uni00000050/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047 /uni00000032/uni00000058/uni00000055/uni00000056 Figure 2. (a) ClassiÔ¨Åcation accuracy and (b) Noise identiÔ¨Åca tion AUC of Empirical Risk Minimization (ERM), SOTA LNL method, and our method under the problem of noisy labels over imbalanced subpopulations. The dataset is the corrupted Water birds dataset under 30% noise rate. We use an improved version of DivideMix as a representative LNL method, whose details are introduced in Sec. 4. This paper studies an underexplored novel problem, i.e., learning with noisy labels over imbalanced subpopulations. As shown by the above discussions and observations, using the classiÔ¨Åcation loss (e.g., crossentropy) is inadequate to discriminate between the tail and noisy samples in this prob lem. Therefore, we resort to another way to estimate the label conÔ¨Ådence (i.e., the probability of a label being clean) by considering the sample relationship in feature space. To this end, we introduce a label conÔ¨Ådence estimation crite rion named Local Label Consistency (LLC). SpeciÔ¨Åcally, the LLC of one given sample is obtained by counting the number of samples in the featurespace nearest neighbor hood whose labels are consistent with the given sample.The design idea behind the LLC metric is that samples from the same class typically come with high feature similarity. Utilizing this idea, we employ Gaussian Mixture Models (GMM) for clustering and mapping LLC scores to label conÔ¨Ådence. Next, the label refurbishment framework uses label conÔ¨Ådence as a weight to correct the given noisy label. We further integrate the refurbished labels into Distribution ally Robust Optimization (DRO) [32,44,46], i.e., construct ing the worstcase loss based on estimated label conÔ¨Ådence. Empirically, we Ô¨Ånd the proposed refurbishDRO paradigm can achieve better tail performance. Through extensive experiments on corrupted Waterbird and CelebA datasets, where subpopulation imbalance and label noise coexist, we show the proposed method consis tently improves the performance on tail subpopulations in contrast to current SOTA methods. As Fig. 2 shows, due to the introduction of LLCbased label conÔ¨Ådence estima tion and refurbishDRO paradigm, our method (blue bar) achieves better classiÔ¨Åcation performance and noise iden tiÔ¨Åcation performance than previous methods (green bar) (details are in Sec. 4). Furthermore, our method also im proves current stateoftheart robust training paradigms on standard LNL benchmarks with possible implicit subpopu lation imbalance, including CIFAR, MiniWebVision, and ANIMAL10N. Our main contributions are summarized as follows: ‚Ä¢ We introduce a novel problem, i.e., learning with noisy labels over imbalanced subpopulations, which has been less explored in the community. Moreover, through empirical studies, we demonstrate how previ ous LNL methods underperform in this new problem. ‚Ä¢ We propose a general framework for this problem. The basic idea of the framework lies in a simple yet effec tive strategy that estimates label conÔ¨Ådence based on sample relations. The label conÔ¨Ådence is then put into our proposed refurbishDRO paradigm to further im prove robustness. ‚Ä¢ To verify the effectiveness of our method, we con duct extensive experiments on various datasets, includ ing corrupted datasets with imbalanced subpopulations and noisy labels (Waterbirds and CelebA) and standard LNL benchmark datasets (CIFAR, MiniWebVision, and ANIMAL10N). We observe that the proposed method consistently outperforms previous methods. 2. Related works "
487,Human-like Clustering with Deep Convolutional Neural Networks.txt,"Classification and clustering have been studied separately in machine
learning and computer vision. Inspired by the recent success of deep learning
models in solving various vision problems (e.g., object recognition, semantic
segmentation) and the fact that humans serve as the gold standard in assessing
clustering algorithms, here, we advocate for a unified treatment of the two
problems and suggest that hierarchical frameworks that progressively build
complex patterns on top of the simpler ones (e.g., convolutional neural
networks) offer a promising solution. We do not dwell much on the learning
mechanisms in these frameworks as they are still a matter of debate, with
respect to biological constraints. Instead, we emphasize on the
compositionality of the real world structures and objects. In particular, we
show that CNNs, trained end to end using back propagation with noisy labels,
are able to cluster data points belonging to several overlapping shapes, and do
so much better than the state of the art algorithms. The main takeaway lesson
from our study is that mechanisms of human vision, particularly the hierarchal
organization of the visual ventral stream should be taken into account in
clustering algorithms (e.g., for learning representations in an unsupervised
manner or with minimum supervision) to reach human level clustering
performance. This, by no means, suggests that other methods do not hold merits.
For example, methods relying on pairwise affinities (e.g., spectral clustering)
have been very successful in many scenarios but still fail in some cases (e.g.,
overlapping clusters).","Clustering, a.k.a unsupervised classiÔ¨Åcation or nonparametric density estimation, is central to many datadriven domains and has been studied heavily in the past. The task in clustering is to group a given collection of unlabeled patterns into meaningful clusters such that objects within a cluster are more similar to each other than they are to objects in other clusters. Clustering provides a summary representation of data at a coarse level and is used widely in many disciplines (e.g., computer ver sion, bioinformatics, text processing) for exploratory data analysis (a.k.a pattern mining) as well as representation learning (e.g., bag of words). Despite the introduction of thousands of clustering al gorithms in the past Aggarwal & Reddy (2013), some challenges still remain. For instance, existing algorithms fall short in dealing with different cluster shapes, high dimensions, automatically deter mining the number of clusters or other parameters, large amounts of data, choosing the appropriate similarity measure, incorporating domain knowledge, and cluster evaluation. Further, no clustering algorithm can consistently win over other algorithms, handle all test cases, and perform at the level of humans. Deep neural networks have become a dominant approach to solve various tasks across many Ô¨Åelds. They have been proven successful in several domains including computer vision Krizhevsky et al. (2012), natural language processing Collobert et al. (2011), and speech recognition Dahl et al. (2012) for tasks such as scene and object classiÔ¨Åcation Krizhevsky et al. (2012), pixellevel labeling for image segmentation Long et al. (2015); Zheng et al. (2015), modeling attention Borji & Itti (2013); Borji et al. (2013), image generation Goodfellow et al. (2014), robot arm control Levine et al. (2015), Authors contributed equally. 1arXiv:1706.05048v2  [cs.LG]  11 Dec 2017speech recognition Graves & Jaitly (2014), playing Atari games Mnih et al. (2015) and beating the Go champion. Deep Convolutional Neural Networks (CNNs) LeCun et al. (1998) have been particularly successful over vision problems. One reason is that nearby pixels in natural scenes are highly correlated. Further natural objects are compositional. These facts allow applying the same Ô¨Ålters across spatial locations (and hence share weights), and build complex Ô¨Ålters from simpler ones to detect high level patterns (e.g., object parts, objects). We advocate that these properties are highly appealing when dealing with clustering problems. For instance, the classic two half moons example can be solved by applying a Ô¨Ålter that is selective to each half moon. Or, when two clusters with different shapes overlap, the problem can be solved by having Ô¨Ålters responding to each shape. Solving these cases is very challenging by just looking at local regions around points and being blind to the highlevel patterns. Incorporating domain knowledge, while working in some cases, does not give a general solution for solving all clustering problems. The human visual system easily solves these 2D problems because it is a general system with a rich set of learned or evolved Ô¨Ålters. We believe that deep CNNs, although imperfect models of the human vision as they lack feedback and lateral connections carry a huge promise for solving clustering tasks. Further, as we will argue, they offer a uniÔ¨Åed solution to both classiÔ¨Åcation and clustering tasks. The current demarcation between classiÔ¨Åcation and clustering becomes murky when we notice that researchers often refer to human judgments in evaluating the outcomes of clustering algorithms. Indeed, humans learn quite a lot about the visual world during their life time. Moreover, the structure of the visual system has been Ô¨Ånetuned through the evolution. Thus, certainly, there is a learning component involved which has been often neglected in formulating clustering algorithms. While this is sensible from an application point of view (e.g., pattern mining), not only it limits the pursuit for stronger algorithms but also narrows our understanding of human vision. Learning techniques have been utilized for clustering in the past (e.g., Bach & Jordan (2004); Pin heiro et al. (2016)), for example for tuning parameters (e.g., Bach & Jordan (2004)). Deep networks have also been exploited for clustering (e.g., Hsu & Kira (2015); Hershey et al. (2016); Wang et al. (2016)). However, to our knowledge, while CNNs have been already adopted for image segmen tation, so far they have not been exploited for generic clustering. Our goal is to investigate such possibility. To this end, instead of borrowing from clustering to do image segmentation, we follow the opposite direction and propose a deep learning based approach to clustering. Our method builds on the fully convolutional network literature, in particular, recent work on edge detection and semantic segmentation which utilize multiscale local and nonlocal cues Ronneberger et al. (2015). Thanks to a high volume of labeled data, high capacity of deep networks, powerful optimization algorithms, and high computational power, deep models win on these tasks. We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects (e.g., Yamins et al. (2014); DiCarlo & Cox (2007); LeCun et al. (1998); Krizhevsky et al. (2012); Borji & Itti (2014). Our study enriches our understanding of the concept of clustering and its relation to classiÔ¨Åcation. 2 R ELATED WORK "
488,System Identification of NN-based Model Reference Control of RUAV during Hover.txt,"UAV control system is a huge and complex system, and to design and test a UAV
control system is time-cost and money-cost. This paper considered the
simulation of identification of a nonlinear system dynamics using artificial
neural networks approach. This experiment develops a neural network model of
the plant that we want to control. In the control design stage, experiment uses
the neural network plant model to design (or train) the controller. We use
Matlab to train the network and simulate the behavior. This chapter provides
the mathematical overview of MRC technique and neural network architecture to
simulate nonlinear identification of UAV systems. MRC provides a direct and
effective method to control a complex system without an equation-driven model.
NN approach provides a good framework to implement MEC by identifying
complicated models and training a controller for it.","Unmanned aerial vehicles (UAVs)  are becoming more and more popular in a wide field of  applications nowadays. UAVs are used in number of military application for gathering  information and military attacks. In the future will likely see unmanned aircraft employed,  offensively, for bombing and ground attack. As a tool for research and rescue, UAVs can  help find humans lost in the wilderness, trapped in collapsed buildings, or drift at sea. It is  also used in civil application in fire station, police observation of crime disturbance and  natural disaster prevention, where the human ob server will be risky to fight the fire. There  is wide variety of UAV shapes, sizes, configuration and characteristics. Therefore, there is a  growing demand for UAV control systems, and many projects either commercial or  academic destined to design a UAV autopilot were held recently. A lot of impressive results  had already been achieved, and many UAVs, mo re or less autonomous, are used by various  organizations.  An Artificial Neural Network (ANN) [3] is an information processing paradigm that is  stimulated by the way biological nervous systems, such as the brain, process information.  The key element of this paradigm is the nove l structure of the information processing  system.  Basically, a neural network (NN) is composed of a set of nodes (Fig. 1). Each node is  connected to the others via a set of links. Info rmation is transmitted from the input to the  output cells depending of the strength of the links. Usually, neural networks operate in two  phases. The first phase is a learning phase where each of the nodes and links adjust their  strength in order to match with the desired outp ut. A learning algorithm is in charge of this  process. When the learning phase is complete , the NN is ready to recognize the incoming  information and to work as a pattern recognition system.  ANNs, like people, learn by example. An ANN is configured for a specific application, such  as pattern recognition or data classification , through a learning process. Learning in  biological systems involves adjustments to the synaptic connections that exist between t he  neurons.   www.intechopen.com A r t i f i c i a l  N e u r a l  N e t w o r k s    I n d u s t rial and Control Engineering Applications   396  In recent years, there is a wide momentum of ANNs in the contr ol system arena, to design  the UAVs. Any system in which input is not pr oportional to output is known as nonlinear  systems. The main advantages of ANNs are having the processing ability to model  nonlinear systems. ANNs are very suitable for identification of nonlinear dynamic systems.  Multilayer Perceptron model have been used to  model a large number of nonlinear plants.  We can vary the number of hidden layers to minimize the mean square error. ANNs has  been used to formulate a variety of control st rategies [1] [2]. The NN approach is a good  alternative for physical modeling techniques for nonlinear systems.      Fig. 1. General Neural Network Architecture  A fundamental difficulty of many nonlinear control systems, which potentially could  deliver better performance, is extremely difficul t to theoretically predict the behavior of a  system under all possible circumstances. In fact, even design envelope of a controller often  remains largely uncertain. Therefore, it become s a challenging task to verify and validate the  designed controller under all possible flight conditions. A practical solution to this pro blem  is extensive testing of the system. Possibly th e most expensive design items are the control  and navigation systems. Therefor e, one of main questions that each system designer has to  face is the selection of appropriate hardware for UAV system. Such hardware should satisfy  the main requirements without contravening th eir boundaries in terms of quality and cost.  In UAV design this kind of consideration is especially important due to the safety  requirements expressed in airw orthiness standards. Therefore question is how to find the  optimal solution. Thus, simulation  is necessary. Basically there are two type of simulation is  needed while designing UAVs sy stems, they are SoftwareInth eLoop (SIL) [5] simulation  and HardwareIntheLoop (HIL) simulation [4].  To utilize the SIL configuration, the uncompiled software source code, which normally runs  on the onboard computer, is comp iled into the simulation tool itself, allowing this software  to be tested on the simulation host computer. This allows the flight software to be tested  without the need to tieup the flight hardware , and was also used in selection of hardware.  HILS simulates (Fig. 2) a process such that  input and output signals show the time dependent values as realtime operating compon ents. It is possible to test embedded system  under real time with various test conditions. It provides the UAV developer to test many  aspects of autopilot hardware, finding the real time problems, test the reliability, and many  more.  The simulation can be done with the help of Matlab Simulink program environment. This  program can be considered as a facility fully competent for this task.  Simulink is the most    www.intechopen.comSystem Identification of NNbased Model Reference Control of RUAV during Hover     397    Fig. 2. UAV Architecture: Hardwareintheloop Simulation  popular tool, it was not only used for a SIL Simulation of the complete UAV system but also  to create the simulation code of a HI L Simulator that runs in real time.  The system identification is the first and crucial step for the design of the controller,  simulation of the system and so on. Frequently it is necessary to analyze the flight data in  the frequency domain to identify the UAV sy stem. This paper demonstrates how ANN can  be used for non linear identification and contro ller design. The simulation processes consists  of designing a simple system, and simulates th at system with the help of model reference  control block in Matlab/Simulink [6].  The paper is organized as follows: Section 2 describes some related work. Section 3 deals  with system identification and control on th e basis of NNs. Details of design and control  system with NNs approaches is describes in section 4. In section 5, simulations are  performed on RUAVs system and finally, conclusions are drawn in section 6.  2. Related work  "
489,How to best use Syntax in Semantic Role Labelling.txt,"There are many different ways in which external information might be used in
an NLP task. This paper investigates how external syntactic information can be
used most effectively in the Semantic Role Labeling (SRL) task. We evaluate
three different ways of encoding syntactic parses and three different ways of
injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling
model. We show that using a constituency representation as input features
improves performance the most, achieving a new state-of-the-art for
non-ensemble SRL models on the in-domain CoNLL'05 and CoNLL'12 benchmarks.","Properly integrating external information into neu ral networks has received increasing attention re cently (Wu et al., 2018; Li et al., 2017; Strubell et al., 2018). Previous research on this topic can be roughly categorized into three classes: i) In put: The external information are presented as ad ditional input features (i.e., dense realvalued vec tors) to the neural network (Collobert et al., 2011). ii) Output : The neural network is trained to pre dict the main task and the external information in a multitask approach (Changpinyo et al., 2018). iii) Autoencoder : This approach, recently proposed by Wu et al. (2018), simultaneously combines the Input andOutput during neural models training. The simplicity of these methods allow them to ap ply to many NLP sequence tasks and various neu ral model architectures. However, previous studies often focus on inte grating wordlevel shallow features such as POS or chunk tags into the sequence labelling tasks. Syntactic information, which encodes the long range dependencies and global sentence structure, has not been studied as carefully. This paper Ô¨Ålls 1Our model source code is available in https:// github.com/GaryYufei/bestParseSRLthis gap by integrating syntactic information to the sequence labelling task. We address three ques tions: 1)How should syntactic information be en coded as wordlevel features? 2)What is the best way of integrating syntactic information? and3) What effect does the choice of syntactic represen tation have on the performance? We study these questions in the context of Se mantic Role Labelling (SRL). A SRL system ex tracts the predicateargument structure of a sen tence.2Syntax was an essential component of early SRL systems (Xue and Palmer, 2004; Pun yakanok et al., 2008). The stateoftheart neu ral SRL systems use a neural sequence labelling model without any syntax knowledge (He et al., 2018, 2017; Tan et al., 2018). We show below that injecting external syntactic knowledge into a neu ral SRL sequence labelling model can improve the performance, and our best model sets a new state oftheart for a nonensemble SRL system. In this paper we express the external syntac tic information as vectors of discrete features, be cause this enables us to explore different ways of injecting the syntactic information into the neural SRL model. SpeciÔ¨Åcally, we propose three dif ferent syntax encoding methods: a)a full con stituency tree representation ( FullC );b)an SRL speciÔ¨Åc span representation ( SRLC ); and c)a dependency tree representation ( Dep). For (a) we adapt the constituency parsing representation from (G ¬¥omezRodr ¬¥ƒ±guez and Vilares, 2018) and encode the tree structure as a set of features for word pairs. For (b), we use a categorical repre sentation of the constituency spans that are most relevant to SRL tasks based on (Xue and Palmer, 2004). Finally, (c)we propose a discrete vector representation that encodes the headmodiÔ¨Åer re lationships in the dependency trees. We evaluate the effectiveness of these encod ings using three different integration methods on 2who didwhat towhom ,where andwhenarXiv:1906.00266v1  [cs.CL]  1 Jun 2019the SRL CoNLL‚Äô05 andCoNLL‚Äô12 benchmarks. We show that using either of the constituency representations in either the Input or the Auto Encoder conÔ¨Ågurations produces the best perfor mance. These results are noticeably better than a strong baseline and set a new stateoftheart for nonensemble SRL systems. 2 Related Work "
490,ReINTEL Challenge 2020: A Multimodal Ensemble Model for Detecting Unreliable Information on Vietnamese SNS.txt,"In this paper, we present our methods for unrealiable information
identification task at VLSP 2020 ReINTEL Challenge. The task is to classify a
piece of information into reliable or unreliable category. We propose a novel
multimodal ensemble model which combines two multimodal models to solve the
task. In each multimodal model, we combined feature representations acquired
from three different data types: texts, images, and metadata. Multimodal
features are derived from three neural networks and fused for classification.
Experimental results showed that our proposed multimodal ensemble model
improved against single models in term of ROC AUC score. We obtained 0.9445 AUC
score on the private test of the challenge.","Recently, fake news detection have received much attention in both NLP and data mining re search community. This year, for the Ô¨Årst time, VLSP 2020 Evaluation Campaign organizers held ReINEL Challenge (Le et al., 2020) to encourage the development of algorithms and systems for de tecting unreliable information on Vietnamese SNS. In ReINTEL Challenge 2020, we need to deter mine a piece of information containing texts, im ages, and metadata is reliable or unreliable. The task is formalized as a binary classiÔ¨Åcation prob lem and training data with unreliable/reliable labels was provided by VLSP 2020 organizers. In this paper, we present a novel multimodal ensemble model for identifying unreiable informa tion on Vietnamese SNS. We use neural networks to obtain feature representations from different data types. Multimodal features are fused and put into a sigmoid layer for classiÔ¨Åcation. SpeciÔ¨Åcally, we use BERT model to obtain feature representations from texts, a multilayer perceptron to encode metadata and textbased features, and a Ô¨Ånetuned VGG 19 network to obtain feature representations from images. We combined two single models in order to improve the accuracy of fake news detection. Our proposed model obtained 0.9445 ROC AUC score on the private test of the challenge. 2 Related Work "
491,Structured learning of metric ensembles with application to person re-identification.txt,"Matching individuals across non-overlapping camera networks, known as person
re-identification, is a fundamentally challenging problem due to the large
visual appearance changes caused by variations of viewpoints, lighting, and
occlusion. Approaches in literature can be categoried into two streams: The
first stream is to develop reliable features against realistic conditions by
combining several visual features in a pre-defined way; the second stream is to
learn a metric from training data to ensure strong inter-class differences and
intra-class similarities. However, seeking an optimal combination of visual
features which is generic yet adaptive to different benchmarks is a unsoved
problem, and metric learning models easily get over-fitted due to the scarcity
of training data in person re-identification. In this paper, we propose two
effective structured learning based approaches which explore the adaptive
effects of visual features in recognizing persons in different benchmark data
sets. Our framework is built on the basis of multiple low-level visual features
with an optimal ensemble of their metrics. We formulate two optimization
algorithms, CMCtriplet and CMCstruct, which directly optimize evaluation
measures commonly used in person re-identification, also known as the
Cumulative Matching Characteristic (CMC) curve.","The task of person reidentiÔ¨Åcation (reid) is to match pedes trian images observed from different and disjoint camera views. Despite extensive research efforts in reid [9, 36, 48, 51, 44, 47, 33, 20, 46], the problem itself is still a very challenging task due to (a) large variation in visual appearance (person‚Äôs appearance often undergoes large variations across different camera views); (b) signiÔ¨Åcant changes in human poses at the time the image was captured; (c) large amount of illumination changes, back ground clutter and occlusions; d) relatively low resolution and the different placement of the cameras. Moreover, the problem becomes increasingly difÔ¨Åcult when there are high variations in pose, camera viewpoints, and illumination, etc. To address these challenges, existing research has concen trated on the development of sophisticated and robust features to describe visual appearance under signiÔ¨Åcant changes. Most of them use appearancebased features that are viewpoint in variant such as color and texture descriptors [7, 9, 11, 39, 23]. However, the system that relies heavily on one speciÔ¨Åc type of visual cues, e.g., color, texture or shape, would not be practical Corresponding author. Email address: chhshen@gmail.com (Chunhua Shen )and powerful enough to discriminate individuals with similar visual appearance. Some studies have tried to address the above problem by seeking a combination of robust and distinctive fea ture representation of person‚Äôs appearance, ranging from color histogram [11], spatial cooccurrence representation [39], LBP [44], to color SIFT [47]. The basic idea of exploiting multiple visual features is to build an ensemble of metrics (distance func tions), in which each distance function is learned using a sin gle feature and the Ô¨Ånal distance is calculated from a weighted sum of these distance functions [7, 44, 47]. These works often predeÔ¨Åne distance weights, which need to be reestimated be forehand for different data sets. However, such a predeÔ¨Åned principle has some drawbacks. ‚Ä¢ Different realworld reid scenarios can have very dif ferent characteristics, e.g., variation in view angle, light ing and occlusion. Simply combining multiple distance functions using predetermined weights may be undesir able as highly discriminative features in one environment might become irrelevant in another environment. ‚Ä¢ The effectiveness of distance learning heavily relies on the quality of the feature selected, and such selection re quires some domain knowledge and expertise. ‚Ä¢ Given that certain features are determined to be more re Preprint submitted to Elsevier 13102018arXiv:1511.08531v2  [cs.CV]  24 May 2016liable than others under a certain condition, applying a standard distance measure for each individual match is undesirable as it treats all features equally without differ entiation on features. In these ends, it necessarily demands a principled approach that is able to automatically select and learn weights for diverse met rics, meanwhile generic yet adaptive to different scenarios. Person reidentiÔ¨Åcation problem can also be cast as a learn ing problem in which either metrics or discriminative models are learned [4, 5, 16, 17, 42, 43, 44, 20, 41, 27, 49], which typically learn a distance measure by minimizing intraclass distance and maximizing interclass distance simultaneously. Thereby, they require sufÔ¨Åcient labeled training data from each class1and most of them also require new training data when camera settings change. Nonetheless, in person reid bench mark, available training data is relatively scarce, and thus in herently undersampled for building a representative class dis tribution. This intrinsic characteristic of person reid problem makes metric learning pipelines easily overÔ¨Åtted and unable to be applicable in small image sets. To combat above difÔ¨Åculties simultaneously, in this paper, we introduce two structured learning based approaches to per son reid by learning weights of distance functions for lowlevel features. The Ô¨Årst approach, CMCtriplet, optimizes the relative distance using the triplet units, each of which contains three person images, i.e., one person with a matched reference and a mismatched reference. Treating these triplet units as input, we formulate a large margin framework with triplet loss where the relative distance between the matched pair and the mismatched pair tends to be maximized. An illustration of CMCtripletis shown in Fig. 1. This triplet based model is more natural for person reid mainly because the intraclass and interclass varia tion may vary signiÔ¨Åcantly for different classes, making it inap propriate to require the distance between a matched/mismatched pair to fall within an absolute range [49]. Also, training images in person reid are relatively scarce, whereas the tripletbased training model is to make comparison between any two data points rather than comparison between any data distribution boundaries or among clusters of data. This thus alleviates the overÔ¨Åtting problem in person reid given undersampled data. The second approach, CMCtop, is developed to maximize the average rank krecognition rate, in which kis chosen to be small, e.g., k<10. Setting the value of kto be small is cru cial for many realworld applications since most surveillance operators typically inspect only the Ô¨Årst ten or twenty items retrieved. Thus, we directly optimize the testing performance measure commonly used in CMC curve, i.e., the recognition rate at rank kby using structured learning. The main contributions of this paper are threefold: ‚Ä¢ We propose two principled approaches, CMCtripletand CMCtop, to build an ensemble of person reid metrics. The standard approach CMCtripletis developed based on triplet information, which is more tolerant to large intra 1Images of each person in a training set form a class.and interclass variations, and alleviate the overÔ¨Åtting problem. The second approach of CMCtopdirectly op timizes an objective closer to the testing criteria by maxi mizing the correctness among top kmatches using struc tured learning, which is empirically demonstrated to be more beneÔ¨Åcial to improving recognition rates. ‚Ä¢ We perform feature quantiÔ¨Åcation by exploring the ef fects of diverse feature descriptors in recognizing per sons in different benchmarks. An ensemble of metrics is formulated into a late fusion paradigm where a set of weights corresponding to visual features are automati cally learned. This late fusion scheme is empirically stud ied to be superior to various early fusions on visual fea tures. ‚Ä¢ Extensive experiments are carried out to demonstrate that by building an ensemble of person reid metrics learned from different visual features, notable improvement on rank 1recognition rate can be obtained. In addition, our ensemble approaches are highly Ô¨Çexible and can be com bined with linear and nonlinear metrics. For nonlinear base metrics, we extend our approaches to be tractable and suitable to largescale benchmark data sets by ap proximating the kernel learning. 2. Related Work "
492,Attention-Aware Noisy Label Learning for Image Classification.txt,"Deep convolutional neural networks (CNNs) learned on large-scale labeled
samples have achieved remarkable progress in computer vision, such as
image/video classification. The cheapest way to obtain a large body of labeled
visual data is to crawl from websites with user-supplied labels, such as
Flickr. However, these samples often tend to contain incorrect labels (i.e.
noisy labels), which will significantly degrade the network performance. In
this paper, the attention-aware noisy label learning approach ($A^2NL$) is
proposed to improve the discriminative capability of the network trained on
datasets with potential label noise. Specifically, a Noise-Attention model,
which contains multiple noise-specific units, is designed to better capture
noisy information. Each unit is expected to learn a specific noisy distribution
for a subset of images so that different disturbances are more precisely
modeled. Furthermore, a recursive learning process is introduced to strengthen
the learning ability of the attention network by taking advantage of the
learned high-level knowledge. To fully evaluate the proposed method, we conduct
experiments from two aspects: manually flipped label noise on large-scale image
classification datasets, including CIFAR-10, SVHN; and real-world label noise
on an online crawled clothing dataset with multiple attributes. The superior
results over state-of-the-art methods validate the effectiveness of our
proposed approach.","CNNs have triumphed over many vision tasks. However, the overwhelming performances of CNNs heavily rely on largescale highquality labeled data, e.g., ImageNet [19], which are typically laborious and costly to collect and an Examples of noisy labels   airplane    ant   car   dog   watch    brain   elephant  scissor  apple  watch  panda  snail  Figure 1. Examples of noisy labels. The image annotations that are manually labeled by amateurs or automatically generated by a machine are not reliable. Worse, the noisy labels and the number of mislabeled images are not speciÔ¨Åed. notate. Nevertheless, there are millions of freely available images with usersupplied labels that can be easily accessed from the web. Although utilizing web images has become a popular research direction in the Ô¨Åeld of large scale im age recognition, the performance is obviously inferior to its counterpart on Ô¨Ånely labeled data. Directly using image sets with a high proportion of noisy labels (e.g., Fig. 1) can even degrade the performance of Ô¨Ånelytrained CNN mod els [6, 24, 26]. Thus, it is highly desired to design a network that is able to mitigate the impact of noisy labels. There are already several label noiserobust algorithms being developed in recent years. Some researchers propose robust loss functions speciÔ¨Åcally for noisy image classiÔ¨Åca tion [16, 21], others try to improve the quality of training data by predicting the label noise type or removing the mis labeled samples [24, 22]. However, these methods either work worse under large proportions of label noise or re quire prior knowledge on the patterns of label noise. There are also CNNbased methods explicitly modeling the noisy distributions by a noise layer [20, 12, 5]. However, these methods are usually based on the assumption that the dis turbance of all samples in the same class is equal, thus they are unable to acquire diverse noisy information, e.g., some 1arXiv:2009.14757v1  [cs.CV]  30 Sep 2020furry dogs are easy to be labeled as cats and some large ones as horses, which are very common for online images with usersupplied labels. In this work, we propose an attentionaware noisy label learning approach, termed A2NL, to improve the discrim inative capability of noiserobust network. In particular, a NoiseAttention (NA) model is proposed to explore multi ple distributions of label noise and a recursive learning strat egy is employed to further boost its learning ability. Both contributions can be applied to any conventional CNNs. To avoid confusion, the network with the NA model is infor mally called attention network in the following. Unlike pre vious works which describe the noisylabel information of an image set by one distribution, our NA model contains multiple units, each of which pays attention to a speciÔ¨Åc noisylabel distribution. The reasons for such an improve ment are two folds. 1) The noisy levels of different classes vary a lot, e.g., in the task of predicting clothes color, it is likely to mix the orange and brown while it is almost impossible to label a red one as a blue one. 2) The indi vidual images of the same class can even present different disturbance. For example, some dogs could be clearly rec ognized, some may be labeled as cats or horses. By mod eling multiple noisylabel distributions, the proposed NA model can not only portray the different noisy levels among classes, but also distinguish the diverse noisylabel distribu tions among images. The proposed recursive learning strategy is inspired by [25, 7, 3] that the soft predictions of a welltrained classiÔ¨Åer usually contain rich information. The soft outputs not only indicate the object class of the input image, but also reÔ¨Çect the relations among classes. For example, if a cloth sample is predicted as an orange one with the conÔ¨Ådence of 80%, a brown one with 15%, and 5% for other classes. The biggest Ô¨Ågure (80%) advocates that the input image contains a cloth in orange, and other Ô¨Ågures suggest that the orange is highly possible to be mixed up with the brown. Thus, to boost the learning ability of the proposed attention network, we re cursively train it by distilling the knowledge from a well trained attention network in the previous iteration. To be speciÔ¨Åc, the outputs of the attention network in the previ ous iteration, coupling with the given training labels, are combined as the training supervisions for the network in the current iteration. Different from directly combining multi ple network models, the recursive learning strategy is able to assemble the network knowledge in previous iterations without introducing more parameters. We conduct extensive experiments on both datasets with synthesized noisy labels (randomly Ô¨Çipping the labels) in cluding SVHN [17], CIFAR10 [13], and a realworld clothes dataset with multiple attributes [8] which naturally contains mislabeled samples. As considering both general and speciÔ¨Åc label noise simultaneously, the proposed framework shows excellent effectiveness and robustness to both synthesized and realworld label noise. Our main contribu tions are summarized as follows: A NoiseAttention model with multiple noisespeciÔ¨Åc units is proposed to explore various distributions of la bel noise, which can be applied to conventional CNNs. A recursive learning strategy, which could assemble the highlevel knowledge learned from multiple net works, is introduced to boost the attention network learning ability. Extensive experiments on both manually Ô¨Çipped label noise and realworld label noise demonstrate the ex cellent effectiveness and robustness of our attention aware noisy label learning framework. 2. Related Works "
493,Automatic Neural Network Hyperparameter Optimization for Extrapolation: Lessons Learned from Visible and Near-Infrared Spectroscopy of Mango Fruit.txt,"Neural networks are configured by choosing an architecture and hyperparameter
values; doing so often involves expert intuition and hand-tuning to find a
configuration that extrapolates well without overfitting. This paper considers
automatic methods for configuring a neural network that extrapolates in time
for the domain of visible and near-infrared (VNIR) spectroscopy. In particular,
we study the effect of (a) selecting samples for validating configurations and
(b) using ensembles.
  Most of the time, models are built of the past to predict the future. To
encourage the neural network model to extrapolate, we consider validating model
configurations on samples that are shifted in time similar to the test set. We
experiment with three validation set choices: (1) a random sample of 1/3 of
non-test data (the technique used in previous work), (2) using the latest 1/3
(sorted by time), and (3) using a semantically meaningful subset of the data.
Hyperparameter optimization relies on the validation set to estimate test-set
error, but neural network variance obfuscates the true error value. Ensemble
averaging - computing the average across many neural networks - can reduce the
variance of prediction errors.
  To test these methods, we do a comprehensive study of a held-out 2018 harvest
season of mango fruit given VNIR spectra from 3 prior years. We find that
ensembling improves the state-of-the-art model's variance and accuracy.
Furthermore, hyperparameter optimization experiments - with and without
ensemble averaging and with each validation set choice - show that when
ensembling is combined with using the latest 1/3 of samples as the validation
set, a neural network configuration is found automatically that is on par with
the state-of-the-art.","This paper considers how to automatically con gure neural network hyperparameters such that it extrapolates in time for visible and nearinfrared (VNIR) spectroscopy. Hyperparameter optimiza tion (HPO) is a signicant undertaking. Neural networks are congured by choosing an architec ture (such as number of layers) and hyperparame ter values (such as learning rate), all of which may be optimized at once during HPO. Even when us ing stateoftheart Bayesian optimization software, Corresponding author Email addresses: mcdirks@cs.ubc.ca (Matthew Dirks), poole@cs.ubc.ca (David Poole)HPO still involves many decisions and intuitions (some of which are explained in a recent tutorial [1]). This paper is about further streamlining the process of hyperparameter optimization in order to do so automatically, without overtting, and in a manner that mimics an expertlytuned model. A dataset is partitioned into test and nontest samples. Given the nontest samples, the goal is to build a predictor that works the best on the test set. The test set is only used to evaluate nal models. If a neural network is trained on all nontest data it will overt. To avoid overtting, the nontest data is partitioned into calibration and validation sets (in machine learning literature, these are often called training and development sets). The calibration set Preprint submitted to Chemometrics and Intelligent Laboratory Systems October 5, 2022arXiv:2210.01124v1  [eess.IV]  3 Oct 2022is used to train the model and the validation set is used as a proxy of the test set. Neural network hyperparameters are chosen to minimize prediction error on the validation set (in this case, it's sometimes called a tuning set). HPO may overt the validation set and the best method to combat this is an open area of research [2]. To avoid overtting, a combination of expert intuition and handtuning is often used. One approach, re cently studied [3] for chemometrics, is to nd stable optima where the RMSE on the validation set (with respect to the hyperparameters) is wide (doesn't change much with slight perturbations) rather than narrow [2]. We take a complementary approach: encourage the model to extrapolate by the choice of validation samples used in hyperparameter opti mization. Extrapolating in time is often dicult because the future is dierent from the past. The dataset of mangoes by Anderson et al [4, 5] is a good ex ample: Using spectra from 3 years, the goal is to predict dry matter (DM) content in the next year. Thus, we want a neural network conguration that doesn't overt the past but extrapolates well to the future. In previous work, the validation set is 1/3 of nontest data, sampled randomly. We test two alternatives to avoid overtting in HPO and encour age the neural network model to extrapolate: First, we use the latest 1/3 of samples (sorted by time). Second, we use a semantically meaningful subset [6]; specically, the latest harvest season (2017). Due to the stochastic nature of training algo rithms, neural networks have dierent weights and dierent errors each time they're trained. We re port the distributions of RMSE scores for the pur pose of fairly evaluating each method. The variance of errors is also problematic for HPO because the prediction error on the validation set is an estimate of how well the model will perform on the test set and in deployment; a poor estimate leads to a sub optimal neural network conguration. We investigate using ensembles to reduce the variance of validationset error during hyperparam eter optimization. Ensembles (of many kinds) have been shown to improve accuracy, reduce variance, and improve robustness to domain shift [7, 8, 9]. Specically, we obtain an ensemble by reinitializing a neural network randomly and retraining it a number of times [7, 10]; this model reduces the por tion of the variance that is due to random initial ization. To test these methods, we do a comprehensivestudy of a heldout 2018 harvest season of mango fruit given VNIR spectra from 3 prior years [4]. We conduct hyperparameter optimization for each choice of validation set and compare HPO with and without ensemble averaging. The results in this study sheds light on reproducible and automated practices for conguring and training neural net works for spectroscopy; these results can inform practitioners what steps to take in building their own models to make predictions for future samples. 2. Methodologies "
494,"Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters.txt","Motivated by the success of ensembles for uncertainty estimation in
supervised learning, we take a renewed look at how ensembles of $Q$-functions
can be leveraged as the primary source of pessimism for offline reinforcement
learning (RL). We begin by identifying a critical flaw in a popular algorithmic
choice used by many ensemble-based RL algorithms, namely the use of shared
pessimistic target values when computing each ensemble member's Bellman error.
Through theoretical analyses and construction of examples in toy MDPs, we
demonstrate that shared pessimistic targets can paradoxically lead to value
estimates that are effectively optimistic. Given this result, we propose MSG, a
practical offline RL algorithm that trains an ensemble of $Q$-functions with
independently computed targets based on completely separate networks, and
optimizes a policy with respect to the lower confidence bound of predicted
action values. Our experiments on the popular D4RL and RL Unplugged offline RL
benchmarks demonstrate that on challenging domains such as antmazes, MSG with
deep ensembles surpasses highly well-tuned state-of-the-art methods by a wide
margin. Additionally, through ablations on benchmarks domains, we verify the
critical significance of using independently trained $Q$-functions, and study
the role of ensemble size. Finally, as using separate networks per ensemble
member can become computationally costly with larger neural network
architectures, we investigate whether efficient ensemble approximations
developed for supervised learning can be similarly effective, and demonstrate
that they do not match the performance and robustness of MSG with separate
networks, highlighting the need for new efforts into efficient uncertainty
estimation directed at RL.","OfÔ¨Çine reinforcement learning (RL), also referred to as batch RL (Lange et al., 2012), is a problem setting in which one is provided a dataset of interactions with an environ ment in the form of a Markov decision process (MDP), and the goal is to learn an effective policy exclusively from this Ô¨Åxed dataset. OfÔ¨Çine RL holds the promise of data efÔ¨Åciency through data reuse, and improved safety due to minimizing the need for policy rollouts. As a result, ofÔ¨Çine RL has been the subject of signiÔ¨Åcant renewed interest in the machine learning literature (Levine et al., 2020). One common approach to ofÔ¨Çine RL in the modelfree set ting is to use approximate dynamic programming (ADP) to learn aQvalue function via iterative regression to backed uptarget values . The predominant algorithmic philosophy with most success in ADPbased ofÔ¨Çine RL is to encour age obtained policies to remain close to the support set of the available ofÔ¨Çine data. A large variety of methods have been developed for enforcing such constraints, examples of which include regularizing policies with behavior cloningobjectives (Kumar et al., 2019; Fujimoto & Gu, 2021), per forming updates only on actions observed inside (Peng et al., 2019; Nair et al., 2020; Wang et al., 2020; Ghasemipour et al., 2021) or close to (Fujimoto et al., 2019) the ofÔ¨Çine dataset, and regularizing value functions to underestimate the value of actions not seen in the dataset (Wu et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021a). The need for such regularizers arises from inevitable inac curacies in value estimation when function approximation, bootstrapping, and offpolicy learning ‚Äì i.e. The Deadly Triad (Van Hasselt et al., 2018) ‚Äì are involved. In ofÔ¨Çine RL in particular, such inaccuracies cannot be resolved through additional interactions with the MDP. Thus, remaining close to the ofÔ¨Çine dataset limits opportunities for catastrophic in accuracies to arise. However, recent works have argued that the aforementioned constraints can be overly pessimistic, and instead opt for approaches that take into consideration theuncertainty about the value function (Buckman et al., 2020; Jin et al., 2021; Xie et al., 2021), thus refocusing the ofÔ¨Çine RL problem to that of deriving accurate lower conÔ¨Ådence bounds (LCB) of Qvalues.arXiv:2205.13703v1  [cs.LG]  27 May 2022Why So Pessimistic? OfÔ¨Çine RL through Ensembles, and Why Their Independence Matters In the empirical supervised learning literature, deep ensem bles1and their more efÔ¨Åcient variants have been shown to be the most effective approaches for uncertainty estimation, to wards learning calibrated estimates and conÔ¨Ådence bounds with modern neural network function approximators (Ova dia et al., 2019). Motivated by this, in our work we take a renewed look into Qensembles, and study how to leverage them as the primary source of pessimism for ofÔ¨Çine RL. In deep RL, a very popular algorithmic choice is to use an ensemble of Qfunctions to obtain pessimistic value es timates and combat overestimation bias (Fujimoto et al., 2018). SpeciÔ¨Åcally, in the policy evaluation procedure, all Qnetworks are updated towards a shared pessimistic tem poral difference target . Similarly in ofÔ¨Çine RL, in addition to the main ofÔ¨Çine RL objective that they propose, several existing methods use such Qensembles (Wu et al., 2019; Kumar et al., 2019; Agarwal et al., 2020; Smit et al., 2021; An et al., 2021; Lee et al., 2021; 2022; Ghasemipour et al., 2021). We begin by mathematically characterizing a critical Ô¨Çaw in the aforementioned ensembling procedure. SpeciÔ¨Åcally, we demonstrate that using shared pessimistic targets can paradoxically lead to Qestimates which are in fact opti mistic ! We verify our Ô¨Ånding by constructing pedagogical toy MDPs. These results demonstrate that the formulation of using shared pessimistic targets is fundamentally illformed. To resolve this problem, we propose Model Standard deviation Gradients (MSG), an ensemblebased ofÔ¨Çine RL algorithm. In MSG, each Qnetwork is trained indepen dently ,without sharing targets . Crucially, ensembles trained with independent target values will always provide pes simistic value estimates. The pessimistic lowerconÔ¨Ådence bound (LCB) value estimate ‚Äì computed as the mean minus standard deviation of the Qvalue ensemble ‚Äì is then used to update the policy being trained. Evaluating MSG on the established D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020) benchmarks for ofÔ¨Çine RL, we demonstrate that MSG matches, and in the more challenging domains such as antmazes, signiÔ¨Åcantly exceeds the prior stateoftheart. Additionally, through a series of ablation experiments on benchmark domains, we verify the signiÔ¨Åcance of our theoretical Ô¨Åndings, study the role of ensemble size, and highlight the settings in which ensembles provide the most beneÔ¨Åt. The use of ensembles will inevitably be a computational bottleneck when applying ofÔ¨Çine RL to domains requiring 1In the deep learning literature, deep ensembles refers to the setting where the same network architectures are trained using the same data and objective functions, with the only difference in ensemble members being the random weight initialization of the networks.large neural network models. Hence, as a Ô¨Ånal analysis, we investigate whether the favorable performance of MSG can be obtained through the use of modern efÔ¨Åcient ensemble approaches which have been successful in the supervised learning literature (Lee et al., 2015; Havasi et al., 2020; Wen et al., 2020; Ovadia et al., 2019). We demonstrate that while efÔ¨Åcient ensembles are competitive with the stateoftheart on simpler ofÔ¨Çine RL benchmark domains, similar to many popular ofÔ¨Çine RL methods they fail on more challenging tasks, and cannot recover the performance and robustness of MSG using full ensembles with separate neural networks. Our work highlights some of the unique and often over looked challenges of ensemblebased uncertainty estimation in ofÔ¨Çine RL. Given the strong performance of MSG, we hope our work motivates increased focus into efÔ¨Åcient and stable ensembling techniques directed at RL, and that it highlights intriguing research questions for the community of neural network uncertainty estimation researchers whom thus far have not employed sequential domains such as ofÔ¨Çine RL as a testbed for validating modern uncertainty estimation techniques. 2. Related Work "
495,Diversity and Generalization in Neural Network Ensembles.txt,"Ensembles are widely used in machine learning and, usually, provide
state-of-the-art performance in many prediction tasks. From the very beginning,
the diversity of an ensemble has been identified as a key factor for the
superior performance of these models. But the exact role that diversity plays
in ensemble models is poorly understood, specially in the context of neural
networks. In this work, we combine and expand previously published results in a
theoretically sound framework that describes the relationship between diversity
and ensemble performance for a wide range of ensemble methods. More precisely,
we provide sound answers to the following questions: how to measure diversity,
how diversity relates to the generalization error of an ensemble, and how
diversity is promoted by neural network ensemble algorithms. This analysis
covers three widely used loss functions, namely, the squared loss, the
cross-entropy loss, and the 0-1 loss; and two widely used model combination
strategies, namely, model averaging and weighted majority vote. We empirically
validate this theoretical analysis with neural network ensembles.","Ensemble methods are one of the most widely used and studied techniques in machine learning (Hansen and Salomon, 1990; Breiman, 1996, 2001a). It has been successfully applied in many realworld problems (Gir shick et al., 2014; Wang et al., 2012; Zhou et al., 2014; Ykhlef and Bouchara, 2017) and is usually part ofthe winning strategies in many machine learning com petitions (e.g., Chen and Guestrin, 2016; Hoch, 2015; Puurula et al., 2014; Stallkamp et al., 2012). Recently, ensembles are also becoming very popular to improve uncertainty modeling in deep neural networks (Laksh minarayanan et al., 2017; Wen et al., 2019; Maddox et al., 2019; Wenzel et al., 2020). Ensembles are created by combining several individ ual predictors. It is widely accepted (Dietterich, 2000; Lu et al., 2010) that the prediction performance of an ensemble jointly depends of the individual perfor mance and the diversity of its individual members. In tuitively speaking, a set of predictors is diverse when their predictions do not coincide on all the samples. We know that when classiers are diverse, they tend to make independent errors, therefore when they are aggregated, their errors tend to cancel out (Berend and Kontorovich, 2016), which improves the ensemble prediction. For this reason, diversity has long been recognized as a key factor in ensemble performance (Kuncheva and Whitaker, 2003; Cunningham and Car ney, 2000; Brown et al., 2005). The same cancela tion of errors eect happens in the case of neural net work ensembles (Hansen and Salomon, 1990; Lee et al., 2016; Lakshminarayanan et al., 2017) where heuristic measures of diversity are usually analyzed to get in sights of the ensemble learning algorithms (Fort et al., 2019; Wen et al., 2019; Wenzel et al., 2020). Unfortunately, there is a lack of consensus surround ing the underlying theory that can explain the role of diversity in the generalization performance of ensem bles. The error rate of an ensemble and an individual predictor, for example, is well dened by the use of a loss function, but there is no wellestablished deni tion of diversity (Kuncheva and Whitaker, 2003). And it is not well known how exactly the diversity among ensemble members aects the generalization error of the ensemble. In this work, we introduce a novel theoretical frame work that explains the relationship between diversity and the generalization performance of an ensemble. This theoretical framework is derived from previouslyarXiv:2110.13786v2  [cs.LG]  16 Feb 2022Diversity and Generalization in Neural Network Ensembles published results with no direct connection among them (Krogh and Vedelsby, 1994; Masegosa, 2020; Masegosa et al., 2020). The main contribution of our work is to nd a theoretically sound way to combine these previous results in a single theoretical framework that explains the role of diversity in the generalization performance of a wide range of dierent ensembles. In our opinion, this general framework could poten tially help the machine learning community to have a better understanding of the underlying tradeos that have to be considered when designing novel ensemble learning algorithms, specially in the context of neural networks. The detailed contributions of this work are the followings: a general measure of ensemble diver sity; a theoretical analysis that shows how the corre lation among ensemble members aects diversity; the exact tradeo that exists between this diversity mea sure, the performance of the individual predictors and the generalization error of the ensemble; an analysis of the strategies used by most of current neural net work ensemble learning algorithms to promote diver sity; and, nally, an empirical evaluation of this theo retical framework. This analysis covers model averag ing and weighted majority vote ensembles under the crossentropy loss, square error and 01 loss. 2 Related Work "
496,On the Effectiveness of Neural Ensembles for Image Classification with Small Datasets.txt,"Deep neural networks represent the gold standard for image classification.
However, they usually need large amounts of data to reach superior performance.
In this work, we focus on image classification problems with a few labeled
examples per class and improve data efficiency by using an ensemble of
relatively small networks. For the first time, our work broadly studies the
existing concept of neural ensembling in domains with small data, through
extensive validation using popular datasets and architectures. We compare
ensembles of networks to their deeper or wider single competitors given a total
fixed computational budget. We show that ensembling relatively shallow networks
is a simple yet effective technique that is generally better than current
state-of-the-art approaches for learning from small datasets. Finally, we
present our interpretation according to which neural ensembles are more sample
efficient because they learn simpler functions.","The advent of deep learning (DL) has revolutionized the computer vision Ô¨Åeld [43]. However, the cost to reach high recognition performances involves the collection and label ing of large quantities of images. This requirement can not always be fulÔ¨Ålled since it may happen that collecting im ages is extremely expensive or not possible at all. Differ ent approaches have been proposed by the research commu nity to mitigate the necessity of training data, tackling the problem from different perspectives. Among them of par ticular interest are deÔ¨Ånitely transfer and fewshot learning [4], [36] [37]. Still, these approaches rely on a large set of imageannotation pairs on which reusable representations can be learnt. In this work, we propose the use of neural ensembles composed of smaller networks to tackle the problem of learning from a small sample and show the superiority of such methodology. Similarly to what has been done in recent works, we benchmark the approaches by varying the number of data points in the training sample while keeping it low with respect to the current standards of computer vi sion datasets [1], [3]. Due to its great difÔ¨Åculty, this problem is still unsolved and hardly experimented despite its primary importance. It has been shown that large convolutional neural net works (CNNs) can handle overÔ¨Åtting and generalize well even if they are severely overparametrized [19], [25]. A recent study has also empirically shown that such behav ior might also be valid in the case of tiny datasets, making large nets a viable choice even when the training sample is limited [6]. On the other hand, a wellknown technique to reduce model variance is to average predictions from a set of weak learners (e.g. random forests [7]). An ensemble of lowbias decorrelated learners, combined with randomized inputs and prediction averaging, generally mitigates over Ô¨Åtting. Despite the high popularity of neural ensembles, our study is the Ô¨Årst one that systematically trains them from scratch on datasets with few samples per class and gives empirical evidence of their advantage over stateof theart approaches. Our strict experimental methodology compares ensembles with stateoftheart methods and sin gle networks keeping a fair comparison in terms of model resources. Therefore, we study ensembles of CNNs in smalldata tasks by a) Ô¨Åxing a computational budget and b) compar ing them to corresponding deeper or wider single variants. According to our empirical study, ensembles are preferable over wider networks that are in turn better than deeper ones. Moreover, obtained results make ensembles of smaller net works a strong baseline and an advantageous basic building block for future works that will tackle the problem of learn ing from small datasets. In summary, the contributions of our work are the following: i) we systematically study neural ensembles with small datasets and show that they generally outper form stateoftheart methods; ii) we make a structured study comparing ensembles of smallerscale networks and their computationally equivalent single competitors with in 9876arXiv:2111.14493v1  [cs.CV]  29 Nov 2021creased depth or width; iii) we explain the better perfor mance of ensembles by showing their bias towards learning less complex functions. 2. Related Work "
497,High performing ensemble of convolutional neural networks for insect pest image detection.txt,"Pest infestation is a major cause of crop damage and lost revenues worldwide.
Automatic identification of invasive insects would greatly speedup the
identification of pests and expedite their removal. In this paper, we generate
ensembles of CNNs based on different topologies (ResNet50, GoogleNet,
ShuffleNet, MobileNetv2, and DenseNet201) altered by random selection from a
simple set of data augmentation methods or optimized with different Adam
variants for pest identification. Two new Adam algorithms for deep network
optimization based on DGrad are proposed that introduce a scaling factor in the
learning rate. Sets of the five CNNs that vary in either data augmentation or
the type of Adam optimization were trained on both the Deng (SMALL) and the
large IP102 pest data sets. Ensembles were compared and evaluated using three
performance indicators. The best performing ensemble, which combined the CNNs
using the different augmentation methods and the two new Adam variants proposed
here, achieved state of the art on both insect data sets: 95.52% on Deng and
73.46% on IP102, a score on Deng that competed with human expert
classifications. Additional tests were performed on data sets for medical
imagery classification that further validated the robustness and power of the
proposed Adam optimization variants. All MATLAB source code is available at
https://github.com/LorisNanni/.","Worldwide pest infestation is a major cause of crop and machine damage and, as a consequence, considerabl e reductions  in  grower  revenues  and economic growth [1, 2] . Invasive pests in the US , for example, were reported in 2016 to be racking  up  losses totaling  at least  seventy  billion USD per year  [3]. Rather than sending experts into the  field, a  common low cost method  for monitoring and controlling pest populations has been  to lure insects into traps with pheromones, color, and light. Once  specimens are collected  in this manner , they must be identified and counted, which is an expensive , timeconsuming task  requiring taxonomic expertise. Automatic identification of invasive insects from trap images is a solution that offers the  prospect of increased accuracy , reduced  costs, and large scale execution . However, species identification and th e  discrimination of pests that are harmful versus those that are safe is complicated by  the small size of many  insects, close     resemblance between species, discoloration caused by soap and alcohol solutions  contained within the traps , the pheromone  cap, the loss of legs in sticky traps, vegetative debris, and insect clumping  [46]. Some of t hese conditions can be overcome  by developing systems that automatically and continuously i nspect pests in the field , but the difficult problem of identifying  insects within complex backgrounds ‚Äîas they appear on different crops or on the ground with occlusions  and variations in  illumination ‚Äîremains [2, 710].   In a recent 2020 survey [4] of the literature on pest detection published between 2015 and 2019 , it was determined  that 63% of the studies were  based on convolutional neur al networks (CNNs)  [11], a powerful deep neural network model  designed specifically to recognize visual patterns from images with minimal preprocessing,  and 29% on more traditional  feature based approache s. An earlier survey published in 2018  [12] that focused on deep learners in pest identification found  that only 42% of papers employed  a CNN approach. These two surveys demonstrate that CNNs have become the classifier  of choice in pest classification, a finding that is not surprising given the power of CNNs, which we highlight  in section 4.   In this paper, we propose a method for classif ying insects that is based on CNN and some new Adam optimization  variants.  During  CNN  training,  millions of parameters are updated via a loss function, which attempts to minimize the  difference between current outputs and the  actual values in the training set. Traditionally, gradient descent  (GD) , especially  the stochastic gradient descent (SGD) [13] has been the preferred method  for minimiz ing an objective function . Although  popular, SGD tends towards the convergence of suboptimal local minima when minimizing highly non convex error  functions due to the fact that  the optimization landscape of SGD is not convex. To overcome this s hortcoming, many  variations of SGD  have been proposed [14] [1517]. To some degree, every SGD variant  utilizes the momentum to direct the  gradient [1416]. In [14] and [18], for instance, optimization is guided by its own inertia . In AdaGrad [15] and Adadelta [16],  which extends AdaGrad,  the learning rate of parameters  is decreased more with large partial derivatives tha n with smaller  partial derivatives.  The mechanism behind AdaGrad is the accumulat ion of  previously squared gradients, unlike Adadelt a,  which keeps  track of an exponentially decaying average of past squared gradients . Another SDG variant , Adam [18], not  only stores an exponentially decaying average of past squared gradients but also  an exponentially decaying average of past  gradients ; this information  is used to decrease the learning  rate of the parameters whose gradient changes more frequently ,  most especially in case s where the gradient changes its sign. Adam is well known for its realization of low minima of the  training loss  and is now frequently used with CNNs  [19].  Adam 's excellent ability to find low minima, however,  fails to  translate into better performance compared with SGD  [20]. As a consequence, many new var iants of Adam have been introduced that aim at  increasing its effectiveness  [17, 21,  22]. In [21], for example, the authors p ropose Nadam, which inserts into Adam the Nesterov momentum. More recently, new   Adam variants have been proposed that are based on the difference between present and past gradients, where the step size  is adjusted for each parameter. In [22], AMSGrad curtails  the step size to prevent it from  increas ing, while in diffGrad [17],  which has obtained state oftheart results,  the step size for every parameter is made proportional to the change in the gradient .  In [23], the performance of different Adam variants is compared with SGD and shown to introduce diversity  in ensembles of  CNNs. In addition, the author s propose two new  Adam optimization methods: 1) DGrad, which is a variant of diffGrad  that  uses a moving average of the squares of parameter gradients, and 2) two adjustments  of DGrad that apply different cycli c  learning rates [24].  In this study, we propose two new variants of DGrad [23]: Exp, which introduc es a scaling factor , and ExpLR,  which varies  Exp by adding an extra  step when calculating  the final learning rate. Sets of CNNs are then trained on two insect  benchmarks  (Deng [9] and IP102 [25]) using these and several other Adam variants. Fusions of these partially independent  classifiers are compared  and evaluated to discover t hose combinations that produc e the best classification results . Our best  method is shown to achieve state of  the art on both data sets. Additional  tests are performed on some medical  data to further  validat e the usefulness of the proposed  Adam optimization variants.   The organization of the remainder of this paper is as follows.  In section 2, a  short review of related papers in pest  classification is provided, followed , in section 3,  by a brief introduction to  the CNN topologies investigate d in this study . In  section 4, the Adam optimization variants tested in this work, along with  the new ones proposed here, are detailed . In section  5, the benchmark data set s are described , and experimental results  are presented. The paper concludes with some final remarks  and some suggestions for future research .    2 Related Work on Insect classification   "
498,Single Model Ensemble using Pseudo-Tags and Distinct Vectors.txt,"Model ensemble techniques often increase task performance in neural networks;
however, they require increased time, memory, and management effort. In this
study, we propose a novel method that replicates the effects of a model
ensemble with a single model. Our approach creates K-virtual models within a
single parameter space using K-distinct pseudo-tags and K-distinct vectors.
Experiments on text classification and sequence labeling tasks on several
datasets demonstrate that our method emulates or outperforms a traditional
model ensemble with 1/K-times fewer parameters.","A model ensemble is a promising technique for increasing the performance of neural network mod els (Lars. and Peter., 1990; Anders and Jesper, 1994). This method combines the outputs of multi ple models that are individually trained using the same training data. Recent submissions to natural language processing(NLP) competitions are primar ily composed of neural network ensembles (Bojar et al., 2018; Barrault et al., 2019). Despite its ef fectiveness, a model ensemble is costly. Because it handles multiple models, it requires increased time for training and inference, increased memory, and greater management effort. Therefore, the model ensemble technique cannot always be applied to real systems, as many systems, such as edge de vices, must work with limited computational re sources. In this study, we propose a novel method that replicates the effects of the ensemble technique with a single model. Following the principle that aggregating multiple models improves per formance, we create multiple virtual models in a shared space. Our method virtually inÔ¨Çates the training data Ktimes withKdistinct pseudotags [Tag 1] I watched this ..[Tag 2] I watched this ..[Tag 3] I watched this ..ùíêùüë ùíÜùüé:ùëªùüë'ùíÜùüé:ùëªùüëùú±(ùë¨ùëµùë™('ùíÜùüé:ùëªùüè))ùú±(ùë¨ùëµùë™('ùíÜùüé:ùëªùüê))ùú±(ùë¨ùëµùë™('ùíÜùüé:ùëªùüë))Aggregateùíêùüè ùíÜùüé:ùëªùüè'ùíÜùüé:ùëªùüèùíêùüê ùíÜùüé:ùëªùüê'ùíÜùüé:ùëªùüêFigure 1: Overview of our proposed method. A single model processes the same input with distinct pseudo tags. Each pseudotag deÔ¨Ånes the kth virtual model, and the corresponding vector okis added to the em bedding. Thus, the model function of a singe model (ENC())generates different outputs. appended to all input data. It also incorporates K distinct vectors, which correspond to pseudotags. Each pseudotag k2f1;:::;Kgis attached to the beginning of the input sentence, and the kth vector is added to the embedding vectors for all tokens in the input sentence. Fig. 1 presents a brief overview of our proposed method. Intuitively, this opera tion allows the model to shift the embedding of the same data to the kth designated subspace and can be interpreted as explicitly creating Kvirtual mod els in a shared space. We thus expect to obtain the same (or similar) effects as the ensemble technique composed of Kmodels with our Kvirtual models generated from a single model. Experiments in text classiÔ¨Åcation and sequence labeling tasks reveal that our method outperforms single models in all settings with the same param eter size. Moreover, our technique emulates or surpasses the normal ensemble with 1=Ktimes fewer parameters on several datasets. 2 Related Work "
499,Ensemble Feature for Person Re-Identification.txt,"In person re-identification (re-ID), the key task is feature representation,
which is used to compute distance or similarity in prediction. Person re-ID
achieves great improvement when deep learning methods are introduced to tackle
this problem. The features extracted by convolutional neural networks (CNN) are
more effective and discriminative than the hand-crafted features. However, deep
feature extracted by a single CNN network is not robust enough in testing
stage. To improve the ability of feature representation, we propose a new
ensemble network (EnsembleNet) by dividing a single network into multiple
end-to-end branches. The ensemble feature is obtained by concatenating each of
the branch features to represent a person. EnsembleNet is designed based on
ResNet-50 and its backbone shares most of the parameters for saving computation
and memory cost. Experimental results show that our EnsembleNet achieves the
state-of-the-art performance on the public Market1501, DukeMTMC-reID and CUHK03
person re-ID benchmarks.","Person reidenti/f_ication (reID) is an important task in computer vision and a/t_tracts lots of a/t_tention for its application in intelligent video surveillance. It aims to match pedestrians across diÔ¨Äerent cameras. Due to the large variations in person appearance, pose, occlusion, illumination and so on, it is a very challenging problem. Fortunately, deep learning techniques have improved the perfor mance eÔ¨Äectively. However, there is a big generalization gap [ 1] between training and testing. /T_he main reason is that person reID Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/f_it or commercial advantage and that copies bear this notice and the full citation on the /f_irst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a fee. Request permissions from permissions@acm.org. XXXX 2019, XXXX, XXXX ¬©2019 ACM. XXXXXXXXXXXXX/XX/XX... $15.00 DOI: XX.XXXX/XXXXXXX.XXXXXXXproblem is an uncloseset matching problem [ 2], where the testing identities are diÔ¨Äerent from the training ones. As we all known, classi/f_ication problem in testing is to predict the label of the sample, which still belongs to the training labels. DiÔ¨Äerent from classi/f_i cation, in person reID, the supervised labels of query person and gallery person both are not in the training set. As a result, it is diÔ¨Écult to learn eÔ¨Äective features for person reID. In a single classical CNN network, only one feature vector can be extracted for person reID, and it may have a limited feature rep resentation ability. So it is possible to fuse multiple feature vectors to promote the representation ability and reduce the generaliza tion gap. In practical, a simplest method is to just use multiple independent networks and concatenate their features to promote the performance. It‚Äôs naturally an ensemble idea. To verify this simplest idea, we train 8 independent ResNet50 networks on Mar ket1501 dataset, and concatenate their features for person reID. /T_he results are showed in Figure 1, where we can /f_ind that the mAP and Rank1 of 8 independent networks are similar and the average of them are showed in red dash line, while the ensemble features can easily achieve be/t_ter results as the number of ensemble net works increases. However, it is inconvenient to manage multiple independent networks for deployment and the timeconsuming increases linearly with the number of the networks. In this paper, we explore to propose a new ensemble model with an endtoend network with be/t_ter generalization ability. /T_he basic idea is create multiple branches to form multiple objectives. Each objective can be optimized to produce a solution for learning feature. Inspired by the partbased models [ 3], we use diÔ¨Äerent partbased model in each branch to make the features complementary. Finally, we evaluate the model and present a possible explanation. /T_he contributions of this work are as follows: An ensemble network (EnsembleNet) is proposed to learn the feature representation for person reID. It‚Äôs based on the ResNet50 and consists of multiple branches. /T_he features extracted from each branch are concatenated to form a feature for each image. It‚Äôs an endtoend architecture and has fewer parameters and computation than fusing multiple independent networks. To evaluate EnsembleNet, in experiments, we explore the eÔ¨Äect of stride size, branch numbers and adaptive average pooling. /T_he special se/t_ting can promote the performance eÔ¨Äectively. Experimental results show that our approach achieves the stateoftheart performance on the public Market1501, DukeMTMCreID and CUHK03 person reID benchmarks. To explain the eÔ¨Äectiveness of ensemble feature, inspiring by a visualization method of twodimensional loss land scape [ 4], we present the landscape of testing performancearXiv:1901.05798v1  [cs.CV]  17 Jan 2019XXXX 2019, XXX XX‚ÄìXX, 2019, XXXX, XXXX J. Wang et al. 1 2 3 4 5 6 7 8767880828486mAP (%) Average Ensemble Independent 1 2 3 4 5 6 7 8899091929394Rank1 (%) Average Ensemble Independent Figure 1: Ensemble multiple networks. /T_he networks are based on ResNet50 backbone, we add a 1 1 convolution to reduce the number of channels from 2048dims to 256dims, following so/f_tmax logloss for classi/f_ication. Note that the mAP and Rank1 of independent networks are presented in histogram, while the mAP and Rank1 of ensemble networks are showed in curve. with the ‚Äú/f_ilter normalization‚Äù. /T_he landscapes show that EnsembleNet has /f_latness of testing performance. 2 RELATED WORKS "
500,CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise.txt,"In this paper, we study the problem of learning image classification models
with label noise. Existing approaches depending on human supervision are
generally not scalable as manually identifying correct or incorrect labels is
time-consuming, whereas approaches not relying on human supervision are
scalable but less effective. To reduce the amount of human supervision for
label noise cleaning, we introduce CleanNet, a joint neural embedding network,
which only requires a fraction of the classes being manually verified to
provide the knowledge of label noise that can be transferred to other classes.
We further integrate CleanNet and conventional convolutional neural network
classifier into one framework for image classification learning. We demonstrate
the effectiveness of the proposed algorithm on both of the label noise
detection task and the image classification on noisy data task on several
large-scale datasets. Experimental results show that CleanNet can reduce label
noise detection error rate on held-out classes where no human supervision
available by 41.5% compared to current weakly supervised methods. It also
achieves 47% of the performance gain of verifying all images with only 3.2%
images verified on an image classification task. Source code and dataset will
be available at kuanghuei.github.io/CleanNetProject.","One of the key factors that drive recent advances in large scale image recognition is massive collections of labeled images like ImageNet [5] and COCO [15]. However, it is normally expensive and timeconsuming to collect large scale manually labeled datasets. In practice, for fast de velopment of new image recognition tasks, a widely used surrogate is to automatically collect noisy labeled data from Internet [6, 11, 25]. Yet many studies have shown that label noise can affect accuracy of the induced classiÔ¨Åers signiÔ¨Å cantly [7, 19, 22, 27], making it desirable to develop algo rithms for learning in presence of label noise. Work performed while working at Microsoft.Learning with label noise can be categorized by type of supervision: methods that rely on human supervision and methods that do not. For instance, some of the largescale training data were constructed using classiÔ¨Åers trained on manually veriÔ¨Åed seed images to remove label noise (e.g. LSUN [37] and Places [38]). Some studies for learning con volutional neural networks (CNNs) with noise also rely on manual labeling to estimate label confusion [20, 35]. The methods using human supervision exhibit a disadvantage in scalability as they require labeling effort for every class. For classiÔ¨Åcation tasks with millions of classes [4, 8], it is in feasible to have even one manual annotation per class. In contrast, methods without human supervision (e.g. model predictionsbased Ô¨Åltering [7] and unsupervised outliers re moval [17, 24, 34]) are scalable but often less effective and more heuristic. Going with any of the existing approaches, either all the classes or none need to be manually veriÔ¨Åed. It is difÔ¨Åcult to have both scalability and effectiveness. In this work, we strive to reconcile this gap. We ob serve that one of the key ideas for learning from noisy data is Ô¨Ånding ‚Äúclass prototypes‚Äù to effectively represent classes. Methods learn from manually veriÔ¨Åed seed images like [37] and methods assume majority correctness like [1] belong to this category. Inspired by this observation, we develop an attention mechanism that learns how to select representa tive seed images in a reference image set collected for each class with supervised information, and transfer the learned knowledge to other classes without explicit human super vision through transfer learning. This effectively addresses the scalability problem of the methods that rely on human supervision. Thus, we introduce ‚Äúlabel cleaning network‚Äù (Clean Net), a novel neural architecture designed for this setting. First, we develop a reference set encoder with the atten tion mechanism to encode a set of reference images of a class to an embedding vector that represents that class. Sec ond, in parallel to reference set embedding, we also build a query embedding vector for each individual image and impose a matching constraint in training to require a query embedding to be similar to its class embedding if the query is relevant to its class. In other words, the model can tell 1arXiv:1711.07131v2  [cs.CV]  25 Mar 2018!"" Query #$%&'%,) $*+,*'%,'%,.'%,/01""Reference Set Encoder 0&"" Query Encoder 0*"" Reference Imagess3,4‚àà6%cos$%&,$*Cosine Similarity Loss,%,)& ,*01""Feature Extraction,%,&,%,.&,%,9&Reference Features,%,:&‚àà;%&<Figure 1. CleanNet architecture for learning a class embedding vectors cand a query embedding vector qwith a similarity matching constraint. There exists one class embedding for each of theLclasses. Details of component g()are depicted in Fig. 2. whether an image is mislabeled by comparing its query em bedding with its class embedding. Since class embeddings generated from different reference sets represents different classes where we wish the model to adapt to, CleanNet can generalize to classes without explicit human supervision. Fig. 1 illustrates the endtoend differentiable model. As the Ô¨Årst step of this work, we demonstrate that Clean Net is an effective tool for label noise detection. Simple thresholding based on the similarity between the reference set and the query image lead to good results compared with existing methods. Label noise detection not only is useful for training image classiÔ¨Åers with noisy data, but also has important values in applications like image search result Ô¨Ål tering and linking images to knowledge graph entities. CleanNet predicts the relevance of an image to its noisy class label. Therefore, we propose to use CleanNet to assign weights to image samples according to the imagetolabel relevance to guide training of the image classiÔ¨Åer. On the other hand, as a better classiÔ¨Åer provides more discrimina tive convolutional image features for learning CleanNet, we refresh the CleanNet using the newly trained classiÔ¨Åer. We introduce a uniÔ¨Åed learning scheme to train the CleanNet and image classiÔ¨Åer jointly. To summarize, our contributions include a novel neural architecture CleanNet that is designed to make label noise detection and learning from noisy data with human super vision scalable through transfer learning. We also propose a uniÔ¨Åed scheme for training CleanNet and the image clas siÔ¨Åer with noisy data. We carried out comprehensive experimentation to evaluate our method for label noise detec tion and image classiÔ¨Åcation on three large datasets with realworld label noise: Clothing1M [35], WebVision [13], and Food101N. Food101N contains 310K images we col lected from Internet with the Food101 taxonomy [2], and we added ‚ÄúveriÔ¨Åcation label‚Äù that veriÔ¨Åes whether a noisy class label is correct for an image1. Experimental results show that CleanNet can reduce label noise detection er ror rate on heldout classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images veriÔ¨Åed on an image classiÔ¨Åcation task. 2. Related Work "
501,Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning.txt,"3D hand pose estimation has made significant progress in recent years.
However, the improvement is highly dependent on the emergence of large-scale
annotated datasets. To alleviate the label-hungry limitation, we propose a
multi-view collaborative self-supervised learning framework, HaMuCo, that
estimates hand pose only with pseudo labels for training. We use a two-stage
strategy to tackle the noisy label challenge and the multi-view ``groupthink''
problem. In the first stage, we estimate the 3D hand poses for each view
independently. In the second stage, we employ a cross-view interaction network
to capture the cross-view correlated features and use multi-view consistency
loss to achieve collaborative learning among views. To further enhance the
collaboration between single-view and multi-view, we fuse the results of all
views to supervise the single-view network. To summarize, we introduce
collaborative learning in two folds, the cross-view level and the multi- to
single-view level. Extensive experiments show that our method can achieve
state-of-the-art performance on multi-view self-supervised hand pose
estimation. Moreover, ablation studies verify the effectiveness of each
component. Results on multiple datasets further demonstrate the generalization
ability of our network.","3D hand pose estimation is essential in various applica tion scenarios, from action recognition and sign language translation to AR/VR [25, 26]. Hand pose estimation has achieved a signiÔ¨Åcant improvement in recent years. How ever, the progress heavily relies on the emergence of many hand pose datasets with accurate 3D annotations. Acquir ing labeled datasets is quite timeconsuming and laborious, exposing a realistic challenge for deep learning models to learn with limited and noisy data. Selfsupervised learning is an emerging solution to the challenge posed by manual annotation. Though worth ex ploring, selfsupervised pose estimation with RGB hand im ages is a challenging and relatively unexplored area with only one pioneering method, S2HAND [13]. S2HAND aims to conduct 3D hand reconstruction from a single RGB image with the noisy offtheshell 2D hand pose estima tion results (OpenPose) for supervision. Unfortunately, S2HAND still suffers from a dilemma that the performance is highly restricted by the quality of the pseudo label in a speciÔ¨Åc view when handling an illposed problem. This observation motivates us to exploit multiview in formation to improve selfsupervised learning due to com plementary multiview observations can alleviate the am biguity of pose estimation. There is some literature using multiview data for selfsupervised human body estimation, including EpiporlarPose [35] and CanonPose [62]. How 1arXiv:2302.00988v1  [cs.CV]  2 Feb 2023ever, these approaches only adopt a singleview network to process isolated views, which makes them not feasible for selfsupervised hand pose estimation, whose utilized 2D pseudo labels generated by the offtheshell 2D hand pose estimator are relatively inaccurate. To our knowledge, no previous work discussed the potential of multiview in self supervised hand pose estimation. In this paper, we push along this direction via multiview collaborative learning. As mentioned in [32], naively enforcing multiview con sistency is prone to generate degenerated solutions. At the early training stage, it may be hamstrung by the majority as ‚Äúthe blind lead the blind‚Äù. To summarize, there are sev eral challenges seldom addressed by previous methods, (1) noisy pseudo labels, (2) limited information from cluttered observations, and (3) earlystage divergence. To this end, we propose a novel twostage strategy to tackle noisy pseudo label and unreliable multiview ‚Äúgroup think‚Äù issues. Formally, we name the pipeline HaMuCo, which stands for Ha nd Mu ltiview Co llaborative learn ing. The core insight is to decouple the initial results from the singleview estimation and the updated predic tions after crossview interaction and fusion. SpeciÔ¨Åcally, HaMuCo has (1) the singleview stage and (2) the multi view stage. The singleview stage provides initial predic tions and visual cues from a simple modelbased network with the MANO [53] hand model as the decoder. Us ing a parametric model provides hand priors to regularize irrational anatomy when guided by noisy pseudo labels. Themultiview stage is an essential part of our framework, which conducts crossview interaction, fusion, and distilla tion. In this stage, we design a crossview interaction net work consisting of a (1) multiview graph feature extraction module to gather useful information from all views, a (2) dualbranch crossview interaction module to capture joint level dependencies across all views, and a (3) parameters re gression module. This network is permutationequivalence and can work with variable input views. To solve the chal lenge of updating the pose effectively towards consistent results without early divergence, we introduce multiview collaborative learning in two folds. On the one hand, we design a dualbranch consistency for crossview level col laborative learning to guide all the views to learn from each other. On the other hand, we achieve selfdistillation by fus ing the results of all views into a Ô¨Ånal prediction and using it to supervise the singleview estimations. This procedure further enhances the collaboration between singleview and multiview networks. We conduct comprehensive experiments to demonstrate that HaMuCo is simple, effective, and versatile. Abun dant experiments on the HanCo [72] dataset validate the efÔ¨Åcacy of each component and analyze our model from different aspects. Meanwhile, our method achieves state oftheart performance both in single and multiview 3Dhand pose estimation, under multiview selfsupervised set tings. As shown in Fig. 1, our model can generalize well to other datasets [37, 74] and inthewild images. Further more, our method can also obtain plausible selfsupervised performance on the Assembly101 [54] dataset. In summary, our contributions are the following: ‚Ä¢ We present the Ô¨Årst multiview selfsupervised learn ing framework for 3D hand pose estimation, which can achieve accurate single and multiview estima tions without any manual annotations for training. ‚Ä¢ We propose a crossview interaction network and su pervise it with dualbranch consistency loss and multi view distillation loss to achieve collaborative learning at a crossview level and multi to singleview level. ‚Ä¢ We experimentally validate the efÔ¨Åcacy of our method on benchmarks and achieve stateoftheart perfor mance both in single and multiview 3D hand pose estimation under multiview selfsupervised settings. 2. Related Work "
502,Learning Discriminative Features via Label Consistent Neural Network.txt,"Deep Convolutional Neural Networks (CNN) enforces supervised information only
at the output layer, and hidden layers are trained by back propagating the
prediction error from the output layer without explicit supervision. We propose
a supervised feature learning approach, Label Consistent Neural Network, which
enforces direct supervision in late hidden layers. We associate each neuron in
a hidden layer with a particular class label and encourage it to be activated
for input signals from the same class. More specifically, we introduce a label
consistency regularization called ""discriminative representation error"" loss
for late hidden layers and combine it with classification error loss to build
our overall objective function. This label consistency constraint alleviates
the common problem of gradient vanishing and tends to faster convergence; it
also makes the features derived from late hidden layers discriminative enough
for classification even using a simple $k$-NN classifier, since input signals
from the same class will have very similar representations. Experimental
results demonstrate that our approach achieves state-of-the-art performances on
several public benchmarks for action and object category recognition.","Convolutional neural networks (CNN) [ 20] have ex hibited impressive performances in many computer vision tasks such as image classiÔ¨Åcation [ 17], object detection [ 5] and image retrieval [ 27]. When large amounts of training data are available, CNN can automatically learn hierarchi cal feature representations, which are more discriminativ e thanprevioushandcraftedones[ 17]. Encouraged by their impressive performance in static image analysis tasks, several CNNbased approaches have been developed for action recognition in videos [ 12,15, 25,28,35,44]. Although promising results have been re ported, the advantagesof CNN approachesover traditional ones [34] are not as overwhelming for videos as in static images. Comparedtostaticimages,videoshavelargervari ationsin appearanceas well as high complexityintroduced by temporal evolution, which makes learning features for recognition from videos more challenging. On the other ‚àóIndicates equal contributions.hand, unlike largescale and diverse static image data [ 2], annotateddata foraction recognitiontasks is usually insu f Ô¨Åcient, since annotatingmassive videosis prohibitivelye x pensive. Therefore,withonlylimitedannotateddata,lear n ingdiscriminativefeaturesviadeepneuralnetworkcanlea d to severe overÔ¨Åttingand slow convergence. To tackle these issues, previous works have introduced effective practica l techniques such as ReLU [ 24] and Dropout [ 10] to im provetheperformanceofneuralnetworks,buthavenotcon sidered directly improving the discriminative capability of neurons. The features from a CNN are learned by back propagatingpredictionerrorfromtheoutputlayer[ 19],and hidden layers receive no direct guidance on class informa tion. Worse, in verydeep networks,the early hiddenlayers often suffer from vanishing gradients, which leads to slow optimization convergence and the network convergingto a poor local minimum. Therefore, the quality of the learned features of the hidden layers might be potentially dimin ished[43,6]. To tackle these problems, we propose a new supervised deep neural network, Label Consistent Neural Network , to learn discriminative features for recognition. Our ap proachprovidesexplicitsupervision, i.e.label information, to late hidden layers, by incorporating a label consistency constraint called ‚Äúdiscriminative representationerror‚Äù loss, which is combined with the classiÔ¨Åcation loss to form the overallobjectivefunction. ThebeneÔ¨Åtsofourapproachare twofold: (1)with explicitsupervisionto hiddenlayers,t he problemof vanishinggradientscan be alleviated and faster convergence is observed; (2) more discriminative late hid denlayerfeaturesleadtoincreaseddiscriminativepowero f classiÔ¨Åers at the outputlayer; interestingly,the learned dis criminative features alone can achieve good classiÔ¨Åcation performance even with a simple kNN classiÔ¨Åer. In prac tice, our new formulation can be easily incorporated into anyneuralnetworktrainedusingbackpropagation. Ourap proach is evaluated on publicly available action and object recognitiondatasets. Althoughwe onlypresentexperimen talresultsforactionandobjectrecognition,themethodca n be applied to other tasks such as image retrieval, compres sion,restorations etc.,sinceitgeneratesclassspeciÔ¨Åccom pactrepresentations. 11.1.Main Contributions ThemaincontributionsofLCNN arethreefold. ‚Ä¢Byaddingexplicitsupervisiontolatehiddenlayersvia a ‚Äúdiscriminative representation error‚Äù, LCNN learns more discriminative features resulting in better clas siÔ¨Åer training at the output layer. The representa tionsgeneratedbylatehiddenlayersarediscriminative enoughtoachievegoodperformanceusingasimple k NNclassiÔ¨Åer. ‚Ä¢Thelabelconsistencyconstraintalleviatestheproblem of vanishinggradientsand leads to faster convergence during training, especially when limited training data isavailable. ‚Ä¢Weachievestateoftheartperformanceonseveralac tion and object category recognition tasks, and the compact classspeciÔ¨Åc representations generated by LCNNcanbe directlyusedinotherapplications. 2. Related Work "
503,Residual Attention Network for Image Classification.txt,"In this work, we propose ""Residual Attention Network"", a convolutional neural
network using attention mechanism which can incorporate with state-of-art feed
forward network architecture in an end-to-end training fashion. Our Residual
Attention Network is built by stacking Attention Modules which generate
attention-aware features. The attention-aware features from different modules
change adaptively as layers going deeper. Inside each Attention Module,
bottom-up top-down feedforward structure is used to unfold the feedforward and
feedback attention process into a single feedforward process. Importantly, we
propose attention residual learning to train very deep Residual Attention
Networks which can be easily scaled up to hundreds of layers. Extensive
analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the
effectiveness of every module mentioned above. Our Residual Attention Network
achieves state-of-the-art object recognition performance on three benchmark
datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and
ImageNet (4.8% single model and single crop, top-5 error). Note that, our
method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69%
forward FLOPs comparing to ResNet-200. The experiment also demonstrates that
our network is robust against noisy labels.","Not only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures [34, 16, 23, 40]. At tention not only serves to select a focused location but also enhances different representations of objects at that loca tion. Previous works formulate attention drift as a sequen tial process to capture different attended aspects. However,as far as we know, no attention mechanism has been applied to feedforward network structure to achieve stateofart re sults in image classiÔ¨Åcation task. Recent advances of image classiÔ¨Åcation focus on training feedforward convolutional neural networks using ‚Äúvery deep‚Äù structure [27, 33, 10]. Inspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed atten tion mechanism in ‚Äúvery deep‚Äù structure. The Residual At tention Network is composed of multiple Attention Mod ules which generate attentionaware features. The attention aware features from different modules change adaptively as layers going deeper. Apart from more discriminative feature representation brought by the attention mechanism, our model also ex hibits following appealing properties: (1) Increasing Attention Modules lead to consistent perfor mance improvement, as different types of attention are cap tured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon. (2) It is able to incorporate with stateoftheart deep net work structures in an endtoend training fashion. Specif ically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network out performs stateoftheart residual networks on CIFAR10, CIFAR100 and challenging ImageNet [5] image classiÔ¨Åca tion dataset with signiÔ¨Åcant reduction of computation ( 69% forward FLOPs). All of the aforementioned properties, which are chal lenging to achieve with previous approaches, are made pos sible with following contributions: (1)Stacked network structure : Our Residual Attention Net work is constructed by stacking multiple Attention Mod ules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.arXiv:1704.06904v1  [cs.CV]  23 Apr 2017Origin imageFeature before maskSoftattentionmaskFeature after maskFeature before maskFeature after mask Lowlevel color feature  Sky maskHighlevel part feature Balloon instance maskClassificationInput Attention Attention mechanismSoftattentionmaskFigure 1: Left: an example shows the interaction between features and attention masks. Right: example images illustrating that different features have different corresponding attention masks in our network. The sky mask diminishes lowlevel background blue color features. The balloon instance mask highlights highlevel balloon bottom part features. (2)Attention Residual Learning : Stacking Attention Mod ules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mecha nism to optimize very deep Residual Attention Network with hundreds of layers. (3)Bottomup topdown feedforward attention : Bottomup topdown feedforward structure has been successfully ap plied to human pose estimation [24] and image segmenta tion [22, 25, 1]. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottomup fast feedforward process and topdown attention feedback in a single feedforward process which allows us to develop an endtoend trainable network with topdown attention. The bottomup topdown structure in our work differs from stacked hourglass network [24] in its intention of guiding feature learning. 2. Related Work "
504,Blind Knowledge Distillation for Robust Image Classification.txt,"Optimizing neural networks with noisy labels is a challenging task,
especially if the label set contains real-world noise. Networks tend to
generalize to reasonable patterns in the early training stages and overfit to
specific details of noisy samples in the latter ones. We introduce Blind
Knowledge Distillation - a novel teacher-student approach for learning with
noisy labels by masking the ground truth related teacher output to filter out
potentially corrupted knowledge and to estimate the tipping point from
generalizing to overfitting. Based on this, we enable the estimation of noise
in the training data with Otsus algorithm. With this estimation, we train the
network with a modified weighted cross-entropy loss function. We show in our
experiments that Blind Knowledge Distillation detects overfitting effectively
during training and improves the detection of clean and noisy labels on the
recently published CIFAR-N dataset. Code is available at GitHub.","Learning with noisy labels is a challenging task in image classiÔ¨Åcation. It is well known that label noise leads to heavy performance drops with standard classiÔ¨Åcation meth ods[Song et al. , 2022 ]. The goal of learning with noisy la bels is therefore to train a classiÔ¨Åcation model with labelled training images and achieve high classiÔ¨Åcation performance on unseen test images, even if the labels for training are noisy and corrupted. Labels are noisy because humans are natu rally unable to classify images perfectly due to ambiguous images, individual human bias, pressure of time, or various other reasons. Many modern methods [Liuet al. , 2022b; Rawat and Wang, 2017 ]are trained on large and potentially noisy datasets and thus it is an interest of the community to make classiÔ¨Åcation robust against noisy labels. To evaluate the robustness of methods for learning with noisy labels, clean image datasets like CIFAR [Krizhevsky and Hinton, 2009 ],Clothing1M [Xiao et al. , 2015 ], orWe bVision [Liet al. , 2017 ]are synthetically corrupted by ran 1https://github.com/TimoK93/blind knowledge distillation0 0:25 0:5 0:75 100:10:20:30:4 1 s 2 P(y=yjx)nNoisy Labels Clean Labels Otsu Distributions Figure 1: Distribution of ground truth label related probabilities PA(y=yjx)at beginning of overÔ¨Åtting (tipping point) and the resulting gaussian distributions after Otsu‚Äôs algorithm for the dataset Worst . Red bars show the normalized distribution of noisy labels and green bars of clean labels, respectively. Note that the gaussian distributions (blue) are scaled for visualization purposes. Our presented Blind Knowledge Distillation enables an adaptive noise estimation via the thresholds 1,s, and2and a robust learning with noisy labels. domly Ô¨Çipping label annotations either symmetrically with out constraints or asymmetrically with predeÔ¨Åned rules to mimic realistic label noise. However, Wei et al. [2022 ]shows that synthetic label noise has different behaviour compared to realworld label noise and is thus not an ideal choice to eval uate robust learning. To close this gap, Wei et al. have made great efforts and presented CIFARN with multiple newly an notated ground truth labels for CIFAR with humaninduced label noise. With these new annotations, robust learning can be evaluated more realistically. In this paper, we introduce a novel method to detect the be ginning of overÔ¨Åtting on sample details during training, that is usually roughly estimated as in [Liet al. , 2020 ], and present a simple but effective method to detect most likely corrupted laarXiv:2211.11355v1  [cs.CV]  21 Nov 2022bels. Our method is inspired by Knowledge Distillation [Hin tonet al. , 2015 ]for neural networks which extracts ‚Äòknowl edge‚Äô from a teacher network to train a student network. Differently than usual, our student network is just trained with a subset of the teachers ‚Äòknowledge‚Äô. SpeciÔ¨Åcally, it does not ‚Äòsee‚Äô the ‚Äòknowledge‚Äô about the given and poten tially corrupted ground truth labels by utilizing the teachers ground truth complementary logits. Therefore we call it Blind Knowledge Distillation . Based on the detected noisy labels, we propose a simple but effective losscorrection method to train the teacher model robustly with label noise. We perform extensive experiments on CIFAR10N and the results show thatBlind Knowledge Distillation ‚Ä¢ successfully estimates the tipping point from Ô¨Åtting to general patterns to (over)Ô¨Åtting to sample details, ‚Ä¢ is an effective method to estimate the likelihood of labels being noisy, ‚Ä¢ and improves the classiÔ¨Åcation accuracy while training with high noise levels. 2 Related Work "
505,Tracking e-cigarette warning label compliance on Instagram with deep learning.txt,"The U.S. Food & Drug Administration (FDA) requires that e-cigarette
advertisements include a prominent warning label that reminds consumers that
nicotine is addictive. However, the high volume of vaping-related posts on
social media makes compliance auditing expensive and time-consuming, suggesting
that an automated, scalable method is needed. We sought to develop and evaluate
a deep learning system designed to automatically determine if an Instagram post
promotes vaping, and if so, if an FDA-compliant warning label was included or
if a non-compliant warning label was visible in the image. We compiled and
labeled a dataset of 4,363 Instagram images, of which 44% were vaping-related,
3% contained FDA-compliant warning labels, and 4% contained non-compliant
labels. Using a 20% test set for evaluation, we tested multiple neural network
variations: image processing backbone model (Inceptionv3, ResNet50,
EfficientNet), data augmentation, progressive layer unfreezing, output bias
initialization designed for class imbalance, and multitask learning. Our final
model achieved an area under the curve (AUC) and [accuracy] of 0.97 [92%] on
vaping classification, 0.99 [99%] on FDA-compliant warning labels, and 0.94
[97%] on non-compliant warning labels. We conclude that deep learning models
can effectively identify vaping posts on Instagram and track compliance with
FDA warning label requirements.","The proportion of the U.S. high school students who report using ecigarettes (aka vaping devices) declined in 2020 during the COVID19 pandemic. In 2020, 19.6% of high school students (3.02 million) reported current ecigarette use, compared to 27.5% (4.11 million) of high students who reported using ecigarettes in 2019 (Wang, NeÔ¨Ä, et al. 2020). However, despite this recent decline, during 2019  2020, the use of youthappealing, lowpriced disposable ecigarette devices increased approximately 1,000% (from 2.4% to 26.5%) among high school current ecigarette users (ibid.). In addition, more than eight in 10 teenage ecigarette users reported consuming Ô¨Çavored ecigarettes (ibid.). Ecigarettes can harm the adolescent brain and increase susceptibility to tobacco addic tion (Health et al. 2016; Fraga 2019; Wang, Gentzke, et al. 2019). As a result, youth who use ecigarettes are more likely to subsequently try combustible cigarettes (The U.S. Food and Drug Administration 2018). Beyond addiction, ecigarettes pose a risk of breathing diÔ¨Éculties, inÔ¨Çamma tory reactions, lowered defense against pathogens and lung diseases (RedÔ¨Åeld 2019; The U.S. Food and Drug Administration 2019). Exposure to visual posts featuring ecigarette products on social media, including promotional images and videos, has been associated with increased ecigarette use among U.S. adolescents (Wang, Gentzke, et al. 2019; King et al. 2016; Maloney et al. 2016; Pokhrel et al. 2018; Kim et al. 2019). Instagram, one of the most popular social media platforms among adolescents, is considered the largest source of ecigarette social media advertisements (cite). Ecigarette stores, distributors and social media inÔ¨Çuencers  users with large followings who post vaping content on behalf of ecigarette brands  promote ENDS (Electronic Nicotine Delivery Sys tems) products on Instagram and other social media (Vassey et al. 2020). In August 2018, the U.S. Food & Drug Administration (FDA) introduced a requirement that e cigarette advertisements, including social media imagery, contain a prominent warning label that reminds consumers that nicotine is addictive. The FDA requires that the warning statement ap pears on the upper portion of the advertisement, occupies at least 20 percent of the advertisement area, and be printed in in conspicuous and legible at least 12point sans serif (e.g. Helvetica or Arial bold) font size (The U.S. Food and Drug Administration 2020). Several studies (Vassey et al. 2020; Laestadius et al. 2020) evaluated compliance with the 2018 FDA requirements for warning labels on Instagram. Vassey et al. (2020) manually reviewed 2,000 images posted in 2019 and found that only 7% included FDAmandated warning statements. Posts uploaded from locations within the 1arXiv:2102.04568v1  [cs.SI]  8 Feb 2021U.S. had the highest prevalence of warning labels, while posts uploaded from other countries were less likely to include warnings. Most of the international posts featured vaping products distributed in the U.S. and would therefore still be subject to compliance with the FDA warninglabel regula tions (Vassey et al. 2020). Laestadius et al. (2020) manually reviewed 1,000 posts collected in late 2018early 2020 and found that only 13% included warning statements. Both studies (Vassey et al. 2020; Laestadius et al. 2020) conducted qualitative analysis to assess the presence of warning statements on a small sample size of Instagram posts and used binary classiÔ¨Åcation (presence or absence of a warning statement) without reporting warning labels that were too small or in the wrong place, which would constitute a partial compliance with the FDA requirements. This study addresses the limitation of prior research by developing an automated deep learning image classiÔ¨Åcation capable of quickly and accurately measuring compliance with the FDA requirements for warning labels on a large sample size. i.e. thousands of images. We tested whether advanced deep learning techniques could provide an eÔ¨Äective, automated method to track vapingrelated posts on Instagram and evaluate compliance with FDA warning label requirements. 2 Related work "
506,Bag of Tricks for Developing Diabetic Retinopathy Analysis Framework to Overcome Data Scarcity.txt,"Recently, diabetic retinopathy (DR) screening utilizing ultra-wide optical
coherence tomography angiography (UW-OCTA) has been used in clinical practices
to detect signs of early DR. However, developing a deep learning-based DR
analysis system using UW-OCTA images is not trivial due to the difficulty of
data collection and the absence of public datasets. By realistic constraints, a
model trained on small datasets may obtain sub-par performance. Therefore, to
help ophthalmologists be less confused about models' incorrect decisions, the
models should be robust even in data scarcity settings. To address the above
practical challenging, we present a comprehensive empirical study for DR
analysis tasks, including lesion segmentation, image quality assessment, and DR
grading. For each task, we introduce a robust training scheme by leveraging
ensemble learning, data augmentation, and semi-supervised learning.
Furthermore, we propose reliable pseudo labeling that excludes uncertain
pseudo-labels based on the model's confidence scores to reduce the negative
effect of noisy pseudo-labels. By exploiting the proposed approaches, we
achieved 1st place in the Diabetic Retinopathy Analysis Challenge.","Diabetic retinopathy (DR) is an eye disease that can result in vision loss and blindness in people with diabetes, but early DR might cause no symptoms or only mild vision problems [7]. Therefore, early detection and management of DR play a crucial role in improving the clinical outcome of eye condition. Color fundus photography,  uorescein angiography (FA), and optical coherence tomog raphy angiography (OCTA) have been used in diabetic eye screening to acquire valuable information for DR diagnosis and treatment planning. Recently, in the screening, ultrawide OCTA (UWOCTA) images have been widely used lever aging their advantages such as more detailed visualization of vessel structures, ?Correspondence to Jaeyoung KimarXiv:2210.09558v1  [eess.IV]  18 Oct 20222 Gitaek Kwon et al. and ability to capture a much wider view of the retinal compared to previous standard approaches [34]. With the advancements of deep learning (DL), applying DLbased methods for medical image analysis has become an active research area in the ophthalmol ogy elds [13, 23, 27, 28]. Notably, the availability to large amounts of annotated fundus photography has been one of the key elements driving the quick growth and success of developing automated DR analysis tools. Sun et al. [29] develop the automatic DR diagnostic models using color fundus images, and Zhou et al. [36] propose a collaborative learning approach to improve the accuracy of DR grading and lesion segmentation by semisupervised learning on the color fundus photography. Although previous studies investigate the eectiveness of applying DL to DR grading and lesion detection tasks based on color fundus images, DR analysis tool leveraging UWOCTA are still underconsideration. One of the rea sons lies in the fact that annotating highquality UWOCTA images is inherently dicult because the annotation of medical images requires manual labeling by experts. Consequently, when we consider about the practical restrictions, it is one of the most crucial things to develop a robust model even in the lack of data. To address the above realworld setting, we introduce the bag of tricks for DR analysis tasks using the Diabetic Retinopathy Analysis Challenge (DRAC22) dataset, which consists of three tasks (i.e., lesion segmentation, image quality assessment, and DR grading) [25]. To alleviate the negative eect introduced by the lack of labeled data, we investigate the eectiveness of data augmentations, ensembles of deep neural networks, and semisupervised learning. Furthermore, we propose reliable pseudo labeling (RPL) that selects reliable pseudolabels based on a trained classier's condence scores, and then the classier is re trained with labeled and trustworthy pseudolabeled data. In our study, we nd that Deep Ensembles [11], testtime data augmentation (TTA), and RPL have powerful eects for DR analysis tasks. Our solutions are combinations of the above techniques and achieved 1st place in all tasks for DRAC22. 2 Related Work "
507,DocLangID: Improving Few-Shot Training to Identify the Language of Historical Documents.txt,"Language identification describes the task of recognizing the language of
written text in documents. This information is crucial because it can be used
to support the analysis of a document's vocabulary and context. Supervised
learning methods in recent years have advanced the task of language
identification. However, these methods usually require large labeled datasets,
which often need to be included for various domains of images, such as
documents or scene images. In this work, we propose DocLangID, a transfer
learning approach to identify the language of unlabeled historical documents.
We achieve this by first leveraging labeled data from a different but related
domain of historical documents. Secondly, we implement a distance-based
few-shot learning approach to adapt a convolutional neural network to new
languages of the unlabeled dataset. By introducing small amounts of manually
labeled examples from the set of unlabeled images, our feature extractor
develops a better adaptability towards new and different data distributions of
historical documents. We show that such a model can be effectively fine-tuned
for the unlabeled set of images by only reusing the same few-shot examples. We
showcase our work across 10 languages that mostly use the Latin script. Our
experiments on historical documents demonstrate that our combined approach
improves the language identification performance, achieving 74% recognition
accuracy on the four unseen languages of the unlabeled dataset.","Language identification is a subfield of image classification and aims to recognize the language of printed and written text appearing in images. One exciting and less researched type of image is scans of historical documents. For instance, a significant and diverse collec tion of textbased historical documents compiled by several major European libraries can be accessed in the IMPACT [ 13] dataset. The creation of such a dataset, together with accompanying ground truth data for a smaller subset, is often connected to the goal of improving the analysis and digital processing of historical docu ments. Although many historical documents primarily contain text, such as text from books, articles, or newsletters, there are also doc uments whose main content is represented through large pictures. One specific example is arthistorical documents. These documents are written records that provide information about works of art and the artists who created them. They include manuscripts, letters, diaries, and other written materials that provide insights into a particular artwork‚Äôs creation, history, and significance. Being one of the broader discussed Computer Vision tasks, Optical Character Recognition ( OCR ) methods aim to convert the text in im ages into a digital format. The language of a document is a valuable and contextual information for the process of character recognition. For example, if the language of the document is known in prior, a languagespecific OCR engine can be selected. This can lead to reducing the amount of ambiguity in character recognition, likely improving recognition quality and speed. In previous years, deep learning methods have achieved signif icant performance improvements on various document analysis problems, such as image classification and OCR [9,11,21]. How ever, there are multiple aspects of document input images that still weaken the recognition performance of the above methods. A poor image resolution, small font sizes, and noise introduced by the input image itself or due to the scanning process are common problems [ 9,11,21] and, at the same time, common characteristics of arthistorical documents. Several supervised learning methodswere proposed to tackle these problems and improve recognition performance [ 11,21]. These methods usually incorporate large amounts of labeled data into the training of a neural network. How ever, most of today‚Äôs data is generated from diverse, unstructured sources, resulting in data without additional ground truth infor mation. Therefore, accurately assigning meaningful labels often requires contextual understanding and human expertise, which is usually timeconsuming and infeasible for large quantities of data. Motivated by this problem, in this paper, we propose DocLangID , a twostage training approach to tackle the problem of domain adaptation for language identification. Given two domains of im ages, one labeled and the other unlabeled, the task is to adapt a model trained on the labeled domain toward identifying the lan guages of the unlabeled domain. In this setting, the languages of both domains do not have to overlap. DocLangID is a simple yet effective fewshot learning method that employs a distancebased classifier, as shown by Chen et al. [2], to improve language identification in the unlabeled domain. We achieve this improvement by using only small amounts of manually labeled examples. Our contributions can be summarized as follows: (1)We achieve high language identification performance for art historical documents in a supervised setting, which was not accomplished previously for languages of the Latin script. (2)We develop a simple but effective method for domain adap tation, which utilizes large amounts of labeled data and few shot examples to transfer the model‚Äôs knowledge to a new domain of historical documents. (3)We analyze patch extraction, a commonly used procedure when training on image data, and the assessment of its im pact on our approach. (4)We implement and evaluate supervised variations of our main DocLangID approach, and we publish our work on Github1. 2 RELATED WORK "
508,Emotional Expression Detection in Spoken Language Employing Machine Learning Algorithms.txt,"There are a variety of features of the human voice that can be classified as
pitch, timbre, loudness, and vocal tone. It is observed in numerous incidents
that human expresses their feelings using different vocal qualities when they
are speaking. The primary objective of this research is to recognize different
emotions of human beings such as anger, sadness, fear, neutrality, disgust,
pleasant surprise, and happiness by using several MATLAB functions namely,
spectral descriptors, periodicity, and harmonicity. To accomplish the work, we
analyze the CREMA-D (Crowd-sourced Emotional Multimodal Actors Data) & TESS
(Toronto Emotional Speech Set) datasets of human speech. The audio file
contains data that have various characteristics (e.g., noisy, speedy, slow)
thereby the efficiency of the ML (Machine Learning) models increases
significantly. The EMD (Empirical Mode Decomposition) is utilized for the
process of signal decomposition. Then, the features are extracted through the
use of several techniques such as the MFCC, GTCC, spectral centroid, roll-off
point, entropy, spread, flux, harmonic ratio, energy, skewness, flatness, and
audio delta. The data is trained using some renowned ML models namely, Support
Vector Machine, Neural Network, Ensemble, and KNN. The algorithms show an
accuracy of 67.7%, 63.3%, 61.6%, and 59.0% respectively for the test data and
77.7%, 76.1%, 99.1%, and 61.2% for the training data. We have conducted
experiments using Matlab and the result shows that our model is very prominent
and flexible than existing similar works.","The human voice is very complex and can show a wide range of emotions. Emotion in speech gives  information about how people act or feel. There are many functions of the human vocal system that make it  possible for humans to speak. These include tone, pitch, energy, entropy , and m any other aspects of the  speech. The increasing need for human machine interactions indicates that more tasks to be done to improve  the results of these interactions, like giving computer and machine interfaces the ability to understand how a  person feels when they speak. Emotions play a big part in how people talk to each other. People and machines  should be able to work together more effectively if computers have built in skills for figuring out how people  feel [2], [5]. Today, a lot of resources  and time  is being spent on improving artificial intelligence and smart  machines to make our life easier and more comfortable. According to the findings of several pieces of  literature, human feelings regulate the decision making process to some extent [1] [4]. If the machine can  figure out how people are feeling when they speak, it will be able to respond and communicate properl y.   Recognizing people's feelings based on what they say is still a challenge. The authors in [3] propose CNNs  (Convolutional Neural Networ ks), RNNs (Recurrent Neural Networks), and time distributed CNNs based  network s in their study, but they do not use any usual hand crafted features that are typically used to identify  emotional speech. To solve the SER (Speech Emotional Recognition) proble m, they  integrate LSTM (Long  Short Term Memory) network layers into a deep hierarchical CNN feature extraction architecture. They have  got almost 87% and 78% accuracy.   2. RELATED WORKS   "
509,Detecting malicious PDF using CNN.txt,"Malicious PDF files represent one of the biggest threats to computer
security. To detect them, significant research has been done using handwritten
signatures or machine learning based on manual feature extraction. Those
approaches are both time-consuming, require significant prior knowledge and the
list of features has to be updated with each newly discovered vulnerability. In
this work, we propose a novel algorithm that uses an ensemble of Convolutional
Neural Network (CNN) on the byte level of the file, without any handcrafted
features. We show, using a data set of 90000 files downloadable online, that
our approach maintains a high detection rate (94%) of PDF malware and even
detects new malicious files, still undetected by most antiviruses. Using
automatically generated features from our CNN network, and applying a
clustering algorithm, we also obtain high similarity between the antiviruses'
labels and the resulting clusters.","1.1 Malware in PDF Malware programs are still making newspapers' headlines. They are used by criminal organizations, governments, and industries to steal money, spy, or other unwanted activities. As millions of new malicious samples are discovered every day, spotting them before they harm a computer or a network remains one of the most important challenges in cybersecurity. During the last two decades, hackers kept nding new attack vectors, giving malware multiple forms. Some use the macros in Microsoft Oce documents while others exploit browser's vulnerabilities with javascript les. This diversity raises the need for new automated solutions. Portable Document Format (PDF) is one of the most popular types of documents. Despite the lack of awareness of the population, it also became an important attack vector (AV) for computer systems. Dozens of vulnerabilities are discovered every year on Adobe Reader, the most popular software for reading PDF les [1], allowing hackers to take control of the victim's computer. PDF malware can be segmented into three main categories: (i) exploits, (ii) phishing, and (iii) misuse of PDF capabilities. Exploits operate by taking advantage of a bug in the API of a PDF reader application, which allows the attacker to execute code on the victim's computer. This is usually done via JavaScript code embedded in the le. In phishing attacks, the PDF itself does not have any malicious behavior but attempts to convince the user to click on a malicious link. Such campaigns have been discovered recently [2] and are, by nature, much harder to identify. The last category 1arXiv:2007.12729v2  [cs.CR]  2 Aug 2020exploits some regular functionality of PDF les such as running a command or launching a le. All those attacks can lead to devastating consequences, such as downloading a malicious executable or stealing credentials from a website. Regardless of recent work in machine learning for malware detection, antivirus companies are still largely focusing on handwritten signatures to detect malicious PDF. This not only requires signicant human resources but is also rarely ecient at detecting unknown variants or zeroday attacks [3]. Another popular solution is the dynamic analysis by running the les in a controlled sandboxed environment [4]. Such approaches increase signicantly the chance of detecting new malware, but take much longer and require access to a sandbox virtual machine. They also still require a human to dene the detection rules according to the le behavior. 1.2 Classical antivirus models for PDF les Antivirus vendors use a few dierent approaches to detect malware in PDF: Signaturebased detection It is the most basic and common method used to identify malicious les [6]. Security analyst manually inspects a malicious le and extract one or several patterns from the byte code, the ""signatures"", that they store in a database. When analyzing a new le, they try to match their code segments with the one in the database. If a match occurs, the le is blocked. Static Analysis Another rudimentary technique commonly used by antivirus is static anal ysis. In consists of applying heuristicbased rules on the content of a le to nd potentially malicious action. The easiest approach is the search for keywords like /JavaScript, /Open Action, or /GoTo which are related to an action that can be harmful to the computer. In absence of those tags, an analyst can condently say that the le is benign [21] (although some attacks are managing to inject javascript code without requiring a javascript tag). Dynamic Analysis It is a more expensive but potentially stronger method for detecting malicious behavior. It consists of running the le in a controlled environment (sandbox) and evaluates and retrieve the API calls and the network activity produced by the possible malware. Then, a program can apply heuristics on top of the activity logs like connecting to a malicious website or launching a subprocess [21]. 1.3 Contribution In this work, we are using an ensemble of Convolutional Neural Network (CNN) in order to detect any type of malicious PDF les. Without any preprocessing of the les, our classier succeeds to detect 94% of the malicious samples of our test set while keeping a False Positive Rate (FPR) at 0.5%. Our classier outperforms most of the antiviruses (AV) vendors available in the VirusTotal website.We also show that our CNN can successfully group more than 75% of the malware into dierent families. Finally, we will present some examples on which we were able to detect an attack before the AV (zeroday). To the best of our knowledge, this is the rst paper using Neural Network to classify PDF malware. It is also the rst one that investigates the ability to automatically classify malicious PDF into dierent families. Finally, as an attempt to build a baseline for detecting PDF malware, we opensourced the list of the les used for the research. They are all downloadable from VirusTotal. 2Paper organization: We rst present the related research in machine learning for detecting malicious PDF and the usage of Deep Learning applied to Malware Detection in executable les (Section 2). We describe how we built our data set in Section 3, and describe our model in Section 4. We show our results on the data sets in Section 5. We investigate the capability of our network to dierentiate between malware types in Section 6. Our conclusion is in Section 7. 2 Related work "
510,Semi-Supervised Confidence Network aided Gated Attention based Recurrent Neural Network for Clickbait Detection.txt,"Clickbaits are catchy headlines that are frequently used by social media
outlets in order to allure its viewers into clicking them and thus leading them
to dubious content. Such venal schemes thrive on exploiting the curiosity of
naive social media users, directing traffic to web pages that won't be visited
otherwise. In this paper, we propose a novel, semi-supervised classification
based approach, that employs attentions sampled from a Gumbel-Softmax
distribution to distill contexts that are fairly important in clickbait
detection. An additional loss over the attention weights is used to encode
prior knowledge. Furthermore, we propose a confidence network that enables
learning over weak labels and improves robustness to noisy labels. We show that
with merely 30% of strongly labeled samples we can achieve over 97% of the
accuracy, of current state of the art methods in clickbait detection.","With the number of social media users increasing by the day, one of the prime objectives of online news media agencies is to lead social media users onto bogus pages through the display of luscious text/images (Loewenstein, 1994). In most cases the content on the landing page is disparate to the headline the user clicked on. Source veriÔ¨Åcation is no longer warranted as news agencies aren‚Äôt held accountable for the content they post online. As (M. Potthast and Hagen, 2016) suggests, at least 15 of the most prominent content creators use clickbaits in some form or the other to honeytrap users. Impetus for such schemes can range fromdirecting trafÔ¨Åc to web sites that force users to pur chase a product, to shaping popular opinion espe cially during elections. Some clickbaits claim to accomplish inconceivable tasks while others rely on a viewer‚Äôs inducement to grapevines. ‚ÄúYou will never believe what this celebrity did at the awards ceremony.‚Äù ‚Äú10 things that will get you fairer in 5 days.‚Äù ‚ÄúMillionaires want to conceal this scheme. It can make you rich in a week.‚Äù Earlier approaches on tackling clickbaits mainly focused on cheap and easy to implement solu tions. Blacklisting URLs has been, to some ex tent, effective in regulating an average user‚Äôs expo sure to clickbaits. (Gianotto, 2014) assumed that most clickbaits are based on a few key phrases, and a naive way yet effective strategy would be to simply look for such phrases. Such an as sumption holds no ground today. As the problem grew to be more pervasive, social media compa nies modeled the probability for a content to be a clickbait based on factors like clicktoshare ra tio, time spent by user on the target page, and other such quantiÔ¨Åers. Recent research focuses on salvaging sentence structures, ngrams & embed dings among other features, in classiÔ¨Åers like Ran dom Forests (RF), Gradient Boosted Trees (GBT), Support Vector Machines (SVM) or the vanilla Logistic Regression (LR). With the advent of on line news agencies, there exists a plethora of such sources, but labeling each of the headlines from them would be a staggering task. This vindicates the need for an unsupervised / semisupervised ap proach. Contributions of this paper include: 1. A novel loss component on the attention weights that en codes prior information from a weak source of laarXiv:1811.01355v1  [cs.CL]  4 Nov 2018bels, which eventually improves the generalizabil ity of the deep learning model that has been trained on a small representative dataset. 2. A joint archi tecture that incorporates into the clickbait classiÔ¨Å cation framework a conÔ¨Ådence network that tack les label noise. 3. Using GumbelSoftmax for gated attentions, thus enforcing peaky attentions over word contexts. 4. Empirically proving the performance of the proposed approach on popu lar clickbait datasets with only a small portion of labeled samples. 2 Related Work "
511,Efficient Stochastic Inference of Bitwise Deep Neural Networks.txt,"Recently published methods enable training of bitwise neural networks which
allow reduced representation of down to a single bit per weight. We present a
method that exploits ensemble decisions based on multiple stochastically
sampled network models to increase performance figures of bitwise neural
networks in terms of classification accuracy at inference. Our experiments with
the CIFAR-10 and GTSRB datasets show that the performance of such network
ensembles surpasses the performance of the high-precision base model. With this
technique we achieve 5.81% best classification error on CIFAR-10 test set using
bitwise networks. Concerning inference on embedded systems we evaluate these
bitwise networks using a hardware efficient stochastic rounding procedure. Our
work contributes to efficient embedded bitwise neural networks.","Research results in recent years have shown tremendous advances in solving complex problems using deep learning approaches. Especially classiÔ¨Åcation tasks based on image data have been a major target for deep neural networks ( DNN s) [8,14]. A challenge for leveraging the strengths of deep learning methods in embedded systems is their massive computational cost. Even relatively small DNN s often require millions of parameters and billions of operations for performing a single classiÔ¨Åcation. Model compression approaches can help to relax memory requirements as well as to reduce the number of required operations of DNN s. While some approaches consider special network topologies [ 8,11], another stream of research focuses on precision reduction of the model parameters. Recent publications of bitwise neural networks ( BNN s) have shown that network weights and activations can be reduced from a highprecision Ô¨Çoatingpoint down to a binary representation, while maintaining classiÔ¨Åcation accuracy on benchmark datasets [ 5]. Stochastic projection of the network weights during training is a key component that enables this strong quantization. Studies These authors contributed equally to this work. yProfessor Gerd Ascheid is Senior Member IEEE. Submitted to 1st International Workshop on EfÔ¨Åcient Methods for Deep Neural Networks at 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Copyright c 2016 Robert Bosch GmbH. Rights reserved.arXiv:1611.06539v1  [cs.NE]  20 Nov 2016which employed this training method have so far only analyzed deterministic projections during testtime [4, 5, 15]. With techniques presented in this paper, we contribute to stochastic inference of bitwise neural networks on hardware. We show that stochastic rounding at testtime improves classiÔ¨Åcation accuracy of networks that were trained with stochastic weight projections (Section 3). Furthermore, we present a method which efÔ¨Åciently realizes stochastic rounding of network weights in a dedicated hardware accelerator (Section 4). We start off with a brief review of the literature on weight discretization (Section 2). 2 Related Work "
512,Is Attention always needed? A Case Study on Language Identification from Speech.txt,"Language Identification (LID), a recommended initial step to Automatic Speech
Recognition (ASR), is used to detect a spoken language from audio specimens. In
state-of-the-art systems capable of multilingual speech processing, however,
users have to explicitly set one or more languages before using them. LID,
therefore, plays a very important role in situations where ASR based systems
cannot parse the uttered language in multilingual contexts causing failure in
speech recognition. We propose an attention based convolutional recurrent
neural network (CRNN with Attention) that works on Mel-frequency Cepstral
Coefficient (MFCC) features of audio specimens. Additionally, we reproduce some
state-of-the-art approaches, namely Convolutional Neural Network (CNN) and
Convolutional Recurrent Neural Network (CRNN), and compare them to our proposed
method. We performed extensive evaluation on thirteen different Indian
languages and our model achieves classification accuracy over 98%. Our LID
model is robust to noise and provides 91.2% accuracy in a noisy scenario. The
proposed model is easily extensible to new languages.","From the inception of research in Natural Language Processing (NLP), researchers have speciÔ¨Åcally rely on ConvolutionNeuralNetworks(CNN)asitutilizeslocalfeatureseÔ¨Äectively.EarlierRecurrentNeuralNetwork(RNN) was eÔ¨Äectively used in diÔ¨Äerent NLP domains, but recent use of Transformer has shown promising results which outperforms all previous experimental results. Attention based models are capable of capturing the contentbased global interactions. Transformer in QuestionAnswering domain, researcher (Yamada et al., 2020) were able to outperform BERT (Devlin et al., 2019), SpanBERT (Joshi et al., 2020), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2020). In Machine Translation domain researcher (Takase and Kiyono, 2021; Gu et al., 2019; Chen and HeaÔ¨Åeld, 2020) usedTransformerandwereabletooutperformotherstateoftheart(sota)algorithms.InotherdomainlikeLanguage Modelling, Text ClassiÔ¨Åcation, Topic Modelling, Emotion ClassiÔ¨Åcation, Sentiment Analysis, etc Transformer has widely used. InthisworkwefocusedonusingdiÔ¨ÄerentapproachesforSpokenLanguageIdentiÔ¨Åcation.Humansarecapableof recognizingalmostimmediatelythelanguagebeingusedbyaspeakerforvoicinganutterance.Thetaskofautomatic language identiÔ¨Åcation (LID) is to automatically classify the language used by a speaker in his/her speech. In the era of Internet of Things , smart and intelligent assistants (e.g., Alexa1, Siri2, Cortona3, Google Assistant4, etc.) can interactwithhumanswithsomedefaultlanguagesettings(mostlyinEnglish)andthesesmartassistantsrelyheavilyon Automatic Speech Recognition (ASR). However, these virtual assistants fail to provide any assistance in multilingual <Corresponding author atanumandal0491@gmail.com (A. Mandal); santanu.pal2@wipro.com (S. Pal); indranildutta.lnl@jadavpuruniversity.in (I. Dutta); languagemahib@gmail.com (M. Bhattacharya); sudipkumar.naskar@jadavpuruniversity.in (S.K. Naskar) ORCID(s):0000000293855897 (A. Mandal); 0000000330796903 (S. Pal); 0000000185225302 (I. Dutta); 0000000315884665 (S.K. Naskar) 1https://developer.amazon.com/enUS/alexa/alexavoiceservice 2https://www.apple.com/in/siri/ 3https://www.microsoft.com/enin/windows/cortana 4https://assistant.google.com/ Mandal et al.: Preprint submitted to Elsevier Page 1 of 21Is Attention always needed? contexts.Tomakesuchsmartassistantsrobust,LIDcanbeusedsothatthesmartassistantscanautomaticallyrecognize the speaker‚Äôs language and change its language setting accordingly. Our approach of identifying spoken language is limited to Indian Languages only because India is world second populated and seventh largest country in landmass and also have dynamic culture. Currently, India has 28 states and 8 Union Territories, where each states and Union Territories has its own language, but none of the language is recognised as the national language of the country. Only, English and Hindi is used as oÔ¨Écial language according to the Constitution of India Part XVII Chapter 1 Article 3435. Currently, only 22 languages have been accepted as regional languages. Sl. No. Language Family Spoken in 1 Assamese IndoAryan Assam 2 Bengali IndoAryanAssam, Jharkhand, Tripura, West Bengal 3 Bodo SinoTibetan Assam 4 Dogri IndoAryan Jammu and Kashmir 5 Gujarati IndoAryanGujrat, Dadra and Nagar Haveli and Daman and Diu 6 Hindi IndoAryanAndaman and Nicobar Islands, Bihar, Chhattisgarh, Dadra and Nagar Haveli and Daman and Diu, Delhi, Haryana, Himachal Pradesh, Jammu and Kashmir, Jharkhand, Ladakh, Madhya Pradesh, Mizoram, Rajasthan, Uttar Pradesh, Uttarakhand 7 Kannada Dravidian Karnataka 8 Kashmiri IndoAryan Jammu and Kashmir 9 Konkani IndoAryanDadra and Nagar Haveli and Daman and Diu, Goa 10 Maithili IndoAryan Jharkhand 11 Malayalam Dravidian Kerala, Lakshadweep, Puducherry 12 Marathi IndoAryanDadra and Nagar Haveli and Daman and Diu, Goa, Maharashtra 13 Meitei SinoTibetan Manipur 14 Nepali IndoAryan Sikkim, West Bengal 15 Odia IndoAryan Jharkhand, Odisha 16 Punjabi IndoAryan Delhi, Haryana, Punjab 17 Sanskrit IndoAryan Himachal Pradesh 18 Santali Austroasiatic Jharkhand 19 Sindhi IndoAryan Rajasthan 20 Tamil Dravidian Tamil Nadu 21 Telugu DravidianAndhra Pradesh, Puducherry, Telangana 22 Urdu IndoAryanBihar, Delhi, Jammu and Kashmir, Jharkhand, Telangana, Uttar Pradesh Table 1 List of oÔ¨Écial languages as per the Eighth Schedule of the Constitution of India, as of 1 December 2007 with their language family and states spoken in. Table 1 describes the 22 languages designated as OÔ¨Écial language according to the Eighth Schedule of the ConstitutionofIndia,asof1December2007.MostoftheIndianlanguagesoriginatedfromIndoAryanandDravidian language family. 5https://www.mea.gov.in/Images/pdf1/Part17.pdf Mandal et al.: Preprint submitted to Elsevier Page 2 of 21Is Attention always needed? It can be seen from the Table 1 that diÔ¨Äerent languages are spoken in diÔ¨Äerent states, however, languages do not obey the geographical boundaries. Therefore, many of these languages, particularly in the neighboring regions, have multiple dialects which are amalgamation of two or more languages. SuchenormouslinguisticdiversitymakesitdiÔ¨ÉcultforthecitizenstocommunicateindiÔ¨Äerentpartsofthecountry. Bilingualism and multilingualism are the norm in India. In this context, an LID system becomes a crucial component foranyspeechbasedsmartassistant.ThebiggestchallengeandhenceanareaofactiveinnovationforIndianlanguage is the reality that most of these languages are under resourced. Every spoken language has its underlying lexical, speaker, channel, environment, and other variations. The likely diÔ¨Äerences among various spoken languages are in their phoneme inventories, frequency of occurrence of the phonemes, acoustics, the span of the sound units in diÔ¨Äerent languages, and intonation patterns at higher levels. The overlap between the phoneme set of two or more familial languages makes it a challenge for recognition. The lowresource status of these languages makes the training of machine learning models doubly diÔ¨Écult. Every spoken languagehasitsunderlyinglexical,speaker,channel,environment,andothervariations.ThelikelydiÔ¨Äerencesamong various spoken languages are in their phoneme inventories, frequency of occurrence of the phonemes, acoustics, the spanofthesoundunitsindiÔ¨Äerentlanguages,andintonationpatternsathigherlevels.Theoverlapbetweenthephoneme setoftwoormorefamiliallanguagesmakesitachallengeforrecognition.Thelowresourcestatusoftheselanguages makes the training of machine learning models doubly diÔ¨Écult. The purpose of our approach is yet to predict the correct spoken language regardless of the abovementioned constraints. In this work we proposed Language IdentiÔ¨Åcation method for Indian Languages using diÔ¨Äerent approaches. Our LID methods cover 13 Indian languages6. Additionally our method is language agnostic. The main contributions of this work can be summarized as follows: ‚Ä¢ThemethodusesConvolutionalNeuralNetwork(CNN),ConvolutionalRecurrentNeuralNetwork(CRNN),and attention based CRNN for the task of LID. We tested 13 Indian languages achieving stateoftheart accuracy. ‚Ä¢Our model also provides stateoftheart performance in languages that belong to the same language family as well as in noisy scenarios. 2. Related Works "
513,Neural Network Pruning with Residual-Connections and Limited-Data.txt,"Filter level pruning is an effective method to accelerate the inference speed
of deep CNN models. Although numerous pruning algorithms have been proposed,
there are still two open issues. The first problem is how to prune residual
connections. We propose to prune both channels inside and outside the residual
connections via a KL-divergence based criterion. The second issue is pruning
with limited data. We observe an interesting phenomenon: directly pruning on a
small dataset is usually worse than fine-tuning a small model which is pruned
or trained from scratch on the large dataset. Knowledge distillation is an
effective approach to compensate for the weakness of limited data. However, the
logits of a teacher model may be noisy. In order to avoid the influence of
label noise, we propose a label refinement approach to solve this problem.
Experiments have demonstrated the effectiveness of our method (CURL,
Compression Using Residual-connections and Limited-data). CURL significantly
outperforms previous state-of-the-art methods on ImageNet. More importantly,
when pruning on small datasets, CURL achieves comparable or much better
performance than fine-tuning a pretrained small model.","Deep neural networks have now become the dominat ing method in various computer vision Ô¨Åelds, such as im age recognition [10, 20, 35] and object detection [5], and we have witnessed a great improvement in model accuracy. But, deploying a large CNN model on resource constrained devices like mobile phones is still challenging. Due to over parameterization, it is both storage and time consuming to run a cumbersome large model on small devices. Network pruning is a useful tool to obtain a satisfac tory balance between inference speed and model accuracy. Among these methods, Ô¨Ålter level pruning aims to remove This research was partially supported by the National Natural Science Foundation of China (61772256, 61921006). J. Wu is the corresponding author. ( a) bott len e ck (b)  h ou r g las s ( c) wall e tFigure 1. Illustration of residual block pruning with different strategies. (a) Bottleneck structure of residual blocks. (b) Only prune channels inside the bottleneck, generating an hourglass structure. (c) Prune channels both inside and outside the residual connection, generating a shape similar to an opened wallet. the whole unimportant Ô¨Ålters according to a certain crite rion. This strategy will not damage the original model struc ture and is attracting more and more attention recently. Although numerous Ô¨Ålter level pruning algorithms have been proposed, there are still several open issues. First, pruning residual connections is very difÔ¨Åcult. As illustrated in Fig. 1, most previous pruning methods only prune Ô¨Ål ters inside the residual connection, leaving the number of output channels unchanged. With a smaller target model (i.e., more Ô¨Ålters pruned), the original bottleneck structure will become an hourglass. Obviously, representation ability of middle layers inside the hourglass structure is severely handicapped. Therefore, pruning channels both inside and outside the residual connection is more preferred for accel erating networks. Then, the pruned block is still bottleneck or in an opened wallet shape. As illustrated in the experiments section, the wallet structure has more advantages compared with hourglass: 1) it is more accurate thanks to a larger pruning space; 2) it is faster even with the same number of FLOPs; 3) it can save more storage space because more weights will be pruned. The second issue is about pruning models with limited data. Most current pruning methods only report their results on toy datasets ( e.g., MNIST [21], CIFAR [19]) or large scale datasets ( e.g., ImageNet [33]), ignoring an important real application scenario: pruning models on small datasets which have few images per category. This is a very common 1arXiv:1911.08114v3  [cs.CV]  25 Apr 2020requirement, because we will not apply ImageNet in a real application. Directly pruning on a target dataset (which is usually small) is necessary. In order to get a small model on a target small dataset, there are two different ways: 1) compress the network us ing the large dataset (or using a small network trained from scratch on the large dataset), and then Ô¨Ånetune on the target small dataset; 2) directly prune the model without access to the large dataset. In many realworld scenarios, the only choice is to compress the network using the small dataset and Ô¨Ånetune on the same small dataset. But, the reality is that directly pruning on a small dataset usually has a signiÔ¨Åcantly lower accuracy than Ô¨Ånetuning a small model which is pruned or trained from scratch on the large scale dataset . This phenomenon widely exists in various networks and datasets. For example, as shown in ThiNet [29], Ô¨Ånetuning a pruned model which is com pressed on ImageNet is a better choice when transferring to other domains. They found that the accuracy of directly pruning on CUB200 [36] is only 66.90%, while Ô¨Ånetuning a pruned ImageNet model can achieve 69.43%. A dilemma is that directly pruning on the target dataset is often the case in realworld applications, where large datasets are either proprietary or too expensive to be used by ordinary users. In this paper, we propose CURL, namely Compression Using Residualconnections and Limiteddata, to address both issues. In order to prune the channels outside of the residual connection, we show that all the blocks in the same stage should be pruned simultaneously due to the short cut connection. We propose a KLdivergence based crite rion to evaluate the importance of these Ô¨Ålters. The chan nels inside and outside the residual connections will both be pruned, leading to a wallet shaped structure. Experi ments on ImageNet show that the proposed residual block pruning method outperforms the previous stateoftheart. To address the problem caused by the lack of enough train ing data, we propose to combine knowledge distillation [14] and mixup [40] together and enlarge the training dataset via image transformation. We also propose a novel method to correct the noise in the logits of the teacher model. All the techniques greatly improve the accuracy of directly pruning with limited data. Our contributions are summarized as follows. We propose a novel way to compress residual blocks. We prune not only channels inside the residual branch, but also channels of its output activation maps (both the identity branch and the residual branch). The re sulting walletshaped structure shows more advantages than previous hourglassshaped structure. Data augmentation is very effective in model Ô¨Åne tuning with limited data. We show that combining data augmentation and knowledge distillation can achievebetter performance. To avoid the inÔ¨Çuence of label noise, we propose a label reÔ¨Ånement strategy which can further improve the accuracy. 2. Related Work "
514,Noisy Concurrent Training for Efficient Learning under Label Noise.txt,"Deep neural networks (DNNs) fail to learn effectively under label noise and
have been shown to memorize random labels which affect their generalization
performance. We consider learning in isolation, using one-hot encoded labels as
the sole source of supervision, and a lack of regularization to discourage
memorization as the major shortcomings of the standard training procedure.
Thus, we propose Noisy Concurrent Training (NCT) which leverages collaborative
learning to use the consensus between two models as an additional source of
supervision. Furthermore, inspired by trial-to-trial variability in the brain,
we propose a counter-intuitive regularization technique, target variability,
which entails randomly changing the labels of a percentage of training samples
in each batch as a deterrent to memorization and over-generalization in DNNs.
Target variability is applied independently to each model to keep them diverged
and avoid the confirmation bias. As DNNs tend to prioritize learning simple
patterns first before memorizing the noisy labels, we employ a dynamic learning
scheme whereby as the training progresses, the two models increasingly rely
more on their consensus. NCT also progressively increases the target
variability to avoid memorization in later stages. We demonstrate the
effectiveness of our approach on both synthetic and real-world noisy benchmark
datasets.","Much of the recent advances in deep learning can be attributed to supervised learning algorithms which require huge amounts of annotated data [6, 20]. However, manu ally annotating the data is laborious and usually expensive task [25] which can be prone to error when not veriÔ¨Åed by multiple annotators. Furthermore, to utilize the widespread opensource data, various techniques were proposed for au tomatically annotating the data using user tags and key words [22, 34] and scaling up crowdsourced datasets [24]. Equal contribution.Accepted as a conference paper at WACV 2021. 0 50 100 150 200012LossCE Clean Noisy 0 50 100 150 2001.52.02.5NCT 0 50 100 150 200 Epoch0255075100Accuracy (%) 0 50 100 150 200 Epoch0255075100Figure 1. Average crossentropy loss and accuracy on CIFAR10 with 50% symmetric label noise for the training samples with clean and noisy labels across the training epoch. Left: As training progresses standard model with crossentropy loss (CE) memorizes the noisy labels. Right: Our proposed method, noisy concurrent training (NCT) effectively pre vents the models from memorizing the noisy labels even though no dis tinction is made between them during training. While these approaches allow the creation of large datasets for training, they lead to noisy annotations. A number of studies have shown that label noise has an adverse effect on the performance of the models [8, 30, 38]. It is therefore pertinent to adapt the training procedure to leverage these datasets. Deep neural networks (DNNs) have been shown to easily Ô¨Åt random labels [2] which makes it challenging to train the models efÔ¨Åciently. The majority of the existing methods for training under label noise can be broadly categorized into two approaches: i) correcting the labels by estimating the noise transition matrix [9, 26], ii) identifying the noisy la bels to either Ô¨Ålter out [10, 37] or downweight those sam ples [13, 23]. However, the former approach depends on accurately estimating the noise transition matrix which is difÔ¨Åcult especially for a high number of classes, and the latter approach requires an efÔ¨Åcient method for identifying noisy labels and/or an estimate of the percentage of noisy instances. Amongst these, there has been more focus on separating the noisy and clean instances where a common criterion is to consider lowloss instances as a proxy for clean labels [1, 10]. However, harder instances can be per ceived as noisy and hence the model can be biased towards 1arXiv:2009.08325v1  [cs.CV]  17 Sep 2020easy instances. Both approaches consider the annotations quality as the primary reason for the decrease in model‚Äôs performance and hence the proposed solutions rely on accu rately relabeling, Ô¨Åltering out or downweighting instances with incorrect labels. Here we provide an alternative viewpoint on the issue of learning with noisy labels and attempt to improve the ro bustness of the underlying training framework. We focus on the insufÔ¨Åciency of the standard training method. The crossentropy loss maximizes a bound on the mutual infor mation between onehot encoded labels and the learned rep resentation. The model receives no information about the similarity of a data point among the classes and hence when the provided label is incorrect, it has no source of useful information about the instance or extra supervision to mit igate the adverse effect of the noisy label. There is also a lack of regularization to discourage the model from memo rizing the training labels. To overcome these issues, we propose noisy concurrent training (NCT) which introduces variability in supervision signal in a collaborative learning framework and takes ad vantage of building consensus among two different models. Each model, in addition to a supervised learning loss, is trained with a mimicry loss that aligns the posterior distri butions of the two models for building consensus on the sec ondary class probabilities as well as the primary class pre diction. To discourage memorization, we derive inspiration from neuroscience where the role of noise in the nervous system has been extensively studied. Based on trialtotrial response variation in the brain [28] and the constructive role noise plays in forcing the biological neural networks to be more robust and explore more states [7], we propose to use a counterintuitive regularization technique we refer to as target variability as a deterrent to memorization and over generalization in DNNs. SpeciÔ¨Åcally, target variability entails randomly changing the labels of a percentage of training samples in a batch, independently for each model. In addition to discourag ing memorization, this keeps the two models sufÔ¨Åciently di verged and therefore retains the beneÔ¨Åts of mutual learning, i.e. Ô¨Åltering different types of errors and avoiding conÔ¨År mation bias in selftraining. Furthermore, since DNNs tend to learn simple patterns Ô¨Årst and memorize the noisy labels in the later epochs [2], NCT employs a dynamic learning scheme whereby as training progresses, the contribution of the supervised learning loss diminishes and the models fo cus more on building consensus. NCT also progressively increases the target variability to counter the higher ten dency of DNNs to memorize the noisy labels at the later stages. We show the efÔ¨Åcacy of our proposed approach on noisy versions of CIFAR10, CIFAR100 [14], and Tiny ImageNet [17] as well as two realworld noisy datasets Clothing1M [35] and WebVisionv1 [19]. Empirical resultsshow the versatility and effectiveness of NCT under differ ent noise types and noise levels. In addition to improving the performance of the model on noisy datasets, NCT also improves the performance on clean datasets which demon strates its utility as a generalpurpose robust learning frame work. 2. Related Work "
515,PADDLES: Phase-Amplitude Spectrum Disentangled Early Stopping for Learning with Noisy Labels.txt,"Convolutional Neural Networks (CNNs) have demonstrated superiority in
learning patterns, but are sensitive to label noises and may overfit noisy
labels during training. The early stopping strategy averts updating CNNs during
the early training phase and is widely employed in the presence of noisy
labels. Motivated by biological findings that the amplitude spectrum (AS) and
phase spectrum (PS) in the frequency domain play different roles in the
animal's vision system, we observe that PS, which captures more semantic
information, can increase the robustness of DNNs to label noise, more so than
AS can. We thus propose early stops at different times for AS and PS by
disentangling the features of some layer(s) into AS and PS using Discrete
Fourier Transform (DFT) during training. Our proposed Phase-AmplituDe
DisentangLed Early Stopping (PADDLES) method is shown to be effective on both
synthetic and real-world label-noise datasets. PADDLES outperforms other early
stopping methods and obtains state-of-the-art performance.","Learning from noisy labels (LNL) [1] is an active area of research within the deep learning community [12,14,32,40, 57,60,62]. Noisy labels are common in realworld applica tions [44,49,54,59], and trustworthy AI should be robust to mislabelling. It has been argued that CNNs learn Ô¨Årst the actual pattern before over Ô¨Åtting the noise [3], which inspired many works in LNL [14,24,25,27,28,51,56]. A training strategy is early stopping (ES), which stops the gradientbased optimization at a speciÔ¨Åc early training step. Due to its effectiveness, ES is widely applied in current LNL models and has achieved promising performance [4, 24, 27, 33, 46]. The frequency and spatial domains are alternative codes *CoÔ¨Årst authors. 1Codes will be available upon acceptance.for depicting signal data such as images and text [36, 45]. Different frequency components contain different informa tion [7]. The amplitude spectrum (AS) quantiÔ¨Åes how much of each sinusoidal component is present, while the phase spectrum (PS) reveals the location of each sinusoidal com ponent within an image. Biological justiÔ¨Åcation and psy chological patterns testing [13, 42] demonstrate that the re sponse of cells in the primary visual cortex (V1) is closely related to the local AS for speciÔ¨Åc image patterns (fre quency and orientation). That is, the AS component usu ally represents the intensity of the patterns in the image. On the other hand, previous qualitative and quantitative stud ies [7, 13] indicate that the PS is the key to locating salient object areas and holds visible structured information for vi sion recognition [11, 23, 35], thus contains more semantic information than the AS. As a robust vision system, human vision focuses on se mantic parts during object recognition, and relies more on the image components related to the PS than the AS [8, 13, 23,35]. This system builds a strong connection between se mantic feature space and label space, helping humans ‚Äòun derstand‚Äô the actual correlation between objects and their corresponding identiÔ¨Åers (labels). The human visual system is very robust to label noise. However, CNNs proÔ¨Åt from human unperceivable highfrequency information in im ages [18, 50]. Without adequate regulations, CNNs model the correlation of objects and their labels mainly based on the connection between AS and the given annotations. Such overdependence is demonstrated as the leading cause of their sensitivity to image perturbation and overconÔ¨Ådence in outofdistribution (OOD) detection [8]. We argue that CNNs‚Äô overdependence of connection between the less se mantic AS and labels may spoil their recognition robust ness, resulting in their vulnerability to label noise. To investigate the impact of label noise on deep models trained with different image components, we generate sym metric label noise [14, 48] with a 50% noise rate and feed it with raw images, PS and AS to a ResNet18 [15] model separately. As shown in Figures 1a and 1b, the convergence speed of CNNs on AS and PS differs. When CNNs startarXiv:2212.03462v1  [cs.CV]  7 Dec 202225 50 75 100 Epochs0.51.01.52.0Training LossTrain_IM Train_AS Train_PS(a) Cleanly labeled examples 25 50 75 100 Epochs1.01.52.02.5Training LossTrain_IM Train_AS Train_PS (b) Wrongly labeled examples 25 50 75 100 Epochs020406080T est AccuracyTrain_IM Train_AS Train_PS (c) Test accuracy with noisy labels Figure 1. Results of training a ResNet18 model on CIFAR10 using original images, amplitude spectrum, and phase spectrum (‚ÄúTrain IM‚Äù, ‚ÄúTrain AS‚Äù, and ‚ÄúTrain PS‚Äù in the Figure) on cleanly and noisily labeled subsets. The curves are averaged across Ô¨Åve random runs. The dotted vertical lines indicate the best performance steps of different image components. The converging speed of the deep model trained on AS and PS differs, especially on wrongly labeled examples. Approaching the end of the training, when the wrong labels begin to be memorized, the model accelerates Ô¨Åtting to AS, resulting in an intersection on the training curves of AS and PS, shown in Figure 1b. Hence, PS can help the deep model become more resistant to label noises than AS. to overÔ¨Åt the noisy labels, they Ô¨Åt AS much faster than PS (Figure 1b). Meanwhile, the convergence speed on PS is slower than AS and the raw images, which indicates that PS can help the CNNs become more robust towards mislabels than AS or raw inputs. Note that the model trained with only AS or PS performs worse than the one trained with the raw images (Figure 1c). This is not surprising as either AS or PS could miss some information from the original image data. Therefore, an intuitive solution to improve the robust ness of the CNNs to the noisy labels is choosing different early stop points for AS and PS, during the training of the CNNs. In this way, we can suppress the overdependence of CNNs on AS while shift to utilize more PS components. Current CNNs are trained based on gradients update via backward propagation. The raw images are Ô¨Åxed and do not need gradient computing during the optimization. There fore, it is hard to control the model optimization on raw AS and PS directly. To tackle this challenge, we propose to use deep features to represent the ‚Äòimage‚Äô, as each ‚Äòpixel‚Äô of the feature map corresponds to an original image patch. Moreover, a similar study to that shown in Figure 1 for the deep features of ResNet blocks supports our solution. We observe that different frequency components from the deep features hold a similar property to those from the raw image (Please refer to the supplemental materials for this study). SpeciÔ¨Åcally, we propose to disentangle the deep im age features into AS and PS at different training steps by Discrete Fourier Transform (DFT). We Ô¨Årst detach the AS component from the gradient computational graph to stop its involvement in the model update, which can alleviate the potential negative effects of AS in the later training stage. With AS being detached, we continue train the deep model with PS components. The optimization on the PS compo nents will be stopped after a few training epochs. Noticethat the detached components will regenerate the deep fea tures in the spatial domain through inverse DFT (iDFT). This is efÔ¨Åcient as there is no modiÔ¨Åcation to the original architecture. Moreover, complete information is used for training. We call the proposed method as PhaseAmplituDe DisentangLed Early Stopping (PADDLES). To the best of our knowledge, PADDLES is the Ô¨Årst method to consider features learned with noisy labels in the frequency domain and thus is orthogonal to existing methods that mainly focus on the spatial domain. Our contributions are as follows: ‚Ä¢ We study learning with noise labels from the frequency domain and Ô¨Ånd that PS can help CNNs become more resistant to label noise than AS. ‚Ä¢ We propose to early stop training at different stages for AS and PS. We demonstrate that our proposed method can beneÔ¨Åt from the robustness of the PS without los ing information on AS during the training of CNNs. ‚Ä¢ Extensive experiments on benchmark datasets such as CIFAR10/100, CIFAR10N/100N, and Clothing1M validate the effectiveness of the proposed method. 2. Related Work "
516,Leaf Identification Using a Deep Convolutional Neural Network.txt,"Convolutional neural networks (CNNs) have become popular especially in
computer vision in the last few years because they achieved outstanding
performance on different tasks, such as image classifications. We propose a
nine-layer CNN for leaf identification using the famous Flavia and Foliage
datasets. Usually the supervised learning of deep CNNs requires huge datasets
for training. However, the used datasets contain only a few examples per plant
species. Therefore, we apply data augmentation and transfer learning to prevent
our network from overfitting. The trained CNNs achieve recognition rates above
99% on the Flavia and Foliage datasets, and slightly outperform current methods
for leaf classification.","Currently, supervised learning of convolutional neural networks (CNNs) for classiÔ¨Åca tion tasks achieve stateoftheart performances on a wide range of datasets, e.g. MNIST [13] and ImageNet [5]. Even though these datasets are usually huge in the amount of ex amples per class optimum values are mostly achieved by using data augmentations. The Flavia [22] and Foliage [9] datasets used in this paper include approximately 60 images per class in Flavia and 120 in Foliage, which is why data augmentation is extremely im portant to obtain a reliable generalization of the trained networks. Moreover, we apply further techniques that help prevent overÔ¨Åtting of the network. Those are dropout [20] and transfer learning which provides an initial guess for the weights of the network, e.g. [6]. As a result, the trained 9layer CNNs achieve outstanding recognition rates above 99% that also outperform slightly the current stateoftheart published by Sulc and Matas [21], who utilize a texturebased leaf identiÔ¨Åcation based on local feature histograms. The following paper is structured as follows. At Ô¨Årst, we introduce similar work which is also used to compare our results. Next, we present the model we used. This includes preprocessing, the network structure, batch generation, data augmentation, pre training, and the execution parameters of our experiments. Finally, we demonstrate the inÔ¨Çuence of augmentations, pretraining, and dropout on the accuracy and compare our results to other published values.arXiv:1712.00967v1  [cs.CV]  4 Dec 20172 Related Work "
517,Bioresorbable Scaffold Visualization in IVOCT Images Using CNNs and Weakly Supervised Localization.txt,"Bioresorbable scaffolds have become a popular choice for treatment of
coronary heart disease, replacing traditional metal stents. Often,
intravascular optical coherence tomography is used to assess potential
malapposition after implantation and for follow-up examinations later on.
Typically, the scaffold is manually reviewed by an expert, analyzing each of
the hundreds of image slices. As this is time consuming, automatic stent
detection and visualization approaches have been proposed, mostly for metal
stent detection based on classic image processing. As bioresorbable scaffolds
are harder to detect, recent approaches have used feature extraction and
machine learning methods for automatic detection. However, these methods
require detailed, pixel-level labels in each image slice and extensive feature
engineering for the particular stent type which might limit the approaches'
generalization capabilities. Therefore, we propose a deep learning-based method
for bioresorbable scaffold visualization using only image-level labels. A
convolutional neural network is trained to predict whether an image slice
contains a metal stent, a bioresorbable scaffold, or no device. Then, we derive
local stent strut information by employing weakly supervised localization using
saliency maps with guided backpropagation. As saliency maps are generally
diffuse and noisy, we propose a novel patch-based method with image shifting
which allows for high resolution stent visualization. Our convolutional neural
network model achieves a classification accuracy of 99.0 % for image-level
stent classification which can be used for both high quality in-slice stent
visualization and 3D rendering of the stent structure.","Coronary heart disease is one of the most frequent causes of death despite being treatable. To treat the obstructive plaques, stenting is commonly used with mostly metallic stents being used in the past. As metal stents come with the risk of late stent thrombosis and instent restenosis,1bioresorbable scaolds such as bioresorbable vascular scaolds (BVS) have gained popularity recently. After implantation and in later followup examinations, the stents have to be assessed by the medical expert in order to detect malapposition or assess endothelialisation. Typically, intravascular optical coherence tomography (IVOCT) is used as an imaging modality for stent analysis2 as it provides high resolution images of the lumen and vessel walls. As a single IVOCT pullback contains hundreds of image slices to be assessed, manual evaluation is laborintensive and time consuming. Therefore, automatic stent detection and visualization methods have been proposed, mostly for metallic stents.3{5These methods largely rely on classic image processing to detect the highintensity metal stent struts. For bioresorbable scaolds, a classic approach has also been proposed.6However, since these struts are less pronounced in IVOCT images and dierent types of scaolds show dierent characteristics, recent approaches have used machine learning methods combined with handcrafted feature extraction for detection and visualization.7While showing promising results, these methods require pixellevel image annotations to learn local detection of stent struts within the slices. This Further author information: (Send correspondence to Nils Gessert) Nils Gessert: nils.gessert@tuhh.dearXiv:1810.09578v1  [cs.CV]  22 Oct 2018is, once again, time consuming and limits the potential dataset size and variability. Moreover, features need to be engineered for a specic stent type which might not be suitable for future stent variations which has already become evident during the transition from metal stents to bioresorbable scaolds.6,7 For this reason, we propose a novel deep learningbased method for stent visualization and potential detection using only imagelevel label annotations. A convolutional neural network (CNN) is trained to classify an IVOCT slice into the categories ""metal stent"", ""bioresorbable scaold"" and ""no device"". This way of imagelevel labeling has been successful for IVOCTbased deep learning8as it is fast and thus allows for larger datasets. Moreover, it is easily extensible to new stent types as a new class simply needs to be added to the learning problem and no new feature engineering is required. After training the model, we employ the concept of weakly supervised localization9to derive local stent strut information from the model. In particular, we compute saliency maps with guided backpropagation10which can be interpreted as a gradient image which shows the regions that were most important for the model's prediction. In our case, the trained network should have learned to focus on stent struts. However, saliency maps are generally diuse and high quality localization from global information only is very challenging.9For this reason, methods such as SmoothGrad11have been proposed which are targeted at improved saliency map quality. As we found this method to be insucient for the problem at hand, we propose a new patchbased approach with image shifting which leads to high resolution, high quality saliency maps that can serve as a visualization. The approach is used for regularization during model training and also for generation of stitched and averaged, smooth saliency maps. In this paper, we introduce our method for BVS visualization. We show that it is eective when visualizing stents for assessment after apposition with struts at the tissue surface as well as for followup review of, e.g. endothelialisation, where struts are starting to decay within the vessel tissue. Moreover, we show visualization of classic metal stents and the very recent Fantom Encore bioresorbable scaold. 2. METHODS AND MATERIALS "
518,Structured Label Inference for Visual Understanding.txt,"Visual data such as images and videos contain a rich source of structured
semantic labels as well as a wide range of interacting components. Visual
content could be assigned with fine-grained labels describing major components,
coarse-grained labels depicting high level abstractions, or a set of labels
revealing attributes. Such categorization over different, interacting layers of
labels evinces the potential for a graph-based encoding of label information.
In this paper, we exploit this rich structure for performing graph-based
inference in label space for a number of tasks: multi-label image and video
classification and action detection in untrimmed videos. We consider the use of
the Bidirectional Inference Neural Network (BINN) and Structured Inference
Neural Network (SINN) for performing graph-based inference in label space and
propose a Long Short-Term Memory (LSTM) based extension for exploiting activity
progression on untrimmed videos. The methods were evaluated on (i) the Animal
with Attributes (AwA), Scene Understanding (SUN) and NUS-WIDE datasets for
multi-label image classification, (ii) the first two releases of the YouTube-8M
large scale dataset for multi-label video classification, and (iii) the
THUMOS'14 and MultiTHUMOS video datasets for action detection. Our results
demonstrate the effectiveness of structured label inference in these
challenging tasks, achieving significant improvements against baselines.","VISUAL content is a rich source of highdimensional structured data, with a wide range of interacting com ponents at varying levels of abstractions. With the prolif eration of largescale image [1], [2], [3], [4] and video [5], [6], [7] datasets, advances in visual understanding were fa cilitated for the exploration and enhancement of intelligent reasoning techniques for modelling structured concepts. In this paper, we exploit these rich structures for modelling concept interactions in a number of different tasks and levels of complexity: multilabel image classiÔ¨Åcation, multi label video classiÔ¨Åcation and action detection in untrimmed videos. Standard image classiÔ¨Åcation is a fundamental problem in computer vision ‚Äì assigning category labels to images. It can serve as a building block for many different computer vision tasks including object detection, visual segmentation, scene parsing and concept localization. Successful deep learning approaches [8], [9], [10], [11] typically assume labels to be semantically independent and adapt either a multiclass or binary classiÔ¨Åer to target labels. In recent work [12], [13], deep learning methods that take advantage of label relations have been proposed to improve classiÔ¨Åca tion performance. However, in realistic settings, these label relationships could form a complicated graph structure. Take Figure 1 as an example. Various levels of interpretation could be formed to represent such visual content. This image of a baseball scene could be described as an outdoor image at coarse level, or with a more concrete concept such assports Ô¨Åeld , or with even more Ô¨Ånegrained labels such as batter‚Äôs box and objects such as grass ,bat,person . Models that incorporate semantic label relationships could be utilized to generate better classiÔ¨Åcation results. Fig. 1: This image example has visual concepts at various levels, from sports Ô¨Åeld at a high level to baseball and person at lower levels. Our model leverages label relations and jointly predicts layered visual labels from an image using a structured inference neural network. In the graph, colored nodes correspond to the labels associated with the image, and red edges encode label relations. The desiderata for these models include the ability to model labellabel relations such as positive or negative correlation, respect multiple concept layers obtainable from sources such as WordNet, and to handle partially observed label data given a subset of accurate labels for this image, infer the remaining missing labels.arXiv:1802.06459v1  [cs.CV]  18 Feb 2018IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , MONTH Z 2 In our previous work [14], we developed a structured inference neural network that permits modeling complex re lations between labels, ranging from hierarchical to within layer dependencies. We achieve this by deÔ¨Åning a network in which a node is activated if its corresponding label is present in an image. We introduce stacked layers among these label nodes. These encode layerwise connectivity among label classiÔ¨Åcation scores, representing dependencies from toplevel coarse labels to bottomlevel Ô¨Ånegrained labels. Activations are propagated bidirectionally and asyn chronously on the label relation graph, passing information about the labels within or across concept layers to reÔ¨Åne the labeling for the entire image. The similarity between multilabel classiÔ¨Åcation on im ages and videos suggests exploitation of structured data to be beneÔ¨Åcial for both tasks. As demonstrated in [15], our method extends beyond its applicability on images and is robust to higher dimensional structured data such as videos. A more challenging problem than multilabel video classiÔ¨Å cation consists of handling a sequential input of frames and inferring the corresponding sequence of dense annotations. Our exploration of this setting for the task of dense action detection [7] is twofold: performing static frame structured inference and spatiotemporal structured inference. The static frame inference can be reduced to a standard image classiÔ¨Åcation problem. However, spatiotemporal structured inference requires modelling crosstemporal re lationships between labels. A natural way of extending [15] to support this feature is allowing communication between concept layers in the hierarchical structure for forward prop agating learned label correlations and exploring labels‚Äô pro gression on untrimmed videos. We achieve this by enriching the hidden state of Long ShortTerm Memory (LSTM) [16] units with extracted information from our structured infer ence method along the frame sequences. The contribution of this paper is binding together the proposed model presented in [14] for performing hierarchi cal inference on image datasets (AwA [3] and SUN397 [4]) and its video extension previously implemented in [15]. In addition, we include novel results for the two recent releases of Youtube8M [5], including partially observed labels and propose a temporal extension for the bidirectional and struc tured inference models, demonstrating that adding cross temporal information in label space (i.e. propagation across concept layers) provides superior performance against a tra ditional technique for incorporating temporal dependencies (i.e. LSTM). The validation of the proposed models was carried out on THUMOS [6] and MultiTHUMOS [7] for the task of action detection. This paper is organized as follows. Section 2 presents prior knowledge, covering previous related work. In Sec tion 3, we bind together the bidirectional and structured inference models formulated in [14] with its video exten sion presented in [15] and derive the proposed formulation for temporal extension of our bidirectional and structured inference methods. Sections 4 and 5 present the work done on multilabel image and video classiÔ¨Åcation respectively. Section 6 describes the work done on the action detection task. We conclude in Section 7 with a brief discussion.2 R ELATED WORK "
519,Weakly Supervised Pseudo-Label assisted Learning for ALS Point Cloud Semantic Segmentation.txt,"Competitive point cloud semantic segmentation results usually rely on a large
amount of labeled data. However, data annotation is a time-consuming and
labor-intensive task, particularly for three-dimensional point cloud data.
Thus, obtaining accurate results with limited ground truth as training data is
considerably important. As a simple and effective method, pseudo labels can use
information from unlabeled data for training neural networks. In this study, we
propose a pseudo-label-assisted point cloud segmentation method with very few
sparsely sampled labels that are normally randomly selected for each class. An
adaptive thresholding strategy was proposed to generate a pseudo-label based on
the prediction probability. Pseudo-label learning is an iterative process, and
pseudo labels were updated solely on ground-truth weak labels as the model
converged to improve the training efficiency. Experiments using the ISPRS 3D
sematic labeling benchmark dataset indicated that our proposed method achieved
an equally competitive result compared to that using a full supervision scheme
with only up to 2$\unicode{x2030}$ of labeled points from the original training
set, with an overall accuracy of 83.7% and an average F1 score of 70.2%.","The recent development of convolutional neural networks (CNNs) has been followed by the progress of point cloud se mantic segmentation tasks, and there are constantly new meth ods that achieve stateoftheart results. Most of these meth ods focus on designing new network structures or convolution kernels based on the characteristics of point cloud data (Hu et al., 2020; Thomas et al., 2019; Huang et al., 2020; Steinsiek et al., 2017), whereas few focus on data labeling. In experi ments, competitive results rely on a large amount of data an notations, which continuously require labeling operators with expert knowledge. The labeling of point cloud data is particu larly difÔ¨Åcult, and the operator usually needs to conÔ¨Årm the cat egory of a point from multiple perspectives. Although labeling work is difÔ¨Åcult and timeconsuming, obtaining raw point cloud data has become easier. With the advancement of light detection and ranging sensor technology and the diversiÔ¨Åcation of data acquisition platforms, obtaining massive unlabeled point cloud data is no longer a problem. Thus, extracting information that helps the task of semantic segmentation from unlabeled data is an essential issue. By using a small amount of annotated data to achieve classiÔ¨Åcation results comparable to full supervision, the workload of data annotation would be considerably reduced and the efÔ¨Åciency in related applications would be improved. Semi and weakly supervised learning are commonly used to address situations in which the number of labels is scarce. There are some comprehensive reviews on these two types of supervised learning methods (Chapelle et al., 2010; Zhou, 2017; Zhu, 2005). Choosing a suitable weak label annotation strategy is key to balancing the labeling workload and experimental results. In image processing, weak labels are represented as a few labeled images (Dong and Xing, 2018), a few labeled Corresponding author. (a)  (b) Figure 1. Comparison of (a) fullylabeled points and (b) weak label. (b) Situation in which only 100 points per class ( 2h) are labeled. The size of labeled points in (b) is enlarged for better visualization. pixels (Bearman et al., 2016), and a few bounding boxes or cat egories that appear on the images (Kolesnikov and Lampert, 2016; Papandreou et al., 2015). Until now, there have been very few works that used weakly supervised methods to pro cess point cloud data. Wei et al. (2020) applied a point class activation map to classify point clouds using only cloudlevel labels. However, we believe that it is not practical for point cloud data, particularly in outdoor scenes that cover a wide region, because the complete data must be divided into many small blocks and categories contained in each block must be speciÔ¨Åed. In comparison, assigning labels to a few points is a more direct approach. Polewski et al. (2015) used an active learning method to detect standing dead trees from airborne laser scanning (ALS) point cloud data combined with infrared images. Lin et al. (2020) proposed an active and incremental learning strategy for ALS semantic segmentation, and manual annotation was iteratively added for training. Nonetheless, the setting of a weak label is used to annotate all points of tiles, andarXiv:2105.01919v1  [cs.CV]  5 May 2021manual intervention is required during training. Through the oretical analysis and experimental results, Xu and Lee (2020) found that when the number of labeled points remains constant, the spatially sparse annotations achieve better results than those samples gathering in certain object instances, and this weak label setting was adopted in this work. An illustration of the sparse weaklabel situation is shown in Fig. 1. A weakly super vised semantic point cloud segmentation framework was pro posed by Xu and Lee (2020), and an approximate result of fully supervised learning was obtained using 10 %labels. However, the characteristic of this weak label can be understood as being spatially continuous at a lower resolution, and the workload of labeling is still large. Guinard and Landrieu (2017) utilized the point cloud segmentation method to improve the classiÔ¨Åcation accuracy with very few labels, but the result largely depends on the segmentation accuracy and focuses on classifying the point cloud of the area where the initial weak label is located, which is transductive learning. Pseudolabels, assigning annotations to unlabeled data based on the predictions of the current model, can enable the use of un labeled data in parameter updates. Through the pseudolabel method, the classiÔ¨Åcation model can calculate the loss func tion and backpropagation from the dataset with more labels, thereby improving the accuracy. Iscen et al. (2019) and Lee (2013) applied pseudolabels in image classiÔ¨Åcation, and Zou et al. (2021) designed pseudolabeling and data augmentation to improve the performance of image semantic segmentation. Ber thelot et al. (2019) mixed several proven semisupervision strategies and proposed a holistic framework. Only a few stud ies have utilized pseudolabels in point cloud processing. Yao et al. (2020) introduced a pseudolabeling method into point cloud semantic segmentation. However, similar to Guinard and Landrieu (2017), the framework is also transductive learning , and the performance of the model has not been veriÔ¨Åed on un trained test data. To cooperate with pseudolabels, it is essential to propose an effective semantic segmentation network structure. Owing to the irregular distribution of point clouds, the network structure is not as uniform as that of twodimensional (2D) CNNs. Early processing methods project point clouds onto images and dir ectly use mature 2D CNNs to train the model (Boulch et al., 2018). Another branch of methods voxelizes point clouds and proposes threedimensional (3D) CNNs to process voxel data (Tchapmi et al., 2017). The disadvantage of these two meth ods is that as point clouds need to be converted into regularized data, geometric information may be lost. PointNet and Point Net++ (Qi et al., 2017a; Qi et al., 2017b) are pioneers in the development of shared multilayer perceptrons (MLPs) to dir ectly analyze point clouds. However, the spatial distribution relationship of the point cloud requires welldesigned MLPs, which complicates network structures. Compared to pointwise MLP networks, graph convolution networks construct a graph through relative spatial positions between points for feature ex traction and fusion (Wang et al., 2019). Point convolution net works are similar to graphbased methods, in which point ker nels are designed to learn local geometric information (Thomas et al., 2019). In this study, we explored how to obtain reliable semantic seg mentation results with very few labels. A pseudolabelassisted point cloud semantic segmentation framework with extremely few annotations is proposed. We used KPConv (Thomas et al., 2019), a point convolution network, as our encoder network. Our weak label was deÔ¨Åned as sparsely labeled points randomlydistributed in space. An initial model was trained using the selected weak labels. Pseudolabels were then generated by the trained model. Considering that the model obtained from weak label training is underÔ¨Åtting to the entire data space, an adaptive threshold is proposed to balance the number and accur acy of pseudolabels. The training procedure that combines the groundtruth labels, referred to weak labels, and pseudolabels was iteratively performed. To accelerate the training progress and reduce the inÔ¨Çuence of pseudo labels containing errors on the model, we updated the pseudo labels when the model con verged on the groundtruth labels. Experiments on the ALS dataset showed the effectiveness of the method. 2. METHODOLOGY "
520,Predicting Ground-Level Scene Layout from Aerial Imagery.txt,"We introduce a novel strategy for learning to extract semantically meaningful
features from aerial imagery. Instead of manually labeling the aerial imagery,
we propose to predict (noisy) semantic features automatically extracted from
co-located ground imagery. Our network architecture takes an aerial image as
input, extracts features using a convolutional neural network, and then applies
an adaptive transformation to map these features into the ground-level
perspective. We use an end-to-end learning approach to minimize the difference
between the semantic segmentation extracted directly from the ground image and
the semantic segmentation predicted solely based on the aerial image. We show
that a model learned using this strategy, with no additional training, is
already capable of rough semantic labeling of aerial imagery. Furthermore, we
demonstrate that by finetuning this model we can achieve more accurate semantic
segmentation than two baseline initialization strategies. We use our network to
address the task of estimating the geolocation and geoorientation of a ground
image. Finally, we show how features extracted from an aerial image can be used
to hallucinate a plausible ground-level panorama.","Learningbased methods for pixellevel labeling of aerial imagery have long relied on manually annotated training data. Unfortunately, such data is expensive to create. Fur thermore, its value is limited because a method trained on one dataset will typically not perform well when applied to another source of aerial imagery. The difÔ¨Åculty in ob taining datasets of sufÔ¨Åcient scale for all modalities has hampered progress in applying deep learning techniques to aerial imagery. There have been a few notable excep tions [21, 23], but these have all used fairly coarse grained semantic classes, covered a small spatial area, and are lim ited to modalities in which human annotators are able to manually assign labels. We propose a novel strategy for obtaining semantic la Cross Entropy Loss  Label Extract  Transform Transfer  Semantics Figure 1. We learn to predict the groundimage segmentation di rectly from an aerial image of the same location, thereby transfer ring the semantics from the ground to the aerial image domain. bels for aerial image segmentation. See Figure 1 for a schematic overview of the approach. Our idea is to use existing methods for semantic image segmentation, which are tailored for ground images, and apply these to a large dataset of geotagged ground images. We use these seman tically labeled images as a form of weak supervision and attempt to predict these semantic labels from an aerial im age centered around the location of the ground image. We do not use a parametric transformation between the aerial and groundlevel viewpoints. Instead, we use a dense rep resentation, similar in spirit to the general representation, dubbed Ô¨Ålter Ô¨Çow, described by Seitz and Baker [26]. There has been signiÔ¨Åcant interest recently in predicting ground image features from aerial imagery for the task of 1arXiv:1612.02709v1  [cs.CV]  8 Dec 2016ground image geolocalization [33]. Our work is unique in that it is the Ô¨Årst to attempt to predict a dense pixellevel seg mentation of the ground image. We demonstrate the value of this approach in several ways. Main Contributions: The main contributions of this work are: (1) a novel convolutional neural network (CNN) architecture that relates the appearance of a aerial image ap pearance to the semantic layout of a ground image of the same location, (2) demonstrating the value of our training strategy for pretraining a CNN to understand aerial im agery, (3) extensions of the proposed technique to the tasks of ground image localization, orientation estimation, and synthesis, and (4) an extensive evaluation of each of these techniques on large, realwold datasets. Together these rep resent an important step in enabling deep learning tech niques to be extended to the domain of aerial image under standing. 2. Related Work "
521,Efficacy of Bayesian Neural Networks in Active Learning.txt,"Obtaining labeled data for machine learning tasks can be prohibitively
expensive. Active learning mitigates this issue by exploring the unlabeled data
space and prioritizing the selection of data that can best improve the model
performance. A common approach to active learning is to pick a small sample of
data for which the model is most uncertain. In this paper, we explore the
efficacy of Bayesian neural networks for active learning, which naturally
models uncertainty by learning distribution over the weights of neural
networks. By performing a comprehensive set of experiments, we show that
Bayesian neural networks are more efficient than ensemble based techniques in
capturing uncertainty. Our findings also reveal some key drawbacks of the
ensemble techniques, which was recently shown to be more effective than Monte
Carlo dropouts.","Although machine learning techniques have achieved a major breakthrough in recent years, their performance comes at a cost of acquiring large volumes of training data. This is especially true for supervised deep learning mod els that demand a substantial amount of labeled data to achieve a reasonable performance. For applications that require expert knowledge such as medical and biological images, labels are extremely hard and expensive to ob tain. Active learning (AL) aims to mitigate this problem by smartly selecting data points to label (from an expert) from a large pool of unlabeled data to improve model per formance. This sampling is typically based on some ac quisition function (AF) which provides a score for each un labeled data that signiÔ¨Åes its level of importance. While there are many approaches to implementing AF [1, 2], uncertaintybased based approaches are shown to be the most effective [3, 4, 5, 6]. Bayesian neural network (BNN) naturally models uncer tainty by learning a probability distribution over the neural network weights. Therefore, for a given input, as we takemultiple realizations of the network, the variance captured by the weights is reÔ¨Çected as the variation in the output, which inturn models uncertainty. BNNs learn by applying a prior distribution over weights and performing variational inference to approximate the posterior distribution. In [7], the authors proved that applying dropout to neural networks is equivalent to a BNN. This theory was further leveraged by proposing Monte Carlo Dropout (MCD) for uncertainty estimation in AL [5]. In a recent work, [6] showed that ensemble of neural networks (EN) outperform MCD when it comes to uncertainty estimation; thus, proving to be the choice for active learning. Consequentially, it is natural to assume EN to perform better than BNN since MCD is equivalent to BNN. However, dropout neural networks form a special class of BNN where the posterior distribution is a special case of spikeandslab distribution. Contrary to this, BNNs allow for a broader class of prior and posterior distributions on weights . In this paper, we reestablish the efÔ¨Åcacy of BNNs in active learning over ensembles and MCD by using a more general scaled normal prior based BNN proposed in [8]. The scaled normal prior is a continuous relaxation of thespikeandslab distribution and subsumes Dropout as a special case. Through extensive experiments on multiple datasets namely, MNIST, Fashion MNIST, CIFAR10 and CIFAR100 and a regression dataset on housing price predic tion we show that the scaled normal prior based BNN pro vides more robust and efÔ¨Åcient active learning over EN and MCD. We perform several experiments to demonstrate the pros and cons of BNN over EN and MCD. For each round of active learning, the models are trained using two different settings: (1) reuse the trained state of the model from pre vious round and retrain on the newly appended datapoints (termed as continual training ) and (2) reset the model pa rameters and retrain from scratch. Our results show that BNN performs signiÔ¨Åcantly better than EN in terms of clas siÔ¨Åcation accuracy when it comes to continual training. In fact, the performance of EN is worse than MCD which can be attributed to overÔ¨Åtting and catastrophic forgetting. That being said, when retrained from scratch, BNN and EN per 4321arXiv:2104.00896v2  [cs.LG]  19 Apr 2021form on a similar level, which is still an advantage for BNN since estimating uncertainty using ensembles is a costly process. We found that EN requires about Ô¨Åve ensembles in order to achieve good active learning performance. This implies, training of Ô¨Åve different i.i.d networks and storing the trained state of every single network instance. BNN on other hand, achieves similar yet a more robust performance with a tradeoff of just doubling parameter size of conven tional neural network. Besides illustrating the overall effectiveness of BNN for active learning, we answer the following questions: (1) do acquisition functions behave the same for Bayesian, ensem ble and MC dropouts? (2) how does model capacity af fect the outcome, do BNNs with lower model capacity per form worse than EN (or MCD)? (3) are BNNs better than EN when predicting challenging class labels? Inspired by the performance of BNNs, we also propose a computation ally efÔ¨Åcient uncertainty estimation method for fully con nected dense layers with ReLU nonlinearity. Since AL in volves repeated uncertainty estimation over large unlabeled dataset, efÔ¨Åcient uncertainty estimation is of huge practi cal importance. In the proposed method, instead of taking multiple instantiations of neural networks to estimate the uncertainty, we perform just one forward pass. In this for ward pass, at each neuron, we approximate the probability distribution parametrically. We show that this algorithm is capable of achieving performances that is onpar with the traditional uncertainty estimation in BNN. To the best of our knowledge, we are the Ô¨Årst to per form comprehensive empirical analysis to demonstrate the efÔ¨Åcacy of BNNs for active learning. While most existing research limit themselves to experiments on small architec tures and dataset, ours does not have such constraints. 2. Related Work "
522,Admix: Enhancing the Transferability of Adversarial Attacks.txt,"Deep neural networks are known to be extremely vulnerable to adversarial
examples under white-box setting. Moreover, the malicious adversaries crafted
on the surrogate (source) model often exhibit black-box transferability on
other models with the same learning task but having different architectures.
Recently, various methods are proposed to boost the adversarial
transferability, among which the input transformation is one of the most
effective approaches. We investigate in this direction and observe that
existing transformations are all applied on a single image, which might limit
the adversarial transferability. To this end, we propose a new input
transformation based attack method called Admix that considers the input image
and a set of images randomly sampled from other categories. Instead of directly
calculating the gradient on the original input, Admix calculates the gradient
on the input image admixed with a small portion of each add-in image while
using the original label of the input to craft more transferable adversaries.
Empirical evaluations on standard ImageNet dataset demonstrate that Admix could
achieve significantly better transferability than existing input transformation
methods under both single model setting and ensemble-model setting. By
incorporating with existing input transformations, our method could further
improve the transferability and outperforms the state-of-the-art combination of
input transformations by a clear margin when attacking nine advanced defense
models under ensemble-model setting. Code is available at
https://github.com/JHL-HUST/Admix.","A great number of works [7, 2, 1] have shown that deep neural networks (DNNs) are vulnerable to adversarial ex amples [31, 7], i.e. the malicious crafted inputs that are *Corresponding author.indistinguishable from the legitimate ones but can induce misclassiÔ¨Åcation on the deep learning models. Such vul nerability poses potential threats to securitysensitive appli cations, e.g. face veriÔ¨Åcation [28], autonomous driving [6] and has inspired a sizable body of research on adversar ial attacks [22, 2, 21, 4, 16, 5, 38, 18]. Moreover, the ad versaries often exhibit transferability across neural network models [25], in which the adversarial examples generated on one model may also mislead other models. The adver sarial transferability matters because hackers may attack a realworld DNN application without knowing any informa tion of the target model. However, under whitebox set ting where the attacker has complete knowledge of the tar get model, existing attacks [2, 11, 1, 21] have demonstrated great attack performance but with comparatively low trans ferability against models with defense mechanisms [21, 33], making it inefÔ¨Åcient for realworld adversarial attacks. To improve the transferability of adversarial attacks, var ious techniques have been proposed, such as advanced gra dient calculations [4, 18, 35], ensemblemodel attacks [19, 15], input transformations [38, 5, 18, 10] and modelspeciÔ¨Åc methods [36]. The input transformation ( e.g. randomly re sizing and padding, translation, scale etc.) is one of the most effective approaches. Nevertheless, we observe that exist ing methods are all applied on a single input image. Since adversarial attacks aim to mislead the DNNs to classify the adversary into other categories, it naturally inspires us to ex plore whether we could further enhance the transferability by incorporating the information from other categories. Themixup operation, that linearly interpolates two ran dom images and corresponding labels, is Ô¨Årstly proposed as a data augmentation approach to improve the generaliza tion of standard training [41, 34, 40]. Recently, mixup is also used for inference [24] or adversarial training [12, 14] to enhance the model robustness. Since mixup adopts the information of a randomly picked image, we try to directly adopt mixup to craft adversaries but Ô¨Ånd that the attack per formance decays signiÔ¨Åcantly under whitebox setting with little improvement on transferability. To craft highly transarXiv:2102.00436v3  [cs.CV]  18 Aug 2021ferable adversaries with the information from other cate gories but not harm the whitebox attack performance, we propose a novel attack method called Admix that calculates the gradient on the admixed image combined with the orig inal input and images randomly picked from other cate gories. Unlike mixup that treats the two images equally and mixes their labels accordingly, the admix operation adds a small portion of the addin image from other categories to the original input but does not change the label. Thus Admix attack could obtain diverse inputs for gradient calculation. Empirical evaluations on standard ImageNet dataset [26] demonstrate that, compared with existing input transforma tions [38, 5, 18], the proposed Admix attack achieves sig niÔ¨Åcantly higher attack success rates under blackbox set ting and maintains similar attack performance under white box setting. By incorporating Admix with other input trans formations, the transferability of the crafted adversaries could be further improved. Besides, the evaluation of the integrated method under the ensemblemodel setting [19] against nine advanced defense methods [17, 37, 39, 20, 8, 3, 27, 23] demonstrates that the Ô¨Ånal integrated method, termed Admix TIDIM, outperforms the stateoftheart SI TIDIM [18] by a clear margin of 3.4% on average, which further demonstrates the high effectiveness of Admix . 2. Related Work "
523,Noise Mitigation for Neural Entity Typing and Relation Extraction.txt,"In this paper, we address two different types of noise in information
extraction models: noise from distant supervision and noise from pipeline input
features. Our target tasks are entity typing and relation extraction. For the
first noise type, we introduce multi-instance multi-label learning algorithms
using neural network models, and apply them to fine-grained entity typing for
the first time. This gives our models comparable performance with the
state-of-the-art supervised approach which uses global embeddings of entities.
For the second noise type, we propose ways to improve the integration of noisy
entity type predictions into relation extraction. Our experiments show that
probabilistic predictions are more robust than discrete predictions and that
joint training of the two tasks performs best.","Knowledge bases (KBs) are important resources for natural language processing tasks like ques tion answering and entity linking. However, KBs are far from complete (e.g., Socher et al. (2013)). Therefore, methods for automatic knowledge base completion (KBC) are beneÔ¨Åcial. Two subtasks of KBC are entity typing (ET) andrelation extraction (RE) . We address both tasks in this paper. As in other information extraction tasks, obtain ing labeled training data for ET and RE is chal lenging. The challenge grows as labels become more Ô¨Ånegrained. Therefore, distant supervision (Mintz et al., 2009) is widely used. It reduces the need for manually created resources. Distant su pervision assumes that if an entity has a type (resp. two entities have a relationship) in a KB, then all sentences mentioning that entity (resp. thosetwo entities) express that type (resp. that relation ship). However, that assumption is too strong and gives rise to many noisy labels. Different tech niques to deal with that problem have been in vestigated. The main technique is multiinstance (MI) learning (Riedel et al., 2010). It relaxes the distant supervision assumption to the assumption that at least one instance of a bag (collection of all sentences containing the given entity/entity pair) expresses the type/relationship given in the KB. Multiinstance multilabel (MIML) learning is a generalization of MI in which one bag can have several labels (Surdeanu et al., 2012). Most MI and MIML methods are based on hand crafted features. Recently, Zeng et al. (2015) in troduced an endtoend approach to MI learning based on neural networks. Their MI method takes the most conÔ¨Ådent instance as the prediction of the bag. Lin et al. (2016) further improved that method by taking other instances into account as well; they proposed MI learning based on selective attention as an alternative way of relaxing the im pact of noisy labels on RE. In selective attention, a weighted average of instance representations is calculated Ô¨Årst and then used to compute the pre diction of a bag. In this paper, we introduce two multilabel ver sions of MI. (i) MIMLMAX takes the maximum instance for each label. (ii) MIMLATT applies, for each label, selective attention to the instances. We apply MIMLMAX and MIMLATT to Ô¨Åne grained ET. In contrast to RE, the ET task we con sider contains a larger set of labels, with a variety of different granularities and hierarchical relation ships. We show that MIMLATT deals well with noise in corpuslevel ET and improves or matches the results of a supervised model based on global embeddings of entities. The second type of noise we address in this pa per inÔ¨Çuences the integration of ET into RE. It hasarXiv:1612.07495v2  [cs.CL]  10 Jan 2017been shown that adding entity types as features im proves RE models (cf. Ling and Weld (2012), Liu et al. (2014)). However, noisy training data and difÔ¨Åculties of classiÔ¨Åcation often cause wrong pre dictions of ET and, as a result, noisy inputs to RE. To address this, we propose a joint model of ET and RE and compare it with methods that integrate ET results in a strict pipeline. The joint model per forms best. Among the pipeline models, we show that using probabilities instead of binary decisions better deals with noise (i.e., possible ET errors). To sum up, our contributions are as follows. (i) We introduce new algorithms for MIML us ing neural networks. (ii) We apply MIML to Ô¨Åne grained entity typing for the Ô¨Årst time and show that it outperforms the stateoftheart supervised method based on entity embeddings. (iii) We show that a novel way of integrating noisy entity type predictions into a relation extraction model and joint training of the two tasks lead to large im provements of RE performance. We release code and data for future research.1 2 Related Work "
524,Refining Pseudo Labels with Clustering Consensus over Generations for Unsupervised Object Re-identification.txt,"Unsupervised object re-identification targets at learning discriminative
representations for object retrieval without any annotations. Clustering-based
methods conduct training with the generated pseudo labels and currently
dominate this research direction. However, they still suffer from the issue of
pseudo label noise. To tackle the challenge, we propose to properly estimate
pseudo label similarities between consecutive training generations with
clustering consensus and refine pseudo labels with temporally propagated and
ensembled pseudo labels. To the best of our knowledge, this is the first
attempt to leverage the spirit of temporal ensembling to improve classification
with dynamically changing classes over generations. The proposed pseudo label
refinery strategy is simple yet effective and can be seamlessly integrated into
existing clustering-based unsupervised re-identification methods. With our
proposed approach, state-of-the-art method can be further boosted with up to
8.8% mAP improvements on the challenging MSMT17 dataset.","Recent years witnessed the remarkable progresses of employing unsupervised representation learning in various downstream visual recognition tasks, such as image classi Ô¨Åcation [1, 13, 14, 20], object detection [23, 22, 17, 33], and object reidentiÔ¨Åcation (reID) [27, 28, 37, 46, 10]. Ob ject reID aims at retrieving objects of interest in largescale gallery images given an object‚Äôs query images. The task of unsupervised object reID further requires learning dis criminative representations to properly model inter/intra identity afÔ¨Ånities without any annotations, which is a more *The Ô¨Årst two authors contribute equally. +hIkG]Y<DIYh<j+hIkG]Y<DIYh<j ""Ijq]gX ""Ijq]gX 0g<Q[Q[Oh<ZdYIh /kdIgpQhIGDsdhIkG]Y<DIYh+hIkG]¬üY<DIYE][NQGI[EIh/kdIgpQhIGDsgINQ[IGdhIkG]Y<DIYh+hIkG]Y<DIYdg]d<O<jQ][ YkhjIgQ[O ][hI[hkhFigure 1: Illustration of the proposed ReÔ¨Åning pseudo La bel with Clustering Consensus (RLCC) framework. Hard pseudo labels or soft pseudolabel conÔ¨Ådences from the pre vious generation t 1can be temporally propagated to gen erationtto effectively reÔ¨Åne the pseudo labels at generation tto signiÔ¨Åcantly improve the performance of unsupervised object reidentiÔ¨Åcation. practical setup in realworld applications. Pseudolabelbased methods with a clusteringbased la bel generation scheme were found effective in stateofthe art semisupervised/unsupervised object reID approaches [27, 46, 10, 9, 47, 8, 49]. An iterative and alternative twostage pipeline is adopted in each training generation (epoch), i.e., creating pseudo labels and training the net work with the generated pseudo labels. Although multi ple attempts on improving the quality of the pseudo la bels have been investigated, the training is still substantially hindered by the inevitable label noise, showing noticeable performance gaps compared to the oracle experiments with groundtruth identities [10]. We argue that properly reÔ¨Åning the pseudo labels is at the core of further improving unsu pervised reID algorithms. To tackle the challenge, we propose a simple yet ef fective pseudo label reÔ¨Ånery strategy following the similar 1arXiv:2106.06133v2  [cs.CV]  23 Aug 2021spirit of temporal ensembling [25], i.e., the pseudo labels from the past generation (epoch) also carry valuable super vision information and can help mitigate the pseudo label noise by smoothing the pseudo label variations. The temporal ensembling technique has been widely adopted in semisupervised learning [25, 35, 9] and self supervised learning [15, 12] tasks. It aims at generating more robust supervision signals via aggregating models or predictions with a moving average strategy over previous generations (epochs). However, it is nontrivial to improve the pseudo labels in unsupervised object reID tasks with offtheshelf label temporal ensembling methods [25, 35], since they assume that the class deÔ¨Ånitions of the recogni tion tasks remain Ô¨Åxed over training generations. In con trast, pseudo labels in different training generations for un supervised reID vary much as the pseudo labels are always updated after each generation. Towards this end, we introduce ReÔ¨Åning pseudo Labels with Clustering Consensus over consecutive training gener ations ( RLCC ). SpeciÔ¨Åcally, we estimate the pseudolabel similarities over two consecutive generations with an In tersection over Union (IoU) criterion over the samplelabel assignments, where a larger value indicates higher consen sus between two pseudo classes in two consecutive genera tions. To exploit the valuable temporal knowledge encoded by the pseudo labels, we propose to propagate hard or soft pseudo labels from the previous generation to the current generation. The propagation is conducted via a random walk over the pseudo labels, guided by the crossgeneration pseudolabel similarities. Given the temporally propagated labels, the noisy pseudo labels at the current generation can be properly reÔ¨Åned via a momentum averaging formula tion. Our proposed reÔ¨Åned pseudo labels can be readily in tegrated into existing clusteringbased unsupervised reID approaches [27, 46, 10] with marginal modiÔ¨Åcations, i.e. replacing the conventional hard pseudo labels with the pro posed temporally propagated and ensembled soft pseudo la bels. Our contributions can be summarized into threefold. (1) We introduce to leverage the spirit of temporal ensembling to regularize the noisy pseudo labels in unsupervised object reID. Note that existing temporal ensembling techniques [25, 35] are all designed for closeset classiÔ¨Åcation mod els, which are not applicable in our task. (2) We propose a simple yet effective pseudo label reÔ¨Ånery strategy: reÔ¨Åning pseudo labels with clustering consensus over training gen erations (epochs). Our proposed strategy is well compati ble with existing pseudolabelbased methods [27, 46, 10] and leads to further improvements on the already high performance baseline. (3) Our method outperforms state ofthearts on multiple benchmarks for unsupervised ob ject reID, surpassing stateoftheart unsupervised method SpCL [10] with up to 8.8% mAP improvements.2. Related Works "
525,Reddening-free Q indices to identify Be star candidates.txt,"Astronomical databases currently provide high-volume spectroscopic and
photometric data. While spectroscopic data is better suited to the analysis of
many astronomical objects, photometric data is relatively easier to obtain due
to shorter telescope usage time. Therefore, there is a growing need to use
photometric information to automatically identify objects for further detailed
studies, specially H{\alpha} emission line stars such as Be stars. Photometric
color-color diagrams (CCDs) are commonly used to identify this kind of objects.
However, their identification in CCDs is further complicated by the reddening
effect caused by both the circumstellar and interstellar gas. This effect
prevents the generalization of candidate identification systems. Therefore, in
this work we evaluate the use of neural networks to identify Be star candidates
from a set of OB-type stars. The networks are trained using a labeled subset of
the VPHAS+ and 2MASS databases, with filters u, g, r, H{\alpha}, i, J, H, and
K. In order to avoid the reddening effect, we propose and evaluate the use of
reddening-free Q indices to enhance the generalization of the model to other
databases and objects. To test the validity of the approach, we manually
labeled a subset of the database, and use it to evaluate candidate
identification models. We also labeled an independent dataset for cross dataset
evaluation. We evaluate the recall of the models at a 99% precision level on
both test sets. Our results show that the proposed features provide a
significant improvement over the original filter magnitudes.","In the big data era, free access to databases in dierent wavelength ranges, from gammarays to radio waves, together with machinelearning methods, has drastically incremented the possibility to study and identify dierent types of peculiar lineemission stars using photometric information (e.g., Vioque et al., 2019 [30]; Akras et al., 2019 [2]; P erezOrtiz et al., 2017 [21]). While spectroscopic techniques are excellent to perform accurate stellar clas sication and deepen into the study of various spectral features, the telescope time required to obtain such information is longer compared to obtaining pho tometric data. The goal of this work is then to use the potential of photometric data to search for emissionline star candidates. These can be later observed and be conrmed spectroscopically as such. Particularly, we are interested in detecting Be star candidates. Be stars are emissionline objects that rotate at high speed (Jaschek et al., 1981 [15]; Struve, O., 1931 [28]) and constitute unique astrophysical laboratories. They are of interest in various branches of stellar physics dedicated to the study of mechanisms of mass loss, angular momentum distribution, astroseismology, among others. The rest of this section describes Be stars in detail, classical techniques to detect plausible candidates as well as previous star candidate proposals based on machinelearning methods. 1.1 Be stars Be stars are dened as nonsupergiant spectral Btype stars that exhibit, or have exhibited, one or more hydrogen lines in emission (Jaschek et al., 1981 [15]; Collins, II, G., 1987 [7]), particularly the H line. In some cases, it is also possible to observe the presence of onceionized helium and metal lines in emission. Thus, this denition not only applies to Btype stars but also to late O and early Atype stars. The analysis of spectrophotometric observations of Be stars at dierent wave lengths, combined with interferometric and polarimetric data (Gies et al., 2007 [11]; Meilland et al., 2007 [18], among others), indicate that the dierent properties shown by these stars could be interpreted by the existence of an opticallythin gaseous circumstellar equatorial disk in Keplerian motion (see Rivinius et al., 2013 [25]). This suggests that the high rotation speed would play a signicant role in the development of the equatorial disk (e.g., Struve, O., 1931 [28]; Huang, S., 1972 [14]; Quirrenbach, A., 1993 [23]; Quirrenbach et al., 1994 [24]; Hirata, R., 1995 [13]). However, despite the increasing observational evidence that Be stars do not rotate at their critical rotational speed (Zorec et al. 2016 [31], Zorec et al. 2017 [32], Aidelman et al. 2018 [1], Cochetti et al., 2019 [6]), there is still no consensus on disk formation mechanism(s). Other observed eects induced by stellar rotation during the main sequence phase of hot stars, are the development of axisymmetric winds, the modicationReddeningfree Qindices to identify Be star candidates 3 in pulsation modes, changes in metallicity or the presence of magnetic elds (see Peters et al., 2020 [22]; Rivinius et al., 2013 [25]). These properties make Be stars perfect stellar laboratories, of interest in dierent astrophysical topics, as mentioned above. In this context, the discovery, classication and analysis of a considerable sample of Be stars in dierent environments are necessary to understand their nature. 1.2 Related work "
526,Epistemic Neural Networks.txt,"Intelligence relies on an agent's knowledge of what it does not know. This
capability can be assessed based on the quality of joint predictions of labels
across multiple inputs. In principle, ensemble-based approaches produce
effective joint predictions, but the computational costs of training large
ensembles can become prohibitive. We introduce the epinet: an architecture that
can supplement any conventional neural network, including large pretrained
models, and can be trained with modest incremental computation to estimate
uncertainty. With an epinet, conventional neural networks outperform very large
ensembles, consisting of hundreds or more particles, with orders of magnitude
less computation. The epinet does not fit the traditional framework of Bayesian
neural networks. To accommodate development of approaches beyond BNNs, such as
the epinet, we introduce the epistemic neural network (ENN) as an interface for
models that produce joint predictions.","Consider a conventional neural network trained to predict whether a random person would classify a drawing as a ‚Äòrabbit‚Äô or a ‚Äòduck‚Äô. As illustrated in Figure 1, given a single drawing, the network outputs a marginal prediction that assigns probabilities to the two classes. If the probabilities are each 0.5, it remains unclear whether this is because labels sampled from random people are equally likely, or whether the neural network would learn a single class if trained on more data. Conventional neural networks do not distinguish these cases, even though it can be critical for decision making systems to know what they do not know. This capability can be assessed through the quality of jointpredictions (Wen et al., 2022). The two tables to the right of Figure 1 represent possible joint predictions that are each consistent with the network‚Äôs uniform marginal prediction. These joint predictions are over pairsof labels for the same image, (y1,y2)‚àà{R,D}√ó{R,D}. For any joint prediction, Bayes‚Äô rule deÔ¨Ånes a conditional prediction for y2giveny1. The Ô¨Årst table indicates inevitable uncertainty that would not be resolved through training on additional data; conditioning on the Ô¨Årst label does not alter the prediction for the second. The second table indicates that additional training should resolve uncertainty; conditioned on the Ô¨Årst label, the prediction for the second label assigns all probability to the same outcome as the Ô¨Årst. Figure 1: Conventional neural nets generate marginal predictions, which do not distinguish genuine ambiguity from insuÔ¨Éciency of data. Joint predictions can make this distinction. ‚àóContact iosband@deepmind.com Preprint. Under review.arXiv:2107.08924v8  [cs.LG]  17 May 2023Figure 1 presents the toy problem of predictions across two identical images as a simple illustration of these types of uncertainty. The observation that joint distributions express whether uncertainty is resolvable extends more generally to practical cases, where the inputs diÔ¨Äer, or where there are more than two simultaneous predictions (Osband et al., 2022a). Bayesian neural networks (BNNs) oÔ¨Äer a statisticallyprincipled way to make eÔ¨Äective joint predictions, by maintaining an approximate posterior over the weights of a base neural network. Assymptotically these can recover the exact posterior, but the computational costs are prohibitive for large models (Welling and Teh, 2011). Ensemblebased BNNs oÔ¨Äer a more practical approach by approximating the posterior distribution with an ensemble of statistically plausible networks that we call particles (Osband and Van Roy, 2015; Lakshmi narayanan et al., 2017). While the quality of joint predictions improves with more particles, practical implementations are often limited to ten or fewer due to computational constraints. In this paper, we introduce an approach that outperforms ensembles of hundreds of particles at a computational cost less than that of two particles . Our key innovation is the epinet: a network architecture that can be added to any conventional neural network to estimate uncertainty. Figure 2 oÔ¨Äers a preview of results presented in Section 6, where we compare these approaches on ImageNet. The quality of the ResNet‚Äôs marginal predictions ‚Äì measured by classiÔ¨Åcation error or marginal logloss ‚Äì does not change much if supplemented with an epinet. However the epinetenhanced ResNet dramatically improves the quality of jointpredictions, as measured by the joint logloss, outperforming the ensemble of 100 particles, with total parameters less than 2 particles. Prior work has shown the importance of jointpredictions in driving eÔ¨Äective decisions (Wen et al., 2022; Osband et al., 2022a). 3e7 1e8 3e8 1e9 3e90.210.220.23lossclassification error enn resnet ensemble epinet 3e7 1e8 3e8 1e9 3e90.800.850.90marginal logloss 3e7 1e8 3e8 1e9 3e9 model size (number of parameters)457joint logloss Figure 2: Quality of marginal and joint predictions across models on ImageNet (Section 6). The epinet does not Ô¨Åt into the traditional framework of BNNs. In particular, it does not represent a distribution over base neural network parameters. To accommodate development of the epinet and other approaches that do not Ô¨Åt the BNN framework, we introduce the concept of epistemic neural networks (ENNs). We establish that all BNNs are ENNs, but there are useful ENNs such as the epinet, that are not BNNs. 2 Related work "
527,Yelp Food Identification via Image Feature Extraction and Classification.txt,"Yelp has been one of the most popular local service search engine in US since
2004. It is powered by crowd-sourced text reviews and photo reviews. Restaurant
customers and business owners upload photo images to Yelp, including reviewing
or advertising either food, drinks, or inside and outside decorations. It is
obviously not so effective that labels for food photos rely on human editors,
which is an issue should be addressed by innovative machine learning
approaches. In this paper, we present a simple but effective approach which can
identify up to ten kinds of food via raw photos from the challenge dataset. We
use 1) image pre-processing techniques, including filtering and image
augmentation, 2) feature extraction via convolutional neural networks (CNN),
and 3) three ways of classification algorithms. Then, we illustrate the
classification accuracy by tuning parameters for augmentations, CNN, and
classification. Our experimental results show this simple but effective
approach to identify up to 10 food types from images.","Nowadays people really love taking photos, especially when they are in a fancy restaurant, in addition that smart mobile phones today are well equipped with highresolution cameras. So, it is not a surprise that you can see there would be thousands of pictures from someone‚Äôs phone album after a year. However, labeling and searching by words for these photos becomes a real hassle. For 1Wikipedia: https://en.wikipedia.org/wiki/Yelp 2Yelp Data Challenge: https://www.yelp.com/dataset/challenge Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). E511‚Äô18, December 2018, Bloomington, IN ¬©2018 Copyright held by the owner/author(s). ACM ISBN 978xxxxxxxxxx/YY/MM. https://doi.org/10.1145/nnnnnnn.nnnnnnnexample, imaging you are talking with a friend about your extraor dinary experience of eating a lobster in Bloomington IN, turning your phone upside down just want to share this picture, oops, you can not find it. This is because you forgot to label this picture and did not remember which day you took this picture. Not just for personal photo album management, this issue raises astonishing importance for companies like Yelp, which does show ing and researching business reviews. Digital photos identification poses a hard problem for those companies which rely on users‚Äô uploaded photos. Since they serve millions of customers and they may have billions of photos [ 4]. It would be almost infeasible to edit photo labels by human editors. So, it would be promising that devel oping some automatically identification solution for user uploaded pictures. More formally, in this work, we would like to build a model that can automatically classify a user uploaded food photo into a set of applicable categories, and the accuracy of prediction should be beyond average human guess. We first consider using pretrained features provided by Yelp Data Challenge and feeding them to traditional machine learning algorithms like Convolutional Neural Network, Support Vector Machine, Gradient Boosting, with cross validation. But unfortunately, the classification result was way below human eyeball. After double check, it becomes evident that the main issue lies on the features. As Yelp is a usergenerated content platform, many pictures in our training set are vague, off topic or mislabeled. Given so, the decision was to do it from scratch. We carefully select 30 pictures for each class, use argumentation methods to enlarge our train set, manually extract features, finally the classifi cation result is satisfactory. The rest of the paper is organized as follows. Section 2 discusses some related works, some of which either address similar issues or adopt similar solution methods that are related to the content of this work. Section 3 presents the overview of our solution, machine learning models, algorithms, and et al. Section 4 and 5 show the overview of the original datasets from Yelp and our experiments of showing the effectiveness of our implementation. Finally, this paper concludes the work and discusses some advantages and limitations of the work in Section 6. 2 RELATED WORK "
528,Nonlinear Local Metric Learning for Person Re-identification.txt,"Person re-identification aims at matching pedestrians observed from
non-overlapping camera views. Feature descriptor and metric learning are two
significant problems in person re-identification. A discriminative metric
learning method should be capable of exploiting complex nonlinear
transformations due to the large variations in feature space. In this paper, we
propose a nonlinear local metric learning (NLML) method to improve the
state-of-the-art performance of person re-identification on public datasets.
Motivated by the fact that local metric learning has been introduced to handle
the data which varies locally and deep neural network has presented outstanding
capability in exploiting the nonlinearity of samples, we utilize the merits of
both local metric learning and deep neural network to learn multiple sets of
nonlinear transformations. By enforcing a margin between the distances of
positive pedestrian image pairs and distances of negative pairs in the
transformed feature subspace, discriminative information can be effectively
exploited in the developed neural networks. Our experiments show that the
proposed NLML method achieves the state-of-the-art results on the widely used
VIPeR, GRID, and CUHK 01 datasets.","Person reidentiÔ¨Åcation aims to recognize people who have been observed from different disjoint cameras, which has become an effective tool for people association and be havior analysis in video surveillance [14, 42]. Due to the complex variations in illumination, pose, viewpoint, occu lusion and image resolution across camera views, person reidentiÔ¨Åcation still remains a challenging problem in com puter vision. Previously proposed approaches which improve the per son reidentiÔ¨Åcation performance [4, 25, 27, 37, 46, 49, 50] can be mainly categorized into two classes: 1) developing robust descriptors to handle the variations in pedestrian im ages; 2) designing discriminative distance metrics to mea sure the similarity of pedestrian images. For the Ô¨Årst cate Apairofpersonimages(ùíôùüè,ùíôùüê) center1centerKglobalùúî((ùíôùüè,ùíôùüê)ùúî)(ùíôùüè,ùíôùüê)ùúî*=ùõΩùúî(ùëë((ùíôùüè,ùíôùüê)ùúî)ùëë)(ùíôùüè,ùíôùüê)ùúî*ùëë*(ùíôùüè,ùíôùüê)DistanceMetric:ùê∑ùíôùüè,ùíôùüê=	  ¬†‚àëùúî1ùíôùüè,ùíôùüêùëë1(ùíôùüè,ùíôùüê))12*Figure 1: Basic idea of the proposed NLML method. The whole network consists of one global neural network and a set of local neural networks which correspond to differ ent local clusters. For a given pair of person images x1 andx2, we compute the nonnegative weight wk(x1;x2) forkth local region and pass it through Klocal and one global deep neural networks to get the representation at out put layerdk(x1;x2). The Ô¨Ånal distance metric D(x1;x2)is deÔ¨Åned as a linear combination of K+ 1matrices. gory, different cues (color, shape, texture) from pedestrian images are employed for feature representation. Repre sentative descriptors in person reidentiÔ¨Åcation include lo cal binary patterns (LBP) [46], ensemble of local feature (ELF) [16], midlevel Ô¨Ålter [50] and local maximal oc currence (LOMO) [27]. For the second category, a dis tance metric is learned from labeled training samples, un der which the interclass and intraclass variations of pedes trian images are increased and decreased, respectively. Typ 1arXiv:1511.05169v1  [cs.CV]  16 Nov 2015ical metric learning algorithms include large margin nearest neighbor (LMNN) [44], information theoretic metric learn ing (ITML) [9], and pairwise constrained component anal ysis (PCCA) [36]. While metric learning methods achieved good perfor mance in many person reidentiÔ¨Åcation systems [3, 7, 9, 20, 21, 26, 37, 38, 44, 46], most of them learn a Maha lanobis distance metric to transform samples into a new feature space, which are not capable enough of exploiting the nonlinear relationship of pedestrian samples distributed in a nonlinear feature space due to large intraclass vari ations. Moreover, a single distance metric usually suffers limitations while handling data which varies locally. To address this, we propose a nonlinear local metric learning (NLML) method for person reidentiÔ¨Åcation. Figure 1 il lustrates the basic idea of the proposed NLML method. Un like most existing metric learning methods, NLML devel ops one global feedforward neutral network and a set of local feedforward neutral networks to jointly learn multi ple sets of nonlinear transformations. The learning proce dure is formulated as a large margin optimization problem and the gradient descent algorithm is employed to estimate the networks. Experimental results on the VIPeR, GRID and CUHK 01 datasets demonstrate the efÔ¨Åcacy of the pro posed NLML method. 2. Related Work "
529,Population-Based Evolutionary Gaming for Unsupervised Person Re-identification.txt,"Unsupervised person re-identification has achieved great success through the
self-improvement of individual neural networks. However, limited by the lack of
diversity of discriminant information, a single network has difficulty learning
sufficient discrimination ability by itself under unsupervised conditions. To
address this limit, we develop a population-based evolutionary gaming (PEG)
framework in which a population of diverse neural networks is trained
concurrently through selection, reproduction, mutation, and population mutual
learning iteratively. Specifically, the selection of networks to preserve is
modeled as a cooperative game and solved by the best-response dynamics, then
the reproduction and mutation are implemented by cloning and fluctuating
hyper-parameters of networks to learn more diversity, and population mutual
learning improves the discrimination of networks by knowledge distillation from
each other within the population. In addition, we propose a cross-reference
scatter (CRS) to approximately evaluate re-ID models without labeled samples
and adopt it as the criterion of network selection in PEG. CRS measures a
model's performance by indirectly estimating the accuracy of its predicted
pseudo-labels according to the cohesion and separation of the feature space.
Extensive experiments demonstrate that (1) CRS approximately measures the
performance of models without labeled samples; (2) and PEG produces new
state-of-the-art accuracy for person re-identification, indicating the great
potential of population-based network cooperative training for unsupervised
learning.","Person reidentification (reID) aims to match persons in an image gallery collected from nonoverlapping cam era networks, which has attracted increasing interest thanks to its wide applications in security and surveilarXiv:2306.05236v1  [cs.CV]  8 Jun 20232 Yunpeng Zhai et al. ResNet 50 DenseNet 169 (a) Before training (b) Single model training (c) Multi model training (Ours ) Fig. 1: Feature distribution of the same samples with different methods where each color denotes a person identity. Single model training(b) uses the selflearning mechanism only to enhance the discrimination ability it already has before training(a) and still suffers from inaccurate pseudolabels. However, multimodel training(c) explores and exploits the complementary information among different models (marked by corresponding colored boxes) and achieves more discrimination. lance. Though supervised reID methods (Yang et al., 2020) (Zheng et al., 2016) have achieved very decent results, they are largely dependent on sufficient data with expensive manual annotation, which also require substantial personal identity information and entail privacy issues. By contrast, unsupervised reID not only reduces the cost of labeling but also protects personal privacy without checking images manually. Commonly, unsupervised reID can be divided into two categories: unsupervised domain adaptation (UDA) (Zhai et al., 2020a) (Zhong et al., 2020) and fully unsupervised reID (FU) (Chen et al., 2021a) (Lin et al., 2019) depending on whether using extra labeled data. In this study, we will mainly focus on the fully unsupervised setting which learns directly from unlabeled images and allows for more scalability in realworld deployments. To address the challenges of unsupervised reID, recent efforts concentrate on training individual neural networks by means of a selfimprovement strategy (Song et al., 2018) (Ge et al., 2020b). They attempt to learn better representations based on selfpredictedpseudolabels via clustering algorithms (Caron et al., 2018) or graph neural networks (Ye et al., 2017). However, a single model can use such a selflearning mechanism only to enhance the discrimination ability it already has and cannot tackle the incorrectly predicted pseudolabels, which prevents it from maximizing its discrimination. Due to the lack of diversity of single models, incorrect pseudolabels are likely to remain the same after unsupervised training such as the false positive samples where images of different persons are clustered into the same group or the false negative samples where the images of the same person are clustered into different groups, as shown in Fig. 1. Importantly, since models learn diverse discrimination with different architectures, the incorrect pseudolabels predicted by a model may be predicted correctly by another model, marked by boxes in Fig. 1(b). In this paper, we attempt to address unsupervised reID by multiple model training, in which the complementary information of different models can be integrated and utilized effectively to explore the various latent knowlPopulationBased Evolutionary Gaming for Unsupervised Person Reidentification 3 edge contained in unlabeled data (the quantitative analysis is shown in Sec. 4.4.1). However, multiple model training still faces two challenging issues: (1) How to learn diverse discrim ination with multiple different models? (2) How to select a set of better models from many diverse models for training? To tackle these issues, we propose a populationbased evolutionary gaming (PEG), which selects and trains discriminative models by exploration and exploitation of their diversity. PEG trains a pop ulation of models concurrently by iterative selection, reproduction, mutation, and population mutual learn ing of neural networks, as shown in Fig. 2. Specifically, selection adapts the whole population to the unlabeled data by selecting and preserving the optimal subset of networks with complementary discrimination ability while abandoning other networks out of the subset. This combinatorial optimization of networks in selection is modeled as a multiagent cooperative game and solved by the best response dynamics, in which each agent attempts to learn the best response to the other agents‚Äô action and thus leads to Nash equilibrium. Then, reproduction and mutation are performed on the selected population to increase its diversity by making multiple copies of each network and applying a stochastic disturbance to their hyperparameters. Selection and reproduction jointly maintain the size of the population. Afterward, population mutual learning is conducted among networks to assemble and further explore the discrimination capacity via knowledge dis tillation within populations. Each network learns rep resentations from both populationshared pseudolabels and softlabels predicted by other individual networks. Utilizing periodically performing selection, reproduc tion and mutation, population mutual learning, the evolutionary gaming process enables favorable traits and knowledge of neural networks to be transmitted through successive generations. In the evolution gaming, a core issue is to define the utility function of the game, that is, the criterion of network selection in the evolution. However, the evaluation of CNN models without labeled datasets has not been well studied. Here, we propose crossreference scatter (CRS), which can approximately evaluate the quality of networks using unlabeled samples. Generally, the pseudolabels predicted by better networks are more accurate; however, their accuracy cannot be directly evaluated when the ground truth is unavailable. More over, models trained by more accurate pseudolabels tend to achieve larger intracluster cohesion and inter cluster separation in the feature space because incorrect labels will enforce models to separate samples of the same class or aggregate samples of different classes.Motivated by this phenomenon, we indirectly evaluate a network according to the feature cohesion and sepa ration of a reference model that is trained by pseudo labels of the evaluated network. Hence, the CRS is defined by the ratio of the intercluster and intracluster variance of features to measure both separation and cohesion. We demonstrate that the CRS approximately reflects the discrimination capacity of models without ground truth data and thus promotes the evolution gaming to learn better representations. A preliminary version of this work has been partially published (Zhai et al., 2020c), which has demonstrated the effectiveness of mutual learning among multiple networks in unsupervised conditions. Based on that version, this manuscript has made great improvements, including: 1) We propose a novel populationbased evolutionary gaming (PEG) framework (Sec. 3.1). The previous algorithm works passively only on given net works, and cannot adaptively select the most suitable models from the model base. Based on the mutual learning, PEG additionally contains an iterative selec tion of networks via a multiagent cooperative game preventing the weak networks to distract the overall discrimination capability (Sec. 3.1.1). 2) We propose a new crossreference scatter (CRS) to approximately measure reID models without labeled data. To evaluate the model discrimination, the previous version intro duced inter/intracluster scatter to roughly modulate the weights of models during mutual learning. However, it cannot be considered as the utility function of the cooperative game in PEG due to the lack of capability to accurately evaluate models. This paper improves inter/intracluster scatter to crossreference scatter by adding a crossreference evaluation (CR) scheme (Sec. 3.1.1). 3) More qualitative and quantitative experi ments are conducted to evaluate the effectiveness of the method, including but not limited to the validation and analysis of CRS, the cooperative game, and PEG. In summary, our contribution is as follow: ‚ÄìIt proposes a novel populationbased evolutionary gaming framework for unsupervised person reID which trains a diverse population of neural networks by iterative selection, reproduction, mutation and mutual learning. ‚ÄìIt introduces a multiagent cooperative game for the selection of networks in the PEG, which aims to find and preserve an optimal subset of the population on unlabeled data. ‚ÄìIt investigates the evaluation of reID models us ing unlabeled data and proposes a crossreference scatter which approximately measures a model‚Äôs discrimination capability by indirectly estimating4 Yunpeng Zhai et al. its predicted pseudolabels according to the cohesion and separation of feature space. ‚ÄìExperiments show that PEG outperforms stateof theart methods on largescale datasets, indicating the great potential of populationbased multiple model training. 2 Related Works "
530,Robust Optimization for Fairness with Noisy Protected Groups.txt,"Many existing fairness criteria for machine learning involve equalizing some
metric across protected groups such as race or gender. However, practitioners
trying to audit or enforce such group-based criteria can easily face the
problem of noisy or biased protected group information. First, we study the
consequences of naively relying on noisy protected group labels: we provide an
upper bound on the fairness violations on the true groups G when the fairness
criteria are satisfied on noisy groups $\hat{G}$. Second, we introduce two new
approaches using robust optimization that, unlike the naive approach of only
relying on $\hat{G}$, are guaranteed to satisfy fairness criteria on the true
protected groups G while minimizing a training objective. We provide
theoretical guarantees that one such approach converges to an optimal feasible
solution. Using two case studies, we show empirically that the robust
approaches achieve better true group fairness guarantees than the naive
approach.","As machine learning becomes increasingly pervasive in real world decision making, the question of ensuring fairness of ML models becomes increasingly important. The deÔ¨Ånition of what it means to be ‚Äúfair‚Äù is highly context dependent. Much work has been d one on developing mathematical fairness criteria according to various societal and ethica l notions of fairness, as well as methods for building machinelearning models that satisfy those fairn ess criteria [see, e.g., 21, 32, 53, 41, 58, 14, 25, 55]. Many of these mathematical fairness criteria are groupbased , where a target metric is equalized or enforced over subpopulations in the data, also known as protected groups . For example, the equality of opportunity criterion introduced by Hardt et al. [32] speciÔ¨Åes that the t rue positive rates for a binary classiÔ¨Åer are equalized across protected group s. The demographic parity [21] criterion requires that a classiÔ¨Åer‚Äôs positive prediction rates are e qual for all protected groups. ‚àóFirst two authors have equal contributions. 34th Conference on Neural Information Processing Systems ( NeurIPS 2020), Vancouver, Canada.One important practical question is whether or not these fai rness notions can be reliably measured or enforced if the protected group information is noisy, mis sing, or unreliable. For example, survey participants may be incentivized to obfuscate their respon ses for fear of disclosure or discrimination, or may be subject to other forms of response bias. Social desi rability response bias may affect par ticipants‚Äô answers regarding religion, political afÔ¨Åliat ion, or sexual orientation [40]. The collected data may also be outdated: census data collected ten years ag o may not an accurate representation for measuring fairness today. Another source of noise arises from estimating the labels of the protected groups. For various image recognition tasks (e.g., face detection), one may want to me asure fairness across protected groups such as gender or race. However, many large image corpora do n ot include protected group labels, and one might instead use a separately trained classiÔ¨Åer to e stimate group labels, which is likely to be noisy [12]. Similarly, zip codes can act as a noisy indicat or for socioeconomic groups. In this paper, we focus on the problem of training binary clas siÔ¨Åers with fairness constraints when only noisy labels, ÀÜG‚àà{1,...,ÀÜm}, are available for mtrue protected groups, G‚àà{1,...,m}, of interest. We study two aspects: First, if one satisÔ¨Åes fairn ess constraints for noisy protected groups ÀÜG, what can one say with respect to those fairness constraints for the true groups G? Second, how can side information about the noise model between ÀÜGandGbe leveraged to better enforce fairness with respect to the true groups G? Contributions: Our contributions are threefold: 1. We provide a bound on the fairness violations with respect to the true groups Gwhen the fairness criteria are satisÔ¨Åed for the noisy groups ÀÜG. 2. We introduce two new robustoptimization methodologies that satisfy fairness criteria on the true protected groups Gwhile minimizing a training objective. These methodologie s differ in convergence properties, conservatism, and noise model speciÔ¨Åcation. 3. We show empirically that unlike the na√Øve approach, our tw o proposed approaches are able to satisfy fairness criteria with respect to the true groups Gon average. The Ô¨Årst approach we propose (Section 5) is based on distribu tionally robust optimization (DRO) [19, 8]. Let pdenotes the full distribution of the data X,Y‚àºp. Letpjbe the distribution of the data conditioned on the true groups being j, soX,Y|G=j‚àºpj; andÀÜpjbe the distribution of X,Y conditioned on the noisy groups. Given an upper bound on the t otal variation (TV) distance Œ≥j‚â•TV(pj,ÀÜpj)for each j‚àà {1,...,m}, we deÔ¨Åne Àúpjsuch that the conditional distributions (X,Y|ÀúG=j‚àºÀúpj) fall within the bounds Œ≥iwith respect to ÀÜG. Therefore, the set of all such Àúpjis guaranteed to include the unknown true group distribution pj,‚àÄj‚ààG. Because it is based on the wellstudied DRO setting, this approach has the advantage o f being easy to analyze. However, the results may be overly conservative unless tight bounds {Œ≥j}m j=1can be given. Our second robust optimization strategy (Section 6) uses a r obust reweighting of the data from soft protected group assignments, inspired by criteria pro posed by Kallus et al. [37] for auditing the fairness of ML models given imperfect group information. Ex tending their work, we optimize a con strained problem to achieve their robust fairness criteria , and provide a theoretically ideal algorithm that is guaranteed to converge to an optimal feasible point, as well as an alternative practical version that is more computationally tractable. Compared to DRO, th is second approach uses a more precise noise model, P(ÀÜG=k|G=j), between ÀÜGandGfor all pairs of group labels j,k, that can be esti mated from a small auxiliary dataset containing groundtru th labels for both GandÀÜG. An advantage of this more detailed noise model is that a practitioner can i ncorporate knowledge of any bias in the relationship between GandÀÜG(for instance, survey respondents favoring one socially pr eferable response over others), which causes it to be less likely than DRO to result in an overlyconservative model. Notably, this approach does notrequire that ÀÜGbe a direct approximation of G‚Äîin fact, G andÀÜGcan represent distinct (but related) groupings, or even gro upings of different sizes, with the noise model tying them together. For example, if Grepresents ‚Äúlanguage spoken at home,‚Äù then ÀÜG could be a noisy estimate of ‚Äúcountry of residence.‚Äù 22 Related work "
531,Human-In-The-Loop Person Re-Identification.txt,"Current person re-identification (re-id) methods assume that (1) pre-labelled
training data are available for every camera pair, (2) the gallery size for
re-identification is moderate. Both assumptions scale poorly to real-world
applications when camera network size increases and gallery size becomes large.
Human verification of automatic model ranked re-id results becomes inevitable.
In this work, a novel human-in-the-loop re-id model based on Human Verification
Incremental Learning (HVIL) is formulated which does not require any
pre-labelled training data to learn a model, therefore readily scalable to new
camera pairs. This HVIL model learns cumulatively from human feedback to
provide instant improvement to re-id ranking of each probe on-the-fly enabling
the model scalable to large gallery sizes. We further formulate a Regularised
Metric Ensemble Learning (RMEL) model to combine a series of incrementally
learned HVIL models into a single ensemble model to be used when human feedback
becomes unavailable.","PERSON reidentiÔ¨Åcation (reid) is the problem of matching people across nonoverlapping camera views distributed in open spaces at different locations, typically achieved by matching detected bounding box images of people [23]. This is an inher ently challenging problem due to the potentially dramatic visual appearance changes caused by uncontrolled variations in human pose and unknown viewing conditions on illumination, occlusion, and background clutter (Fig. 1). A reid model is required to differentiate images of different categories (persons) with similar appearances, which can be considered as solving a Ô¨Ånegrained visual categorisation problem [14,78], whilst also able to recognise a same category (person) with visually dissimilar appearances. Unlike conventional biometrics identiÔ¨Åcation problems, e.g. face recognition, a person reid model has no labelled training data on target classes , i.e. similar to a zeroshot learning problem [31] that requires the model to perform inherently transfer learning between a training population (seen) and a target population (unseen). Moreover, person reid requires implicitly a model to perform crossdomain transfer learning [53] if each camera view is considered as a speciÔ¨Åc domain of potentially signiÔ¨Åcant difference to other domains (views). This is more difÔ¨Åcult than a standard zeroshot learning problem. Current reid methods are dominated by supervised learning techniques [23,34,38,39,40,50,55,79,80,88,94,97], which typically employ a ‚Äú trainonceanddeploy ‚Äù scheme (Fig. 2(a)). That is, a prelabelled training dataset of pairwise true and falsematching Hanxiao Wang is with the Electrical and Computer Engineering De partment, Boston University, US. Email: hxw@bu.edu. Shaogang Gong and Tao Xiang are with the School of Electronic Engineering and Computer Science, Queen Mary University of London, UK. Email: s.gong@qmul.ac.uk, t.xiang@qmul.ac.uk. Xiatian Zhu is with Vision Se mantics Ltd., London, UK. Email: eddy@visionsemantics.com. (a)Crossview appearance variations  (b)Similar appearance among different people Fig. 1. Person reidentiÔ¨Åcation Challenges. (a) A signiÔ¨Åcant visual ap pearance change of the same person across camera views. (b) Strong appearance similarities among different people. identities (training population) is collected by human annotators for every pair of cameras through manually examining a vast pool of image/video data. This training dataset is used to train an ofÔ¨Çine reid model. It is tacitly assumed by most that such a trained model can be deployed plausibly as a fully automated solution to reidentify target (unseen during model training) person images at test time, without any human assistance nor model adaptation. Based on this assumption, the reid community has witnessed everincreased matching accuracies on increasingly larger sized benchmarks of more training identity classes over the past two years. For instance, the CUHK03 benchmark [38] contains 13,164 images of 1,360 identities, of which 1,260 are used for training and 100 for testing, signiÔ¨Åcantly larger than the earlier VIPeR [25] (1,264 images of 632 people with 316 for training) and iLIDS [96] (476 images for 119 people with 69 for training). The stateofthe art Rank1 accuracy on CUHK03 has exceeded 70% [86], tripling the best performance reported only two years ago [38]. Despite such rapid progresses, current automatic reid solu tions remain illsuited for a practical deployment. This is because: (1)A manually prelabelled pairwise training data set for every camera pair does not exist, due to either being prohibitively expensive to collect in the realworld as there are a quadratic number of camera pairs, or nonexistence of sufÔ¨Åciently large number of training people reappearing in every pair of cameraarXiv:1612.01345v2  [cs.CV]  4 May 20182 Deployable to further  population   Large training  dataset  Exhaustively label   ALL camera pairs   (a) Human OutoftheLoop re id scheme  Offline   model training  ReId model   Deploy to the same camera pairs  Annotation Stage   Deployment Stage   (c) HVIL: Human Verification Incremental Learning  Probe  population  Gallery   population   rank/re rank  user feedback   reid models   optimised in isolation   Training Stage   Gallery   population   Model N Human intheloop  (b) POP: Post rank optimisation  Model 1  rank/re rank  user feedback   rank/re rank  user feedback  Model 1  Strong  model   Human intheloop  reid models   optimised incrementally   limited labour budget   Not deployable to  further population   Model 2 Model 2  Fig. 2. Illustration of two person reid schemes. (a)The conventional HumanOutoftheLoop (HOL) reid scheme requires exhaustive prelabelled training data collection for supervised ofÔ¨Çine model learning. The learned model is assumed sufÔ¨Åciently generalisable and then deployed to perform fully automated person reid tasks without human in the loop. (b)POP [44]: A recent HumanIntheLoop (HIL) reid approach which optimises probespeciÔ¨Åc models in isolation from human feedback veriÔ¨Åcations in the deployment time. All probe people requires human in the loop. (c)HVIL: The proposed new incremental HIL reid model capable of not only progressively learning a generalised model from human veriÔ¨Åcations across all probed people while carrying out the HIL reid tasks, but also performing the HOL reid tasks when human effort becomes unavailable. views. (2)Assuming the size of the training population is either signiÔ¨Åcantly greater or no less than that of the target population is unrealistic. For instance, the standard CUHK03 benchmark test deÔ¨Ånes the training set having paired images of 1,260 people from six different camera views (on average 4.8 image samples per person per camera view), whilst the test set having only 100 identities each with a single image. The test population is thus 10 times smaller than the training population, with approximately 50 times less images. In practice, any deployment gallery size (test population) is almost always much greater than any labelled training data size even if such training data were available. In a public space such as an underground station, there are easily thousands of people passing through a camera view every hour [1], with a typical gallery population size of over 10,000 per day. We observed on the CUHK03 dataset that, only a 10fold increase in gallery size leads to a 10fold decrease in reid Rank1 performance, leading to a singledigit Rank1 score, even when the stateoftheart reid models were trained from sufÔ¨Åciently sized labelled data. Given such low Rank1 scores, in practice human operators (users) would still be required to verify any true match of a probe from an automatically generated ranking list. In this work, we explore an alternative approach to person re id by formulating a hybrid humancomputer learning paradigm with humans in the model matching loop (Fig. 2(c)). We call this semiautomated scheme HumanIntheLoop (HIL) reid, de signed to optimise reid performance given a small number of human veriÔ¨Åcation feedback and a largersized test population, as compared to the conventional HumanOutoftheLoop (HOL) reid models that are mostly designed to optimise reid given a largersized prelabelled training data and a smallsized test population. This HIL reid scheme has three signiÔ¨Åcant advantages over the conventional HOL models: (1)Less human labelling effort : HIL reid requires much less human labelling effort, since it does not necessarily require the expensive construction of a prelabelled training set. More importantly, it prioritises directly the human labour effort on each given reid task in deployment, rather than optimising the model learning error on an independent training set. More speciÔ¨Åcally, the number of feedback from human veriÔ¨Åcation is typically in tens as compared to thousands of ofÔ¨Çine prelabelled training data required by HOL methods. (2)Model transfer learning : Our HIL model is able to achieve greater transferability with better reid performance in test domains. This is because a HIL model focuses on reid matching optimisation directly in the deployment gallery population, rather than learning a distance metric from a separate training set and assuming its blind transferability to independent (unseen) test data. It enables a human operator to interactively validate model matching results for each reid task and inform on model mistakes (similar in spirit to hard negative mining). (3)Reinforcing visual consistency : As computer vision algorithms are intrinsically very different from the human visual system, a reid model can make mistakes that generate ‚Äúunexpected‚Äù (visually inconsistent) reid ranking results, readily identiÔ¨Åable by a human observer. By learning directly from the inconsistency between a computer vision model and human observation, a HIL reid model is guided to maximise visually more consistent ranking lists favoured by human observations, and thus rendering the learned model more discriminative and capable of avoiding future mistakes that seem insensible to human observation. The main contribution of this work is a novel HIL reid model that enables a user to reidentify rapidly a given probe person image after only a handful of feedback veriÔ¨Åcations even when the search gallery size is large. More speciÔ¨Åcally, a Human VeriÔ¨Åcation Incremental Learning (HVIL) model (Fig. 2(c)) is formulated to simultaneously minimise humanintheloop feed back and maximise model reid accuracy by incorporating: (1) Sparse feedback  HVIL allows for easier human feedback on a few dissimilar matching results without the need for exhaustive eyeball search of true/false in the entire rank list. It aims to rectify rapidly unexpected model mistakes by focusing only on minimising visually obvious errors (hard negatives) identiÔ¨Åed by human observation. This is reminiscent to learning by hard negative mining but with human in the loop, so to improve model learning with less training data. (2)Immediate beneÔ¨Åt  HVIL introduces a new online incremental distance metric learning algorithm, which enables realtime model response to human feedback by rapidly presenting a freshly optimised ranking list for further human feedback, quickly leading to identifying a true match. (3)The older the wiser  HVIL is updated cumulatively on theÔ¨Çy utilising multiple user feedback per probe, with incremental3 ‚Ä¶ ‚Ä¶ A large person image pool ‚Ä¶ ID #i  (a) Exhuastive labelling Actively/randomly selected image pairs  ‚Ä¶ (b) True or false pairwise labelling Probe shorts backpack blue sneakers (c) Attribute labelling Topk Ranked Gallery Images  Probe (d) Top ranks labelling (true match, negative) Fig. 3. Different human labelling processes are employed in person reid model training and deployment. (a)Large size ofÔ¨Çine labelling of crossview positive and negativepairs of training data with identity labels [40,52,75,91]. (b)Selective or random sampling of person image pairs for human veriÔ¨Åcation in either model training [48] or deployment [15]. (c)Finegrained attribute labelling in either training [64] or deployment [15]. (d)True match veriÔ¨Åcation among the top ranked sublist in model deployment [26,44,76], or veriÔ¨Åcation of both visually dissimilar and similar wrong matches in top ranks (strong/hard and weak negative mining) in model deployment [44,76]. model optimisation for each new probe given what have been learned from all previous probes. (4)A strong ensemble model An additional Regularised Metric Ensemble Learning (RMEL) model is introduced by taking all the incrementally optimised per probe models as a set of ‚Äúweak‚Äù models [4,59] and constructing a ‚Äústrong‚Äù ensemble model for performing HOL reid tasks when human feedback becomes unavailable. Extensive comparative ex periments on three benchmark datasets (CUHK03 [38], Market 1501 [95], and VIPeR [25]) demonstrate that this HVIL model outperforms the stateoftheart methods for both the proposed new HIL and the conventional HOL reid deployments. 2 R ELATED WORK "
532,Transfer Learning in Conversational Analysis through Reusing Preprocessing Data as Supervisors.txt,"Conversational analysis systems are trained using noisy human labels and
often require heavy preprocessing during multi-modal feature extraction. Using
noisy labels in single-task learning increases the risk of over-fitting.
Auxiliary tasks could improve the performance of the primary task learning
during the same training -- this approach sits in the intersection of transfer
learning and multi-task learning (MTL). In this paper, we explore how the
preprocessed data used for feature engineering can be re-used as auxiliary
tasks, thereby promoting the productive use of data. Our main contributions
are: (1) the identification of sixteen beneficially auxiliary tasks, (2)
studying the method of distributing learning capacity between the primary and
auxiliary tasks, and (3) studying the relative supervision hierarchy between
the primary and auxiliary tasks. Extensive experiments on IEMOCAP and SEMAINE
data validate the improvements over single-task approaches, and suggest that it
may generalize across multiple primary tasks.","The sharp increase in uses of videoconferencing creates both a need and an opportunity to better understand these conversations (Kim et al., 2019a). In postevent applications, analyzing conversations can give feedback to improve communication skills (Hoque et al., 2013; Naim et al., 2015). In realtime applications, such systems can be useful in legal trials, public speaking, ehealth services, and more (Poria et al., 2019; Tanveer et al., 2015). Analyzing conversations requires both human expertise and a lot of time. However, to build au tomated analysis systems, analysts often require a training set annotated by humans (Poria et al., 2019). The annotation process is costly, thereby limiting the amount of labeled data. Moreover, thirdparty annotations on emotions are often noisy. Deep networks coupled with limited noisy labeleddata increases the chance of overÔ¨Åtting (James et al., 2013; Zhang et al., 2016). Could data be used more productively? From the perspective of feature engineering to analyze videoconferences, analysts often employ prebuilt libraries (Baltru≈°aitis et al., 2016; V oka turi, 2019) to extract multimodal features as inputs to training. This preprocessing phase is often com putationally heavy, and the resulting features are only used as inputs. In this paper, we investigate how the preprocessed data can be reused as aux iliary tasks which provide inductive bias through multiple noisy supervision (Caruana, 1997; Lip ton et al., 2015; Ghosn and Bengio, 1997) and consequently, promoting a more productive use of data. SpeciÔ¨Åcally, our main contributions are (1) the identiÔ¨Åcation of beneÔ¨Åcially auxiliary tasks, (2) studying the method of distributing learning capacity between the primary and auxiliary tasks, and (3) studying the relative supervision hierar chy between the primary and auxiliary tasks. We demonstrate the value of our approach through pre dicting emotions on two publicly available datasets, IEMOCAP (Busso et al., 2008) and SEMAINE (McKeown et al., 2011). 2 Related Works and Hypotheses "
533,Convolutional Neural Associative Memories: Massive Capacity with Noise Tolerance.txt,"The task of a neural associative memory is to retrieve a set of previously
memorized patterns from their noisy versions using a network of neurons. An
ideal network should have the ability to 1) learn a set of patterns as they
arrive, 2) retrieve the correct patterns from noisy queries, and 3) maximize
the pattern retrieval capacity while maintaining the reliability in responding
to queries. The majority of work on neural associative memories has focused on
designing networks capable of memorizing any set of randomly chosen patterns at
the expense of limiting the retrieval capacity. In this paper, we show that if
we target memorizing only those patterns that have inherent redundancy (i.e.,
belong to a subspace), we can obtain all the aforementioned properties. This is
in sharp contrast with the previous work that could only improve one or two
aspects at the expense of the third. More specifically, we propose framework
based on a convolutional neural network along with an iterative algorithm that
learns the redundancy among the patterns. The resulting network has a retrieval
capacity that is exponential in the size of the network. Moreover, the
asymptotic error correction performance of our network is linear in the size of
the patterns. We then ex- tend our approach to deal with patterns lie
approximately in a subspace. This extension allows us to memorize datasets
containing natural patterns (e.g., images). Finally, we report experimental
results on both synthetic and real datasets to support our claims.","The ability of neuronal networks to memorize a large set of patterns and reliably retrieve them in the presence of noise, has attracted a large body of research over the past three decades to design artiÔ¨Åcial neural associative memories with sim ilar capabilities. Ideally, a perfect neural associative memory should be able to learn patterns, have a large pattern retrieval capacity and be noisetolerant . This problem, called ‚Äùassociative memory"", is in spirit very similar to reliable informa tion transmission faced in communication systems where the goal is to efÔ¨Åciently decode a set of transmitted patterns over a noisy channel. Despite this similarity and common methods deployed in both Ô¨Åelds (e.g., graphical models, iterative algorithms, to name a few), we have witnessed a huge gap between the efÔ¨Åciency achieved by them. More speciÔ¨Åcally, by deploying mod ern coding techniques, it was shown that the number of reliably transmitted patterns over a noisy channel can be made exponential inn, the length of the patterns. This was particularly achieved by imposing redundancy among transmitted patterns. In contrast, the maximum number of patterns that can be reliably memorized by most current neural networks scales linearly in the size of the patterns. This is due to the common assumption that a neural network should be able to memorize anysubset of patterns drawn randomly from the set of all possible vectors of length n(see, for example HopÔ¨Åeld, 1982, Venkatesh and Psaltis, 1989, Jankowski et al., 1996, Muezzinoglu et al., 2003). Recently, Kumar et al. (2011) suggested a new formulation of the problem where only a suitable set of patterns was considered for storing. To enforce the set of constraints, they formed a bipartite graph (as opposed to a complete graph considered in the earlier work) where one layer feeds the patterns to the network and the other takes into account the inherent structure. The role of bipartite graph is indeed similar to the Tanner graphs used in modern coding techniques (Tanner, 1981). Using this model, Kumar et al. (2011) provided evidence that the resulting network can memorize an exponential number of patterns at the expense of cor recting only a single error during the recall phase. By introducing a multilayer 2structure, Salavati and Karbasi (2012) could further improve the error correction performance to constant number of errors. In this paper, similar to the model considered by Kumar et al. (2011), we only consider a set of patterns with weak minor components, i.e., patterns that lie in a subspace. By making use of this inherent redundancy We introduce the Ô¨Årst convolutional neural associative network with prov ably exponential storage capacity. We prove that our architecture can correct a linear fraction of errors. We develop an online learning algorithm with the ability to learn patterns as they arrive. This property is speciÔ¨Åcally useful when the size of the dataset is massive and patterns can only be learned in a streaming manner. We extend our results to the case where patterns lie approximately in a sub space. This extension in particular allows us to efÔ¨Åciently memorize datasets containing natural patterns. We evaluate the performance of our proposed architecture and the learning algorithm through numerical simulations. We provide rigorous analysis to support our claims. The storage capacity and error correction performance of our method is informationtheoretically order optimum, i.e., no other method can signiÔ¨Åcantly improve the results (except for constants). Our learning algorithm is an extension of the subspace learning method proposed by Oja and Kohonen (1988), with an additional property of imposing the learned vectors to be sparse . The sparsity is essential during the noiseelimination phase. The remainder of this paper is organized as follows. In Section 2 we provide an overview of the related work in this area. In Section 3 we introduce our notation and formally state the problems that is the focus of this work, namely, learning phase, recall phase, and storage capacity. We present our learning algorithm in Section 4 and our error correction method in Section 5. Section 6 is devoted to the pattern retrieval capacity. We then report our experimental results on synthetic and natural datasets in Section 7. Finally, all the proofs are provided in Section 8. 2 Related Work "
534,Ensemble Knowledge Distillation for Learning Improved and Efficient Networks.txt,"Ensemble models comprising of deep Convolutional Neural Networks (CNN) have
shown significant improvements in model generalization but at the cost of large
computation and memory requirements. In this paper, we present a framework for
learning compact CNN models with improved classification performance and model
generalization. For this, we propose a CNN architecture of a compact student
model with parallel branches which are trained using ground truth labels and
information from high capacity teacher networks in an ensemble learning
fashion. Our framework provides two main benefits: i) Distilling knowledge from
different teachers into the student network promotes heterogeneity in feature
learning at different branches of the student network and enables the network
to learn diverse solutions to the target problem. ii) Coupling the branches of
the student network through ensembling encourages collaboration and improves
the quality of the final predictions by reducing variance in the network
outputs. Experiments on the well established CIFAR-10 and CIFAR-100 datasets
show that our Ensemble Knowledge Distillation (EKD) improves classification
accuracy and model generalization especially in situations with limited
training data. Experiments also show that our EKD based compact networks
outperform in terms of mean accuracy on the test datasets compared to
state-of-the-art knowledge distillation based methods.","Ensemble methods have shown considerable improvements in model generalization and produced state of the art results in several ma chine learning competitions (e.g., Kaggle) [4]. These ensemble meth ods typically contain multiple deep Convolutional Neural Networks (CNN) as subnetworks which are pretrained on largescale datasets to extract discriminative features from the input data. The size of an ensemble is not constrained by training because the subnetworks can be trained independently, and their outputs can be computed in parallel. However, in many applications limited training data is not sufÔ¨Åcient to effectively train deep CNN models compared to small or compact networks. For instance in healthcare applications, the amount of available data is constrained by the number of patients. Therefore, improving generalization capability of compact network without requiring largescale annotated datasets is of utmost impor tance. Furthermore, today‚Äôs high performing deep CNN based en semble models have GigaFLOPS compute and GigaBytes storage requirements [12], making them prohibitive in resource constrained systems (e.g., mobile or edgedevices) which have stringent require ments on memory, latency and computational power. 1IBM Research Australia, email: umarasif@au1.ibm.com 2IBM Research Australia, email: jbtang@au1.ibm.com 3IBM Research Australia, email: sharrer@au1.ibm.comTo overcome these challenges, model compression techniques such as parameter pruning [24] is a common way to reduce model size with tradeoffs between accuracy and efÔ¨Åciency. Other tech niques include hand crafting efÔ¨Åcient CNN architectures such as SqueezeNets [13], MobileNets [11], and ShufÔ¨ÇeNets [26]. Recently, neural network search showed an effective way to generate efÔ¨Åcient CNN architectures [21, 3] by extensively tuning parameters such as network width, depth, Ô¨Ålter types and sizes. These models showed better efÔ¨Åciency than handcrafted networks but, at the cost of ex tremely large tuning cost. Another stream of work in building efÔ¨Å cient networks for resource constrained scenarios is through Knowl edge distillation [10]. It enables small low memory footprint net works to mimic the behavior of large complex networks by training small networks using the predictions of large networks as soft labels in addition to the ground truth hard labels. In this paper we also explore Knowledge Distillation (KD) based strategies to improve model generalization and classiÔ¨Åcation perfor mance for applications with memory and compute restrictions. For this, we present a CNN architecture with parallel branches which dis till high level features from different teacher networks during train ing and maintains low computational overhead during inference. Our architecture provides two main beneÔ¨Åts: i)It combines a student network with different teacher networks and distills diverse feature representations into the student network during training. This pro motes heterogeneity in feature learning and enables the student net work to mimic diverse highlevel feature spaces produced by the teacher networks. ii)It combines the distilled information through parallelbranches in an ensembling manner. This reduces variance in the branchlevel outputs and improves the quality of the Ô¨Ånal predic tions of the student network. In summary, the main contributions of this paper are as follows: 1. We present an Ensemble Knowledge Distillation (EKD) frame work which improves classiÔ¨Åcation performance and model gen eralization of small and compact networks by distilling knowledge from multiple teacher networks into a compact student network using an ensemble architecture. 2. We present a novel training objective function to distill ensemble knowledge into a single student network. Our objective function optimizes the parameters of the student network with a goal of learning mappings between input data and ground truth labels, and a goal of minimizing the difference between high level features of the teacher networks and the student network. 3. We perform ablation study of our framework on CIFAR10 and CIFAR100 datasets in terms of different CNN architectures, vary ing ensemble sizes, and limited training data scenarios. Experi ments show that by encouraging heterogeneity in feature learn ing through the proposed ensemble distillation, our EKDbasedarXiv:1909.08097v3  [cs.CV]  2 Apr 2020compact networks produce superior accuracy compared to the net works without using knowledge distillation. 2 Related Work "
535,Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss.txt,"The task of Fine-grained Entity Type Classification (FETC) consists of
assigning types from a hierarchy to entity mentions in text. Existing methods
rely on distant supervision and are thus susceptible to noisy labels that can
be out-of-context or overly-specific for the training sentence. Previous
methods that attempt to address these issues do so with heuristics or with the
help of hand-crafted features. Instead, we propose an end-to-end solution with
a neural network model that uses a variant of cross- entropy loss function to
handle out-of-context labels, and hierarchical loss normalization to cope with
overly-specific ones. Also, previous work solve FETC a multi-label
classification followed by ad-hoc post-processing. In contrast, our solution is
more elegant: we use public word embeddings to train a single-label that
jointly learns representations for entity mentions and their context. We show
experimentally that our approach is robust against noise and consistently
outperforms the state-of-the-art on established benchmarks for the task.","Finegrained Entity Type ClassiÔ¨Åcation (FETC) aims at labeling entity mentions in context with one or more speciÔ¨Åc types organized in a hier archy (e.g., actor as a subtype of artist , which in turn is a subtype of person ). Finegrained types help in many applications, including rela tion extraction (Mintz et al., 2009), question an swering (Li and Roth, 2002), entity linking (Lin et al., 2012), knowledge base completion (Dong et al., 2014) and entity recommendation (Yu et al., 2014). Because of the high cost in labeling large training corpora with Ô¨Ånegrained types, current FETC systems resort to distant supervision (Mintz et al., 2009) and annotate mentions in the train ing corpus with all types associated with the en tity in a knowledge graph. This is illustrated inFigure 1, with three training sentences about en titySteve Kerr . Note that while the entity be longs to three Ô¨Ånegrained types ( person ,athlete , andcoach ), some sentences provide evidence of only some of the types: person andcoach from S1,person andathlete from S2, and just person forS3. Clearly, direct distant supervision leads to noisy training data which can hurt the accuracy of the FETC model. One kind of noise introduced by distant super vision is assigning labels that are outofcontext (athlete inS1andcoach inS2) for the sentence. Current FETC systems sidestep the issue by ei ther ignoring outofcontext labels or using simple pruning heuristics like discarding training exam ples with entities assigned to multiple types in the knowledge graph. However, both strategies are in elegant and hurt accuracy. Another source of noise introduced by distant supervision is when the type isoverlyspeciÔ¨Åc for the context. For instance, ex ample S3does not support the inference that Mr. Kerr is either an athlete or acoach . Since existing knowledge graphs give more attention to notable entities with more speciÔ¨Åc types, overlyspeciÔ¨Åc labels bias the model towards popular subtypes in stead of generic ones, i.e., preferring athlete over person . Instead of correcting for this bias, most existing FETC systems ignore the problem and treat each type equally and independently, ignor ing that many types are semantically related. Besides failing to handle noisy training data there are two other limitations of previous FETC approaches we seek to address. First, they rely on handcrafted features derived from various NLP tools; therefore, the inevitable errors introduced by these tools propagate to the FETC systems via the training data. Second, previous systems treat FETC as a multilabel classiÔ¨Åcation problem: during type inference they predict a plausibility score for each type, and, then, either classify typesarXiv:1803.03378v2  [cs.CL]  14 Apr 2018Figure 1: With distant supervision, all the three mentions of Steve Kerr shown are labeled with the same types in oval boxes in the target type hierarchy. While only part of the types are correct: person andcoach forS1,person andathlete forS2, and just person forS3. with scores above a threshold (Mintz et al., 2009; Gillick et al., 2014; Shimaoka et al., 2017) or per form a topdown search in the given type hierarchy (Ren et al., 2016a; Abhishek et al., 2017). Contributions: We propose a neural network based model to overcome the drawbacks of exist ing FETC systems mentioned above. With pub licly available word embeddings as input, we learn two different entity representations and use bidi rectional longshort term memory (LSTM) with attention to learn the context representation. We propose a variant of cross entropy loss function to handle outofcontext labels automatically during the training phase. Also, we introduce hierarchical loss normalization to adjust the penalties for corre lated types, allowing our model to understand the type hierarchy and alleviate the negative effect of overlyspeciÔ¨Åc labels. Moreover, in order to simplify the problem and take advantage of previous research on hierar chical classiÔ¨Åcation, we transform the multilabel classiÔ¨Åcation problem to a singlelabel classiÔ¨Åca tion problem. Based on the assumption that each mention can only have one typepath depending on the context, we leverage the fact that type hier archies are forests, and represent each typepath uniquely by the terminal type (which might not be a leaf node). For Example, typepath root personcoach can be represented as just coach , while rootperson can be unambiguously repre sented as the nonleaf person . Finally, we report on an experimental validationagainst the stateoftheart on established bench marks that shows that our model can adapt to noise in training data and consistently outperform previ ous methods. In summary, we describe a single, much simpler and more elegant neural network model that attempts FETC ‚Äúendtoend‚Äù without postprocessing or adhoc features and improves on the stateoftheart for the task. 2 Related Work "
536,Overhead MNIST: A Benchmark Satellite Dataset.txt,"The research presents an overhead view of 10 important objects and follows
the general formatting requirements of the most popular machine learning task:
digit recognition with MNIST. This dataset offers a public benchmark extracted
from over a million human-labelled and curated examples. The work outlines the
key multi-class object identification task while matching with prior work in
handwriting, cancer detection, and retail datasets. A prototype deep learning
approach with transfer learning and convolutional neural networks (MobileNetV2)
correctly identifies the ten overhead classes with an average accuracy of
96.7%. This model exceeds the peak human performance of 93.9%. For upgrading
satellite imagery and object recognition, this new dataset benefits diverse
endeavors such as disaster relief, land use management, and other traditional
remote sensing tasks. The work extends satellite benchmarks with new
capabilities to identify efficient and compact algorithms that might work
on-board small satellites, a practical task for future multi-sensor
constellations. The dataset is available on Kaggle and Github.","The most popular  starting test  for both new and established machine learning  algorithms  relies on handwritten digit  [1] or letter [2]  recognition . If a method  does not work with the M odified National Institute of Standards and Technology  dataset (MNIST), it most likely will not work on more challenging tasks. As  illustrated in Figure 1, t he core task corresponds to a multi class image challenge,  one which  proves common and useful in other fields [3] outside of algorithms to  interpret handwriting. Research ers over the last two decades [4] have spawned  more than 48,000 MNIST related publications , with a quarter of those appearing  in 2020. The reverse of this  universality stems from the relative ease which   modern algorithms have solve d the problem (>99% accuracy after a few  iterations)  [56].  Critics of the digit dataset {09} note that it contains near  duplicates  and lacks diversity in example data such that  modifying a single pixel  (among 784 pixels in a 28x28 image) can flip some algorithms to misidentify the expected digit  [79].  Alternative practical extensions of the digit recognition task now include alphabetic h andwriting in multiple  languages ( e.g. English  [2], Chinese  [10], Russian  [11], Kan nada [12], American Sign Language  [13]) and  related everyday object  recognition , the most popular of which includes 10 categories of  skin cancer [14]  and clothing  [15] in thumbnail grayscale images  (HAM 1000  [14] and Fashion MNIST  [15]).   The present work provides another challenging object recognition task : labelling objects from  overhead  satellite imagery  (Figure 2) .  To take advantage of the vast machine learning literature on digit recognition,  we mirror the format of the original MNIST closely [1] and thus , like Fashion MNIST  [15], we aim to  provide the research community with a nother  dropin replacement for bench marking  [16]. The grayscale  (28x28 pixel) imagery provides  a challenging object recognition task  [17]. As viewed from above, objects   such as planes, ships, and stadiums  offer no obvious preferred orientation, so rotating or image shearing  may not augment d ataset diversity  [18]. Overhead object classifiers also suffer from scale variations that   Figure 1. Examples of  MNIST families for digits,  letters, signing, and objects  can range  more than  two orders of magnitude between a small car to a stadium  [1921]. Compared to  alternative terrestrial (color) datasets (like CIFAR 10 thumbnail [22]), a classifier of satellite imagery may  span different camera resolutions, orientations, and daynight contrast levels  in different seasons and  shadows. The research offers  a novel benchmark  [23] for recognizing objects in thumbnail satellite images,  a reformatting strategy to connect  with the vast MNIST algorithm literature and finally, an initial solution  to the classification problem using transfer learning and MobileNet V2 [24]. The original contributions of  the present work include: 1) generalizing the standard MNIST format  [1,16]  and dataset design to handle  challenging satellite object detection (called Overhead MNIST); 2) exploring the unique aspects of  overhead object recognition such as diverse object scale  lengths  and rotational invariance  [17,21 ]; 3)  classifying 10 classes of objects with multiple algorithms, including some efficient enough to run on board  satellites for automated tasking and cueing. Absent the 70,00 0 overhead thumbnails  of the original digit  recognition [1] , we examine the requirements for solving the problem as a function of sample size.     Figure 2.  Ten Classes Labelled for Overhead MNIST.  The class selection includes dynamic (car, plane, helicopter, ship) and static  (parking lot, runway mark, harbor) targets along with infrastructure related objects (storage tanks, stadium, oil gas fields)   2. METHODS   "
537,Multi-attention Networks for Temporal Localization of Video-level Labels.txt,"Temporal localization remains an important challenge in video understanding.
In this work, we present our solution to the 3rd YouTube-8M Video Understanding
Challenge organized by Google Research. Participants were required to build a
segment-level classifier using a large-scale training data set with noisy
video-level labels and a relatively small-scale validation data set with
accurate segment-level labels. We formulated the problem as a multiple instance
multi-label learning and developed an attention-based mechanism to selectively
emphasize the important frames by attention weights. The model performance is
further improved by constructing multiple sets of attention networks. We
further fine-tuned the model using the segment-level data set. Our final model
consists of an ensemble of attention/multi-attention networks, deep bag of
frames models, recurrent neural networks and convolutional neural networks. It
ranked 13th on the private leader board and stands out for its efficient usage
of resources.","With the fast development of digital recording devices and online video sharing platforms, the number of videos available is increasing exponentially, making video under standing a challenging problem in computer vision. As a Ô¨Årst step towards video understanding, a signiÔ¨Åcant amount of work has been dedicated to video classiÔ¨Åcation. How ever, the video understanding problem goes beyond a stan dard classiÔ¨Åcation problem. Temporally localizing the pres ences of objects/actions can help us to identify relevant mo ments in a video and thus better understand its content. This work was presented at the 3rd Workshop on YouTube8M Large Scale Video Understanding, at the International Conference on Computer Vision (ICCV 2019) in Seoul, Korea.Moreover, a video can contain a number of topics that are not always characterized by every time segment within the video. Hence, a better temporal localization algorithm can enable applications such as improved video search (search within a video), highlights extraction, etc. To accelerate the research of temporal localization, Google AI recently released the YouTube8M Segment Dataset. In this work, we focus on a segmentlevel classiÔ¨Åcation task presented in the third YouTube8M Challenge using this segmentlevel dataset. Previous YouTube8M challenges focused on developing models for videolevel predictions. Standard deep neural networks like convolutional neural networks (CNNs) [9, 21] and recurrent neural networks (RNNs) [12, 16] have been used for videolevel classiÔ¨Åcation and both have achieved stateoftheart results. Pooling via clustering schemes, such as Vector of Locally aggregated Description (VLAD) [7, 15] and deep bagofframes (DBoF) [1, 18], has also been widely used among the top competitors. However, these framelevel classiÔ¨Åers are designed to classify video level labels and cannot necessarily perform segmentlevel predictions well. Different temporal action localization net works have also been proposed to solve the temporal lo calization problem. One popular structure is a twostage, proposalplusclassiÔ¨Åcation framework [3]. But to utilize large videolabeled training sets, such a model cannot be directly applied. To better leverage the large training dataset which only has noisy videolevel labels together with a comparatively smaller segmentlevel validation dataset, we propose to uti lize a multiinstance learning (MIL) model [13, 5] to simul taneously temporally localize and classify the target seg ments. The core idea is to use multiple attention weights to emphasize critical frames from different highlevel top ics in the video. The proposed model performed better than both standard neural networks and pooling via clustering methods using our training procedure. Before discussing 1arXiv:1911.06866v1  [cs.CV]  15 Nov 2019the models for the task in this challenge, we will give a brief overview of the dataset and the unique challenges it poses. 1.1. YouTube8M Segment Dataset The YouTube8M Segment Dataset is an extension of the previous YouTube8M Dataset [1, 11] which in cludes humanveriÔ¨Åed segmentlevel labels. The previous YouTube8M dataset contains 6 million highquality video samples from YouTube, which were split into 3 partitions: training, validation and test sets, following approximately 70%,20% and10% split. The video samples were encoded as a hidden representation produced by Inceptionv3 [20] pretrained on the ImageNet dataset [6] for both audio and video features taken at a rate of 1 Hz. This dataset only contains videolevel annotations with 3862 class labels and an average of 3 labels per video. In the YouTube8M Segment Dataset, multiple 5second segments are sampled based on classiÔ¨Åer scores to encour age both positive and negative within a video. Then each segment is labeled by human raters from a subset of original vocabulary, excluding entities that are not temporally local izable. In total, 237K segments covering 1000 categories are labeled. These video segment labels provide a valuable resource for temporal localization. In the 3rd YouTube8M video understanding challenge, we are required to predict segmentlevel labels in the test set. Submissions are evalu ated using the Mean Average Precision @ Ks(MAP@Ks), whereKs= 100;000. For each entity, the MAP@ Ksscore is calculated as MAP @Ks=1 CCX c=1KsP k=1P(k)rel(k) Nc(1) whereCis the number of Classes, P(k)is the precision at cutoffk,Ksis the number of segments predicted per class, rel(k)is an indicator function equaling 1if the item at rank kis a relevant (correct) class, or zero otherwise, and Ncis the number of positivelylabeled segments for each class. The paper is organized as follows. In section 2, we re view the general model architectures we used, along with some related work. Details of our models are introduced in section 3. Section 4 presents the evaluation of all models, and section 5 concludes the paper. 2. Related work "
538,Multi-Modal Emotion Detection with Transfer Learning.txt,"Automated emotion detection in speech is a challenging task due to the
complex interdependence between words and the manner in which they are spoken.
It is made more difficult by the available datasets; their small size and
incompatible labeling idiosyncrasies make it hard to build generalizable
emotion detection systems. To address these two challenges, we present a
multi-modal approach that first transfers learning from related tasks in speech
and text to produce robust neural embeddings and then uses these embeddings to
train a pLDA classifier that is able to adapt to previously unseen emotions and
domains. We begin by training a multilayer TDNN on the task of speaker
identification with the VoxCeleb corpora and then fine-tune it on the task of
emotion identification with the Crema-D corpus. Using this network, we extract
speech embeddings for Crema-D from each of its layers, generate and concatenate
text embeddings for the accompanying transcripts using a fine-tuned BERT model
and then train an LDA - pLDA classifier on the resulting dense representations.
We exhaustively evaluate the predictive power of every component: the TDNN
alone, speech embeddings from each of its layers alone, text embeddings alone
and every combination thereof. Our best variant, trained on only VoxCeleb and
Crema-D and evaluated on IEMOCAP, achieves an EER of 38.05%. Including a
portion of IEMOCAP during training produces a 5-fold averaged EER of 25.72%
(For comparison, 44.71% of the gold-label annotations include at least one
annotator who disagrees).","Due to the growing presence of AIpowered systems in our lives, affective computing has become an important part of humancomputer interaction. Emotion plays a role in our thoughts and actions and is an integral part of the way we communicate (Choi et al., 2018). The ability to leverage context to understand emotions communicated both verbally and nonverbally is trivial for humans but remains difÔ¨Åcult for machines (Chen et al., 2019). Emotional responses depend on both our psyche and physiology and are governed by our perception of situations, people and objects. They also depend on our mental state (mood, motivation, temperament) (Tripathi et al., 2018a). The way we exhibit and perceive emotion may also differ based on our age, gender, race, culture and accent (Latif et al., 2019). In addition to all of this, unlike targets in other classiÔ¨Åcation tasks, the emotions we experience are rarely distinct: they often coexist without clear temporal boundaries, adding considerable complexity to the task (Tzirakis et al., 2017). Despite these difÔ¨Åculties, automated emotion recognition has social and commercial applications that make it worth pursuing. In the medical domain, it has exciting potential: to identify and diagnose depression and stress in individuals (Zhu et al., 2017; Rana et al., 2019), to monitor and help people with bipolar disorder (Rana, 2016) and to assist the general public in maintaining mental health. Commercial applications include call center customer management, advertising through neuromarketing and social media engagement (Tzirakis et al., 2017; Choi et al., 2018; Chen et al., 2019). As intelligent chatbots and virtual assistants have become more widely used, emotion detection has become a vital component in the design, development and deployment of these conversational agents (Yoon et al., 2018).arXiv:2011.07065v1  [eess.AS]  13 Nov 2020Early research in emotion detection focused on binary classiÔ¨Åcation in a single modality, whether in text, speech (Chernykh and Prikhodko, 2017; Neumann and Vu, 2017), or images (Dhall et al., 2015). Textbased classiÔ¨Åers used the ngram vocabulary of sentences to predict their polarity and speech models modeled the vocal dynamics that characterize these emotions. These approaches are inherently limited: a binary granularity and cues from a single modality are far removed from the actual human process they‚Äôre meant to model. As a result, joint approaches which leverage all available modalities (e.g., both speech and text in applications like home assistants) are promising. While existing multimodal emotion corpora like IEMOCAP (Busso et al., 2008) and CremD (Cao et al., 2014) have been critical for the progress in affective computing to date, they suffer from three issues that are the focus of our work. First, these corpora tend to be small due to the high costs of annotating for emotion. This precludes the use of deep neural models with high model complexity as they require many training samples to generalize well. This also compounds the second difÔ¨Åculty inherent to many emotion datasets: while there are usually many neutral, happy and sad training examples, there are often very few examples of rarer emotions like disgust making them difÔ¨Åcult to classify. This issue is not easily solved by combining different corpora due to the third issue, their lack of mutual compatibility ‚Äì they differ in the emotions identiÔ¨Åed, the types of dialogue and number of speakers represented and the naturalness of the recordings (see Figure 1). This severely restricts the generalizability of models trained on a single corpus. Contemporary literature has dealt with these problems by dropping labels (Pappagari et al. (2020); Chen and Zhao (2020); Yoon et al. (2020)). Hard and scarce emotions like disgust are dropped from the corpus and the models are trained and evaluated on the trimmed corpus. This allows evaluating models on different corpora by using utterances exhibiting only the most common emotions. While this is a reasonable, the resulting performance is not a complete reÔ¨Çection of how these models perform once deployed to production. When emotion models are used in realworld applications, we can expect them to encounter utterances corresponding to dropped labels. For such cases, these models are likely to exhibit degraded performance by predicting one of the known, but incorrect labels. In this work, we address the problem of data sparsity by transfer learning via the pretrainthenÔ¨Ånetune paradigm. Deep complex models can be trained on large datasets for an auxiliary but related task to learn network parameters that reÔ¨Çect abstract notions related to the target task. As the expression of emotions is highly dependent on the individual, we train a multilayer TDNN (Waibel et al. (1989)) on the task of speaker identiÔ¨Åcation using the V oxCeleb corpus (Chung et al. (2018)) and then Ô¨Ånetune its Ô¨Ånal few layers on the task of emotion identiÔ¨Åcation using the CremaD corpus (Cao et al. (2014)). Using this network, we extract speech embeddings for CremaD from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a Ô¨Ånetuned BERT model (Devlin et al. (2018)) and then train an LDA  pLDA (Fisher (1936); Ioffe (2006)) model on the resulting dense representations. pLDA allows our model to more easily adapt to previously unseen classes and domains, a requirement for both evaluating against a different emotion corpus with an incompatible label set and performing well in the wild. To understand the merits of each component, we exhaustively evaluate the predictive power of every permutation: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof. Our best variant, trained on only V oxCeleb and CremaD and evaluated on IEMOCAP, achieves an Equal Error Rate (EER) of 38:05%. Including a portion of IEMOCAP during training produces a 5fold averaged EER of 25:72%. 2 Related Work "
539,Data efficient surrogate modeling for engineering design: Ensemble-free batch mode deep active learning for regression.txt,"In a computer-aided engineering design optimization problem that involves
notoriously complex and time-consuming simulator, the prevalent approach is to
replace these simulations with a data-driven surrogate that approximates the
simulator's behavior at a much cheaper cost. The main challenge in creating an
inexpensive data-driven surrogate is the generation of a sheer number of data
using these computationally expensive numerical simulations. In such cases,
Active Learning (AL) methods have been used that attempt to learn an
input--output behavior while labeling the fewest samples possible. The current
trend in AL for a regression problem is dominated by the Bayesian framework
that needs training an ensemble of learning models that makes surrogate
training computationally tedious if the underlying learning model is Deep
Neural Networks (DNNs). However, DNNs have an excellent capability to learn
highly nonlinear and complex relationships even for a very high dimensional
problem. To leverage the excellent learning capability of deep networks along
with avoiding the computational complexity of the Bayesian paradigm, in this
work we propose a simple and scalable approach for active learning that works
in a student-teacher manner to train a surrogate model. By using this proposed
approach, we are able to achieve the same level of surrogate accuracy as the
other baselines like DBAL and Monte Carlo sampling with up to 40 % fewer
samples. We empirically evaluated this method on multiple use cases including
three different engineering design domains:finite element analysis,
computational fluid dynamics, and propeller design.","In the engineering and scientic community, surrogate modeling is a prevalent approach in the design process that involves computationally costly, complex simulators. Surrogate is a datadriven approximation for a physicsbased simulation typically involving interpolation or regression on a set of data generated from the original simulator[1]. Learning these highly complex nonlinear hyperplanes using learning models can assist human designers to nd good designs much faster than traditional methods. The expected benet of creating a surrogate is two folds: rst, leverage the generalization capability of learning models to get a good approximate prediction on other designs in the design space, and second, parallel faster evaluation of design points, which will speed up the whole design decision process. Creating a surrogate for a reasonably sized design space for a complex simulation process has two main issues: rst, since these simulation models and the simulation process have an iterative subroutine and consequently long simulation time, without any strategic sampling approach, the trivial randomized sampling approach to generate data becomes unavailing in high dimension design space because the number of data points needed to give reasonably uniform coverage rises exponentially this phenomenon is infamously called the curse of dimensionality[2]. The second issue is the stepsizing of the sampling i.e., without any prior information about the involved nonlinearity in the solution space manifold, setting an arbitrary step size for sample selection consequently aects the quality of the collected data and trained surrogate. Corresponding author Email address: harsh.vardhan@vanderbilt.edu (Harsh Vardhan) Preprint submitted to Engineering applications of Articial IntelligencearXiv:2211.10360v1  [cs.LG]  16 Nov 2022The utopian solution would be a dynamically adaptive step sizing to select samples with maximum informational value to learn these hyperplanes that produce an accurate surrogate in a few strategic samples. Traditional surrogate modeling approaches use learning models like kriging[1], Gaussian Process[3], etc., to do strategic adaptive sampling and step sizing but these learning models are not scalable for high dimensions or big design space. The recent advances in Machine Learning (ML) models, especially in Deep Learning (DL) have shown it as a promising paradigm in this scenario. Deep learning models have powerful learning capabilities, can learn in highdimension space and automatically extract salient features, and have been used in various engineering surrogate problems[4, 5, 6]. However, for using a deep learning model for surrogate modeling in a meaningful design space, we need to address the issue of strategic sampling and adaptive step sizing. In most learning problem settings, we rely on theories for data modeling, which often assume that the data is provided by a source that we do not control, but we can sample from the data source. However, for a practical design problem, the data labeling /evaluation is left to the designer and which is relatively computationally expensive or slow in some complex engineering domains. In this scenario, the designer needs to be frugal and should select samples with the goal to learn as much as possible. To this end, a framework or algorithm is required that can objectively estimate the utility of candidate data points. The AL framework directly addresses this by estimating the utility of candidate data points. By combining DL with AL (this subeld of ML is called DeepAL[7]), it is possible to retain the strong high dimensional learning capability of DL while also beneting from strategic samples selected by AL. Compared with the multiple research work on Deep active learning for classication, there are only a few approaches for Deep active learning for a regression problem. For a classication problem, since the output is the probability of a class, that can be easily used with other statistical measures (like Shannon's Entropy[8] function) to create a query strategy. Many research work has leveraged it to create one or another  avor of query strategy. However, most scientic surrogate modeling problem involved in the design process are regression problems and their output is a scalar value or a vector of scalar values. In such cases, direct statistical measurement to give information about the quality of the sample cannot be deployed. To counter this, researchers took the route of the Bayesian framework to measure variance in estimation by an ensemble of predictors that can estimate uncertainty directly such as the Bayesian Neural network[9], or to estimate uncertainty indirectly by ensembling models[10]. Bayesian methods become expensive when the number of samples is in thousands, or the input dimension exceeds 10. Ensemble methods require independent training of several models, which is also timeconsuming. Another challenge in applying AL is that most AL framework works on a single sample per iteration. In contrast, retraining a neural network after each single data point is not computationally feasible. In such a scenario, although AL relatedresearch on query strategy is quite rich, its direct application in the DL framework for a regression problems is complex. To solve adaptive sampling and stepsizing for training a deep learning model for regression problem, in this work we propose a scalable feedbackbased learning architecture called `studentteacher' architecture. It is an iterative learning process similar to AL paradigm in which the teacher is an another neural network with the goal to nd the regions where the student network is failing and can guide sampling and data generation in the next iteration. In the context of feedbackbased learning, an analogy can be found in actorcritic based learning[11] in Reinforcement learning[12] literature. Empirical evaluation of the proposed approach shows better accuracy of the trained surrogate model compared to other baseline methods in dierent engineering domains. The major contribution of this work is summarized below: ‚Ä¢Development of a noble approach of deep active learning for regression problems by selecting strategic samples during surrogate training to learn the inputoutput behavior of a function/simulator with fewer evaluations. ‚Ä¢Development of a scalable and easytotrain batch mode DeepAL approach for surrogate training that is tractable as it needs to train only one main network called `student/learner' and one small network called `teacher/guide'. ‚Ä¢Empirical evaluation of the proposed method on dierent realworld engineering design domains and comparison with other baseline methods to establish the performance gains from this novel approach in terms of sample eciency or better accuracy. 2‚Ä¢ To our knowledge, all active learning{based adaptive sampling approach for regression problem uses the Bayesian approach, which is not scalable or dicult to train for high dimensional design space. Our method of active sampling is scalable since it uses only one main network (student) and one small (in comparison to the student network) teacher network. It selects a batch of the most informative samples and automatically handles explorationexploitation dilemmas during design exploration. This computationally scalable approach can make learning in a big design space possible, which was not possible with current approaches. Accordingly, surrogate training and trained model can make a big leap in the design automation process. We also observed that in high dimensional design space, on a given number of labeled samples the gap between the accuracy of our method and other baselines is wider in comparison to low dimensional design space. This is intuitive because spacelling sampling used in baseline methods needs combinatorial exponential samples while our approach is very strategic in labeling. The code and data used for our method is available publicly at https://github.com/vardhah/epsilon_weighted_Hybrid_Query_Strategy . The remainder of this paper is organized as follows: Section 2 talks about the related work in this eld. In Section 3, we formalize the problem and discuss our methodology and baselines in detail. In Section 4, we present our experiments and evaluation of results. We conclude in Section 5 with a brief discussion of future work. 2. Related Work "
540,Meta Self-Refinement for Robust Learning with Weak Supervision.txt,"Training deep neural networks (DNNs) under weak supervision has attracted
increasing research attention as it can significantly reduce the annotation
cost. However, labels from weak supervision can be noisy, and the high capacity
of DNNs enables them to easily overfit the label noise, resulting in poor
generalization. Recent methods leverage self-training to build noise-resistant
models, in which a teacher trained under weak supervision is used to provide
highly confident labels for teaching the students. Nevertheless, the teacher
derived from such frameworks may have fitted a substantial amount of noise and
therefore produce incorrect pseudo-labels with high confidence, leading to
severe error propagation. In this work, we propose Meta Self-Refinement (MSR),
a noise-resistant learning framework, to effectively combat label noise from
weak supervision. Instead of relying on a fixed teacher trained with noisy
labels, we encourage the teacher to refine its pseudo-labels. At each training
step, MSR performs a meta gradient descent on the current mini-batch to
maximize the student performance on a clean validation set. Extensive
experimentation on eight NLP benchmarks demonstrates that MSR is robust against
label noise in all settings and outperforms state-of-the-art methods by up to
11.4% in accuracy and 9.26% in F1 score.","Finetuning pretrained language models has led to great success across NLP tasks. Nonetheless, it still requires a substantial amount of manual labels to achieve satisfying performance on many tasks. In reality, obtaining large amounts of highquality labels is costly and laborintensive (Davis et al., 2013). For certain domains, it is even infeasible due to legal issues and lack of data or domain experts. Weak supervision is a widelyused approach for reudcing such cost by leveraging labels from weak sources, e,g., heuristic rules, knowledge bases orlowerquality inexpensive crowdsourcing (Ratner et al., 2017; Zhou et al., 2020; Lison et al., 2020). It has raised increasing attention in recent years, and efforts have been made to quantify the progress on weakly supervised learning, like the WRENCH benchmark (Zhang et al., 2021). Although weak labels are inexpensive to ob tain, they are often noisy and inherit biases from weak sources. Training neural networks with weak labels is challenging because of their immense capacity, which leads them to heavily overÔ¨Åt to the noise distribution, resulting in inferior gener alization performance (Zhang et al., 2017). Vari ous approaches have been proposed to tackle this challenge. Earlier research focused primarily on simulated noise (Bekker and Goldberger, 2016; Hendrycks et al., 2018), required prior knowl edge (Ren et al., 2020; Awasthi et al., 2020) or relied on contextfree aggregation rules without leveraging modern pretrained language models (Ratner et al., 2017; Fu et al., 2020). Recently, Yu et al. (2021) proposed a contrastive regularized selftraining framework that achieved stateoftheart (SOTA) performance in several NLP tasks from the WRENCH benchmark. It trains a teacher network on weak labels, then it eratively applies the teacher to produce pseudo labels for training a new student model. To pre vent error propagation, it Ô¨Ålters the pseudolabels with the model conÔ¨Ådence scores and adds con trastive feature regularization to enforce more dis tinguishable representations. However, we Ô¨Ånd that this approach is effective on easy tasks but fragile on challenging ones , where the initial teacher model already have memorized a substan tial amount of biases with high conÔ¨Ådence. Con sequently, conÔ¨Ådencebased Ô¨Åltering is misleading and all future students will be reinforced with these initial wrong biases from the teacher. To address this weakness, one strategy is learn ing to reweight the pseudolabels with meta learnarXiv:2205.07290v2  [cs.CL]  30 Apr 2023ing (Ren et al., 2018; Shu et al., 2019; Wang et al., 2020). By this means, sample weights are dynam ically adjusted to minimize the validation loss in stead of preÔ¨Åxed with potentially misleading conÔ¨Å dence scores. Nevertheless, if the initial teacher is weak and mostly produces incorrect pseudolabels, simply reweighting the labels does not sufÔ¨Åce to extract enough useful training signals. In this paper, we propose Meta SelfReÔ¨Ånement (MSR) to go one step further. The teacher is jointly trained with a meta objective such that the student, after one gradient step, can achieve better perfor mance on the validation set. In each training step, a copy of the current student performs one step of gradient descent based on the teacher predic tions. The teacher will then update itself towards the gradient direction that minimizes the validation loss of the student. Finally, the actual student is trained by the updated teacher. In MSR, teacher‚Äôs predictions are iteratively reÔ¨Åned , instead of only ‚Äúreweighted‚Äù, based on the meta objective. This will enable more efÔ¨Åcient data utilization since the teacher still has the opportunity to reÔ¨Åne itself to provide the proper training signal, even if its initial output label is wrong. To further stabilize the train ing, we enhance our framework with conÔ¨Ådence Ô¨Åltering when teaching the student and apply a lin early scaled learning rate scheduler to the teacher. In summary, the main contributions are as fol lows: 1)We propose a metalearning based self reÔ¨Ånement framework, MSR, that allows robust learning with label noise induced by weak supervi sion. 2)We analyze and quantify how label noise impacts model predictions and representation learn ing. We Ô¨Ånd existing methods become less effec tive in challenging cases when the label noise can be easily Ô¨Åtted. In contrast, MSR is more stable and learns better representation. 3)Extensive experi ments demonstrate that MSR consistently reduces the negative impact of the label noise, matching or outperforming SOTAs on six sequence classiÔ¨Åca tion and two sequence labeling tasks.1 2 Related Work "
541,Double Descent Optimization Pattern and Aliasing: Caveats of Noisy Labels.txt,"Optimization plays a key role in the training of deep neural networks.
Deciding when to stop training can have a substantial impact on the performance
of the network during inference. Under certain conditions, the generalization
error can display a double descent pattern during training: the learning curve
is non-monotonic and seemingly diverges before converging again after
additional epochs. This optimization pattern can lead to early stopping
procedures to stop training before the second convergence and consequently
select a suboptimal set of parameters for the network, with worse performance
during inference. In this work, in addition to confirming that double descent
occurs with small datasets and noisy labels as evidenced by others, we show
that noisy labels must be present both in the training and generalization sets
to observe a double descent pattern. We also show that the learning rate has an
influence on double descent, and study how different optimizers and optimizer
parameters influence the apparition of double descent. Finally, we show that
increasing the learning rate can create an aliasing effect that masks the
double descent pattern without suppressing it. We study this phenomenon through
extensive experiments on variants of CIFAR-10 and show that they translate to a
real world application: the forecast of seizure events in epileptic patients
from continuous electroencephalographic recordings.","The evolution of the generalization error during training is often closely analyzed by machine learning practitioners to make substantial decisions about hyperparameter tuning, model architecture, or collection of additional data. The estimation of the generalization error can consequently be considered to be at the core of machine learning. Knowing when to stop training neural networks is crucial to reaching optimal generalization perfor mance. Estimating the exact optimal stopping time is still subject to debate, with novel earlystopping strategies continuously being proposed in the literature [ 8,27]. Most early stopping strategies would assume a steady increase in the generalization error across epochs as a reason to stop training. On new, unresolved tasks, networks may not have access to enough input information to predict the labels correctly. In this scenario, most machine learning practitioners would also stop training after observing a divergence of the generalization error, and potentially modify their research direction. Preprint. Under review.arXiv:2106.02100v2  [cs.LG]  17 Sep 2021For example, this can occur in medical datasets when the acquired patient data could be insufÔ¨Åcient to identify the target outcome. In most cases, most early stopping algorithms and machine learning practitioners would assume a steady increase in the generalization error across epochs as a reason to stop training. However, research has shown that small and noisy datasets may trigger an epochwise double descent optimiza tion pattern [ 22]. This means that after getting worse through the epochs, the generalization error reduces again, potentially leading to an overall smaller generalization error. Stopping the training before this second descent would be suboptimal. Double descent has been qualitatively deÔ¨Åned by others [ 22]. In this article, we mathematically deÔ¨Åne epochwise double descent and conÔ¨Årm earlier Ô¨Åndings demonstrating that double descent appears when the training set is both small and has noisy labels. We also empirically show that: ‚Ä¢Both training and generalization sets must have noisy labels for a double descent pattern to appear. ‚Ä¢If only the labels of the training set are noisy, double descent does not occur. Instead, a plateau pattern may appear. ‚Ä¢Even when the training set is small and labels are noisy in both training and generalization sets, there exists a learning rate for which the double descent pattern does not appear. ‚Ä¢The double descent pattern appears when the learning rate is too small, with the exact value of the learning depending on the dataset and task. ‚Ä¢Increasing the learning rate may create an aliasing effect that hides the double descent pattern without suppressing it. 2 Related Work "
542,Learning with Noisy Labels for Robust Point Cloud Segmentation.txt,"Point cloud segmentation is a fundamental task in 3D. Despite recent progress
on point cloud segmentation with the power of deep networks, current deep
learning methods based on the clean label assumptions may fail with noisy
labels. Yet, object class labels are often mislabeled in real-world point cloud
datasets. In this work, we take the lead in solving this issue by proposing a
novel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing
noise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with
the spatially variant noise rate problem specific to point clouds.
Specifically, we propose a novel point-wise confidence selection to obtain
reliable labels based on the historical predictions of each point. A novel
cluster-wise label correction is proposed with a voting strategy to generate
the best possible label taking the neighbor point correlations into
consideration. We conduct extensive experiments to demonstrate the
effectiveness of PNAL on both synthetic and real-world noisy datasets. In
particular, even with $60\%$ symmetric noisy labels, our proposed method
produces much better results than its baseline counterpart without PNAL and is
comparable to the ideal upper bound trained on a completely clean dataset.
Moreover, we fully re-labeled the validation set of a popular but noisy
real-world scene dataset ScanNetV2 to make it clean, for rigorous experiment
and future research. Our code and data are available at
\url{https://shuquanye.com/PNAL_website/}.","In recent years, the development of deep neural networks (DNNs) has led to great success in 3D point cloud segmen tation [11, 36, 32]. Thanks to the powerful learning capac ity, once highquality annotations are given, DNNsbased point segmentation methods can achieve remarkable perfor mance. However, such high learning capacity is a double edged sword, i.e., it can also overÔ¨Åt label noise and incur *Jing Liao is the corresponding author.performance degradation if there are incorrect annotations. In fact, compared to annotating 2D images, clean 3D data labels are more difÔ¨Åcult to obtain. It is mainly be cause: 1) the point number to annotate is often very mas sive, e.g., million scale in annotating a typical indoor scene in ScanNetV2 [6]; 2) the annotating process is inherently more complex and requires more expertise for the annota tors, e.g., constantly changing the view, position and scale to understand the underlying 3D structure. As a result, even the commonly used 3D scene dataset ScanNetV2 [6], which is already a version after reÔ¨Åning the label from the Scan Net, has a large portion of label noise, as shown in Figure 1. Based on the above considerations, there is an urgent need to study how to learn with noisy labels for robust point cloud segmentation. However, to the best of our knowl edge, most research works about learning with noisy la bels focus on image classiÔ¨Åcation, and no previous study exists for point cloud segmentation. More importantly, such works designed for image recognition cannot be di rectly applied to point cloud segmentation. For example, among the most popular methods, sample selection meth ods [12, 33, 24, 26, 17] often assume that the noise rate of all samples is a known constant value. However, the noise rates are often unknown and variable. Robust loss function methods [35, 30] cannot achieve consistent noise robustness to large noise rates. Whereas label correction methods [23, 26, 1] are designed to correct for imagelevel label noise, the point cloud segmentation task requires to correct pointlevel noises. Considering that the point labels within each instance are strongly correlated, directly apply ing these methods to each point independently without con sidering the local correlation is suboptimal. In this paper, we present a novel point noiseadaptive learning (PNAL) framework, which is the Ô¨Årst attempt to empower the point cloud segmentation model with re sistance to annotation noise. SpeciÔ¨Åcally, to cope with unknown, possibly heavy, and varying noise rates, we designed a pointlevel conÔ¨Ådence selection mechanism, which obtains reliable labels based on the historical predic tions of each point without requiring a known noise rate. Next, in order to fully utilize the local correlation amongarXiv:2107.14230v2  [cs.CV]  5 Aug 2021chairbed floorcabinet tabletable desk bookshelf deskInput Scenes RealWorld Noisy GT Labels Predictions of PNAL Figure 1. Illustration of the instancelevel label noise concept in point cloud segmentation. From left to right are the input (noisy instances highlighted red boxes), the manual annotation given by the realworld dataset ScanNetV2, and the prediction of PNAL (more in line with the real category). It is noticeable that this popular dataset suffers from label noise, such as mislabeling the Ô¨Çoor as a chair, even that it is already a relabeled version of ScanNet. Our PNAL framework is trained on this noisy dataset but still achieves correct predictions. labels, we propose a label correction process performed at cluster level . This is done by the proposed voting strat egythat tries to merge reliable labels from relevant points to provide the best possible label for each point cluster, with acomputationally efÔ¨Åcient implementation. To demonstrate the effectiveness of our PNAL, we com pare the proposed framework with various possible base lines based on different network backbones on the syn thetic noisy label dataset from stanford largescale 3d in door spaces (S3DIS) [2], which shows the great advantage of PNAL on both performance and efÔ¨Åciency. On the real world noisy dataset ScannetV2 [6], we notice that both its training and validation set suffers from the noisy label is sue. Therefore, we not only conducted experiments on the original training and validation set. Also, for a more rig orous evaluation, we reÔ¨Åne the validation set by manually correcting the noisy labels and evaluate PNAL on this clean set. These results indicate that PNAL is also robust to real world noise. To further explore PNAL, a complete ablation study, training process analysis, and robustness test were also performed. To summarize, our contributions are fourfold. ‚Ä¢ To the best of our knowledge, this is the Ô¨Årst work in vestigating noisy labels on point cloud data, which has a wide and urgent need for 3D applications where the volume of data is growing drastically. ‚Ä¢ A novel noiserate blind PNAL framework is proposed to handle spatially variant noise rates in point cloud. It consists of pointlevel conÔ¨Ådence selection, cluster level label correction with voting mechanism, and can be easily applied to different network architectures. ‚Ä¢ Extensive experiments are conducted to show the clear improvements by PNAL, on both synthetic and realworld noisy label datasets. ‚Ä¢ We relabeled the validation set of ScanNetV2 by cor recting noises and will make it public to facilitate both point cloud segmentation and noise label learning. 2. Related Work "
543,Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion Classification.txt,"Deep neural networks (DNNs) have achieved great success in a wide variety of
medical image analysis tasks. However, these achievements indispensably rely on
the accurately-annotated datasets. If with the noisy-labeled images, the
training procedure will immediately encounter difficulties, leading to a
suboptimal classifier. This problem is even more crucial in the medical field,
given that the annotation quality requires great expertise. In this paper, we
propose an effective iterative learning framework for noisy-labeled medical
image classification, to combat the lacking of high quality annotated medical
data. Specifically, an online uncertainty sample mining method is proposed to
eliminate the disturbance from noisy-labeled images. Next, we design a sample
re-weighting strategy to preserve the usefulness of correctly-labeled hard
samples. Our proposed method is validated on skin lesion classification task,
and achieved very promising results.","Aiming to improve the performance of Deep Neural Net works (DNNs) on medical image analysis, the community is in the requirement of a huge amount of annotated im age data. Meanwhile, the huge capacity of DNNs makes it easily Ô¨Åt noisy labels. Incorrect in training labels can hurt the performance of DNNs on the test dataset [1]. Medical images‚Äôannotation quality is prone to experience, which re quires years of professional training and domain knowledge. For example, melanoma is the leading death cause of skin cancer, the accuracy of melanoma dermoscopy diagnosis in clinical is 50% to 82.3%; the unreliable image label issue can be very severe. With the high demanding of computeraided diagnosis of melanoma in clinical, it is of signiÔ¨Åcant impact to address the noisy label issue. Despite the label quality problem, DNNs are prone to other training set biases, espe cially class imbalance and hard samples [2]. An example of the typical hard samples in melanoma dermoscopy data is shown in Fig. 1, the appearance of benign and malignant Fig. 1 . Typical example of melanoma dermoscopy with clin ical diagnosis. cases can be vary similar. These hard samples are normally ambiguous and hence brings about extra challenges for iden tifying wronglabelled samples. In this study, we mainly focus on the noisy label issue, as the class imbalance issue can be solved easily during preprocessing or data collection. Although some approaches have been considered to ad dress the noisy label issue, it is still an ongoing challenge in deep learning for medical imaging. Aiming to simulate the relationship between noisy label and the latent clean la bel, Goldberger et al. [3] proposed to add a fully connected layer after softmax, where the updated weight represents the transition matrix between noisy and clean label. Patrini et al. [4] proposed a corrected loss by combining the noise transi tion matrix with traditional softmax cross entropy loss. These methods are heavily dependent on the accurate assumption of noise distribution, which is usually unknown in real practice. From the assumption that clean data will have a smaller loss than noisy data, Jiang et al. [5] proposed Mentornet, which learns small loss samples Ô¨Årst. Tanaka et al. [6] proposed to change the label of training data according to the softmax out put during training. Their methods have treated weak agree ment sample as noise, but the performance of these methods on medical image are degraded because of the hard samples that are usually presented in the medical image. There are also methods that are supervised by an extra group of clean data, such as a label clean network proposed by Veit et al. [7] and an adaptive weight learning method demonstrated by Ren et al. [8]. But these methods still need to maintain a set of expert annotated images. Those methods have demonstrated promising perfor mance in natural images. Not many studies have addressed the medical image noisy label issue. One pioneer work is [9]arXiv:1901.07759v2  [cs.CV]  24 Jan 2019Fig. 2 . The framework of the proposed learning approach.The network is jointly optimized by two types of losses: reweighted softmax loss and online uncertainty sample mining loss. by Dgani et al., they utilized the method of [3] on mammog raphy classiÔ¨Åcation task and outperforms standard training methods, but it is heavily dependent on the noise label distri bution assumption, and the hard samples and minority class in melanoma dataset will obstruct the assumption process. In this paper, we propose an iterative learning strategy with the aim of detecting noisy label in the training data and enhance the performance of the neural network. Notably, it is a tailormade strategy for medical images. SpeciÔ¨Åcally, an online uncertainty sample mining strategy is proposed to suppress the noisy samples, and an individual reweighting module is developed to preserve the hard samples and mi nority class. Extensive ablation studies demonstrate that the two components both contribute to the performance gain. The main contributions of this paper include: 1) An deep learning model based noisy label training strategy is proposed, which can enhance the model performance when the training data contains noisy labels; 2) A novel noisy label training loss is derived, which considers the hard samples as well as noisy labels. 2. METHOD "
544,Autoselection of the Ensemble of Convolutional Neural Networks with Second-Order Cone Programming.txt,"Ensemble techniques are frequently encountered in machine learning and
engineering problems since the method combines different models and produces an
optimal predictive solution. The ensemble concept can be adapted to deep
learning models to provide robustness and reliability. Due to the growth of the
models in deep learning, using ensemble pruning is highly important to deal
with computational complexity. Hence, this study proposes a mathematical model
which prunes the ensemble of Convolutional Neural Networks (CNN) consisting of
different depths and layers that maximizes accuracy and diversity
simultaneously with a sparse second order conic optimization model. The
proposed model is tested on CIFAR-10, CIFAR-100 and MNIST data sets which gives
promising results while reducing the complexity of models, significantly.","Machine learning has made great progress in the last few years due to ad vances in the use of Deep Neural Networks (DNN), but many of the proposed neural architectures come with high computational and memory requirements Blalock et al. (2020). These demands increase the cost of training and deploy ing these architectures, and constrain the spectrum of devices that they can be used on. Although deep learning has been very promising in many areas in terms of technology in recent years, it has some problems that need improvement. Even though deep learning can distinguish changes and subtle dierences in data with interconnected neural networks, it makes it very dicult to dene hyperparameters and determine their values before training the data. For this reason, dierent pruning methods have been proposed in the literature to reduce the parameters of convolutional networks Han et al. (2016); Hanson & Pratt (1988); LeCun & Cortes (2010); Str om (1997). The common problem of deep learning and pruning algorithms that have been studied in recent years is the decision of the pruning percentage with the heuristic approach at the pruning stage, making the success percentage of the deep learning algorithm dependent on the pruning parameter. On the other hand, the optimization models proposed with zeronorm penalty to ensure sparsity ignore the diversity of the layers as they only take into account the percentage of success. The combination of the layers that are close to each other does not increase the percentage of accuracy. As the primal example of DNNs, Convolutional Neural Networks (CNNs) are feedforward architectures originally proposed to perform image processing tasks, Li et al. (2021) but they oered such high versatility and capacity that allowed them to be used in many other tasks including time series prediction, signal identication and natural language processing. Inspired by a biological visual perceptron that displays local receptive elds Goodfellow et al. (2016), a CNN uses learnable kernels to extract the relevant features at each processing 2level. This provides the advantage of local processing of information and weight sharing. Additionally, the use of the pooling mechanism allows the data to be downsampled and the features to be invariant representations of previous features. Recently, a great deal of attention has been paid to obtain and train com pact CNNs with the eects of pruning Wen et al. (2016); Zhou et al. (2016). Generally, pruning techniques dier from each other in the choice of structure, evaluation, scheduling, and netuning Blalock et al. (2020). However, tuning these choices according to its costs and requirements, the network should be pruned. This is a systematic reduction in neural network parameters while aiming to maintain a performance prole comparable to the original architec ture Blalock et al. (2020). Pruning of parameters in neural networks has been used to reduce the complexity of the models and prevent overtting. Dier ent methods on pruning in deep neural networks are proposed in the literature such as; Biased Weight Decay Hanson & Pratt (1988), Optimal Brain Damage LeCun et al. (1989) and Optimal Brain Surgeon Hassibi & Stork (1992). These aim to identify redundant parameters represented by connections within archi tectures and perform pruning by removing such redundancies Srinivas & Babu (2015); Han et al. (2016). In another study, a new layerbased pruning technique was proposed for deep neural networks where the parameters of each layer are pruned independently according to the secondorder derivatives of a layerbased error function with respect to the relevant parameters Dong et al. (2017). Poor estimation performance is observed after pruning is limited to a linear combi nation of reconstructed errors occurring in each layer Dong et al. (2017). Some pruning techniques, such as unstructured pruning, work on individual parame ters that induce sparsity while the memory footprint of the architecture reduces although there is not any speed advantage. Other techniques work on groups of parameters, where a neuron lter layer is considered as a separate target for the algorithm, thus provides the ability to take advantage of existing computational hardware Li et al. (2016). Evaluation of pruned parameters can be based on relevance to training 3metrics, absolute value, activation, and contribution of gradients Blalock et al. (2020). The architecture can be considered and evaluated as a whole Lee et al. (2019); Frankle & Carbin (2019), while parameters can be scored locally with reference to a small group of closely related groups of networks Han et al. (2016). Iterative pruning is performed by considering a subset of the archi tecture Han et al. (2016) and the speed of the pruning algorithm varies in a single step process Liu et al. (2019). Changing the pruning rate according to a certain rule have also been tried and implemented in the literature Gale et al. (2019). After applying the pruning algorithm, some techniques continued to use the same training weights; while the reduced architecture obtained in the studies was retrained to a certain state Frankle & Carbin (2019) or initialized and restarted from the initial state Liu et al. (2019). Deep Ensembling techniques increase reliability of DNNs as multiple diverse models are combined into a stronger model Fort et al. (2019). It was shown that members of the deep ensemble models provide dierent predictions on the same input increasing diversity Garipov et al. (2018) and providing more calibrated probabilities Lakshminarayanan et al. (2017). In one of the recent studies, ensemble technique is chosen to increase the complexity by training end toend two EcientNetb0 models with bagging. Adaptive ensemble technique is used by netuning within a trainable combination layer which outperforms dierent studies for widely known datasets such as CIFAR10 and CIFAR100 Bruno et al. (2022). Since pretrained models boosts eciency while simplifying the hyperparameter tuning, increasing the performance on these datasets are achieved with the help of transfer learning and pretraining Kolesnikov et al. (2020); Sun et al. (2017). Researchers accelerated CNNs but get lower perfor mance metrics on CIFAR100. One of these techniques is FATNET algorithm where high resolution kernels are used for classication while reducing the num ber of trainable parameters based on Fourier transform Ibadulla et al. (2022). Ensemble pruning methods are used to reduce the computational complexity of ensemble models, and remove the duplicate models existing in the ensemble. Finding a small subset of classiers that perform equivalent to a boosted ensem 4ble is an NPhard problem Tamon & Xiang (2000). Search based methods can be used to select an ensemble as an alternative. These methods conduct a search in the ensemble space and evaluate performance of various subsets of classiers Sagi & Rokach (2018). Deep learning can actually be thought as multilayer ar ticial neural networks, which is an example of ensemble learning. The overall percentage of success for ensemble learning is proportional to the percentage of average accuracy of the ensemble and the diversity of each learner within the ensemble. However, the percentage of accuracy and diversity have tradeos among themselves. In other words, an increase in the percentage of accuracy within the community causes a decrease in diversity, which means redundant merge of similar layers. It has been shown that a pruning problem can be re framed to a quadratic integer problem for classication to seek a subset that optimizes accuracydiversity tradeo using a semidenite programming tech nique Zhang et al. (2006). In the vein of that research, multiple optimization models have been proposed to utilize the quadratic nature of pruning problems. Sparse problem minimization and loworder approximation techniques have recently been used in numerous elds such as computer vision, machine learn ing, telecommunications, and more Quach et al. (2017). In such problems, in cluding zeronorm corresponds to sparsity which makes the objective function nonconvex in the optimization problem. For this reason, dierent relaxation techniques have been proposed in the optimization literature for nonconvex ob jective functions. For class of a nonconvex quadratic constrained problem, SDP relaxation techniques have been developed based on the dierence of two con vex functions (Dierence of Convex { DC) decomposition strategy Zheng et al. (2011); OzogurAkyuz et al. (2020). However, since SDP algorithms for high dimensional problems are slow and occupy a lot of memory, SDP algorithms have been relaxed into quadratic conic problems. SecondOrder Cone programming (SOCP) is a problemsolving class that lies between linear (LP) or quadratic programming (QP) and semidenitive programming (SDP). Like LP and SDP, SOCPs can be solved eciently with primal dual interiorpoint methods Potra & Wright (2000). Also, various en 5gineering problems can be formulated as quadratic conic problems Lobo et al. (1998). In the literature, new convex relaxations are suggested for nonconvex quadratically constrained quadratic programming (QCQP) problems related to this issue. Since basic semidenite programming relaxation is often too loose for general QCQP, recent studies have focused on enhancing convex relaxations using valid linear or SOC inequalities. Valid secondorder cone constraints were created for the nonconvex QCQP and the duality dierence was reduced by us ing these valid constraints Jiang & Li (2019). In addition, a branch and bound algorithm has been developed in the literature to solve continuous optimiza tion problems in which a nonconvex objective function is minimized under nonconvex inequality constraints that satisfy certain solvability assumptions Beck & Pan (2017). In this study, we propose a novel optimization model to prune the ensemble of CNNs with a SecondOrder Cone Programming (SOCP) model. The proposed optimization model selects the best subset of models in the ensemble of CNNs by optimizing the accuracydiversity tradeo while reducing the number of models and computational complexity. In the following chapters; denition of the loss function and relaxations for SOCP model will be introduced in Section 2, developing the ensemble of CNNs with hardware, software specications, datasets and results will be given in Section 3 and lastly, the conclusions and future work will be mentioned in Section 4, respectively. Contribution of The Study The contributions of the proposed study are listed as follows; ‚Ä¢One of the unique values that this study will add to the literature is the fact that the parameter called the pruning percentage, which will meet the needs of the deep learning literature, will be directly obtained by the proposed secondorder conical optimization model. ‚Ä¢Another important original value will be a solution to the ignorance of the diversity criterion in the literature for pruning deep learning networks 6only on the basis of accuracy performance. The objective function in the proposed optimization model simultaneously optimizes the accuracy and diversity criteria by pruning the ensemble of CNNs. ‚Ä¢Since SOCP models gives more successful and faster results than the other sparse optimization models. Hence, the number of models in the ensemble of CNNs will be reduced signicantly by using the proposed SOCP model. 2. Methods and Materials "
545,Ensembles of Convolutional Neural Networks models for pediatric pneumonia diagnosis.txt,"Pneumonia is a lung infection that causes 15% of childhood mortality, over
800,000 children under five every year, all over the world. This pathology is
mainly caused by viruses or bacteria. X-rays imaging analysis is one of the
most used methods for pneumonia diagnosis. These clinical images can be
analyzed using machine learning methods such as convolutional neural networks
(CNN), which learn to extract critical features for the classification.
However, the usability of these systems is limited in medicine due to the lack
of interpretability, because of these models cannot be used to generate an
understandable explanation (from a human-based perspective), about how they
have reached those results. Another problem that difficults the impact of this
technology is the limited amount of labeled data in many medicine domains. The
main contributions of this work are two fold: the first one is the design of a
new explainable artificial intelligence (XAI) technique based on combining the
individual heatmaps obtained from each model in the ensemble. This allows to
overcome the explainability and interpretability problems of the CNN ""black
boxes"", highlighting those areas of the image which are more relevant to
generate the classification. The second one is the development of new ensemble
deep learning models to classify chest X-rays that allow highly competitive
results using small datasets for training. We tested our ensemble model using a
small dataset of pediatric X-rays (950 samples) with low quality and anatomical
variability (which represents one of the biggest challenges). We also tested
other strategies such as single CNNs trained from scratch and transfer learning
using CheXNet. Our results show that our ensemble model outperforms these
strategies obtaining highly competitive results. Finally, we confirmed the
robustness of our approach using another pneumonia diagnosis dataset [1].","Pneumonia is the most common infectious disease in humans and the leading cause of childhood morbidity refers to having a disease or a symptom of disease, or to the amount of disease within a population) and mortality in the world [ 2] that causes inÔ¨Çammation of the alveoli [ 3]. It especially affects children under 2 years old and elderly above 65. Globally, 15% of childhood mortality is caused by this disease, over 808,694 children in 2017 [ 4]. This mortality still remains around 800,000 children in 2018 under Ô¨Åve every year, or 2,200 children every day, resulting, as UNICEF shows in [ 5], the dramatic number of one child dead every 39 seconds, which includes over 153,000 newborns. Pneumonia is mainly caused by viruses or bacteria. Most frequent associated viruses are respiratory syncytial virus (RSV), inÔ¨Çuenza virus and human parainÔ¨Çuenza virus (HPIV) [ 6]. Most frequent bacteria are Streptococcus pneumoniae ,Haemophilus inÔ¨Çuenzae , Streptococcus pyogenes andStaphylococcus aureus , and Mycoplasma pneumoniae [6]. The global prevalence of viral pneumonia origin in children is 1462%, being higher in children under two years old and decreasing with age [7, 8]. Children with bacterial pneumonia should receive antibiotics as soon as possible, whereas children with viral pneumonia usually only need supportive care. However, antivirals may have a relevant role in the treatment of viral infections [9]. When in doubt, and given that bacterial pneumonia is more serious, the symptoms are usually resolved by using empirical antibiotic treatment, which in most cases is unnecessary since a virus is the most frequent cause of communityacquired pneumonia (CAP) [10]. Pneumonia diagnosis is fundamentally clinical, without reaching an etiological diagnosis most of the time. Due to the high economic cost, and the time it takes to obtain results (days, or even weeks), pediatricians often diagnose based on: laboratory tests, xrays, and the examination of the patient. Xrays are one of the most important to diagnose CAP [ 11]. Furthermore, it is clear that a high proportion of all pneumonia cases are in fact vialbacterial coinfections, complicating decisions regarding antibiotic administration [ 12]. Therefore, pediatricians have to decide empirically whether the child needs antibiotics, and choose the best treatment with their limited available tools. As a result, most children receive antibiotics. These results in what is considered overtreatment with antibiotics, leading to the need to narrow the indications by an appropriate discriminative diagnosis [ 13]. Proxies for typical bacterial pneumonia have been proposed, but the consensus is that no single biomarker alone is enough for diagnosing bacterial pneumonia [ 14,15]. The old paradigm that bacterial pneumonia is associated with a speciÔ¨Åc radiographic pattern different from the pattern of viral pneumonia is now often criticized, although the radiological pictures of alveolar pneumonia (also termed lobar pneumonia, or pneumonia with consilidation ) appear to be bacterial in most of the cases [ 16]. The interpretation of the chest Xray radiography (CXR) is usually performed following the standards of the ‚ÄúWHO Vaccine Trial Investigators Radiology Working Group‚Äù [ 17]. These standards establish two possible interpretations: ‚Äúconsolidation‚Äù (including consolidation and/or pleural effusion as per WHO standards) and ‚Äúother inÔ¨Åltrates‚Äù. However, the interobserver agreement for these two categories is low [18]. Convolutional Neural Networks (CNNs) are wellknown Deep Learning architectures. Recently great advances have undergone helping to solve several visualrelated tasks [ 19]. This kind of neural networks is inspired by the biological neurons of the visual cortex [ 20], which allows them to solve problems such as image classiÔ¨Åcation [ 21] and object recognition [ 22], among other image and video processing and recognition tasks. CNNs are also successful in other problems such as speech recognition [ 23], malware detection [ 24,25], natural language processing [ 26,27], among many others. These systems process the information in two main steps: feature extractor , where relevant features are detected; and classiÔ¨Åcation , where these features obtained from the previous step are analyzed and different probabilities will be assigned to the detected structures to carry out the classiÔ¨Åcation. In areas such as medicine, where the diagnosis is often based on the analysis of clinical images (e.g. xrays), CNNs and Deep Learning methods have proven both their usefulness and effectiveness in the detection and classiÔ¨Åcation of multiple diseases [ 20,28,29]. The classiÔ¨Åcation performance in computer vision problems, such as those mentioned above, can be improved through ensembles of models [ 30,31]. An ensemble is a kind of method based on the combination of multiple learning algorithms, the 2APREPRINT  JUNE 7, 2021 ensemble is designed to improve the performance of the particular elements that builds up the new algorithm. This technique combine the individual predictions to produce a consensus prediction. Its advantages over individual models are the performance, because the combination of multiples models can improve their individual power and the Ô¨Ånal model can approximate better the optimal solution [ 32]; and robustness, because the ensemble models reduce the variance of prediction errors made but the contributing models by adding bias, avoiding overÔ¨Åtting of the Ô¨Ånal model [33]. For this reason this technique can be critical in small datasets like ours, where the information is particular limited in both size and quality. However, CNNs, like many other Deep Learning and machine learning methods, are considered as "" blackbox "" algorithms, where both the input and output can be easily analysed and interpreted by the users, but where the inference process carried out by the algorithm is opaque hinders endusers‚Äô conÔ¨Ådence in the results obtained, and therefore makes decisionmaking negatively affected. It makes this essential process (‚Äùhow‚Äù and ‚Äùwhy‚Äù the algorithm has obtained this outcome) uninterpretable for the human being [ 34]. This may limit its application in Ô¨Åelds such as medicine, where the practitioners need to know how the algorithm has inferred the output for each speciÔ¨Åc patient [ 35] (e.g. why the algorithm is assigning a 90 % probability for alveolar pneumonia?). This limitation can be overcome using automatic explanatory systems, called explainable AI (XAI) [ 36], which allows us to visualize which areas of the image (features) have been used to obtain the outcome generated by the algorithm, or at least alleviating, the aforementioned problem. These XAIbased systems will generate new images highlighting the areas of highest interest that the system uses to obtain the result (e.g. in our case to predict a particular kind of disease) [37]. The combination of Deep Learning models with medical knowledge allows the development of new clinical decision support systems (CDSS). These automatic systems can help in medical diagnosis reducing some typical clinical problems such as subjectivity in the interpretation of medical tests or human errors (fatigue, distraction, etc.) [38]. This combination of machine learning methods with humanbased knowledge can improve the performance of the diagnosis process, as it was stated in [ 39]. In that project, leaded by Dr. Andrew Beck, it was demonstrated that the combination of pathologists and Deep Learning models provide a signiÔ¨Åcant reduction of the error rate for breast cancer diagnosis. In the initial results pathologists obtained a 3.5% of error during classiÔ¨Åcation of the pathology, whereas the Deep Learning algorithm obtained a slightly better result of 2.9% error. However, when both humans and AI model were combined this error decreased to an impressive 0.5 % (so, the 99.5% of cases were correctly classiÔ¨Åed) [39]. The main contribution of this work can be brieÔ¨Çy summarized as the design and development of a novel Machine Learning system based on ensembles for pneumonia diagnosis in childhood. Our system estimates the probability that the Xray has a consolidation or other inÔ¨Åltrates, which would be a helpful tool for the unclear cases in case of disagreement among professionals, or in case of work overload. The result generated from the system should be userfriendly (from a health professional perspective), to achieve that, the system creates a graphical visualization using an explainable AI technique named heatmap, which highlights the areas of the image which are more relevant for the diagnosis according to the AI system [40]. An interesting point of this work is to understand how the neural network infers the pneumonia type (alveolar versus non alveolar) from the chest Xray. This could help to expedite the treatment of patients who require medication. On the other hand, this could also help to avoid giving antibiotics to patients who do not need them. This is crucial since an incorrect use of antibiotics (that is, using them in patients who do not have a bacterial infection), or an excessive use of broadspectrum antibiotics, can cause antibiotic resistance . This can become a global problem making it more difÔ¨Åcult to treat patients, the only solution being the development of new, more powerful antibiotics [ 13]. Therefore, in order to reduce the overuse of antibiotics in viral pneumonia, a correct diagnosis of bacterial pneumonia is crucial. This article has been structured as follows: Section 2 provides a short description of some relevant works in the area of AIbased detection of lung diseases; Section 3 describes the methodology followed to design and train our CNN models; Section 4 shows the experimental results; Section 5 presents the conclusions and Ô¨Ånally, future work, with some future lines of work. 2 Related work "
546,ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal Self-Ensemble for Active Learning.txt,"Modern deep learning has achieved great success in various fields. However,
it requires the labeling of huge amounts of data, which is expensive and
labor-intensive. Active learning (AL), which identifies the most informative
samples to be labeled, is becoming increasingly important to maximize the
efficiency of the training process. The existing AL methods mostly use only a
single final fixed model for acquiring the samples to be labeled. This strategy
may not be good enough in that the structural uncertainty of a model for given
training data is not considered to acquire the samples. In this study, we
propose a novel acquisition criterion based on temporal self-ensemble generated
by conventional stochastic gradient descent (SGD) optimization. These
self-ensemble models are obtained by capturing the intermediate network weights
obtained through SGD iterations. Our acquisition function relies on a
consistency measure between the student and teacher models. The student models
are given a fixed number of temporal self-ensemble models, and the teacher
model is constructed by averaging the weights of the student models. Using the
proposed acquisition criterion, we present an AL algorithm, namely
student-teacher consistency-based AL (ST-CoNAL). Experiments conducted for
image classification tasks on CIFAR-10, CIFAR-100, Caltech-256, and Tiny
ImageNet datasets demonstrate that the proposed ST-CoNAL achieves significantly
better performance than the existing acquisition methods. Furthermore,
extensive experiments show the robustness and effectiveness of our methods.","Deep neural networks (DNNs) require a large amount of training data to opti mize millions of weights. In particular, for supervisedlearning tasks, labeling of training data by human annotators is expensive and timeconsuming. The label ingcostcanbeamajorconcernformachinelearningapplications,whichrequires a collection of realworld data on a massive scale (e.g., autonomous driving) or ?Corresponding AuthorarXiv:2207.02182v2  [cs.CV]  16 Oct 2022the knowledge of highly trained experts for annotation (e.g., automatic medical diagnosis). Active learning (AL) is a promising machine learning framework that maximizes the eÔ¨Éciency of labeling tasks within a Ô¨Åxed labeling budget [35]. ThisstudyfocusesonthepoolbasedALproblem,wherethedatainstancesto belabeledareselectedfromapoolofunlabeleddata.InapoolbasedALmethod, the decision to label a data instance is based on a sample acquisition function . The acquisition function, a(x;f), takes the input instance xand the currently trained model fand produces a score to decide if xshould be labeled. Till date, various types of AL methods have been proposed [9‚Äì11,20,35,36,38,46,47]. The predictive uncertaintybased methods [9,10,36,46] used wellstudied theo reticmeasuressuchasentropyandmutualinformation.Recently,representation based methods [20,34,38,47] have been widely used as a promising AL approach to exploit highquality representation of DNNs. However, the acquisition used for these methods rely on a single trained model f, failing to account for model uncertainty arising given a limited labeled dataset. To solve this problem, sev eral AL methods [2,35] attempted to utilize an ensemble of DNNs and design acquisition functions based on them. However, these methods require signiÔ¨Åcant computational costs to train multiple networks. Whentheamountoflabeleddataislimited,semisupervisedlearning(SSL)is another promising machine learning approach to improve performance with low labelingcosts.SSLimprovesthemodelperformancebyleveragingalargenumber of unlabeled examples [31]. Consistency regularization is one of the most suc cessful approach to SSL [11,22,30,40]. In a typical semisupervised learning, the modelistrainedusingtheconsistencyregularizedlossfunction Ece+Econ,where Ecedenotes the crossentropy loss and Econdenotes the consistencyregularized loss. Minimization of Econregularizes the model to produce consistent predic tions over the training process, improving the performance for a given task. model [22] applied a random perturbation to the input of the DNN and measured the consistency between the model outputs. Mean Teacher (MT) [40] produced the temporal selfensemble through SGD iterations and measured the consistency between the model being trained and the teacher model obtained by taking the exponential moving average (EMA) of the selfensemble. These meth ods successfully regularized the model to produce consistent predictions while using perturbed predictions on unlabeled samples. The objective of this study is to improve the sample acquisition criterion for poolbased AL methods. Inspired by the consistency regularization for SSL, we build a new sample acquisition criterion that measures consistency between multiple ensemble models obtained during the training phase. The proposed ac quisition function generates the temporal selfensemble by sampling the models at the intermediate checkpoints of the weight trajectory formed by the SGD optimization. This provides a better acquisition performance and eliminates the additionalcomputationalcostrequiredbypreviousensemblebasedALmethods. In [1,17,27], the aforementioned method has shown to produce good and diverse selfensembles, which were used to improve the inference model via stochastic weight averaging (SWA) [1,17,27]. We derive the acquisition criterion based onFig.1:Acquisition criterion : Both labeled and unlabeled samples are repre sented in the feature space in the binary classiÔ¨Åcation problem. Low consistency samples produce the predictions which signiÔ¨Åcantly diÔ¨Äer between the student and teacher models. Low conÔ¨Ådence samples are found near the decision bound aries of the student models. The proposed STCoNAL evaluates the consistency measure for each data sample and selects those with the highest consistency score. the temporal selfensemble models used in SWA. We present the AL method, referred to as studentteacher consistencybased AL (STCoNAL) , which mea sures the consistency between the student and teacher models. The selfensemble model constitutes a student model, and a teacher model is formed by taking an equallyweighted average (EWA) of the parameters of the student models. Treat ing the output of the teacher model as a desired supervisory signal, STCoNAL measures the Kullback‚ÄìLeibler (KL) divergence of each teacherstudent output pairs. The acquisition function of STCoNAL acquires the samples to be labeled that yield the highest inconsistency. Though STCoNAL was inspired by the consistency regularization between student and teacher models of the MT, these two methods are quite diÔ¨Äerent in the following aspects. MT constructs the teacher model by assigning larger weights to more recent model weights obtained through SGD iterations. Due to this constraint, as training progresses, the teacher model in MT tends to be correlated with the student models, making it diÔ¨Écult to measure good enough consistency measure for AL acquisition. To address this problem, STCoNAL generatesabetterteachermodelbytakinganequallyweightedaveraging(EWA) of the weights of the student models instead of EMA used in MT. Similar to previousALmethods[9,10]thatutilizeensemblemodelstocapturetheposterior distribution of model weights for a given data set, the use of student model weights allows our acquisition criterion to account for model uncertainty.We further improve our STCoNAL method by adopting the principle of entropyminimizationusedforSSL[3,4,12,24,39,44].Weapplyoneoftheentropy minimization methods, sharpening to the output of the teacher model. When the sharpened output is used for our KL divergence, our acquisition criterion can measure the uncertainty of the prediction for the given sample. Our evaluation shows that the STCoNAL is superior to other AL methods on various image classiÔ¨Åcation benchmark datasets. Fig.1 illustrates that these low consistency samples lie in the region of the feature space where the student models produce the predictions of a larger vari ation. Note that these samples are not necessarily identical to the low conÔ¨Ådence samples located near the decision boundary speciÔ¨Åed by the teacher model. The proposed STCoNAL prefers the acquisition of the inconsistent samples rather than the lowconÔ¨Ådence samples. The main contributions of our study are summarized as follows: ‚ÄìWe propose a new acquisition criterion based on the temporal selfensemble. Temporal selfensemble models are generated by sampling DNN weights through SGD optimization. STCoNAL measures the consistency between these selfensemble models and acquires the most inconsistent samples for labeling. We evaluated the performance of STCoNAL on four diÔ¨Äerent pub lic datasets for multiclass image classiÔ¨Åcation tasks. We observe that the proposed STCoNAL method achieves the signiÔ¨Åcant performance gains over other AL methods. ‚ÄìWe identiÔ¨Åed a work relevant to ours [11]. While both ours and their work aim to exploit consistency regularization for AL, our work diÔ¨Äers from theirs inthefollowingaspects.WhileCSSAL[11]reliesontheinputperturbationto a single Ô¨Åxedmodel, STCoNAL utilizes the selfensemble models tomeasure the consistency. Note that the beneÔ¨Åts of using model ensembles for AL have been demonstrated in [2] and our Ô¨Åndings about the superior performance of STCoNAL over CSSAL are consistent with the results of these studies. 2 Related Work "
547,FabricNet: A Fiber Recognition Architecture Using Ensemble ConvNets.txt,"Fabric is a planar material composed of textile fibers. Textile fibers are
generated from many natural sources; including plants, animals, minerals, and
even, it can be synthetic. A particular fabric may contain different types of
fibers that pass through a complex production process. Fiber identification is
usually carried out through chemical tests and microscopic tests. However,
these testing processes are complicated as well as time-consuming. We propose
FabricNet, a pioneering approach for the image-based textile fiber recognition
system, which may have a revolutionary impact from individual to the industrial
fiber recognition process. The FabricNet can recognize a large scale of fibers
by only utilizing a surface image of fabric. The recognition system is
constructed using a distinct category of class-based ensemble convolutional
neural network (CNN) architecture. The experiment is conducted on recognizing
50 different types of textile fibers. This experiment includes a significantly
large number of unique textile fibers than previous research endeavors to the
best of our knowledge. We experiment with popular CNN architectures that
include Inception, ResNet, VGG, MobileNet, DenseNet, and Xception. Finally, the
experimental results demonstrate that FabricNet outperforms the
state-of-the-art popular CNN architectures by reaching an accuracy of 84% and
F1-score of 90%.","Textile Ô¨Åbers are the components that are used to construct fabrics. Commonly, the types of Ô¨Åbers are split into two categories: natural Ô¨Åbers and synthetic Ô¨Åbers. Natural Ô¨Åbers are extracted from environmental sources, whereas synthetic Ô¨Åbers are manufactured through machinery and chemical compounds. Such instances of natural Ô¨Åbers are silk, wool, cotton, etc. whereas, nylon, polyester, rayons, etc. are the example of synthetic Ô¨Åbers. Raw Ô¨Åbers are used toarXiv:2101.05564v1  [cs.CV]  14 Jan 2021FabricNet: A Fiber Recognition Architecture Using Ensemble ConvNets A.Q. O HI ET AL . Figure 1: Different yarn cross sectional shapes of particular Ô¨Åbers: Tenasco (a,b,i), Nylon (e,g,j,l), Viscose (f,k) and Terylene. The image is adopted from the work of Hearle et al. [4]. assemble yarns. A single yarn is assembled using one or more types of raw Ô¨Åbers. The yarns are further utilized to construct fabrics and particular garments. Fiber recognition is the process of identifying raw Ô¨Åbers from fabrics, and it is widely used in different industrial applications. It is a wellapplied method in fabric reverse engineering [ 1]. Garment identiÔ¨Åcation is also possible using Ô¨Åber recognition systems since each garment type mostly requires Ô¨Åxed sets of Ô¨Åber elements [ 2]. The Ô¨Åber recognition system can also be implemented as fabric fault detection and garment inquiry systems [3]. Identifying raw textile Ô¨Åbers is considered difÔ¨Åcult due to the complicated weave structure and aging of Ô¨Åbers [5]. Moreover, present fabrics pass through complex printing procedures that may alter the yarn structure. Classical biological methods, such as soaking, cleaning, heating, etc. are considered less effective in raw textile Ô¨Åber identiÔ¨Åcation. However, microscopic observations are proven to be more accurate in identifying raw textile Ô¨Åbers. The textile Ô¨Åber recognition from manufactured fabrics is complicated since a single yarn (used to construct the fabric) can contain multiple textile Ô¨Åbers. In some cases, fabrics are mostly preprocessed. Moreover, microscopic observations may lead to false recognition, as numerous Ô¨Åbers can be used to generate a single textile yarn. Generally, most systems identify textile Ô¨Åbers through microscopic crosssection images [ 6] and spectroscopic features [7,8]. Fibers can be distinguished by microscopic crosssection images due to their unique geometrical properties [9]. Figure 1 illustrates different crosssection shapes of different types of yarns. Extricating crosssectional shots is nearly impossible for industrial usage of textile Ô¨Åber identiÔ¨Åcation systems, as it requires a careful preprocessing and microscopes. Using crosssectional images for recognizing textile Ô¨Åbers from fabrics is a critical approach in realtime industrial aspects. Hence, the crosssectional investigation requires laboratories and is timeconsuming. On the contrary, spectroscopybased methods can be used for industrial purposes, but it is limited to recognizing only a single textile Ô¨Åber from a fabric. Further, the method is not suitable for individual usage. 2FabricNet: A Fiber Recognition Architecture Using Ensemble ConvNets A.Q. O HI ET AL . Figure 2: The dataset contains fabric images in different light and orientations [ 2]. The Ô¨Årst row illustrates fabric made of artiÔ¨Åcial leather. The second row illustrates the fabric made of silk. The third row contains images of fabric which is made of polyester and viscose (rayon). The research endeavor‚Äôs sole purpose is to overcome the information gathering complexities of the Ô¨Åber recognition procedure. Smartphones and high deÔ¨Ånition cameras have made image capturing one of the most superÔ¨Åcial attempts for information gathering procedures. Therefore, we introduce a novel architecture that can recognize the textile Ô¨Åbers by a fabric surface image. Because of the availability of cameras, our proposed textile Ô¨Åber recognition process can be performed by individuals, and even by automated machines in a much more convenient and Ô¨Çexible way. Figure 2 represents some image samples that are used to conduct the training of our FabricNet architecture. The overall architecture performs CNN based image processing using an ensemble architecture. Since our proposed model recognizes textile Ô¨Åbers through fabric surface image, it can be widely applied for diverse industrial and individual applications for fault checking and authentication. Our process can further be used for textile fraud prevention and fault detection. The overall contribution of the research endeavor includes the following: ‚Ä¢We exploit the surface image of fabric in recognizing textile Ô¨Åbers, as it is one of the easiest ways for image collection. ‚Ä¢We outline different categories of ensemble methods, and introduce a classbased ensemble architecture that receives downsampled image data through a head CNN architecture. In classbased ensemble architecture, every single ensemble memorizes only one class. Therefore, the accuracy of FabricNet architecture increases. ‚Ä¢We experiment with seventeen different implementations of famous image classiÔ¨Åcation architectures, includ ing Inception, ResNet, VGG, MobileNet, DenseNet, Xception, and CUNet. Through the result analysis, we afÔ¨Årm that FabricNet architecture provides better accuracy. The remainder of this paper is outlined as follows. Section 2 demonstrates the procedures that are modeled to identify textile Ô¨Åbers. Section 3 presents the motivation and the architectural fundamentals of the FabricNet. Section 4 contains the experimental results that are performed to evaluate FabricNet architecture. Finally, Section 5 concludes the paper. 2 Related Works "
548,Exemplar Normalization for Learning Deep Representation.txt,"Normalization techniques are important in different advanced neural networks
and different tasks. This work investigates a novel dynamic
learning-to-normalize (L2N) problem by proposing Exemplar Normalization (EN),
which is able to learn different normalization methods for different
convolutional layers and image samples of a deep network. EN significantly
improves flexibility of the recently proposed switchable normalization (SN),
which solves a static L2N problem by linearly combining several normalizers in
each normalization layer (the combination is the same for all samples). Instead
of directly employing a multi-layer perceptron (MLP) to learn data-dependent
parameters as conditional batch normalization (cBN) did, the internal
architecture of EN is carefully designed to stabilize its optimization, leading
to many appealing benefits. (1) EN enables different convolutional layers,
image samples, categories, benchmarks, and tasks to use different normalization
methods, shedding light on analyzing them in a holistic view. (2) EN is
effective for various network architectures and tasks. (3) It could replace any
normalization layers in a deep network and still produce stable model training.
Extensive experiments demonstrate the effectiveness of EN in a wide spectrum of
tasks including image recognition, noisy label learning, and semantic
segmentation. For example, by replacing BN in the ordinary ResNet50,
improvement produced by EN is 300% more than that of SN on both ImageNet and
the noisy WebVision dataset.","Normalization techniques are one of the most essential components to improve performance and accelerate train ing of convolutional neural networks (CNNs). Recently, a family of normalization methods is proposed includ ing batch normalization (BN) [14], instance normalization Equal contribution (a) The learning dynamic of EN ratios of four categories in three layers. (b) Performance of EN and its counterparts on various CV tasks. Figure 1. (a) The proposed Exemplar Normalization (EN) enables different categories to learn to select different normalizers in dif ferent layers. The four categories of ImageNet ( i.e. Ô¨Çute, bald ea gle, newfoundland and castle) in three layers ( i.e. bottom, middle and top) of ResNet50 are presented. (b) EN outperforms its coun terparts on various computer vision tasks ( i.e. image classiÔ¨Åcation, noisysupervised classiÔ¨Åcation and semantic image segmentation ) by using different network architectures. Zoom in three times for the best view. (IN) [37], layer normalization (LN) [1] and group normal ization (GN) [40]. As these methods were designed for dif ferent tasks, they often normalize feature maps of CNNs from different dimensions. To combine advantages of the above methods, switch able normalization (SN) [23] and its variant [34] were pro posed to learn linear combination of normalizers for each convolutional layer in an endtoend manner. We term this normalization setting as static ‚Äòlearningtonormalize‚Äô. De spite the successes of these methods, once a CNN is op timized by using them, it employed the same combinationarXiv:2003.08761v2  [cs.CV]  20 Mar 2020ratios of the normalization methods for all image samples in a dataset, incapable to adapt to different instances and thus rendering suboptimal performance. As shown in Fig. 1, this work studies a new learn ing problem, that is, dynamic ‚Äòlearningtonormalize‚Äô, by proposing Exemplar Normalization (EN), which is able to learn arbitrary normalizer for different convolutional layers, image samples, categories, datasets, and tasks in an end toend way. Unlike previous conditional batch normaliza tion (cBN) that used multilayer perceptron (MLP) to learn datadependent parameters in a normalization layer, suffer ing from overÔ¨Åtting easily, the internal architecture of EN is carefully designed to learn datadependent normalization with merely a few parameters, thus stabilizing training and improving generalization capacity of CNNs. EN has several appealing beneÔ¨Åts. (1) It can be treated as an explanation tool for CNNs. The exemplarbased important ratios in each EN layer provide information to analyze the properties of different samples, classes, and datasets in various tasks. As shown in Fig. 1(a), by train ing ResNet50 [9] on ImageNet [6], images from different categories would select different normalizers in the same EN layer, leading to superior performance compared to the ordinary network. (2) EN makes versatile design of the normalization layer possible, as EN is suitable for vari ous benchmarks and tasks. Compared with stateofthe art counterparts in Fig. 1(b), EN consistently outperforms them on many benchmarks such as ImageNet [6] for im age classiÔ¨Åcation, Webvision [18] for noisy label learning, ADE20K [43] and Cityscapes [5] for semantic segmenta tion. (3) EN is a plug and play module. It can be in serted into various CNN architectures such as ResNet [9], Inception v2 [36], and ShufÔ¨ÇeNet v2 [26], to replace any normalization layer therein and boost their performance. The contributions of this work are threefold. (1) We present a novel normalization learning setting named dynamic ‚Äòlearningtonormalize‚Äô, by proposing Exemplar Normalization (EN), which learns to select different nor malizers in different normalization layers for different im age samples. EN is able to normalize image sample in both training and testing stage. (2) EN provides a Ô¨Çexible way to analyze the selected normalizers in different layers, the re lationship among distinct samples and their deep represen tations. (3) As a new building block, we apply EN to vari ous tasks and network architectures. Extensive experiments show that EN outperforms its counterparts in wide spectrum of benchmarks and tasks. For example, by replacing BN in the ordinary ResNet50 [9], improvement produced by EN is300% more than that of SN on both ImageNet [6] and the noisy WebVision [18] dataset.2. Related Work "
549,Student Beats the Teacher: Deep Neural Networks for Lateral Ventricles Segmentation in Brain MR.txt,"Ventricular volume and its progression are known to be linked to several
brain diseases such as dementia and schizophrenia. Therefore accurate
measurement of ventricle volume is vital for longitudinal studies on these
disorders, making automated ventricle segmentation algorithms desirable. In the
past few years, deep neural networks have shown to outperform the classical
models in many imaging domains. However, the success of deep networks is
dependent on manually labeled data sets, which are expensive to acquire
especially for higher dimensional data in the medical domain. In this work, we
show that deep neural networks can be trained on much-cheaper-to-acquire
pseudo-labels (e.g., generated by other automated less accurate methods) and
still produce more accurate segmentations compared to the quality of the
labels. To show this, we use noisy segmentation labels generated by a
conventional region growing algorithm to train a deep network for lateral
ventricle segmentation. Then on a large manually annotated test set, we show
that the network significantly outperforms the conventional region growing
algorithm which was used to produce the training labels for the network. Our
experiments report a Dice Similarity Coefficient (DSC) of $0.874$ for the
trained network compared to $0.754$ for the conventional region growing
algorithm ($p < 0.001$).","Lateral ventricles are anatomical parts of the ventricular system in the brain, where the cerebrospinal  uid is produced. Ventricular volume and its progression are associated with several brain diseases. In certain forms of dementia, the increase of lateral ventricular volume has been associated to decline in cognitive function.1 Some psychiatric illnesses such as schizophrenia have also been linked to enlargement in ventricular volume.2 Additionally, asymmetrical shapes between the left and the right lateral ventricles together with the size of the ventricles can be indicative of abnormalities in the brain.3 Even though a rough estimation of the ventricular volume such as the number of slices that the ventricles appear in, might be sucient for some applications, more accurate quantitative measurements are necessary to longitudinally study subtle dierences. It has also been shown that leveraging spatial information using ventricles as landmarks are benecial for the detection of a number of pathologies in the brain including white matter hyperintensities4and lacunes.5Though manual annotation of lateral ventricles might be an option on Send correspondence to Jonas Teuwen: jonas.teuwen@radboudumc.nl.arXiv:1801.05040v2  [cs.CV]  3 Mar 2018smaller datasets and crosssectional studies, this would not be feasible otherwise as the task is timeconsuming, laborious and subjective. Therefore an accurate, objective and independent segmentation of the left and right ventricles is desirable in clinical practice. With the success of deep neural networks6,7in visual pattern recognition, many studies have been successfully conducted in the medical image analysis domain during the past few years,8,9that have resulted in intelligent systems that reach or surpass the level of medical experts on dierent tasks and domains.10{12 Since the recent deep learning approaches follow a datadriven strategy to learn the optimal representations for the specic tasks at hand, these methods often require large sets of annotated data to train on. Several recent studies have shown strong implications of training dataset size on the quality of trained networks. For instance, it has been shown that even with gigantic datasets, the performance of the trained network linearly scales with logarithm of the size of the training data.13 Given the reasoning above, the computer vision community has created enormous labeled datasets using crowd sourcing methods, for instance using Amazon mechanical turk. However this solution is not feasible for medical datasets, as the labeling process requires specic expertise that is only possible with medical experts available. Therefore, the high costs of gathering large medical datasets have still hindered feasibility of gigantic datasets that fully leverage the high capacity of the deep neural networks on various medical image analysis domains. Another strategy to provide large labeled datasets is to use (not necessarily very accurate) available methods for the task in order to provide pseudolabels. Using this, one can provide arbitrarily large datasets as far as unlabeled data is available. This however, arises a few interesting questions to be answered: 1) Considering an imposed tradeo between the dataset size and its relative label accuracy, would that make sense to train neural networks with noisy but large datasets rather than smaller ones with more accurate labels, and 2) In case we opt for the latter, is the low accuracy of the provided pseudolabels necessarily an upperbound for the accuracy of a trained network? In this study, we aim to answer the aforementioned rather important questions by reporting a deep neural network that achieves high accuracy in segmenting the left and right ventricles separately, being trained on noisy pseudolabels. We also show that, though desirable, accurate manual labels are not mandatory to produce good results, given a large set of (unbiased) noisylabeled images. 2. METHODS "
550,Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding.txt,"We investigate the usage of convolutional neural networks (CNNs) for the slot
filling task in spoken language understanding. We propose a novel CNN
architecture for sequence labeling which takes into account the previous
context words with preserved order information and pays special attention to
the current word with its surrounding context. Moreover, it combines the
information from the past and the future words for classification. Our proposed
CNN architecture outperforms even the previously best ensembling recurrent
neural network model and achieves state-of-the-art results with an F1-score of
95.61% on the ATIS benchmark dataset without using any additional linguistic
knowledge and resources.","The slot Ô¨Ålling task in spoken language understanding (SLU) is to assign a semantic concept to each word in a sentence. In the sentence I want to Ô¨Çy from Munich to Rome , an SLU system should tag Munich as the departure city of a trip and Rome as the arrival city. All the other words, which do not correspond to real slots, are then tagged with an artiÔ¨Åcial class O. Tradi tional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random Ô¨Åelds (CRF) [2, 3]. More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8]. Overall, RNNs outperformed other NN models and achieved the stateoftheart results on the ATIS benchmark dataset [9]. Furthermore, bidirectional RNNs have worked best so far showing that information from both the past and the future is important in predicting the semantic label of the current word. It is, however, well known that it is difÔ¨Åcult to train an RNN due to the vanishing gradient problem [10]. Introducing long short term memory (LSTM) [11] or other variants of LSTM such as the gated recurrent unit (GRU) can solve this problem but, in turn increases the number of parameters signiÔ¨Åcantly. Previous results reported in [8] did not show any improvement on the ATIS data set using LSTM or GRU. In contrast to previous papers which reported stateofthe art results with RNNs, we explore the usage of convolutional neural networks for a sequence labeling task like slot Ô¨Ålling. Previous research in [6] showed promising results on the slot Ô¨Ålling task. The motivation behind this is to allow the model to search for patterns in order to predict the label of the cur rent word independent of the feature representation of the pre vious word. Moreover, CNNs provide several advantages: itpreserves the word order information, it is faster and easier to train and does not mix up the word sequence and therefore it is able to interpret the features learnt for the current task to some extent. This study investigates the usage of CNNs for a sequential labeling task like slot Ô¨Ålling with the following contributions: (1) We propose a novel CNN architecture for sequence la beling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context. (2) We extend the proposed CNN model to a bidirectional sequential CNN (bisCNN) which combines the information from past and future words for prediction. (3) We compare the impact of two different ranking objec tive functions on the recognition performance and analyze the most important ngrams for semantic slot Ô¨Ålling. (4) On the ATIS benchmark dataset, the proposed bi directional sequential CNN outperforms all RNN related mod els and deÔ¨Ånes a new startoftheart F1score of 95.61%. 2. Related Work "
551,Cross-dataset Person Re-Identification Using Similarity Preserved Generative Adversarial Networks.txt,"Person re-identification (Re-ID) aims to match the image frames which contain
the same person in the surveillance videos. Most of the Re-ID algorithms
conduct supervised training in some small labeled datasets, so directly
deploying these trained models to the real-world large camera networks may lead
to a poor performance due to underfitting. The significant difference between
the source training dataset and the target testing dataset makes it challenging
to incrementally optimize the model. To address this challenge, we propose a
novel solution by transforming the unlabeled images in the target domain to fit
the original classifier by using our proposed similarity preserved generative
adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the
generative adversarial networks with the cycle consistency constraint to
transform the unlabeled images in the target domain to the style of the source
domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is
measured by a siamese deep convolutional neural network, to preserve the
similarity of the transformed images of the same person. Comprehensive
experiments based on multiple real surveillance datasets are conducted, and the
results show that our algorithm is better than the state-of-the-art
cross-dataset unsupervised person Re-ID algorithms.","As one of the most important and challenging problems in the Ô¨Åeld of surveillance video analysis, person reidentiÔ¨Åcation (ReID) aims to match the image frames which contain the same person in the surveillance videos. How to extract the view invariant features from the images and design a robust visual classiÔ¨Åer to identify the persons is the core challenge of the ReID algorithms. Due to the privacy problem regarding the collection of surveillance videos and the expensive cost of data labeling, most of the proposed ReID algorithms [16] [12] con duct supervised learning on small labeled datasets. Directly deploying these trained models to the realworld largescale camera networks may lead to a poor performance, ?The work described in this paper was supported by the grants from NSFC (No. U1611461), Science and Technology Program of Guangdong Province, China (No. 2016A010101012), and CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, 100190, Beijing, China.(No.CASNDST201703).arXiv:1806.04533v2  [cs.CV]  21 Jun 20182 J. Lv et al. Market1501  GRID  CUHK03   Fig. 1: Samples in different datasets. because the images captured from different camera networks usually have totally differ ent backgrounds, noise distributions, brightness and resolution as shown in Fig.1. How to incrementally optimize the ReID algorithms based on the abundant unlabeled data collected from the target domain is a practical and extremely challenging problem. To address this problem, some unsupervised algorithms [7] [10] are proposed to extract view invariant features and measure the similarity of pedestrians based on the unlabeled dataset. Without the powerful supervision based on labels, this kind of pure unsupervised learning based algorithms working on one single dataset have a poor per formance in most cases. Recently, a crossdataset unsupervised transfer learning algo rithm, named UMDL [11] , is proposed to make use of data samples from both labeled source datasets and the unlabeled target dataset to learn the viewinvariant feature rep resentation and similarity measurement by the dictionary learning mechanism. UMDL gains much better performance than purely unsupervised algorithms, but is still much weaker than the stateoftheart supervised algorithms based on the labeled dataset. Most of above algorithms try to incrementally optimize the visual classiÔ¨Åer, which is pretrained in the source dataset, to Ô¨Åt the new data in the target domain. However, with out the labels in the target domain, it is hard to Ô¨Ånetune the classiÔ¨Åer to suit the source and target datasets simultaneously, which have diverse feature distributions. We address this crossdataset ReID challenge in a totally new direction in this pa per. Instead of optimizing the classiÔ¨Åer to Ô¨Åt the new data, we transform the unlabeled images in the target domain to Ô¨Åt the classiÔ¨Åer by using our proposed similarity pre served generative adversarial networks model, SimPGAN. As shown in Fig.5 regarding an example from the real datsets, after the transformation, the features of the images in the target datasets are projected into the features close to the distribution in the source dataset, and are easier to be processed by the visual classiÔ¨Åer trained in the source dataset. The main contributions of this paper are summarized as follows: ‚ÄìWe propose a novel efÔ¨Åcient solution, named SimPGAN, to improve the perfor mance of crossdataset person ReID by transforming the unlabeled images in theCrossdataset Person ReIdentiÔ¨Åcation 3 target domain into the style of the source domain to Ô¨Åt the visual classiÔ¨Åer trans ferred from the source domain. ‚ÄìSimPGAN adopts the generative adversarial networks with cycle consistency con straint to avoid sharp change of the images after transformation. Meanwhile, SimP GAN uses the similarity consistency loss, which is measured by a siamese deep convolutional neural network, to preserve the similarity of the transformed images of the same person. ‚ÄìWe conduct comprehensive experiments based on real datasets (Market1501[17], CUHK01[14] , GRID[1]), which show that SimPGAN is better than the stateof theart crossdataset unsupervised transfer learning algorithm[11] . The rest of this paper is organized as follows. Section 2 reviews the related work of ReID. Section 3 offers clear deÔ¨Ånitions of the problem about ReID in unlabeled dataset. Section 4 presents our proposed methods. Section 5 evaluates the performance of this system by conducting experiments on real datasets. We conclude the work in Section 6. 2 Related work "
552,Learning with Noisy Labels for Sentence-level Sentiment Classification.txt,"Deep neural networks (DNNs) can fit (or even over-fit) the training data very
well. If a DNN model is trained using data with noisy labels and tested on data
with clean labels, the model may perform poorly. This paper studies the problem
of learning with noisy labels for sentence-level sentiment classification. We
propose a novel DNN model called NetAb (as shorthand for convolutional neural
Networks with Ab-networks) to handle noisy labels during training. NetAb
consists of two convolutional neural networks, one with a noise transition
layer for dealing with the input noisy labels and the other for predicting
'clean' labels. We train the two networks using their respective loss functions
in a mutual reinforcement manner. Experimental results demonstrate the
effectiveness of the proposed model.","It is well known that sentiment annotation or la beling is subjective ( Liu,2012 ). Annotators of ten have many disagreements. This is especially so for crowdworkers who are not well trained. That is why one always feels that there are many errors in an annotated dataset. In this paper, we study whether it is possible to build accurate sen timent classiÔ¨Åers even with noisylabeled training data. Sentiment classiÔ¨Åcation aims to classify a piece of text according to the polarity of the senti ment expressed in the text, e.g., positive ornega tive(Pang and Lee ,2008 ;Liu,2012 ;Zhang et al. , 2018 ). In this work, we focus on sentencelevel sentiment classiÔ¨Åcation (SSC) with labeling er rors. As we will see in the experiment section, noisy labels in the training data can be highly damag ing, especially for DNNs because they easily Ô¨Åt the training data and memorize their labels even ‚àóCorresponding authorwhen training data are corrupted with noisy labels (Zhang et al. ,2017 ). Collecting datasets annotated with clean labels is costly and timeconsuming as DNN based models usually require a large num ber of training examples. Researchers and practi tioners typically have to resort to crowdsourcing. However, as mentioned above, the crowdsourced annotations can be quite noisy. Research on learning with noisy labels dates back to 1980s ( Angluin and Laird ,1988 ). It is still vibrant today ( Mnih and Hinton ,2012 ; Natarajan et al. ,2013 ,2018 ;Menon et al. ,2015 ; Gao et al. ,2016 ;Liu and Tao ,2016 ;Khetan et al. , 2018 ;Zhan et al. ,2019 ) as it is highly challeng ing. We will discuss the related work in the next section. This paper studies the problem of learning with noisy labels for SSC. Formally, we study the fol lowing problem. Problem DeÔ¨Ånition : Given noisy labeled train ing sentences S={(x1,y1),...,(xn,yn)}, where xi|n i=1is theith sentence and yi‚àà {1,...,c}is the sentiment label of this sentence, the noisy labeled sentences are used to train a DNN model for a SSC task. The trained model is then used to classify sentences with clean labels to one of the csenti ment labels. In this paper, we propose a convolutional neural NETwork with ABnetworks (N ETAB) to deal with noisy labels during training, as shown in Figure 1. We will introduce the details in the subsequent sections. Basically, N ETABconsists of two convolutional neural networks (CNNs) (see Figure 1), one for learning sentiment scores to predict ‚Äòclean‚Äô1labels and the other for learning a noise transition matrix to handle input noisy labels. We call the two CNNs Anetwork and 1Here we use clean with single quotes as it is not com pletely clean. In practice, models can hardly produce com pletely clean labels.ABnetwork, respectively. The fundamental here is that (1) DNNs memorize easy instances Ô¨Årst and gradually adapt to hard instances as training epochs increase ( Zhang et al. ,2017 ; Arpit et al. ,2017 ); and (2) noisy labels are the oretically Ô¨Çipped from the clean/true labels by a noise transition matrix ( Sukhbaatar et al. ,2015 ; Goldberger and BenReuven ,2017 ;Han et al. , 2018a ,b). We motivate and propose a CNN model with a transition layer to estimate the noise transition matrix for the input noisy labels, while exploiting another CNN to predict ‚Äòclean‚Äô labels for the input training (and test) sentences. In training, we pretrain Anetwork in early epochs and then train A Bnetwork and Anetwork with their own loss functions in an alternating manner. To our knowledge, this is the Ô¨Årst work that ad dresses the noisy label problem in sentencelevel sentiment analysis. Our experimental results show that the proposed model outperforms the stateoftheart methods. 2 Related Work "
553,Multiple Expert Brainstorming for Domain Adaptive Person Re-identification.txt,"Often the best performing deep neural models are ensembles of multiple
base-level networks, nevertheless, ensemble learning with respect to domain
adaptive person re-ID remains unexplored. In this paper, we propose a multiple
expert brainstorming network (MEB-Net) for domain adaptive person re-ID,
opening up a promising direction about model ensemble problem under
unsupervised conditions. MEB-Net adopts a mutual learning strategy, where
multiple networks with different architectures are pre-trained within a source
domain as expert models equipped with specific features and knowledge, while
the adaptation is then accomplished through brainstorming (mutual learning)
among expert models. MEB-Net accommodates the heterogeneity of experts learned
with different architectures and enhances discrimination capability of the
adapted re-ID model, by introducing a regularization scheme about authority of
experts. Extensive experiments on large-scale datasets (Market-1501 and
DukeMTMC-reID) demonstrate the superior performance of MEB-Net over the
state-of-the-arts.","Person reidentication (reID) aims to match persons in an image gallery col lected from nonoverlapping camera networks [40], [14], [16]. It has attracted increasing interest from the computer vision community thanks to its wide ap plications in security and surveillance. Though supervised reID methods have achieved very decent results, they often experience catastrophic performance drops while applied to new domains. Domain adaptive person reID that can well generalize across domains remains an open research challenge. ?Corresponding author.arXiv:2007.01546v3  [cs.CV]  13 Jul 20202 Y. Zhai et al. Unsupervised domain adaptation (UDA) in reID has been studied exten sively in recent years. Most existing works can be broadly grouped into three categories. The rst category attempts to align feature distributions between source and target domains [35], [39], aiming to minimize the interdomain gap for optimal adaptation. The second category addresses the domain gap by employ ing generative adversarial networks (GAN) for converting sample images from a source domain to a target domain while preserving the person identity as much as possible [22], [5], [36], [24]. To leverage the target sample distribution, the third category adopts selfsupervised learning and clustering to predict pseudo labels of targetdomain samples iteratively to netune reID models [43], [15], [7], [37], [30], [8]. Nevertheless, the optimal performance is often achieved by en semble that integrates multiple subnetworks and their discrimination capability. However, ensemble learning in domain adaptive reID remains unexplored. How to leverage specic features and knowledge of multiple networks and optimally adapt them to an unlabelled target domain remains to be elaborated. In this paper, we present an multiple expert brainstorming network (MEB Net), which learns and adapts multiple networks with dierent architectures for optimal reID in an unlabelled target domain. MEBNet conducts iterative training where clustering for pseudolabels and models feature learning are al ternately executed. For feature learning, MEBNet adopts a mutual learning strategy where networks with dierent architectures are pretrained in a source domain as expert models equipped with specic features and knowledge. The adaptation is accomplished through brainstormingbased mutual learning among multiple expert models. To accommodate the heterogeneity of experts learned with dierent architectures, a regularization scheme is introduced to modulate the experts' authority according to their feature distributions in the target do main, and further enhances the discrimination capability of the reID model. The contributions of this paper are summarized as follows. {We propose a novel multiple expert brainstorming network (MEBNet) based on mutual learning among expert models, each of which is equipped with knowledge of an architecture. {We design an authority regularization to accommodate the heterogeneity of experts learned with dierent architectures, modulating the authority of experts and enhance the discrimination capability of reID models. {Our MEBNet approach achieves signicant performance gain over the state oftheart on commonly used datasets: Market1501 and DukeMTMCreID. 2 Related Works "
554,Meta-Learning via Feature-Label Memory Network.txt,"Deep learning typically requires training a very capable architecture using
large datasets. However, many important learning problems demand an ability to
draw valid inferences from small size datasets, and such problems pose a
particular challenge for deep learning. In this regard, various researches on
""meta-learning"" are being actively conducted. Recent work has suggested a
Memory Augmented Neural Network (MANN) for meta-learning. MANN is an
implementation of a Neural Turing Machine (NTM) with the ability to rapidly
assimilate new data in its memory, and use this data to make accurate
predictions. In models such as MANN, the input data samples and their
appropriate labels from previous step are bound together in the same memory
locations. This often leads to memory interference when performing a task as
these models have to retrieve a feature of an input from a certain memory
location and read only the label information bound to that location. In this
paper, we tried to address this issue by presenting a more robust MANN. We
revisited the idea of meta-learning and proposed a new memory augmented neural
network by explicitly splitting the external memory into feature and label
memories. The feature memory is used to store the features of input data
samples and the label memory stores their labels. Hence, when predicting the
label of a given input, our model uses its feature memory unit as a reference
to extract the stored feature of the input, and based on that feature, it
retrieves the label information of the input from the label memory unit. In
order for the network to function in this framework, a new memory-writingmodule
to encode label information into the label memory in accordance with the
meta-learning task structure is designed. Here, we demonstrate that our model
outperforms MANN by a large margin in supervised one-shot classification tasks
using Omniglot and MNIST datasets.","Deep learning is heavily dependent on big data. Traditional gradient based neural networks require extensive and itera tive training using large datasets. In these models, training occurs through a continuous update of weight parameters in order to optimize the loss function during training. However, when there is only a little data to learn from, deep learning is prone to poor performance because traditional networks will not acquire enough knowledge about the speciÔ¨Åc taskvia weight updates, and hence, they fail to make accurate predictions when tested. Previous works have approached the task of learning from few samples using different methods such as prob abilistic models based on Bayesian learning (FeiFei, Fer gus, and Perona 2006), generative models using probability density functions (Lake et al. 2011; Rezende et al. 2016), Siamese neural networks (Koch 2015), and metalearning based memory augmented models (Santoro et al. 2016; Vinyals et al. 2016). In this work, we revisited the problem of metalearning using memory augmented neural networks. Metalearning is a twotiered learning framework in which an agent learns not only about the speciÔ¨Åc task, for instance, image clas siÔ¨Åcation, but also about how the task structure varies across target domains (Christophe, Ricardo, and Pavel 2004; Santoro et al. 2016). Neural architectures with an external memory such as Neural Turing Machines (NTMs) (Graves, Wayne, and Danihelka 2014) and memory networks (We ston, Chopra, and Bordes 2014) have shown the ability of metalearning. Recent memory augmented neural networks for meta learning such as MANN (Santoro et al. 2016) use a plain memory matrix as an external memory. In these models, in put data samples and their labels are bound together in the same memory locations.In models such as the MANN, the input data samples and their appropriate labels from previ ous step are bound together in the same memory locations. This often leads to memory interference when performing a task as they have to retrieve a feature of an input from a certain memory location and read only the label information bound to that location. Our primary contribution in this work is designing a different version of NTM (Graves, Wayne, and Danihelka 2014) by splitting the external memory into feature and label memories to avoid any catastrophic interference. The fea ture memory is used to store input data features and the la bel memory is used to encode the label information of the inputs. Therefore, during testing, ideal performance in our model requires using the feature memory as a reference to accurately retrieve the stored feature of an input image and effectively reading the corresponding label information from the label memory. In order to accomplish this, we designed a new memory writing module based on the metalearningarXiv:1710.07110v1  [cs.LG]  19 Oct 2017  Shuffle:   Labels   Classes   Samples    ... ...  (x1,0) (x2,y1) (xt,yt1) (ùë•2,ùë¶1)   (ùë•1,0)   (ùë•ùë°,ùë¶ùë°‚àí1)   (ùë•ùë°+1,ùë¶ùë°)   Class Prediction    (a) Task setup                    ...  ùë•ùë°:  ùë¶ùë°‚àí1: 4 ùë•ùë°+1:  ùë¶ùë°: 2  ùë•ùë°+2:  ùë¶ùë°+1: 3  2       3  ùë•ùë°+ùëõ:  ùë¶ùë°+ùëõ‚àí1: 5    2  3 2 Feature   Memory    Label   Memory    Feature   Memory    Label   Memory     Backpropagation    2  2  (b) Encoding and Retrieving Figure 1: Metalearning task structure. (a) Omniglot images , xt, are presented along with labels in a temporally offset manner. At time step t, the network sees an input image xtwith a label yt 1from the previous time step. Labels are also shufÔ¨Çed from episode to episode. This prevents the model from learning sampleclass bindings via weight updates, instead it learns to regulate input and output information using its two memories. (b) Here is how the model works. When the network sees an input image for the Ô¨Årst time at a certain time step, it stores a particular feature of the input in the feature memory. When the appropriate label is presented at the next time step, the network stores the label information of the input in the label memory. Then, sampleclass bindings will be formed between the two memories. When the network is given the same class of image in later time step, the network retrieves the input feature from the feature memory and uses the retrieved information to read the corresponding label memory for prediction. task structure that monitors the way in which information is written into the label memory. 2 Related Work "
555,GrabQC: Graph based Query Contextualization for automated ICD coding.txt,"Automated medical coding is a process of codifying clinical notes to
appropriate diagnosis and procedure codes automatically from the standard
taxonomies such as ICD (International Classification of Diseases) and CPT
(Current Procedure Terminology). The manual coding process involves the
identification of entities from the clinical notes followed by querying a
commercial or non-commercial medical codes Information Retrieval (IR) system
that follows the Centre for Medicare and Medicaid Services (CMS) guidelines. We
propose to automate this manual process by automatically constructing a query
for the IR system using the entities auto-extracted from the clinical notes. We
propose \textbf{GrabQC}, a \textbf{Gra}ph \textbf{b}ased \textbf{Q}uery
\textbf{C}ontextualization method that automatically extracts queries from the
clinical text, contextualizes the queries using a Graph Neural Network (GNN)
model and obtains the ICD Codes using an external IR system. We also propose a
method for labelling the dataset for training the model. We perform experiments
on two datasets of clinical text in three different setups to assert the
effectiveness of our approach. The experimental results show that our proposed
method is better than the compared baselines in all three settings.","Automated medical coding is a research direction of great interest to the health care industry [3,22], especially in the Revenue Cycle Management (RCM) space, as a solution to the traditional humanpowered coding limitations. Medical cod ing is a process of codifying diagnoses, conditions, symptoms, procedures, tech niques, the equipment described in a medical chart or a clinical note of a patient. The codifying process involves mapping the medical concepts in context to one or more accurate codes from the standard taxonomies such as ICD (Interna tional Classication of Diseases) and CPT (Current Procedure Terminology). The ICD taxonomy is a hierarchy of diagnostic codes maintained by the World Health Organisation. Medical coders are trained professionals who study a med ical chart and assign appropriate codes based on their interpretation. The mostarXiv:2207.06802v1  [cs.LG]  14 Jul 20222 J. Chelladurai et al. Chief Complaint: Abdominal P ain Stated Complaint: Right Flank P ain since this AM Time Seen b y Provider: XXXXXX 22:09 Source: patient and RN notes reviewed Mode of arriv al: Ambulatory Limitations: no limitations History of Present Illness HPI narr ative: Patient presents for ev aluation of an abdominal pain with occasional r adiation to the back that began this morning while patient was engaged in light activit y.  The pain is intermittent and tends to be worse with movement. The patient reports pain in the right lower quadr ant  The patient also has had roughly 3 episodes of nonbloody diarrhea per da y over the past 2 days.  P atient denies associated fev er, cold symptoms, v omiting, r ashes, chest pain or dyspnea.  P ast medical history is pertinent for anxiet y, OCD , gener al phobia and insomnia. Clinical Impression: Nonspecific abdominal pain Patient Disposition: Home, Self CareHistory ( C0019664 ) right ( C0441994  ) lower quadrant ( C1631280) pain ( C0030193 ) intermittent ( C0205267 ) worse ( C0332271 ) movement ( C0026649 ) patient ( C0030705 ) episodes ( C0332189 ) nonbloody diarrhea ( C0151594 ) day ( C0439228 ) days ( C0439228 ) Patient ( C0030705 ) medical history ( C1704706 ) anxiety ( C0003467 ) OCD ( C0009595 ) general phobia ( C0349231 ) insomnia ( C1963237 ) Query  ExtractionNonspecific abdominal painNamed Entity Recognition and  Entity Linking Extracted QueryExtracted Entities Clinical TextContextualized Graph External Knowledge Graph rightlower quadrant pain Abdominal Pain episodesworsenon bloody diarrhea movement Nonspecific abdominal pain Right lower quadrant  R10.31Contextualized Query Predicted ICD CodeContextual  Graph  GenerationRelevant  Node  Detection IR SystemContextualized Graph  with Relevant Nodes Selectedrightlower quadrant pain Abdominal Pain episodesworsenon bloody diarrhea movementGrabQC Fig. 1: The gure describes the overall pipeline of our proposed method. signicant drawbacks in manual coding are its TurnAround Time (TAT), typi cally 2448 hours, and the inability to scale to large volumes of data. Automatic medical coding addresses both problems by applying AI and Natural Language Understanding (NLU) by mimicking and automating the manual coding process. The problem of automating the assignment of ICD codes to clinical notes is challenging due to several factors, such as the lack of consistent document struc ture, variability in physicians' writing style, choice of vocabularies to represent a medical concept, nonexplicit narratives, typographic and OCR conversion errors. Several approaches to solve these problems are available in the litera ture such as methods based on Deep Learning [22,2,18,14,23], Knowledge Bases [19] and Extractive Text Summarisation [5]. The most recent works treat this problem as a multilabel classication problem and solve them using various deep learning model architectures. Although approaches based on deep learn ing greatly reduce the manual labor required in the feature engineering process, there are certain challenges in applying these to the medical coding task: {Lack of explainability {Requirement of large amounts of labelled data {Large label space (ICD9 14,000 codes, ICD10 72,000 codes) There have been several attempts to address these challenges, for e.g., using an attention mechanism [14,10,21] , transfer learning [23] and extreme classica tion [1,23]. However, much more needs to be done to develop truly satisfactory deployable systems. In this work, we focus on the question of operating with large label space. We study how a medical coder arrives at the nal ICD codes for a given clinical note or medical chart. We observe that the medical coders study entities from dierent subsections of a document, such as Chief Complaint, Procedure, Impression, Diagnosis, Findings, etc., to construct evidence for every ICD code. We also observe that the medical coders use several commercial and noncommercial Information Retrieval (IR) tools such as Optum EncoderPro , AAPC Codify and open tools such as CDC's ICDCM10 Browser Tool for assoGrabQC: Graph based Query Contextualization for automated ICD coding 3 ciating the entities to relevant ICD codes. We propose a solution for automated ICD coding by automatically constructing a contextually enhanced text query containing entities from the clinical notes along with an existing information retrieval system. Fig. 1 depicts our method, which extracts an accurate entities based query, with which the relevant ICD codes could be fetched by querying an ICD IR system. Our method provides explainability to the retrieved codes in terms of the contextual entities, usually lacking in the endtoend DL based solutions. We propose GrabQC, a Graphbased Query contextualization method, along side an existing Information Retrieval system to automatically assign ICD codes to clinical notes and medical records. The overall architecture of our proposed method, which consists of four essential modules, is shown in Fig. 1. The rst module (Section 3.1) extracts all the data elements (entities) from the clinical notes along with their respective types such as condition, body part, symptom, drug, technique, procedure, etc. The second module (Section 3.2) extracts the primary diagnosis available typically under the Chief Complaint section of a medical chart. The third module (Section 3.3) constructs a graph from the enti ties enriched by a preconstructed external Knowledge Base. The fourth module (Section 3.4) prunes the constructed graph based on relevance to the clinical note concepts. We then construct the contextualized query for the integrated IR system to fetch the relevant ICD codes. The main contributions in this work are as follows: {GrabQC , a Graphbased Query Contextualization Module to extract and generate contextually enriched queries from clinical notes to query an IR system. {A Graph Neural Network (GNN) model to lter relevant nodes in a graph to contextualize the concepts. {A distant supervised method to generate labelled dataset for training the GNN model for relevant node detection. The rest of this paper is organised as follows. In Section 2, we provide a summary of the other approaches in the literature that solve the problem of ICD coding. We describe the proposed GrabQC module in Section 3. We present our experimental setup and the results in Section 4 and 5 respectively. We nally give concluding remarks and possible future directions of our work in Section 6. 2 Related Work "
556,Weakly supervised CRNN system for sound event detection with large-scale unlabeled in-domain data.txt,"Sound event detection (SED) is typically posed as a supervised learning
problem requiring training data with strong temporal labels of sound events.
However, the production of datasets with strong labels normally requires
unaffordable labor cost. It limits the practical application of supervised SED
methods. The recent advances in SED approaches focuses on detecting sound
events by taking advantages of weakly labeled or unlabeled training data. In
this paper, we propose a joint framework to solve the SED task using
large-scale unlabeled in-domain data. In particular, a state-of-the-art general
audio tagging model is first employed to predict weak labels for unlabeled
data. On the other hand, a weakly supervised architecture based on the
convolutional recurrent neural network (CRNN) is developed to solve the strong
annotations of sound events with the aid of the unlabeled data with predicted
labels. It is found that the SED performance generally increases as more
unlabeled data is added into the training. To address the noisy label problem
of unlabeled data, an ensemble strategy is applied to increase the system
robustness. The proposed system is evaluated on the SED dataset of DCASE 2018
challenge. It reaches a F1-score of 21.0%, resulting in an improvement of 10%
over the baseline system.","Great attention has been paid to developing advanced  approaches to understand the sounds of everyday life in the  contexts of practical applications such as smart cars [1],  surveillance  [2, 3], healthcare [4]. Sound event  detection  (SED) has been studied to automatically  achieve the strong  temporal annotations for the occurrences of sound events in  an audio recording . As the rapid development in recen t  years, deep learning methods have become the main  approaches to solve the SED task, especially in the IEEE  audio event and scene detection challenges (DCASE) [1, 5]. Ideally strongly labeled audio data is preferable for SED  model development in terms of supervised learning manner  [6, 7]. However, the cost of producing strongly labeled  dataset is remarkably high for the data intensive de ep learning methods. Therefore, it is desirable to develop  weakly supervised SED methods t hat can exploit weakly  labeled data and  unlabeled data effectively. As the release of  Google AudioSet [8] that consists of approximately 2  million weakly labeled short audio clips, the progress of  weakly supervised learning SED approaches has been  significantly accelerated.   1.1 Sound event detection by weakly supervised l earning   Apart from supervised learning, the weakly supervised SED  approaches can be normally grouped into three domains:   (1) The methods developed under inexact supervision   [9] to predict strong annotations of sound events using  weakly labeled training data. In terms of using the weak  labels for SED, there are mainly four ways. One way is to  directly assign the clip level labels (weak labels) to all the  frame level segments (strong labels) to train the model [10,  11], which may introduce noise to the frame level labels.  Another way is to do the source separation at first and  obtain the frame level labels [12, 13] based on the separated  sources. The third way is to use the a ttention mechanisms  [14, 15] to learn the relationship  between frame level labels  and clips level labels in training process. The fourth way is  to formulate the weak label SED job as a multiple instance  learning (MIL) problem [1618], where the audio clips are  treated as bags and audio segments are treated as instances.   (2) The methods implemented as incomplete  supervision  [9] to make use of unlabeled training data to  increase performance. The semi supervised lear ning is the  major technique for this purpose [1921]. Moreover, virtual  adverserial training (VAT) can  also be used to process un  labelled data [20, 22]. Mean teacher algorithm can improve  the performance of semi supervised SED system by using  unlabeled data [23]. The application of ensemble  mechanisms is a good strategy to make the semi supervised  learning more reliable [24].    1 The code  in https://github.com/Cocoxili/DCASE2018Task2/    (3) The methods used to deal with the noisy labels of  training data in an inaccurate supervision  [9] manner. To  address the noisy label problem in SED, a practical idea is  to identify the mislabeled samples and make some  corrections. The majority voting strategy and sample re  weight  strategy [25] employed in the ensemble methods are  widely used techniques. Some effort is also made to  implement an iteratively fine tuning framework by means of  selfverifying the training observations [26].  1.2 Our contributions   In this paper, we aim to develop a scalable system based on  a CRNN framework using th e well developed neural  networks for weakly supervised SED. The main  contributions of our work can be summarized as: (1) In  order to make use of unlabe led training data, we use our  audio tagging system  (NUDT system)  ranked as the top in  the DCASE 2018 chal lenge [27] to contribute on predicting   the weak labels for unlabeled data more effectively. (2) We  explore to integrate well developed CNN architectures such  as ResNet  [28] and Xception  [29] into the C RNN  framework for more  effective feature extract ion. The SED  performance significantly  benefit s from finetuning these  pretrained CNN models . (3) To address influence of us ing  unlabeled  training  data on SED results, we make a  comparative study by respectively adding unlabeled data  with different confidence levels  into the training. (4) In  order to tackle the noisy label problem caused by  using   unlabeled training data, we app ly a model ensemble  technique to increase the robustness of proposed system .  Our code can be  referred to  https://github.com/Blank  Wang/DCASE2018 Task4 .    2. METHODOLOGY   "
557,Differencing based Self-supervised pretraining for Scene Change Detection.txt,"Scene change detection (SCD), a crucial perception task, identifies changes
by comparing scenes captured at different times. SCD is challenging due to
noisy changes in illumination, seasonal variations, and perspective differences
across a pair of views. Deep neural network based solutions require a large
quantity of annotated data which is tedious and expensive to obtain. On the
other hand, transfer learning from large datasets induces domain shift. To
address these challenges, we propose a novel \textit{Differencing
self-supervised pretraining (DSP)} method that uses feature differencing to
learn discriminatory representations corresponding to the changed regions while
simultaneously tackling the noisy changes by enforcing temporal invariance
across views. Our experimental results on SCD datasets demonstrate the
effectiveness of our method, specifically to differences in camera viewpoints
and lighting conditions. Compared against the self-supervised Barlow Twins and
the standard ImageNet pretraining that uses more than a million additional
labeled images, DSP can surpass it without using any additional data. Our
results also demonstrate the robustness of DSP to natural corruptions,
distribution shift, and learning under limited labeled data.","Scene change detection (SCD) is a critical perception task that helps to identify changes in a scene captured at different times. In recent years, SCD has been gaining popularity in the Ô¨Åeld of computer vision, robotics, and remote sensing (Hamaguchi et al., 2019; Sakurada et al., 2020; Alcantarilla et al., 2018) as its various real world applications such as ecosystem monitoring, urban expansion, remote surveillance, autonomous driving, and damage assessment have an immensely positive impact on society. For instance, in autonomous driving and robotics applications, the problem of generating and maintaining maps of everchanging environments is of utmost importance for dynamic localization, and robust operation of vehicles/robots in urban landscapes. SCD helps to alleviate the problem of mapping and efÔ¨Åcient maintenance by continuously monitoring the changes of the scene at different time instances (Alcantarilla et al., 2018). Thus, it plays an important role in many real world applications by perceiving the changes occurring in the environment. Generally, in SCD, the changed region is smaller than the unchanged region with uncertainty in change location and direction. Moreover, the changed region that needs to be detected depends on the nature of the application and is classiÔ¨Åed into semantic changes (relevant) and noisy changes (irrelevant). The structural changes caused by the appearance or disappearance of objects present in a scene are considered as semantic changes, while the changes induced by the radiometric (illumination, shadows, seasonal changes) and geometric variations (viewpoint differences caused by camera rotation) are considered as noisy changes (Alcantarilla et al., 2018; Sakurada & Okatani, 2015; Guo et al., 2018). A critical challenge in SCD is that these noisy changes are entangled with the semantic changes that alter the appearance of an image, thus degrading the change detection performance (Guo et al., 2018). Previous studies based on deep neural networks have proposed to extract multilevel feature representations from the input images to improve the performance of SCD against noisy changes (Guo et al., 2018; Alcantarilla et al., 2018; Varghese et al., 2018; Lei et al., 2020). However, the success of these stateoftheart methods hinges on a large quantity of annotated data. For instance, on average, it takes around 20 minutes and 156 minutes to annotate a single pair of images in the panoramic change detection (PCD) (Sakurada et al., 2013) and panoramic semantic change detection (PSCD) (Sakurada et al., 2020) dataset, respectively. Therefore, largescale labeled datasets for SCD are still scarce, and expensive to obtain (Shi et al., 2020). To address the dependency on labeled data, various SCD approaches initially pretrain their models on largescale datasets such as ImageNet (Deng et al., 2009) in a supervised Equal advising.1We open source our code at https://github.com/NeurAILab/DSP . 1arXiv:2208.05838v1  [cs.CV]  11 Aug 2022Published at 1st Conference on Lifelong Learning Agents, 2022 manner and then Ô¨Ånetune with a small quantity of pixellevel annotations on domainspeciÔ¨Åc dataset (Guo et al., 2018; Sakurada et al., 2020; Chen et al., 2021). However, there still exists the problem of (1) domain shift as the distribution of the ImageNet data widely differs from that of SCD datasets, (2) nature of feature representation learned by transfer learning models from classiÔ¨Åcation tasks is suboptimal for SCD. These problems lead to the degradation of change detection performance in SCD methods. 1% 10% 50% 100% Percentage of Labeled Data (%)01020304050607080F1score Rand Init SupIm Barlow Twins DSP Figure 1: Performance comparisons of supervised and the proposed selfsupervised pretraining (DSP) on VLCMU CD dataset under limited label scenario.To attenuate the reliance of SCD models on a large amount of dense pixellevel annotations and trans fer learning from largescale labeled outofdistribution data, we propose a novel selfsupervised pretraining ap proach that utilizes unlabeled data to learn taskspeciÔ¨Åc representations for the downstream task of SCD. Our method, DSP, uses feature differencing to learn discrim inatory representations corresponding to the changed re gions that are beneÔ¨Åcial for the downstream task of SCD. Furthermore, we propose invariant prediction (IP) ob jective and change consistency regularization (CR) , to gether referred to as temporal consistency (TC) loss, to reduce the effects of differences in the lighting con ditions or camera viewpoints by enhancing the image alignment between the temporal images in the decision and feature space, respectively. With extensive experi ments, we show that our proposed approach achieves re markable performance compared to ImageNet pretrain ing under limited labels scenario as seen in Figure 1. To the best of our knowledge, this is the Ô¨Årst work on SCD that relaxes the requirement of largescale annotated datasets and the need to pretrain on additional largescale labeled data in a computationally efÔ¨Åcient way. Our contribution can be summarized as follows: ‚Ä¢ We propose a differencing based selfsupervised pretraining (DSP) method that learns change representations (taskspeciÔ¨Åc) relevant for scene change detection. ‚Ä¢ We propose an invariant prediction (IP) objective and change consistency regularization (CR) to mitigate the effect of noisy changes across a pair of views. ‚Ä¢ We evaluated the proposed methods on two challenging SCD datasets. DSP surpasses the widely used Ima geNet pretraining without any additional data. Also, DSP pretraining enhances the SCD performance com pared to the standard Barlow Twins (Zbontar et al., 2021) method. ‚Ä¢ Current scene change detection models are vulnerable to severe performance impairments on images with natural corruptions, and the proposed selfsupervised pretraining signiÔ¨Åcantly enhances the robustness of the model to natural corruptions. ‚Ä¢ The effectiveness of the proposed selfsupervised pretraining under limited labels and generalization to out ofdistribution data is veriÔ¨Åed. 2 R ELATED WORK "
558,Dimensionality-Driven Learning with Noisy Labels.txt,"Datasets with significant proportions of noisy (incorrect) class labels
present challenges for training accurate Deep Neural Networks (DNNs). We
propose a new perspective for understanding DNN generalization for such
datasets, by investigating the dimensionality of the deep representation
subspace of training samples. We show that from a dimensionality perspective,
DNNs exhibit quite distinctive learning styles when trained with clean labels
versus when trained with a proportion of noisy labels. Based on this finding,
we develop a new dimensionality-driven learning strategy, which monitors the
dimensionality of subspaces during training and adapts the loss function
accordingly. We empirically demonstrate that our approach is highly tolerant to
significant proportions of noisy labels, and can effectively learn
low-dimensional local subspaces that capture the data distribution.","Deep Neural Networks (DNNs) have demonstrated excellent performance in solving many complex problems, and have been widely employed for tasks such as speech recognition (Hinton et al., 2012), computer vision (He et al., 2016) and gaming agents (Silver et al., 2016). DNNs are capable of learning very complex functions, and can generalize well even for a huge number of parameters (Neyshabur et al., 2014). However, recent studies have shown that DNNs may generalize poorly for datasets which contain a high propor tion noisy (incorrect) class labels (Zhang et al., 2017). It is important to gain a fuller understanding of this phenomenon, with a view to development of new training methods that can *Equal contribution1The University of Melbourne, Mel bourne, Australia2Tsinghua University, Beijing, China3National Institute of Informatics, Tokyo, Japan. Correspondence to: Yisen Wang <wangys14@mails.tsinghua.edu.cn >, Xingjun Ma <xingjun.ma@unimelb.edu.au >. Proceedings of the 35thInternational Conference on Machine Learning , Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).achieve good generalization performance in the presence of variable amounts of label noise. One simple approach for noisy labels is to ask a domain expert to relabel or remove suspect samples in a preprocess ing stage. However, this is infeasible for large datasets and also runs the risk of removing crucial samples. An alterna tive is to correct noisy labels to their true labels via a clean label inference step (Vahdat, 2017; Veit et al., 2017; Jiang et al., 2017; Li et al., 2017). Such methods often assume the availability of a supplementary labelled dataset contain ing preidentiÔ¨Åed noisy labels which are used to develop a model of the label noise. However, their effectiveness is tied to the assumption that the data follow the noise model. A different approach to tackle noisy labels is to utilize cor rection methods such as loss correction (Patrini et al., 2017; Ghosh et al., 2017), label correction (Reed et al., 2014), or additional linear correction layers (Sukhbaatar & Fergus, 2014; Goldberger & BenReuven, 2017). In this paper, we Ô¨Årst investigate the dimensionality of the deep representation subspaces learned by a DNN and pro vide a dimensionalitydriven explanation of DNN general ization behavior in the presence of (class) label noise. Our analysis employs a dimensionality measure called Local In trinsic Dimensionality (LID) (Houle, 2013; 2017a), applied to the deep representation subspaces of training examples. We show that DNNs follow twostage of learning in this scenario: 1) an early stage of dimensionality compression , that models lowdimensional subspaces that closely match the underlying data distribution, and 2) a later stage of di mensionality expansion , that steadily increases subspace dimensionality in order to overÔ¨Åt noisy labels. This second stage appears to be a key factor behind the poor general ization performance of DNNs for noisy labels. Based on this Ô¨Ånding, we propose a new training strategy, termed DimensionalityDriven Learning , that avoids the dimen sionality expansion stage of learning by adapting the loss function. Our main contributions are: We show that from a dimensionality perspective, DNNs exhibit distinctive learning styles with clean labels ver sus noisy labels. We show that the local intrinsic dimensionality canarXiv:1806.02612v2  [cs.CV]  31 Jul 2018DimensionalityDriven Learning with Noisy Labels be used to identify the stage shift from dimensionality compression to dimensionality expansion. We propose a DimensionalityDriven Learning strategy (D2L) that modiÔ¨Åes the loss function once the turning point between the two stages of dimensionality com pression and expansion is recognized, in an effort to prevent overÔ¨Åtting. We empirically demonstrate on MNIST, SVHN, CIFAR10 and CIFAR100 datasets that our DimensionalityDriven Learning strategy can ef fectively learn (1) lowdimensional representation subspaces that capture the underlying data distribution, (2) simpler hypotheses, and (3) highquality deep representations. 2. Related Work "
559,RH-Net: Improving Neural Relation Extraction via Reinforcement Learning and Hierarchical Relational Searching.txt,"Distant supervision (DS) aims to generate large-scale heuristic labeling
corpus, which is widely used for neural relation extraction currently. However,
it heavily suffers from noisy labeling and long-tail distributions problem.
Many advanced approaches usually separately address two problems, which ignore
their mutual interactions. In this paper, we propose a novel framework named
RH-Net, which utilizes Reinforcement learning and Hierarchical relational
searching module to improve relation extraction. We leverage reinforcement
learning to instruct the model to select high-quality instances. We then
propose the hierarchical relational searching module to share the semantics
from correlative instances between data-rich and data-poor classes. During the
iterative process, the two modules keep interacting to alleviate the noisy and
long-tail problem simultaneously. Extensive experiments on widely used NYT data
set clearly show that our method significant improvements over state-of-the-art
baselines.","Relation extraction (RE) is a preliminary task in natural lan guage processing (NLP), which aims to capture the relation between two target entities. Recently, RE based on conven tional supervised learning has made a great success. How ever, it heavily relies on human annotations. In order to obtain largescale training corpus, distant su pervision relation extraction (DSRE) [Mintz et al. , 2009 ]was proposed to generate heuristic labeling data by aligning en tity pairs in raw text. It assumes that if two target entities have a semantic relation in KG, all the raw text containing the two entities can be labeled as this relation class. How ever, this solution makes an overstrong assumption and in evitably brings in massive wrong labeling data. For exam ple, as shown in Figure 1, given a fact ( Obama ,born in , US.) from existing KG, DS will regard all sentences with two linked entities Obama andUS.express the relation born in . In consequence, only the Ô¨Årst sentence is correct, but actually the second expresses the relation president of while the third cannot Ô¨Ånd the predeÔ¨Åned relation. Additionally, according (Obama, born in, US.) Aligned Sentences Real Label In 1961, [Obama]h [was born in]r Hawaii, [US.]t born in [Barack Obama]h takes the Oath of Office as the 44th  [president of]r the [United State]tpresident of Last night, [Obama]h gave a radical speech at  McCormick Place, Chicago, [US.]tunknown ... ...Figure 1: The example of sentence alignment from fact ( Obama , born in ,US.) by distance supervision. It shows that only the Ô¨Årst is correct labeling data and others are noise. to a series works [Liet al. , 2020; Xu and Barbosa, 2019; Hanet al. , 2018b; Zhang et al. , 2019 ], DS always suffers from longtail distribution problem. We analyze that there are two main reasons: 1) existing knowledge bases are far from com pletion and they contain the overlapping problem, 2) the num ber of noisy labeling sentences in some of the relation labels is larger than correct data, which causes the semantics or data insufÔ¨Åcient. Inevitably, the Ô¨Årst factor relies on the quality of KG, which is Ô¨Åxed before alignment with plain text. There fore, we only devote to Ô¨Ånd the target solution corresponding to the second factor. By intuition, if there‚Äôre a lot of noisy sentences under a relation class, fewer highquality sentences can be sufÔ¨Åciently used to train the model, which results in the longtail. In other words, To improve the relation extrac tion, the noisy labeling and longtail problem should be con sidered simultaneously. Recently, most approaches have been presented to solve the noisy labeling problem [Hoffmann et al., 2011; Zeng et al. , 2015; Jat et al. , 2018; Ji et al. , 2017; Feng et al. , 2018; Qin et al. , 2018b; Qin et al. , 2018a; Zeng et al. , 2018 ]and longtail problems [Vashishth et al. , 2018; Liet al. , 2020; Xu and Barbosa, 2019; Han et al. , 2018b; Zhang et al. , 2019 ]. Despite the success and popularity of these methods, little works handle both two problems simul taneously, which ignore the mutual interactions. In this paper, in order to jointly solve two problems, we propose a novel framework named RHNet, which incor porates Reinforcement learning and Hierarchical relational searching module. At Ô¨Årst, we leverage reinforcement learnarXiv:2010.14255v2  [cs.CL]  2 Feb 2021ing to select highquality data. Concretely, given an original bag1, the agent splits its into the correct set and noisy set, and we train the downstream module only on the correct set. This idea is motivated by the previous work [Feng et al. , 2018 ], but the difference is that we enhance the agent by integrating pre trained implicit relation information. For the second problem, we regard that the semantics of datarich can be shared with similar datapoor relations. For example, the datarich rela tion /people/person/place ofbirth in NYT corpus can rep resent a fourlayers tree, from top to down are root,/peo ple,/people/person and /people/person/place ofbirth , re spectively, where root is virtual node, /people and /peo ple/person are subrelations. When given a datapoor rela tionpeople/person/religion , it can be integrated with related instances at the layer of root,/people , and /people/person . In contrast to [Han et al. , 2018b ]and[Zhang et al. , 2019 ], we view RE as a tree search task from the root to the leaf node. During the search processing, we leverage the gating mecha nism to save and combine the semantics of related instances at the current node, and calculate the score of each candidate child nodes and choose the maximum one. The two main components joint training at each iterative stage to capture the interactions. The contributions of this paper are as follows: ‚Ä¢ We are the Ô¨Årst to transform the relation extraction into a tree search task. We propose the hierarchical relational searching strategy to share the correlated instance se mantics at each node. ‚Ä¢ We propose a novel framework RHNet, which is ca pable of simultaneously solving the noisy labeling and longtail problem. At the iterative training stage, our method takes advantage of the mutual interactions be tween them. ‚Ä¢ Extensive experiments on the NYT data set demonstrate that if we consider both two problems, the proposed method outperforms stateoftheart baselines. 2 Related Work "
560,CNN: Single-label to Multi-label.txt,"Convolutional Neural Network (CNN) has demonstrated promising performance in
single-label image classification tasks. However, how CNN best copes with
multi-label images still remains an open problem, mainly due to the complex
underlying object layouts and insufficient multi-label training images. In this
work, we propose a flexible deep CNN infrastructure, called
Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment
hypotheses are taken as the inputs, then a shared CNN is connected with each
hypothesis, and finally the CNN output results from different hypotheses are
aggregated with max pooling to produce the ultimate multi-label predictions.
Some unique characteristics of this flexible deep CNN infrastructure include:
1) no ground truth bounding box information is required for training; 2) the
whole HCP infrastructure is robust to possibly noisy and/or redundant
hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may
be well pre-trained with a large-scale single-label image dataset, e.g.
ImageNet; and 5) it may naturally output multi-label prediction results.
Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasets
well demonstrate the superiority of the proposed HCP infrastructure over other
state-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3%
after the fusion with our complementary result in [47] based on hand-crafted
features on the VOC2012 dataset, which significantly outperforms the
state-of-the-arts with a large margin of more than 7%.","SINGLE label image classiÔ¨Åcation, which aims to assign a label from a predeÔ¨Åned set to an image, has been extensively studied during the past few years [14], [18], [10]. For image representation and classiÔ¨Åcation, conventional approaches utilize care fully designed handcrafted features, e.g., SIFT [32], along with the bagofwords coding scheme, followed by the feature pooling [25], [44], [37] and classic classiÔ¨Åers, such as Support Vector Machine (SVM) [4] and random forests [2]. Recently, in contrast to the handcrafted features, learnt image features with deep network structures have shown their great potential in various vision recognition tasks [26], [21], [24], [36]. Among these architectures, one of the great est breakthroughs in image classiÔ¨Åcation is the deep convolutional neural network (CNN) [24], which has achieved the stateoftheart performance (with 10% gain over the previous methods based on hand crafted features) in the largescale singlelabel object recognition task, i.e., ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [10] with more than one million images from 1,000 object categories. Multilabel image classiÔ¨Åcation is however a more general and practical problem, since the majority of Yunchao Wei is with Department of Electrical and Computer Engineering, National University of Singapore, and also with the Institute of Informa tion Science, Beijing Jiaotong University, email: wychao1987@gmail.com. Yao Zhao is with the Institute of Information Science, Beijing Jiaotong University, Beijing 100044, China. Bingbing Ni is with the Advanced Digital Sciences Center, Singapore. Wei Xia, Junshi Huang, Jian Dong and Shuicheng Yan are with De partment of Electrical and Computer Engineering, National University of Singapore.realworld images are with more than one objects of different categories. Many methods [37], [6], [12] have been proposed to address this more challenging problem. The success of CNN on singlelabel image classiÔ¨Åcation also sheds some light on the multi label image classiÔ¨Åcation problem. However, the CNN model cannot be trivially extended to cope with the multilabel image classiÔ¨Åcation problem in an inter pretable manner, mainly due to the following reasons. Firstly, the implicit assumption that foreground ob jects are roughly aligned, which is usually true for singlelabel images, does not always hold for multi label images. Such alignment facilitates the design of the convolution and pooling infrastructure of CNN for singlelabel image classiÔ¨Åcation. However, for a typical multilabel image, different categories of ob jects are located at various positions with different scales and poses. For example, as shown in Figure 1, for singlelabel images, the foreground objects are roughly aligned, while for multilabel images, even with the same label, i.e., horse and person , the spa tial arrangements of the horse and person instances vary largely among different images. Secondly, the interaction between different objects in multilabel images, like partial visibility and occlusion, also poses a great challenge. Therefore, directly applying the original CNN structure for multilabel image classiÔ¨Å cation is not feasible. Thirdly, due to the tremendous parameters to be learned for CNN, a large number of training images are required for the model training. Furthermore, from singlelabel to multilabel (with n category labels) image classiÔ¨Åcation, the label space has been expanded from nto2n, thus more trainingarXiv:1406.5726v3  [cs.CV]  9 Jul 2014JOURNAL OF L ATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2014 2 Single label images from ImageNet   Multi label images from Pascal VOC  horse&   person   dog&   person   Fig. 1. Some examples from ImageNet [10] and Pascal VOC 2007 [13]. The foreground objects in singlelabel images are usually roughly aligned. However, the as sumption of object alighment is not valid for multilabel images. Also note the partial visibility and occlusion between objects in the multilabel images. data is required to cover the whole label space. For singlelabel images, it is practically easy to collect and annotate the images. However, the burden of collection and annotation for a large scale multilabel image dataset is generally extremely high. To address these issues and take full advantage of CNN for multilabel image classiÔ¨Åcation, in this paper, we propose a Ô¨Çexible deep CNN structure, called HypothesesCNNPooling (HCP). HCP takes an arbitrary number of object segment hypotheses as the inputs, which may be generated by the sateofthe art objectiveness detection techniques, e.g., binarized normed gradients (BING) [8], and then a shared CNN is connected with each hypothesis. Finally the CNN output results from different hypotheses are aggre gated by max pooling to give the ultimate multi label predictions. Particularly, the proposed HCP in frastructure possesses the following characteristics: No groundtruth bounding box information is required for training on the multilabel image dataset. Different from previous works [12], [5], [15], [35], which employ groundtruth bounding box information for training, the proposed HCP requires no bounding box annotation. Since bounding box annotation is much more costly than labelling, the annotation burden is signiÔ¨Åcantly reduced. Therefore, the proposed HCP has a better generalization ability when transferred to new multilabel image datasets. The proposed HCP infrastructure is robust to the noisy and/or redundant hypotheses. To suppress the possibly noisy hypotheses, a crosshypothesismaxpooling operation is carried out to fuse the outputs from the shared CNN into an integrative prediction. With max pooling, the high predictive scores from those hypotheses containing objects are reserved and the noisy ones are ignored. Therefore, as long as one hypothesis contains the object of interest, the noise can be suppressed after the crosshypothesis pooling. Redundant hypotheses can also be well addressed by max pooling. No explicit hypothesis label is required for training. The stateoftheart CNN models [15], [35] utilize the hypothesis label for training. They Ô¨Årst compute the IntersectionoverUnion (IoU) overlap between hypotheses and groundtruth bounding boxes, and then assign the hypothesis with the label of the groundtruth bounding box if their overlap is above a threshold. In contrast, the proposed HCP takes an arbitrary number of hypotheses as the inputs without any explicit hypothesis labels. The shared CNN can be well pretrained with a largescale singlelabel image dataset. To address the problem of insufÔ¨Åcient multilabel training images, based on the HypothesesCNNPooling architecture, the shared CNN can be Ô¨Årst well pretrained on some largescale singlelabel dataset, e.g., ImageNet, and then Ô¨Ånetuned on the target multilabel dataset. The HCP outputs are intrinsically multilabel prediction results. HCP produces a normalized probability distribution over the labels after the softmax layer, and the the predicted probability values are intrinsically the Ô¨Ånal classiÔ¨Åcation conÔ¨Ådence values for the corresponding categories. Extensive experiments on two challenging multi label image datasets, Pascal VOC 2007 and VOC 2012, well demonstrate the superiority of the proposed HCP infrastructure over other stateofthearts. The rest of the paper is organized as follows. We brieÔ¨Çy review the related work of multilabel classiÔ¨Åcation in Section 2. Section 3 presents the details of the HCP for image classiÔ¨Åcation. Finally the experimental results and conclusions are provided in Section 4 and Section 5, respectively. 2 R ELATED WORK "
561,Confidence-based Reliable Learning under Dual Noises.txt,"Deep neural networks (DNNs) have achieved remarkable success in a variety of
computer vision tasks, where massive labeled images are routinely required for
model optimization. Yet, the data collected from the open world are unavoidably
polluted by noise, which may significantly undermine the efficacy of the
learned models. Various attempts have been made to reliably train DNNs under
data noise, but they separately account for either the noise existing in the
labels or that existing in the images. A naive combination of the two lines of
works would suffer from the limitations in both sides, and miss the
opportunities to handle the two kinds of noise in parallel. This work provides
a first, unified framework for reliable learning under the joint (image,
label)-noise. Technically, we develop a confidence-based sample filter to
progressively filter out noisy data without the need of pre-specifying noise
ratio. Then, we penalize the model uncertainty of the detected noisy data
instead of letting the model continue over-fitting the misleading information
in them. Experimental results on various challenging synthetic and real-world
noisy datasets verify that the proposed method can outperform competing
baselines in the aspect of classification performance.","Deep Neural Networks (DNNs) have obtained great success in a wide spectrum of computer vision applications [ 26,41,19,18], especially when a large volume of carefullyannotated lowdistortion images are available. However, the images collected from the wild in realworld tasks are unavoidably polluted by noise in the images themselves (e.g., image corruptions [ 20] and background noise [ 43]) or the associated labels [ 36], termed as image noise ( xnoise ) and label noise ( ynoise ) respectively. Previous investigations show that the DNNs naively trained under ynoise [2,52] orxnoise [11,53] suffer from detrimental overÔ¨Åtting issues, thus exhibit poor generalization performance and serious overconÔ¨Ådence. There has been a large body of attempts towards dealing with data noise, but they mainly focus on a limited setting, where noise only exists in either the label (i.e., noisy labels) [ 36,1,31,8] or the image [ 13,27,50]. It is nontrivial to extend them to exhaustively deal with dual noises (i.e., the joint (x,y)noise ). Moreover, the techniques for handling xnoise suffer from nontrivial limitations. For example, most image denoising methods work on wellpreserved image texture [ 12], thus may easily fail when facing images that are globally blurred (see Fig. 5 in Appendix); alternative image SuperResolution (SR) solutions are usually computationally expensive [ 46]. These issues raise the requirement of a uniÔ¨Åed approach for reliable learning under dual noises. The corresponding author. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2302.05098v1  [cs.CV]  10 Feb 2023Compared to deterministic DNNs, uncertaintybased deep models (e.g., Bayesian Neural Networks (BNNs) [ 3] and deep ensemble [25]) reason about the uncertainty and hence have the potential to mitigate the overÔ¨Åtting to noisy data. Empowered by this insight, we Ô¨Årst perform a systematical investigation on leveraging uncertaintybased deep models to cope with dual noises. We observe that, despite with less overÔ¨Åtting, the uncertaintybased deep models may still suffer from the bias in the noisy data and yield compromising results. To further ameliorate the pathologies induced by data noise and achieve reliable learning, we propose a novel workÔ¨Çow for the learning of uncertaintybased deep models under dual noises. Firstly, inspired by the recent success of using predictive conÔ¨Ådence to detect the outofdistribution data [ 21], we propose to detect both the noisy images and the noisy labels by the predictive conÔ¨Ådence produced by uncertaintybased deep models. Concretely, we use the predictive probability corresponding to the label (i.e., label conÔ¨Ådence) to Ô¨Ålter out the samples with ynoise , and use the maximum conÔ¨Ådence to Ô¨Ålter out the samples with xnoise . After doing so, we propose to penalize the uncertainty [ 23] of the detected noisy data to make use of the valuable information inside the images without relying on the misleading supervisory information. Given the merits of deep ensemble [25] for providing calibrated conÔ¨Ådence and uncertainty under distribution shift revealed by related works [ 37] and our studies, we opt to place our workÔ¨Çow on deep ensemble to establish a strong, scalable, and easytoimplement baseline for learning under dual noises. Of note that the developed strategies are readily applicable to other uncertaintybased deep models like BNNs. We perform extensive empirical studies to evidence the effectiveness of the proposed method. We Ô¨Årst show that the proposed method signiÔ¨Åcantly outperforms competitive baselines on CIFAR100 and TinyImageNet datasets with different levels of synthetic (x,y)noise . We then verify the superiority of the proposed method on the challenging WebVision benchmark [ 28] which contains extensive real world noise. We further provide insightful ablation studies to show the robustness of our approach to multiple hyperparameters. 2 Related Work "
562,Label-free timing analysis of modularized nuclear detectors with physics-constrained deep learning.txt,"Pulse timing is an important topic in nuclear instrumentation, with
far-reaching applications from high energy physics to radiation imaging. While
high-speed analog-to-digital converters become more and more developed and
accessible, their potential uses and merits in nuclear detector signal
processing are still uncertain, partially due to associated timing algorithms
which are not fully understood and utilized. In this paper, we propose a novel
method based on deep learning for timing analysis of modularized nuclear
detectors without explicit needs of labelling event data. By taking advantage
of the inner time correlation of individual detectors, a label-free loss
function with a specially designed regularizer is formed to supervise the
training of neural networks towards a meaningful and accurate mapping function.
We mathematically demonstrate the existence of the optimal function desired by
the method, and give a systematic algorithm for training and calibration of the
model. The proposed method is validated on two experimental datasets. In the
toy experiment, the neural network model achieves the single-channel time
resolution of 8.8 ps and exhibits robustness against concept drift in the
dataset. In the electromagnetic calorimeter experiment, several neural network
models (FC, CNN and LSTM) are tested to show their conformance to the
underlying physical constraint and to judge their performance against
traditional methods. In total, the proposed method works well in either ideal
or noisy experimental condition and recovers the time information from waveform
samples successfully and precisely.","Pulse timing is an important research topic in nuclear detector signal processing, with applications ranging from high energy physics to radiation imaging. Accurate time measurements are meaningful for precisely determining the vertex of interactions as well as the dynamics of incident particles. In the past decade, highspeed analogto digital converters (ADC) were designed for frontend electronics of nuclear detectors [1] and incorporated into their electronic data ow. Traditionally, some xed algorithms can be used to extract time information from a time series of pulse samples, such as leading edge discrimination or constant fraction discrimination [2]. However, when they work in noisy or changing conditions, the performance of these xed algorithms drops signicantly [3]. On the other hand, machine learning techniques, especially neural networks (NN) and deep learning, open another door for possible solution of time extraction from waveform samples. Recent literature has demonstrated that NNs can approximate the Cram er Rao lower bound in a broad range of working conditions [3]. It is estimated that NNs, the key components of intelligent frontend processing, will be widely used in future nuclear detector systems [4, 5, 6, 7], empowered by the evergrowing development of hardware acceleration for NN inference [8, 9]. However, one prerequisite for NNs to achieve superior performance is a justied reference (groundtruth label) in training. While labelled data are easily available in computer vision and many other machine learning tasks, they are not so for nuclear detector experiments. To provide accurate time references for realworld nuclear detectors, additional timing equipments or specic experimental schemes are needed, which will reversely shrink the signicance of using machine learningbased methods. Therefore, it is worthwhile to exploit the builtin structure of nuclear detectors for potential timing correlations and make use of them in the training process. The idea of training NNs without explicit labels was originally invented to locate or detect particular objects in images in the domain of computer vision [10]. Researches in multiple disciplines combined the idea into the formulation of loss functions to include a physicsconstrained term for better accuracy and consistency with the underlying physical laws [11, 12, 13, 14, 15, 16]. For example, in [11] physical constraints were converted into the matrix multiplication form and worked as soft or hard constraints to assist the training process with the standard loss function; in [12] two domainspecic loss terms were used in conjunction with the original loss of NNs to improve prediction performance. Some recent studies went even further to eliminate the canonical loss term and totally rely on physical relations. For example, in [17] the authors proposed an unsupervised learning scheme only requiring the loss from optical laws and performed a physicsbased pretraining with the loss function; in [18] a weaklysupervised learning framework was devised without the need of groundtruth labels for indoor position estimation. Finally, the same principle was extended to solve partial dierential equations which worked as mathematical constraints together with boundary conditions,Labelfree timing for modularized nuclear detectors 3 while NNs were used to represent the solution of equations regarding input variables [19, 20, 21]. In this work, we focus on the aspect of timing analysis and propose a novel method building on the intrinsic modularization of common nuclear detectors. The motivation of the work is to take full advantage of the universal approximation abilities of NNs while oering an available way to avoid using labelled data and to ease optimization of the model. Compared to some former works which also used nuclear detector signals to estimate timeof ight (such as [22, 23]), the major innovations and contributions of the paper include: We propose a practical methodology to use NNs for pulse timing within the conventional nuclear detector data ow without explicit needs of labelled data. A loss function from physical constraints in combination with a regularizer automatically guides the NN model to nd the optimal solution. We invent an algorithmic framework to make the single NN generate the desired time estimates based on an arbitrary time origin by posttraining calibration . We conduct experiments with practical modularized nuclear detectors, validate our method on two experimental datasets, and show its feasibility and accuracy when applying to nuclear detector signals. 2. Methodology "
563,DBLFace: Domain-Based Labels for NIR-VIS Heterogeneous Face Recognition.txt,"Deep learning-based domain-invariant feature learning methods are advancing
in near-infrared and visible (NIR-VIS) heterogeneous face recognition. However,
these methods are prone to overfitting due to the large intra-class variation
and the lack of NIR images for training. In this paper, we introduce
Domain-Based Label Face (DBLFace), a learning approach based on the assumption
that a subject is not represented by a single label but by a set of labels.
Each label represents images of a specific domain. In particular, a set of two
labels per subject, one for the NIR images and one for the VIS images, are used
for training a NIR-VIS face recognition model. The classification of images
into different domains reduces the intra-class variation and lessens the
negative impact of data imbalance in training. To train a network with sets of
labels, we introduce a domain-based angular margin loss and a maximum angular
loss to maintain the inter-class discrepancy and to enforce the close
relationship of labels in a set. Quantitative experiments confirm that DBLFace
significantly improves the rank-1 identification rate by 6.7% on the EDGE20
dataset and achieves state-of-the-art performance on the CASIA NIR-VIS 2.0
dataset.","NIRVIS heterogeneous face recognition refers to the problem of matching face images across the two visual domains. It has been widely adopted to various applica tions, such as video surveillance and user authentication in deÔ¨Åcient lighting conditions. With the evolution of deep learning models, a number of deep learningbased methods [10, 7, 17] are presented and achieved signiÔ¨Åcant improve ment on the popular benchmarks ( e.g., CASIA NIRVIS 2.0 [23] and OuluCASIA NIRVIS [18]). However, NIR VIS heterogeneous face recognition remains a challenging problem due to the large crossmodality gap and the lack of largescale training data with both VIS and NIR images. Datasets of VIS images ( e.g., MS1M [13] and Glint [5]) (a)  (b) Figure 1: Depiction of a set of two labels of a subject: (a) the VIS images with label 1 and (b) the NIR images with label 2. The images depicted on (a) and (b) belong to the same subject, but they are labeled as two separate classes. that contain millions of faces with various face poses and illuminations play a vital role in the success of face recog nition algorithms in the VIS domain. In comparison to these datasets, crossdomain face datasets contain a signiÔ¨Å cantly fewer number of subjects and images. For instance, CASIA NIRVIS 2.0, one of the largest crossspectral face datasets, contains only 17,580 images of 725 subjects cap tured in a constrained environment. The amount of data from CASIA NIRVIS 2.0 is insufÔ¨Åcient for training a face recognition system that can accurately identify images in both visual domains. Several deep learningbased domain invariant feature learning methods [19, 15, 17] have been proposed to overcome the high intraclass variation by train ing on a largescale dataset of VIS images and Ô¨Ånetuning on a smallscale dataset of both VIS and NIR images. Al though training or Ô¨Ånetuning on a smallscale dataset alle viates the gap between the source and the target domains,arXiv:2010.03771v1  [cs.CV]  8 Oct 2020it also reduces the generalization capability of the trained models. The lack of generalization leads to poor perfor mance on new data with much difference to the training data. To avoid Ô¨Ånetuning on a smallscale dataset, we pro pose to train a face recognition model on a joint dataset of a largescale dataset of VIS images and a smallscale dataset of both VIS and NIR images. The number of NIR images in the joint dataset is signiÔ¨Åcantly fewer than the number of VIS images. Therefore, the NIR images become outliers in their classes. To enhance the contribution of NIR images to the training process, we assume that a subject is not repre sented by a single label but by a set of labels. Each label represents images of a speciÔ¨Åc visual domain. In particular, a set of two labels per subject, one for the NIR images and one for the VIS images, are used for training a NIRVIS face recognition model. Based on this assumption, NIR im ages can contribute to the learning of their class representa tions without being dominated by VIS images. In addition, the classiÔ¨Åcation of images of each subject into two differ ent labels reduces the intraclass variation since each class contains images of only one visual domain. Inspired by the additive angular margin loss (ArcFace) [6], we introduce a domainbased angular margin loss (DAML) to maintain the interclass discrepancy. Based on our assumption, the relationship between classes is not equal. It is redundant to enforce the discrepancy between classes that correspond to the same subject. Therefore, the DAML is designed to enforce the margin between classes of different subjects only. In addition, the classes that cor respond to the same subject should be close to each other in the representation space. Therefore, we introduce a max imum angular loss (MAL) to minimize the angle between class representations that correspond to the same subject. The geometrical interpretation of our DAML and MAL is depicted in Fig. 2. The key contributions of this work are as follows: ‚Ä¢ We introduce DBLFace, a learning approach based on the assumption that a subject is represented by a set of two labels, one for the VIS images and one for the NIR images. The classiÔ¨Åcation of images reduces the intra class variation and lessens the negative impact of data imbalance in training (Section 1). ‚Ä¢ We propose a DAML to enforce the angular margin between classes of different subjects (Section 3.1). ‚Ä¢ We propose a MAL that controls the close relationship of class representations of the same subject. The MAL allows class representations of the same subject to be close in the representation space (Section 3.2).2. Related Work "
564,Skeptical Deep Learning with Distribution Correction.txt,"Recently deep neural networks have been successfully used for various
classification tasks, especially for problems with massive perfectly labeled
training data. However, it is often costly to have large-scale credible labels
in real-world applications. One solution is to make supervised learning robust
with imperfectly labeled input. In this paper, we develop a distribution
correction approach that allows deep neural networks to avoid overfitting
imperfect training data. Specifically, we treat the noisy input as samples from
an incorrect distribution, which will be automatically corrected during our
training process. We test our approach on several classification datasets with
elaborately generated noisy labels. The results show significantly higher
prediction and recovery accuracy with our approach compared to alternative
methods.","Generally, label noise comes from the stochastic process that the labels subject to before being presented to the learning algorithm (Angluin and Laird 1988). Typically, the noise is brought to the dataset during annotating. In recent years, deep neural networks have achieved great success in clas siÔ¨Åcation tasks, especially those with large perfectly labeled datasets. In some applications, however, it is very costly to annotate such large datasets by expert level annotators. Ac cepting amateur annotators or crowdsourcing are good solu tions, but those labels should be less credible, which result in label noise. Moreover, some untrusted annotators may label maliciously (Paudice, Mu ÀúnozGonz ¬¥alez, and Lupu 2018), which is a tricky problem. We aim for the cases that no any other prior information has been obtained except the noisylabeled dataset. Neither a set of clean labels nor the model of label noise is avail able. The problem is, since their validation sets are also with noisy labels in these cases, tuning can encounter an obsta cle. Therefore, it is important for a solution to gain the con Ô¨Ådence of the users. The method should be adaptive in order to face the various type of label noise. Also, methods with both good theoretical and empirical result will gain more application value. Our goal is to Ô¨Ånd the best method that meets the criteria above. There are several challenges here. First, we need to Ô¨Ånd a framework that accommodates most kinds of noise types. Second, we should derive a practical method that is robust tovarious types of label noise. Also, we need massive datasets with label noise that is difÔ¨Åcult for a deep neural network. We should Ô¨Ågure out how to generate those confusing labels from existed perfect datasets. In this paper, we Ô¨Årst introduce an approach named distri bution correction. We assume that the noisy dataset is sam pled from an incorrect distribution. Then, we derive how the expectation of values in the correct distribution can be repre sented in the noisy distribution. Also, We use this approach to explain the forward loss correction (Patrini et al. 2017). We implement the distribution correction by skeptical learn ing, where we substitute the correct distribution by the ex pressions with the model‚Äôs predictions. Although deep neural networks are usually more toler ant to the massive label noise (Rolnick et al. 2017) com pared to other machine learning algorithms such as SVMs (Nettleton, OrriolsPuig, and Fornells 2010), many previ ous works (Van Rooyen, Menon, and Williamson 2015; Xiao et al. 2015; Goldberger and BenReuven 2016; Ghosh, Kumar, and Sastry 2017; Patrini et al. 2017; Sukhbaatar et al. 2014) have shown that the mere logistic loss is not the best option. Typically, they either changed the loss function, or added dynamic processes to each label. People can also adopt heuristic solutions such as trick (Li and Long 2000) orbound (Cristianini and ShaweTaylor 2000), which use the times of prediction failures in the training process as a measure of the conÔ¨Ådence to each data entry. We believe these heuristic solutions are based on the generalization abil ity of a model and force the model to be more consistent with itself. However, when it comes to complex classiÔ¨Åca tion tasks of multiple labels, it usually takes many iterations to converge to the extent that we have enough conÔ¨Ådence in its predictions. As the dataset we have is polluted, we usually do not know when the predictions have become referable, so the solution should be adaptive. Finally, our solution com bines these two aspects into an adaptive training process. We also propose a solution to generate a noisylabeled dataset that is confusing to a model from an existed dataset, by sampling the incorrect label from the predictions of a trained deep neural networks. The type of label noise can be categorized as a completely random, a label relevant, and both label and feature dependent random process (Fr ¬¥enay and Verleysen 2014). Usually, The latter two types better de scribe the noise in reality, while they are difÔ¨Åcult to be genarXiv:1811.03821v3  [cs.LG]  13 Jan 2019erated manually. Indeed, researchers can collect actual noisy labels (Vahdat 2017), but it is still costly to collect enough such datasets to do massive tests. Our solution generates noisy datasets of high quality in large quantities, which can make the empirical experiments more convincing. 2 Related Works "
565,SplitNet: Learnable Clean-Noisy Label Splitting for Learning with Noisy Labels.txt,"Annotating the dataset with high-quality labels is crucial for performance of
deep network, but in real world scenarios, the labels are often contaminated by
noise. To address this, some methods were proposed to automatically split clean
and noisy labels, and learn a semi-supervised learner in a Learning with Noisy
Labels (LNL) framework. However, they leverage a handcrafted module for
clean-noisy label splitting, which induces a confirmation bias in the
semi-supervised learning phase and limits the performance. In this paper, we
for the first time present a learnable module for clean-noisy label splitting,
dubbed SplitNet, and a novel LNL framework which complementarily trains the
SplitNet and main network for the LNL task. We propose to use a dynamic
threshold based on a split confidence by SplitNet to better optimize
semi-supervised learner. To enhance SplitNet training, we also present a risk
hedging method. Our proposed method performs at a state-of-the-art level
especially in high noise ratio settings on various LNL benchmarks.","Deep Neural Networks (DNNs) generally rely on largescale training data with humanannotated good labels for achieving satisfactory perfor mance [24]. However, due to the high costs and complexity of labeling the data, the labels are oftencontaminatedbynoise,andthusmanyworks have strived to develop alternative methods that are robust to label noise, which is often called Learning with Noisy Labels (LNL) [37]. Recent studies for LNL, in general, have attempted to distinguish clean samples from the noisy dataset using handcrafted methods, e.g.,Gaussian Mixture Models (GMMs), and then use these clean samples as labeled samples in the SemiSupervised Learning (SSL) phase [29, 38]. However, the shape of the loss distribution often does not follow the Gaussian distribution [2], and data with loss values that are not large or small enoughcannotbeproperlydistinguished.Further more, the dominant approaches maintain multiple models to avoid the risk attributable to the ability ofDNNstoÔ¨Åtarbitrarylabels,butthisoftenleads to complicated training procedures [20]. More over, in the aforementioned LNL methodology that leverages SSL techniques, the weight of the 1arXiv:2211.11753v2  [cs.LG]  19 Dec 2022Springer Nature 2021 L ATEX template 2 Article Title SplitNetMain NetworkPrediction History,LabelClean Train Dataset,Split Confidence Fig. 1:The concept of our alternating update framework with SplitNet. When the main network outputs the prediction history and label,SplitNetusesthemtogenerateacleantrain ing dataset and deliver it to the main network, which is then used as the labeled data of SSL. These two phases are alternatively used to boost convergence and performance. unlabeled loss, one of the most substantial hyper parameter, must be adjusted carefully depending on the noise ratio to prevent the model from over Ô¨Åtting. However, the noise ratio is challenging to tease out in a realworld environment, proving to be an unrealistic approach. To overcome these limitations, we present a novel framework incorporating a learnable net work, called SplitNet, which splits the clean and noisy data in a datadriven manner. Contrary to conventional methods [29, 38] that Ô¨Åt GMMs solely based on persample loss distribution to select clean samples, our SplitNet can addition ally incorporate the prediction history as input, which allows us to better distinguish ambiguous samples that cannot be precisely distinguished by GMM. In addition, we use a split conÔ¨Ådence, a score indicating how conÔ¨Ådently SplitNet divides the samples, to determine whether to apply unsu pervisedloss,enablingmorestablelearningofSSL method in LNL settings. More speciÔ¨Åcally, our overall framework begins with a warmup and then iteratively learns the main network and SplitNet. As shown in Fig. 1, by formulating the main network and SplitNet in an iterative manner, the two learners are alter natelyupdated,eachusingthedatafromtheother network. For SplitNet training, the main network provides class prediction and loss distribution, while for the main network training, SplitNet provides split conÔ¨Ådences to Ô¨Çexibly adjust the threshold for its SSL procedure. By doing so,whereas previous stateoftheart methods [29, 38] require diÔ¨Äerent hyperparameter settings for dif ferent noise ratios, our proposed model achieves superior performance on all benchmarks, despite its simplicity, requiring only one model and a hyperparameter setting. In particular, taking into account the learn ing status of the main network and the estimated noise ratio of the data set, the thresholds are auto matically calculated to distinguish conÔ¨Ådently clean and noisy samples. This process which we dub risk hedging, results in a favorable learning environment for SplitNet to mitigate conÔ¨Årmation bias. As the number of conÔ¨Ådently clean and noisy sample increase throughout the process, SplitNet enjoys the beneÔ¨Åt of a natural curriculum with the aid of the gradually increasing number of hard samples. The key contributions of this method are as follows: ‚Ä¢Our method eÔ¨Äectively distinguishes clean sam ples from noisy datasets compared to other methods through a learnable network called SplitNet. ‚Ä¢As our method enables the learning curricu lum to adjust automatically depending on noise ratio, we propose the SSL method that is favorable to LNL by utilizing split conÔ¨Ådence obtained through SplitNet. ‚Ä¢Our method signiÔ¨Åcantly outperforms stateof theart results on numerous benchmarks with diÔ¨Äerent types and levels of label noise. 2 Related Work "
566,Hierarchically-Refined Label Attention Network for Sequence Labeling.txt,"CRF has been used as a powerful model for statistical sequence labeling. For
neural sequence labeling, however, BiLSTM-CRF does not always lead to better
results compared with BiLSTM-softmax local classification. This can be because
the simple Markov label transition model of CRF does not give much information
gain over strong neural encoding. For better representing label sequences, we
investigate a hierarchically-refined label attention network, which explicitly
leverages label embeddings and captures potential long-term label dependency by
giving each word incrementally refined label distributions with hierarchical
attention. Results on POS tagging, NER and CCG supertagging show that the
proposed model not only improves the overall tagging accuracy with similar
number of parameters, but also significantly speeds up the training and testing
compared to BiLSTM-CRF.","Conditional random Ô¨Åelds (CRF) (Lafferty et al., 2001) is a stateoftheart model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, pre vious work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTMsoftmax gives better accuracies compared to BiLSTM CRF for partofspeech (POS) tagging. In addi tion, the stateoftheart neural Combinatory Cat egorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong rep resentation power of neural sentence encoders such as BiLSTMs allow models to capture im plicit longrange label dependencies from input TheycanfishandalsotomatoeshereInput PRP0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ MD0.7VB0.2‚Ä¶‚Ä¶ VB0.5NN0.4‚Ä¶‚Ä¶ CC0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ RB0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ NN0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ RB0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ PRP0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ MD0.3VB0.7‚Ä¶‚Ä¶ NN0.8VB0.2‚Ä¶‚Ä¶ CC0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ RB0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ NN0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶ RB0.9‚Ä¶‚Ä¶‚Ä¶‚Ä¶PRPVBNNCCRBNNRB First LayerLast LayerOutputFigure 1: Visualization of hierarchicallyreÔ¨Åned Label Attention Network for POS tagging. The numbers in dicate the label probability distribution for each word. word sequences alone (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Teng and Zhang, 2018), thereby allowing the output layer to make local predictions. In contrast, though explicitly capturing output label dependencies, CRF can be limited by its Markov assumptions, particularly when being used on top of neural encoders. In addition, CRF can be computationally expensive when the number of labels is large, due to the use of Viterbi decoding. One interesting research question is whether there is neural alternative to CRF for neural se quence labeling, which is both faster and more ac curate. To this question, we investigate a neural network model for output label sequences. In par ticular, we represent each possible label using an embedding vector, and aim to encode sequences of label distributions using a recurrent neural net work. One main challenge, however, is that the number of possible label sequences is exponential to the size of input. This makes our task essen tially to represent a fullexponential search space without making Markov assumptions. We tackle this challenge using a hierarchically reÔ¨Åned representation of marginal label distribu tions. As shown in Figure 1, our model consists of a multilayer neural network. In each layer, each input words is represented together with itsarXiv:1908.08676v3  [cs.CL]  7 Nov 2019marginal label probabilities, and a sequence neu ral network is employed to model unbounded de pendencies. The marginal distributions space are reÔ¨Åned hierarchically bottomup, where a higher layer learns a more informed label sequence dis tribution based on information from a lower layer. For instance, given a sentence ‚ÄúThey 1can2Ô¨Åsh3 and4also 5tomatoes 6here 7‚Äù, the label distribu tions of the words can2andÔ¨Åsh3in the Ô¨Årst layer of Figure 1 have higher probabilities on the tags MD (modal verb) and VB (base form verb), re spectively, though not fully conÔ¨Ådently. The ini tial label distributions are then fed as the inputs to the next layer, so that longrange label depen dencies can be considered. In the second layer, the network can learn to assign a noun tag to Ô¨Åsh3 by taking into account the highly conÔ¨Ådent tag ging information of tomatoes 6(NN) , resulting in the pattern ‚Äú can2(VB) Ô¨Åsh 3(NN) ‚Äù. As shown in Figure 2, our model consists of stacked attentive BiLSTM layers, each of which takes a sequence of vectors as input and yields a sequence of hidden state vectors together with a sequence of label distributions. The model performs attention over label embeddings (Wang et al., 2015; Zhang et al., 2018a) for deriving a marginal label distributions, which are in turn used to calculate a weighted sum of label embeddings. Finally, the resulting packed label vector is used together with input word vectors as the hidden state vector for the current layer. Thus our model is named label attention network (LAN). For se quence labeling, the input to the whole model is a sentence and the output is the label distributions of each word in the Ô¨Ånal layer. BiLSTMLAN can be viewed as a form of multilayered BiLSTMsoftmax sequence labeler. In particular, a singlelayer BiLSTMLAN is iden tical to a singlelayer BiLSTMsoftmax model, where the label embedding table serves as the soft max weights in BiLSTMsoftmax, and the label attention distribution is the softmax distribution in BiLSTMsoftmax. The traditional way of mak ing a multilayer extention to BiLSTMsoftmax is to stack multiple BiLSTM encoder layers before the softmax output layer, which learns a deeper input representation. In contrast, a multilayer BiLSTMLAN stacks both the BiLSTM encoder layer and the softmax output layer, learning a deeper representation of both the input and can didate output sequences.On standard benchmarks for POS tagging, NER and CCG supertagging, our model achieves sig niÔ¨Åcantly better accuracies and higher efÔ¨Åcien cies than BiLSTMCRF and BiLSTMsoftmax with similar number of parameters. It gives highly competitive results compared with top performance systems on WSJ, OntoNotes 5.0 and CCGBank without external training. In addition to accuracy and efÔ¨Åciency, BiLSTMLAN is also more interpretable than BiLSTMCRF thanks to visualizable label embeddings and label distri butions. Our code and models are released at https://github.com/Nealcly/LAN . 2 Related Work "
567,Distilling the Knowledge of Romanian BERTs Using Multiple Teachers.txt,"Running large-scale pre-trained language models in computationally
constrained environments remains a challenging problem yet to be addressed,
while transfer learning from these models has become prevalent in Natural
Language Processing tasks. Several solutions, including knowledge distillation,
network quantization, or network pruning have been previously proposed;
however, these approaches focus mostly on the English language, thus widening
the gap when considering low-resource languages. In this work, we introduce
three light and fast versions of distilled BERT models for the Romanian
language: Distil-BERT-base-ro, Distil-RoBERT-base, and
DistilMulti-BERT-base-ro. The first two models resulted from the individual
distillation of knowledge from two base versions of Romanian BERTs available in
literature, while the last one was obtained by distilling their ensemble. To
our knowledge, this is the first attempt to create publicly available Romanian
distilled BERT models, which were thoroughly evaluated on five tasks:
part-of-speech tagging, named entity recognition, sentiment analysis, semantic
textual similarity, and dialect identification. Our experimental results argue
that the three distilled models offer performance comparable to their teachers,
while being twice as fast on a GPU and ~35% smaller. In addition, we further
test the similarity between the predictions of our students versus their
teachers by measuring their label and probability loyalty, together with
regression loyalty - a new metric introduced in this work.","Knowledge transfer from Transformerbased language models (Vaswani et al., 2017) trained on large amounts of data achieves stateoftheart results on most Natural Language Processing (NLP) tasks (Devlin et al., 2019; Liu et al., 2019; He et al., 2020). However, the best performing models usually have billions (Brown et al., 2020) or even trillions (Fedus et al., 2021) of parame ters, making them impractical in certain realworld sit uations. Moreover, both training and using these lan guage models usually comes at a high environmental cost (Strubell et al., 2019). Several attempts were made to reduce the size of mod els by distilling their knowledge (Hinton et al., 2015) accumulated during the pretraining phase (Sanh et al., 2019), after Ô¨Ånetuning the model on a speciÔ¨Åc task (Turc et al., 2019), or both pretraining and Ô¨Ånetuning (Jiao et al., 2020). Other methods consider shrinking the size of the models by either quantizing their weights to integer values (Shen et al., 2020), or pruning parts of the neural network (Brix et al., 2020). In addition, efÔ¨Åcient attention mechanisms were developed to over come the quadratic bottleneck in the sequence length of multihead attention (Zaheer et al., 2020; Choromanski et al., 2020). However, the vast majority of these efforts focused on developing English models, and little attention was paid on increasing the efÔ¨Åciency of pretrained mod *Work done during an interhsip at the Research Institute for ArtiÔ¨Åcial Intelligence, Romanian Academy.els on other languages, with few singular cases of such compressed models like BERTino (Muffo and Bertino, 2020) for Italian, MBERTA for Arabic (Alyafeai and Ahmad, 2021), or GermDistilBERT1for German. As a response to this issue, we focus on Romanian, a lan guage on which BERT has recently attracted a surge of attention from the local community and has shown promising results in various areas like dialect identi Ô¨Åcation (Zaharia et al., 2021; Popa and S ,tefÀòanescu, 2020; Zaharia et al., 2020), document classiÔ¨Åcation (Avram et al., 2021) or satire detection in news (Ro goz et al., 2021). Thus, our work introduces three com pressed BERT versions for the Romanian language that were obtained through a distillation process: ‚Ä¢DistilBERTbasero2was obtained by distilling the knowledge of BERTbasero (Dumitrescu et al., 2020) using its original training corpus and to kenizer; ‚Ä¢DistilRoBERTbase3was created from RoBERTbase (Masala et al., 2020) in simi lar conditions (i.e., using both original training corpus and tokenizer); 1https://huggingface.co/ distilbertbasegermancased 2https://huggingface.co/racai/ distilbertbaseromaniancased 3https://huggingface.co/racai/ distilbertbaseromanianuncasedarXiv:2112.12650v3  [cs.CL]  13 Apr 2022‚Ä¢DistilMultiBERTbasero4considered the dis tillation of the knowledge from an ensemble con sisting of BERTbasero and RoBERTbase, while relying on the combined corpus and coupled with the tokenizer of the former model. Our three compressed models were further evaluated on Ô¨Åve Romanian datasets and the results showed that they maintained most of the performance of the original models, while being approximately twice as fast when run on a GPU. In addition, we also measure the label, probability and regression loyalties between each of the three distilled models and their teachers, as quantifying the performance on speciÔ¨Åc tasks does not show how similar the predictions between a teacher and a student really are. The models, together with the distillation and evaluation scripts were opensourced to improve the reproducibility of this work5. The rest of the paper is structured as follows. The next section presents a series of solutions related to the knowledge distillation of pretrained language models. The third section outlines our approach of distilling the knowledge of Romanian BERTs, whereas the fourth section presents the evaluation setup and their perfor mance on various Romanian tasks. The Ô¨Åfth section evaluates the prediction loyalty between each distilled version and its teacher, while the sixth section evalu ates their inference speed. The Ô¨Ånal section concludes our work and outlines potential future work. 2. Related Work "
568,Learning Robust Kernel Ensembles with Kernel Average Pooling.txt,"Model ensembles have long been used in machine learning to reduce the
variance in individual model predictions, making them more robust to input
perturbations. Pseudo-ensemble methods like dropout have also been commonly
used in deep learning models to improve generalization. However, the
application of these techniques to improve neural networks' robustness against
input perturbations remains underexplored. We introduce Kernel Average Pooling
(KAP), a neural network building block that applies the mean filter along the
kernel dimension of the layer activation tensor. We show that ensembles of
kernels with similar functionality naturally emerge in convolutional neural
networks equipped with KAP and trained with backpropagation. Moreover, we show
that when trained on inputs perturbed with additive Gaussian noise, KAP models
are remarkably robust against various forms of adversarial attacks. Empirical
evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show
substantial improvements in robustness against strong adversarial attacks such
as AutoAttack without training on any adversarial examples.","Model ensembles have long been used to improve robustness in the presence of noise. Classic methods like bagging Breiman (1996), boosting Freund (1995); Freund et al. (1996), and random forests Breiman (2001) are established approaches for reducing the variance in estimated prediction functions that build on the idea of constructing strong predictor models by combining many weaker ones. As a result, performance of these ensemble models (especially random forests) is surprisingly robust to noise variables (i.e. features) Hastie et al. (2009). Model ensembling has also been applied in deep learning Zhou et al. (2001); Agarwal et al. (2021); Liu et al. (2021); Wen et al. (2020); Horv√°th et al. (2022). However, the high computational cost of training multiple neural networks and averaging their outputs at test time can quickly become prohibitively expensive (also see work on averaging network weights across multiple finetuned versions Wortsman et al. (2022)). To tackle these challenges, alternative approaches have been proposed to allow learning pseudoensembles of models by allowing individual models within the ensemble to share parameters Bachman et al. (2014); Srivastava et al. (2014); Hinton et al. (2012); Goodfellow et al. (2013). Most notably, dropoutHinton et al. (2012); Srivastava et al. (2014) was introduced to approximate the process of combining exponentially many different neural networks by ‚Äúdropping out‚Äù a portion of units from layers of the neural network for each batch. While these techniques often improve generalization for i.i.d. sample sets, they are not as effective in improving the network‚Äôs robustness against input perturbations and in particular against adversarial attacks 1arXiv:2210.00062v2  [cs.LG]  30 May 2023Wang et al. (2018). Adversarial attacks Szegedy et al. (2013); Biggio et al. (2013); Goodfellow et al. (2014), slight but carefully constructed input perturbations that can significantly impair the network‚Äôs performance, are one of the major challenges to the reliability of modern neural networks. Despite numerous works on this topic in recent years, the problem remains largely unsolved Kannan et al. (2018); Madry et al. (2017); Zhang et al. (2019); Sarkar et al. (2021); Pang et al. (2020); Bashivan et al. (2021); Rebuffi et al. (2021); Gowal et al. (2021). Moreover, the most effective empirical defense methods against adversarial attacks (e.g. adversarial training Madry et al. (2017) and TRADES Zhang et al. (2019)) are extremely computationally demanding (although see more recent work on reducing their computational cost Wong et al. (2019); Shafahi et al. (2019)). Our central premise in this work is that if ensembles can be learned at the level of features (the unit activity at the intermediate layers of the network; in contrast to class likelihoods), the resulting hierarchy of ensembles in the neural network could potentially lead to a much more robust classifier . To this end, we propose a simple method for learning ensembles of kernels in deep neural networks that significantly improves the network‚Äôs robustness against adversarial attacks. In contrast to prior methods such as dropoutthat focus on minimizing feature coadaptation and improving the individual features‚Äô utility in the absence of others, our method focuses on learning feature ensembles that form local ‚Äúcommittees‚Äù similar to those used in Boosting and Random Forests. To create these committees in layers of a neural network, we introduce the Kernel Average Pooling (KAP) operation that computes the average activity in nearby kernels within each layer ‚Äì similar to how spatial Average Pooling layer computes the locally averaged activity within each spatial window, but instead along the kernel dimension. We show that incorporating KAP into convolutional networks leads to learning kernel ensembles that are topographically organized across the tensor dimensions over which the kernels are arranged (i.e. kernels are arranged in a vector or matrix according to their functional similarity). When such networks are trained on inputs perturbed by additive Gaussian noise, these networks demonstrate a substantial boost in robustness against adversarial attacks. In contrast to other ensemble approaches to adversarial robustness, our approach does not seek to train multiple independent neural network models and instead focuses on learning kernel ensembles within a single neural network. Moreover, compared to neural network robustness methods such as Adversarial Training Madry et al. (2017) and TRADES Zhang et al. (2019), training on Gaussian noise is about an order of magnitude more computationally efficient. Our contributions are as follows: ‚Ä¢We introduce kernel average pooling as a simple method for learning kernel ensembles in deep neural networks. ‚Ä¢We demonstrate how kernel average pooling leads to learning topographically organized kernel en sembles that in turn substantially improve model robustness against input noise. ‚Ä¢Through extensive experiments on a wide range of benchmarks, we demonstrate the effectiveness of kernel average pooling on robustness against strong adversarial attacks. 2 Related works and background "
569,Lightweight Adaptive Mixture of Neural and N-gram Language Models.txt,"It is often the case that the best performing language model is an ensemble
of a neural language model with n-grams. In this work, we propose a method to
improve how these two models are combined. By using a small network which
predicts the mixture weight between the two models, we adapt their relative
importance at each time step. Because the gating network is small, it trains
quickly on small amounts of held out data, and does not add overhead at scoring
time. Our experiments carried out on the One Billion Word benchmark show a
significant improvement over the state of the art ensemble without retraining
of the basic modules.","The goal of statistical language modeling is to es timate the probability of sentences or sequences of words (Bahl et al., 1990). By the chain rule of probability theory, this is equivalent to esti mation of the conditional probability of a word given all preceding words. This problem is key to natural language processing, with applications not only in typeahead systems, but also ma chine translation (Brown et al., 1993) and au tomatic speech recognition (Bahl et al., 1990). While earlier work on statistical language model ing focused on ngram language models (Kneser and Ney, 1995; Chen and Goodman, 1999), re cent advances are based on variants of neural lan guage models (Bengio et al., 2003; Mikolov et al., 2010; Dauphin et al., 2016), which have yielded state of the art performance on several large scale benchmarks (Jozefowicz et al., 2016). Neural ap proaches require less memory than ngrams and they generalize better, but with a substantial in crease in computational complexity both at train ing and test time. Despite the superior perfor mance of neural models, even better results in 1 2 3 4 5 Target word frequency bucket0.500.751.001.251.50Ratio of perplexities PPL(lstm)/PPL(ensemble) PPL(lstm)/PPL(ngram)Figure 1: As the frequency of the wordtopredict de creases (from left to right), the relative performance of neural models gets better compared to ngrams (orange curve). Yet, ensembling the two models (with a Ô¨Åxed scalar weight) is more effective on rarer words (blue curve). Bins were built by sorting words by frequency and by dividing them into buckets with equal probabil ity mass. terms of perplexity can be achieved by ensembling neural models with ngrams (Mikolov et al., 2011; Chelba et al., 2013; Jozefowicz et al., 2016). How ever, Fig. 1, which shows results using a single constant scalar to weigh the output distribution of a neural model and an ngram model, suggests that the relative contributions of the two models are not simple. For example, the neural model generalizes better than the ngram on rarer words, yet on rarer words the ensemble yields the largest gains. In this work we will study more sophisticated methods for combining the results of ngram and neural language models than a Ô¨Åxed scalar weight. We will propose a simple gating network which takes as input a handful of features based on fre quency statistics to produce as output an input de pendent weight to be used in the ensemble, effec tively turning the ensemble into an adaptive mix ture of experts model. We show that given already trained neural and ngram language models, the gating network can be trained quickly on a handarXiv:1804.07705v2  [cs.CL]  26 Oct 2018ful of examples. The gating network consistently yields better results than the ensemble which uses a Ô¨Åxed weight in the mixture, while adding a neg ligible computational cost. We evaluated our pro posed approach on the One Billion Word bench mark (Chelba et al., 2013), the biggest publicly available benchmark for language modeling, and on the Wall Street Journal corpus, demonstrating seizable gains on both datasets. 2 Related work "
570,Mining Significant Microblogs for Misinformation Identification: An Attention-based Approach.txt,"With the rapid growth of social media, massive misinformation is also
spreading widely on social media, such as microblog, and bring negative effects
to human life. Nowadays, automatic misinformation identification has drawn
attention from academic and industrial communities. For an event on social
media usually consists of multiple microblogs, current methods are mainly based
on global statistical features. However, information on social media is full of
noisy and outliers, which should be alleviated. Moreover, most of microblogs
about an event have little contribution to the identification of
misinformation, where useful information can be easily overwhelmed by useless
information. Thus, it is important to mine significant microblogs for a
reliable misinformation identification method. In this paper, we propose an
Attention-based approach for Identification of Misinformation (AIM). Based on
the attention mechanism, AIM can select microblogs with largest attention
values for misinformation identification. The attention mechanism in AIM
contains two parts: content attention and dynamic attention. Content attention
is calculated based textual features of each microblog. Dynamic attention is
related to the time interval between the posting time of a microblog and the
beginning of the event. To evaluate AIM, we conduct a series of experiments on
the Weibo dataset and the Twitter dataset, and the experimental results show
that the proposed AIM model outperforms the state-of-the-art methods.","With the rapid growth of social media, such as Facebook, Twitter, and Weibo, people are sharing information and expressing their attitudes publicly. Social media brings great convenience to users, and information can be spread rapidly and widely nowadays. However, misinformation can also be spread on the Internet more easily. Misinformation brings significant harm to daily life, social harmony, or even public security. With the growth of the Internet and social media, such harm Author‚Äôs addresses: Q. Liu, F. Yu, S. Wu and L. Wang, the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA) and the University of Chinese Academy of Sciences (UCAS), Beijing 100080, China; emals: {qiang.liu, feng.yu, shu.wu, wangliang}@nlpr.ia.ac.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ¬©2009 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. 21576904/2010/3ART39 $15.00 https://doi.org/0000001.0000001 ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 39. Publication date: March 2010.arXiv:1706.06314v1  [cs.IR]  20 Jun 201739:2 Qiang Liu, Feng Yu, Shu Wu, and Liang Wang will also grow greater. For instance, as the loss of MH370 has drawn worldwide attention, a great amount of rumors has spread on social media, e.g., MH370 has landed in China1, the loss of MH370 is caused by terrorists2, and Russian jets are related to the loss of MH3703. These rumors about MH370 mislead public attitudes to a wrong direction and delay the search of MH370. Up to March 15, 2017, on the biggest Chinese microblog website Sina Weibo4, 32,076 rumors have been reported and collected in its misinformation management center5. Accordingly, it is crucial to evaluate information credibility and to detect misinformation on social media. Nowadays, to automatically identify misinformation on social media, some methods have been recently proposed. Usually, and event, which may be misinformation or true information, contains a group of microblogs. Thus, most of existing methods identify misinformation at the microblog level [3,11,30] or the event level [ 18,25,44]. Some studies investigate the aggregation of credibility from the microblog level to the event level [ 14]. On the contrary, considering dynamic information, some work designs temporal features based on the prorogation properties over time [ 18] or trains a model with features generated from different time periods [ 25]. Recently, Recurrent Neural Networks (RNN) [ 26] have been incorporated for misinformation identification, and the Gated Recurrent Unit (GRU) structure [ 7] is proved to have satisfactory performance [ 24]. Moreover, some methods take usage of users‚Äô feedbacks (comments and attitudes) to evaluate the credibility [9, 32, 44]. For instance, [ 44] takes out signal tweets, which indicates users‚Äô skepticism about factual claims for detecting misinformation. Though above methods succeed in misinformation identification, they have severe drawbacks. Among these methods, some identify misinformation according to global statistical features of an event or a time window, some calculate the credibility of each microblog and then aggregate them to the the credibility of the whole event. However, information on social media is full of noisy and outliers, which should be alleviated. Moreover, most of microblogs about an event have little contribution to the identification of misinformation. As shown in the example of misinformation on Sina Weibo in Table 1, most users are simply reposting the fake news, or credence to the misinformation. Only a few users express their questions about the misinformation. Thus, it is important to select those significant microblogs, and obtain a reliable misinformation identification method. Fortunately, the attention mechanism [ 13] is suitable for selecting most significant components of information. Via the attention mechanism, components which contribute more to a specific task have large weights for satisfying the objective as much as possible. The attention mechanism has succeed in multiple tasks, such as visual object detection [ 1], image caption [ 39], machine translation [ 2], text summarization [ 33] and text classification [ 35]. Accordingly, with the attention mechanism, we are able to mine signification microblogs for identifying misinformation, and design a reliable automatic detection method. Moreover, misinformation early detection is another important and practical task, in which we need to detect misinformation as early as possible [ 24,44]. Thus, we can take immediate actions at the beginning stage of spreading of misinformation, and minimize the baneful influence. For early detection, we need to identify misinformation with the first several microblogs. And with 1http://www.fireandreamitchell.com/2014/03/07/rumormalaysiaairlinesmh370landedchina/ 2http://www.csmonitor.com/World/AsiaPacific/2014/0310/MalaysiaAirlinesflightMH370Chinaplaysdownterrorism theoriesvideo 3http://www.inquisitr.com/1689765/malaysiaairlinesflightmh370russianjetsinbalticmayholdcluetohowflight 370vanished/ 4http://weibo.com 5http://service.account.weibo.com/?type=5&status=4 ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 39. Publication date: March 2010.An Attentionbased Approach for Identification of Misinformation 39:3 Table 1. An example of misinformation on Sina Weibo. posting time content 2014/03/20 23:55Hearing from an Australian friend: The plane has been found in the international waters near Perth. It is proven to be MH370 according to a major component of the plane. 2014/03/01 23:56 May God bless them! 2014/03/20 23:57 Reposting 2014/03/20 23:58 It is serious to spread rumors! 2014/03/20 23:59 Reposting 2014/03/21 00:00 Hopefully it‚Äôs not true. 2014/03/21 00:02 Reposting 2014/03/21 00:03 Really??? 2014/03/21 00:04 Waiting for official confirmation tomorrow. 2014/03/21 00:06 Reposting 2014/03/21 00:07 Reposting 2014/03/21 00:17 LetƒÖ≈ïs watch the exact news tomorrow morning. Anyway, may God bless them! 2014/03/21 00:18 Reposting 2014/03/21 00:21 What a bad news! 2014/03/21 00:22 Reposting 2014/03/21 00:32 Reposting 2014/03/21 00:35 Reposting unreliable information, what an expert! 2014/03/21 00:46 Reposting 2014/03/21 00:51 No! No! No! 2014/03/21 01:06 Dare to post misinformation! 2014/03/21 01:09 Reposting 2014/03/21 01:25 Is it reliable? 2014/03/21 01:32 Reposting the attention mechanism, we can identify misinformation with several significant microblogs. Accordingly, the attention mechanism is naturally suitable for misinformation early detection. In this work, we propose an Attentionbased approach for Identification of Misinformation (AIM ). First, for each microblog belonging to an event, we calculate corresponding attention value based on its textual features. This attention value is named as content attention. Second, considering microblogs posted at different time have distinct significance for the event, we calculate dynamic attention for each microblog. Dynamic attention can be determined related to the time interval between the posting time of a microblog and the beginning of the event. Then, we aggregate the content attention and the dynamic attention, and obtain the final attention weights for each microblog belonging to an event. Weighted sum of these microblogs can be performed to generate the representation of the whole event. Finally, the prediction of misinformation or true information can be made based on the event representation. In summary, the main contributions of this work are listed as follows: ‚Ä¢We incorporate the attention mechanism for misinformation identification on social media, which mines the most significant microblogs. ‚Ä¢We design both content attention and dynamic attention, for capturing different aspects of significance of microblogs for misinformation identification. ‚Ä¢Experiments conducted on two realworld datasets, i.e., the Weibo dataset and the Twitter dataset, show that AIM is effective and outperforms stateoftheart methods significantly. ‚Ä¢Visualization of the leaned attention mechanism in AIM demonstrates the rationality of our proposed method. The rest of this paper is organized as follows. In section 2, we review some related work on misinformation identification and attention mechanism. Then we detail the proposed AIM model in section 3. In section 4, we conduct and analyze experiments on two realworld datasets, and compare with several stateoftheart methods. In section 5, we illustrate some visualization examples of the leaned attention mechanism. Section 6 concludes this work and discusses future research directions. ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 39. Publication date: March 2010.39:4 Qiang Liu, Feng Yu, Shu Wu, and Liang Wang 2 RELATED WORK "
571,Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer.txt,"The success of Vision Transformer (ViT) in various computer vision tasks has
promoted the ever-increasing prevalence of this convolution-free network. The
fact that ViT works on image patches makes it potentially relevant to the
problem of jigsaw puzzle solving, which is a classical self-supervised task
aiming at reordering shuffled sequential image patches back to their natural
form. Despite its simplicity, solving jigsaw puzzle has been demonstrated to be
helpful for diverse tasks using Convolutional Neural Networks (CNNs), such as
self-supervised feature representation learning, domain generalization, and
fine-grained classification.
  In this paper, we explore solving jigsaw puzzle as a self-supervised
auxiliary loss in ViT for image classification, named Jigsaw-ViT. We show two
modifications that can make Jigsaw-ViT superior to standard ViT: discarding
positional embeddings and masking patches randomly. Yet simple, we find that
Jigsaw-ViT is able to improve both in generalization and robustness over the
standard ViT, which is usually rather a trade-off. Experimentally, we show that
adding the jigsaw puzzle branch provides better generalization than ViT on
large-scale image classification on ImageNet. Moreover, the auxiliary task also
improves robustness to noisy labels on Animal-10N, Food-101N, and Clothing1M as
well as adversarial examples. Our implementation is available at
https://yingyichen-cyy.github.io/Jigsaw-ViT/.","Vision Transformer (ViT) [ 1] is an architecture inherited from Natural Language Processing [ 2] while applied to image classiÔ¨Åcation with taking raw image patches as inputs. Different from classical Convolutional Neural Networks (CNNs), the architectures of ViTs are based on selfattention modules [ 2], which aim at modeling global interactions of all pixels in feature maps. More precisely, ViTs take sequential image patches as inputs, and the attention mechanism enables interaction and aggregation directly among patch information. Therefore, compared to CNNs where image features are progressively learnt from local to global context via reducing spatial resolution, ViT enjoys obtaining global information from the very beginning. Up till now, such convolutionfree networks have been achieving great success on various computer vision tasks, including image classiÔ¨Åcation [ 3,4,5,6,7,8], object detection [ 9,6,10], semantic segmentation [11, 9, 10] and image generation [12], etc. The fact that ViTs work on image patches makes it potentially relevant to one classical image patchbased learning task, that is, jigsaw puzzle solving. Solving jigsaw puzzle aims at reordering shufÔ¨Çed sequential image patches back to their original form. In practice, the problem is interesting for cultural heritage and archaeology to search the correct conÔ¨Åguration given numerous fragments of an art masterpiece [ 13]. However, in the Computer Vision community, thearXiv:2207.11971v2  [cs.CV]  5 Jan 2023Figure 1: Overview framework of our JigsawViT. (Top) We incorporate jigsaw puzzle solving (in blue Ô¨Çow) into the standard ViT for image classiÔ¨Åcation (in red). During the training, we jointly learn the two tasks. (Bottom) The details of our jigsaw puzzle Ô¨Çow. We drop several patches, i.e., patch masking, and remove positional embeddings before feeding to ViT. For each unmasked patch, the model should predict the class corresponding to the patch position. most interesting aspect could be that it provides offtheshelf annotations for free considering a given image. Despite its simplicity, it has shown effectiveness in diverse Computer Vision tasks based on CNNs such as: selfsupervised feature representation learning [ 14], domain generalization [ 15] and Ô¨Ånegrained classiÔ¨Åcation [ 16]. Motivated by the fact that both jigsaw puzzle solving and ViT share the same basis of learning from image patches, we consider incorporating solving jigsaw puzzle to ViT for image classiÔ¨Åcation tasks. In this paper, we explore leveraging the jigsaw puzzle solving problem as a selfsupervised auxiliary loss of a standard ViT, named JigsawViT. Precisely, as shown in Fig. 1, in addition to the standard classiÔ¨Åcation Ô¨Çow in the endtoend training, we add a jigsaw Ô¨Çow whose goal is to predict the absolute positions of the input patches by solving a classiÔ¨Åcation problem. Notably, we make two important modiÔ¨Åcations compared to the naive jigsaw puzzle when feeding input patches to ViTs: i)we get rid of the positional embeddings in the jigsaw Ô¨Çow, by which we prevent the model from cheating from explicit clues in the positional embeddings; ii)we randomly mask some input patches, i.e., patch masking , and then aim at predicting only the positions of those unmasked patches, hence making the prediction rely on global context rather than several particular patches. Despite its simplicity, we Ô¨Ånd that our JigsawViT is able to improve on both generalization and robustness over the standard ViT, which is usually rather a tradeoff [ 17]. To be speciÔ¨Åc, in terms of generalization, we observe a steady increase in classiÔ¨Åcation accuracy on ImageNet1K [ 18] that our jigsaw Ô¨Çow brings to the ViTs. As for robustness, we Ô¨Årst show that the proposed jigsaw Ô¨Çow provides consistent improvement against noisy labels on three important realworld benchmarks, i.e., Animal10N [ 19], Food101N [ 20] and Clothing1M [ 21]. Then, we show that our proposed JigsawViTs can effectively enhance the robustness of ViTs against adversarial attacks in both blackbox and whitebox attack settings. To summarize, our contributions are as follows: First , we propose to introduce the jigsaw puzzle solving task into ViTbased models, namely JigsawViT, with two techniques: removing positional embeddings, and randomly masking patches. Second , empirical results suggest that our jigsaw Ô¨Çow not only improves the generalization ability of ViTs on largescale image classiÔ¨Åcation, but also the robustness against label noise and adversarial examples. Our implementation is available at https://yingyichencyy.github.io/JigsawViT . 2 Related work "
572,Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting.txt,"Producing high-quality forecasts of key climate variables such as temperature
and precipitation on subseasonal time scales has long been a gap in operational
forecasting. Recent studies have shown promising results using machine learning
(ML) models to advance subseasonal forecasting (SSF), but several open
questions remain. First, several past approaches use the average of an ensemble
of physics-based forecasts as an input feature of these models. However,
ensemble forecasts contain information that can aid prediction beyond only the
ensemble mean. Second, past methods have focused on average performance,
whereas forecasts of extreme events are far more important for planning and
mitigation purposes. Third, climate forecasts correspond to a spatially-varying
collection of forecasts, and different methods account for spatial variability
in the response differently. Trade-offs between different approaches may be
mitigated with model stacking. This paper describes the application of a
variety of ML methods used to predict monthly average precipitation and two
meter temperature using physics-based predictions (ensemble forecasts) and
observational data such as relative humidity, pressure at sea level, or
geopotential height, two weeks in advance for the whole continental United
States. Regression, quantile regression, and tercile classification tasks using
linear models, random forests, convolutional neural networks, and stacked
models are considered. The proposed models outperform common baselines such as
historical averages (or quantiles) and ensemble averages (or quantiles). This
paper further includes an investigation of feature importance, trade-offs
between using the full ensemble or only the ensemble average, and different
modes of accounting for spatial variability.","Highquality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales, dened here as the time range between two weeks and two months, has long been a gap in operational forecasting [1]. Advances in weather forecasting on time scales of days to about a week [2, 3, 4, 5] or seasonal forecasts on time scales of two to nine months [6] do not translate to the challenging subseasonal regime. Skillful climate forecasts on subseasonal time scales would have immense value in agriculture, insurance, and economics. The importance of improved subseasonal predictions has been detailed in Ban et al. [1], Council [7]. The National Centers for Environmental Prediction (NCEP), part of the National Oceanic and Atmospheric Administration (NOAA), currently issues a \week 34 outlook"" for the continental U.S.1 The NCEP outlooks are constructed using a combination of dynamical and statistical forecasts, with statistical forecasts based largely on how the local climate in the past has varied (linearly) with indices of the El Ni~ noSouthern Oscillation (ENSO), MaddenJulian Oscillation (MJO), and global warming (i.e., the 30year trend). There exists great potential to advance subseasonal prediction using machine learning (ML) tech niques. A realtime forecasting competition called the Subseasonal Climate Forecast Rodeo [8], spon sored by the Bureau of Reclamation in partnership with NOAA, USGS, and the U.S. Army Corps of 1https://www.cpc.ncep.noaa.gov/products/predictions/WK34/ 1arXiv:2211.15856v1  [cs.LG]  29 Nov 2022Engineers, illustrated that teams using ML techniques can outperform forecasts from NOAA's opera tional seasonal forecast system. This paper focuses on developing MLbased forecasts that leverage ensembles of forecasts produced by NCEP in addition to observed data and other features. Past work, including successful methods in the Rodeo competition (e.g., Hwang et al. [9]), incorporated the ensemble average as a feature in their ML systems, but did not use any other information about the ensemble. In other words, variations among the ensemble members are not re ected in the training data or incorporated into the learned model. In contrast, this paper demonstrates that the full ensemble contains important information for subseasonal climate forecasting outside the ensemble mean. Specically, we consider the test case of predicting monthly 2meter temperatures and precipitation two weeks in advance over 3000 locations over the continental United States using physicsbased predictions, such as NCEPCFSv2 hindcasts [10, 11], using an ensemble of 24 distinct forecasts. We repeat this experiment for the Global Modeling and Assimilation Oce from the National Aeronautics and Space Administration (NASAGMAO) ensemble, which has 11 ensemble members [12]. In this context, this paper makes the following contributions: ‚Ä¢We train a variety of ML models (including neural networks, random forests, linear regres sion, and model stacking) that input all ensemble member predictions as features in addition to contemporaneous observations of geopotential heights, relative humidity, precipitation, and temperature from past months to produce new forecasts with higher accuracy than the ensemble mean; forecast accuracy is measured with a variety of metrics (Section 7). These models are considered in the context of regression, quantile regression, and tercile classication. Systematic experiments are used to characterize the in uence of individual ensemble members on predictive skill (Section 8.1). ‚Ä¢The collection of ML models employed allow us to consider dierent modes of accounting for spatial variability. ML models can account for spatial correlations among both features and responses; specically, when predicting Chicago precipitation, our models can leverage not only information about Chicago, but also about neighboring regions. Specically, we consider the following learning frameworks: (a) learning a predictive model for each spatial location indepen dently; (b) learning a predictive model that inputs the spatial location as a feature and hence can be applied to any single spatial location; (c) learning a predictive model for the full spa tial map of temperature or precipitation { i.e., predicting an outcome for all spatial locations simultaneously. Techniques such as positional encoding in ML models helps ensure that the right neighborhood information is used for each location, adapting to geographic features such as mountains or plains. In addition, ML models present a range of options for accounting for spatial variability, each with distinct advantages and disadvantages. Our application of model stacking allows our nal learned model to exploit the advantages of each method. ‚Ä¢We conduct a series of experiments to help explain the learned model and which features the model uses most to make its predictions. We systematically explore the impact of using lagged observational data in addition to ensemble forecasts and positional encoding to account for spatial variations (Section 8.3). ‚Ä¢The ensemble of forecasts from a physicsbased model (e.g., NCEPCFSv2 or NASAGMAO) contain information salient to precipitation and temperature forecasting besides their mean, and ML models that leverage the full ensemble generally outperform methods that rely on the ensemble mean alone (Section 8.1). ‚Ä¢Finally, we emphasize that the nal validation of our approach was conducted on data from 2011 to 2020 that was not used during any of the training, model development, parameter tuning, or model selection steps. We only conducted our nal assessment of predictive skill for 2011 to 2020 after we had completed all other aspects of this manuscript. Because of this, our nal empirical results accurately re ect the anticipated performance of our methods on new data. 22 Related work "
573,Unsupervised Person Re-identification via Multi-label Classification.txt,"The challenge of unsupervised person re-identification (ReID) lies in
learning discriminative features without true labels. This paper formulates
unsupervised person ReID as a multi-label classification task to progressively
seek true labels. Our method starts by assigning each person image with a
single-class label, then evolves to multi-label classification by leveraging
the updated ReID model for label prediction. The label prediction comprises
similarity computation and cycle consistency to ensure the quality of predicted
labels. To boost the ReID model training efficiency in multi-label
classification, we further propose the memory-based multi-label classification
loss (MMCL). MMCL works with memory-based non-parametric classifier and
integrates multi-label classification and single-label classification in a
unified framework. Our label prediction and MMCL work iteratively and
substantially boost the ReID performance. Experiments on several large-scale
person ReID datasets demonstrate the superiority of our method in unsupervised
person ReID. Our method also allows to use labeled person images in other
domains. Under this transfer learning setting, our method also achieves
state-of-the-art performance.","Recent years have witnessed the great success of person reidentiÔ¨Åcation (ReID), which learns discriminative fea tures from labeled person images with deep Convolutional Neural Network (CNN) [43, 14, 9, 27, 16, 26, 28, 29]. Be cause it is expensive to annotate person images across mul tiple cameras, recent research efforts start to focus on un supervised person ReID. Unsupervised person ReID aims to learn discriminative features from unlabeled person im ages. Compared with supervised learning, unsupervised learning relieves the requirement for expensive data anno tation, hence shows better potential to push person ReID towards real applications. The challenge of unsupervised person ReID lies in learn ing discriminative features without true labels. To conquer Multi label Classification input CNN memory single labelscores multi label MMCL MPLP Multi label Classification input CNN memory single class labelscores multi class  label MMCL MPLP Figure 1. Illustrations of multilabel classiÔ¨Åcation for unsuper vised person ReID. We target to assign each unlabeled person im age with a multiclass label reÔ¨Çecting the person identity. This is achieved by iteratively running MPLP for prediction and MMCL for multilabel classiÔ¨Åcation loss computation. This procedure guides CNN to produce discriminative features for ReID. this challenge, most of recent works [37, 46, 22, 19, 30] de Ô¨Åne unsupervised person ReID as a transfer learning task, which leverages labeled data on other domains for model initialization or label transfer. Among them, some works assign each image with a singleclass label [46]. Some oth ers leverage spatiotemporal cues or additional attribute an notations [22, 19, 30]. Detailed review of existing meth ods will be presented in Sec. 2. Thanks to the above ef forts, the performance of unsupervised person ReID has been signiÔ¨Åcantly boosted. However, there is still a con siderable gap between supervised and unsupervised per son ReID. Meanwhile, the setting of transfer learning leads to limited Ô¨Çexibility. For example, as discussed in many works [21, 35, 31], the performance of transfer learning is closely related to the domain gap, e.g., large domain gap degrades the performance. It is nontrivial to estimate the domain gap and select suitable source datasets for transfer learning in unsupervised person ReID. This paper targets to boost unsupervised person ReID without leveraging any labeled data. As illustrated in Fig. 1, we treat each unlabeled person image as a class and train the ReID model to assign each image with a multiclass label. In other words, the ReID model is trained to classify eacharXiv:2004.09228v1  [cs.CV]  20 Apr 2020image to multiple classes belonging to the same identity. Because each person usually has multiple images, multi label classiÔ¨Åcation effectively identiÔ¨Åes images of the same identity and differentiates images from different identities. This inturn facilitates the ReID model to optimize inter and intra class distances. Compared with previous meth ods [20, 34], which classify each image into a single class, the multilabel classiÔ¨Åcation has potential to exhibit better efÔ¨Åciency and accuracy. Our method iteratively predicts multiclass labels and updates the network with multilabel classiÔ¨Åcation loss. As shown in Fig. 1, to ensure the quality of predicted labels, we propose the Memorybased Positive Label Prediction (MPLP), which considers both visual similarity and cycle consistency for label prediction. Namely, two images are assigned with the same label if they a) share large similar ity and b) share similar neighbors. To further ensure the accuracy of label prediction, MPLP utilizes image features stored in the memory bank, which is updated with aug mented features after each training iteration to improve fea ture robustness. Predicted labels allow for CNN training with a multi label classiÔ¨Åcation loss. Since each image is treated as a class, the huge number of classes makes it hard to train clas siÔ¨Åers like Fully Connected (FC) layers. As shown in Fig. 1, we adopt the feature of each image stored in the memory bank as a classiÔ¨Åer. SpeciÔ¨Åcally, a Memorybased Multi label ClassiÔ¨Åcation Loss (MMCL) is introduced. MMCL accelerates the loss computation and addresses the vanish ing gradient issue in traditional multilabel classiÔ¨Åcation loss [39, 5] by abandoning the sigmoid function and enforc ing the classiÔ¨Åcation score to 1 or 1. MMCL also involves hard negative class mining to deal with the imbalance be tween positive and negative classes. We test our approach on several largescale person ReID datasets including Market1501 [42], DukeMTMC reID [25] and MSMT17 [31] without leveraging other la beled data. Comparison with recent works shows our method achieves competitive performance. For instance, we achieve rank1 accuracy of 80.3% on Market1501, signif icantly outperforming the recent BUC [20] and DBC [4] by 14.1% and 11.1%, respectively. Our performance is also better than the HHL [45] and ECN [46], which use extra DukeMTMCreID [25] for transfer learning. Our method is also compatible with transfer learning. Leverag ing DukeMTMCreID for training, we further achieve rank 1 accuracy of 84.4% on Market1501. In summary, our method iteratively runs MPLP and MMCL to seek true labels for multilabel classiÔ¨Åcation and CNN training. As shown in our experiments, this strat egy, although does not leverage any labeled data, achieves promising performance. The maintained memory bank reinforces both label prediction and classiÔ¨Åcation. Ourwork also shows that, unsupervised training has potential to achieve better Ô¨Çexibility and accuracy than existing transfer learning strategies. 2. Related Work "
574,Camera On-boarding for Person Re-identification using Hypothesis Transfer Learning.txt,"Most of the existing approaches for person re-identification consider a
static setting where the number of cameras in the network is fixed. An
interesting direction, which has received little attention, is to explore the
dynamic nature of a camera network, where one tries to adapt the existing
re-identification models after on-boarding new cameras, with little additional
effort. There have been a few recent methods proposed in person
re-identification that attempt to address this problem by assuming the labeled
data in the existing network is still available while adding new cameras. This
is a strong assumption since there may exist some privacy issues for which one
may not have access to those data. Rather, based on the fact that it is easy to
store the learned re-identifications models, which mitigates any data privacy
concern, we develop an efficient model adaptation approach using hypothesis
transfer learning that aims to transfer the knowledge using only source models
and limited labeled data, but without using any source camera data from the
existing network. Our approach minimizes the effect of negative transfer by
finding an optimal weighted combination of multiple source models for
transferring the knowledge. Extensive experiments on four challenging benchmark
datasets with a variable number of cameras well demonstrate the efficacy of our
proposed approach over state-of-the-art methods.","Person reidentiÔ¨Åcation (reid), which addresses the problem of matching people across different cameras, has attracted intense attention in recent years [8, 31, 53]. Much progress has been made in developing a variety of methods to learn features [17, 22, 23] or distance metrics by exploit ing unlabeled and/or manually labeled data. Recently, deep learning methods have also shown signiÔ¨Åcant performance Equal Contribution yThis work was done while AL was a visiting student at UC Riverside. ùê∂!ùê∂""ùê∂# ùê∂!ùê∂""ùê∂#ùëÄ""#ùëÄ!""ùëÄ!# ùê∂!ùê∂""ùê∂#ùëÄ""#ùëÄ!""ùëÄ!# ùê∂$ùëÄ$#= ?ùëÄ$""= ?ùëÄ$!= ?Source data used for pairwise training of existing networkProvided pairwise metrics without access to source data New limited labeled data (colors  indicate corresponding camera)New camera with new limited pairwise labeled data with existing camerasLearned models , source data discarded  Onboard new camera (ùê∂$)Figure 1: Consider a three camera ( C1,C2andC3) net work, where we have only three pairwise distance metrics (M12,M23andM13) available for matching persons, and no access to the labeled data due to privacy concerns. A new camera,C4, needs to be added into the system quickly, thus, allowing us to have only very limited labeled data across the new camera and the existing ones. Our goal in this pa per is to learn the pairwise distance metrics ( M41,M42and M43) between the newly inserted camera(s) and the existing cameras, using the learned source metrics from the existing network and a small amount of labeled data available after installing the new camera(s). improvement on person reid [1, 16, 33, 34, 46, 54]. How ever, with the notable exception of [26, 27], most of these works have not yet considered the dynamic nature of a cam era network, where new cameras can be introduced at any time to cover a certain related area that is not wellcovered by the existing network of cameras. To build a more scal able person reidentiÔ¨Åcation system, it is very essential to consider the problem of how to onboard new cameras into an existing network with little additional effort. Let us consider Knumber of cameras in a network for which we have learned K 2 number of optimal pairwise 1arXiv:2007.11149v2  [cs.CV]  5 Aug 2020matching metrics, one for each camera pair (see Figure 1 for an illustrative example). However, during an opera tional phase of the system, new camera(s) may be temporar ily introduced to collect additional information, which ide ally should be integrated with minimal effort. Given newly introduced camera(s), the traditional reid methods aim to relearn the pairwise matching metrics using a costly train ing phase. This is impractical in many situations where the newly added camera(s) need to be operational soon after they are added. In this case, we cannot afford to wait a long time to obtain signiÔ¨Åcant amount of labeled data for learn ing pairwise metrics, thus, we only have limited labeled data of persons that appear in the entire camera network after ad dition of the new camera(s). Recently published works [26, 27] attempt to address the problem of onboarding new cameras to a network by utiliz ing old data that were collected in the original camera net work, combined with newly collected data in the expanded network, and source metrics to learn new pairwise met rics. They also assume the same set of people in all camera views, including the new camera (i.e., before and after on boarding new cameras) for measuring the view similarity. However, this is unrealistic in many surveillance scenarios as source camera data may have been lost or not accessible due to privacy concerns. Additionally, new people may ap pear after the target camera is installed who may or may not have appeared in existing cameras. Motivated by this obser vation, we pose an important question: How can we swiftly onboard new camera(s) in an existing reid framework (i) without having access to the source camera data that the original network was trained on, and (ii) relying upon only a small amount of labeled data during the transient phase, i.e., after adding the new camera(s). Transfer learning, which focuses on transferring knowl edge from a source to a target domain, has recently been very successful in various computer vision problems [19, 24, 32, 48, 51]. However, knowledge transfer in our sys tem is challenging, because of limited labeled data and ab sence of source camera data while onboarding new cam eras. To solve these problems, we develop an efÔ¨Åcient model adaptation approach using hypothesis transfer learn ingthat aims to transfer the knowledge using only source models (i.e., learned metrics) and limited labeled data, but without using any original source camera data. Only a few labeled identities that are seen by the target camera, and one or more of the source cameras, are needed for effective transfer of source knowledge to the newly introduced target cameras. Henceforth, we will refer to this as target data . Furthermore, unlike [26, 27], which identify only one best source camera that aligns maximally with the target camera, our approach focuses on identifying an optimal weighted combination of multiple source models for transferring the knowledge.Our approach works as follows. Given a set of pairwise source metrics and limited labeled target data after adding the new camera(s), we develop an efÔ¨Åcient convex opti mization formulation based on hypothesis transfer learn ing [4, 14] that minimizes the effect of negative transfer from any outlier source metric while transferring knowl edge from source to the target cameras. More speciÔ¨Åcally, we learn the weights of different source metrics and the op timal matching metric jointly by alternating minimization, where the weighted source metric is used as a biased reg ularizer that aids in learning the optimal target metric only using limited labeled data. The proposed method, essen tially, learns which camera pairs in the existing source net work best describe the environment that is covered by the new camera and one of the existing cameras. Note that our proposed approach can be easily extended to multiple addi tional cameras being introduced at a time in the network or added sequentially one after another. 1.1. Contributions We address the problem of swiftly onboarding new camera(s) into an existing person reidentiÔ¨Åcation network without having access to the source camera data, and rely ing upon only a small amount of labeled target data in the transient phase, i.e., after adding the new cameras. Towards solving the problem, we make the following contributions. We propose a robust and efÔ¨Åcient multiple metric hy pothesis transfer learning algorithm to efÔ¨Åciently adapt a newly introduced camera to an existing person reid framework without having access to the source data. We theoretically analyse the properties of our algo rithm and show that it minimizes the risk of negative transfer and performs closely to fully supervised case even when a small amount of labeled data is available. We perform rigorous experiments on multiple bench mark datasets to show the effectiveness of our pro posed approach over existing alternatives. 2. Related Work "
575,Enhancing Privacy against Inversion Attacks in Federated Learning by using Mixing Gradients Strategies.txt,"Federated learning reduces the risk of information leakage, but remains
vulnerable to attacks. We investigate how several neural network design
decisions can defend against gradients inversion attacks. We show that
overlapping gradients provides numerical resistance to gradient inversion on
the highly vulnerable dense layer. Specifically, we propose to leverage
batching to maximise mixing of gradients by choosing an appropriate loss
function and drawing identical labels. We show that otherwise it is possible to
directly recover all vectors in a mini-batch without any numerical optimisation
due to the de-mixing nature of the cross entropy loss. To accurately assess
data recovery, we introduce an absolute variation distance (AVD) metric for
information leakage in images, derived from total variation. In contrast to
standard metrics, e.g. Mean Squared Error or Structural Similarity Index, AVD
offers a continuous metric for extracting information in noisy images. Finally,
our empirical results on information recovery from various inversion attacks
and training performance supports our defense strategies. These strategies are
also shown to be useful for deep convolutional neural networks such as LeNET
for image recognition. We hope that this study will help guide the development
of further strategies that achieve a trustful federation policy.","Federated learning (FL) enables distributed client nodes to contribute to the training of a centralised global model without exposing their private data (McMahan et al., 2017; Kairouz et al., 2021; Yang et al., 2019). The promises of federated learning are signiÔ¨Åcant and have wide applicabil ity in industry. For example through federated learning it is possible for hospitals to collaborate on training a centralised model around the globe, without sharing or moving the ac tual private patient information across institutions (Rieke et al., 2020). As it potentially protects sensitive data, it can better align with data protection regulations such as GDPR (Commission, 2018). For example, FL has been already applied to prediction of treatment side effects in medicine (Jochems et al., 2016) or learning a predictive keyboard for smartphones (Bonawitz et al., 2019; Kone Àácn¬¥y et al., 2016). The reduction of data movement is an addi tional important advantage, as it is costly and time consum ing for large industrial applications. Given the potential impact of FL, many authors have since examined the se curity and privacy of FL (Zhao et al., 2020; Geiping et al., 2020; Yin et al., 2021; Zhu et al., 2019; Huang et al., 2020; Phong et al., 2018; Carlini et al., 2020; Shokri et al., 2017; Melis et al., 2019). A standard FL conÔ¨Åguration is typically achieved with a central aggregator node which exchanges gradients for centralised aggregation. At each training step (t), a client node receives neural network model weights,F(Wt), from the aggregator server and calculates loss ( l) with a local data xt;ytfor a minibatch, B, which generates gradients with respect to the model weights: Wt=  BX b<B@l(F(xt;b;yt;b)) @Wt(1) The gradients are aggregated by the centralised server, usu ally by averaging and with rate,  . The gradients, Wt, shared by the client can expose the client to a potential in version attack instigated by a malicious eavesdropper. The inversion attacks have shown to be surprisingly successful in many pioneer studies (Zhao et al., 2020; Geiping et al., 2020; Zhu et al., 2019). This compromised privacy prevents federated learning from becoming a fully trustful framework for distributed training of machine learning models. Whilst most prior studies are focused on developing successful in version attacks on the gradients, in this work, we identify conditions under which privacy can be attained with con Ô¨Ådence against inversion attacks. We explore the effect of the objective loss function and its relation to the minibatch class distributions for dense layers. In addition, we also conduct a similar study for convolutional neural networks on the benchmark task of image classiÔ¨Åcation. Through our analysis, we demonstrate that maximising the mixing of gradients can be an effective defense strategy to counteract gradient inversion attacks. Furthermore this defense strategy does not require or minimise the need for masking of thearXiv:2204.12495v1  [cs.LG]  26 Apr 2022Enhancing Privacy against Inversion Attacks in Federated Learning by using Mixing Gradients Strategies gradients by pruning or noise addition which often leads to a degradation on model accuracy (Zhu et al., 2019; Wei et al., 2020; Tram `er & Boneh, 2021). In more detail, our contributions in this paper are threefold: ‚Ä¢Inversion of batch, label distribution and loss func tion: we revisit the linear dense layer and show that it is possible to directly invert a full batch without an optimisationbased gradient attack. In this context, we provide insights into the role of the loss function and batch label distribution on the recovery rate of gradient inversion attacks. ‚Ä¢Strategies for better privacy: resulting from the above study, we suggest and provide a quantitative analysis of training strategies that are stable against attack. In contrast to existing defence mechanisms (e.g. differential privacy), our training strategies require no noise, therefore preserving accuracy in classiÔ¨Åcation tasks with no performance degradation. ‚Ä¢Absolute Variation Distance (A VD): it is usually dif Ô¨Åcult to tell automatically whether information leakage has obtained from a gradient reversal attack, and met rics such as mean squared error between the ground truth and recovered data are inadequate due to the high degree of noise. We introduce a more effective met ric for detecting information recovery that we term as absolute variation distance, a variant of total variation metric (Rudin et al., 1992). This metric can have an important applications for creating a policy as an au tomatic tool for continuous monitoring of information leakage in a federation, and in future studies of defence strategies for FL. 2. Related Work "
576,Treatment Learning Transformer for Noisy Image Classification.txt,"Current top-notch deep learning (DL) based vision models are primarily based
on exploring and exploiting the inherent correlations between training data
samples and their associated labels. However, a known practical challenge is
their degraded performance against ""noisy"" data, induced by different
circumstances such as spurious correlations, irrelevant contexts, domain shift,
and adversarial attacks. In this work, we incorporate this binary information
of ""existence of noise"" as treatment into image classification tasks to improve
prediction accuracy by jointly estimating their treatment effects. Motivated
from causal variational inference, we propose a transformer-based architecture,
Treatment Learning Transformer (TLT), that uses a latent generative model to
estimate robust feature representations from current observational input for
noise image classification. Depending on the estimated noise level (modeled as
a binary treatment factor), TLT assigns the corresponding inference network
trained by the designed causal loss for prediction. We also create new noisy
image datasets incorporating a wide range of noise factors (e.g., object
masking, style transfer, and adversarial perturbation) for performance
benchmarking. The superior performance of TLT in noisy image classification is
further validated by several refutation evaluation metrics. As a by-product,
TLT also improves visual salience methods for perceiving noisy images.","Although deep neural networks (DNNs) have surpassed humanlevel ‚Äúaccuracy‚Äù in many image recognition tasks [Ron neberger et al., 2015, He et al., 2016, Huang et al., 2017], current DNNs still implicitly rely on the assumption [Pearl, 2019] on the existence of a strong correlation between training and testing data. Moreover, increasing evidence and concerns [Alcorn et al., 2019] show that using the correlation association for prediction can be problematic against noisy images [Xiao et al., 2015], such as poseshifting of identical objects [Alcorn et al., 2019] or imperceptible perturbation [Goodfellow et al., 2015]. In practice, realworld image classiÔ¨Åcation often involves rich, noisy, and even chaotic contexts, intensifying the demand for generalization in the wild. Putting in a uniÔ¨Åed descriptive framework, To address machine perception against noisy images, we are inspired by how human performs visual recognition. Human‚Äôs learning processes are often mixed with logic inference (e.g., a symbolic deÔ¨Ånition from books) and rep resentation learning (e.g., an experience of viewing a visual pattern). One prominent difference between current DNNs and human recognition systems is the capability in causal inference. Mathematically, causal learning [Pearl, 1995a, Peters et al., 2014] is a statistical inference model that infers beliefs or probabilities under uncertain conditions, which aims to identify latent variables (called ‚Äúconfounders‚Äù) that inÔ¨Çuence both intervention and outcome. The unobserved confounders may be abstract in a cognitivelevel (e.g., concepts) but could be observed via their noisy view in the realworld (e.g., objects). For instance, as shown in Fig. 1 (a), confounder learning aims to model aarXiv:2203.15529v1  [cs.CV]  29 Mar 2022Treatment Learning Transformer for Noisy Image ClassiÔ¨Åcation A P REPRINT ICCV CGM Huck CH Yang March 2021 1 Introduction Zy X t (a) CGM Figure 1: Causal graphical model (CGM) for causal video multimodal sum marization (CVS) training phase (a) and testing phase (b). White nodes X (e.g., input data) and y(e.g., output label) are observable. Node Z, colored by grey, is a not observable and latent confounder from representation learning. Xis a noisy view on the hidden confounder Z, say the input text query and video. Node t, colored by grey in (b), is a treatment, e.g., visual or textual per turbation, referring to Section 3.5 for details, which is only observable during training. 1 (i) Train (ii) Test (clean) (iv) Context noise (iii) Additive noise  Figure 1: (a)An example of deployed causal graphical model (CGM), where Zdenotes unobservable confounder variable (e.g., the concept of ‚Äúcat‚Äù), Xdenotes a noisy observation of confounder (e.g., an image can still be recognized as a cat), ydenotes outcome (e.g., a label), and tdenotes the information of a binary treatment (e.g., the existence of extra semantic patterns or additive noise; thus, it is equal to 0or1), which is observable during training and unobservable during testing time. (b)Images with ‚Äúcat‚Äù labels, where (i) and (ii) share the same context of ‚Äúindoor‚Äù; (iii) shows a noisy setup of (ii) undergoing additive Gaussian perturbation; (iv) shows another setup of introducing extra noisy semantic patterns (e.g., ‚Äúwaterside‚Äù) in NICO [He et al., 2020] noisy images dataset. prediction process by Ô¨Ånding a representation (e.g., ‚Äúcat‚Äù) and avoiding relying on irrelevant patterns (e.g., ‚Äúwaterside‚Äù). Intuitively, with causal modeling and confounder inference, correct prediction can be made on noisy inputs, where the generative estimation process, such as causal effect variational autoencoder (CEV AE) [Louizos et al., 2017], affects multiple covariates for predicting data proÔ¨Åles. In this work, we aim to incorporate the effects of causal confounder learning to image classiÔ¨Åcation, as motivated by cognitive psychology for causal learning. SpeciÔ¨Åcally, we use the attention mechanism for noiseresilience inference from patterns. We design a novel sequencetosequence learning model, Treatment Learning Transformer (TLT) , which leverages upon the conditional querybased attention and the inference power from a variational causal inference model. Our TLT tackles noisy image classiÔ¨Åcation by jointly learning to a generative model of Zand estimating the effects from the treatment information ( t), as illustrated in Fig. 1 (a). This model consists of unobservable confounder variables Zcorresponding to the groundtruth but inaccessible information (e.g., the ontological concept [Trampusch and Palier, 2016] of a label), input data Xfrom a noisy view of Z(e.g., images), a treatment [Pearl et al., 2016] information tgiven XandZ(e.g., secondary information as visual patterns and additive noise without directly affecting our understanding the concept of ‚Äúcat‚Äù), and a classiÔ¨Åcation label yfrom the unobservable confounder. Built upon this causal graphical model, our contributions are: ‚Ä¢A transformer architecture (TLT) for noisy image classiÔ¨Åcation are presented, which is based on a treatment estimation architecture and a causal variational generative model with competitive classiÔ¨Åcation performance against noisy image. ‚Ä¢We further curated a new noisy images datasets, Causal Pairs (CPS), to study generalization under different artiÔ¨Åcial noise settings for general and medical images. ‚Ä¢We use formal statistical refutations tests to validate the causal effect of TLT, and show that TLT can improve visual saliency methods on noisy images. 2 Related Work "
577,IRNet: Iterative Refinement Network for Noisy Partial Label Learning.txt,"Partial label learning (PLL) is a typical weakly supervised learning, where
each sample is associated with a set of candidate labels. The basic assumption
of PLL is that the ground-truth label must reside in the candidate set.
However, this assumption may not be satisfied due to the unprofessional
judgment of the annotators, thus limiting the practical application of PLL. In
this paper, we relax this assumption and focus on a more general problem, noisy
PLL, where the ground-truth label may not exist in the candidate set. To
address this challenging problem, we propose a novel framework called
""Iterative Refinement Network (IRNet)"". It aims to purify the noisy samples by
two key modules, i.e., noisy sample detection and label correction. Ideally, we
can convert noisy PLL into traditional PLL if all noisy samples are corrected.
To guarantee the performance of these modules, we start with warm-up training
and exploit data augmentation to reduce prediction errors. Through theoretical
analysis, we prove that IRNet is able to reduce the noise level of the dataset
and eventually approximate the Bayes optimal classifier. Experimental results
on multiple benchmark datasets demonstrate the effectiveness of our method.
IRNet is superior to existing state-of-the-art approaches on noisy PLL.","PARTIAL label learning (PLL) [1], [2] (also called ambigu ous label learning [3], [4] and superset label learning [5], [6]) is a speciÔ¨Åc type of weakly supervised learning [7]. In PLL, each sample is associated with a set of candidate labels, only one of which is the groundtruth label. Due to the high monetary cost of accurately labeled data, PLL has become an active research area in many tasks, such as web mining [8], object annotation [4], [9] and ecological informatics [10]. Unlike supervised learning [11], the groundtruth label is hidden in the candidate set and invisible to PLL [12], [13], which increases the difÔ¨Åculty of model training. Researchers have proposed various approaches to address this problem. These methods can be roughly divided into averagebased [14], [15] and identiÔ¨Åcationbased methods [16], [17]. In averagebased methods, each candidate label has the same probability of being the groundtruth label. They are easy to implement but may be affected by false positive labels [18], [19]. To this end, researchers introduce identiÔ¨Åcationbased methods that treat the groundtruth label as a latent variable and maximize its estimated probability by the maximum margin criterion [20], [21] or the maximum likelihood crite rion [9], [18]. Due to their promising results, identiÔ¨Åcation based methods have attracted increasing attention recently. Zheng Lian, Lan Chen and Bin Liu are with National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Bejing, China, 100190. Email: lianzheng2016@ia.ac.cn; chen lan2016@ia.ac.cn; liubin@nlpr.ia.ac.cn. Licai Sun and Mingyu Xu are with the School of ArtiÔ¨Åcial Intelligence, University of Chinese Academy of Sciences, Beijing, China, 100049. E mail: sunlicai2019@ia.ac.cn; xumingyu2021@ia.ac.cn. Jianhua Tao is with Department of Automation, Tsinghua University, Bejing, China, 100084 and with the School of ArtiÔ¨Åcial Intelligence, University of Chinese Academy of Sciences, Beijing, China, 100049. E mail: jhtao@tsinghua.edu.cn. Manuscript received xxxxxxxx ; revised xxxxxxxx . (Corresponding author: Jianhua Tao, Bin Liu)The above PLL methods rely on a fundamental assump tion that the groundtruth label must reside in the candidate set. However, this assumption may not be satisÔ¨Åed in real world scenarios [22], [23]. Figure 1 shows some typical examples. In online object annotation (see Figure 1(a)), different annotators assign distinct labels to the same image. However, due to the complexity of the image and the unpro fessional judgment of the annotator, the groundtruth label may not be in the candidate set. Another typical application is automatic face naming (see Figure 1(b)). An image with faces is often associated with the text caption, by which we can roughly know who is in the image. However, it is hard to guarantee that all faces have corresponding names in the text caption. Therefore, we relax the assumption of PLL and focus on a more general problem, noisy PLL, where the groundtruth label may not exist in the candidate set. Due to its intractability, few works have studied this problem. The core challenge of noisy PLL is how to deal with noisy samples. To this end, we propose a novel framework called ‚ÄúIterative ReÔ¨Ånement Network (IRNet)‚Äù, which aims to pu rify noisy samples and reduce the noise level of the dataset. Ideally, we can approximate the performance of traditional PLL if all noisy samples are puriÔ¨Åed. IRNet is a multi round framework consisting of two key modules, i.e., noisy sample detection and label correction. To guarantee the performance of these modules, we start with warmup train ing and exploit data augmentation to further increase the reliability of prediction results. We also perform theoretical analysis and prove the feasibility of our proposed method. Qualitative and quantitative results on multiple benchmark datasets demonstrate that our IRNet outperforms currently advanced approaches under noisy conditions. The main contribution of this paper can be summarized as follows: Unlike traditional PLL where the groundtruth labelarXiv:2211.04774v5  [cs.CV]  8 Mar 20232 Annotation from user A:  Horse Ground Truth:  Donkey Annotation from user C:  Deer Annotation from user B:  Mule (a) online object annotation Phoebe is telling  Joey that she  knows who the father of  Rachel ‚Äôs baby is. Ground Truth:  Monica  (b) automatic face naming Fig. 1. Typical applications of noisy PLL. (a) Candidate labels can be provided by crowdsourcing. However, the groundtruth label may not be in the candidate set due to the unprofessional judgments of the annotators. (b) Candidate names can be extracted from the text caption. However, there are general cases of faces without names. must be in the candidate set, we focus on a seldomly discussed but vitally important task, noisy PLL, Ô¨Åll ing the gap of current works. To deal with noisy PLL, we design IRNet, a novel framework with theoretical guarantees. Experimental results on benchmark datasets show the effectiveness of our method. IRNet is superior to existing stateoftheart methods on noisy PLL. The remainder of this paper is organized as follows: In Section 2, we brieÔ¨Çy review some recent works. In Section 3, we propose a novel framework for noisy PLL with theoreti cal guarantees. In Section 4, we introduce our experimental datasets, comparison systems and implementation details. In Section 5, we conduct experiments to demonstrate the effectiveness of our method. Finally, we conclude this paper and discuss our future work in Section 6. 2 R ELATED WORKS "
578,Combining data assimilation and machine learning to emulate a dynamical model from sparse and noisy observations: a case study with the Lorenz 96 model.txt,"A novel method, based on the combination of data assimilation and machine
learning is introduced. The new hybrid approach is designed for a two-fold
scope: (i) emulating hidden, possibly chaotic, dynamics and (ii) predicting
their future states. The method consists in applying iteratively a data
assimilation step, here an ensemble Kalman filter, and a neural network. Data
assimilation is used to optimally combine a surrogate model with sparse noisy
data. The output analysis is spatially complete and is used as a training set
by the neural network to update the surrogate model. The two steps are then
repeated iteratively. Numerical experiments have been carried out using the
chaotic 40-variables Lorenz 96 model, proving both convergence and statistical
skill of the proposed hybrid approach. The surrogate model shows short-term
forecast skill up to two Lyapunov times, the retrieval of positive Lyapunov
exponents as well as the more energetic frequencies of the power density
spectrum. The sensitivity of the method to critical setup parameters is also
presented: the forecast skill decreases smoothly with increased observational
noise but drops abruptly if less than half of the model domain is observed. The
successful synergy between data assimilation and machine learning, proven here
with a low-dimensional system, encourages further investigation of such hybrids
with more sophisticated dynamics.","GeophysicalÔ¨Çuiddynamicsisadomainofscienceinwhichthephysicalanddynamicallawsgoverningthesystems arereasonablywellknown. ThisknowledgeisexpressedbypartialdiÔ¨Äerentialequationsthatarethendiscretisedinto numerical models (Randall et al., 2007). However, due to the misrepresentation of unresolved smallscales features or to neglected physical processes, parts of the numerical models have to be represented by empirical submodels or parameterizations. Earth observations are thus needed for two tasks: Ô¨Årst for model tuning and selection (best realism of the model; i.e. estimating/selecting the best possible parameterizations; see e.g. Metref et al., 2019) and then for data assimilation (best accuracy of the model; i.e. estimating the system state; see e.g. Carrassi et al., 2018). Sometimesthesamedataareusedforbothtasks. Inthelastdecades,thevolumeandqualityofEarthobservationshave increased dramatically, particularly thanks to remote sensing (see, e.g., Kuenzer et al., 2014). At the same time, new developmentsinmachinelearning(ML),particularlydeeplearning(Lecunetal.,2015),havedemonstratedimpressive skillsinreproducingcomplexspatiotemporalprocesses(seee.g.Tranetal.,2015,forvideoprocessing)byeÔ¨Éciently using a huge amount of data, thus paving the path for their use in Earth system science (Reichstein et al., 2019). ForecastingandsimulatingaretwodiÔ¨ÄerentobjectivesthatcanbothbeachievedusingML.VariousMLalgorithms have been already applied to produce surrogate models of fully observed loworder chaotic systems and then used for forecastingpurposes;examplesincludereservoirmodeling(Pathaketal.,2017,2018),residualneuralnetwork(Fablet etal.,2018)andrecurrentnetworks(ParkandYanZhu,2002;Park,2010). Machinelearninghasalsobeenappliedfor <Corresponding author julien.brajard@nersc.no, julien.brajard@sorbonneuniversite.fr (J. Brajard) https://jbrlod.loceanipsl.upmc.fr/website/index.html (J. Brajard) ORCID(s):0000000306341482 (J. Brajard); 0000000307225600 (A. Carrassi); 0000000326750347 (M. Bocquet); 0000000212207207 (L. Bertino) J. Brajard et al.: Preprint submitted to Elsevier Page 1 of 18Combining data assimilation and machine learning nowcastingbasedonrealobservations,suchasseasurfacetemperature(deBezenacetal.,2017)orprecipitation (Shi etal.,2015,2017). AlongwiththeseadvancementsinML,theissueofdeterminingasurrogatemodelofanunknown underlyingprocessbasedonobservationshasalsobeenaddressedusingsparseregression(Bruntonetal.,2017;Zhang and Lin, 2018) and, more recently, data assimilation (Bocquet et al., 2019). Diverse numerical results have proven the eÔ¨Äectiveness of ML to reconstruct dynamics under the condition that the ML algorithm is trained on noisefree and complete observations of the system (i.e. when the system state is fully observed). Nevertheless, these circumstances are almost never encountered in practice and certainly not in the geosciences. In the case of partial and noisy observations, one can use a nearestneighbour or an analogue approach: it consists in Ô¨Ånding similar past data (if available) as a forecast (Lguensat et al., 2018). Machine learning techniques havebeenalsoappliedinsituationswhereonlyadenseportionofthesystemisobserved(e.g.deBezenacetal.,2017; Lu et al., 2017) or when the observations are subsampled in time (Nguyen et al., 2019). MostoftheMLalgorithmsusedintheaforementionedstudiesarenotsuitedtorealisticcasesofnoisyandsparse observations. By‚Äúsparse‚Äù,wemeanherethatthestateofthesystemisnotdenselyobserved,andfurthermorethatthe locations and the number of these sparse observations may vary in space and time. Data assimilation (DA) is a natural framework to deal with this observational scenario: it is designed to estimate the state of a system given noisy, unevenly distributed, observations and a dynamical model (Carrassi et al., 2018). The output of DA depends explicitly on the uncertainties in the numerical model (Harlim, 2017), which has led to developing techniques for taking into account model errors in the DA process. This can be done by a parametrization of the model error embedded in the model equations (Aster et al., 2005; Carrassi and Vannitsem, 2011; Bocquet, 2012)orinformofastochasticnoiseaddedtothedeterministicmodel(e.g.Tr√©molet,2006;Ruizetal.,2013;Raanes et al., 2015; Sakov et al., 2018). In any case, a dynamical model must be at disposal and its presence and use are key in DA. In Bocquet et al. (2019) though, this constraint is relaxed to the point that only the general form of the diÔ¨Äerential equations, representing the model, is speciÔ¨Åed. Notably, and a key to the present study, it was shown that the optimization problem solved by DA in the presence of a model error is equivalent to a ML problem. A similar equivalence was also shown by Abarbanel et al. (2018), starting from the point of view of a ML problem. A pure DA approach, however, does not leverage on recent ML developments, that bring Ô¨Çexibility and facilitate parallel calculations. By including explicit or implicit regularization processes, ML algorithms make possible optimizing in highdimension without the need for additional information under the form of an explicit prior. This paper stands at the crossroads of DA and ML, and focuses on the cases where both the system state and its dynamical model have to infer based on noisy and sparse observations. The proposed hybrid algorithm relies on DA to estimate the state of the system and on ML to emulate the surrogate model. Thepaperisorganisedasfollows. Insection2,weformulatetheproblemanddetailhowDAandMLarecombined. Insection3,wepresenttheexperimentalsetupwhereassection4describesthenumericalresultsusingdiÔ¨Äerentmetrics anddiscussesthealgorithmsensitivitytothenumberofobservationsandtheirnoisestatisticsaswellastoothercontrol parameters. Conclusions and perspective are drawn in section 5. 2. Methodology "
579,Modelling Instance-Level Annotator Reliability for Natural Language Labelling Tasks.txt,"When constructing models that learn from noisy labels produced by multiple
annotators, it is important to accurately estimate the reliability of
annotators. Annotators may provide labels of inconsistent quality due to their
varying expertise and reliability in a domain. Previous studies have mostly
focused on estimating each annotator's overall reliability on the entire
annotation task. However, in practice, the reliability of an annotator may
depend on each specific instance. Only a limited number of studies have
investigated modelling per-instance reliability and these only considered
binary labels. In this paper, we propose an unsupervised model which can handle
both binary and multi-class labels. It can automatically estimate the
per-instance reliability of each annotator and the correct label for each
instance. We specify our model as a probabilistic model which incorporates
neural networks to model the dependency between latent variables and instances.
For evaluation, the proposed method is applied to both synthetic and real data,
including two labelling tasks: text classification and textual entailment.
Experimental results demonstrate our novel method can not only accurately
estimate the reliability of annotators across different instances, but also
achieve superior performance in predicting the correct labels and detecting the
least reliable annotators compared to state-of-the-art baselines.","In many natural language processing (NLP) ap plications, the performance of supervised machine learning models depends on the quality of the cor pus used to train the model. Traditionally, la bels are collected from multiple annotators/experts 1Code is available at https://github.com/ createmomo/instancelevelreliabilitywho are assumed to provide reliable labels. How ever, in reality, these experts may have varying levels of expertise depending on the domains, and thus may disagree on labelling in certain cases (Aroyo and Welty, 2013). A rapid and cost effective alternative is to obtain labels through crowdsourcing (Snow et al., 2008; Poesio et al., 2013, 2017). In crowdsourcing, each instance is presented to multiple expert or nonexpert anno tators for labelling. However, labels collected in this manner could be noisy, since some annotators could produce a signiÔ¨Åcant number of incorrect la bels. This may be due to differing levels of exper tise, lack of Ô¨Ånancial incentive and interest (Poesio et al., 2017), as well as the tedious and repetitive nature of the annotation task (Raykar et al., 2010; Bonald and Combes, 2017). Thus, in order to ensure the accuracy of the la belling and the quality of the corpus, it is crucial to estimate the reliability of the annotators automati cally without human intervention. Previous studies have mostly focused on evalu ating the annotators‚Äô overall reliability (Gurevych and Kim, 2013; Sheshadri and Lease, 2013; Poe sio et al., 2017). Measuring the reliability on a perinstance basis is however useful as we may expect certain annotators to have more expertise in one domain than another, and as a consequence certain annotation decisions will be more difÔ¨Åcult than others. This resolves a potential issue of mod els that only assign an overall reliability to each annotator, where such a model would determine an annotator with expertise in a single domain to be unreliable for the model, even though the anno tations are reliable within the annotator‚Äôs domain of expertise. Estimating perinstance reliability is also help ful for unreliable annotator detection and task allocation in crowdsourcing, where the cost of labelling data is reduced using proactive learnarXiv:1905.04981v1  [cs.CL]  13 May 2019ing strategies for pairing instances with the most costeffective annotators (Donmez and Carbonell, 2008; Li et al., 2017). Although reliability esti mation has been studied for a long time, only a limited number of studies have examined how to model the reliability of each annotator on a per instance basis. Additionally, these in turn have only considered binary labels (Yan et al., 2010, 2014; Wang and Bi, 2017), and cannot be extended to multiclass classiÔ¨Åcation in a straightforward manner. In order to handle both binary and multiclass labels, our approach extends one of the most popu lar probabilistic models for label aggregation, pro posed by Hovy et al. (2013). One challenge of extending the model is the deÔ¨Ånition of the label and reliability probability distributions on a per instance basis. Our approach introduces a classi Ô¨Åer which predicts the correct label of an instance, and a reliability estimator, providing the probabil ity that an annotator will label a given instance cor rectly. The approach allows us to simultaneously estimate the perinstance reliability of the annota tors and the correct labels, allowing the two pro cesses to inform each other. Another challenge is to select appropriate training methods to learn a model with high and stable performance. We in vestigate training our model using the EM algo rithm and cross entropy. For evaluation, we ap ply our method to six datasets including both syn thetic and realworld datasets (see Section 4.1). In addition, we also investigate the effect on the per formance when using different text representation methods and text classiÔ¨Åcation models (see Sec tion 4.2). Our contributions are as follows: Ô¨Årstly, we pro pose a novel probabilistic model for the simulta neous estimation of perinstance annotator relia bility and the correct labels for natural language labelling tasks. Secondly, our work is the Ô¨Årst to propose a model for modelling perinstance relia bility for both binary and multiclass classiÔ¨Åcation tasks. Thirdly, we show experimentally how our method can be applied to different domains and tasks by evaluating it on both synthetic and real world datasets. We demonstrate that our method is able to capture the reliability of each annotator on a perinstance basis, and that this in turn helps improve the performance when predicting the un derlying label for each instance and detecting the least reliable annotators.2 Related Work "
580,Adaptive Early-Learning Correction for Segmentation from Noisy Annotations.txt,"Deep learning in the presence of noisy annotations has been studied
extensively in classification, but much less in segmentation tasks. In this
work, we study the learning dynamics of deep segmentation networks trained on
inaccurately-annotated data. We discover a phenomenon that has been previously
reported in the context of classification: the networks tend to first fit the
clean pixel-level labels during an ""early-learning"" phase, before eventually
memorizing the false annotations. However, in contrast to classification,
memorization in segmentation does not arise simultaneously for all semantic
categories. Inspired by these findings, we propose a new method for
segmentation from noisy annotations with two key elements. First, we detect the
beginning of the memorization phase separately for each category during
training. This allows us to adaptively correct the noisy annotations in order
to exploit early learning. Second, we incorporate a regularization term that
enforces consistency across scales to boost robustness against annotation
noise. Our method outperforms standard approaches on a medical-imaging
segmentation task where noises are synthesized to mimic human annotation
errors. It also provides robustness to realistic noisy annotations present in
weakly-supervised semantic segmentation, achieving state-of-the-art results on
PASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE","Semantic segmentation is a fundamental problem in com puter vision. The goal is to assign a label to each pixel in an image, indicating its semantic category. Deep learn ing models based on convolutional neural networks (CNNs) achieve stateoftheart performance [9, 39, 51, 65]. These models are typically trained in a supervised fashion, which requires pixellevel annotations. Unfortunately, gathering pixellevel annotations is very costly, and may require signif icant domain expertise in some applications [17, 32, 40, 48]. *The Ô¨Årst two authors contribute equally, order decided by coin Ô¨Çipping. 1Code is available at https://github.com/Kangningthu/ ADELEInput Ground Truth Baseline Baseline+ADELE Figure 1. Visualization of the segmentation results of the base line method SEAM [52] and the baseline combined with the pro posed ADaptive EarlyLearning corrEction (ADELE). Our pro posed ADELE improves segmentation quality. More examples can be found in Appendix A.1. Furthermore, annotation noise is inevitable in some appli cations. For example, in medical imaging, segmentation annotation may suffer from interreader annotation varia tions [22, 63]. Learning to perform semantic segmentation from noisy annotations is thus an important topic in practice. Prior works on learning from noisy labels focus on clas siÔ¨Åcation tasks [33, 46, 57]. There are comparatively fewer works on segmentation, where existing works focus on de signing noiserobust network architecture [50] or incorpo rating domain speciÔ¨Åc prior knowledge [42]. We instead focus on improving the performance in a more general per spective by studying the learning dynamics. We observe that the networks tend to Ô¨Årst Ô¨Åt the clean annotations during an ‚Äúearlylearning‚Äù phase, before eventually memorizing the false annotations, thus jeopardizing generalization perfor mance. This phenomenon has been reported in the context of classiÔ¨Åcation [33]. However, this phenomenon in seman tic segmentation differs signiÔ¨Åcantly from its counterpart in classiÔ¨Åcation in the following ways: ‚Ä¢The noise in segmentation labels is often spatially de pendent. Therefore, it is beneÔ¨Åcial to leverage spatial information during training. 1arXiv:2110.03740v2  [cs.CV]  6 Mar 2022ImagesCategory Labels person   dog    sofabird Pixelwise initial annotationsCAM Segmentation modelClassification modelNoisy Segmentation Training imagesPixelwise noisy annotations bike Figure 2. A prevailing pipeline for training WSSS. We aim to improve the segmentation model from noisy annotations. ‚Ä¢In semantic segmentation, early learning and memoriza tion do not occur simultaneously for all semantic cate gories due to pixelwise imbalanced labels. Previous meth ods [28,33] in noisy label classiÔ¨Åcation often assume class balanced data and thus either detecting or handling wrong labels for different classes at the same time. ‚Ä¢The annotation noise in semantic segmentation can be ubiquitous (all examples have some errors) while the state oftheart methods in classiÔ¨Åcation [28,33,67] assume that some samples are completely clean. Inspired by these observations, we propose a new method, ADELE (ADaptive EarlyLearning corrEction), that is de signed for segmentation from noisy annotations. Our method detects the beginning of the memorization phase by monitor ing the Intersection over Union (IoU) curve for each category during training. This allows it to adaptively correct the noisy annotations in order to exploit earlylearning for individual classes. We also incorporate a regularization term to promote spatial consistency, which further improves the robustness of segmentation networks to annotation noise. To verify the effectiveness of our method, we consider a setting where noisy annotations are synthesized and con trollable. We also consider a practical setting ‚Äì Weakly Supervised Semantic Segmentation (WSSS), which aims to perform segmentation based on weak supervision signals, such as imagelevel labels [24, 54], bounding box [11, 44], or scribbles [30]. We focus on a popular pipeline in WSSS. This pipeline consists of two steps (See Figure 2). First, a classiÔ¨Åcation model is used to generate pixellevel annota tions. This is often achieved by applying variations of Class Activation Maps (CAM) [66] combined with postprocessing techniques [3, 25]. Second, these pixellevel annotations are used to train a segmentation model (such as deeplabv1 [8]). Generated by a classiÔ¨Åcation model, the pixelwise anno tations supplied to the segmentation model are inevitably noisy, thus the second step is indeed a noisy segmentation problem. We therefore apply ADELE to the second step. In summary, our main contributions are:‚Ä¢We analyze the behavior of segmentation networks when trained with noisy pixellevel annotations. We show that the training dynamics can be separated into an early learning and a memorization stage in segmentation with annotation noise. Crucially, we discover that these dynam ics differ across each semantic category. ‚Ä¢We propose a novel approach (ADELE) to perform se mantic segmentation with noisy pixellevel annotations, which exploits early learning by adaptively correcting the annotations using the model output. ‚Ä¢We evaluate ADELE on the thoracic organ segmentation task where annotations are corrupted to resemble human errors. ADELE is able to avoid memorization, outper forming standard baselines. We also perform extensive experiments to study ADELE on various types and levels of noises. ‚Ä¢ADELE achieves the state of the art on PASCAL VOC 2012 for WSSS. We show that ADELE can be combined with several different existing methods for extracting pixel level annotations [3,14,52] in WSSS, consistently improv ing the segmentation performance by a substantial margin. 2. Methodology "
581,Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource Sequence Labeling.txt,"In this paper, we address the problem of effectively self-training neural
networks in a low-resource setting. Self-training is frequently used to
automatically increase the amount of training data. However, in a low-resource
scenario, it is less effective due to unreliable annotations created using
self-labeling of unlabeled data. We propose to combine self-training with noise
handling on the self-labeled data. Directly estimating noise on the combined
clean training set and self-labeled data can lead to corruption of the clean
data and hence, performs worse. Thus, we propose the Clean and Noisy Label
Neural Network which trains on clean and noisy self-labeled data simultaneously
by explicitly modelling clean and noisy labels separately. In our experiments
on Chunking and NER, this approach performs more robustly than the baselines.
Complementary to this explicit approach, noise can also be handled implicitly
with the help of an auxiliary learning task. To such a complementary approach,
our method is more beneficial than other baseline methods and together provides
the best performance overall.","For many lowresource languages or domains, only small amounts of labeled data exist. Raw or unlabeled data, on the other hand, is usually avail able even in these scenarios. Automatic annota tion or distant supervision techniques are an option to obtain labels for this raw data, but they often require additional external resources like human generated lexica which might not be available in a lowresource context. Selftraining is a popu lar technique to automatically label additional text. There, a classiÔ¨Åer is trained on a small amount of labeled data and then used to obtain labels for xThis work was started while the authors were at Saarland University.unlabeled instances. However, this can lead to unreliable or noisy labels on the additional data which impede the learning process (Pechenizkiy et al., 2006; Nettleton et al., 2010). In this pa per, we focus on overcoming this slowdown of selftraining. Hence, we propose to apply noise reduction techniques during selftraining to clean the selflabeled data and learn effectively in a low resource scenario. Inspired by the improvements shown by the Noisy Label Neural Network ( NLNN , Bekker and Goldberger (2016)), we can directly apply NLNN to the combined set of the existing clean data and the noisy selflabeled data. However, such an ap plication can be detrimental to the learning pro cess (Section 6). Thus, we introduce the Clean and Noisy Label Neural Network ( CNLNN ) that treats the clean and noisy data separately while training on them simultaneously (Section 3). This approach leads to two advantages over NLNN (Section 6 and 7) when evaluating on two sequencelabeling tasks, Chunking and Named Entity Recognition. Firstly , when adding noisy data, CNLNN is robust showing consistent im provements over the regular neural network, whereas NLNN can lead to degradation in per formance. Secondly , when combining with an indirectnoise handling technique, i.e. with an auxiliary target in a multitask fashion, CNLNN complements better than NLNN in the multitask setup and overall leads to the best performance. 2 Related Work "
582,Automated Detection of Label Errors in Semantic Segmentation Datasets via Deep Learning and Uncertainty Quantification.txt,"In this work, we for the first time present a method for detecting label
errors in image datasets with semantic segmentation, i.e., pixel-wise class
labels. Annotation acquisition for semantic segmentation datasets is
time-consuming and requires plenty of human labor. In particular, review
processes are time consuming and label errors can easily be overlooked by
humans. The consequences are biased benchmarks and in extreme cases also
performance degradation of deep neural networks (DNNs) trained on such
datasets. DNNs for semantic segmentation yield pixel-wise predictions, which
makes detection of label errors via uncertainty quantification a complex task.
Uncertainty is particularly pronounced at the transitions between connected
components of the prediction. By lifting the consideration of uncertainty to
the level of predicted components, we enable the usage of DNNs together with
component-level uncertainty quantification for the detection of label errors.
We present a principled approach to benchmarking the task of label error
detection by dropping labels from the Cityscapes dataset as well from a dataset
extracted from the CARLA driving simulator, where in the latter case we have
the labels under control. Our experiments show that our approach is able to
detect the vast majority of label errors while controlling the number of false
label error detections. Furthermore, we apply our method to semantic
segmentation datasets frequently used by the computer vision community and
present a collection of label errors along with sample statistics.","In many applications such as automated driving and medical imaging, large amounts of data are collected and labeled with the longterm goal of obtaining a strong pre dictor for such labels via artiÔ¨Åcial intelligence, in particular via deep learning [13, 19, 21, 24, 25, 29]. Acquisition of socalled semantic segmentation ground truth, i.e., the pixel wise annotation within a chosen set of classes on which we focus in this work, involves huge amounts of human labor. A German study states an effort of about 1.5 working hours per high deÔ¨Ånition street scene image [33]. Typically, in dustrial and scientiÔ¨Åc labelling processes consist of an iter ative cycle of data labeling and quality assessment. Since the longterm goal of acquiring enough data to train e.g. deep neural networks (DNNs) to close to ground truth per formance requires huge amounts of data, partial automa tion of the labeling cycle is desirable. Two research direc tions aiming at this goal are active learning, which aims at labeling only those data points that leverage the model performance a lot (see e.g. [6, 22, 34]), and the automated detection of label errors (see [9, 28]). Currently, in active learning for semantic segmentation, a moderate number of methods exists. This is also due to the fact that active learn ing comes with an increased computational cost as a DNN has to be trained several times over the course of the ac tive learning iterations [6, 22]. Typically, these methods as sume that perfect ground truth can be obtained by an ora cle/teacher in each active learning iteration. In practice this is not the case and annotations are subject to multiple review loops. In that regard, current methods mostly study how noisy labels affect the model performance [16,44], with the insight that DNNs can deal with a certain amount of label noise quite well. Methods for modeling label uncertainty in medical image segmentation, semantic street scene seg mentation and everyday scene segmentation were proposed in [18, 23, 38, 42, 45]. For image classiÔ¨Åcation tasks, the detection of label er rors was studied in [28]. Importantly, it was pointed out that label errors harm the stability of machine learning bench marks [27]. This stresses the importance of being able to detect label errors, which will help to improve model bench marks and speed up dataset review processes. In this work, we for the Ô¨Årst time study the task of de tecting label errors in semantic segmentation in settings of low inter and intraobserver variability. While DNNs proarXiv:2207.06104v1  [cs.CV]  13 Jul 2022vide predictions on pixel level, we assess DNN predictions on the level of connected components belonging to a given class by utilizing [31]. Note that this is crucial since a connected component has uncertain labels at its boundary, which makes label error detection on pixel level a complex task. For each connected component, we estimate the prob ability of that prediction being correct. If a connected com ponent has a high estimated probability of being correct, while it is signaled to be false positive w.r.t. ground truth, we consider that component as a potential label error. We study the performance of our label error detection method on syn thetic image data from the driving simulator CARLA [10] and on Cityscapes [8]. CARLA gives us a guarantee of be ing per se free of label errors such that we can provide a clean evaluation. To this end, we remove objects from the ground truth and study whether our method is able to iden tify these components as overlooked by the ground truth. Cityscapes provides high quality ground truth with only a small amount of label errors. The ground truth is available in terms of polygons such that we can drop connected com ponents as well. In both cases it turns out that our method is able to detect most of the dropped labels while keeping the amount of false positive label errors under control. We be lieve that our method offers huge potential to make labeling processes more efÔ¨Åcient. Our contribution can be summa rized as follows: ‚Ä¢ We for the Ô¨Årst time present a method that detects label errors in semantic segmentation. ‚Ä¢ Utilizing [31] we detect label errors on the level of con nected components. ‚Ä¢ We introduce a principled benchmark for the detection of label errors based on [10] and [8]. ‚Ä¢ We apply our method to additional datasets [1, 12, 47] and provide examples of label errors that we found. By manually assessing samples of that data we evaluate the precision of our method on those datasets. For all of those four realworld datasets we studied, we achieved a precision between 47:5%and67:5%of correctly predicted label errors. We show that our method is able to Ô¨Ånd both overlooked and classwise Ô¨Çipped labels while keeping the amount of prediction to review considerably low. 2. Related Work "
583,Ensemble Neural Networks (ENN): A gradient-free stochastic method.txt,"In this study, an efficient stochastic gradient-free method, the ensemble
neural networks (ENN), is developed. In the ENN, the optimization process
relies on covariance matrices rather than derivatives. The covariance matrices
are calculated by the ensemble randomized maximum likelihood algorithm (EnRML),
which is an inverse modeling method. The ENN is able to simultaneously provide
estimations and perform uncertainty quantification since it is built under the
Bayesian framework. The ENN is also robust to small training data size because
the ensemble of stochastic realizations essentially enlarges the training
dataset. This constitutes a desirable characteristic, especially for real-world
engineering applications. In addition, the ENN does not require the calculation
of gradients, which enables the use of complicated neuron models and loss
functions in neural networks. We experimentally demonstrate benefits of the
proposed model, in particular showing that the ENN performs much better than
the traditional Bayesian neural networks (BNN). The EnRML in ENN is a
substitution of gradient-based optimization algorithms, which means that it can
be directly combined with the feed-forward process in other existing (deep)
neural networks, such as convolutional neural networks (CNN) and recurrent
neural networks (RNN), broadening future applications of the ENN.","  Artificial neural networks (ANN) are computing systems inspired by biological neural  networks that constitute animal brains. ANN is capable of approximating nonlinear functional  relationships between input and output variables  (Kim et al.,  2018). From a ma thematical  perspective, a neural network can model any function up to any given precision with a sufficiently  large number of basis functions  (Cybenko, 1989; Hornik, 1991). In addition, we can even use much  smaller models by constructing hierarchy neural n etworks (Delalleau & Bengio, 2011; Gal, 2016).  The basic processing elements of neural networks are neurons. A collection of neurons is referred  to as a layer, and the collection of interconnected layers forms the neural networks (Kim  et al. , 2018).  A four layer neural network is illustrated in Fig. 1 as an example. In a neuron, the output is calculated  by a nonlinear function of the sum of its inputs. The connections between different neurons from  adjacent layers are represented by the weights in a model. The weights adjust as learning proceeds ,  and they represent the strength of the signal at a connection. The nonlinear function is also called  the activation function, and the most popular choices are sigmoid, tansig, and ReLU (Li et al.,  2015). 2   ANN has bee n widely applied to solving real world engineering problems, and the following three  topics are significant for effective applications .        Fig. 1.  The structure of an artificial neural network.  ijm  denotes the weight between the ith neuron in a layer and  the jth neuron in the next layer. A neuron is a combination of a linear summation of inputs and an activation  function.     The first topic is uncertainty quantification. Uncertainty is inevitable in all kinds of prediction  models, in cluding neural networks. Predictive uncertainty results from data uncertainty caused by  noisy data, and model uncertainty comes from model parameters and model structure. Uncertainty  quantification determines how much confidence one has in a certain predic tion. This information is  desirable in numerous fields that have the possibility to directly or indirectly affect human life , and  control of them has been gradually handed over to automated systems  (Gal, 2016) , such as life  sciences (Herzog & Ostwald, 2013 ; Acharya et al., 2018 ) and autonomous vehicles (Widrow et al.,  1994 ; Tian et al., 2018 ).  The second topic concerns  data availability. Although data are the most precious resource in  machine learning, data collection is very expensive and time consuming in  many real world  engineering problems. For example, in the field of gas resource evaluation in petroleum engineering,  adsorbed gas content estimation is significant (Wu et al., 2014). However, an adsorption experiment  could take a week to collect a single pair of data, and it is normal to spend millions of dollars in  coring processes to obtain experimental material. Thus, most adsorbed shale gas datasets comprise  less than 100 data (Chen et al., 2017), which hinders the application of neural networks. Data  availability is especially important for deep learning , in which  tens of thousands of weights need to  be trained (He et al., 2016).   The final topic is not yet critical , but has the potential to greatly broaden the scope of neural  network applications. In s ome circumstances, it is desirable to have a gradient free optimization  method. For example, in the field of brain inspired computing, the Hodgkin Huxley (HH) model  (Hodgkin & Huxley, 1952) is utilized as the neuron model rather than the traditional McCull och Pitts (MCP) model (McCulloch & Pitts, 1943), in which the neuron structure is a linear combination  of inputs with an activation function. Although the HH model is much more elaborate and  biomimetic, and thus more accurate, it is described by a set of n onlinear differential equations and  obtaining the derivatives is challenging. A gradient free optimization method could be applied to  natural language processing, as well. Bilingual evaluation understudy is an algorithm for evaluating  3   the quality of text t hat has been machine translated (Papineni et al., 2002 ; Reiter, 2018 ). However,  it is difficult to build a loss function based on this evaluation criterion since it is not differentiable.  However, this will no longer pose a problem if we can find a gradient free optimization method.   Considering the aforementioned problems, a salient question is: are there any alternatives for  the optimization method in a neural network that are able to perform uncertainty analysis and  perform well with a small datase t, but do not rely on derivative calculations?   These obstacles are encountered in numerous engineering fields, such as petroleum  engineering. Uncertainty quantification is critical because underground geological parameters are  highly heterogeneous. High dimension models are always solved based on a small dataset due to  the expensive and time consuming data collection. It is also difficult to identify gradients of a target  variable with respect to model parameters because the corresponding physical models ar e highly  nonlinear and too complicated to solve analytically. In response to these problems, the ensemble  randomized maximum likelihood algorithm (EnRML) is proposed by Gu and Oliver (2007) in the  field of history matching in petroleum engineering. History  matching is an inverse modeling method,  which adjusts a model of a reservoir until it closely reproduces its past behavior (Oliver et al., 2008 ;  Stordal & N√¶vdal, 2018 ). It should be mentioned that the word ‚Äúensemble ‚Äù here indicates a different  meaning fr om that in ensemble averaging (Naftaly et al., 1997). In the former, it means the ensemble  of realizations generated from the same model, rather than multiple models in the latter. The most  prominent feature of the EnRML is that it constitutes a gradient free optimization method because  covariance matrices computed from the realizations are utilized for optimization instead of search  gradients.  Moreover , the EnRML is designed to solve high dimensional problems, which is an  advantage over other gradient free methods, such as the covariance matrix adaptation evolution  strategy (CMA ES) (Hansen  & Kern , 2004) . Chen  and Oliver  (2010) have successfully solved a  267300 dimension al SPE benchmark problem with the EnRML  based on 104 realizations.   The objective of this study is to find a method that is capable  to perform uncertainty analysis ,  perform  well with a small dataset , and do es not rely on derivative calculations. To achieve this  objective, ensemble neural networks (ENN) is proposed based on  the EnRML algorithm. In the  ENN, the feed forward process is the same as the common fully connected neural networks, but the  network training process is adjusted by substituting the EnRML for the traditional gradient descent  algorithm. Uncertainty quantif ication is straightforward in the ENN since it is based on the Bayesian  theorem. In addition, the ENN is not sensitive to data size, and its optimization process does not  necessitate the calculation of derivatives.    The ENN is a gradient free stochastic me thod, which combines the EnRML method of  historical matching with neural networks for the first time. The ENN method also shows that the  neural networks can be trained through correlation information from stochastic realizations  without  calculating the der ivative s. This study verifies the characteristics of the ENN through three  computational experiments , which are regression based on a toy dataset, sanity check based on a  highly nonlinear ideal dataset, and generalization test based on real world datasets .    2. Methodology   "
584,"HyperGAN: A Generative Model for Diverse, Performant Neural Networks.txt","Standard neural networks are often overconfident when presented with data
outside the training distribution. We introduce HyperGAN, a new generative
model for learning a distribution of neural network parameters. HyperGAN does
not require restrictive assumptions on priors, and networks sampled from it can
be used to quickly create very large and diverse ensembles. HyperGAN employs a
novel mixer to project prior samples to a latent space with correlated
dimensions, and samples from the latent space are then used to generate weights
for each layer of a deep neural network. We show that HyperGAN can learn to
generate parameters which label the MNIST and CIFAR-10 datasets with
competitive performance to fully supervised learning, while learning a rich
distribution of effective parameters. We also show that HyperGAN can also
provide better uncertainty estimates than standard ensembles by evaluating on
out of distribution data as well as adversarial examples.","It is well known that it is possible to train deep neu ral networks from different random initializations and ob tain models that, albeit having quite different parameters, achieve similar loss values (Freeman & Bruna, 2016). It has further been found that ensembles of deep networks that are trained in such a way have signiÔ¨Åcant performance advantages over single models (Maclin & Opitz, 2011), similar to the classical bagging approach in statistics. En sembles are also more robust to outliers, and can provide uncertainty estimates over their inputs (Lakshminarayanan et al., 2017). In Bayesian deep learning, there is a signiÔ¨Åcant interest in 1School of Electrical Engineering and Computer Science, Ore gon State University. Correspondence to: Neale Ratzlaff <rat zlafn@oregonstate.edu >, Li Fuxin<lif@oregonstate.edu >. Proceedings of the 36thInternational Conference on Machine Learning , Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).learning approximate posterior distributions over network parameters. Past approaches mostly leverage variational in ference to model this likely intractable distribution. (Gal & Ghahramani, 2016) formulated dropout as a Bayesian ap proximation, and showed that networks with dropout fol lowing each layer are equivalent to a deep Gaussian pro cess (Damianou & Lawrence, 2013) marginalized over its covariance functions. They proposed MC dropout as a sim ple way to estimate model uncertainty. Applying dropout to every layer however, may result in underÔ¨Åtting of the data. Moreover, dropout only integrates over the space of mod els reachable from a single (random initialization). As an other interesting direction, hypernetworks (Ha et al., 2016) are neural networks which output parameters for a target neural network. The hypernetwork and the target network together form a single model which is trained jointly. Orig inally, hypernetwork produced the target weights as a deter ministic function of its own weights, but Bayesian Hyper networks (BHNs) (Krueger et al., 2017), and Multiplica tive Normalizing Flows (MNF) (Louizos & Welling, 2016) learn variational approximations by transforming samples from a Gaussian prior through a normalizing Ô¨Çow. Normal izing Ô¨Çows can model complicated posteriors, but they are composed of invertible bijections, which limits their scala bility and the variety of learnable functions. In this paper we explore an approach which generates all the parameters of a neural network in a single pass, with out assuming any Ô¨Åxed noise models or functional form of the generating function. To keep our method scalable, we do not restrict ourselves to invertible functions as in Ô¨Çow based approaches. We instead utilize ideas from genera tive adversarial networks (GANs). We are especially moti vated by recent Wasserstein Autoencoder (Tolstikhin et al., 2017) approaches, which have demonstrated an impressive capability to model complicated, multimodal distributions. One of the issues in generating weights for every layer is the connectivity of the network. Namely, the output of the previous layer becomes the input of the next layer, hence the network weights must be correspondent in or der to generate valid results. In our approach, we sam ple from a simple multidimensional Gaussian distribution, and propose to transform this sample into multiple differ ent vectors. We call this procedure a mixer since it in troduces correlation to the otherwise independent randomarXiv:1901.11058v3  [cs.LG]  14 Jul 2020HyperGAN noise. Then each random vector is used to generate all the weights within one layer of a deep network. The genera tor is then trained with conventional maximum likelihood (classiÔ¨Åcation/regression) on the weights that it generates, and an adversarial regularization keeps it from collapsing onto only one mode. In this way, it is possible to generate much larger networks than the dimensionality of the latent code, making our approach capable of generating all the weights of a deep network with a single GPU. Somewhat surprisingly, with just this approach we can already generate complete, multilayer convolutional net works which do not require additional Ô¨Ånetuning. We are able to easily sample many welltrained networks from the generator which each achieve low loss on the target dataset. Moreover, our diversity constraints result in models sig niÔ¨Åcantly more diverse than training with multiple random starts (ensembles) or past variational methods. We believe our approach is widely applicable to a variety of tasks. One area where populations of diverse networks show promise is in uncertainty estimation and anomaly de tection. We show through a variety of experiments that populations of diverse networks sampled from our model are able to generate reasonable uncertainty estimates by calculating the entropy of the predictive distribution of sampled networks. Such uncertainty estimates allow us to detect out of distribution samples as well as adversarial ex amples. Our method is straightforward, as well as easy to train and sample from. We hope that we can inspire future work in the estimation of the manifold of neural networks. We summarize our contribution as follows: We propose HyperGAN, a novel approach to approx imating the posterior of neural network parameters for a target architecture. HyperGAN contains a novel mixer that mixes input noise into separate vectors that generate each layer of the network respectively. Different from prior GANs, HyperGAN does not re quire repeated samples to start with (e.g. no need to train 1;000networks as a training set) but trains di rectly using maximum likelihood. This signiÔ¨Åcantly improve training efÔ¨Åciency. The generated networks perform well without need for further Ô¨Ånetuning. On classiÔ¨Åcation experiments, 100 network ensem bles generated by HyperGAN signiÔ¨Åcantly improves accuracy. To validate the uncertainty estimates given by ensembles from HyperGAN, we performed experi ments on a synthetic regression task, an opencategory classiÔ¨Åcation task and an adversarial detection task. 2. Related Work "
585,SLADE: A Self-Training Framework For Distance Metric Learning.txt,"Most existing distance metric learning approaches use fully labeled data to
learn the sample similarities in an embedding space. We present a self-training
framework, SLADE, to improve retrieval performance by leveraging additional
unlabeled data. We first train a teacher model on the labeled data and use it
to generate pseudo labels for the unlabeled data. We then train a student model
on both labels and pseudo labels to generate final feature embeddings. We use
self-supervised representation learning to initialize the teacher model. To
better deal with noisy pseudo labels generated by the teacher network, we
design a new feature basis learning component for the student network, which
learns basis functions of feature representations for unlabeled data. The
learned basis vectors better measure the pairwise similarity and are used to
select high-confident samples for training the student network. We evaluate our
method on standard retrieval benchmarks: CUB-200, Cars-196 and In-shop.
Experimental results demonstrate that our approach significantly improves the
performance over the state-of-the-art methods.","Existing distance metric learning methods mainly learn sample similarities and image embeddings using labeled data [22, 17, 2, 29], which often require a large amount of data to perform well. A recent study [21] shows that most methods perform similarly when hyperparameters are properly tuned despite employing various forms of losses. The performance gains likely come from the choice of net work architecture. In this work, we explore another direc tion that uses unlabeled data to improve retrieval perfor mance. Recent methods in selfsupervised learning [14, 6, 5] and selftraining [32, 7] have shown promising results us ing unlabeled data. Selfsupervised learning leverages un *Work done during an internship at Amazon. Figure 1. A selftraining framework for retrieval. In the training phase, we train the teacher and student networks using both la beled and unlabeled data. In the testing phase, we use the learned student network to extract embeddings of query images for re trieval. labeled data to learn general features in a taskagnostic manner. These features can be transferred to downstream tasks by Ô¨Ånetuning. Recent models show that the features produced by selfsupervised learning achieve comparable performance to those produced by supervised learning for downstream tasks such as detection or classiÔ¨Åcation [5]. Selftraining methods [32, 7] improve the performance of fullysupervised approaches by utilizing a teacher/student paradigm. However, existing methods for selfsupervised learning or selftraining mainly focus on classiÔ¨Åcation but not retrieval. We present a SeLftrAining framework for Distance mEtric learning (SLADE) by leveraging unlabeled data. Figure 1 illustrates our method. We Ô¨Årst train a teacher model on the labeled dataset and use it to generate pseudo labels for the unlabeled data. We then train a student model on both labels and pseudo labels to generate a Ô¨Ånal featurearXiv:2011.10269v2  [cs.CV]  29 Mar 2021embedding. We utilize selfsupervised representation learning to ini tialize the teacher network. Most deep metric learning ap proaches use models pretrained on ImageNet ([17], [29], etc). Their extracted representations might overÔ¨Åt to the pretraining objective such as classiÔ¨Åcation and not gen eralize well to different downstream tasks including dis tance metric learning. In contrast, selfsupervised repre sentation learning [5, 6, 7, 14] learns taskneutral features and is closer to distance metric learning. For these reasons, we initialize our models using selfsupervised learning ap proaches. Our experimental results (Table 3) provide an em pirical justiÔ¨Åcation for this choice. Once the teacher model is pretrained and Ô¨Ånetuned, we use it to generate pseudo labels for unlabeled data. Ide ally, we would directly use these pseudo labels to generate positive and negative pairs and train the student network. However in practice, these pseudo labels are noisy, which affects the performance of the student model (cf. Table 4). Moreover, due to their different sources, it is likely that the labeled and unlabeled data include different sets of cate gories (see Section 4.1 for details about labeled and unla beled datasets). The features extracted from the embedding layer may not adequately represent samples from those un seen classes. To tackle these issues, we propose an addi tional representation layer after the embedding layer. This new layer is only used for unlabeled data and aims at learn ing basis functions for the feature representation of unla beled data. The learning objective is contrastive, i.e. im ages from the same class are mapped close while images from different classes are mapped farther apart. We use the learned basis vectors to compute the feature representation of each image and measure pairwise similarity for unlabeled data. This enables us to select highconÔ¨Ådent samples for training the student network. Once the student network is trained, we use it to extract embeddings of query images for retrieval. We evaluate our model on several standard retrieval benchmarks: CUB200, Cars196 and Inshop. As shown in the experimental section, our approach outperforms several stateoftheart methods on CUB200 and Cars196, and is competitive on Inshop. We also provide various ablation studies in the experimental section. The main technical contributions of our work are: ‚Ä¢ A selftraining framework for distance metric learning, which utilizes unlabeled data to improve retrieval per formance. ‚Ä¢ A feature basis learning approach for the student net work, which better deals with noisy pseudo labels gen erated by the teacher network on unlabeled data.2. Related work "
586,Selective-Supervised Contrastive Learning with Noisy Labels.txt,"Deep networks have strong capacities of embedding data into latent
representations and finishing following tasks. However, the capacities largely
come from high-quality annotated labels, which are expensive to collect. Noisy
labels are more affordable, but result in corrupted representations, leading to
poor generalization performance. To learn robust representations and handle
noisy labels, we propose selective-supervised contrastive learning (Sel-CL) in
this paper. Specifically, Sel-CL extend supervised contrastive learning
(Sup-CL), which is powerful in representation learning, but is degraded when
there are noisy labels. Sel-CL tackles the direct cause of the problem of
Sup-CL. That is, as Sup-CL works in a \textit{pair-wise} manner, noisy pairs
built by noisy labels mislead representation learning. To alleviate the issue,
we select confident pairs out of noisy ones for Sup-CL without knowing noise
rates. In the selection process, by measuring the agreement between learned
representations and given labels, we first identify confident examples that are
exploited to build confident pairs. Then, the representation similarity
distribution in the built confident pairs is exploited to identify more
confident pairs out of noisy pairs. All obtained confident pairs are finally
used for Sup-CL to enhance representations. Experiments on multiple noisy
datasets demonstrate the robustness of the learned representations by our
method, following the state-of-the-art performance. Source codes are available
at https://github.com/ShikunLi/Sel-CL","Deep networks are powerful in various tasks, e.g., im age recognition [19, 62], object detection [58], visual track ing [11] and text matching [3]. The power is largely at tributed to the collection of largescale datasets with high quality annotated labels. In supervised learning, with the *Shiming Ge is the corresponding author. Figure 1. Left: learning a classiÔ¨Åer with ideal representations in duced by clean labels; Right : learning a classiÔ¨Åer with corrupted representations caused by noisy labels. Circles represent the repre sentations of positive examples while triangles represent the rep resentations of negative examples. When the representations are corrupted by noisy labels, the decision boundary of the classiÔ¨Åer will be largely changed. Therefore, the learned classiÔ¨Åer in this case cannot generalize well on test examples. data ( i.e., the instance and label pairs) in such datasets, deep networks Ô¨Årst learn ideal latent representations of the in stances and then complete following tasks with the repre sentations [14, 57]. However, it is extremely expensive to obtain largescale highquality annotated labels. Alterna tively, we can collect labels based on web search and user tags [36, 55]. These labels are cheap but inevitably noisy. Noisy labels impair the generalization performance of deep networks [15, 60]. It is because, supervised by the datasets with noisy labels, the mislabeled data provide in correct signals when inducing latent representations for the instances. The corrupted representations then cause inac curate decisions for following tasks and hurt generaliza tion [29, 52]. For example, as shown in Fig. 1, the cor rupted representations result in an inprecise classiÔ¨Åcation boundary. Therefore, it is crucial to induce robust latent representations of instances for learning with noisy labels, which is also our focus in this paper. Recent works [7, 8, 13, 18, 65] show that, working in a pairwise manner, contrastive learning (CL) methods can bring good latent representations to help following tasks. Based on whether supervised information is provided, the 1arXiv:2203.04181v1  [cs.CV]  8 Mar 2022CL methods can be grouped into supervised contrastive learning (SupCL) [27] and unsupervised contrastive learn ing (UnsCL) [7, 8, 18]. It has been shown that SupCL can exploit the supervised information to learn better rep resentations than UnsCL, but relies on the quality of su pervised information [41]. If the supervised information is corrupted by noisy labels, built pairs by training exam ples are noisy, following corrupted representations learned by SupCL. Motivated by this phenomenon, prior meth ods use generalpurpose techniques in tackling noisy labels for robust representation learning with SupCL, e.g., intro ducing regularization [41] or generating pseudolabels [32]. Although these methods can work Ô¨Åne in some cases, the generalpurpose techniques fail to consider the remarkable pairwise characteristic of SupCL in strengthening repre sentation learning. The achieved performance by them is thus argued to be suboptimal . In this paper, we propose selectivesupervised con trastive learning (SelCL) to address the above issue. Sel CL can make use of the pairwise characteristic to learn ro bust latent representations. The core idea of SelCL is (1) select conÔ¨Ådent pairs out of noisy pairs; (2) employ the con Ô¨Ådent pairs to learn robust latent representations. Note that it is hard to identify conÔ¨Ådent pairs directly for representa tion learning. The main reason is that we always need to set a threshold with the noise rate for precise identiÔ¨Åcation, e.g., see [15, 16, 26]. Nevertheless, it is difÔ¨Åcult to estimate the noise rate of noisy pairs. To handle this problem, we propose to Ô¨Årst employ conÔ¨Ådent examples [16, 41], which is much easier to identiÔ¨Åed, to build a reliable set of conÔ¨Å dent pairs at each epoch. Then, based on the representation similarity distribution of conÔ¨Ådent pairs in this set, we set a dynamic threshold to selected more conÔ¨Ådent pairs out of all noisy pairs. By this pairwise selection, we can make better use of not only the pairs whose class labels are correct, but also the pairs whose class labels are incorrect, but the ex amples in them are misclassiÔ¨Åed to the same class. All se lected conÔ¨Ådent pairs are utilized to enhance representation learning with SupCL. As the selected conÔ¨Ådent pairs are less noisy, the learned representations with this selective supervised paradigm will be more robust, naturally follow ing promising generalization. The main contributions of this paper are summarized as three aspects: 1) We propose selectivesupervised con trastive learning with noisy labels, which can obtain robust pretrained representations by effectively selecting conÔ¨Å dent pairs for performing SupCL. 2) Without knowing the noise rate of pairs, our approach selects the pairs built by identiÔ¨Åed conÔ¨Ådent examples, and the pairs built by the ex amples with high representation similarities. It fulÔ¨Åls a pos itive cycle, where better conÔ¨Ådent pairs result in better rep resentations and better representations will identify better conÔ¨Ådent pairs. 3) We conduct experiments on syntheticand realworld noisy datasets, which clearly demonstrate our approach achieves better performance compared with the stateoftheart methods. Comprehensive ablation stud ies and discussions are also provided. 2. Related Works "
587,Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning.txt,"Distantly supervised named entity recognition (DS-NER) efficiently reduces
labor costs but meanwhile intrinsically suffers from the label noise due to the
strong assumption of distant supervision. Typically, the wrongly labeled
instances comprise numbers of incomplete and inaccurate annotation noise, while
most prior denoising works are only concerned with one kind of noise and fail
to fully explore useful information in the whole training set. To address this
issue, we propose a robust learning paradigm named Self-Collaborative Denoising
Learning (SCDL), which jointly trains two teacher-student networks in a
mutually-beneficial manner to iteratively perform noisy label refinery. Each
network is designed to exploit reliable labels via self denoising, and two
networks communicate with each other to explore unreliable annotations by
collaborative denoising. Extensive experimental results on five real-world
datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising
methods.","Named Entity Recognition (NER) is the task of detecting entity spans and then classifying them into predeÔ¨Åned categories, such as person, location and organization. Due to the capability of extract ing entity information and beneÔ¨Åting many NLP applications (e.g., relation extraction (Lin et al., 2017), question answering (Li et al., 2019)), NER appeals to many researchers. Traditional super vised methods for NER require a large amount of highquality corpus for model training, which is extremely expensive and timeconsuming as NER requires tokenlevel labels. Therefore, in recent years, distantly supervised named entity recognition (DSNER) has been pro posed to automatically generate labeled training set Corresponding author 1The source code and data can be found at https:// github.com/AIRobotZhang/SCDL . Jack  Lucas  was  born  in  the  Amazon  region  . BPER  I PER    O       O OO BLOC O      O O         O O O OO BORG O      OGolden Labels : Noisy Labels :Figure 1: A noisy sample generated by distantly supervised methods, where Jack Lucas is the incom plete annotation and Amazon is inaccurate. by aligning entities in knowledge bases (e.g., Free base) or gazetteers to corresponding entity men tions in sentences. This labeling procedure is based on a strong assumption that each entity mention in a sentence is a positive instance of the correspond ing type according to the extra resources. How ever, this assumption is far from reality. Due to the limited coverage of existing resources, many entity mentions in the text cannot be matched and are wrongly annotated as nonentity, resulting in incomplete annotations. Moreover, two entity men tions with the same surface name can belong to different entity types, thus simple matching rules may fall into the dilemma of labeling ambiguity and produce inaccurate annotations. As illustrated in Figure 1, the entity mention ‚Äú Jack Lucas ‚Äù is not recognized due to the limited coverage of extra resources and ‚Äú Amazon ‚Äù is wrongly labeled with organization type owing to the labeling ambiguity. Recently, many denoising methods (Shang et al., 2018b; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019; Li et al., 2021) have been developed to handle noisy labels in DSNER. For example, Shang et al. (2018b) obtained highquality phrases through AutoPhrase (Shang et al., 2018a) and de signed AutoNER to model these phrases that may be potential entities. Peng et al. (2019) proposed a positiveunlabeled learning algorithm to unbiasedly and consistently estimate the NER task loss, and Li et al. (2021) used negative sampling to elimi nate the misguidance brought by unlabeled entities. Though achieving good performance, most studies mainly focus on solving incomplete annotationsarXiv:2110.04429v2  [cs.CL]  15 Feb 2023with a strong assumption of no inaccurate ones existing in DSNER. Meanwhile, these methods aim to reduce the negative effect of noisy labels by weakening or abandoning the wrongly labeled instances. Hence, they can at most alleviate the noisy supervision and fail to fully mine useful in formation from the mislabeled data. Intuitively, if we can rectify those unreliable annotations into positive instances for model training, a higher data utilization and better performance will be achieved. We argue that an ideal DSNER denoising system should be capable of solving two kinds of label noise (i.e., incomplete and inaccurate annotations) and making full use of the whole training set. In this work, we strive to reconcile this gap and propose a robust learning framework named SCDL (SelfCollaborative Denoising Learning). SCDL cotrains two teacherstudent networks to form in ner and outer loops for coping with label noise without any assumption, as well as making full exploration of mislabeled data. The inner loop in side each teacherstudent network is a self denois ing scheme to select reliable annotations from two kinds of noisy labels, and the outer loop between two networks is a collaborative denoising proce dure to rectify unreliable instances into useful ones. SpeciÔ¨Åcally, in the inner loop, each teacherstudent network selects consistent and highconÔ¨Ådence la beled tokens generated by the teacher to train the student, and then updates the teacher gradually via exponential moving average (EMA)2based on the retrained student. And as for the outer loop, the highquality pseudo labels generated by one net work‚Äôs teacher are used to update the noisy labels of the other network thanks to the stability of EMA and different noise sensitivities between two net works. Moreover, the inner and outer loop proce dures will be performed alternately. Obviously, a successful self denoising process (inner loop) can generate highquality pseudo labels which beneÔ¨Åt the collaborative learning procedure (outer loop) a lot and a promising outer loop will promote the inner loop by reÔ¨Åning noisy labels, thus handling the label noise in DSNER effectively. We evaluate our method on Ô¨Åve DSNER datasets. Experimental results indicate that SCDL consistently achieves superior performance over previous competing approaches. Extensive valida 2A momentum technique that has been explored in several studies, e.g., Adam (Kingma and Ba, 2015), semisupervised (Tarvainen and Valpola, 2017) and self supervised (Grill et al., 2020) learning.tion studies demonstrate the rationality and robust ness of our selfcollaborative denoising framework. 2 Related Work "
588,A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels.txt,"Learning with noisy labels is an important topic for scalable training in
many real-world scenarios. However, few previous research considers this
problem in the online setting, where the arrival of data is streaming. In this
paper, we propose a novel gradient-based approach to enable the detection of
noisy labels for the online learning of model parameters, named Online
Gradient-based Robust Selection (OGRS). In contrast to the previous sample
selection approach for the offline training that requires the estimation of a
clean ratio of the dataset before each epoch of training, OGRS can
automatically select clean samples by steps of gradient update from datasets
with varying clean ratios without changing the parameter setting. During the
training process, the OGRS method selects clean samples at each iteration and
feeds the selected sample to incrementally update the model parameters. We
provide a detailed theoretical analysis to demonstrate data selection process
is converging to the low-loss region of the sample space, by introducing and
proving the sub-linear local Lagrangian regret of the non-convex constrained
optimization problem. Experimental results show that it outperforms
state-of-the-art methods in different settings.","Online learning is a widely used learning framework for streaming data in many realworld scenarios. In recent years, online training of deep neural networks (DNNs) has garnered increased attention to enable largescale training [1 ‚Äì3], to face the challenge of increasingly large datasets. Such a largescale training process of DNNs, especially online DNNs training, is highly sensitive to the noisy labels in the datasets [4], which is more pronounced with the streaming and dynamically changing online data. The noisy label problem refers to the presence of incorrect or mislabeled annotations in a training dataset. Usually, the data samples with incorrect labels are defined as noisy data, and the correct one is called clean data. This issue has been identified as a common challenge in many datasets. For instance, researchers in [5] found 6% label errors in the Imagenet validation set and 10% label errors in the QuickDraw dataset. Similarly, up to 30% label errors were found in the Google Emotions Preprint. Under review.arXiv:2306.05046v1  [cs.LG]  8 Jun 2023dataset [6] and 37% errors in the MS COCO dataset [7]. Label errors vary across different datasets and appear with varying probabilities of occurrence in data streams at different time slots. In recent years, the robustness issue of training with noisy labels has been widely studied in different research areas [4, 8, 9]. Among various approaches, sample selection methods enjoy the flexibility to support any type of deep learning architecture and do not need to maintain additional neural networks. The concept of multiround sample selection for scalable models was first introduced in [10], where the authors proposed an iterative training loss minimization (ITLM) method that leverages samples selected at the beginning of each training epoch. Building on this idea, INCV [11] employs crossvalidation to detect noisy training data and remove largeloss samples. O2UNet [12] first repeats the entire training process to collect loss statistics, then retrains the neural network from scratch only with clean samples detected. These works proposed different methods to estimate the ratio of a dataset and using sorting methods to filter out noisy data based on that ratio, but they all follow the same idea of detecting clean samples based on a fixed preestimated scale parameter, which is hard to be set for streaming online datasets with changing clean ratios. In this paper, we introduce Online Gradientbased Robust Selection ( OGRS ), a novel gradientbased multiiteration sample selection approach that enables the online training of DNNs with dynamically changing proportions of noisy labels. Since clean data normally produces much lower training loss compared to noisy data based on the observations in [10], our proposed method capitalizes on the significant disparity in the gradient of the training loss at the clean and noisy data points respectively, which initially updates the data selection towards the clean region. To prevent the risk of overfitting, which may arise from the repeated selection of the same samples, we additionally propose a constraint function to mitigate the overlap of selected data. As a result, we formulate the problem as a nonconvex constrained optimization problem. This structure enables our approach to dynamically adapt to varying noise proportions, thereby boosting the robustness of the online DNNs training against noisy labels. In the realm of nonconvex constrained optimization, a critical unresolved issue pertains to providing a theoretical guarantee for convergence analysis. Since the recent decade, gradient descent optimization methods have been widely used to solve a wide variety of problems, like the controls of robotic systems [13], bayesian inference [14], recommendation systems [15, 16] and the training of DNNs [17‚Äì20, 45]. While [21] studied the constrained nonconvex optimization problem using quadratic approximations, a straightforward analysis for this problem remains elusive due to the computational intractability of minimizing standard regret in nonconvex cases. To address this challenge, we introduce a new metric for nonconvex constrained optimization, termed local Lagrangian regret. We conduct a detailed theoretical analysis to validate our approach and show that a constant number of updating steps ensure our method finds a balance between the sample selection performance and the computation expense. In the subsequent experimental evaluation, we incrementally input data selected by the OGRS method into various online training models. These results are then benchmarked against stateoftheart methods to demonstrate the effectiveness of our approach. In general, our main contributions are summarized below: ‚Ä¢We introduce a novel gradientbased sample selection approach designed to facilitate effec tive online DNNs training with dynamically varying clean ratios. ‚Ä¢We define a new local Lagrangian regret for the nonconvex optimization problem and propose an efficient algorithm that is specifically tailored to address the sample selection problem. ‚Ä¢We give a way for theoretical proof of the effectiveness and efficiency of our sample selection methods with our newly defined regret metric. ‚Ä¢We conduct experiments by simulating realworld online training cases and make compar isons between different sample selection methods. 2 Related Work "
589,L_DMI: An Information-theoretic Noise-robust Loss Function.txt,"Accurately annotating large scale dataset is notoriously expensive both in
time and in money. Although acquiring low-quality-annotated dataset can be much
cheaper, it often badly damages the performance of trained models when using
such dataset without particular treatment. Various methods have been proposed
for learning with noisy labels. However, most methods only handle limited kinds
of noise patterns, require auxiliary information or steps (e.g. , knowing or
estimating the noise transition matrix), or lack theoretical justification. In
this paper, we propose a novel information-theoretic loss function,
$\mathcal{L}_{DMI}$, for training deep neural networks robust to label noise.
The core of $\mathcal{L}_{DMI}$ is a generalized version of mutual information,
termed Determinant based Mutual Information (DMI), which is not only
information-monotone but also relatively invariant. \emph{To the best of our
knowledge, $\mathcal{L}_{DMI}$ is the first loss function that is provably
robust to instance-independent label noise, regardless of noise pattern, and it
can be applied to any existing classification neural networks straightforwardly
without any auxiliary information}. In addition to theoretical justification,
we also empirically show that using $\mathcal{L}_{DMI}$ outperforms all other
counterparts in the classification task on both image dataset and natural
language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a
variety of synthesized noise patterns and noise amounts, as well as a
real-world dataset Clothing1M. Codes are available at
https://github.com/Newbeeer/L_DMI .","Deep neural networks, together with large scale accurately annotated datasets, have achieved remark able performance in a great many classiÔ¨Åcation tasks in recent years ( e.g.,[11,18]). However, it is usually money and time consuming to Ô¨Ånd experts to annotate labels for large scale datasets. While collecting labels from crowdsourcing platforms like Amazon Mechanical Turk is a potential way to get annotations cheaper and faster, the collected labels are usually very noisy. The noisy labels hampers the performance of deep neural networks since the commonly used cross entropy loss is not noiserobust. This raises an urgent demand on designing noiserobust loss functions. Some previous works have proposed several loss functions for training deep neural networks with noisy labels. However, they either use auxiliary information[12,29](e.g., having an additional set of clean data or the noise transition matrix) or steps[20,33](e.g. estimating the noise transition matrix), or ‚àóEqual Contribution. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1909.03388v2  [cs.LG]  4 Nov 2019make assumptions on the noise[7,48]and thus can only handle limited kinds of the noise patterns (see perliminaries for deÔ¨Ånition of different noise patterns). One reason that the loss functions used in previous works are not robust to a certain noise pattern, say diagonally nondominant noise, is that they are distancebased, i.e., the loss is the distance between the classiÔ¨Åer‚Äôs outputs and the labels ( e.g. 01 loss, cross entropy loss). When datapoints are labeled by a careless annotator who tends to label the a priori popular class ( e.g. For medical images, given the prior knowledge is 10% malignant and 90% benign, a careless annotator labels ‚Äúbenign‚Äù when the underline true label is ‚Äúbenign‚Äù and labels ‚Äúbenign‚Äù with 90% probability when the underline true label is ‚Äúmalignant‚Äù.), the collected noisy labels have a diagonally nondominant noise pattern and are extremely biased to one class (‚Äúbenign‚Äù). In this situation, the distancedbased losses will prefer the ‚Äúmeaningless classiÔ¨Åer"" who always outputs the a priori popular class (‚Äúbenign‚Äù) than the classiÔ¨Åer who outputs the true labels. To address this issue, instead of using distancebased losses, we propose to employ information theoretic loss such that the classiÔ¨Åer, whose outputs have the highest mutual information with the labels, has the lowest loss. The key observation is that the ‚Äúmeaningless classiÔ¨Åer"" has no information about anything and will be naturally eliminated by the informationtheoretic loss. Moreover, the informationmonotonicity of the mutual information guarantees that adding noises to a classiÔ¨Åer‚Äôs output will make this classiÔ¨Åer less preferred by the informationtheoretic loss. However, the key observation is not sufÔ¨Åcient. In fact, we want an information measure I to satisfy I(classiÔ¨Åer 1‚Äôs output ;noisy labels )>I(classiÔ¨Åer 2‚Äôs output ;noisy labels ) ‚áîI(classiÔ¨Åer 1‚Äôs output ;clean labels )>I(classiÔ¨Åer 2‚Äôs output ;clean labels ): Unfortunately, the traditional Shannon mutual information (MI) does not satisfy the above formula, while we Ô¨Ånd that a generalized information measure, namely, DMI (Determinant based Mutual Information), satisÔ¨Åes the above formula. Like MI, DMI measures the correlation between two random variables. It is deÔ¨Åned as the determinant of the matrix that describes the joint distribution over the two variables. Intuitively, when two random variables are independent, their joint distribution matrix has low rank and zero determinant. Moreover, DMI is not only informationmonotone like MI, but also relatively invariant because of the multiplication property of the determinant. The relative invariance of DMI makes it satisfy the above formula. Based on DMI, we propose a noiserobust loss function LDMI which is simply LDMI(data;classiÔ¨Åer )‚à∂=‚àílog[DMI(classiÔ¨Åer‚Äôs output ;labels)]: As shown in theorem 4.1 later, with LDMI, the following equation holds: LDMI(noisy data ;classiÔ¨Åer )=LDMI(clean data ;classiÔ¨Åer )+noise amount ; and the noise amount is a constant given the dataset. The equation reveals that withLDMI, training with the noisy labels is theoretically equivalent with training with the clean labels in the dataset, regardless of the noise patterns, including the noise amounts . In summary, we propose a novel information theoretic noiserobust loss function LDMI based on a generalized information measure, DMI. Theoretically we show that LDMI is robust to instance independent label noise. As an additional beneÔ¨Åt, it can be easily applied to any existing classiÔ¨Åcation neural networks straightforwardly without any auxiliary information. Extensive experiments have been done on both image dataset and natural language dataset including FashionMNIST, CIFAR10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts as well as a realworld dataset Clothing1M. The results demonstrate the superior performance of LDMI. 2 Related Work "
590,GLAD: GLocalized Anomaly Detection via Human-in-the-Loop Learning.txt,"Human analysts that use anomaly detection systems in practice want to retain
the use of simple and explainable global anomaly detectors. In this paper, we
propose a novel human-in-the-loop learning algorithm called GLAD (GLocalized
Anomaly Detection) that supports global anomaly detectors. GLAD automatically
learns their local relevance to specific data instances using label feedback
from human analysts. The key idea is to place a uniform prior on the relevance
of each member of the anomaly detection ensemble over the input feature space
via a neural network trained on unlabeled instances. Subsequently, weights of
the neural network are tuned to adjust the local relevance of each ensemble
member using all labeled instances. GLAD also provides explanations which can
improve the understanding of end-users about anomalies. Our experiments on
synthetic and real-world data show the effectiveness of GLAD in learning the
local relevance of ensemble members and discovering anomalies via label
feedback.","DeÔ¨Ånition 1 ( Glocal )ReÔ¨Çecting or characterized by both local and global considerations1. Endusers Ô¨Ånd it easier to trust algorithms they understand and are familiar with. Such algorithms are typically built on broadly general and simplifying assumptions over the entire *Equal contribution from Ô¨Årst two authors.1School of EECS, Washington State University, Pullman, WA 99163 2School of Engineering & Computer Science, The Uni versity of Texas at Dallas, TX 75080. Correspondence to: Md Rakibul Islam <mdrakibul.islam@wsu.edu>, Shub homoy Das <shubhomoy.das@wsu.edu>, Janardhan Rao Doppa <jana.doppa@wsu.edu>, Sriraam Natarajan <Sri raam.Natarajan@utdallas.edu>. Workshop on Human in the Loop Learning at 37thInternational Conference on Machine Learning , Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s). 1https://en.wikipedia.org/wiki/Glocal (retrieved on May21 2020)feature space (i.e., global behavior), which may not be ap plicable universally (i.e., not relevant locally in some parts of the feature space) in an application domain. This observa tion is true of most machine learning algorithms including those for anomaly detection. We propose a principled tech nique referred as GLocalized Anomaly Detection (GLAD) which allows a human analyst to continue using anomaly detection ensembles with global behavior by learning their local relevance in different parts of the feature space via label feedback. Ensembles of anomaly detectors often outperform single detectors (Aggarwal & Sathe, 2017). Additionally, anoma lous instances can be discovered faster when the ensembles are used in conjunction with active learning, where a hu man analyst labels the queried instance(s) as nominal or anomaly (Veeramachaneni et al., 2016; Das et al., 2016; 2018; Siddiqui et al., 2018). A majority of the active learn ing techniques for discovering anomalies employ a weighted linear combination of the anomaly scores from the ensem ble members. This approach works well when the members are themselves highly localized, such as the leaf nodes of treebased detectors (Das et al., 2018). However, when the members of the ensemble are global (such as LODA pro jections (Pevny, 2015)), it is highly likely that individual detectors are incorrect in at least some local parts of the input feature space. To overcome this drawback, our GLAD algorithm automati cally learns the local relevance of each ensemble member in the feature space via a neural network using the label feed back from a human analyst. One interesting observation related to the key insight behind active learning with tree based models (TreeAAD) (Das et al., 2018) and GLAD is as follows: uniform prior over weights of each subspace (leaf node) in TreeAAD and uniform prior over input feature space for the relevance of each ensemble member in GLAD are highly beneÔ¨Åcial for labelefÔ¨Åcient active learning. We can consider GLAD as very similar to the TreeAAD ap proach. TreeAAD partitions the input feature space into discrete subspaces and then places a uniform prior over those subspaces (i.e., the uniform weight vector to combine ensemble scores). If we take this view to an extreme by imagining that each instance in feature space represents a subspace, we can see the connection to GLAD. While TreeAAD assigns the scores of discrete subspaces to inarXiv:1810.01403v4  [cs.LG]  15 Jul 2020GLAD: GLocalized Anomaly Detection via HumanintheLoop Learning stances (e.g., node depths for Isolation Forest), the scores assigned by GLAD are continuous, deÔ¨Åned by the global ensemble members. The relevance in GLAD is analogous to the learned weights in TreeAAD. Our GLAD technique is similar in spirit to dynamic ensem ble weighting (Jimenez, 1998). However, since we are in an active learning setting for anomaly detection, we need to consider two important aspects: (a)Number of labeled examples is very small (possibly none), and (b)To reduce the effort of the human analyst, the algorithm needs to be primed so that the likelihood of discovering anomalies is very high from the Ô¨Årst feedback iteration itself. SpeciÔ¨Åcally, we employ a neural network to predict the local relevance of each ensemble member. This network is primed with unlabeled data such that it places a uniform prior for the relevance of each ensemble member over the input feature space. In each iteration of the active learning loop, we se lect one unlabeled instance for querying, and update the weights of the neural network to adjust the local relevance of each ensemble member based on all the labeled instances. Our code and datasets are publicly available at https: //github.com/shubhomoydas/ad_examples . 2. Related Work "
591,Robust Product Classification with Instance-Dependent Noise.txt,"Noisy labels in large E-commerce product data (i.e., product items are placed
into incorrect categories) are a critical issue for product categorization task
because they are unavoidable, non-trivial to remove and degrade prediction
performance significantly. Training a product title classification model which
is robust to noisy labels in the data is very important to make product
classification applications more practical. In this paper, we study the impact
of instance-dependent noise to performance of product title classification by
comparing our data denoising algorithm and different noise-resistance training
algorithms which were designed to prevent a classifier model from over-fitting
to noise. We develop a simple yet effective Deep Neural Network for product
title classification to use as a base classifier. Along with recent methods of
stimulating instance-dependent noise, we propose a novel noise stimulation
algorithm based on product title similarity. Our experiments cover multiple
datasets, various noise methods and different training solutions. Results
uncover the limit of classification task when noise rate is not negligible and
data distribution is highly skewed.","Product classiÔ¨Åcation is a quintessential E commerce machine learning problem in which product items are placed into their respective cate gories. With recent advancements of Deep Learn ing, various unimodal (i.e., text only) and multi modal (e.g., text and image) models have been de veloped to predict larger numbers of items and cate gories with better accuracy (Gao et al., 2020; Chen et al., 2021a; Brinkmann and Bizer, 2021). How ever, one of the fundamental assumptions behind such models is the availability of large and high quality labeled datasets. Access to such datasets is usually costly or infeasible in some settings. Large product datasets usually suffer from annotation errors, i.e., products are assigned to incorrect cat egories, partially due to complex category struc ture, confusing categories and similar titles. The problem of noisy labels is even more severe when product category distribution is highly imbalanced with heavytail (Shen et al., 2012; Das et al., 2016). Therefore, a text classiÔ¨Åer which is robust to noisy labels present in training data is critical for high performing product classiÔ¨Åcation applications. While machine learning in the presence of label noise has been studied for decades, most of prior studies experimented in computer vision domain (Gu et al., 2021; Song et al., 2022), and only a few research was conducted in text classiÔ¨Åcation (Jindal et al., 2019; Garg et al., 2021). Without an annotated dataset with manuallyidentiÔ¨Åed label noise, classical approaches for label noise stimula tion assume classconditional noise (CCN) where the probability of an item having label corrupted depends on the original and noisy labels. With this assumption, all products of ‚ÄúMen‚Äôs Watches‚Äù cat egory have the sample probability to be assigned ‚ÄúWomen‚Äôs Watches‚Äù label. This is not generally correct. For instance, product titles having phrase ‚Äúmen‚Äôs watches‚Äù are less likely mislabeled. Re cent research addresses more general label noise, i.e., instancedependent noise (IDN), that an item is mislabeled with a probability depending on its original label and features. In this paper, we present a comprehensive study on improving product title classiÔ¨Åcation in the pres ence of IDN. We develop a simple yet effective Deep Neural Network for text classiÔ¨Åcation and show that our model performs well on different product title datasets ranging from small to medium sizes, balanced to skewed distributions, and tens to over a hundred categories. To generate noisy labels for experiments, our Ô¨Årst contribution is an IDN stimulation algorithm which Ô¨Çips an item‚Äôs label based on its similarity to items of other categories. Noisy label data generated by our method is comarXiv:2209.06946v1  [cs.CL]  14 Sep 2022pared with prior IDN stimulation methods for their impact to model accuracy degradation. To make the model robust to label noise, our second contri bution is a data augmentation method that reduces noise rate and thus improves model‚Äôs accuracy. We compare three stateoftheart Deep Neural Net work training algorithms to train a classiÔ¨Åer on data with label noise generated by different meth ods. From experimental results we discuss lessons learned for product title classiÔ¨Åcation in produc tion. To the best of our knowledge, this work is the Ô¨Årst time that noiseresistance model training is studied in Ecommerce domain, which is our third contribution. 2 Related Work "
592,A Gradient Mapping Guided Explainable Deep Neural Network for Extracapsular Extension Identification in 3D Head and Neck Cancer Computed Tomography Images.txt,"Diagnosis and treatment management for head and neck squamous cell carcinoma
(HNSCC) is guided by routine diagnostic head and neck computed tomography (CT)
scans to identify tumor and lymph node features. Extracapsular extension (ECE)
is a strong predictor of patients' survival outcomes with HNSCC. It is
essential to detect the occurrence of ECE as it changes staging and management
for the patients. Current clinical ECE detection relies on visual
identification and pathologic confirmation conducted by radiologists. Machine
learning (ML)-based ECE diagnosis has shown high potential in the recent years.
However, manual annotation of lymph node region is a required data
preprocessing step in most of the current ML-based ECE diagnosis studies. In
addition, this manual annotation process is time-consuming, labor-intensive,
and error-prone. Therefore, in this paper, we propose a Gradient Mapping Guided
Explainable Network (GMGENet) framework to perform ECE identification
automatically without requiring annotated lymph node region information. The
gradient-weighted class activation mapping (Grad-CAM) technique is proposed to
guide the deep learning algorithm to focus on the regions that are highly
related to ECE. Informative volumes of interest (VOIs) are extracted without
labeled lymph node region information. In evaluation, the proposed method is
well-trained and tested using cross validation, achieving test accuracy and AUC
of 90.2% and 91.1%, respectively. The presence or absence of ECE has been
analyzed and correlated with gold standard histopathological findings.","Head and neck squamous cell carcinoma (HNSCC) is one of the most com mon cancers worldwide, diagnosed in more than 550,000 patients and causing over 300,000 deaths annually (Jemal et al., 2013). For HNSCC diagnosis and treatment plan selection, CT scans are collected and analyzed to identify clin ical tumor and lymph node features (Kann et al., 2018a). In spite of modern imaging techniques, there are certain radiographic features that remain di cult to detect by clinicians, especially the presence of lymph node extracapsular extension (ECE). ECE occurs when metastatic tumor cells within the lymph node break through the nodal capsule into surrounding tissues. It is crucial to identify whether ECE occurs or not for HNSCC patient treatment management. However, current detection in practice mainly relies on the visual identication, including lymph node annotation and pathologic conrmation, which can be extremely laborintense and timeconsuming. Human errors are also inevitable. Therefore, we perform ECE identication automatically using advanced 3D deep learning technique with explainable gradientbased approach, and at the same time, not requiring manual lymph node annotation. Deep neural networks (DNNs) have demonstrated a great success in many image recognition tasks. Nevertheless, applying neural networks models on high resolution CT scans is very computational extensive. The dimension of CT scans for head and neck cancer (HNC) patients is usually 512 512 with more than 2a hundred slices. To handle such high resolution input, existing models use a combination of downsampling, dividing, and/or coarsetone schemes (Hou et al., 2016; Chlebus et al., 2018; Vorontsov et al., 2018). In this research, we perform a twostep learning scheme. The rst step is volumes of interest (VOIs) selfextraction with DNNs to provide explainable insights. The second step will train a classier with the extracted explainable VOIs. In this way, accurate ECE related lymph node regions are not demanded, and segmentation eort could be saved. Even though deep learning models have achieved impressive prediction ac curacies, their inner nonlinear structure makes them highly nontransparent to be explained by human that what information in the input data makes them actually arrive at their decisions (Samek et al., 2017). Gradientweighted class activation mapping (GradCAM) is one of the methods that look into the deep learning black boxes. The gradients of the target concept is used,  owing into the nal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept (Selvaraju et al., 2017). In this paper, we propose a novel deep learning framework, Gradient Map ping Guided Explainable Network (GMGENet), to detect ECE from 3D head and neck CT scans without requiring annotated lymph node region information. The task is to classify patients' CT scans with ECE positive/negative categories and point out the causes in the 3D CT scans. Dierent from the typically ECE detection algorithms, where the annotation of lymph node regions are required, the proposed GMGENet model includes a selfextractor, which can be trained separately to extract the explainable VOIs that are related to the ECE positive class. The selfextractor is designed based on GradCAM. Then, a DenseNet classier is trained based on the VOIs sug gested by the selfextractor and tested on an independent test set. Ground truth label has been provided for validating the selfextraction performance, and model explainability analysis are designed in the experiments. The proposed network is trained by the inputs that are highly related to 3ECE information. Therefore, extracted VOIs will contain explainable features for further classication task. In summary, the contributions of this research are highlighted as follows: (1) We propose a novel deep learning framework GMGENet with twostep learn ing scheme for ECE identication in head and neck cancer. The proposed ar chitecture includes a selfextractor, which can extract the explainable VOIs that are related to the class ECE positive. The proposed GMGENet has achieved better performance than conventional networks. (2) We train and test our proposed model on a realworld collected dataset. Lymph node label is not required in the classier, which will promote the implementation as well as improve the eciency of articial intelligence assist ECE detection. (3) To further illustrate ECE identication performance, we apply a gradient based mapping approach to generate 3D ECE probability heatmaps to pro vide visualization during the training. This technique will indicate the im portant regions related to ECE and increase the model explainability. The structure of this paper is organized as following. Section 2 provides an overview of related algorithms and applications in head and neck ECE identi cation. Section 3 describes the details of our proposed explainable GMGENet model for ECE classication. Data preparation and experimental results are illustrated in Section 4. The research ndings and future work are concluded in Section 5. 2. Related Works "
593,Detecting human and non-human vocal productions in large scale audio recordings.txt,"We propose an automatic data processing pipeline to extract vocal productions
from large-scale natural audio recordings. Through a series of computational
steps (windowing, creation of a noise class, data augmentation, re-sampling,
transfer learning, Bayesian optimisation), it automatically trains a neural
network for detecting various types of natural vocal productions in a noisy
data stream without requiring a large sample of labeled data. We test it on two
different data sets, one from a group of Guinea baboons recorded from a primate
research center and one from human babies recorded at home. The pipeline trains
a model on 72 and 77 minutes of labeled audio recordings, with an accuracy of
94.58% and 99.76%. It is then used to process 443 and 174 hours of natural
continuous recordings and it creates two new databases of 38.8 and 35.2 hours,
respectively. We discuss the strengths and limitations of this approach that
can be applied to any massive audio recording.","There is a growing number of massive continuous audio recordings made in natural environments that aim to study the vocal productions of dierent animal species. The need to collect such data is particularly important in comparative approaches. It is essential notably to further progress on the issue of language evo lution [1]. In particular, studies on the vocal productions of dierent species allow us to question hypotheses that were no longer discussed for decades [2, 3]. Beyond wild ecosystems and the study of nonhuman animals, Gilkerson et al. [4] note the importance of studying the vocal productions of human children in their natural environment with their parents. By quantifying these vocalizations over time, one can estimate the relationship between certain covariates and child development. Cabon et al. [5] have shown, for example, the value of recording and retrieving the cries of newborns in neonatal wards to ensure the proper development of these children. Similarly, ter Haar et al. [6] showed that forms of babbling, an important phase in human language development, can be found in species other than humans. For all these questions, there is a need for new methods to eciently and rapidly analyze massive audio data to further study these critical developmental phases in detail. To deal with this type of problem, deep learning approaches have proven their eciency in dierent areas to treat massive data, possibly noisy, with increasingly good results [7]. However, one problem with deep learning approaches is the need for huge amount of data and computational resources to learn the relevant information. Here we propose a complete pipeline based on deep learning neural networks that has been Corresponding Author Email address : guillem.bonafos@univamu.fr Preprint submitted to ElsevierarXiv:2302.07640v1  [cs.SD]  14 Feb 2023designed to quickly detect the target signal (i.e., vocalizations from a given species), to treat massive and noisy data, and to minimise the loss of information. The originality of this approach is multiple. The pipeline is complete and truly endtoend, from learning to prediction. It has been tested on real data and its generalizability has been evaluated on two qualitatively distinct data sets from two animal species, Guinea baboons ( Papio papio ) and human infants ( Homo sapiens ). In each case, training of the model has been done on a restricted labeled data set while predictions were done on massive records. Computations were relatively fast and done on a laptop. Finally, the model provides supplementary information about the class of each detected vocalization. In the following sections, we rst provide a review of the pattern recognition literature that address the issue of detecting and classifying vocalizations from large scale audio recordings. Second, we present the proposed pipeline. Third, we test it on two completely dierent data sets. Fourth, we show that the results reach stateoftheart performances on comparable data sets and how the method can be easily applied to other data sets. 2. Related work "
594,PENCIL: Deep Learning with Noisy Labels.txt,"Deep learning has achieved excellent performance in various computer vision
tasks, but requires a lot of training examples with clean labels. It is easy to
collect a dataset with noisy labels, but such noise makes networks overfit
seriously and accuracies drop dramatically. To address this problem, we propose
an end-to-end framework called PENCIL, which can update both network parameters
and label estimations as label distributions. PENCIL is independent of the
backbone network structure and does not need an auxiliary clean dataset or
prior information about noise, thus it is more general and robust than existing
methods and is easy to apply. PENCIL can even be used repeatedly to obtain
better performance. PENCIL outperforms previous state-of-the-art methods by
large margins on both synthetic and real-world datasets with different noise
types and noise rates. And PENCIL is also effective in multi-label
classification tasks through adding a simple attention structure on backbone
networks. Experiments show that PENCIL is robust on clean datasets, too.","DEEPlearning has shown very impressive performance on various vision problems, e.g., classiÔ¨Åcation, detec tion and semantic segmentation. Although there are many factors for the success of deep learning, one of the most important is the availability of largescale datasets with clean annotations like ImageNet [1]. However, collecting a large scale dataset with clean labels is expensive and timeconsuming. On one hand, expert knowledge is necessary for some datasets such as the Ô¨Ånegrained CUB200 [2], which demands knowledge from ornithologists. On the other hand, we can easily collect a large scale dataset with noisy annotations from various websites [3], [4], [5]. These noisy annotations can be obtained by extracting labels from the surrounding texts or using the searching keywords [6]. For a huge dataset like JFT300M (which contains 300 million images), it is impossible to manually label it and inevitably about 20% noisy labels exist in this dataset [7]. Hence, being able to deal with noisy labels is essential. The label noise problem has been studied for a long time [8], [9]. Along with the recent successes of various deep learning methods, noise handling in deep learning has gained momentum, too [6], [10], [11]. However, existing methods often have prerequisites that may not be practical in many applications, e.g., an auxiliary set with clean labels [6] or prior information about the noise [12]. Some methods are very complex [13], which hurts their deployment capability. OverÔ¨Åtting to noise is another serious difÔ¨Åculty. For a DNN with enough capacity, it can memorize the random labels [14]. Thus, some noise handling methods may Ô¨Ånally still overÔ¨Åt and their performance decline seriously, i.e., they are not robust. Their accuracies on the clean test set reach a peak This research was partially supported by the National Natural Science Foundation of China (61772256, 61422203). K. Yi, G.H. Wang and J. Wu are with the National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Email:fyik,wangguohua,wujx g@lamda.nju.edu.cn. J. Wu is the corresponding author. Manuscript receivedin the middle of the training process, but will degrade afterwards and the accuracies after the Ô¨Ånal training epoch are poor [12], [15]. We attack the label noise problem from two aspects. First, we model the label for an image as a distribution among all possible labels [16] instead of a Ô¨Åxed categorical value. This probabilistic modeling lends us the Ô¨Çexibility to handle noisecontaminated and noisefree labels in a uniÔ¨Åed manner. Second, inspired by [17], we maintain and update the label distributions in both network parameter learning (in which label distributions act as labels) and label learning (in which label distributions are updated to correct noise). Unlike [17] which updates labels simply by using the running average of network predictions, we correct noise and update our label distributions in a principled endtoend manner. The proposed framework is called PENCIL, meaning probabilistic endtoend noise correction in labels . The PENCIL framework only uses the noisy labels to initialize our label distributions, then iteratively correct the noisy labels by updating the label distributions, and the network loss function is computed using the label distributions rather than the noisy labels. Our contributions are as follows. We propose an endtoend framework PENCIL for noisy label handling. PENCIL is independent of the backbone network structure and does not need an auxiliary clean dataset or prior information about noise, thus it is easy to apply. PENCIL utilizes back propagation to probabilistically update and correct image labels in addition to updating the network parameters. To the best of our knowledge, PENCIL is the Ô¨Årst method in this line. We propose a variant of the DLDL method [16], which is essential for correcting noise contained in our label distributions. PENCIL achieves stateoftheart accuracy on datasets with both synthetic and real world noisy labels (e.g., CIFAR10, CIFAR100 and Clothing1M). We also propose an attention structure and extend the PENCIL framework to handle multi label tasks without orwith label noise.arXiv:2202.08436v1  [cs.CV]  17 Feb 20222 Unlike DLDL, we use inverse KLdivergence in our method. And we show that inverse KLdivergence is indeed more suitable for noise correction than the original KLdivergence. PENCIL is robust. It is not only robust in learning with noisy labels, but also robust enough to apply in datasets with zero or small amount of potential label noise (e.g., CUB200) to improve accuracy. A preliminary version of the PENCIL framework has appeared as a conference publication [18]. 2 R ELATED WORKS "
595,Increasing Trustworthiness of Deep Neural Networks via Accuracy Monitoring.txt,"Inference accuracy of deep neural networks (DNNs) is a crucial performance
metric, but can vary greatly in practice subject to actual test datasets and is
typically unknown due to the lack of ground truth labels. This has raised
significant concerns with trustworthiness of DNNs, especially in
safety-critical applications. In this paper, we address trustworthiness of DNNs
by using post-hoc processing to monitor the true inference accuracy on a user's
dataset. Concretely, we propose a neural network-based accuracy monitor model,
which only takes the deployed DNN's softmax probability output as its input and
directly predicts if the DNN's prediction result is correct or not, thus
leading to an estimate of the true inference accuracy. The accuracy monitor
model can be pre-trained on a dataset relevant to the target application of
interest, and only needs to actively label a small portion (1% in our
experiments) of the user's dataset for model transfer. For estimation
robustness, we further employ an ensemble of monitor models based on the
Monte-Carlo dropout method. We evaluate our approach on different deployed DNN
models for image classification and traffic sign detection over multiple
datasets (including adversarial samples). The result shows that our accuracy
monitor model provides a close-to-true accuracy estimation and outperforms the
existing baseline methods.","Deep neural networks (DNNs) have achieved unprecedent edly high classiÔ¨Åcation accuracy and found success in nu merous applications, including image classiÔ¨Åcation, speech recognition, and nature language processing. Nonetheless, training an errorfree or 100% accurate DNN is impossible in most practical cases. Inference accuracy is a crucial met ric for quantifying the performance of DNNs. Typically, the reported inference accuracy of a DNN is measured ofÔ¨Çine on test datasets with labels, but this can signiÔ¨Åcantly differ Accepted by the AISafety workshop colocated with IJCAI PRICAI 2020.from the true accuracy on a user‚Äôs dataset because of, e.g., data distribution shift away from the training dataset or even adversarial modiÔ¨Åcation to the user‚Äôs data [Cheet al. , 2019; Kull et al. , 2019; Malinin and Gales, 2018 ]. Moreover, ob taining the true accuracy is very challenging in practice due to the lack of groundtruth labels. The unknown inference accuracy has further decreased the transparency of already hardtoexplain DNNs and raised signiÔ¨Åcant concerns with their trustworthiness, especially in safetycritical applications. Consequently, studies on increas ing trustworthiness of DNNs have been proliferating. For example, many studies have considered outofdistribution (OOD) detection and adversarial sample detection, since OOD and adversarial samples often dramatically decrease in ference accuracy of DNNs [Hendrycks and Gimpel, 2017; Cheet al. , 2019; Lee et al. , 2018; Liang et al. , 2018 ]. While these efforts can offer an increased assurance of DNNs to users to some extent, they do not provide a quantitative mea sure of actual classiÔ¨Åcation accuracy, which is a more di rect and sensible measure of the target DNN‚Äôs performance. Some other studies propose (posthoc) processing to quan tify/estimate the prediction conÔ¨Ådence of a DNN [Guo et al. , 2017; Kull et al. , 2019; Snoek et al. , 2019 ]. Nonetheless, they typically require the target DNN‚Äôs training/validation dataset to train a (sometimes complicated) new transformation model for conÔ¨Ådence calibration, and do not transfer well to new un seen datasets. The accuracy of a target DNN on a user‚Äôs op erational dataset can also be estimated via selective random sampling, but it can suffer from a high estimation variance [Liet al. , 2019 ]. Contribution. In this paper, we propose a simple yet ef fective posthoc method ‚Äî accuracy monitoring ‚Äî which increases the trustworthiness of DNN classiÔ¨Åcation results by estimating the true inference accuracy on an actual (possibly OOD/adversarial) dataset. Concretely, as shown in Fig. 1, we propose a neural networkbased accuracy monitor model, which only takes the deployed DNN‚Äôs softmax probability output as its input and directly predicts if the DNN‚Äôs predic tion result is correct or not. Thus, over a sequence of pre diction samples from a user‚Äôs dataset, our accuracy monitor can form an estimate of the target DNN‚Äôs true inference ac curacy. Furthermore, we employ an ensemble of monitoring models based on the MonteCarlo dropout method, providing a robust estimate of the target DNN‚Äôs true accuracy.arXiv:2007.01472v1  [cs.LG]  3 Jul 2020Deployed  ModelModel  ProviderAccuracy Monitor User DataSoftmax Probabilities Monitor  ModelsEstimated AccuracyTraining DatasetLabled DatasetTraining1 212Figure 1: Accuracy monitoring for a deployed/target DNN. Utilizing as little information as the target DNN‚Äôs soft max probability output for accuracy estimation provides bet ter transferability than more complicated calibration methods [Kull et al. , 2019 ]. SpeciÔ¨Åcally, we can pretrain an accu racy monitor model based on a labeled dataset relevant to the target application of interest (e.g., public datasets for im age classiÔ¨Åcation). Then, for model transfer, we can selec tively label a small amount (1% in our work) of data from the user‚Äôs test dataset with active learning via an entropy acqui sition function [Beluch et al. , 2018 ], and retrain our monitor models on the selectively labeled data using transfer learn ing. In addition, without the need of accessing the target DNN‚Äôs training/validation datasets, our accuracy monitoring method can be easily applied as a plugin module on top of the target DNN to monitor its runtime performance on a va riety of datasets. Thus, our method is not restricted to the DNN providers themselves; instead, even an end user can em ploy our method to monitor the target DNN‚Äôs accuracy perfor mance on its own, bringing further increased trustworthiness of accuracy monitoring. To evaluate the effectiveness of our accuracy monitoring method, we consider different target DNN models for image classiÔ¨Åcation (10 classes and 1000 classes) and for trafÔ¨Åc sign detection in autonomous driving, respectively. Our results show that, by only utilizing the prediction class and softmax probability output of the deployed DNN model and labeling 1% of the user‚Äôs dataset, our method can monitor the healthy of the target DNN models, providing a remarkably accurate estimation of the true classiÔ¨Åcation accuracy on a variety of user‚Äôs datasets. 2 Related Works "
596,Towards Label-free Scene Understanding by Vision Foundation Models.txt,"Vision foundation models such as Contrastive Vision-Language Pre-training
(CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot
performance on image classification and segmentation tasks. However, the
incorporation of CLIP and SAM for label-free scene understanding has yet to be
explored. In this paper, we investigate the potential of vision foundation
models in enabling networks to comprehend 2D and 3D worlds without labelled
data. The primary challenge lies in effectively supervising networks under
extremely noisy pseudo labels, which are generated by CLIP and further
exacerbated during the propagation from the 2D to the 3D domain. To tackle
these challenges, we propose a novel Cross-modality Noisy Supervision (CNS)
method that leverages the strengths of CLIP and SAM to supervise 2D and 3D
networks simultaneously. In particular, we introduce a prediction consistency
regularization to co-train 2D and 3D networks, then further impose the
networks' latent space consistency using the SAM's robust feature
representation. Experiments conducted on diverse indoor and outdoor datasets
demonstrate the superior performance of our method in understanding 2D and 3D
open environments. Our 2D and 3D network achieves label-free semantic
segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%,
respectively. And for nuScenes dataset, our performance is 26.8% with an
improvement of 6%. Code will be released
(https://github.com/runnanchen/Label-Free-Scene-Understanding).","Scene understanding aims to recognize the semantic information of objects within their contextual environment, which is a fundamental task for autonomous driving, robot navigation, digital city, etc. Existing methods have achieved remarkable advancements in 2D and 3D scene understanding [ 1‚Äì 10]. However, they heavily rely on extensive annotation efforts and often struggle to identify novel object categories that were not present in the training data. These limitations hinder their practical applicability in realworld scenarios where acquiring highquality labelled data can be expensive and novel objects may appear [ 11‚Äì17]. Consequently, labelfree scene understanding, which aims to perform semantic segmentation in realworld environments without requiring labelled data, emerges as a highly valuable yet relatively unexplored research topic. Vision foundation models, e.g., Contrastive VisionLanguage Pretraining (CLIP) [ 18] and Segment Anything (SAM) [ 19], have garnered significant attention due to their remarkable performance in addressing openworld vision tasks. CLIP, trained on a largescale collection of freely available image text pairs from websites, exhibits promising capabilities in openvocabulary image classification. On the other hand, SAM learns from an extensive dataset comprising 1 billion masks across 11 million 1https://github.com/runnanchen/LabelFreeSceneUnderstanding . Preprint. Under review.arXiv:2306.03899v1  [cs.CV]  6 Jun 2023CLIPSAM Noisy pseudolabels Clean object masksImagesLabelfree 3D scene understandingLabelfree 2D scene understandingTransfer  to 3D3D2D 3D ground truth2D ground truthFigure 1: We study how vision foundation models enable networks to comprehend 2D and 3D environments without relying on labelled data. To accomplish this, we introduce a novel framework called Crossmodality Noisy Supervision (CNS). By effectively harnessing the strengths of CLIP and SAM, our approach simultaneously trains 2D and 3D networks, yielding remarkable performance. images, achieving impressive zeroshot image segmentation performance. However, initially designed for image classification, CLIP falls short in segmentation performance. Conversely, SAM excels in zeroshot image segmentation but lacks object semantics (Fig. 1). Additionally, both models are trained exclusively on 2D images without exposure to any 3D modal data. Given these considerations, a natural question arises: Can the combination of CLIP and SAM imbue both 2D and 3D networks with the ability to achieve labelfree scene understanding in realworld open environments? Despite recent efforts [ 20] leverage CLIP for imagebased semantic segmentation, the pseudo labels generated by CLIP for individual pixels often exhibit significant noise, resulting in unsatisfactory performance. The uptodate work [ 17] has extended this method to encompass 3D labelfree scene understanding by transferring 2D knowledge to 3D space via projection. However, the pseudo labels assigned to 3D points are considerably noisier due to calibration errors, significantly limiting the accuracy of the networks. The primary challenge of utilizing vision foundation models for scene understanding is effectively supervising the networks using exceptionally noisy pseudo labels. Inspired by SAM‚Äôs impressive zeroshot segmentation capabilities, we propose a novel Crossmodality Noisy Supervision (CNS) framework incorporating CLIP and SAM to train 2D and 3D networks simultaneously. Specifically, we employ CLIP to densely pseudolabel 2D image pixels and transfer these labels to 3D points using the calibration matrix. Since pseudolabels are extremely noisy, leading to unsatisfactory performance, we refine pseudolabels using SAM‚Äôs masks, producing more reliable pseudolabels for supervision. To further mitigate error propagation and prevent the networks from overfitting the noisy labels, we consistently regularize the network predictions, i.e., cotraining the 2D and 3D networks using the randomly switched pseudo labels, where the labels are from the prediction of 2D, 3D, and CLIP networks. Moreover, considering that individual objects are well distinguished in SAM feature space, we use the robust SAM feature to consistently regularize the latent space of 2D and 3D networks, which aids networks in producing semantic predictions with more precise boundaries and further reduces label noise. Note that the SAM feature space is frozen during training, thus severed as the anchor metric space for aligning the 2D and 3D features. To verify the labelfree scene understanding capability of our method in 2D and 3D real open worlds, we conduct experiments on indoor and outdoor datasets, i.e., ScanNet and nuScenes, where 2D and 3D data are simultaneously collected. Extensive results show that our method significantly outperforms stateoftheart methods in understanding 2D and 3D scenes without training on any labelled data. Our 2D and 3D network achieves labelfree semantic segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%, respectively. And for the nuScenes dataset, the performance on 3D semantic segmentation is 26.8% mIoU with the improvement of 6%. Quantitative and qualitative ablation studies also demonstrate the effectiveness of each module in our method. The key contributions of our work are summarized as follows. ‚Ä¢We present the first work that incorporating CLIP and SAM for labelfree scene understand ing in 2D and 3D realworld open environments. 2‚Ä¢We propose a novel Crossmodality Noisy Supervision framework to effectively and syn chronously train 2D and 3D networks with severe label noise, including prediction consis tency and latent space consistency regularization schemes. ‚Ä¢Experiments conducted on indoor and outdoor datasets show that our method significantly outperforms stateoftheart methods on 2D and 3D semantic segmentation tasks. 2 Related Work "
597,Generalized Negative Correlation Learning for Deep Ensembling.txt,"Ensemble algorithms offer state of the art performance in many machine
learning applications. A common explanation for their excellent performance is
due to the bias-variance decomposition of the mean squared error which shows
that the algorithm's error can be decomposed into its bias and variance. Both
quantities are often opposed to each other and ensembles offer an effective way
to manage them as they reduce the variance through a diverse set of base
learners while keeping the bias low at the same time. Even though there have
been numerous works on decomposing other loss functions, the exact mathematical
connection is rarely exploited explicitly for ensembling, but merely used as a
guiding principle. In this paper, we formulate a generalized bias-variance
decomposition for arbitrary twice differentiable loss functions and study it in
the context of Deep Learning. We use this decomposition to derive a Generalized
Negative Correlation Learning (GNCL) algorithm which offers explicit control
over the ensemble's diversity and smoothly interpolates between the two
extremes of independent training and the joint training of the ensemble. We
show how GNCL encapsulates many previous works and discuss under which
circumstances training of an ensemble of Neural Networks might fail and what
ensembling method should be favored depending on the choice of the individual
networks. We make our code publicly available under
https://github.com/sbuschjaeger/gncl","Ensemble algorithms offer state of the art performance in many Machine Learning applications and often outperform single classiÔ¨Åers by a large margin. One of the main theoret 1ArtiÔ¨Åcial Intelligence Group, TU Dortmund Univer sity, Germany. Correspondence to: Sebastian Buschj ¬®ager <sebastian.buschjaeger@tudortmund.de >.ical driving forces behind the understanding of ensembles is the biasvariance decomposition. The biasvariance decom position decomposes the algorithm‚Äôs error into two additive parts ‚Äì its bias and its variance. Hence, a good algorithm should try to minimize both at the same time which often leads to a difÔ¨Åcult balancing act between the two quanti ties. Ensemble algorithms are wellknown to reduce the variance if a diverse set of base models is trained while also keeping the bias low making them such an effective class of algorithms. The biasvariance decomposition has been mathematically proven for the meansquared error, which sparked a plethora of different ensembling algorithms for different loss functions exploiting the general notion of ‚Äòdi versity‚Äô in ensemble construction (Webb, 2000; Geurts et al., 2006; Brown et al., 2005; Melville & Mooney, 2005; Lee et al., 2015; Zhou & Feng, 2017; Dvornik et al., 2019). In terestingly, even though there have been numerous works on decomposing other loss functions such as 0‚àí1loss or exponential families (e.g. loglikelihood loss), none of these theoretical insights have directly inspired new learning al gorithms. The general notion of diversity in an ensemble is still one of the main driving forces in designing new ensem bling algorithms, while the exact mathematical connection is rarely exploited explicitly. We argue, that diversity can be hurtful sometimes and must be controlled with respect to the base learners. To do so, we formulate a generalized biasvariance decomposition and study it in the context of Deep Learning. From this decompo sition, we derive two different algorithmic extremes: Either, we train the entire ensemble jointly in an endtoend fashion or we train each model completely independent from each other. We present a generalization of Negative Correlation Learning (GNCL) that smoothly interpolates between these two extremes and thus can capitalize on the entire spectrum of methods inbetween. We show, how GNCL generalizes many existing ensembling techniques in a single framework and use it to explore under which circumstances training of an ensemble might fail and what ensembling methods should be favored depending on the choice of the individual networks. Our contributions are: ‚Ä¢A Generalized BiasVariance Decomposition: We present the Ô¨Årst biasvariance decomposition for arbi trary twice differentiable loss functions.arXiv:2011.02952v2  [cs.LG]  9 Dec 2020Generalized Negative Correlation Learning for Deep Ensembling ‚Ä¢Generalized Negative Correlation Learning: From this decomposition we derive Generalized Negative Correlation Learning (GNCL) and show how it gener alizes existing NCLlike algorithms into a single frame work. ‚Ä¢Experimental evaluation: We compare our ap proach against stateoftheart ensemble algorithms for Deep Learning methods. We show how GNCL smoothly interpolates between different ensembling techniques offering the overall best performance. Our code is available under https://github.com/ sbuschjaeger/gncl . ‚Ä¢Explanation of results: Our theoretical results ac curately explain when certain ensembling methods should be favored over others: For small capacity Neu ral Networks, EndtoEnd learning should be favored, whereas, for larger capacity models, ensembling should shift towards independent training of the individual models. The paper is organized as follows: The next section surveys related work and focuses on the biasvariance decomposi tion as well as ensembling methods in the realm of Deep Learning. Section 3 then derives the biasvariance decompo sition, whereas section 4 formalizes it into the Generalized Negative Correlation Learning algorithm. In section 5 we experimentally evaluate our method and section 6 concludes the paper. 2. Related Work "
598,Joint Binary Neural Network for Multi-label Learning with Applications to Emotion Classification.txt,"Recently the deep learning techniques have achieved success in multi-label
classification due to its automatic representation learning ability and the
end-to-end learning framework. Existing deep neural networks in multi-label
classification can be divided into two kinds: binary relevance neural network
(BRNN) and threshold dependent neural network (TDNN). However, the former needs
to train a set of isolate binary networks which ignore dependencies between
labels and have heavy computational load, while the latter needs an additional
threshold function mechanism to transform the multi-class probabilities to
multi-label outputs. In this paper, we propose a joint binary neural network
(JBNN), to address these shortcomings. In JBNN, the representation of the text
is fed to a set of logistic functions instead of a softmax function, and the
multiple binary classifications are carried out synchronously in one neural
network framework. Moreover, the relations between labels are captured via
training on a joint binary cross entropy (JBCE) loss. To better meet
multi-label emotion classification, we further proposed to incorporate the
prior label relations into the JBCE loss. The experimental results on the
benchmark dataset show that our model performs significantly better than the
state-of-the-art multi-label emotion classification methods, in both
classification performance and computational efficiency.","Multilabel emotion classiÔ¨Åcation, is a subtask of the text emotion classiÔ¨Åcation, which aims at identifying the coexisting emotions (such as joy, anger and anxiety, etc.) expressed in the text, has gained much attention due to its wide potential applications. Taking the following sentence Example 1: ‚Äú Feeling the warm of her hand and the attach ment she hold to me, I couldn‚Äôt afford to move even a little, fearing I may lost her hand ‚Äùfor instance, the coexisting emotions expressed in it contain joy,love, and anxiety . Traditional multilabel emotion classiÔ¨Åcation methods nor mally utilize a twostep strategy, which Ô¨Årst requires to de velop a set of handcrafted expert features (such as bagof words, linguistic features, emotion lexicons, etc.), and then makes use of multilabel learning algorithms [Xuet al. , 2012; Liet al. , 2015; Wang and Pal, 2015; Zhou et al. , 2016; Yan and Turtle, 2016 ]for multilabel classiÔ¨Åcation. How ever, the work of feature engineering is laborintensive and timeconsuming, and the system performance highly depend s on the quality of the manually designed feature set. In recent years, deep neural networks are of growing attention due to their capacity of automatically learn the internal representa tions of the raw data and integrating feature representation learning and classiÔ¨Åcation into one endtoend framework. Existing deep learning methods in multilabel classiÔ¨Åca tion can be roughly divided into two categories: ‚Ä¢Binary relevance neural network (BRNN), which con structs an independent binary neural network for each label, where multilabel classiÔ¨Åcation is considered as a set of isolate binary classiÔ¨Åcation tasks and the predic tion of the label set is composed of independent predic tions for individual labels. ‚Ä¢Threshold dependent neural network (TDNN), which normally constructs one neural network to yield the probabilities for all labels via a softmax function, where the probabilities sum up to one. Then, an additional threshold mechanism (e.g., the calibrated label ranking algorithm) is further needed to transform the multiclass probabilities to multilabel outputs. The structure of BRNN and TDNN are shown in Figure 1 (a) and (b), respectively. However, both kinds of methods have their shortcomings. The former one, BRNN, usually known in the literature as binary relevance (BR) transformation [Spyromitros et al. , 2008 ], not only ignores dependencies between labels, but al so consumes much more resources due to the need of training a unique classiÔ¨Åer and make prediction for each label. The latter one, TDNN, although has only one neural network, can only yield the category probabilities of all class labels. In stead, it needs an additional threshold function mechanism to transform the category probabilities to multilabel outputs. arXiv:1802.00891v1  [cs.LG]  3 Feb 2018InputLayerProbabilityOutputsMultilabelOutputs (a)	Binary	Relevance	NeuralNetwork(BRNN)(b)	Threshold	Dependent	 Neural	Network(TDNN)(c)	Our	Proposed	 Joint	 Binary	Neural	Network	 (JBNN)NeuralNetworkArgmaxArgmaxThresholdDecisionFigure 1: Different ways of constructing neural networks for multilabel classiÔ¨Åcation. However, building an effective threshold function is also full of challenges for multilabel learning [Zhang and Zhou, 2006; Read and PerezCruz, 2014; Nam et al. , 2014; Xu et al. , 2017; Lenc and Kr ¬¥al, 2017 ]. In this paper, we propose a simple joint binary neural net work (JBNN), to address these two problems. We display the structure of JBNN in Figure 1 (c). As can be seen, in JBNN, the bottom layers of the network are similar to that in TND D. SpeciÔ¨Åcally, we employ a Bidirectional Long ShortTerm Memory (BiLSTM) structure to model the sentence. The attention mechanism is also constructed to get the sentence representation. After that, instead of a softmax function used in TDNN, we feed the representation of a sentence to mul tiple logistic functions to yield a set of binary probabilities. That is, for each input sentence, we conduct multiple binary classiÔ¨Åcations synchronously in one neural network frame work. Different from BRNN, the word embedding, LSTMs, and the sentence representation are shared among the multi ple classiÔ¨Åcation components in the network. Moreover, the relations between labels are captured based on a joint binary learning loss. Finally, we convert the multivariate Bernoulli distributions into multilabel outputs, the same as BRNN. The JBNN model is trained based on a joint binary cross entropy (JBCE) loss. To better meet the multilabel emotion classiÔ¨Å cation task, we further proposed to incorporate the prior label relations into the JBCE loss. We evaluate our JBNN model on the widelyused multilabel emotion classiÔ¨Åcation dataset RenCECps [Quan and Ren, 2010 ]. We compare our mod el with both traditional methods and neural networks. The experimental results show that: ‚Ä¢Our JBNN model performs much better than the state oftheart traditional multilabel emotion classiÔ¨Åcation methods proposed in recent years; ‚Ä¢In comparison with the BRNN and TDNN systems, our JBNN model also shows the priority, in both classiÔ¨Åca tion performance and computational efÔ¨Åciency.2 Related Work "
599,End-to-end Semantic Role Labeling with Neural Transition-based Model.txt,"End-to-end semantic role labeling (SRL) has been received increasing
interest. It performs the two subtasks of SRL: predicate identification and
argument role labeling, jointly. Recent work is mostly focused on graph-based
neural models, while the transition-based framework with neural networks which
has been widely used in a number of closely-related tasks, has not been studied
for the joint task yet. In this paper, we present the first work of
transition-based neural models for end-to-end SRL. Our transition model
incrementally discovers all sentential predicates as well as their arguments by
a set of transition actions. The actions of the two subtasks are executed
mutually for full interactions. Besides, we suggest high-order compositions to
extract non-local features, which can enhance the proposed transition model
further. Experimental results on CoNLL09 and Universal Proposition Bank show
that our final model can produce state-of-the-art performance, and meanwhile
keeps highly efficient in decoding. We also conduct detailed experimental
analysis for a deep understanding of our proposed model.","Semantic role labeling (SRL), as one of the core tasks to identify the semantic predicates in text as well as their se mantic roles, has sparked much interest in natural language processing (NLP) community (Pradhan et al. 2005; Lei et al. 2015; Xia et al. 2019). SRL is a shallow semantic parsing, aiming to uncover the predicateargument structures, such as ‚Äòwho did what to whom, when and where ‚Äô, The task can be beneÔ¨Åcial for a range number of downstream tasks, such as information extraction (Christensen et al. 2011; Bastianelli et al. 2013), question answering (Shen and Lapata 2007; Be rant et al. 2013) and machine translation (Xiong, Zhang, and Li 2012; Shi et al. 2016). Traditionally, SRL is accomplished via two pipeline steps: predicate identiÔ¨Åcation (Scheible 2010) and argument role labeling (Pradhan et al. 2005). More recently, there is grow ing interest in endtoend SRL, which aims to achieve both two subtasks by a single model (He et al. 2018). Given a sentence, the goal is to recognize all possible predicates to gether with their arguments jointly. Figure 1 shows an exam ple of endtoend SRL. The endtoend joint architecture can *Corresponding author Copyright ¬© 2021, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved. A1ATMPA3 A1 When victims and witnesses stay silent , nothing changes (2)(1) stay.01 change.01 1                2             3                 4                5            6       7          8                   9Figure 1: An example of endtoend SRL, where two propo sitions are shown in one sentence. greatly alleviate the error propagation problem, and mean while simplify the overall decoding process, thus receives increasing attention. Graphbased models have been the mainstream methods to endtoend SRL, which are achieved by enumerating all the possible predicates and their argu ments exhaustively (He et al. 2018; Cai et al. 2018; Li et al. 2019). Their results show that endtoend modeling can ob tain better SRL performance. Alternatively, the transitionbased framework offers an other solution for endtoend modeling, which is totally or thogonal to the graphbased models. Transitionbased mod els have been widely exploited for endtoend sequence la beling (Zhang and Clark 2010; Lyu, Zhang, and Ji 2016; Zhang, Zhang, and Fu 2018), structural parsing (Zhou et al. 2015; Dyer et al. 2015; Yuan, Jiang, and Tu 2019) and rela tion extraction (Wang et al. 2018; Zhang et al. 2019), which are closely related to SRL. These models can also achieve very competitive performances for a range of tasks, and meanwhile maintain high efÔ¨Åciencies with lineartime de coding complexity. In this work, we present the Ô¨Årst work of exploiting the neural transitionbased architecture to endtoend SRL. The model handles SRL incrementally by predicting a sequence of transition actions step by step, which are used to detect all predicates as well as their semantic roles in a given sentence. The two subtasks of endtoend SRL, predicate identiÔ¨Åca tion and argument role labeling, are performed mutually in a single model to make full interactions of them. For argument role labeling, the recognition is conducted in a closeÔ¨Årst way, where the nearpredicate roles are processed Ô¨Årst. The partial outputs of the incremental processing are denoted as transition states. In addition, we suggest explicit highorder compositions to enhance our transitionbased model, leverarXiv:2101.00394v1  [cs.CL]  2 Jan 2021aging the precedent partiallyrecognized argumentpredicate structures for the current action classiÔ¨Åcation. Our neural transition system is built upon standard embeddingbased word representations, and then is im proved with dependencyaware representations by using re cursive TreeLSTM (Tai, Socher, and Manning 2015). Con cretely, we embed the surface words, characters, POS tags and dependency structures as input representations. Dur ing decoding, we represent the transition states by using standard BiLSTM (Hochreiter and Schmidhuber 1997) and StackLSTM (Dyer et al. 2015) to encode the elements in buffers and stacks, respectively. Finally, we predict transi tion actions incrementally based on the state representations. We conduct experiments on dependencybased SRL benchmarks, including CoNLL09 (Haji Àác et al. 2009) for the English language, and Universal Proposition Bank (Akbik et al. 2015; Akbik and Li 2016) for seven other languages. Our endtoend neural transition model wins the best re sults against the baselines, giving the stateoftheart perfor mances on both the predicate identiÔ¨Åcation and argument role labeling, meanwhile keeping efÔ¨Åcient on decoding. We also show that with recent contextualized word representa tions, e.g., ELMo (Devlin et al. 2019), BERT (Peters et al. 2018) or XLNet (Yang et al. 2019), the overall SRL perfor mances can be further improved. Indepth analysis is con ducted to uncover the important components of our Ô¨Ånal model, which can help comprehensive understanding of our model. Following we summarize our contributions: We Ô¨Åll the gap in the literature of employing neural transitionbased model for endtoend SRL. We also en hance the parsing procedure with a closeÔ¨Årst scheme. We compose the highorder features (i.e., with one more predicaterole attachments from multiple predicates) in our transition framework for endtoend SRL to model long term substructure information explicitly. Our transition framework wins new stateoftheart performances against all current graphbased methods on benchmark datasets, meanwhile being faster on decoding. 2 Related Work "
600,Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network.txt,"This paper proposes a neural network architecture and training scheme to
learn the start and end time of sound events (strong labels) in an audio
recording given just the list of sound events existing in the audio without
time information (weak labels). We achieve this by using a stacked
convolutional and recurrent neural network with two prediction layers in
sequence one for the strong followed by the weak label. The network is trained
using frame-wise log mel-band energy as the input audio feature, and weak
labels provided in the dataset as labels for the weak label prediction layer.
Strong labels are generated by replicating the weak labels as many number of
times as the frames in the input audio feature, and used for strong label layer
during training. We propose to control what the network learns from the weak
and strong labels by different weighting for the loss computed in the two
prediction layers. The proposed method is evaluated on a publicly available
dataset of 155 hours with 17 sound event classes. The method achieves the best
error rate of 0.84 for strong labels and F-score of 43.3% for weak labels on
the unseen test split.","Sound event detection (SED) is the task of recognizing sound events and its respective start and end timings in an audio recording. Rec ognizing such sound events and its temporal information can be use ful in different applications such as surveillance [1, 2], biodiversity monitoring [3, 4] and query based multimedia retrieval [5]. Tra ditionally, SED has been tackled with datasets that have temporal information for each of the sound event present [6, 7]. We refer to such temporal information of sound events as strong labels in this paper. The internet has a vast collection of audio data. Many collab orative and social websites like Freesound1and YouTube2allow users to upload multimedia with metadata like captions and tags. We can potentially automate the collection of audio data associated with a given tag from these online sources in considerably less time and manual effort. Recently, Gemekke et al. [8] carried out this with 632 sound event tags on YouTube and collected nearly two The research leading to these results has received funding from the Eu ropean Research Council under the European Unions H2020 Framework Programme through ERC Grant Agreement 637422 EVERYSOUND. The authors also wish to acknowledge CSCIT Center for Science, Finland, for computational resources. 1https://freesound.org/ 2https://www.youtube.com/million 10 second audio recordings. While these tags indicate that the sound event is present in the audio recording, the tags do not contain the information as to how many times they occur or at what time they occur. In this paper, we call such tags without any tempo ral information as weak labels. The task of identifying weak labels of an audio is also referred as audio tagging in literature [9, 10]. Collecting and annotating data with strong labels to train SED methods is a timeconsuming task involving a lot of manual labor. On the other hand, collecting weakly labeled data takes much less time to annotate manually, since the annotator has to mark only the active sound event classes and not its exact time boundaries. If we can build SED methods which can learn strong labels from such weakly labeled data, then the methods can learn on a large amount of data. In this paper, we propose to implement such a strong label learning SED method using weakly labeled training data. Similar research of using weakly labeled data to learn strong labels has been done in neighboring audio domains such as mu sic [11, 12], and bird classiÔ¨Åcation [13, 14]. Liu et al. [11] used a fully convolutional neural network (FCN) to recognize instru ments and tempo for each time frame of an audio clip given only the clip level information. They further extended this network to sound event detection [15] and experimented on publicly available datasets. The advantage of using an FCN is it can handle audio input of any length. On the other hand, the limitation is that the frame wise strong labels are obtained by an upscaling layer which repli cates segmentwise output to as many number of frames required. Similar FCN as [15] was proposed in [16] without the upscaling layer, thereby estimating labels for short segments of length 1.5 s instead of frame wise labels. The study compares the performance of this FCN with a VGGlike network [17] like network which out puts sound event labels in segments of 1.5 s. The FCN network is trained using the entire audio, and its respective weak label. On the other hand, the VGG network is trained on subsegments of the en tire audio, assuming that the recording level weak label annotation remains the same in all its subsegments. The study showed that using an FCN performs better SED than using the VGG method. Kumar et al. [18] proposed a multiple instance learning (MIL) ap proach [19] for this task, though the results were promising the ap proach was claimed to be not scalable to large datasets by the same authors in [16]. Sound events in real life most often overlap each other. A SED method which can recognize such overlapping sound events is re ferred as polyphonic SED method. The state of the art for poly phonic SED, trained using strong labels, was proposed recently in [20], where log melband energy feature was used along with a stacked convolutional and recurrent neural network and evaluated on multiple datasets. Similar stacked convolutional and recurrent neural network has also been shown to outperform state of the artarXiv:1710.02998v1  [cs.SD]  9 Oct 2017Detection and ClassiÔ¨Åcation of Acoustic Scenes and Events 2017 16 November 2017, Munich, Germany methods in audio tagging tasks [9, 10]. Motivated by the perfor mance of this method in SED and audio tagging, in this paper, we propose to extend the method to perform both SED and audio tag ging together, given only the audio and its respective weak labels. In particular, we use the log melband audio feature extracted from the audio and extend the stacked convolutional and recurrent neural network to predict two outputs sequentially, the strong followed by the weak labels. To train the proposed network we generate dummy strong labels by replicating the weak labels as many times as the number of frames in the audio input feature. We further propose to control the information that the network learns by separately scaling the loss calculated in the weak and strong prediction layers. Networks similar to the proposed stacked convolutional and neural network are the current state of the arts for audio tag ging [9, 10]. This shows that the architecture is capable of learn ing the relevant information in temporal domain and mapping it to active classes. In this paper, we show that the proposed training scheme can extract this temporal information that the network is learning in the intermediate layers and can be used as strong labels. In comparison to previous works [15, 16], the proposed method sup ports higher time resolution for strong labels by its inherent design. The feature extraction and the proposed network is described in Section 2. The dataset, metric and evaluation procedure is discussed in Section 3. Finally, the results and discussions of the evaluation performed are presented in Section 4. 2. METHOD "
601,Cell Detection from Imperfect Annotation by Pseudo Label Selection Using P-classification.txt,"Cell detection is an essential task in cell image analysis. Recent deep
learning-based detection methods have achieved very promising results. In
general, these methods require exhaustively annotating the cells in an entire
image. If some of the cells are not annotated (imperfect annotation), the
detection performance significantly degrades due to noisy labels. This often
occurs in real collaborations with biologists and even in public data-sets. Our
proposed method takes a pseudo labeling approach for cell detection from
imperfect annotated data. A detection convolutional neural network (CNN)
trained using such missing labeled data often produces over-detection. We treat
partially labeled cells as positive samples and the detected positions except
for the labeled cell as unlabeled samples. Then we select reliable pseudo
labels from unlabeled data using recent machine learning techniques;
positive-and-unlabeled (PU) learning and P-classification. Experiments using
microscopy images for five different conditions demonstrate the effectiveness
of the proposed method.","Cell detection is an essential task in cell image analysis, which has been widely used for cell counting and cell tracking. Many image processing based methods have been proposed for automatically detecting cells e.g., using a threshold ing [13,24,3], region growing [25] and graph cuts [1]. These methods are usually designed on the basis of image characteristics, so they may only work under certain conditions. Recently, deep learningbased detection methods have shown to be eective for various types of cells if they are trained on enough training data for specic conditions [22,10,6,12]. Moreover, deep learningbased methods usually require fully annotating all the cells in an entire image for the network to learn both the foreground (cell) and the background area. If some of the cells are not anno tated ( i.e., imperfect annotation), the nonannotated cell regions are mistakenlyarXiv:2107.09289v2  [cs.CV]  21 Jul 20212 K. Fujii et al. treated as background regions in training. The detection performance signi cantly degrades due to such noisy labels. However, it is very costly to annotate all the cells in an image since there are as many as hundreds or thousands of cells in an image. Therefore, some of the current public datasets only provide partially annotated cells (imperfect annotation). The aim of our study is to make cell detection feasible from imperfect an notated data by using pseudo labeling. This would enable the use of imperfect datasets, which have already been made public, and reduce the annotation cost for biologists in real applications. In the proposed method, we rst create a mask that covers only the annotated cells, in which the loss is ignored outside the mask. The masked loss facilitates the reduction of false negatives but produces many false positives since it learns part of the foreground (cell) region but not the background. When the detector is applied to the dataset that contains par tially labeled cells (positive data), the detected positions excluding the labeled cell can be considered unlabeled data, which may contain both positive (cell) and negative (background) positions. To address the overdetection problem, we propose a semisupervised method that selects reliable background as pseudo negative labels and additional foreground regions as pseudo positive labels from the unlabeled data and adds these to the training data in the next step. We applied positive and unlabeled (PU) learning [9] to extract the optimal image features that separate the feature distribution of positive (cell) and negative la bel (background). In order to minimize the risk of selecting incorrect labels as the pseudo labels, we performed ranking learning using Pclassication [5] which aims to learn a ranking (scoring) function so that reliable positive samples are ranked higher. These processes are iteratively performed. This improves the per formance of the detector network by adding reliable labels for both negative and positive positions. The experiments using microscopy images for ve dierent conditions demonstrate the eectiveness of the proposed method. 2 Related work "
602,Stochastic Generalized Adversarial Label Learning.txt,"The usage of machine learning models has grown substantially and is spreading
into several application domains. A common need in using machine learning
models is collecting the data required to train these models. In some cases,
labeling a massive dataset can be a crippling bottleneck, so there is need to
develop models that work when training labels for large amounts of data are not
easily obtained. A possible solution is weak supervision, which uses noisy
labels that are easily obtained from multiple sources. The challenge is how
best to combine these noisy labels and train a model to perform well given a
task. In this paper, we propose stochastic generalized adversarial label
learning (Stoch-GALL), a framework for training machine learning models that
perform well when noisy and possibly correlated labels are provided. Our
framework allows users to provide different weak labels and multiple
constraints on these labels. Our model then attempts to learn parameters for
the data by solving a non-zero sum game optimization. The game is between an
adversary that chooses labels for the data and a model that minimizes the error
made by the adversarial labels. We test our method on three datasets by
training convolutional neural network models that learn to classify image
objects with limited access to training labels. Our approach is able to learn
even in settings where the weak supervision confounds state-of-the-art weakly
supervised learning methods. The results of our experiments demonstrate the
applicability of this approach to general classification tasks.","Recent success of deep learning has seen an explosion in interest towards building largescale models for various ap plications. Training deep models often involves using mas sive amounts of training data whose labels are not easily obtained or available. Collecting labeled data for training these largescale models is a major bottleneck since these labels are usually provided by expert annotators and can be expensive to gather. Weak supervision offers an alternative for training machine learning models because it relies on ap proximate labels that are easily obtained. Weakly supervised learning alleviates some of the difÔ¨Åculties and cost associ ated with supervised learning by only requiring annotators Copyright c 2020, Association for the Advancement of ArtiÔ¨Åcial Intelligence (www.aaai.org). All rights reserved.to provide rules or approximate indicators that automatically label the data. Ideally, these annotators or human experts provide several weak rules in the form of feature annotations, heuristic patterns, or programmatically generated labels. The difÔ¨Åculty lies in combining multiple weak supervision sig nals that may be from various sources, make dependent er rors, or sometimes have different noise levels within them. Researchers have developed methods (Ratner et al .2017; Ratner et al .2016; Arachie and Huang 2019) to combine the different weak supervision signals to train models robust to redundancies and errors in the weak supervision. These ap proaches are limited in ways such as being overly optimistic about the independence of weak signals, being only able to handle binary classiÔ¨Åcation, or having poor scalability. We address these limitations in this paper. We develop a general framework that encodes multiple linear constraints on the weak supervision signals. Our algo rithm learns from the weak supervision by solving a nonzero sum game between an adversary and the model. By using a nonzero sum game, our formulation provides more Ô¨Çex ibility than previous methods and allows for different loss functions to be used. Thus, we enable learning for other forms such as multiclass classiÔ¨Åcation, multilabel classiÔ¨Åcation, and structured prediction. Our framework is stochastic and uses different forms of weak supervision, thus making it possi ble to train large scale models like deep neural networks. In summary, we make the following contributions : We deÔ¨Åne a general framework for adversarial label learn ing that uses multiple linear constraint forms. We develop a stochastic approach that enables training for largescale models, e.g., deep neural networks. Our algorithm provides greater Ô¨Çexibility by solving a non zero sum game thus allowing learning of different forms. Our evaluation tests weak supervision provided in different forms, either as weak rules from human experts, heuristic patterns, or programmatically generated labels. We validate our approach on three image classiÔ¨Åcation datasets. We use error and precision constraints to solve mul ticlass image classiÔ¨Åcation tasks for deep neural network models. In the experiments, we provide weak supervision sig nals that are both generated by humans and programmaticallyarXiv:1906.00512v2  [cs.LG]  18 Sep 2019generated. Our results show that our approach outperforms stateoftheart methods on deep image classiÔ¨Åcation tasks. Our experiments also highlight the difÔ¨Åculties in providing adequate weak supervision signals for solving multiclass image classiÔ¨Åcation tasks. 2 Related Work "
603,Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks.txt,"We present a novel learning-based approach to estimate the
direction-of-arrival (DOA) of a sound source using a convolutional recurrent
neural network (CRNN) trained via regression on synthetic data and Cartesian
labels. We also describe an improved method to generate synthetic data to train
the neural network using state-of-the-art sound propagation algorithms that
model specular as well as diffuse reflections of sound. We compare our model
against three other CRNNs trained using different formulations of the same
problem: classification on categorical labels, and regression on spherical
coordinate labels. In practice, our model achieves up to 43% decrease in
angular error over prior methods. The use of diffuse reflection results in 34%
and 41% reduction in angular prediction errors on LOCATA and SOFA datasets,
respectively, over prior methods based on image-source methods. Our method
results in an additional 3% error reduction over prior schemes that use
classification based networks, and we use 36% fewer network parameters.","Estimating the directionofarrival (DOA) of sound sources has been an important problem in terms of analyzing multichannel recordings [1, 2]. In these applications, the goal is to predict the azimuth and elevation angles of the sound source relative to the microphone, from a sound clip recorded in any multi channel setting. One of the simpler problems is the estima tion of the DOA on the horizontal plane [3]. More complex problems include DOA estimation in threedimensional space or the identiÔ¨Åcation of both direction and distance of an audio source. Even more challenging problems correspond to per forming these goals in noisy and reverberant environments. To analyze spatial information from sound recordings, at least two microphones with known relative positions must be used. In practice, various spatial recording formats including binaural, 5.1channel, 7.1channel, etc. have been applied to spatial audio related systems [4]. The Ambisonics format de composes a soundÔ¨Åeld using a spherical harmonic function ba sis [5]. Compared with its alternatives, Ambisonics has the ad vantage of being hardware independent‚Äìit does not necessarily encode microphone speciÔ¨Åcations into the recording. Recent work [6] has applied the Ambisonics format to DOA estimation and trained a CRNN classiÔ¨Åer that yields more ac curate predictions than a baseline approach using independent component analysis. While a regression formulation seems more natural for the problem of DOA estimation, some recent This work was supported in part by ARO grant W911NF181 0313 and Intel. Project page https://gamma.umd.edu/pro/ speech/doawork [3] suggests a regression formulation may yield worse per formance than that of the classiÔ¨Åcation formulation for multi layer perceptrons. In this work, we present a novel learning based approach for estimating DOA of a single sound source from ambisonic audio, building on an existing deep learning framework [6]. We present a CRNN which predicts DOA as a 3D Cartesian vector. We introduce a method to generate synthetic data using geometric sound propagation that models specular and diffuse reÔ¨Çections, which results in up to 43% er ror reduction compared with imagesource methods. We con duct a fourway comparison between the Cartesian regression network, two classiÔ¨Åcation networks trained with crossentropy loss, and a regression network trained using angular loss. Fi nally, we investigate results on two 3rdparty datasets: LO CATA [7] and SOFA [8], where our best model reduces angular prediction error by 43% compared to prior methods. Section 2 gives an overview of prior work. We propose our method in Section 3. Section 4 presents our results on two benchmarks and we conclude in Section 5. 2. Related Work "
604,SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels.txt,"Deep neural networks are prone to overfitting noisy labels, resulting in poor
generalization performance. To overcome this problem, we present a simple and
effective method self-ensemble label correction (SELC) to progressively correct
noisy labels and refine the model. We look deeper into the memorization
behavior in training with noisy labels and observe that the network outputs are
reliable in the early stage. To retain this reliable knowledge, SELC uses
ensemble predictions formed by an exponential moving average of network outputs
to update the original noisy labels. We show that training with SELC refines
the model by gradually reducing supervision from noisy labels and increasing
supervision from ensemble predictions. Despite its simplicity, compared with
many state-of-the-art methods, SELC obtains more promising and stable results
in the presence of class-conditional, instance-dependent, and real-world label
noise. The code is available at https://github.com/MacLLL/SELC.","The recent success of deep neural networks (DNNs) for vi sion tasks owes much to the availability of largescale, cor rectly annotated datasets. However, obtaining such high quality datasets can be extremely expensive, and sometimes even impossible. The common approaches, such as web queries [Liet al. , 2017 ]and crowdsourcing [Song et al. , 2019 ], can easily provide extensive labeled data, but unavoid ably introduce noisy labels . Existing studies [Arpit et al. , 2017; Zhang et al. , 2021 ]have demonstrated that DNNs can easily overÔ¨Åt noisy labels, which deteriorates the generaliza tion performance. Thus, it is essential to develop noiserobust algorithms for learning with noisy labels. Given a noisy training set consisting of clean samples and mislabeled samples, a common category of approaches [Reed et al. , 2015; Arazo et al. , 2019; Zhang et al. , 2020 ]to mit igating the negative inÔ¨Çuence of noisy labels is to identify and correct the mislabeled samples. However, the correction procedure in these methods only updates the noisy labels us ing the model prediction from the most recent training epoch directly, thus it may suffer from the false correction as themodel predictions for noisy samples tend to Ô¨Çuctuate. Take a bird image mislabeled as an airplane as an example. Dur ing the training, the clean bird samples would encourage the model to predict a given bird image as a bird, while the bird images with airplane labels regularly pull the model back to predict the bird as an airplane. Hence, the model prediction gathered in one training epoch may change back and forth between bird and airplane, resulting in false correction. We investigate the reason for performance degradation by analyzing the memorization behavior of the DNNs models. We observe that there exists a turning point during training. Before the turning point, the model only learns from easy (clean) samples, and thus model prediction is likely to be con sistent with clean samples. After the turning point, the model increasingly memorizes hard (mislabeled) samples. Hence model prediction oscillates strongly on clean samples. Trig gered by this observation, we seek to make the model retain the earlylearning memory for consistent predictions on clean samples even after the turning point. In this paper, we propose selfensemble label correction (SELC), which potentially corrects noisy labels during train ing thus preventing the model from being affected by the noisy labels. SELC leverages the knowledge provided in the model predictions over historical training epochs to form a consensus of prediction (ensemble prediction) before the turning point. We demonstrate that combining ensemble pre diction with the original noisy label leads to a better target. Accordingly, the model is gradually reÔ¨Åned as the targets be come less noisy, resulting in improving performance. How ever, it is challenging to Ô¨Ånd the turning point. Existing works estimate the turning point based on a test set or noise infor mation, which are unobservable in practice. We propose a metric to estimate the turning point only using training data, allowing us to select a suitable initial epoch to perform SELC. Overall, our contributions are summarized as follows: ‚Ä¢ We propose a simple and effective label correction method SELC based on selfensembling. ‚Ä¢ We design an effective metric based on unsupervised loss modeling to detect the turning point without requir ing the test set and noise information. ‚Ä¢ SELC achieves superior results and can be integrated with other techniques such as mixup [Zhang et al. , 2018 ] to further enhance the performance.arXiv:2205.01156v1  [cs.CV]  2 May 2022(a) Training accuracy  (b) Test accuracy  (c) CE  (d) CE  (e) SELC  (f) SELC Figure 1: Plots (a) and (b) show the training and test accuracy on CIFAR10 with different ratios of label noise using crossentropy (CE) loss. We investigate the memorization behavior of DNNs on CIFAR10 with 60% label noise using CE loss and SELC. Plot (c) and (e) show the fraction of clean samples that are predicted correctly (blue) and incorrectly (black). Plot (d) and (f) show the fraction of mislabeled samples that are predicted correctly (blue), memorized (i.e. the prediction equals to the wrong label, shown in red), and incorrectly predicted as neither the true nor the given wrong label (black). Compared to CE, SELC effectively prevents memorization of mislabeled samples and reÔ¨Ånes the model to attain correct predictions on both clean and mislabeled samples. 2 Related Work "
605,Expert Sample Consensus Applied to Camera Re-Localization.txt,"Fitting model parameters to a set of noisy data points is a common problem in
computer vision. In this work, we fit the 6D camera pose to a set of noisy
correspondences between the 2D input image and a known 3D environment. We
estimate these correspondences from the image using a neural network. Since the
correspondences often contain outliers, we utilize a robust estimator such as
Random Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the
pose parameters. When the problem domain, e.g. the space of all 2D-3D
correspondences, is large or ambiguous, a single network does not cover the
domain well. Mixture of Experts (MoE) is a popular strategy to divide a problem
domain among an ensemble of specialized networks, so called experts, where a
gating network decides which expert is responsible for a given input. In this
work, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a
MoE. Our main technical contribution is an efficient method to train ESAC
jointly and end-to-end. We demonstrate experimentally that ESAC handles two
real-world problems better than competing methods, i.e. scalability and
ambiguity. We apply ESAC to fitting simple geometric models to synthetic
images, and to camera re-localization for difficult, real datasets.","In computer vision, we often have a model that explains an observation with a small set of parameters. For exam ple, our model is the 6D pose (translation and rotation) of a camera, and our observations are images of a known 3D environment. The task of camera relocalization is then to robustly and accurately predict the 6D camera pose given the camera image. However, inferring model parameters from an observation is difÔ¨Åcult because many effects are not explained by our model. People might move through the environment, and its appearance varies largely due to lighting effects such as day versus night. We usually map our observation to a representation from which we can in fer model parameters more easily. For example, in camera Gating Prediction: Environment: Query Image: Experts: ... Low Sample Consensus High Sample ConsensusExpert Predictions: Office 1 Office 2 Office 2 Office 1Figure 1. Camera ReLocalization Using ESAC. Given an envi ronment consisting of several ambiguous rooms (top) and a query image (middle), we estimate the 6D camera pose (bottom). A gat ing network (black) predicts a probability for each room. We dis tribute a budget of pose hypotheses to expert networks specialized to each room. We choose the pose hypothesis with maximum sam ple consensus (green), i.e. the maximum geometric consistency. We train all networks jointly and endtoend. relocalization we can train a neural network to predict cor respondences between the 2D input image and the 3D envi ronment. Inferring the camera pose from these correspon dences is much easier, and various geometric solvers for this problem exist [21, 16, 26]. Because some predictions of the network might be erroneous, i.e. we have outlier cor respondences, we utilize a robust estimator such as Random Sample Consensus (RANSAC) [14], resp. its differentiable counterpart Differentiable Sample Consensus (DSAC) [6], or other differentiable estimators [53, 35] for training. 1arXiv:1908.02484v1  [cs.CV]  7 Aug 2019For some tasks, the problem domain is large or ambigu ous. In camera relocalization, an environment could fea ture repeating structures that are unique locally but not glob ally, e.g. ofÔ¨Åce equipment, radiators or windows. A single feedforward network cannot predict a correct correspon dence for such objects because there are multiple valid solu tions. However, if we train an ensemble of networks where each network specializes in a local part of the environment, we can resolve such ambiguities. This strategy is known in machine learning as Mixture of Experts (MoE) [20]. Each expert is a network specialized to one part of the problem domain. An additional gating network decides which expert is responsible for a given observation. More speciÔ¨Åcally, the output of the gating network is a categorical distribution over experts, which either guides the selection of a single expert, or a weighted average of all expert outputs [30]. In this work, we extend Mixture of Experts for Ô¨Åtting parametric models. Each expert specializes to a part of all training observations, and predicts a representation to which we Ô¨Åt model parameters using DSAC. We argue that two realizations of a Mixture of Experts model are not op timal: i) letting the gating network select one expert only [19, 51, 3, 43]; ii) giving as output a weighted average of all experts [20, 1]. In the Ô¨Årst case, we ignore that the gat ing network might attribute substantial probability to more than one expert. We might choose the wrong expert, and get a poor result. In the second case, we calculate an average in model parameter space which can be instable in learn ing [6]. In our realization of a Mixture of Experts model, we integrate the gating network into the hypothesizeand verify framework of DSAC. To estimate model parameters, DSAC creates many model hypotheses by sampling small subsets of data points, and Ô¨Åtting model parameters to each subset. DSAC scores hypotheses according to their con sistency with all data points, i.e. their sample consensus. One hypothesis is selected as the Ô¨Ånal estimate according to this score. Hypothesis selection is probabilistic, and train ing aims at minimizing the expected task loss. Instead of letting the gating network pick one expert, and Ô¨Åt model parameters only to this expert‚Äôs prediction, we dis tribute model hypotheses among experts. Each expert re ceives a share of the total number of hypotheses according to the gating network. For the Ô¨Ånal selection, we score each hypothesis according to sample consensus, irrespective of what expert it came from, see Fig 1. Therefore, as long as the gating network attributes some probability to the cor rect expert, we can still get an accurate model parameter estimate. We call this framework Expert Sample Consensus (ESAC). We train the network ensemble jointly and endto end by minimizing the expected task loss. We deÔ¨Åne the expectation over both, hypotheses sharing according to the gating network, and hypothesis selection according to sam ple consensus.We demonstrate our method on a toy problem where the gating network has to decide which model to Ô¨Åt to syn thetic data  a line or a circle. Compared to naive expert selection, our method proves to be extremely robust regard ing the gating network‚Äôs ability to assign the correct expert. Our method also achieves stateoftheart results in camera relocalization where each expert specializes in a separate, small part of a larger indoor environment. We give the following main contributions : We present Expert Sample Consensus (ESAC), an en semble formulation of Differentiable Sample Consen sus (DSAC) which we derive from Mixture of Experts (MoE). A method to train ESAC jointly and endtoend. We demonstrate the properties of our algorithm on a toy problem of Ô¨Åtting simple parametric models to noisy, synthetic inputs. Our formulation improves on two realworld aspects of learningbased camera relocalization, scalability and ambiguity. We achieve stateoftheart results on difÔ¨Å cult, public datasets for indoor relocalization. 2. Related Work "
606,Deep Visual Re-Identification with Confidence.txt,"Transportation systems often rely on understanding the flow of vehicles or
pedestrian. From traffic monitoring at the city scale, to commuters in train
terminals, recent progress in sensing technology make it possible to use
cameras to better understand the demand, i.e., better track moving agents
(e.g., vehicles and pedestrians). Whether the cameras are mounted on drones,
vehicles, or fixed in the built environments, they inevitably remain scatter.
We need to develop the technology to re-identify the same agents across images
captured from non-overlapping field-of-views, referred to as the visual
re-identification task. State-of-the-art methods learn a neural network based
representation trained with the cross-entropy loss function. We argue that such
loss function is not suited for the visual re-identification task hence propose
to model confidence in the representation learning framework. We show the
impact of our confidence-based learning framework with three methods: label
smoothing, confidence penalty, and deep variational information bottleneck.
They all show a boost in performance validating our claim. Our contribution is
generic to any agent of interest, i.e., vehicles or pedestrians, and outperform
highly specialized state-of-the-art methods across 5 datasets. The source code
and models are shared towards an open science mission.","An important goal of transportation research is to improve and provide efÔ¨Åcient public transportation systems that can accommodate many agents, whether it be vehicles or pedestrians, every day. This is especially important nowadays with the huge trafÔ¨Åc congestion costing billions of dollars [ 1]. As a result, research efforts have been directed towards management and control of vehicle and pedestrian Ô¨Çows. Important prerequisites for such transportation network analysis are origindestination (OD) matrices, which allow researchers to understand a population‚Äôs trip demand. With the recent developments in new methods, such as datadriven methods [ 2], OD estimation have achievedarXiv:1906.04692v2  [cs.CV]  10 Sep 2020DestinationOrigin Destination 42% 15%OriginFigure 1: In this work, we present a new visual reidentiÔ¨Åcation method, i.e., whether agents (vehicles or pedestrians) captured in different images in nonoverlapping views belong to the same agent. Agents with the orange bounding box belong to the same agent. Note different agents can be visually very similar. good performance in various tasks of trafÔ¨Åc management. However, it still faces the issue of how representative the chosen samples are of the population. One way to deal with this problem is to collect more data from the population, which is expensive and timeconsuming when using traditional methods such as surveys or interviews. Another way is to make use of smartphones to collect such data [ 3]. Visual reidentiÔ¨Åcation is a faster and cheaper way to collect these data. Visual reidentiÔ¨Åcation is the task of associating images of the same agent taken from different cameras or from the same camera in different occasions. This is represented in Fig. 1. This task is nowadays possible due to the complex network of cameras already places in and around cities. Moreover, recent works have been pushing towards the use of drone technology to collect massive amounts of data in order to study the trafÔ¨Åc phenomena, such as the recently released pNEUMA dataset [ 4]. All these collected data can be used as inputs to a visual reidentiÔ¨Åcation model to associate different agents together and obtain their paths. The task of reidentiÔ¨Åcation (reid) has long been a task of extracting features/representations from two observations and measuring how similar these features are. Since different variations affect these features, many works have introduced different methods to improve their extraction. Initially, 2these features were handcrafted and include spatiotemporal information such as color, width and height, and salient edge histograms. Some work have also tried to use different input modalities such as depth [ 5,6,7], infrared [ 8], LiDAR [ 9], or Inductive Loop Detectors for vehicles [ 10]. These features, however, fail drastically when dealing with unexpected scenarios. To remedy this problem and with the advent of machine learning, researchers are now beneÔ¨Åting from the strength of deep learning to be able to extract more general and more discriminative features allowing them to reach high performance. Since then, an arms race of methods was built on top of this by making use of different objectspeciÔ¨Åc characteristics ( e.g., human semantic segmentation, pose) and by learning features through the supervision of a crossentropy loss. A main pitfall of learning with crossentropy supervision is the fact that it separates the different inputs solely based on the labels without taking into consideration the actual similarity between the inputs. None of the recent methods have tackled the problem where, even though two very similar agents are distinct, their similarity score should encode information about how similar they appear while also distinguishing them. The network usually tries its best to Ô¨Ånd a boundary between the different classes even for inputs that are very similar. This leads the network to Ô¨Ånd unreasonable explanations for the differences in labels and thus would negatively affect its generalizability. Since reid also deals with the problem of having a small set of images per class, it would aggravate this issue. Controlling the network‚Äôs conÔ¨Ådence in its predictions would alleviate this problem. To the best of your knowledge, this is the Ô¨Årst paper to apply this concept in the Ô¨Åeld of reidentiÔ¨Åcation. In this paper, we propose to model conÔ¨Ådence when learning representations appropriate for reid. By inducing doubt while training a network, we are able to tackle the inherent problem discussed previously when crossentropy is used in a distance metric and representation learning problem such as person or vehicle reidentiÔ¨Åcation. Inspired by previous works that use uncertainty to regularize the network, we study three alternatives that aim at reducing the conÔ¨Ådence of the network and show a gain of 67 % in mAP across 5 different datasets. Although these methods have shown only a small improvement in other image classiÔ¨Åcation tasks [ 11,12,13], they drastically improve the performance of reid models due to its innate problem (Section 3). By combining our methods with advanced ranking methods, we outperform stateoftheart models without modeling additional characteristics speciÔ¨Åc to the object in question. The software is opensource and is made available online1. 2 Related Work "
607,Group-aware Label Transfer for Domain Adaptive Person Re-identification.txt,"Unsupervised Domain Adaptive (UDA) person re-identification (ReID) aims at
adapting the model trained on a labeled source-domain dataset to a
target-domain dataset without any further annotations. Most successful UDA-ReID
approaches combine clustering-based pseudo-label prediction with representation
learning and perform the two steps in an alternating fashion. However, offline
interaction between these two steps may allow noisy pseudo labels to
substantially hinder the capability of the model. In this paper, we propose a
Group-aware Label Transfer (GLT) algorithm, which enables the online
interaction and mutual promotion of pseudo-label prediction and representation
learning. Specifically, a label transfer algorithm simultaneously uses pseudo
labels to train the data while refining the pseudo labels as an online
clustering algorithm. It treats the online label refinery problem as an optimal
transport problem, which explores the minimum cost for assigning M samples to N
pseudo labels. More importantly, we introduce a group-aware strategy to assign
implicit attribute group IDs to samples. The combination of the online label
refining algorithm and the group-aware strategy can better correct the noisy
pseudo label in an online fashion and narrow down the search space of the
target identity. The effectiveness of the proposed GLT is demonstrated by the
experimental results (Rank-1 accuracy) for Market1501$\to$DukeMTMC (82.0\%) and
DukeMTMC$\to$Market1501 (92.2\%), remarkably closing the gap between
unsupervised and supervised performance on person re-identification.","Person reidentiÔ¨Åcation (ReID) is the important task of matching person images captured from nonoverlapping camera networks, which is widely used in practical appli *Equal contribution ‚Ä†Corresponding author 1Full codes are available in https://github.com/zkcys001/ UDAStrongBaseline and https://github.com/JDAICV/fastreid Network  Network Label  Refinery  Groupaware Label T ransfer Conventional Methods Our Method  Figure 1. Illustration of conventional methods and our group aware label transfer method. In our method, each instance is as signed to multiple prototypes with different granularity f or gen erating multigroup pseudo labels, and then its noisy multi group pseudo labels are online reÔ¨Åned. By learning the reÔ¨Åning mul ti group pseudo labels, our GLT method can learn an embedding space that encodes the semantic multigranularity structu re of data. cations such as automatic surveillance, contentbased re trieval, and behavior analysis [ 17,32,39,5]. It has been proved that existing approaches can achieve remarkable performance when the training and testing data are collecte d from the same application scenario but often fail to gener alize well to other scenarios due to the inevitable domain gaps. Therefore, it is necessary for both academia and in dustry to study the Unsupervised Domain Adaptive (UDA) person reidentiÔ¨Åcation (ReID) problem. Existing UDAReID approaches [ 10,24,34,40] typi cally include three steps: feature pretraining with label ed source domain data, clusteringbased pseudolabel predic  tion for the target domain data, and feature representation learning/Ô¨Ånetuning with the pseudolabels. The last two steps are usually iteratively conducted to strengthen or pr o mote each other. However, the Ô¨Årst problem is that the pseudolabels assigned through clustering usually contai n incorrect labels due to the divergence/domain gap between 1the source and target data, and the imperfect nature of the clustering algorithm. Such noisy labels may mislead the feature learning and harm the domain adaptation perfor mance. Although the label reÔ¨Åning objective in clustering is tractable, it is an ofÔ¨Çine timeconsuming scheme as it re quires a pass over the entire dataset. Therefore, online re Ô¨Åning those incorrect samples when training can help model learn more robust and accurate representation. Another problem is that the target domain lacks ID infor mation, so it is difÔ¨Åcult to cluster the person images accord  ing to human identity. However, in the real world, each per son has their own characteristics in his or her appearance. There may be common appearances shared by a group of people but they are not the same identity (e.g. two men with the same red coats and similar black pants as shown in Fig. 1). Therefore, groupbased description [ 15] that in volves common characteristics in a pseudo group, can be useful to narrow down the set of candidates and beneÔ¨Åcial to identify the exact persons. This groupaware strategy ca n cluster a person into multigroup clustering prototypes, a nd is able to efÔ¨Åciently embed a signiÔ¨Åcant number of people and brieÔ¨Çy describe an unknown person. Inspired by this, as shown in Fig. 1, combining the online label reÔ¨Åning al gorithm with the groupaware strategy may be beneÔ¨Åcial to the success of domain adaptation. In this paper, we propose a Groupaware Label Trans fer (GLT) algorithm that facilitates the online interactio n and mutual promotion of the pseudo labels prediction and feature learning. SpeciÔ¨Åcally, our method simultaneously uses pseudo labels to train the data while online reÔ¨Åning the pseudo labels via the label transfer algorithm. This label transfer algorithm regards the resulting label reÔ¨Åning pro b lem as optimal transport, which explores the minimum cost for assigning M samples to N pseudo labels. This prob lem can therefore be solved by the SinkhornKnopp algo rithm [ 4] of linear programming in polynomial time. Mean while, the label transfer algorithm is scalable, which can b e trained with several batches or the whole dataset and can scale to unlimited amounts of data. More importantly, we introduce a groupaware strategy to assign implicit attrib ute group IDs to samples. Explicit grouping requires manual annotation. Therefore, we adopt a groupaware clustering algorithm to generate multigroup pseudo labels. By adopt ing the concept of grouping, the ReID network can reduce the search space and Ô¨Çexibly embed a signiÔ¨Åcant number of identities into an embedding feature. As shown in Fig. 1, we combine the online label reÔ¨Åning algorithm with the multi group pseudo labels. The combination of the online label reÔ¨Åning algorithm and the groupaware strategy can better correct the noisy pseudo label online and narrow down the search space of the target identity and predict more accurat e pseudo labels. In addition, we design a target instance mem ory bank combined with weighted contrastive loss [ 25] tomake the model more powerful for features representation. The proposed GLT framework achieves stateoftheart per formance on MarkettoDuke, DuketoMarket, Marketto MSMT, and DuketoMSMT with unsupervised domain adaptation, and in particular the i.e. DuketoMarket per formances (92.2 Rank1 and 79.5 mAP) are almost com parable with the supervised learning performances (94.1 Rank1 and 85.7 mAP). Our method signiÔ¨Åcantly closes gap between unsupervised and supervised performance on per son reidentiÔ¨Åcation, i.e., 92.2 vs. 94.1 top1 accuracy in DukeMTMC to Market1501 transfer. The main contributions of this paper can be summarized in four aspects: ‚Ä¢ We make the Ô¨Årst attempt towards integrating clus tering and feature learning in a uniÔ¨Åed framework through the label transfer method for UDAReID. It can online reÔ¨Åne the predicted pseudo labels to im prove the feature representation ability of the model on the target domain. ‚Ä¢ We propose a groupaware feature learning strategy based on label transfer to reÔ¨Åne multigroup pseudo la bels, which provides good latent pseudolabel groups for improving the quality of representation learning. ‚Ä¢ The GLT framework achieves signiÔ¨Åcant performance improvements compared to stateoftheart approaches on Market‚ÜíDuke, Duke‚ÜíMarket, Market‚ÜíMSMT, Duke‚ÜíMSMT ReID tasks. Even for the supervised learning methods, our algorithm is remarkably closing the gap. 2. Related Work "
608,Dynamic Label Graph Matching for Unsupervised Video Re-Identification.txt,"Label estimation is an important component in an unsupervised person
re-identification (re-ID) system. This paper focuses on cross-camera label
estimation, which can be subsequently used in feature learning to learn robust
re-ID models. Specifically, we propose to construct a graph for samples in each
camera, and then graph matching scheme is introduced for cross-camera labeling
association. While labels directly output from existing graph matching methods
may be noisy and inaccurate due to significant cross-camera variations, this
paper proposes a dynamic graph matching (DGM) method. DGM iteratively updates
the image graph and the label estimation process by learning a better feature
space with intermediate estimated labels. DGM is advantageous in two aspects:
1) the accuracy of estimated labels is improved significantly with the
iterations; 2) DGM is robust to noisy initial training data. Extensive
experiments conducted on three benchmarks including the large-scale MARS
dataset show that DGM yields competitive performance to fully supervised
baselines, and outperforms competing unsupervised learning methods.","Person reidentiÔ¨Åcation (reID), a retrieval problem in it s essence [ 39,33,38], aims to search for the queried person from a gallery of disjoint cameras. In recent years, im pressive progress has been reported in video based reID [34,20,37], because video sequences provide rich visual and temporal information and can be trivially obtained by tracking algorithms [ 11,12] in practical video surveillance applications. Nevertheless, the annotation difÔ¨Åculty lim its the scalability of supervised methods in largescale camer a networks, which motivates us to investigate an unsupervise d solution for video reID. The difference between unsupervised learning and su pervised learning consists in the availability of labels. C on sidering the good performance of supervised methods, an 1Code is available at www.comp.hkbu.edu.hk/%7emangye/Labels Data  Graphs Matching Learn  Re ID  Labels Data  Graphs Matching Reweighting  fLearn  ReID  Metric Learn Update  Figure 1. Pipeline Illustration. Graph matching is conduct ed af ter constructing a graph for samples in each camera to obtain the intermediate labels. Instead of using the labels directly, label re weighting is introduced to handle the noisy intermediate la bels. Iteratively, the graph is updated, labels are estimated, an d distance metrics are learnt. intuitive idea for unsupervised learning is to estimate re ID labels as accurately as possible. In previous works, part from directly using handcrafted descriptors [ 30,14,19, 16], some other unsupervised reID methods focus on Ô¨Ånd ing shared invariant information (saliency [ 36] or dictionary [9,22]) among cameras. Deviating from the idea of esti mating labels, these methods [ 36,9,22] might be less com petitive compared with the supervised counterparts. Mean while, these methods also suffer from large crosscamera variations. For example, salient features are not stable du e to occlusions or viewpoint variations. Different from the existing unsupervised person reID methods, this paper is based on a more customized solution, i.e., crosscamera label estimation. In other words, we aim to mine the la bels (matched or unmatched video pairs) across cameras. With the estimated labels, the remaining steps are exactly the same with supervised learning. To mine labels across cameras, we leverage the graph matching technique ( e.g., [28]) by constructing a graph for samples in each camera for label estimation. Instead of es timating labels independently, the graph matching approac h has shown good property in Ô¨Ånding correspondences by minimize the globally matching cost with intragraph rela tionship. Meanwhile, label estimation problem for reID task is to link the same person across different cameras, which perfectly matches the graph matching problem by treating each person as a graph node. However, labels di rectly estimated by existing graph matching are very likelyto be inaccurate and noisy due to the signiÔ¨Åcant appearance changes across cameras. So a Ô¨Åxed graph constructed in the original feature space usually does not produce satisfying results. Moreover, the assumption that the assignment cost or afÔ¨Ånity matrix is Ô¨Åxed in most graph matching methods may be unsuitable for reID due to large crosscamera vari ations [ 13,4,2,28]. In light of the above discussions, this paper proposes a dynamic graph matching (DGM) method to improve the label estimation performance for unsupervised video reID (the main idea is shown in Fig. 1). SpeciÔ¨Åcally, our pipeline is an iterative process. In each iteration, a bipartite grap h is established, labels are then estimated, and then a discrim inative metric is learnt. Throughout this procedure, label s gradually become more accurate, and the learnt metric more discriminative. Additionally, our method includes a label reweighting strategy which provides soft labels instead o f hard labels, a beneÔ¨Åcial step against the noisy intermediat e label estimation output from graph matching. The main contributions are summarized as follows: ‚Ä¢We propose a dynamic graph matching (DGM) method to estimate crosscamera labels for unsupervised re ID, which is robust to distractors and noisy initial train ing data. The estimated labels can be used for further discriminative reID models learning. ‚Ä¢Our experiment conÔ¨Årms that DGM is only slightly in ferior to its supervised baselines and yields competi tive reID accuracy compared with existing unsuper vised reID methods on three video benchmarks. 2. Related Work "
609,GAN-based Vertical Federated Learning for Label Protection in Binary Classification.txt,"Split learning (splitNN) has emerged as a popular strategy for addressing the
high computational costs and low modeling efficiency in Vertical Federated
Learning (VFL). However, despite its popularity, vanilla splitNN lacks
encryption protection, leaving it vulnerable to privacy leakage issues,
especially Label Leakage from Gradients (LLG). Motivated by the LLG issue
resulting from the use of labels during training, we propose the Generative
Adversarial Federated Model (GAFM), a novel method designed specifically to
enhance label privacy protection by integrating splitNN with Generative
Adversarial Networks (GANs). GAFM leverages GANs to indirectly utilize label
information by learning the label distribution rather than relying on explicit
labels, thereby mitigating LLG. GAFM also employs an additional cross-entropy
loss based on the noisy labels to further improve the prediction accuracy. Our
ablation experiment demonstrates that the combination of GAN and the
cross-entropy loss component is necessary to enable GAFM to mitigate LLG
without significantly compromising the model utility. Empirical results on
various datasets show that GAFM achieves a better and more robust trade-off
between model utility and privacy compared to all baselines across multiple
random runs. In addition, we provide experimental justification to substantiate
GAFM's superiority over splitNN, demonstrating that it offers enhanced label
protection through gradient perturbation relative to splitNN.","Federated learning trains algorithms across multiple decentralized remote devices or siloed data centers without sharing sensitive data. There are three types of federated learning depending on the data partitioning methods used: horizontal federated learning (HFL), vertical federated learning (VFL), and federated transfer learning [ 36]. VFL partitions the data vertically, where local participants have datasets with the same sample IDs but different features [ 17]. With stricter data privacy regulations like CCPA1 [ 24] and GDPR3 [ 31], VFL is a viable solution for enterpriselevel data collaborations, as it facilitates collaborative training and privacy protection. However, VFL faces challenges in terms of high memory costs and processing time overheads, attributed to the complex cryptographic operations employed to provide strong privacy guarantees [ 38], including additive homomorphic encryption [ 17] and secure multiparty computation [ 22], which are computationally intensive. To address these issues, split learning [ 14,30,2] has emerged as an efÔ¨Åcient solution, allowing multiple participants to jointly train federated models without encrypting intermediate results, thus reducing computational costs. SplitNN [ 6], which applies the concept of split learning to neural networks, has been used successfully in the analysis of medical data [26, 15]. Split learning, while reducing computational costs, poses substantial privacy risks due to the absence of encryption protection for model privacy. One speciÔ¨Åc privacy risk is Label Leakage from Gradients (LLG) [ 33], in which gradients Ô¨Çowing from the label party to the nonlabel (only data) party can expose the label information [ 10,40,32]. LLG is susceptible to exploitation for stealing label information in binary classiÔ¨Åcation and has limited proposed solutions to address it. Recent work by Li et al. [ 21] indicates that in binary classiÔ¨Åcation, the gradient norm of positive instancesarXiv:2302.02245v2  [cs.LG]  17 May 2023APREPRINT  M AY18, 2023 is generally larger than negative ones , which could potentially enable attackers to easily infer sample labels from intermediate gradients in splitNN. Despite the fact that binary classiÔ¨Åcation is widely used in various federated scenarios, such as healthcare, Ô¨Ånance, credit risk, and smart cities [ 9,5,8,39], and is vulnerable to LLG, limited research has been conducted on addressing the LLG issue in binary classiÔ¨Åcation. Previous studies [ 6,10,29,25] have focused mainly on securing the data information of nonlabel parties, while ignoring the risk of leaking highly sensitive label information of the label party. Therefore, it is critical to address how splitNN can resist LLG in binary classiÔ¨Åcation tasks. In this work, in order to prevent inferring sample labels from the gradient calculation, we introduce a novel Generative Adversarial Federated Model (GAFM), which synergistically combines the vanilla splitNN architecture with Generative Adversarial Networks (GANs) to indirectly incorporate labels into the model training process. SpeciÔ¨Åcally, the GAN discriminator within GAFM allows federated participants to learn a prediction distribution that closely aligns with the label distribution, effectively circumventing the direct use of labels inherent in the vanilla SplitNN approach. Moreover, to counteract the potential degradation of model utility induced by GANs, we enhance our method by incorporating additional label information via an additional crossentropy loss, which encourages the intermediate results generated by the nonlabel party and the labels with added noise provided by the label party to perform similarly. The entire framework of the proposed GAFM and the training procedures is displayed in Figure 1. Our contributions are highlighted as follows. ‚Ä¢We propose a novel GANbased approach, called GAFM, which combines vanilla splitNN with GAN to mitigate LLG in binary classiÔ¨Åcation (section 3.2). Our analysis in section 3.4 demonstrates that GAFM protects label privacy by generating more mixed intermediate gradients through the mutual gradient perturbation of both the GAN and crossentropy components. ‚Ä¢We enrich the existing gradientbased label stealing attacks by identifying two additional simple yet practical attack methods, namely mean attack and median attack in section 3.5. The experimental results in section 4.2.1 demonstrate that our new attacks are more effective in inferring labels than the existing ones. ‚Ä¢We evaluate the effectiveness of GAFM on various datasets. Empirical results in section 4 show that GAFM mitigates LLG without signiÔ¨Åcant model utility degradation and the performance of GAFM across different random seeds is more stable compared to baselines. We also provide additional insights based on the ablation experiment (section 4.2.2) to demonstrate the necessity of combining both the GAN and crossentropy components in GAFM. Compared to the stable balance between utility and privacy that can be achieved by using both components, GAFM with only the GAN component provides enhanced privacy protection at the cost of reduced utility, while GAFM with only the crossentropy component delivers superior utility but offers limited privacy protection. 2 Related Work "
610,Should Ensemble Members Be Calibrated?.txt,"Underlying the use of statistical approaches for a wide range of applications
is the assumption that the probabilities obtained from a statistical model are
representative of the ""true"" probability that event, or outcome, will occur.
Unfortunately, for modern deep neural networks this is not the case, they are
often observed to be poorly calibrated. Additionally, these deep learning
approaches make use of large numbers of model parameters, motivating the use of
Bayesian, or ensemble approximation, approaches to handle issues with parameter
estimation. This paper explores the application of calibration schemes to deep
ensembles from both a theoretical perspective and empirically on a standard
image classification task, CIFAR-100. The underlying theoretical requirements
for calibration, and associated calibration criteria, are first described. It
is shown that well calibrated ensemble members will not necessarily yield a
well calibrated ensemble prediction, and if the ensemble prediction is well
calibrated its performance cannot exceed that of the average performance of the
calibrated ensemble members. On CIFAR-100 the impact of calibration for
ensemble prediction, and associated calibration is evaluated. Additionally the
situation where multiple different topologies are combined together is
discussed.","Deep learning approaches achieve stateoftheart performance in a wide range of applications, including image classiÔ¨Åcation. However, these networks tend to be overconÔ¨Ådent in their predictions, they often exhibit poor calibration. A system is well calibrated, if when the system makes a prediction with probability of 0.6 then 60% of the time that prediction is correct. Calibration is very important in deploying system, especially in risksensitive tasks, such as medicine (Jiang et al., 2012), autodriving (Bojarski et al., 2016), and economics (Gneiting et al., 2007). It was shown by NiculescuMizil & Caruana (2005) that shallow neural networks are well calibrated. However, Guo et al. (2017) found that more complex neural network model with deep structures do not exhibit the same behaviour. This work motivated recent research into calibration for general deep learning systems. Previous research has mainly examined calibration based on samples from the true data distributionfx(i);y(i)gN i=1p(x;!);y(i)2f!1;:::;!Kg(Zadrozny & Elkan, 2002; Vaicenavicius et al., 2019). This analysis relies on the limiting behaviour as N!+1to deÔ¨Åne a well calibrated system P(y= ^yjP(^yjx;) =p) =p() lim N!+1X i2Sp j(y(i);^y(i)) jSp jj=p (1) whereSp j=fijP(^y(i)=jjx(i);) =p;i= 1;:::;Ngand^y(i)the model prediction for x(i). (s;t) = 1 ifs=t, otherwise 0. However, Eq. (1) doesn‚Äôt explicitly reÔ¨Çect the relation between P(y= ^yjP(^yjx;) =p)and the underlying data distribution p(x;y). In this work we examine Preprint. Under review.arXiv:2101.05397v1  [cs.LG]  13 Jan 2021this explicit relationship and use it to deÔ¨Åne a range of calibration evaluation criteria, including the standard samplebased criteria. One issue with deeplearning approaches is the large number of model parameters associated with the networks. Deep ensembles (Lakshminarayanan et al., 2017) is a simple, effective, approach for handling this problem. It has been found to improve performance, as well as allowing measures of uncertainty. In recent literature there has been ‚Äúcontradictory‚Äù empirical observations about the relationship between the calibration of the members of the ensemble and the calibration of the Ô¨Ånal ensemble prediction (Rahaman & Thiery, 2020; Wen et al., 2020). In this paper, we examine the underlying theory and empirical results relating to calibration with ensemble methods. We found, both theoretically and empirically, that ensembling multiple calibrated models decreases the conÔ¨Ådence of Ô¨Ånal prediction, resulting in an illcalibrated ensemble prediction. To address this, strategies to calibrate the Ô¨Ånal ensemble prediction, rather than individual members, are required. Additionally we empiricaly examine the situation where the ensemble is comprised of models with different topologies, and resulting complexity/performance, requiring nonuniform ensemble averaging. In this study, we focus on posthoc calibration of ensemble, based on temperature annealing. Guo et al. (2017) conducted a thorough comparison of various existing posthoc calibration methods and found that temperature scaling was a simple, fast, and often highly effective approach to calibration. However, standard temperature scaling acts globally for all regions of the input samples, i.e. all logits are scaled towards one single direction, either increasing or decreasing the distribution entropy. To address this constraint, that may hurt some legitimately conÔ¨Ådent predictions, we investigate the effect of regionspeciÔ¨Åc temperatures. Empirical results demonstrate the effectiveness of this approach, with minimal increase in the number of calibration parameters. 2 Related Work "
611,Semi-supervised Feature Learning For Improving Writer Identification.txt,"Data augmentation is usually used by supervised learning approaches for
offline writer identification, but such approaches require extra training data
and potentially lead to overfitting errors. In this study, a semi-supervised
feature learning pipeline was proposed to improve the performance of writer
identification by training with extra unlabeled data and the original labeled
data simultaneously. Specifically, we proposed a weighted label smoothing
regularization (WLSR) method for data augmentation, which assigned the weighted
uniform label distribution to the extra unlabeled data. The WLSR method could
regularize the convolutional neural network (CNN) baseline to allow more
discriminative features to be learned to represent the properties of different
writing styles. The experimental results on well-known benchmark datasets
(ICDAR2013 and CVL) showed that our proposed semi-supervised feature learning
approach could significantly improve the baseline measurement and perform
competitively with existing writer identification approaches. Our findings
provide new insights into offline write identification.","Handwritten texts, speech, ngerprints, and faces are often applied in physiological biometric identiers. Especially, handwritten text plays an im portant role for forensics and security in proving someone's authenticity. Re search into writer identication has received renewed interest in recent years, such as historical document analysis for the massdigitization processes of historical documents [24, 29, 49] through machine learning; unfortunately, this process requires considerable time and detection costs. Therefore, many researchers have proposed stateoftheart pattern recognition approaches to automatically recognize writing styles [1, 7, 30, 38, 50]. Writer identication aims to search and recognize texts written by the same writer in a query database. Writer identication has been investigated on dierent handwritten scripts, such as English [3, 40], Chinese [18, 19, 48], Arabic [1], Indic [30], Persian [20] and Latin scripts [9]. This task gener ally presents substantial challenges because it requires the documents to be sorted according to high similarity (e.g., the distance of feature vectors). Writer identication can be classied as online writer identication and of  ine writer identication according to the handwritten document acquisition method. The latter approach can be further categorized into allographbased and textualbased methods. Texturalbased methods compute global statis tics directly from handwritten documents (pages) [2, 14, 16, 32, 33]. For example, the angles of stroke directions, the width of the ink trace, and the histograms of local binary patterns (LBP) and local ternary patterns (LTP) have been used for writer identication purposes. Allographbased methods rely on local descriptors computed from small patches (allographs), and then a global document descriptor is statistically calculated using the local descrip tors of one document [7, 8, 18]. These two methods can be further combined to form a discriminative global feature [3, 17, 48]. The semisupervised fea ture learning pipeline proposed in this work is based on allographs for oine writer identication. Although writer identication has achieved excellent performance on some benchmark datasets, there are considerable challenges in realworld applica tions. First, the use of dierent pens, the physical condition of the writer, the presence of distractions (such as multitasking and noise), and the changes 2in writing style with age are key factors resulting in the unsatisfactory per formance of writer identication. Second, the writers of the training set are dierent than those of the test set, and every writer only contributes a few handwritten text images in the typically used benchmark datasets. Third, the number of handwritten documents in benchmark datasets is highly in sucient for convolutional neural network (CNN) model training; therefore, training a reliable CNN model using limited data is a challenge. Moreover, almost all published methods are based on supervised learning, which cannot achieve landmark results due to the limited amount of labeled data present in the benchmarks. Some researchers utilize dierent data augmentation meth ods to address these problems. However, these data augmentation methods that are used in writer identication easily lead to model overtting and re quire a considerable amount of extra data. To overcome the aforementioned challenges and then tightly integrate with writer identication in practice, we propose a novel insight for writer identication. CNNs are a wellknown deep learning architecture inspired by the natu ral visual perception mechanism of living creatures. CNNs have been widely used and have achieved exciting performance in the elds of image classi cation, object recognition and object detection and tracking [15, 25, 41, 43] due to their powerful ability to learn deep features. The recent progress in writer identication is mainly attributed to advancements in CNNs based on supervised [6, 7, 8, 11, 17, 45, 49] and unsupervised feature learning [9]. The features extracted from CNNs perform better as discriminative characteris tics compared to handcrafted features. For example, Xing and Qiao et al. [49] designed a multistream CNN structure for writer identication and achieved a high identication accuracy on the IAM [31] and HWDB [28] datasets using a small amount of handwritten documents. In [8], Christlein proposed using activation features from CNNs as local descriptors for writer identication and improved the identication performance on the ICDAR2013 dataset. R. Eldan et al. [10] showed that a deeper network would learn a more discrim inative representation but will need more resources to train. Therefore, we recommend that a tradeo and a deep residual neural network with 50 layers (ResNet50) could be applied in our work. In contrast to the supervised learning approaches, semisupervised learn ing signicantly surpasses supervised learning when annotated data are lim ited in the training set, e.g., weakly labeled or unlabeled data [21, 47, 52]. In particular, semisupervised learning saves the time and budget needed for annotating data when the volume of clean labeled data is limited. Some 3recent studies investigated a semisupervised learning pipeline by combining unsupervised learning with supervised learning [39, 46] to assign an original label or a new label to unlabeled data [26, 34, 36]. Motivated by the previ ous studies, we attempt to use a modied semisupervised learning method by assigning a weighted uniform label distribution to extra unlabeled data (extra data) according to the original labeled data (real data). We believe that the proposed approach has the potential to regularize the baseline for improving identication performance. Therefore, we proposed a semisupervised method that leverages a deep CNN and the weighted label smoothing regularization (WLSR) to form a powerful model that learns discriminative representations for oine writer identication in our work. Specically, we rst preprocess the original labeled data and the extra unlabeled data. Then, these original labeled data and extra unlabeled data are fed into a deep residual neural network (ResNet) [15] simultaneously. Furthermore, the WLSR method regularizes the learn ing process by integrating the unlabeled data, which can reduce the risk of overtting and direct the model to learn more eective and discriminative features. Finally, the local features of every test handwritten document are extracted and encoded as a global feature vector for identication. To summarize, this study makes the following contributions: A. This study is a pioneering work that uses a semisupervised feature learning pipeline to integrate extra unlabeled images and original labeled images into the ResNet model for writer identication. B. The WLSR method of semisupervised learning is used to regularize the identication model with unlabeled data. We thoroughly evaluate its availability on public datasets. C. Our results show that the proposed semisupervised learning model had a consistent improvement over the deep residual neural network baseline and achieved better performance than existing approaches on benchmark datasets. The remainder of this paper is organized as follows. Sec. 2 provides an overview of the related works in the eld of writer identication. The process of the semisupervised learning pipeline is presented in Sec. 3. The perfor mance and evaluation are given in Sec. 4. Sec. 5 presents the discussion. Sec. 6 provides a summary and the outlook for future research. 42. Related Work "
612,Divergence Optimization for Noisy Universal Domain Adaptation.txt,"Universal domain adaptation (UniDA) has been proposed to transfer knowledge
learned from a label-rich source domain to a label-scarce target domain without
any constraints on the label sets. In practice, however, it is difficult to
obtain a large amount of perfectly clean labeled data in a source domain with
limited resources. Existing UniDA methods rely on source samples with correct
annotations, which greatly limits their application in the real world. Hence,
we consider a new realistic setting called Noisy UniDA, in which classifiers
are trained with noisy labeled data from the source domain and unlabeled data
with an unknown class distribution from the target domain. This paper
introduces a two-head convolutional neural network framework to solve all
problems simultaneously. Our network consists of one common feature generator
and two classifiers with different decision boundaries. By optimizing the
divergence between the two classifiers' outputs, we can detect noisy source
samples, find ""unknown"" classes in the target domain, and align the
distribution of the source and target domains. In an extensive evaluation of
different domain adaptation settings, the proposed method outperformed existing
methods by a large margin in most settings.","Deep neural networks (DNNs) have achieved impressive results with largescale annotated training samples, but the performance declines when the domain of the test data dif fers from the training data. To address this type of distribu tion shift between domains with no extra annotations, unsu pervised domain adaptation (UDA) has been proposed to learn a discriminative classiÔ¨Åer while there is a shift be tween training data in the source domain and test data in the target domain [1, 8, 9, 11, 25, 27, 27, 29, 33, 36]. Most existing domain adaptation methods assume that the source and target domains completely share the classes, but we do not know the class distribution of samples in the target domain in realworld UDA. Universal domain adap Figure 1: Problem setting of Noisy UniDA. Our proposed setting assumes that some source samples have corrupted labels, some classes of the source domain do not appear in the target domain, and the classes of some target samples are not shared by the source domain. tation (UniDA) [41] is proposed to remove the constraints on the label sets, where target samples may contain un known samples belonging to classes that do not appear in the source domain and some source classes may not appear in the target samples. However, UniDA is still an ideal sce nario, where existing UniDA methods require source sam ples with correct annotations to train the model. This re quirement limits the application of existing UniDA meth ods in real domain adaptation problems, where clean and highquality datasets are time consuming and expensive to collect. Data can more easily be collected from a crowd sourcing platform or crawled from the Internet or social me dia, but such data are inevitably corrupted with noise ( e.g. YFCC100M [35], Clothing1M [40], and ImageNet [3]). Hence, we consider a new realistic setting called ‚ÄúNoisy Universal Domain Adaptation‚Äù (Noisy UniDA), as shown in Fig. 1, which has the following properties: ‚Ä¢ Labeled data of the source domain contains noisy la bels.1 ‚Ä¢ Some classes of the source domain do not appear in the target domain, and these classes are named source 1The labels of target samples are not considered because they are not available in the setting of UDA. 1arXiv:2104.00246v1  [cs.CV]  1 Apr 2021private classes. ‚Ä¢ Some classes of the target domain are not shared by the source domain, and these classes are named target private classes. Some existing methods [28, 17, 30, 7, 41] aim to solve certain parts of Noisy UniDA. For example, [30] attempted to train domainadaptive models on noisy source data, [7] worked on the partial problem that the source private classes are absent from the target domain, [28] and [17] attempted to solve the openset problem of target private classes, and [41] addressed the settings with the partial problem and the openset problem together. However, a method that can solve all these problems at the same time does not exist. Instead of solving each problem separately, we focus on the divergence of DNNs to address all the problems of Noisy UniDA. Inspired by Cotraining for multiview learn ing and semisupervised learning [4, 31], when different models having different parameters are trained on the same data, they learn distinct views of each sample because they have different abilities to learn. As a result, different mod els in each view would agree on the labels of most samples, and it is unlikely for compatible classiÔ¨Åers trained on in dependent views to agree on a wrong label. We Ô¨Ånd this property can be effective in Noisy UniDA, where the noisy source samples have wrong labels, and target private sam ples can also be considered to have incorrect labels because their true label is not contained in the label set. When these data are input to different networks, the networks are more likely to output different results because they have differ ent parameters. Therefore, we utilize a twohead network architecture with two independent classiÔ¨Åers to detect all these unwanted samples simultaneously. The proposed twohead network consists of one common feature generator and two separate label classiÔ¨Åers for clas siÔ¨Åcation. The two classiÔ¨Åers are updated by the same data at the minibatch level, but they are initialized differently to obtain different classiÔ¨Åers. To detect noisy source samples in each minibatch, we calculate the divergence between the two classiÔ¨Åers‚Äô outputs on the source data, and only source samples with small divergences are chosen to update the network by supervised loss. Using the same principle, tar get samples with larger divergence are more likely to be tar get private samples, and we further separate the divergence of the classiÔ¨Åers on common and target private samples to reject target private samples. Consequently, we align the distributions of the clean samples from the common classes shared by both domains, where the methods that align the entire distribution are inÔ¨Çuenced by incorrect source labels, the source private classes and target private classes. We evaluated our method on a diverse set of domain adaptation settings. In many settings, our method outper forms existing methods by a large margin. We summarize the contributions of this paper as follows:Method Noisy labels Partial DA Openset DA DANN [10]    TCL [30]    ETN [7]    STA [17]    UAN [41]    DANCE [24]    Proposed    Table 1: Summary of recent related methods. UniDA con sists of Partial DA and Openset DA. Our proposed method is the only method that covers all the settings. ‚Ä¢ We propose a novel experimental setting and a novel training methodology for noisy universal domain adap tation (Noisy UniDA). ‚Ä¢ We propose a divergence optimization framework to detect noisy source samples, Ô¨Ånd target private sam ples, and align the distributions of the source and tar get domains according to the divergence of two label classiÔ¨Åers. ‚Ä¢ We evaluate our method across several realworld do main adaptation tasks. 2. Related Work "
613,Centrality and Consistency: Two-Stage Clean Samples Identification for Learning with Instance-Dependent Noisy Labels.txt,"Deep models trained with noisy labels are prone to over-fitting and struggle
in generalization. Most existing solutions are based on an ideal assumption
that the label noise is class-conditional, i.e., instances of the same class
share the same noise model, and are independent of features. While in practice,
the real-world noise patterns are usually more fine-grained as
instance-dependent ones, which poses a big challenge, especially in the
presence of inter-class imbalance. In this paper, we propose a two-stage clean
samples identification method to address the aforementioned challenge. First,
we employ a class-level feature clustering procedure for the early
identification of clean samples that are near the class-wise prediction
centers. Notably, we address the class imbalance problem by aggregating rare
classes according to their prediction entropy. Second, for the remaining clean
samples that are close to the ground truth class boundary (usually mixed with
the samples with instance-dependent noises), we propose a novel
consistency-based classification method that identifies them using the
consistency of two classifier heads: the higher the consistency, the larger the
probability that a sample is clean. Extensive experiments on several
challenging benchmarks demonstrate the superior performance of our method
against the state-of-the-art.","Deep learning has shown transformative power in various realworld applications but is notoriously datahungry[10,11,29,9,21,45]. There are some other alterna tives which try to reduce the cost of human labor for data annotation, such as ‚ãÜCorresponding authors are Guanbin Li and Yizhou Yu.arXiv:2207.14476v1  [cs.CV]  29 Jul 20222 G. Zhao et al. : Decision boundary of IDN : Ground truth class boundary : Samples with different ground truth : Samples with different IDN labels Fig. 1: Example of IDN. The different shapes of the markers represent different ground truth classes. The different colors of the markers represent the noisy (IDN) labels. Different from random noise, IDN samples tend to be distributed near the ground truth class boundary, thus confusing the classifier and leading to overfitted decision boundaries. crawling web images and using machinegenerated labels. However, such data are usually noisy, which impedes the generalization of deep learning models due to overfitting. Addressing the aforementioned issue, Learning with Noisy Labels (LNL) was proposed as a new topic and has attracted increasing attention in both academia and industry. Existing LNL methods mostly focus on the learning with class conditional noise (CCN), which aims to recover a noise transition matrix that contains classdependent probabilities of a clean label flipping into a noisy label. However, CCN is too ideal for realworld LNL as it ignores the dependence of noise on the content of individual images, a.k.a. instancedependent noise (IDN). Unlike random noise or CCN that can be countered by collecting more (noisy) data[4], IDN has some important characteristic that makes it difficult to be tack led. First, classifiers can easily overfit to the IDN because the noisy labels are dependent on sample features. As Fig. 1 shows, mislabeled IDN samples (sam ples with the same shape but with different colors) share similar image features to their mislabeled classes, and thus tend to be distributed near the boundary between their ground truth class and the mislabeled class. As a result, the clas sifier can easily be confused and overfits to IDN samples, leading to specious decision boundaries (red lines in Fig. 1). In addition, the challenge of IDN can be further amplified in the presence of interclass imbalance and differences. Con sider Clothing1M [38], an IDN dataset verified by [3], in which the noise is highly imbalanced and asymmetric. In Clothing1M, the IDN samples are unevenly dis tributed as the samples from similar classes ( e.g. sweater and knitwear) can be extremely ambiguous, while those from other classes ( e.g. shawl and underwear) are easily distinguishable. Such unevenly distributed IDN samples can be further amplified by the class imbalance problem, as there is no guarantee of a balanced dataset due to the absence of ground truth labels.TwoStage Clean Samples Identification for Learning with IDN 3 Shawl(92%),  Knitwear(3%),  Windbreaker(1%), ‚Ä¶‚Ä¶ Sweater(11%),  Knitwear(62%), TShirt(14%), ‚Ä¶‚Ä¶ Vest(46%), Dress(27%), TShirt(9%), ‚Ä¶‚Ä¶‚úî ‚úò ‚úò ‚úî ‚úò ‚úò ‚úî ‚úò ‚úò Fig. 2: The transition matrix of Clothing1M copied from [38]. The distribu tion of noisy labels are highly imbalanced. Some classes are almost clean (e.g. Shawl) while some classes has more mislabeled samples than correct labels (e.g. Sweater). In this paper, we follow DivideMix [17] that formulates LNL as a semi supervised learning problem and propose a novel twostage method to identify clean versus noisy samples in the presence of IDN and the class imbalance prob lem. In the first stage, we employ a classlevel featurebased clustering procedure to identify easily distinguishable clean samples according to their cosine simi larity to the corresponding classwise prediction centers. Specifically, we collect the normalized features of samples belonging to different classes respectively and calculate their classwise centers located on a unit sphere. Then, we ap ply Gaussian Mixture Model (GMM) to binarily classify the samples according to their cosine similarity to their corresponding class centers and identify the ones closer to class centers as clean samples. Notably, we propose to augment the GMM classification by aggregating rare classes based on their prediction entropy, thereby alleviating the impact of the class imbalance problem. In the second stage, we propose a consistencybased classification method to identify the hard clean samples that are mixed with IDN samples around the ground truth class boundaries. Our key insight is that such clean samples can be identi fied by the prediction consistency of two classifiers. Compared to IDN samples, clean samples should produce more consistent predictions. Specifically, we in corporate two regularizers into the training: one applied to the feature extractor to encourage it to facilitate consistent outputs of the two classifiers; one applied to the two classifiers to enforce them generating inconsistent predictions. Af ter training, we use another GMM to binarily classify the samples with smaller GMM means as clean samples. After identifying all clean samples, we feed them into the semisupervised training as labeled samples, thereby implementing our learning with instancedependent noisy labels. In summary, our contributions could be summarized as: ‚ÄìWe propose a method that delving into the instancedependent noise, and design a classlevel feature clustering procedure focusing on the imbalanced and IDN samples detection.4 G. Zhao et al. ‚ÄìWe further propose to identify the hard clean samples around the ground truth class boundaries by measuring the prediction consistency between two independently trained classifiers, and further improves the accuracy of clean versus noisy classification. ‚ÄìOur method achieves stateoftheart performance in some challenging bench marks, and is proved to be effective in different kinds of synthetic IDN. 2 Related Work "
614,NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing.txt,"Large-scale datasets in the real world inevitably involve label noise. Deep
models can gradually overfit noisy labels and thus degrade model
generalization. To mitigate the effects of label noise, learning with noisy
labels (LNL) methods are designed to achieve better generalization performance.
Due to the lack of suitable datasets, previous studies have frequently employed
synthetic label noise to mimic real-world label noise. However, synthetic noise
is not instance-dependent, making this approximation not always effective in
practice. Recent research has proposed benchmarks for learning with real-world
noisy labels. However, the noise sources within may be single or fuzzy, making
benchmarks different from data with heterogeneous label noises in the real
world. To tackle these issues, we contribute NoisywikiHow, the largest NLP
benchmark built with minimal supervision. Specifically, inspired by human
cognition, we explicitly construct multiple sources of label noise to imitate
human errors throughout the annotation, replicating real-world noise, whose
corruption is affected by both ground-truth labels and instances. Moreover, we
provide a variety of noise levels to support controlled experiments on noisy
data, enabling us to evaluate LNL methods systematically and comprehensively.
After that, we conduct extensive multi-dimensional experiments on a broad range
of LNL methods, obtaining new and intriguing findings.","Largescale labeled data has become indispensable in the notable success of deep neural networks (DNNs) in various domains and tasks (Russakovsky et al., 2015; Wang et al., 2019). Due to imper fect sources like crowdsourcing and web crawl ing (Xiao et al., 2015; Zhang et al., 2017b; Lee Corresponding author. 1The dataset is publicly available at https://github. com/tangminji/NoisywikiHow .Input Output (a) Take prescription weight loss medications. Losing Weight(b) Check calories on food packaging. (c) Include cultural and ethnic foods in your plan. (d) Talk about food differently. Table 1: Instances (a)‚Äì(d) depict examples of our task. Input : a procedural event. Output : a plausible inten tion toward that event. et al., 2018), datasets frequently include realworld label noise (Chen et al., 2021), which may induce model overÔ¨Åtting to noisy labels and hurt the gener alization of deep models (Zhang et al., 2017a; Wu et al., 2022a,b). To alleviate this issue, learning with noisy labels (LNL) methods for robustly train ing deep models have been studied extensively. Due to the lack of appropriate benchmarks, pre vious research often studied synthetic label noise to simulate realworld label noise (Zhang et al., 2018; Lukasik et al., 2020). As a general and real istic noise, realworld noise may have several noise sources (i.e., be heterogeneous ) (Northcutt et al., 2021) and be instancedependent (i.e.,P(~yjy;x), where the probability of an instance being assigned to the incorrect label ~ydepends on the original groundtruth label yand datax) (Han et al., 2021; Song et al., 2022). However, synthetic noise is generated from an artiÔ¨Åcial distribution and is thus instanceindependent (i.e.,P(~yjy)), which may not always work well in practice. Recently, various benchmarks for learning with realworld noisy labels have been proposed across Ô¨Åelds like computer vision (CV) (Li et al., 2017), audio signal processing (ASP) (Gemmeke et al., 2017), and natural language processing (NLP) (Hedderich et al., 2021). To fully evalu ate robust learning methods with realworld label noise, benchmarks should be as close to realworld scenarios as possible. Meanwhile, controlled exarXiv:2305.10709v1  [cs.CL]  18 May 2023periments are encouraged to verify whether LNL methods can remain effective over a wide range of noise levels (Jiang et al., 2020). Nevertheless, the noise levels in most datasets are Ô¨Åxed and unknown, resulting in uncontrolled label noise (Fonseca et al., 2019a; Song et al., 2019). Moreover, the noise therein often comes from the same or ambiguous sources (Li et al., 2017; Jiang et al., 2020), which conÔ¨Çicts with the heterogeneous characteristics of realworld noise. These problems prevent a better understanding of LNL methods. To bridge this gap, we present NoisywikiHow, a new NLP benchmark for evaluating LNL meth ods focusing on the intention identiÔ¨Åcation task. Intention identiÔ¨Åcation promotes numerous down stream natural language understanding tasks, from commonsense reasoning (Sap et al., 2019) to di alogue systems (Pepe et al., 2022). Additionally, the complexity of the task (total of 158 categories) facilitates a deeper investigation of the efÔ¨Åcacy of LNL approaches. The task form is shown in Ta ble 1. To make the benchmark more representative of realworld scenarios, we propose a practical as sumption: Realworld label noise in a dataset is mainly induced by human errors, regardless of whether the dataset‚Äôs construction is automated or crowdsourced. Existing psychological and cog nitive evidence further supports our hypothesis. It shows that different annotators have different pref erences and biases (Beigman and Klebanov, 2009; Burghardt et al., 2018), which means human la beling errors typically result from multiple noise sources. Furthermore, humans may make random labeling errors due to random attention slips. But they are more likely to produce label noise when labeling hard cases (Klebanov et al., 2008) (i.e., noise is instancedependent), such as instance (c) in Table 1. Motivated by this human cognition, we Ô¨Årst col lect data from the wikiHow website,2which con tains a collection of professionally edited howto guideline articles, providing a vast quantity of clean scripts and corresponding categories for free to help achieve controlled experiments and ensure benchmark quality. After that, we explicitly in ject a variety of noise sources into clean data to replicate human annotation errors, thus introducing realworld label noise into the benchmark. Notably, training samples in our benchmark exhibit a long 2https://www.wikihow.comtailed class distribution, which is in line with the facts, i.e., data in realworld applications is heav ily imbalanced (Van Horn et al., 2018; Liu et al., 2019b). Besides, we achieve minimal human su pervision by using a series of automated labeling procedures, saving lots of time and human effort. To evaluate NoisywikiHow, we carry out exten sive experimentation across various model archi tectures and noise sources, execute plentiful LNL methods on our benchmark, compare the more real istic realworld noise with the extensively studied synthetic noise, and investigate a case study and longtailed distribution characteristics. 2 Related Work "
615,ExpertNet: Adversarial Learning and Recovery Against Noisy Labels.txt,"Today's available datasets in the wild, e.g., from social media and open
platforms, present tremendous opportunities and challenges for deep learning,
as there is a significant portion of tagged images, but often with noisy, i.e.
erroneous, labels. Recent studies improve the robustness of deep models against
noisy labels without the knowledge of true labels. In this paper, we advocate
to derive a stronger classifier which proactively makes use of the noisy labels
in addition to the original images - turning noisy labels into learning
features. To such an end, we propose a novel framework, ExpertNet, composed of
Amateur and Expert, which iteratively learn from each other. Amateur is a
regular image classifier trained by the feedback of Expert, which imitates how
human experts would correct the predicted labels from Amateur using the noise
pattern learnt from the knowledge of both the noisy and ground truth labels.
The trained Amateur and Expert proactively leverage the images and their noisy
labels to infer image classes. Our empirical evaluations on noisy versions of
CIFAR-10, CIFAR-100 and real-world data of Clothing1M show that the proposed
model can achieve robust classification against a wide range of noise ratios
and with as little as 20-50% training data, compared to state-of-the-art deep
models that solely focus on distilling the impact of noisy labels.","The everincreasing selfgenerated contents on social media, e.g., Instagram images, power up the deep neural networks, but also aggravate the challenge of noisy data. Large portion of images accessible on the public domain come with labels which are unfortunately noisy due to careless annotations [1, 28] or even adversary strategies [4, 13, 22]. The learning capacity of deep neural networks is shown to be hindered by such noisy labels [30] due to the memo rization effect of networks. ClassiÔ¨Åcation accuracy on standard image benchmarks degrades drastically in the presence of dirty labels. For example, the accuracy of a trained AlexNet to classify CIFAR10 images drops from 77% to 10%, when trained on noisy labels [30]. c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2007.05305v2  [cs.LG]  13 Jul 20202 GHIASSI, BIRKE, CHEN: EXPERTNET Motivated by the signiÔ¨Åcant impact of noisy labels, the prior art [7] derives different robust deep networks with a central theme to distill the inÔ¨Çuence of noisy labels in the model training process without the knowledge of the label ground truth. As a result, the learned networks can robustly classify images in a standalone manner. D2L [26] estimates the Local Intrinsic Dimension (LID) at each epoch as a proxy to indicate the existence and impact of dirty labels. Coteaching [5] trains two networks simultaneously by exchanging the weights updated from possibly clean data. Forward [19] uses a noiseaware correction matrix to correct labels and train the network. Bootstrap [20] has a loss function which combines predicted and noisy labels. While prior art signiÔ¨Åcantly improves the robustness of deep networks, the preassumed scenarios overlook the opportunity of noisy labels. On the one hand, today‚Äôs image data are often bundled with labels of questionable quality and detrimental impact on the learning. On the other hand, such labels provide auxiliary information which can compliment the learnt knowledge of deep networks trained solely on image inputs. The core idea behind visual semantic models, e.g., DeVise [2], is to combine the learning capacities of labeled images and annotated data. CGAN [17] (conditional generative adversarial network) improves the quality of images synthesized by the generator network via additional label information and RCGAN [23] further addresses the challenge of dirty labels for CGAN. In this paper, we advocate to leverage the noisy labels as an additional feature to derive a stronger classiÔ¨Åer. We consider learning scenarios where at training time both the ground truth and noisy labels are available, and only noisy labels at inference time. In particular difÔ¨Åcult classiÔ¨Åcation problems, whose labels require a high degree of expertise, can Ô¨Åt this scenario well. One such example is cancer detection from medical images. This is a daunting task, and even trained experts are prone to make errors. Hence, these images are evaluated by multiple doctors of varying expertise. In such a setting, both noisy (Ô¨Årst evaluation by one expert) and true labels (e.g., stemming from a committee or subsequent indepth exams) are available at the same time. We derive a robust network, namely ExpertNet, composed of Amateur and Expert, where the former classiÔ¨Åes images based on the feedback from Expert and the latter learns how to correct the output of Amateur like human experts. Both models are trained simultaneously at each minibatch. Amateur learns to classify the input images to the corrected labels from Expert, and the softmax output of Amateur plus the given labels are inputs to train Expert to match the ground truth. Amateur can be seen as a regular image classiÔ¨Åer, which Expert helps it to be aware of the presence of noisy labels. Such trained Amateur and Expert can then classify images based on the image and corresponding noisy label. We empirically evaluate ExpertNet on synthetic noise injected into CIFAR10 and CIFAR 100, and noise drawn from real world contained in Clothing1M. For a fair comparison with stateoftheart robust deep models, we present the classiÔ¨Åcation accuracy in both Amateur only and complete ExpertNet model under different subsets of training inputs. ExpertNet consistently outperforms existing imageonly models, i.e., D2L, Coteaching, Forward and Boostrap, especially for CIFAR100. When using the same amount of training data, Ex pertNet can achieve absolute accuracy improvements of 5% up to 30%. ExpertNet reaches similar or higher accuracy than imageonly models even with just 20% of training data in the case of CIFAR100. Our contribution can be summarized as follows. First, we derive a novel network frame work, i.e., ExpertNet, that turns noisy labels into auxiliary learning advantages via imitating human experts. Secondly, we signiÔ¨Åcantly improve the robustness of deep networks against noisy labels compared to models based on images only.GHIASSI, BIRKE, CHEN: EXPERTNET 3 1.1 Problem statement                              Robust Deep Network Image   Noisy Image Label  Training  Image   Image Label Prediction  Inference  Robust Deep Network Image Label Prediction  Robust Deep Network Image   Noisy Image Label  Training Image Label Prediction  Image   Inference   Robust Deep Network  Noisy Image Label  Image Label Prediction  Ground Truth  Figure 1: Training and testing image classiÔ¨Åers with noisy labels. The problem considered here is as follows. Images collected in the public domain are tagged with preexisting noisy labels, whose true classes can be corrupted. We assume label noises follow random distribution. We illustrate in Fig. 1 (black elements only) the learning procedure that is commonly deployed by robust deep networks [5, 19, 20, 26]. The deep networks are trained by a set of images and labels, which are noisy, meaning with incorrect label classes. The objective of the training process is to minimize the loss function, which may be modiÔ¨Åed to be noise tolerant [26]. The network architecture may consist of different components, e.g., two networks that parallelly [5] or sequentially [19] train each other via stochastic gradient descent. In the inference phase, images are then fed into the trained network, and the prediction accuracy is computed based on true labels. The core idea behind such a learning process is to Ô¨Ålter out the negative impact of noisy labels during training and learn a model from clean information. In contrast to indirectly learn the label noise dynamics, our core idea is to leverage noisy labels as part of the training and inference input, as shown in Fig. 1 (including green el ements), to directly learn the noisy label dynamics and incorporate that as auxiliary input into the training process. To such an end, the ground truth of labels is assumed from human experts or oracles and provided as part of the training input. Essentially, the networks are trained by three inputs: images, their noisy labels, and the ground truth labels. Afterward, the trained network will be tested on images and their noisy labels. The classiÔ¨Åer can then classify images based on image inputs and limited label info. 2 Related Work "
